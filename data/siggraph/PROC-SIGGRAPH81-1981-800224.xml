<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>08-03-1981</start_date>
		<end_date>08-07-1981</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Dallas]]></city>
		<state>Texas</state>
		<country>USA</country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>800224</proc_id>
	<acronym>SIGGRAPH '81</acronym>
	<proc_desc>Proceedings of the 8th annual conference</proc_desc>
	<conference_number></conference_number>
	<proc_class>conference</proc_class>
	<proc_title>Computer graphics and interactive techniques</proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn>0-89791-045-1</isbn>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>1981</copyright_year>
	<publication_date>08-03-1981</publication_date>
	<pages>337</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source>ACM Order Number: 428810</other_source>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node>I.3</cat_node>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<ccs2012>
		<concept>
			<concept_id>0.10010147.10010371</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010371</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
			<concept_significance>500</concept_significance>
		</concept>
	</ccs2012>
	<general_terms>
		<gt>Theory</gt>
	</general_terms>
	<chair_editor>
		<ch_ed>
			<person_id>P329952</person_id>
			<author_profile_id><![CDATA[81547262956]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[Doug]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Green]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[]]></affiliation>
			<role><![CDATA[Chairman]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
		<ch_ed>
			<person_id>P334115</person_id>
			<author_profile_id><![CDATA[81321495469]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>2</seq_no>
			<first_name><![CDATA[Tony]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Lucido]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[]]></affiliation>
			<role><![CDATA[Chairman]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
		<ch_ed>
			<person_id>PP39094251</person_id>
			<author_profile_id><![CDATA[81332499724]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>3</seq_no>
			<first_name><![CDATA[Henry]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Fuchs]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[]]></affiliation>
			<role><![CDATA[Chairman]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>1981</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>806783</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Filtering edges for gray-scale displays]]></title>
		<page_from>1</page_from>
		<page_to>5</page_to>
		<doi_number>10.1145/800224.806783</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806783</url>
		<abstract>
			<par><![CDATA[<p>While simple line-drawing techniques produce &#8220;jagged&#8221; lines on raster images, more complex anti-aliasing, or filtering, techniques use gray-scale to give the appearance of smooth lines and edges. Unfortunately, these techniques are not frequently used because filtering is thought to require considerable computation. This paper presents a simple algorithm that can be used to draw filtered lines; the inner loop is a variant of the Bresenham point-plotting algorithm. The algorithm uses table lookup to reduce the computation required for filtering. Simple variations of the algorithm can be used to draw lines with different thicknesses and to smooth edges of polygons.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Anti aliasing]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Gray scale]]></kw>
			<kw><![CDATA[Line drawing]]></kw>
			<kw><![CDATA[Raster displays]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Grayscale manipulation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Filtering</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Line and curve generation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31078486</person_id>
				<author_profile_id><![CDATA[81332502486]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Satish]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gupta]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Carnegie-Mellon University, Pittsburgh, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P246551</person_id>
				<author_profile_id><![CDATA[81100302488]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Sproull]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Carnegie-Mellon University, Pittsburgh, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bresenham, J. E. Algorithm for computer control of a digital plotter. IBM Systems Journal 4(1):25-30, July, 1965.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>359869</ref_obj_id>
				<ref_obj_pid>359863</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Crow, F. C. The aliasing problem in computer-generated shaded images. Comm. ACM 20:799-805, Nov., 1977.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807507</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Feibush, Eliot A., Levoy, Mark, and Cook, Robert L. Synthetic Texturing Using Digital Filters. Computer Graphics 14(3):294-301, July, 1980.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807509</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Leler, William J. Human Vision, Anti-aliasing, and the Cheap 4000 Line Display. Computer Graphics 14(3):308-313, July, 1980.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>359027</ref_obj_id>
				<ref_obj_pid>359024</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Pitteway, M. L. V. and Watkinson, D. J. Bresenham's Algorithm with Grey Scale. CACM 23(11):625-626, November, 1980.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Sproull Robert F. Using Program Transformations to Derive Line-Drawing Algorithms. Technical Report, Carnegie-Mellon University, Computer Science Department, 1981.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807508</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Warnock John E. The Display of Characters Using Gray Level Sample Arrays. Computer Graphics 14(3):302-307, July, 1980.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Filtering Edges for Gray-Scale Displays Satish Gupta Robert F. Sproull Computer Science Department Carnegie-Mellon 
University Pittsburgh, PA 15213 Abstract While simple line-drawing techniques produce "jagged" lines 
on raster images, more complex anti-aliasing, or filtering, techniques use gray-scale to give the appearance 
of smooth lines and edges. Unfortunately, these techniques are not frequently used because filtering 
is thought to require considerable computation. This paper presents a simple algorithm that can be used 
to draw filtered lines; the inner loop is a variant of the Bresenham point-plotting algorithm. The algorithm 
uses table lookup to reduce the computation required for filtering. Simple variations of the algorithm 
can be used to draw lines with different thicknesses and to smooth edges of polygons. Keywords: Computer 
Graphics, line-drawing, aJlti-aliasing, gray- scale, raster displays CR Categories: 8.2, 5.25 This research 
is supported by the Defense Advanced Research Projects Agency, Department of Defense, ARPA Order 3597, 
monitored by the Air Force Avionics Laboratory under contract F33615-78-C-1551. The views and conclusions 
contained in this document are those of the authors and should not be interpreted as representing the 
official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the 
U.S. Government. Permission to copy without fee all or part of this material is granted provided that 
the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and 
the title of the publication and its date appear, and notice is given that copying is by permission of 
the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific 
permission, 1. Introduction Computer-generated images that are not properly filtered and sampled display 
annoying visual effects known as a/iasing. The most renowned of these effects is the "jaggy" or "staircase" 
appearance of lines and edges that arises from sampling errors. On a display device that can present 
gray-scale images,these effects can be avoided by displaying lines and edges using gray values lying 
between white and black. To determine these intermediate intensities, the image must be filtered using 
an appropriate low-pass filter before it is sampled [2]. This task can be computationally expensive. 
In this paper we present an efficient algorithm for producing images with smooth edges of lines and polygons. 
We will describe the algorithm used to draw straight lines with unit thickness, and then discuss its 
variations for lines of different thicknesses and for edges of polygons. The algorithm is a variation 
of the point- plotting algorithm developed by Bresenham [1], which requires one integer comparison and 
one integer addition in the inner loop. The inner loop of our algorithm requires slightly more arithmetic 
precision and a table-lookup step. Unlike Bresenham's algorithm, line endpoints must be treated as a 
special case, and are the subject of Section 4. This paper does not present a new filtering technique, 
but rather shows how existing filtering techniques can be implemented efficiently. Doubtless algorithms 
of the sort we present are already in use; our objective is to promote the use of filtering by publicizing 
a simple method. 2. Filtering Filtering an image with a low-pass filter is an averaging process: the 
intensity of a pixel is determined by the image brightnesses within a small distance of the pixel, not 
by the brightness at a single point, such as the pixel's center. Filtering is controlled by a filter 
function, which describes the spatial distribution of light emitted by a pixel on the display. The filter 
function in effect supplies a weighting function for the averaging process. Filter functions used to 
prepare images need not match the light emission properties of the display exactly; in practice, a filter 
function is selected for its mathematical tractability and is altered until the image appears acceptable 
[7]. A popular approximate lifter is a conical function: the function has its maximum value at the center 
of a pixel and decreases linearly to zero at a distance r from the pixel center. The radius of the filter 
is r, measured using the convention that a unit distance is the distance between two adjacent pixel centers. 
The filter &#38;#169;1981 ACM O-8971-045-1/81-0800-0001 $00.75 function is normalized so that the enclosing 
volume is 1. Figure 1 shows a filter function of radius 1, which we shall use for the remainder of this 
paper? Figure 1 : Conical pixel filter. Crosses mark pixel centers. Figu re 2: Line intersecting a pixel. 
tPl F(p,1) Ipl FLo,1) 0/16 0.780 13/16 0.228 1/16 0.775 14/16 0.184 2/16 0.760 15/16 0.145 3/16 0.736 
16/16 0.110 4/16 0.703 17/16 0.080 5/16 0.662 18/16 0.056 6/16 0.613 19/16 0.036 7/16 0.558 20/16 0.021 
8/16 0.500 21/16 0.010 9/16 0.441 22/16 0.004 10/16 0.383 23/16 0.001 11/16 0.328 ~24/16 O.O00 12/16 
0.276 Table 1 : Values of pixel intensity given distance to line. 1Selecting a filter radius is a compromise 
between a desire for very smooth edges (choose a large r) and preserving high spatial frequencies (choose 
a small r). A filter of radius 1, on most displays, will produce edges that still appear somewhat wavy. 
A filter of radius 1.5 will produce quite smooth edges. Volume 15, Number 3 August 1981 When a line 
passes through a pixel, the pixel's intensity should be proportional to the volume of the cone intersected 
by the line (see Figure 2). Because of the circular symmetry of the filter, only two parameters are needed 
to determine the volume intersected: the thickness t of the line and the perpendicular distance p from 
the pixel center to the line center. Thus we may write I = F(p,t), where F is determined solely by the 
choice of filter function. Table 1 illustrates values of F for lines of thickness 1, assuming a conical 
filter, of radius 1. Note that F(p,t) = 0 for p > r + t/2. The table is obtained by convolving the filter 
function and the line intensity, i.e., by numerically integrating the filter function over the region 
covered by the line. Similar tables may be built for other values of t and r or for other filter functions. 
3. The algorithm The algorithm to draw a line rfiust compute the distance p between each pixel center 
and the line. To reduce computation, this calculation is performed incrementally, as in Bresenham's algorithm[I]. 
Our discussion is restricted to lines of unit thickness in the first octant (i.e., 0 <_ y < x); extensions 
to other regions are obvious. Such lines intensify two or three pixels in each column of pixels 2 (see 
Figure 3). The algorithm will keep track of the location of the center pixel and the perpendicular distance 
to the line's center from the pixel center. Suppose a line is to be drawn from (Xl, yt) to (x2, y2). 
We shall assume that line endpoints lie at pixel centers, and hence that Xl, Yt' x2' and Y2 are integers. 
If dx = x2-x t and dy = y2-Yl, then from our first octant assumption, the slope m = dy/dx has a value 
between 0 and 1. In algorithm A1 below, x and y track the central pixel in each column through which 
the line passes, and v is the vertical distance from that pixel to the line. This vertical distance v 
is a signed value; a positive value indicates that the center of the line is above the center of the 
pixel, and a negative value indicates the opposite. The variable s is a threshold distance used to decide 
whether the central pixel in the next column lies diagonally or horizontally across from the central 
pixel in the current column. =1 J J Figure 3: Three pixels are shaded in each column. 2The algorithms 
and tables presented in this paper are designed to produce images using 4-bit intensity values. A diagonal 
line at almost 45 degrees will actually intersect five rather than three pixets, but the top and bottom 
pixels are intensified at less than 0.2% of the maximum. Our algorithm ignores these pixels because a 
4-bit intensity value will record zero for such an intensity. If a wider range of intensities is availabte, 
the algorithm may be modified in an obvious way to illuminate more pixels in each column; the tables 
must also provide more precision. Algorithm AI: PROCEDURE PlotLfne(xl,yl,xZ,y2 : INTEGER); VAR x,y : 
INTEGER; v,m,s : REAL; m := (y2-yl)/(x2-xl); v := O; s := O.5-m; y := yt; FOR x := xl TO x2 DO BEGIN 
Shads plxsls at (x,y-1),(x,y),(x,y+l); IF (v > s) THEN BEGIN y := y+l; v := v+m-1; END ELSE V := v+m; 
END; The pixel at (x, y) is located at a vertical distance v from the line, and the pixels at (x, y-l) 
and (x, y+l) are at distances v-1 and v + 1 respectively. In order to determine the shade of the pixels, 
we need to compute the perpendicular distance p from the pixel to the line. The vertical distances are 
related to the perdendicular distances by a factor of c = dx/sqrt(dx 2 + dy2)), such that p = cv. Algorithm 
A2 shows the modifications to compute perpendicular distances. Algorithm A2: PROCEDURE PlotLine(xl,yt,x2,y2 
: INTEGER); VAR x,y : INTEGER; p,m,c,s : REAL; m := (y2-yl)/(x2-xl); c := 1/sqrt(m*m+l); p := O; s := 
(O.6-m)*c; y :=yl; FOR x := xl TO x2 DO BEGIN Shade ptxels at (x,y-1),(x,y),(x,y+l); IF (p > s) THEN 
 BEGIN y := y+t; p := p+(m-1)*c; END ELSE p := p+m*c; END; The two expressions (m-1)*c and m*c can be 
precomputed and do not have to be computed repeatedly in the inner loop. To compute the pixel shades, 
the absolute values of p, p-c, and p + c are used as indices into Table 1 to determine intensities at 
(x, y), (x, y-1) and (x, y + 1 ) respectively.  4. Line endpoints The algorithm presented in the previous 
section does not compute the sampled values of pixels lying on or near the endpoints of the line. The 
situation is illustrated in Figure 4, which shows an endpoint of a line. The pixels shown with their 
surrounding filters are not intensified properly by algorithm A2; in fact, some of them are not intensified 
at all. There are several methods available to compute such pixel intensities. As for other pixels near 
the line, the intensity of one of the pixels is obtained by convolvin 9 the filter function and the line, 
i.e., by integrating the filter function over the region covered by the line. This computation may be 
performed exactly using geometric operations [3], or may be approximated by sampling the image at points 
much more closely spaced than pixel centers. Both of these approaches are computationally expensive. 
Figure 4: Endpoint of a line, marked by a large dot, showing the six pixels that may be intensified. 
Slope = 0/16 Slope = 1/16 Slope = 2/16 0.000 0.056 0.000 0.063 0.000 0.072 O.O00 0.393 O.O00 0.390 O.O00 
0.390 O.O00 0.056 0.000 0.048 0.000 0.942 Slope = 3/16 Slope = 4/16 Slope = 5/16 0.000 0.082 0.000 0.094 
0.000 0.107 0.000 0.390 0.000 0.390 0.000 0.390 0.000 0.037 O.OOO 0.032 0.000 0.028 Slope = 6/16 Slope 
= 7/16 Slope = 8/16 0.000 0.121 0.000 0.136 0.000 0.152 0.001 0.390 0.001 0.390 0.002 0.391 0.000 0.025 
0.000 0.022 0.000 0.019 Slope = 9/16 Slope = 10/16 Slope = 11/16 0.000 0.169 0.000 0.187 0.000 0.206 
0.002 0.390 0.003 0.390 0.003 0.390 0.000 0.017 0.000 0.015 0.000 0.013 Slope = 12/16 Slope = 13/16 Slope 
= 14/16 0.000 0.225 0.000 0.245 0.000 0.264 0.004 0.390 0.005 0.390 0.006 0.390 0.000 0.012 0.000 0.010 
0.000 0.009 Slope -- 15/16 Slope = 16/16 0.001 0.284 0.001 0.304 0.007 0.390 0.007 0.391 0.000 0.008 
0.000 0.007 Table 2: Endpoints for lines with different slopes. The pixel intensities can be precomputed 
and stored in a table, as we did for lines in the preceding section. To display the endpoint of a line, 
the intensities of the six pixels in the vicinity are determined from the table. For the accuracy we 
desire, it is sufficient to compute a set of endpoints for lines with slopes between 0 and 1 at intervals 
of 1/16 (see Table 2). The entries in the table are in the same configuration as the six pixels shown 
in Figure 4. Using a combination of mirroring and transposition transformations, these endpoint intensities 
can be used for lines in every octant. 5. Variations Simple variations of the algorithm presented above 
can handle lines of different thicknesses and can produce smooth edges for polygons. Lines of different 
thickness can be produced by preparing different tables for various line thicknesses and using the thickness 
of the line to select an appropriate table. In addition, if the thickness is greater than one, the algorithm 
must be changed to illuminate more than three pixels in each column. The endpoint table must be modified, 
because more than six pixels may be illuminated near wide lines. Alternative geometries for line endpoints 
(see Figure 5) can be accommodated by building separate endpoint tables. The tables are built by numerically 
integrating the filter function in the region covered by the line. Figure 5: Alternative endpoint geometries 
for a line. Figure 6: Polygon edge intersecting a pixel.  Volume 15, Number 3 August 1981 Polygon edges 
can be produced in a similar manner by having a special table which contains the intensity values when 
the pixel is covered by the edge of a polygon (see Figure 6). Table 3 is used to look up the intensity 
of the pixel based on the perpendicular distance p from the edge of the polygon to the center of the 
pixel; p is negative if the pixel center lies outside the edge. Note that if p < -r, F(p) = 0; ifp > 
r, F(p) = 1. The tables required to draw lines can be derived from Table 3 because the part of the pixel 
covered by a line is the difference between the parts of the pixel covered by the two edges of the line. 
However, because line drawing is a frequent operation, it is advantageous to compile separate tables. 
p F(p) p FLo) ~-16/16 0.000 1/16 0.559 -15/16 0.001 2/16 0.617 -14/16 0.004 3/16 0.672 -13/16 0.010 4/16 
0.724 -12/16 0.021 5/16 0.772 -11/16 0.036 6/16 0.816 -10/16 0.056 7/16 0.855 -9/16 0.080 8/16 0.890 
-8/16 0.110 9/16 0.920 -7/16 0.145 10/16 0.944 -6/16 0.184 11/16 0.964 -5/16 0.228 12/16 0.979 -4/16 
0.276 13/16 0.990 -3/16 0.328 14/16 0.996 -2/16 0.383 15/16 0.999 -1/16 0.441 ~16/16 1.000 0/16 0.500 
Table. 3: Pixel intensities given distance to edge of polygon. Another desirable variation is the ability 
to experiment with different filters, because the aesthetic appearance of the output depends upon the 
filter used for anti-aliasing. The optimal filter will be different for different output devices. In 
this paper we have used a circularly symmetric filter which has the property that only the distance to 
the line is needed to compute the intensity of pixels. The use of an asymmetric filter will require knowledge 
of the slope of the line to compute intensities. This does not affect the algorithm significantly because 
we can use a few bits of the slope (2 or 3 depending upon the filter) to select a table that can then 
be used by our algorithm. The algorithm may be easily adapted to display lines of any shade on backgrounds 
of any shade by mixing intensities. The pixel intensity is / = ILFLO,t) + IB(1-F(p,t)), where I L is 
the line shade, and I B is the background shade. On color displays, the red, green and blue components 
can be mixed independently. The algorithm can be adapted to display lines and edges drawn between endpoints 
that are not integers; that is, that do not lie on the pixel grid. Such a scheme is necessary to avoid 
additional aliasing, for example the jitter in a moving image caused by quantizing endpoints to lie on 
pixel centers. To accommodate non-integer endpoints, two modifications must be made. First, the initialization 
of algorithm A2 must be changed to compute the (x, y) coordinate of the first pixel to be illuminated 
and to compute the initial value for p at this point. Algorithm A3 shows these modifications. Algorithm 
A3: PROCEDURE Plot, Ltns(xl,,yl,x?_,,y2 : INTEGER); VAR x,y : INTEGER; p,m,c,s : REAL; m := (yZ-yl)/(x2-xl); 
c := 1/sqrt(m,m+1); y% := yt+m'(round(xl_)-x%); y := round(yl); p := (yl-y)'c ; s := (O.5-m)'c; FOR x 
:= round(xt) TO round(x2) DO BEGIN Shads pixels at, (x,y-1),(x,y),(x,y+%); IF (p > s) THEN BEGIN y := 
y+%; p := p+(m-%)'c; END ELSE p := p+m*c; END; The second change is that samples near endpoints need 
to take account of the exact location of the line endpoint. We either have to spend a lot of processing 
to filter these pixels or use a much larger table to look up the filtered values. If the table contains 
endpoints at intervals of 1/16 for each of the coordinates and an interval of 1/16 for the slope, then 
the size of the table at six pixels per endpoint would be 29478 entries! 6. Precision One of the major 
accomplishments of Bresenham's algorithm is to perform line drawing computations using only integer additions 
and comparisons. Although we use floating-point numbers in the exposition above, the algorithms do not 
require the large range provided by a floating-point representation. To understand the precision required, 
let us examine the variable p in algorithm A2. Since the intensity table is spaced at intervals of 2 
.4 , we need to know p accurately to within 2 .4 to find the correct intensity value. Assume that the 
value of p is computed incrementally using at most 210 additions for a display that is 1024 pixels wide. 
Consequently, the expressions m°c and (m-1)*c must be known to an accuracy of 2 15 (i.e., if the error 
in representing one of these numbers is as much as 2 -15, 210 additions will accumulate an error of 2 
.5 , which is not enough to introduce an error in the intensity selected). Thus p can be replaced in 
the algorithm by an integer q = 215p; since p lies between -0.5 and + 0.5, q will lie between -2 TM and 
+2 TM. We observe too that p = (q/211)/16, and therefore that q/211 can be used as an index into Table 
1 3 The precision required in the incremental computation of p is a direct consequence of the size of 
the display (the number of incremental additions) and the intensity precision we are trying to achieve. 
If the function F(p, t) were linear, in order to achieve n bits of gray- scale precision, n bits of distance 
precision would be required, and therefore the lookup tables would record entries spaced a distance L5n 
apart. Although F is not precisely linear, the deviations are sufficiently small that the distance precision 
need be no greater than the intensity precision. The choice of gray- scale precision is an aesthetic 
one, which depends upon factors 3The precision problem is actually somewhat more intricate than this 
paragraph implies. The problem is that even the tiniest error in the value of p may change the outcome 
of the test in the inner loop. However, the shading v~ill not be affected by this change, because the 
distance p is still sufficiently accurate. When the Bresenham algorithm is used on binary devices, this 
error would result in a non- optimat line being displayed [6].  Volume 15, Number 3 August 1981 such 
as type of output device, viewing distance, etc. [4]. Thus the choice of intensity precision will determine 
the number of entries needed in the various tables of the algorithms. 7. Conclusion Our objective in 
presenting a fast algorithm for producing properly filtered images of lines and edges on gray-scale displays 
is to demonstrate that filtering need not be expensive. The algorithm achieves its speed by using table 
lookup to avoid complex filtering computations. The computations are so simple that it is not reasonable 
to exploit gray-scale and not filter properly (e.g., in [5]). The algorithm leaves open the important 
problem of sampling two or more interacting lines or edges properly. The algorithm presented by Feibush 
et. a/. [3] treats this problem properly, but at considerable computational expense. Approximations often 
produce usable results, e.g., taking the maximum intensity or multiplying the intensities of the two 
objects that lie in a single pixel. References [1] Bresenham, J.E. Algorithm for computer control of 
a digital plotter. ~aM Systems Journa/ 4(1):25-30, July, 1965. [2] Crow, F.C. The aliasing problem in 
computer-generated shaded images. Comm. ACM 20:799-805, Nov., 1977. [3] Feibush, Eliot A., Levoy, Mark, 
and Cook, Robert L. Synthetic Texturing Using Digital Filters. Computer Graphics 14(3):294-301, July, 
1980. [4] Leler, William J. Human Vision, Anti-aliasing, and the Cheap 4000 Line Display. Computer 
Graphics 14(3):308-313, July, 1980. [5] Pitteway, M.L.V. and Watkinson, D.J. Bi'esenham's Algorithm with 
Grey Scale. CACM 23(11):625-626, November, 1980. [6] Sproull Robert F. Using Program Transformations 
to Derive Line-Drawing Algorithms. Technical Report, Carnegie-Mellon University, Computer Science Department, 
1981. [7] Warnock John E. The Display of Characters Using Gray Level Sample Arrays. Computer Graphics 
14(3):302-307, July, 1980.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806784</article_id>
		<sort_key>7</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Filtering high quality text for display on raster scan devices]]></title>
		<page_from>7</page_from>
		<page_to>15</page_to>
		<doi_number>10.1145/800224.806784</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806784</url>
		<abstract>
			<par><![CDATA[<p>Recently several investigators have studied the problem of displaying text characters on grey level raster scan displays. Despite arguments suggesting that grey level displays are equivalent to very high resolution bitmaps, the performance of grey level displays has been disappointing. This paper will show that much of the problem can be traced to inappropriate antialiasing procedures. Instead of the classical (sin x)/x filter, the situation calls for a filter with characteristics matched both to the nature of display on CRTs and to the human visual system. We give examples to illustrate the problems of the existing methods and the advantages of the new methods. Although the techniques are described in terms of text, the results have application to the general antialiasing problem&#8212;at least in theory if not in practice.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Filtering</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Grayscale manipulation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Line and curve generation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39074081</person_id>
				<author_profile_id><![CDATA[81100653012]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kajiya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, California Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332250</person_id>
				<author_profile_id><![CDATA[81100113493]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[M.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ullner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, California Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>908845</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Blinn J.F. (1978) "Computer Display of Curved surfaces" U. Utah Ph.D. Thesis, December 1978.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Collatz L. and Wetterling W. (1975) OPTIMIZATION METHODS, Springer Verlag, Berlin.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Cornsweet T. (1970) VISUAL PERCEPTION, Academic Press, New York.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Crow F.C. (1976) "The aliasing problem in Computer-synthesized shaded images" U. Utah Computer Science Tech Rept. UTEC-Csc-76-015.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Gabriel S.A. (1977) Private communication.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Graham N. and Nachmias J. (1971) "Detection of grating patterns containing two spatial frequencies: a comparison of single channel and multiple channel models" Vision Res., v. 11, pp. 251-259.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>908989</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Kajiya, J.T. (1979) "Toward a Mathematical Theory of Perception", Ph.D. Thesis, U. Utah.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Kajiya J.T. (1981) "On fast methods for two dimensional spectral factorization", to appear.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Levinson N. (1947) "The Wiener RMS error criterion in filter design and prediction" J. Math. Phys. v.25, no. 4, pp. 261-278.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Mostafavi H. and Sakrison D.J. (1976) "Structure and properties of a single channel in the human visual system" Vision Res., v. 16, pp. 957-968.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Oetken G.,Parks T.W., Schuessler H.W. (1975) "New results in the design of digital interpolators" IEEE Trans. ASSP, v.ASSP-23, pp. 301-309.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>108781</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Pratt W.K. (1978) DIGITAL IMAGE PROCESSING, Wiley-Interscience.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>26851</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Rudin W. (1966) REAL AND COMPLEX ANALYSIS, McGraw-Hill, New York.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Sachs M.B., Nachmias J. and Robson J.G. (1971) "Spatial frequency channels in human vision" J. Optical Soc. Am., v. 61, pp. 1176-1186.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Sakrison D.J. (1977) "On the role of the observer and a distortion measure in image transmission" IEEE Trans. on Communications, v. COM-25, pp. 1251-1267.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Schafer R.W. and Rabiner L.R. (1973) "A digital signal processing approach to interpolation" Proc. IEEE v.61, pp. 692-702.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Trench W.F. (1964) "An algorithm for the inversion of finite Toeplitz matrices" J. SIAM v. 12, no. 3, pp. 515-522.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Seitz C., et. al. "Digital Video Display system with a Plurality of Grey-scale levels" US Patent 4,158,200.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807508</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Warnock J.E. (1980) "The Display of Characters Using Gray level Sample arrays" ACM SIGGRAPH80, pp. 302-307.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 FILTERING HIGH QUALITY TEXT FOR DISPLAY ON RASTER 
SCAN DEVICES by d. Kajiya and M. Ullner Computer Science Department California Institute of Technology 
Ahstract. Recently several investigators have studied the problem of displaying text characters on grey 
level raster scan displays. Despite arguments suggesting that grey level displays are equivalent to very 
high resolution bitmaps, the performance of grey level displays has been disappointing. This paper will 
show that much of the problem can be traced to inappropriate antialiasing procedures. Instead of the 
classical (sin x)/x filter, the situation calls For a filter with characteristics matched hath to the 
nature of display on CRTs and to the human visual system. We give examl)les to illustrate the problems 
of the existing melhods and the advantages of the new methods. Although the techniques are described 
in terms of text, the results have application to the general antialiasing problem--at least in theory 
if not in practice. I. INTRODUCTION lhe computer age nearly destroyed quality printed and displayed text. 
Many of us remember our First sight of the ugly, uneven impression of a high speed chain printer. At 
the time, it seemed inevitable that high technology would sweep more beautiful--and less utilitarian--methods 
of text display aside for all but the most premium of uses. Recently this prospect has changed. With 
the growing availability of raster scan displays we have witnessed a technology with the capability of 
generating alphanumeric text that is more than just readable but pleasant to view as well. It is an exciting 
dream of men like Donald Knuth to be able to compose locally and transmit for publication high quality 
text containing multiple fonts and mathematical equations. This dream would be made more attractive if 
an author would be able to see the result immediately, rather than having to wait several days for the 
output of a $100,000 machine. The ideal would be to close the loop: to make available to the author an 
inexpensive real time device able to display high performance images. Permission to copy without fee 
all or part of this material is granted provided that the copies are not made or distributed for direct 
commercial advantage, the ACM copyright notice and the title of the publication and its date appear, 
and notice is given that copying is by permission of the Association for Computing Machinery. To copy 
otherwise, or to republish, requires a fee and/or specific permission. Furthermore, the effect of high 
quality real time displays on the activities of computer science itsel.F has yet to he accurately assessed. 
But--aside from the creation of the word processing industry--the introduction of expanded codes to include 
relatively mundane Features such as lower case characters has tremendously changed the flavor if not 
the substance of programmin(t: surely none of us would wish i:o return to the 5-hit Baudot code! More 
importantly, the astonishing power and economy of computer languages and mathematical notations which 
incorporate special symbols (such as APL and symbolic logic) certainly has hidden lessons for the computer 
science community. Many researchers soon discovered that the problem of displaying synthetically generated 
images on raster scan devices was a nontrivial task [Crow 1976, aline 1979]. lhe so called aliasing prohlem 
was encountered due to the high frequency content of artificially synthesized images, lhese researchers 
developed methods to overcome this problem which can he viewed alternatively as interpolation of brightnesses 
between pixels or filtering with a triangular convolution kernel. We shall, for definiteness, refer to 
this popular scheme as triangular Filtering. lhe First attempts to display text on raster scan devices 
used these intuitive filtering schemes that worked surprisingly well in practice [Warnock Ig80, Seitz]. 
It is the aim of this paper to analyze the performance of these schemes For the general image case as 
well as the text case. We also propose a new method for choosing the pixel values which make up a synthetic 
image, show some preliminary results, and finally discuss the future directions that this research may 
take. By now, th~ aliasing problem for computer generated images is well known to all in the field, as 
arc the frequency domain interpretation of the phenomenon and the first order approximation to its solution. 
We wish to examine i~ detail the performance of this first order approximation, It is well known that 
the triangular filtering algorithm is cheap, fast, easy to implement, and produces an adequate antialiased 
image for very many applications. There are other applications, however, that require higher performance. 
Text is just such an application. Characters consist almost entirely of sharp edges and contain small 
subl)ixel features, such as serifs. Also, the processing of text can be done orfline and the result 
stored in permanent memory. The method we present here is expensive, slow, and relatively hard to implement, 
but it produces higher quality images than the usual triangular filtering scheme. It is not now suitable 
for real time, or near real time, raster display applications. &#38;#169;1981 ACM O-8971-045-1/81-0800-0007 
$00.75 Computer Graphics Volume 15, Number 3 August 1981 It should be emphasized that these methods and 
analyses are applicable--at least in theory--to the general problem of antialiasing arbitrary images. 
We have chosen to focus on the display of text rather than arbitrary graphic objects because certain 
computational advantages accrue due to the small size of characters. If the computational implementations 
of these methods currently were not economically impracticable we would be reporting on the general synthetic 
image display problem as well. 2. AN ANALYSIS OF POPULAR ANTIAI_IASING SCIIEHES LINEAR FILTERING WITH 
A TRIANGULAR PSF. This section will analyze how well the triangular filtering algorithm does in removing 
aliasing while not otherwise distorting the picture. The details of the development are necessarily mathematical, 
but we present the key ideas here for those who want an overview of the section. There are two primary 
sources of error in the triangular filtering scheme. (I) The triangle interpolation kernel is not an 
ideal low pass filter and passes frequencies that are beyond the Nyquist limit. Thus, it is subject to 
aliasing. (2) The interpolation kernel does not take into account the reconstruction kernel. That is, 
it ignores the fact that pixels on the CRT display are Gaussian spots. The Gaussian spots are not ideal 
Iowpass filters either and given the usual focus setting the the frequency response of the reconstruction 
is far from flat. flow well does the usual scheme work? The answer depends, of course, on the nature 
of the images displayed, and the mathematics tells you how to calculate the answer for your image. The 
gist of the answer, though, is that for certain images like t.ext, there is plenty of room for improvement. 
lhe most popular scheme for antialiasing is to linearly filter the input signal with some sort of interpolation 
kernel [Schafer and Rabiner 1973, Oetken, et. al. |975]. We focus on the case of artificial images such 
as text and computer generated calligraphic and halftone images for display on raster scan frame buffers. 
There is some controversy about the characteristics of the optimum interpolation kernel with respect 
to the amount of ringing, and whether negative lobes are desirable [Gabriel 1977]. Many workers have 
settled on a triangular interpolation kernel as a simple compromise that gives good results in practice 
and is easy to compute [Crow 1976, Warnock 1980]. In the interest of concreteness we shall restrict the 
ensuing analysis to the triangular interpolation case; however, the reader can readily discern that the 
arguments involved are quite general. Parts of this analysis are similar to those found in [Pratt 1978]. 
The triangular PSF is shown in Figure 1 and is given by the equation: To perform the sampling and reconstruction 
we convolve the ideal image with the interpolation kernel, sample with raster pitch T, and reconstruct 
by convoIving the sampled signal with the reconstruction kernel. As is well known, these steps are best 
visualized in the frequency domain. The Fourier transform of k(~), the triangular kernel, is K(~) as 
shown in Figure 2. This is compared with the ideal Nyquist kernel of sin(F-jv"F)/{~/f~. The analytic 
expression for K(cO) is lhis figtire clearly shows a possible source of aliasing error allowed by this 
kernel. Namely for an input image f(~) with Fourier transform F(ua) the root mean square aliasing energy 
is given by D Now if F has most of its energy concentrated in the low frequencies then the aliasing error 
energy is quite Small since the two terms in Equation (2.1) are roughly the same. Unfortunately, most 
artificial images, and especially text, have a Fourier spectrum that resides almost exclusively in the 
high frequency portions of signal space. To get some idea of the energy error involved, let us take a 
"line source", viz. a line of delta functions. This situation is to be met, for example, in the very 
thin strokes of classic Roman capitals, and in the diagonal strokes of a capital A for the Bodoni typeface. 
In this case f(~) approaches a Dirac delta function whose Fourier transform, in turn, approaches a flat 
spectrum. Equation (2.1) then gives f° F or ~-~ J-~ the relative aliasing energy is roughly E (lJ+l'~ 
( i"i'l. J6). To perform the sampling step in the frequency domain we merely replicate the signal at 
the sampling frequency. Assume we have already folded in all the aliasing energy so that our modified 
signal appears as in Figure 3. Sampling now replicates this modified signal to something shown in Figure 
4. Now we can reconstruct the signal by passing it through a reconstruction filter. If the reconstruction 
kernel is the Nyquist kernel, then the signal is a low pass filtered version of the original. However, 
in this matter we are not free to exercise a choice for our reconstruction kernel, except for a very 
limited range. The reconstruction kernels available to us are fixed by the physics of the output devices 
at our disposal, whether they be electrostatic printer, COM devices, or CRT based displays. To take the 
most common example, it is well known that the spot luminance distribution for a CRT is Gaussian, the 
variance of which is set by the focusing. At the proper focus point, a flat-field raster just becomes 
smooth, this is given roughly at the point where ~ is the standard deviation of the Gaussian, Computer 
Graphics Volume 15, Number 3 August 1981 Now, the Fourier transform of this reconstruction kernel is 
Thus the Final output signal looks more like Figure 5 instead of appearing as in the normal case. In 
this picture we can see two sources of error arising from the mismatch of triangular and Gaussian kernel: 
Imaging errors and equalization errors. Imaging errors are produced by the leakage of spurious copies 
of the original signal. This is given by When the Gausslan spot Is focused properly this error is quite 
small. (Otherwise a flat field wouldn't have appeared flat.) By far the more serious error is caused 
by the mismatch between the triangle and Gaussian frequency responses. Let us for the-moment ignore the 
effects of aliasing and imaging, say by attempting to display an already perfectly bandlimited signal 
on a properly focused display (0-=T/.66). In this case the overall modulation transfer function, is given 
by lhis curve is plotted on a dB scale in Figure 6. Note that for the higher Frequencies the MTF is down 
by almost 10 (IB! Clearly, this amount of attenuation is causing a significant amount of sharpness loss, 
particularly in the Fine features of high quality fonts. Thtls, if one is constrained to use to the linear 
Filtering approach, a high frequency preemphasis is clearly called for--at the expense of an increased 
aliasing error tradeoff. OTHER KERNELS Many ad hoc schemes besides linear filtering have been proposed. 
Many are equivalent to linear filtering with triangular or other kernels. These include proportional 
weighting of the area of a given pixel covered, trapezoidal decomposition, contour smoothing, and nearest 
neighbor schemes. One may wonder if the above remarks apply to all interpolation kernels as well as the 
triangle. Furthermore, there are a wealth of possible nonlinear schemes that come to mind. One can imagine 
an Edison-type programme involving a massive amount of experiment in order to converge on the correct 
solution. There is, however, a non ad hoe approach that is closely related to the roots of the Whittaker-Shannon 
sampling theorem, from which the original frequency domain analysis is derived. 3. OPTIMLIM LINEAR SAMPLING 
AND RECONSTRUCTION Instead of choosing an arbitrary kernel and calctilating its performance, in this 
section we present an approach that calculates the optimum linear antialiasing filter for a given output 
restoration kernel. It turns out that this method has a Flaw which is corrected in the next section. 
The flaw is that images with negative outputs will be generated. In a way, we mny think of the image 
sampling and reconstruction procedure as an function approximation pr'oi}lem. We are given as basis functions 
the Gaussian spots on a CRT. The question we may pose then is: "What are the optimum weights to linearly 
combine the basis vectors for approximation of the ideal signal?" In other words, we are free to vary 
the brightness of each pixel spot (which is a Gaussian distribution) in order to make the reconstructed 
signal as "close" to the original ideal image as possible. In a CRT, the reconstructed image is given 
by a weighted sum of Gaus.sian bumps: go In this equation, x~ represents the pixel value, and gi(~) 
represents a Gaussian distribution centered at the i ~k pixel. See Figure 7. Each Gaussian is a shifted 
version of a canonical bump: Now the question is. how do we measure the closeness of two images? Namely, 
given two images how do we assign a non-negative real number which corresponds to the distance between 
them? The choice of such a distance metric is a nontrivial task--a choice on which the ultimate visual 
quality of the images is strongly influenced. We will discuss the choice of other image metrics based 
on the human visual system below, but for now we choose a particular metric which has many pleasing analytic 
(if not visual) properties, the mean square metric. The distance between two images f~, ~ is given by 
The sampling problem may now be stated thus: Given an input image f(~), what are the optimum pixel values 
x i minimizing the error between th~ original image and the reconstructed image? That is, find the values 
..... x~, xo, x i .... minimizing _ i=-~o Now, in practice, there are only a finite number of sample 
points to be determined, say xa, xc,...,x~_ I . In order to minimize this functional we take its gradient 
and set it to zero.  Computer Graphics Volume 15, Number 3 August 1981 This give rise to a system of 
equations Thus, the operation may be very roughly interpreted as follows: pm ~-t : or Doing a little 
algebra we obtain: ,do gO i=O -re Now we use the identity (2.2) to get ~'=o The quantities on both sides 
of this equation have names, where R~((i-k)T) is the autocorrelation Function of the Ga(~ssian spot evaluated 
at the point (i-k)T. The quantity on the left is simply Rfa(kT) the cross correlation of f and g at kT. 
7J Setting R_q((i-k)T)=alk" and R fzl(kT)=bj~ the above normal equations take the familial'form Ax=b. 
 Where A is an n n matrix called the Gram matrix and b is an n-dimensional column vector, where n is 
the number of pixels in the output. The optimum pixel values are then given by the solution of this system 
of equations. It is well known that the autocorrelation matrix A is of the so called symmetric Toeplitz 
form, viz. it is constant along the major diagonals. There exist fast methods to invert such matrices 
[Levinson 1947, Trench 1964]. These stem from the fact that there are not really n~ independent elements 
but rather n. Inversion with the Le~inson-Trench scheme is O(n 2-) instead of the Usual n -~ . Note that 
in the 2-dimensional case the matrix is no longer Toeplitz but rather Block Toeplitz, with Toeplitz sub-blocks, 
thus enabling significant economies in the storage and computation of the solution vectors [Kajiya 1981]. 
These savings can be quite significant since For a picture n pixels square, the foll Gram matrix requires 
n ~r elements and take time O(n (e) time to solve, for n=512 the straightforward inversion scheme is 
well beyond the capabilities of even the largest of computers, while the Levinson-Trench recursion is 
quite practical. It may seem that for text character fonts, much of this discussion is moot since characters 
are quite small, say lOx13 pixels. However, even for this size, the matrices have 16900 elements, and 
for a 30~30 pixel font the full autocorrelation matrix requires almost a million entries! An important 
point concerns the reconstruction kernels. If they were not Gaussians as in a CRT but rather Nyquist 
kernels as in the ideal case, then the Sampling theorem obtains. The Gram autocorrelation matrix reduces 
to the identity due to the orthonormality of the Nyquist kernels, and the cross correlation step corresponds 
to a perfect lowpass filtering and sampling operation. To reconstrtlct with a given waveForm, first filter 
by that waveform (take the inner product) then solve the matrix problem with the Gram matrix of autocorrelations. 
If the matrix is large we may be able to ignore edge effects and consider the matrix simply as a convolution 
with the "Green's function" of the Gram operator, which serves as an equalizing filter to flatten the 
response of the initial filter. Thus, the above process is a linear process and, we might add, one that 
is quite familiar in certain circles. It is, however, inadequate from several standpoints. The two major 
inadequacies are, first, positivity constraints stemming from the physics of light and the physics of 
the display devices and, second, the inadequacy of the least square image metric as a suitable model 
for vision. In the next section we discuss the first of these shortcomings, while in a later section 
we treat the second. 4. TilE POSITIVITY CONSTRAINT In this section we analyze the cause of the negative 
lobes output by the optimum linear filter. We also explore methods For correcting the negative lobe output. 
It turns out that the obvious method of truncating the negative lobes at zero may or may not work, depending 
upon the form of the restoration kernel. We analyze the criteria under which truncation works. Unfortunately, 
for the case of interest, viz. Gaussian reconstruction kernels, the criteria are not met. Figure 8 shows 
the minimum mean square error reconstruction of an impulse using Gaussian reconstruction kernels. The 
relative extrema represent the strongest contributions of each individual Gaussian spot. This response 
can be couched in almost teleological terms as follows: To make an impulse with a series of Gaussian 
bumps, take an initial bump and shave off the sides to narrow the bump by subtracting a small Gaussian 
from either side. Now to compensate for these negative lobes we add'in some positive Gaussians of smaller 
proportions a little farther away, Now to compensate for these postive lobes, .... etc. This procedure 
cannot be followed if we have, say, a series of potentiometers controlling the brightness of a number 
of Gaussian spots. This is b ecauae the pots cannot be turned negative. There is no way to make negative 
light--much to the Frustration of many workers concerned with these kind of display problems. Thus the 
display of a reconstructed image is constrained to the positive cone xo>O, x~>O, x~>O .... ~.l>O. This 
puts on additional constraint on the sampling problem. A succinct statement of the problem is now: lO 
 Computer Graphics Volume 15, Number 3 August 1981 Sampling problem (with positivity): with x restricted 
to the positive cone How do we approach this problem? Well, one popular method has been to ignore it 
completely: simply solve For the unconstrained optimum reconstruction and set any negative values to 
zero. This method may work in certain cases. For example, if the picture is sufficiently bright everywhere, 
the negative lobe may never dip below zero. Whenever we need to truncate negative values, however, possibly 
severe inaccuracies result. Characters, lines, and many other graphic objects are binary pictures with 
the lesser value being zero. Thus in graphics the need to truncate arises often. lhe typical case is 
illustrated by the previous example, viz. sampling an impulse. Simply truncating the negative lobes leaves 
a curious "ringing" pattern around the impulse (Figure 9). The ringing pattern in no way contributes 
to the minimization of the mean square metric, since their pr in¢ipal Function was to compensate for 
the negative lobes. Setting the positive sidelobes to zero also happens to be very close to the minimum 
mean square error picture: a single Gaussian bump. Geometrically, the situation may be visualized as 
in Figure 10. We have suppressed all dimensions except two and drawn contour lines for error. The actual 
optimum can be seen to be at point A in which xc£>O but x~<O. Setting x~_=O projects the point A onto 
B, a point which satisfies the constraints but which isn't very close to the true constrained minimum 
given by point C. How badly do we do by setting co-ordinates to zero? That depends on the eccentricity 
of the e11ipse and the angles that the major axis of the ellipse forms with the co-ordinate axes. With 
a nearly round ellipse, one whose major and minor axes are very nearly equal in length, one comes very 
close indeed to the to the optimum, when one sets the offending co-ordinates to zero (Figure 11). In 
the case of reconstructing say a I0X13 pixel character we are confronted with a 130 dimensional ellipsoid. 
The ellipsoid is formed from the level surfaces of the mean square error functional. To find its eccentricity 
we merely find the ratio of the largest to the smallest of the eigenvalues of the Hessian of the (quadratic) 
error functional, i.e. the matrix of second order partial derivatives This is none other than our old 
friend the Gram autocorrelation matrix. Thus we find that the set of eigenvalues, and hence the eccentricity 
of the ellipsoid, depends on the reconstruction kernel. Now, for an erthonormal reconstruction kernel, 
such as the Nyquist kernel, the Hessian is the identity matrix. Hence, the eigenvalues are all unity 
and the ellipsoid is perfectly round. Therefore, simply setting the offending co-ordinates to zero ~ives 
the constrained optim.um exact, l),. We do not have Nyquist kernels at our disposal, however, but rather 
Gaussians--which are decidedly not orthonormal. It turns out that finding the eigenvalues and eigenvectors 
of the Gram autocorrelation matrix is a well-known procedure called the Karhunen-Loeve transformation. 
Speaking very loosely, the eigenvalues correspond to the values of the Power spectral density, viz. the 
square of the Fourier transform magnitude of the autocorrelation Function. But the autocorrelation of 
a Gaussian distribution is again a Gaussian with double the standard deviation, and it's well known that 
the Fourier transform of a Gaussian is also a Gaussian. Thus, there is a tremendous range in the magnitude 
of the eigenvalues encountered, the values being governed by an exponential of a term proportional to 
the square of the abcissa. In other words For a Gaussian distribution the ellipsoid is very eccentric. 
Furthermore, the angle of inclination of the ellipsoid with respect to the co-ordinate hyperplanes (xi=O) 
is given by direction cosines that correspond to the inner product of a Gaussian with the Karhunen-Loeve 
eigenvectors. These eigenvectors are very roughly sinusoids. These direction cosines, at least For the 
largest eigenvalues, are roughly equal, so the ellipsoid is pitched at an angle of about 45°--the worst 
case. To sum up the discussion so Far: While setting the negative coordinates to zero For orthonormal 
basis Functions is a very good procedure, For the Gaussian restoration kernels it is very bad. TIIE KUIIN-TUCKER 
CONDITIONS Since simply truncating the negative lobes of tile output signal will not provide an optimum 
solution subject to the positivity constraint, we must search for methods that will provide us with the 
optimum constrained solution to the antialiasing problem, Note that we now are talking about some non-linear 
filtering procedure that will provide us with the optimum sample values For some input image. Fortunately, 
the structure of the aliasing problem is such that certain key conditions are met. This simplifies the 
optimization problem immensely and gives us a relatively straightforward way to solve the problem. These 
conditions are the Kuhn-Tucker conditions and the method of solution is known as the method of feasable 
directions. To optimize a nonlinear functional subject to inequality constraints is, in general, a very 
difficult task. If the Functional and its constraints satisfy the following assumptions then we may apply 
the Kuhn-Tucker theorems for inequality constrained mathematical programming problems. The Kuhn-Tucker 
conditions are: (i) The functional is convex. That is if x~ and x~ (i=0,I .... ,n-l) are two points in 
the solution sp@ce, then In other words, the functional lles below a line joining any two of its values. 
See Figure 12. (ii) The feasible set, i.e. the set of points satisfying the constraints, is convex and 
has a nonempty interior. Fortunately, both these assumptions hold for the case at hand: reconstruction 
with Gaussian kernels and positive weights. Its evident that the square error metric is convex (for other 
visual metrics this condition may no longer hold). The second assumption is also satisfied. The positive 
cone is obviously convex and has a generous interior. ]] Computer Graphics We now state a Kuhn-Tucker 
theorem. Knhn-Tucker Theorem. Under the conditions mentioned above, the nonlinear programming problem 
(Equation 4.1) has a minimal sollltion x*>O iff there exists ~,*>0 such that the Lagrangian ~=0 has 
a saddle point at (x*,~,*). A useful interpretation of this theorem [Collatz and Wetterling 1975] is 
that at the optimum point x*, the gradient of the functional is perpendicular to the active constraining 
hyperplanes, viz. the coordinate hyperplanes in which x* has a zero component. If there is no constraining 
active hyperplane then the gradient must be zero. The Kuhn-Tncker theorem provides the basis for a number 
of different optimization algorithms. One of the simplest (and slowest) is the one we have chosen in 
this work: the method of feasible directions. In this iterative method, a point x ~ is updated by a vector 
proportional to the gradient of the functional projected upon a subspace which maintains the new iterate 
in the positive cone: Namely, % : -6~ where ~J is the gradient of the functional. P is the projection 
operator which limits ~J to a feasible subspace, and ~>0 is a sequence of numbers chosen to make the 
Jacobian decrease at each iteration (Steepest descent). There are several salient points about this 
method which should be mentioned. First, this method is nonlinear, e.g. In particular, if c is chosen 
to make the bulk of cf negative, then the output will be zero. Second, the method can in certain cases 
collapse to the unconstrained case. For an input image that lies deep in the feasible set, i.e. it is 
positive everywhere, then one can afford the luxury of negative lobes because the ultimate answer will 
still have only positive coefficients. For an input image on the boundary of the feasible set, i.e. one 
that has many pixels set to zero, the method will suppress negative lobes. The next section will demonstrate 
how these constraints control ringing. 5. RESULTS The above algorithm was programmed on a DECSYSTEM-20 
for both the one and two dimensional cases. For the input images we hand digitized characters on either 
a IxIDO or I00XI00 grid. The decimation ratio was set to 100:16 or 6.25. The coefficients were then reconstructed 
with artificial Gaussian distributions of known variances. Volume 15, Number 3 August 1981 Results for 
the one dimensional case are as follows: An impulse response centered on a sample value gives the identical 
answer as the triangular kernel interpolant, a single Gaussian spot. Also shown is the unconstrained 
optimLJm for a box, which appears in Figure 13 as a ringing sinc-like function. Figure 14 shows the effect 
of constraints upon the negative lobes of a step response. Note that suppression occurs for not only 
the negative sidelobes but also for the residual positive sidelobes. We still have ringing on the positive 
portions of the step, however. These can be removed by a ran~)o constraint: if we know that the images 
have values between 0 and 1 (as do many graphic objects) then constraining the x~ to O<xL<1, gives the 
reconstruction shown in Figure ]5. Finally the response of the algorithm to a chirp signal is shown for 
comparison with the triangular case. For the two dimensional case, we reconstructed several characters 
with an artificial Gaussian spot. Rather than reconstruct with the natural electron beam spot on the 
CRT we chose to reconstruct with a much larger simulated spot. We did this for several reasons. First, 
the pictures were easier to analyze for artifacts. Second, at the time of writing we had as yet not measured 
the variance of the spot on the screen. Third, the image display available to us had only 4 bits per 
pixel both for the memory and the colormap. Additionally, the gamma of the display system was not adequately 
compensated for (this was a display intended for VLSI design aids). It is well known that for anti-aliasing 
experiments proper gamma correction is crucial [Crow 1975]. Rather than lose precious bits by trying 
to gamma correct in tile color map (remember we only have 16 levels in and out) we decided to reconstruct 
with large spots, gamma correct with high precision inside the computer and dither down. to four bits, 
trading spatial resolution for gray scale resolution. Figures 16-18 show the results of the algorithm 
compared with the original and the triangular filter reconstruction. 6. FUTURE WORK We see many ways 
to continue this work. We have not addressed at all the important problem of overlapping images. At 
present the expedient we use is to place each character In frambs that does not overlap. If we were to 
allow overlapping frames then we would be allowing more pi×eIs per character to yield a higher effective 
output resolution. There are dangers in allowing overlapping images, however. Because characters almost 
never occlude one another in the text case there is ustially no danger when one is performing linear 
processing: one simply adds the resulting images. In the case of our nonlinear processing the situation 
is more delicate. A careful analysis has yet to be done. 12 Computer Graphics Volume 15, Number 3 August 
1981 The foremost task yet to be done is the inclusion of a better visual metric than the least squares 
metric. It will be clear from the following discussion that a minimum mean square reconstruction is quite 
far from optimum compared with a reconstruction which takes into account certain key features of the 
human visual system. TIIIT SINGLE CHANNEL MODEL The simplest model of the visual system is the so-called 
Lateral Inhibition model (also known as the single channel model). The model is given by the following 
equation Where f(~) is the input image, ~(~) is the reconstrncted~ image and h(~) is a PSF known as the 
lateral inhihition kernel (or RatliFf kernel). Thus, in this model, each of the individual images to 
be compared undergoes a logarithmic point transformation after which the difference is filtered and then 
summed in a mean square procedure. The frequency response corresponding to h(~) is shown in Figure Ig 
where the peak of the response curve is at about 3 cy/deg [Cornsweet 1971]. Let us analyze this image 
metric a bit. Recall that tile Parseval theorem relates mean square error in the spatial domain to that 
in the frequency domain [Rudin 1966]: ~0 o9 Applying this formula to the expression for the visual model 
response we obtain (via the convolution theorem) where H(~) is the Fourier transform of h(~), ~i (~) 
and ~2_ ((~) are the Fourier transforms of log fl and log f~-. Note that in this form the response is 
just a classical weighted least square error metric between the logs of the images. From the HTF of the 
visual system (Figure ]9) we see that this weighting favors the high frequencies much more than the low 
frequencies. There are two essential features that cause the optimum image reconstruction for ti~e ordinary 
least square metric and for the lateral inhibition model metric to differ significantly. The first concerns 
the relative importance given to errors at different luminance levels and the second concerns the relative 
importance given to errors at different spatial frequencies. The logarithmic point transformation, which 
forms tile nonlinearity in the initial stage of the model,, implements Weber's law:  E__AZ where AI 
is the just noticeable difference in luminance and I is the average luminance. This law holds over a 
range of intensities that easily encompasses the range encountered in graphics and text display. It has 
been found that k~.02. The precise form for the function implementing Weber's law is in dispute. However, 
all proposed functions are reasonably close to the logarithm for a range of intensities and share most 
of its important properties--such as concavity. In words, this law states that small luminance errors 
for the low luminance portions of an image are far more objectionable than those for the high luminance 
portions. An optimal approximator using this image metric wil] thus be more fastidious about the low 
luminance portions of an image, spending more of its error budget to fit the data there. Since the human 
visual system is more sensitive to errors in this portion (hence the logarithm in the model) images reconstructed 
in this manner should compare favorably to those of the ordinary least squares metric in which all the 
errors are treated as equals. A second difference is manifested by the frequency weighting which appears 
in Equation 6.1. Here the model correctly predicts that we are by far more sensitive to high spatial 
frequency errors. It is this increased sensitivity at high frequencies that makes the artifacts of fuzzy 
edges produced by the triangular kernel--and the persistent ringing produced by the unconstrained linear 
least squares approximant--so objectionable. Both artifacts are high frequency effects whose effect is 
enhanced by our visual system. Using the lateral inhibition metric will result in improved image sharpness 
and lower ringing at the expense or of higher errors at the low frequency portions of the image, which 
presumably we do not see. The methods For calcnlation of an optimal approximant for such an image metric 
have yet to be resolved. Introduction of the logarithmic nonlinearity (or any other nonlinearity popular 
in visual modelling) causes a loss of convexity for the functional. Thus, the Kuhn-Tucker theorems may 
not be applied directly. We are currently investigating implementations which will bypass this difficulty. 
HULTICHANNEL MODELS More realistic visual models can be considered for use in the optimal approximant 
scheme. Currently popular in the psychophysical literature are a class of models know as multichannel 
models [Graham and Nachmias 1971, Hostafavi and Sakrison 1976, Kajiya 1979]. In these models the image, 
after passing through a nonlinearity, is not filtered by a single frequency shaping network but rather 
by many bandpass channels of varying sensitivities. There is a certain amount of controversy over the 
characteristics of such channels and the mode of summation of the outputs of such channels, but it is 
a promising possibility that L metrics rather than L metrics may be closer to the truth. If this is the 
case then the door is open for nonlinear Chebyshev techniques to be applied to the antialiasing problem. 
CONCLUSIONS There is far more to the antialiasing problem than simple linear filtering. We have analyzed 
the performance of the linear Filtering approach to antialiasing, and introduced the use of more powerful 
techniques for certain critical applications such as the display of high quality text. Perhaps someday 
computational techniques will be discovered to perform these calculations for more general synthetic 
images. For now, though, the practical use of our technique is limited to images with relatively small 
pixel sizes--such as the display of text characters. 13 Computer Graphics Volume 15, Number 3 August 
1981 REFERENCES Blinn J.F. (1978) "Computer" Display of Curved surf'aces" U. Utah Ph.D. Thesis, December 
1978. Collatz L. and Wetterling W. ~1975) OPTIMIZATION M[IIIODS, Springer Verlag, Berlin. Cornsweet T. 
(1970) VISUAL PERCEPTION, Academic Press, New York. Crow F.C. (1976) "The aliasing prol)lem in Computer-synthesized 
shaded images" U. Utah Computer Science Teeh Rept. UTEC-Csc-TG-OI5. Gabriel S.A. (1977) Private communication. 
Graham N. and Nachmias J. (1971) "Detection of gr~,ling patterns containing two spatial frequencies: 
a comparison of single channel and multiple channel models" Vision Res., v. 11, pp. 251-259. Kajiya, 
J.T. (1979) "Toward a Hathematical Theory of Perception", Ph.D. Thesis, U. Utah. Kajiya J.T. (1981) "On 
fast methods for two dimensional spectral faetorization", to appear. Levinson N. (1947) "The Wiener RMS 
error criterion in filter design and prediction" J. Math. Phys. v.25, no. 4, pp.261-278. Hostafavi H. 
and Sakrison D.J. (1976) "Structure and properties of a single channel in the human visual system" Vision 
Res., v. IG, pp. 957-968. Oetken G.,Parks T.W., Schuessler H.W. (1975) "New results in the design of 
digital interpolators" ]EEE Trans. ASSP, v.ASSP-23, pp.30]-309. Pratt W.K. (1978) DIGITAL IMAGE PROCESSING, 
Wiley-lnterscience. Rtmdin W. (1966) REAL AND COMPLEX ANALYSlS, McGraw-llill, New York. Sachs M.B., 
Nachmias J. and Robson J.G. (1971) "Spatial frequency channels in human vision" J. Optical Soc. Am., 
v. 61, pp. 1176-1186. Sakrison D.O. (1977) "On the role of the observer and a distortion measure in image 
transmission" IEEE Trans. on Communications, v. C0M-25, pp. 1251-1267. Schafer R.W. and Rabiner L.R. 
(1973) "A digital signal processing approach to interpolation" Prec. IEEE v.61, pp.692-702. Trench W.F. 
(1964) "An algorithm for the inversion of finite loeplitz matrices " J. SIAM v. 12, no. 3, pp.515-522. 
Seitz C., eL. al. "Digital Video Display system with a Plurality of Grey-scale levels" US Patent 4,158,200. 
Warnock O.E. (1980) "The Display of Characters Using Gray level Sample arrays" ACM SIGGRAPH80, pp.302-307. 
Figure t. Figure 2. / Figure 3. tr 7- T Figure 4. FT ..... Figure 5. Figure 6. Figure 7. ]4  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806785</article_id>
		<sort_key>17</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[A visible polygon reconstruction algorithm]]></title>
		<page_from>17</page_from>
		<page_to>27</page_to>
		<doi_number>10.1145/800224.806785</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806785</url>
		<abstract>
			<par><![CDATA[<p>An algorithm for determining visible lines or visible surfaces in polygonal form, at object resolution, is presented. The original scene must consist of non-intersecting planar polygons. The procedure relies on image coherence, since the sampling is dependent on the complexity of the image. The reconstruction method is based on an elaborate data structure, which allows the polygonal output to be easily obtained. The polygonal output is useful for smooth shaded or textured images, as well as the for creation of shadows.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Visible surface algorithms]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14205479</person_id>
				<author_profile_id><![CDATA[81332526147]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stuart]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sechrest]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University, Ithaca, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P68459</person_id>
				<author_profile_id><![CDATA[81100196982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Donald]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Greenberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University, Ithaca, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807403</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Atherton, P., Weiler, K. and Greenberg, D., "Polygon Shadow Generation," Computer Graphics, Vol. 12, No 3 (August 1978), pp 275-281.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Cook, R. L., "Polygonal Database User's Guide," unpublished, 1981.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807507</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Feibush, E. A., Levoy, M. and Cook, R. L., "Synthetic Texturing Using Digital Filters," Computer Graphics, Vol. 14, No 3 (July 1980), pp 294-301.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807480</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Franklin, W. R., "A Linear Time Exact Hidden Surface Algorithm," Computer Graphics, Vol. 14, No 3 (July 1980), pp 117-123.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563895</ref_obj_id>
				<ref_obj_pid>965141</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Hamlin, G. Jr and Gear C. W., "Raster-Scan Hidden Surface Algorithm Techniques," Computer Graphics, Vol. 11, No 2 (Summer 1977), pp 206-213.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I. E., "Polygon Sorting by Subdivision: A Solution to the Hidden-Surface Problem," unpublished, 1973.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563896</ref_obj_id>
				<ref_obj_pid>965141</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Weiler, K. and Atherton, P., "Hidden Surface Removal using Polygon Area Sorting," Computer Graphics, Vol. 11, No 2 (Summer 1977), pp 214-222.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 A VISIBLE POLYGON RECONSTRUCTION ALGORITHM Stuart 
Seehrest and Donald P. Greenberg Program of Computer Graphics Cornell University Ithaca, New York 14853 
 Abstract An algorithm for determining visible lines or visible surfaces in polygonal form, at object 
resolution, is presented. The original scene must consist of non-intersecting planar polygons. The procedure 
relies on image coherence, since the sampling the image. The reconstruction method structure, which 
allows the polygonal polygonal output is useful for smooth as the for creation of shadows. Key words: 
computer graphics, visible Computing Reviews category: 8.2 A. Introduction A topic in computer graphics 
of long-standing interest is the determination of the visible surfaces of a modelled environment consisting 
of planar polygons. Most currently used algorithms are only concerned with producing a single static 
image of fixed resolution. However, there are significant advantages in returning a precise definition 
of the visible polygons. Algorithms for determining visible surfaces fall into two major categories: 
those that work at image resolution and those that work at object resolution. Image resolution algorithms, 
such as scan-llne algorithms or depth buffer algorithms, determine the polygons that are visible for 
regions of the picture plane that are of fixed shape and greater than some minimum size. Computations 
are normally performed down to the level of the display resolution. Object resolution algorithms determine 
a set of visible polygons, which constitute the image. The Permission to copy without fee all or part 
of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. &#38;#169;1981 ACM O-8971-045-1/81-0800-0017 
$00.75 is dependent on the complexity of is based on an elaborate data output to be easily obtained. 
The shaded or textured images, as well surface algorithms shapes of these visible polygons are not 
fixed, but, rather, depend upon the shapes and overlap of the input polygons. Computations are performed 
at object resolution. Previous algorithms that return visible polygons at object resolution include 
those developed by Sutherland [6], Weiler and Atherton [7], and Franklin [4]. The first two algorithms 
obtain the visible polygons by pairwise polygon clipping. Polygons front to back and shallower polygons 
further from the algorithm eliminates clearly hidden and regions where the overlap to find the within 
each region. Polygonal output variety of ways. The are processed from clip the shapes of from those 
lying viewer. Franklin's polygons which are then samples the remaining polygons shallowest polygon is 
useful in a resulting output can be used for both raster and vector displays, since it can yield both 
hidden surfaces and hidden lines. Polygons obtained from visible surface programs may be compactly stored 
for later display. They allow the polygons of a scene to be textured one at a time rather, than all at 
once [3]. In addition, the eomputed visible surfaces have been used to produce shadows and to simulate 
transparency [I]. This paper discusses a new algorithm for obtaining, at object resolution and in a 
polygonal form, the visible surfaces for a scene consisting of non-lntersecting, Computer Graphics Volume 
15, Number 3 August 1981 opaque polygons. The algorithm relies on image coherence using a similar approach 
 to one previously introduced by Hamlin and Gear [5] to obtain visible line output. The important difference 
is that rather than just producing visible lines, information about the connections of these lines is 
retained so that they may be reconstructed into closed polygons. The retention of sufficient information 
to reconstruct these polygons does not greatly burden the algorithm. The actual reconstruction of the 
polygons is very rapid. B. Data Representation The environment consists of a set of non-intersecting, 
opaque, planar polygons that may be concave and may have holes. The outer contours of the polygons are 
assumed to have a clockwise orientation, and the holes a counter-clockwise orientation, when seen from 
the viewer. This convention allows the easy determination of which polygon, of two sharing an edge, is 
on the right and which is on the left. It is also assumed that all polygons have been rotated and translated 
to the desired view and that all perspective transformations have already been applied. To this environment 
two polygons are added. One is a background polygon placed behind all other polygons. The other is a 
mask polygon placed in front of all others with a hole (or holes) serving as a viewport. Both of these 
polygons are perpendicular to the viewer. Neither outer contour of the two polygons is considered visible 
and no visible edges are produced for either of them. Portions of the background polygon that are visible 
through the viewport are reconstructed, but no portion of the mask polygon is. Many of the problems 
encountered in computing the visible portions of polygons are alleviated by retaining information known 
at the time of the environment's creation. Rather than reducing the description of the environment to 
lists of each polygon's vertex coordinates, the vertices, edges and polygons in the database are represented 
by data structures indicating their interconnections. Edges and vertices which are shared by more than 
one polygon are explicitly represented as shared. These data structures and database access routines 
are currently used at the Program of Computer Graphics at Cornell University and are more fully described 
in [2]. Alternatively, the information retained could be obtained by preprocessing or by tests in the 
course of the algorithm. Hamlin and Gear [5], for instance, found all shared edges during preprocessing 
by comparing vertex coordinates, and found the edges sharing a vertex by comparing the ends of edges 
occuring close together in the course of the algorithm. Each vertex, edge and contour in the environment 
is represented by a separate record. These records are part of large arrays kept in virtual memory, and 
each has a unique identifying number. A block representing a vertex in the database includes the following 
information: I. The coordinates of the vertex 2. A pointer to a copy, if one exists, of the vertex 
 3. The vertex, if any, from which this vertex was copied  4. A list of all edges to which the vertex 
belongs  5. A list of all polygons to which the vertex belongs.  An edge is represented by a block 
 containing the following information: I. The identifying numbers of the two end vertices 2. A llst 
of the polygons to which the edge belongs. A polygon consists of an outer contour and hole contours. 
It is identified by the number of its outer contour. A contour block includes I. The polygon's plane 
equation 2. A bit, set if the contour is a hole  3. If the contour is an outer contour, a list of the 
hole contours within the polygon  4. A list of the contour vertices  5. A list of the contour edges. 
  The lists in each block are implemented so as not to place limits on their size. A fixed number 
of words are allocated for each list within the block and, if necessary, additional blocks are allocated 
 to be used as extensions to the lists. Pointers to the extension blocks are stored in the last word 
of the oversized list if necessary. C. Overview of Visible Line Algorithm This section describes an 
algorithm for extracting the visible lines of a scene for which the information described above is known. 
To facilitate the explanation of this algorithm, it is convenient to adopt the following definitions 
(Figure I): I. A visible line separates a region on the picture plane where one polygon is visible from 
an adjacent region where another polygon is visible. Since a background polygon is used, a visible llne 
might separate a polygon from the background. The Computer Graphics Volume 15, Number 3 August 1981 
first procedure outlined below returns only these undirected visible lines. Visible lines cannot easily 
be used for polygon reconstruction since it is not clear which lines connect or which polygons they bound. 
 2. A visible ~_~ is a directed copy of a visible line, and is a candidate for becoming a polygon edge. 
A visible segment is produced on each side of a visible line, and is used to bound the polygons on that 
side of the line. Segments on opposite sides of a line are oriented in opposite directions. The boundaries 
of visible polygons are made up of visible edges. A segment becomes an edge when it has been added to 
the ordered list of edges that define the visible portions of the polygons. Both input and output polygons 
are defined by edges. The segments surrounding each visible portion of a polygon must become connected 
edges forming loops. These loops will be the contours of the output polygons. As with input polygons, 
the contours surrounding the output polygons will have a clockwise orientation and the holes a counter-clockwise 
orientation. 3.  In typical scan-line algorithms the horizontal strips are usually sampled along a 
single horizontal line, rather than throughout their width. This can lead to aliasing problems, particularly 
with very small polygons which could, for example, fall entirely between two sampling lines and, thus, 
disappear. To some extent, however, the method tends to oversample. If an edge crosses several scan-lines 
without crossing any other edge, no change will take place in the visibility of the regions on either 
side of the edge. The only points at which visibility changes can occur are the points at which edges 
begin, end, or cross. Rather than sample at every scan-llne, it is only necessary to sample for one horizontal 
level between two of these "events," if no third event falls within the horizontal strip they bound. 
Th@ picture plane is therefore divided into strips of varying width defined by the locations of the vertices 
and crossings of the scene (Figure 2). The number of strips and their width are not fixed, but depend 
upon the content of the scene. This procedure has three major advantages. First, the number of lines 
which are sampled depends on the complexity of the image. Thus, the image is densely sampled in areas 
of great complexity and sparsely sampled in areas of little complexity. Second, the Figure I. a. Visible 
lines b. One visible line has been replaced by two oriented visible segments. Each connects two output 
vertices and belongs to the polygon on its side of the original edge.  c. The visible segments have 
been formed into closed loops with clock- wise orientation defineing polygon contours. aliasing problems 
frequently associated with small polygons will be avoided, since these events are determined at object 
resolution, without regard to the de~iCe on which results might be displayed. Third the algorithm is 
suitable for obtaining visible lines for vector scope display as well as visible surfaces for shaded 
images. Since the desired output from the algorithm is the set of visible lines of the scene, rather 
than a Partition of each horizontal strip, the algorithm proceeds differently from a conventional scan-line 
algorithm. The procedure is as follows: I. Find all local minima and sort them by their location in 
the picture plane. Repeatedly: 2. Select the next vertex or crossing from the active edge list.  3. 
If this point is lower than the next local minimum, call this the next event; otherwise the local minimum 
is the next event.  4. Update the active edge llst. At vertices, remove edges that end at the event 
and add edges which begin. At crossings, reorder the edges on the list. If a visible edge ends or is 
occluded, write out the coordinates of a visible line.   Computer Graphics Volume 15, Number 3 August 
1981 Output is only produced when an edge that had divided visible polygons ends or is obscured by another 
polygon at an edge crossing. Thus, output is only produced at vertices or edge crossings and is only 
produced for edges impinging on the vertex or crossing at that point. The output is simply a list of 
the coordinates of the points at which the visible lines begin and end. The picture plane need not be 
divided, ~ Priori, into a complete set of strips. A priori division would require that the location of 
every vertex and edge crossing be known at the start and would clearly require a substantial effort. 
Rather, locally minimal vertices are first identified and sorted by their position in the picture plane 
(Figure 3). This sorting may be done using any of several standard algorithms. Points on the picture 
plane are ordered primarily in y and secondarily in x. This is similar to the scan-line method where 
lower numbered scan-lines are processed before those with higher numbers, and where, within a scan-llne, 
points are ordered by their x coordinates. Therefore, a point belonging only to edges leading upward 
or to the right is locally minimal. This provides an initial set of strips, within each of which all 
events are "predictable." The locations of non-mlnlmal vertices within these strips are known as the 
ends of active edges. The locations of edge crossings may be discovered by arithmetically testing active 
edges for crossings within their lengths. Thus, subdivision of the strips bounded by locally minimal 
vertices can be done "on the fly." The locally minimal vertices are kept, in sorted order on a doubly-linked 
"entry list." For any horizontal strip, it is easy to determine exactly which edges are active. The 
active edge list is updated at each vertex. Only two types of vertices exist, locally minimal vertices 
and non-minimal vertices. When a minimal vertex is encountered, all edges emanating upward from this 
vertex must be added to the active edge list. From this information, the adjacent non-minimal vertices 
can be predicted. Since the database includes lists of all edges impinging upon every vertex, any edge 
 upward from a non-minimal vertex can be added to the active edge list when the edges leading up to 
that vertex are removed. Crossings do not effect the set of edges that are active. When an edge becomes 
active, a block representing it is placed on the active edge list. The block has the form shown in Figure 
4. The list is doubly linked to facilitate rapid insertion, deletion and reordering of the edge blocks. 
The blocks are maintained in an order determined by the edges' positions within a particular horizontal 
strip. The blocks contain information about the edge including its database identification number, the 
line equation of the edge's projection onto the picture plane, and bits indicating that the edge is part 
of the polygon to its right or to its left or of both of them. The block also contains a bit indicating 
whether it is currently visible and, if it is visible, the identification numbers of  "I\ / \ / \/ ~ 
~'#" MINIMAL VERTICES @ EDGE CROSSINGS D Figure The tal 2. picture strips plane by the divided into 
occurance of horizon-vertices Figure 3. The locations environment. of events in a simple and edge crossings. 
 20 Computer Graphics Volume 15, Number 3 August 1981 the visible polygons separated by the edge. For 
each edge a prediction is made that the edge will or will not cross one of its neighbors before the edge 
ends. It is only necessary to test for crossings of adjacent edges on the active edge list since the 
crossing of any non-adjacent edges must necessarily follow the end of or the crossing of all intervening 
edges. Thus, the next event (i.e. the upper bound of the current horizontal strip) may be found at any 
point by searching the active edge list for its next predicted event and comparing this to the next local 
minimum. This procedure is, in general, O(mn), where n is the average number of edges active and m is 
the number of events. If n is likely to be large, this procedure could be reduced to O(mlogn) by inserting 
predicted events into a y-sorted event list. The next event would always be the first event on the list. 
 The active edge list is updated at vertices and edge crossings. At a vertex, the edges which end at 
the vertex are removed from the active edge list, and those leading upward are added. The visibility 
of any edges added to the active list must be determined as the event is handled. These events are inserted 
from left to right so that the list will remain in x sorted order. At a crossing no edges become active 
or terminate. The positions, on the active list, of the two edges are simply reversed to maintain the 
sorted order. At crossings and at most vertices two test are made to determine future crossings. When 
two edges cross, they are tested for crossings with their new adjacent neighboring edges. When edges 
are added to the active edge list the rightmost and leftmost among them must be tested for crossings 
with the edges adjacent to the right and left respectively. If no edges lead upward from a vertex, then 
the two edges, formerly separated by the edges ending at this vertex, are tested for crossings. Therefore, 
less than 2m crossing tests are performed. The visibility of edges following crossings is done using 
tests which avoid unnecessary arithmetic tests [5]. Edges emerging from vertices are judged to be visible 
if they bound the polygon already visible to their left. If they bound any other polygon to their left 
they are invisible. If neither of these cases hold, their visibility is tested arithmetically. If an 
edge is found to be visible, the polygons visible to the left and right of the edge are recorded in the 
polygon's active edge block, along with the coordinates of the point where the edge becomes visible. 
 Visible lines are produced when a visible edge is obscured at an edge crossing, or when the edge ends 
at a vertex. The coordinates of the visible lines are simply written into an array. The x and y coorainates 
are those of the events where the line begins and ends. The z coordinate is calculated from x, y and 
the plane equation of the polygon to which the original edge belongs. D. Visible Surface Reconstruction 
 I. Overview The step from visible lines to visible surfaces is not a large one. Taking it requires 
the reconstruction of the polygons bounded by visible lines. This may be done by preserving more information 
about these lines than was preserved in the algorithm outlined above. The production of the visible polygons 
falls into three parts. First visible segments, rather than visible lines, must be created and stored. 
Second, the segments surrounding a particular visible region of a polygon must be extracted. Third, these 
segments must then be reconstructed to form entire polygons with the vertices of their contours correctly 
ordered. To efficiently reconstruct these segments into closed contours, it is necessary to keep information 
about the vertices which "they connect. The data structures used to store this information is presented 
below. ACTICE EDGE BLOCK " ......... ~;~;~'i~ ......... "-i .... , < ...... LIST ................................ 
÷ CROSSING pREDICTION VISIBLE l BELONGS TO LEFT POLYGON I i BELONGS TO RIGHT POLYGON I ................................ 
÷ LAST EVENT LOCATION ~X 1 .... y~--~ LEFT BASE VERT ................................ VISIBLE POLYGON 
TO RIGHT RIGHT BASE VERT ................................ IDATASASE NUf~ER OF ORIGINAL EDGEI +. . 
..... ÷ EQUATION ----+ ~'-OF EDGE ~-.............................. +  Figure 4. A block used for 
storing information concerning an active edge. The block is put into an x-sorted active edge list. 
 Computer Graphics Volume 15, Number 3 August 1981 Besides edges of contours, a second type of reconstructable 
segment is useful This is an invisible "thread" which ties an outer contour to what might be a hole in 
the polygon (Figure 5). These are stored as reconstructable segments, distinguished by a "thread bit," 
linking a vertex, that has visible edges above it but none below, to the nearest visible edge on its 
left. Unnecessary threads connecting two vertices of the same contour may be produced, but these may 
be easily discarded in the process of reconstruction. Similarly one hole might be attached by several 
threads. Nonetheless, every hole will be securely associated with its outer contour, either airectly 
or through other hole contours. Because of this association, the reconstruction routine does not have 
to match holes with their outer contours. 2. Data Structures The data structures used to store information 
for reconstruction, must quickly provide answers to four questions: I. Are there any visible segments 
belonging to a particular polygon? 2. What are the vertices of a segment chosen?  3. Which segments 
leave a particular vertex?  4. To which polygon does a particular segment belong?   The answer to 
the first question indicates whether any reconstruction is necessary. When visible segments are created, 
they are placed on a list belonging to a particular input polygon. Visible polygons are formed only 
for those input polygons to which some visible segments have been assigned. If visible segments for a 
particular polygon exist, the reconstruction process can be initiated by selecting the first segment 
on the list. The vertices of this segment must then be determined. The'information required to answer 
the third question ensures the continuity of the output polygon's contours. Given a visible segment entering 
a vertex, this information is necessary to select the next visible segment of the contour. Several segments, 
belonging to different polygons, may leave a vertex so the fourth question must be answered to allow 
segments from the same polygon to be connected. Two data structures, which together answer the above 
questions, are used to store the scene's visible segments and threads. The first is a segment buffer. 
The second is an output vertex pair. For each visible segment, a record is added to the segment buffer. 
The buffer Figure 5. The contours of an output polygon are defined by visible segments (solid arrows). 
Hole contours are connected to outer contours by means of threads (dotted arrows). Not all threads 
are necessary, however. The topmost of the three threads illustrated is not used in reconstruction. 
 has been implemented as a one dimensional array of segment records (Figure 6), so that each record 
has an index number. Each segment's record includes fields which allow it to be placed on a doubly 
 linked list of segments belonging to a particular input polygon. The list of segments belonging to 
each polygon is pointed to by a field in the polygon's database record. Each segment record also contains 
the identifying numbers of the output vertices with which the segment begins and ends and a bit which 
is set if the segment is a thread. Information on the visible segments only does not allow easy reconstruction, 
since it would be necessary to sort each polygon's list of segments to reconnect them. This can be avoided 
by ensuring that two segments which meet at a point share the vertex created at that point. Output vertices 
are actually created in pairs. One output vertex is created for insertion into the output polygon, the 
other is used to keep track of the reconstructable segments emanating from that vertex and the polygons 
to which they belong. These will be referred to as the insertion and record keeping vertices respectively. 
Through this mechanism, a visible segment impinging on an output vertex pair is linked to the next segment 
in the contour. Figure 6 shows a vertex pair, from which emanate three reconstructable segments. The 
segments Computer Graphics Volume 15, Number 3 August 1981 62 OUTPUT VERTEX PAIR: 5 0 RECORD-KEEPING 
VERTEX INSERTION VERTEX 52 ~ 2 54 SEG~NTS: INPUT POLYGONS: .................................. Z = 0.7987a3 
X = 0.798?43 Y = -0.327497 Y = -0.32749? Z : 0.478920 Z : 0.478920 .................................. 
I EDGES: 3 5 I ................. OUTPUT ~-................ A POLYGONS: I E I C I SEG~NT BUFFER INDEX 
. ~- +_ ÷ Figure 6. An output polygon pair for vertex 56 the cube, along with a sample segment buffer 
record for one of the segments leaving the vertex. in 3 T----;~ ..... T----~; ..... T ..... ; ..... "-~---T--;---~ 
I ........... 1 ........... 1 ........... 1 ..... 1 ..... 1 ........... ~ ........... ~ ..................... 
÷  a b a b Figure 7. Figure 8. Production of visible segments at a Production of visible segments 
at edge vertex. Solid arrows represent visible crossings. Note the presence of a segments produced at 
the vertex, dashed background polygon. lines represent segments produced later. a. A shared edge is occluded 
by a The vertices created serve as top shallower polygon. One output vertex vertices for the segments 
produced, and is created. It is shared by the as base vertices for edges produced portions of the polygons 
sharing the later. Note the presence of a original edge. background polygon. b. A silhouette edge is 
occluded. Two a. Three vertices are created for  output vertices are crested, one for insertion into 
three output polygons. each of the polygons on either side  b. Two vertices are created, one of of 
the visible line. which is shared by two polygons. c. A shared edge emerges from behind a shallower 
polygon. One vertex is created and shared by the visible portions of the polygons to either side of the 
shared edge.  d. A silhouette edge emerges. Two output vertices are created.  Computer Graphics and 
the polygons to which they belong are identified by their index numbers in their respective lists. Two 
copies of the vertex are necessary in order not to confuse the index numbers of segments leaving the 
vertex with the identifying numbers of edges of reconstructed polygons, and the numbers of input polygons 
with those of output polygons. In the following discussion the term "output vertex" will refer to the 
record keeping copy, unless otherwise stated. 3. Obtaining Visible Segments The method for producing 
visible segments is similar to the algorithm previously described for extracting visible lines. However, 
since the segments will be used for polygon reconstruction, it is necessary to retain  more information. 
To precisely define a segment, it is necesary to record its beginning and end points, its direction, 
 and the polygon to which it belong. This is accomplished as follows. The orthographic projection of 
the visible segments coincide geometrically  with the visible lines, Thus it is possible to create 
them using similar routines to those outlined above for obtaining visible lines. When an edge, emerging 
from a vertex or an edge  crossing, is found to be visible, an output vertex is created for the segments 
 on both sides of the edge. This is located at the point of the event. The identification numbers of 
the output vertices are recorded in the visible edge's active edge block. These will be referred to 
as the left and right "base" vertices. When a visible edge ends or is occluded, vertices are created 
for each segment at the point of the new event. These will be referred to as the left and right "top" 
vertices. When both base and top vertices have been created, each visible segment is added to the lists 
of segments for the polygons separated by the edge. Directionality is implicit in the convention used. 
The left segment always extends from the left top vertex to the left base vertex. The right segment 
has the opposite orientation, extending from the base vertex to the top vertex (Figure I). At each 
vertex, for any visible edge entering the vertex from below or from the left, both left and right visible 
segments are recorded. One is used to reconstruct the polygon to the edge's left, the other to reconstruct 
the polygon to its right. These segments connect the newly created output vertices with previously created 
vertices located at prior events. Visibility tests are performed as in the visible line algorithm and 
described by Hamlin and Gear[5]. All edges entering or leaving a vertex are processed  Volume 15, 
Number 3 August 1981 together. When processing a vertex, edges entering the vertex are removed from the 
active edge list. Those edges removed that are visible are inserted on a "visible vertex and their a 
edge re vis list." Edges leaving added to the active edge ibility determined. the list In many applications 
it does not matter if edges shared by two output polygons are explicitly represented as being shared. 
However, in some cases~ this information is necessary. This is true, for example, when the polygons are 
to be smoothly shaded. Since output vertices are created after the visibilities of all edges involving 
the vertex have been established, it is possible to produce either form of output. If shared edges arenot 
to be explicitly represented, then the production of visible segments at a vertex is relatively straight 
forward. Each pair of visible edges impinging upon the vertex bounds a polygon. An output vertex pair 
is created for each such pair of edges. The output vertices are given the x and y coordinates of the 
event vertex and the z coordinate is calculated from the plane equation of the polygon bounded by the 
two edges. For each edge on the visible edge list, two visible segments are written with the proper orientation. 
These segments are then added to the linked lists in the segment buffer for the polygons to the left 
and right of the edge. The left segment's segment buffer index number and the left polygon's number are 
recorded in the left top vertex's segment and polygon lists. For each edge upward, the numbers of the 
vertices created to the edge's left and right are recorded in the edge's active edge block as the left 
and right base vertices. If there are no visible edges below the vertex, a thread is written. The base 
vertex of the next visible edge to the left of the freshly inserted active edges is used as the base 
vertex of the thread. The output vertex for the region below the vertex, bounded by the leftmost and 
rightmost visible edges leaving the event vertex, is used as the top vertex (Figure 7). If' the edges 
and vertices of the output polygons must be explicitly shared, then the output vertices are created somewhat 
differently. The output vertices used for the visible segment on the left and right of an edge ought 
to differ only if the polygons separated by the edge do not both include the event vertex. Note that 
it is not necessary that the edge be shared by the polygons to its sides for the output polygons to share 
a vertex. Shared output vertices can be created in the following manner. An output vertex is first created 
for the polygon bounded by the leftmost edge on the visible edge  24 Computer Graphics Volume 15, Number 
3 August 1981 list and the leftmost visible edge added to the active edge list. The edges on the active 
edge list are processed in clockwise order, and those on the visible edge list in counter-clockwise order. 
Two conditions can result. In the first case, all the polygons can be found to include the event vertex 
and, thus, all se&#38;ments will include the same output vertex. In the second case, edges can be found 
that divide a polygon (or polygons) sharing the vertex from a polygon (or polygons) not including it. 
If" this occurs, a new output vertex is created and processing then continues on the remaining edges. 
 Visible segments are also produced at certain edge crossings. If a visible edge is obscured as it is 
crossed by the silhouette of a shallower polygon then three visible segments are written: the segments 
on either side of the obscured edge and a segment on one side of the occluding edge (Figure 8). The number 
of output vertices created depends on whether the edge that is obscured is a shared edge or a silhouette 
edge. If the edge is shared, and output polygons are to share edges, then only one output vertex is created; 
if' the edge is a silhouette two are required. The base vertex of the occluding edge for the side on 
which the polygon is visible will be changed to a new vertex. If an edge emerges from under a shallower 
polygon and becomes visible, then one segment is written. Again one or two vertices may be created, deDendin~ 
on whether the newly visible edge is shared or not. The base vertices of the emerging edge will be the 
vertex or vertices created, and the base vertex on one side of the edge of the shallower polygon will 
change. 4. Reconstruction To construct a visible piece of a given input polygon, the reconstruction 
routine follows the chain of reconstructable segments pointed to in the input polygon's database block 
and then recurses to reconstruct any holes connected by threads with the first chain. The first segment 
on this list will be termed the "initial segment." Since the initial segment was the last inserted, this 
segment contains the highest visible vertex in the polygon, and is thus part of the outer contour of 
the visible polygon. The initial segment is removed from the segment list and its tail vertex added to 
a list of contour vertices. The vertex block at its head is searched for both the next segment and all 
threads belonging to this polygon. The threads are put on a thread list and the tail of the next segment 
is added to the list of contour vertices. The head vertex of this new segment is again searched for threads 
and the next segment. Eventually a closed loop is formed. Since the list of contour vertices consists 
of record keeping vertices from the output vertex pairs, the insertion vertices of the pairs are substituted 
for their mates. This list of vertices is then added to the database as the outer contour of a polygon. 
 The procedure reconstructs holes in an outer contour by recursion. This is done by passing the thread 
list from the previous level of recursion into the routine and passing the first thread on the thread 
list as an initial segment. Some threads may have been added to the thread list which are of no use. 
These are the threads connecting two vertices of contours which have already been reconstructed. Therefore, 
before recursion, any threads ending with a vertex in a contour already constructed are culled from 
the thread list. If there are any threads remaining on the thread list then there are holes to reconstruct. 
 If after all holes have been reconstructed, there are still line segments pointed to by the parent polygon, 
other pieces need to be reconstructed. This is continued until all segments have been used. Reconstruction 
is quite rapid. It is not necessary to search the segment buffer for successive segments. Only the list 
of segments leaving a particular vertex must be searched to connect one segment with the next in the 
contour. These lists are very short. Hole contours do not have to be matched with outer contours, since 
they are tied together by threads. It is necessary, however, to remove unnecessary threads from the thread 
list. Checking for this requires a pass through the thread list for each vertex in the contours constructed. 
 E. Discussion and Conclusion An algorithm for determining visible lines or visible surfaces in polygonal 
form, at object resolution, has been presented. The original scene must consist of non-intersecting planar 
 polygons. The procedure relies on image coherence since the sampling is dependent on the complexity 
of the image. The reconstruction method is based on an elaborate data structure, which allows the polygonal 
output to be easily obtained. There are several factors to consider in assessing the appropriateness 
of this algorithm for a particular use. First, as with all scan-line algorithms, this algorithm works 
best for scenes of low depth complexity. This is due to the fact that all edges, including those of totally 
obscured polygons, must be considered in the visibility calculations. For environments of great complexity 
this cost can become excessive. Second, the 25   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806786</article_id>
		<sort_key>29</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Contour filling in raster graphics]]></title>
		<page_from>29</page_from>
		<page_to>36</page_to>
		<doi_number>10.1145/800224.806786</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806786</url>
		<abstract>
			<par><![CDATA[<p>The paper discusses algorithms for filling contours in raster graphics. Its major feature is the use of the line adjacency graph for the contour in order to fill correctly nonconvex and multiply connected regions, while starting from a &#8220;seed.&#8221; Because the same graph is used for a &#8220;parity check&#8221; filling algorithm, the two types of algorithms can be combined into one. This combination is useful for either finding a seed through a parity check, or for resolving ambiguities in parity on the basis of connectivity.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Connectivity filling]]></kw>
			<kw><![CDATA[Graph traversal]]></kw>
			<kw><![CDATA[Parity check filling]]></kw>
			<kw><![CDATA[Phototypesetting]]></kw>
			<kw><![CDATA[Scan conversion]]></kw>
			<kw><![CDATA[Seed filling]]></kw>
			<kw><![CDATA[Shading]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>B.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010750</concept_id>
				<concept_desc>CCS->Hardware->Robustness</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
			<gt>Reliability</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39026364</person_id>
				<author_profile_id><![CDATA[81100082845]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Theo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pavlidis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Laboratories, Murray Hill, NJ]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Newman, W. M. and Sproull, R. F. Principles of Interactive Graphics, second edition, McGraw-Hill, 1979.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807434</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Brassel, K. E. and R. Fegeas "An Algorithm for Shading of Regions on Vector Display Devices," Proc. SIGGRAPH 79, pp. 126-133.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Pavlidis T., "Filling algorithms for raster graphics," Comput, Graphics Image Proc. 10, 1979, pp. 126-141. Also in preliminary form in Proc. SIGGRAPH 78, pp. 161-166.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Sproull, R. F. Review of {3} in Computing Reviews November 1979, p. 500. (Review No.35, 467.)]]></ref_text>
				<ref_id>3a</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Ackland, B.D. and Weste, N. "The Edge Flag Algorithm - A Fill Method for Raster Scan Displays," IEEE Trans. Computers, C-30 (1980), pp. 41-48.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807380</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Lieberman, M. "How to color in a coloring book," Proc. SIGGRAPH 78, pp. 111-116.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807456</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Smith, A. R. "Tint fill," Proc. SIGGRAPH 79, pp. 276-283.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Caspers, B. E. and Denes, P. B. "An Interactive Terminal for the Design of Advertisements," Bell System Technical Journal, 58 (1979), pp. 2189-2216.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807511</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Shani, U. "Filling regions in binary raster images - a graphtheoretic approach," Proc. SIGGRAPH 80, pp. 321-327.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578775</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Aho, A. V., J. E. Hopcroft, and J. D. Ullman, The Design and Analysis of Computer Algorithms Addison-Wesley, 1974.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Pavlidis, T. Structural Pattern Recognition, Springer Verlag, Berlin-Heidelberg-New York, 1977.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Pavlidis, T. and Steiglitz, K. "The Automatic Counting of Asbestos Fibers in Air Samples," IEEE Trans. Computers, C-27, 1978, pp. 258-261.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015089</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Shapiro, L. "Data Structures for Picture Processing," Proc. SIGGRAPH 78, pp. 140-146.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578095</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Rosenfeld, A. and Kak, A. C. Digital Picture Processing, Academic Press, New York, 1976.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>538576</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Pavlidis, T. Algorithms for Graphics and Image Processing, Computer Science Press, Maryland, 1981 (in press).]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 CONTOUR FILLING IN RASTER GRAPHICS Theo Pavlidis Bell 
Laboratories Murray Hill, NJ 07974 ABSTRACT The paper discusses algorithms for filling contours in raster 
graphics. Jts major feature is the use of the line adjacency graph for the contour in order to fill correctly 
nonconvex and multiply connected regions, while start- ing from a '~seed." Because the same graph is 
used for a "parity check" fil- ling algorithm, the two types of algorithms can be combined into one. 
This combination is useful for either finding a seed through a parity check, or for resolving ambiguities 
in parity on the busis of connectivity. Keywords: Connectivity Filling. Graph Traversal, Parity Check 
Filling, Pho- totypesetting, Scan Conversion, Seed Filling, Shading. 1. INTRODUCTION AND REVIEW The problem 
of filling the contour of a region with a given color can be attacked by techniques belonging to two 
broad classes: "polygon based" and "pixel based". "Polygon based" techniques are applicable to both vector 
and raster graphics while "pixel based" can be used only in raster graphics. "Pixd based" techniques 
can be further subdivided into "parity check" and "connectivity" methods. The purpose of this paper is 
to present a "connectivity" method based on some simple graph theoretic considerations and review briefly 
the other methods. "Polygon based" techniques are also known as "ordered edge list" or "scan conversion" 
techniques [1] and are applicable whenever the contour is given as a polygon. The sides or edges are 
sorted according to their coordinates, and then the sorted list is scanned. At each stage one maintains 
a list of active edges that intersect a horizon- tal line. Then the line is filled between the first 
and second, third and fourth, etc., edges. When the horizontal line crosses a maximum in the y direction, 
a new pair of edges must be added to tile list, while when the line crosses a minimum, a pair of edges 
is removed. There-fore the sorted list of the edges must contain certain flags indicating the occurrence 
of such points. See Chapter 16 of [1] for further dis- cussion of such methods and [2] for a detailed 
description of an algo-rithm suitable for vector graphics. Most of the computational cost occurs in the 
preparation of the list and the actual filling is very fast because each p61nt of the interior is visited 
only once and no outside points are ever examined. The method is appropriate for applications where a 
contour is displayed repeatedly, as it is the case in photo- typesetting, lndeed, an impressive implementation 
can be found in the Mergenthaler Linotron 202 phototypesetter for creating fonts out of contour descriptions. 
(This text has been produced by such a machine.) Permission to copy without fee all or part of this 
material is granted provided that the copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the publication and its date appear, and notice is given that 
copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, 
requires a fee and/or specific permission. &#38;#169;1981 ACM O-8971-045-1/81-0800-0029 $00.75 29 While 
the filling of the sorted contours can be much faster than the filling done by "pixel based" techniques, 
this is not true for the total processing time that includes the sorting [3]. Thus a comparison of techniques 
without including the sorting time [4] is valid only for applications where the same contours are filled 
repeatedly. The major advantage of "pixel based" techniques is that they are not supposed to require 
any preprocessing of the contour or assume any regularity in its form. Furthermore the cost of sorting 
and interpolation is avoided because these operations are done, in effect, by the device that displays 
the contour. The price of this convenience is the loss of topological information of the contour although 
such information may not have been available in the first place. This is the case when one draws a contour 
on display in an interactive sys- tem, or when a contour is found by edge detection or thresholding an 
image. It is also essential in such techniques to have well defined colors for the contour and/or the 
interior of the picture. In particular if a contour of color Z is to be filled there must not be any 
other objects with color Z intersecting it. (See [3] and [3a] for more on these points.) One set of "pixel 
based" techniques performs a parity check similar to that of "scan conversion" techniques. The contour 
is inter- sected by a set of horizontal lines and pixels are filled if they lie to the right of an odd 
number of intersections (assuming that the leftmost part of the line is outside the region). The major 
difficulty facing such a scheme is that different arcs of the contour may be mapped on the same pixel 
(Figure 1). Such an occurrence is not a problem in "scan conversion" techniques where the decision to 
fill is made in terms of a list but it causes serious problems here. We shall use the term "undersampling" 
to denote this case. _J Figure I Illustration of undersampling: some single pixels (marked by *) contain 
two arcs of the contour It can be shown [3] that it is theoretically impossible to fill an arbitrary 
contour correctly by considering only a finite number of scan lines at a time without retracing. Therefore 
two remedies are avail-able, both eliminating some of the advantages of the method. In one some preprocessing 
of the contour is done so that a simple parity check works correctly on the result. This principle has 
been used in Algorithm A4 of [3]. Computer Graphics Volume 15, Number 3 August 1981 The other remedy 
is to assume certain regularity of the contour, in particular a limit in the number of arcs mapped on 
the same pixel. This principle is used in Algorithms A2 and A3 of [3]. Practical experience with an implementation 
of these algorithms suggests that contours drawn on an interactive graphics system can be handled suc- 
cessfully by them while this is not the case with contours obtained from edge detection or thresholding 
of images. "'Connectivity" or "seeding" techniques start with a pixel which is known to lie in the interior 
of a region, a seed, and the interior is found as all pixels connected to the seed. Such methods have 
been described recently by H. Lieberman [5], A. R. Smith [6], B. E. Caspers and P. B. Denes [7], and 
U. Shani [8]. Caspers and Denes discuss the problem within the context of phototypesetting and they use 
the term '+screening" to denote the filling of a contour by a "screen" pattern. Shani's paper is important 
because it emphasizes the relation of contour filling to graph traversal and uses this connec- tion to 
advantage by correcting mistakes in one of the earlier algo- rithms. Furthermore, the modeling of the 
problem in terms of graphs suggests various ways to improve upon the earlier algorithms, and most important, 
suggests ways to integrate "parity check" with "con- nectivity" fill because the same data structure 
can be used for both. Such combinations are discussed in Section 4, and the first three examples shown 
in Section 5 have been produced in this way: a parity check is used to find a seed, and then the connectivity 
algorithm takes over. In the sequel we assume that the reader is familiar with elemen- tary graph theory 
and graph traversal algorithms. (See pp. 50-55 and 172-176 of [9] or pp. 46-57 of [10] for an introduction 
to the subject.) A discussion of "connectivity" filling would not be complete without mentioning the 
+'obvious" algorithms that is based on recur-sion. The filling procedure fills the seed and then calls 
itself four times, once for each of the four direct neighbors of the seed. These pixels play now the 
role of the seed, and so forth. If such a "seed" is found to be already filled no further calls are made 
from that point. (See p. 253 of [1] for details.) The apparent simplicity of this algo- rithm is misleading 
because each time a procedure is called there is a certain computational cost that is not obvious in 
the listing. In partic- ular, one must save the values of registers, as well as the necessary information 
for returning to the original point. On the other hand, all the other "pixel based" algorithms make a 
subroutine call only at contour points, so that the number of such calls equals approximatelly the number 
of horizontal contour intervals. Pixels in the interior of the region are addressed in a sequential fashion 
and for each of them one does only a simple test: whether they are on the contour or not. The sequential 
addressing can be implemented very efficiently with inde,~: registers on most machines. 2. GRAPH TRAVERSAL 
AND LINE ADJACENCY GRAPHS In order to traverse an image in an orderly fashion it is cus- tomary to create 
a graph whose nodes correspond to pixels, or groups of pixels, and branches connect nodes corresponding 
to areas which are geometrically close. The simplest graph which can be used for this purpose is the 
grid graph where nodes correspond to single pixels and each node is connected to its four or eight neighbors. 
This representation is too cumbersome and a more convenient data struc- ture is the line adjaeen~ 3, 
graph (LAG). Its nodes correspond to intervals of adjacent pixels of the same color along horizontal 
raster lines. If two such intervals lie on adjacent lines, are of the same color, and touch each other 
on a side, then a branch is established between the corresponding nodes. Because the nodes can be ordered 
according to their vertical coordinate the graph can be made to be directed. Thus at each node we have 
an above degree (branches into it from the nodes on the line above) and a below degree (branches from 
it to the nodes on the line below). This graph was used by Shani [8] in the context of connectivity filling 
and by this author for parity check filling [3]. It has also been used in pattern recognition [10-12]. 
On a given picture one can form such a graph for,each color. In particular, we can have one graph for 
the contour pixels (c- LAG) and another for the interior (blank) pixels (i-LAG) as shown in Figure 2. 
The filling problem is then equivalent to traversing a connected component of the i-LAG. There is an 
important distinction between the two graphs. It is well known in image process- ing (e.g., [1% pp. 57-62 
or [13], pp.335-341) that in order to obtain results which make sense topologically one must use different 
type of connectivity for each of the two colors in a binary picture. Thus if we assume intervals to be 
connected along sides for the interior, we must assume that intervals of the contour are considered to 
be connected even if they touch only at a corner. (See Figure 2.) Figure 2 Definition of the c-LAG and 
the i-LAG and demons- tration of some of their properties. Node A of the i-LAG has below degree 2 and 
it is above node C of the c-LAG which has above degree 0. A symmetrical situation exists for nodes B 
and D. In this paper we will present an algorithm for filling by travers- ing the c-LAG instead of the 
i-LAG. The main motivation for that choice is the use of the same data structure as with parity check 
algo- rithms and the possibility of a speed up of the filling process. We dis- cuss the general methodology 
in the remainder of this section, the algorithm in Section 3, the combination of the methodologies in 
Sec- tion 4, and we present examples of implementation in Section 5. There are numerous algorithms for 
graph traversal. Typically, the nodes are marked initially as "new" and when they are visited their mark 
is changed to "old". If the current node has more than one neighbor marked "new", one of them is chosen 
as the next current node and the others are placed on a stack. If the current node has no neighbors marked 
"new", then the next current node is found by popping the stack until a node marked "new" is found. Lieberman 
[5] and Shani [8] also mark nodes before placing them in the stack so that no nodes are placed there 
twice. This is not neces-sary for a correct traversal because multiple traversals are avoided by examining 
the mark of the node when it is taken from the stack. However, allowing the multiple introduction of 
nodes into the stack may cause it to grow to a very large size, of order N 2 for an arbitrary graph with 
N nodes. On the other hand the LAG is a graph where most nodes have both above and below degrees equal 
to one, and nodes where either.of these degrees exceeds one correspond to places where the contour has 
either a maximum in the vertical direction with the interior of the region above it (such as nodes A 
and C in Figure 2), or a minimum in the vertical direction with the interior of the region below it (such 
as nodes B and D in Figure 2). Therefore the size of the stack is not a major concern in the traversal 
of LAG's and the proposed algorithm allows multiple entries in the stack. The above observation can be 
used to advantage in the design of a filling algorithm. Because of its importance, we repeat it for- 
mally in terms of node degrees. We introduce the notation (re,n) to denote the degrees of a node: m for 
above and n for below. Proposition 1: if a node of the i-LAG has degree (re,n) with n>l then there will 
be a node of the c-LAG with degree (0,d) in the line below. Similarly, if m>l there will be a node of 
the c-LAG with degree (d,0) above it. The converse is also true in both cases. Proof: A formal p~oof 
of this proposition can be made easily as following: Let Xleft and )fright be the coordinates of the 
endpoints of an interior interval. If the i-LAG has above degree greater than one, then there must be 
at least two interior intervals on the line above, the leftmost with endpoints tlleft,Uright and the 
other with Vlefi,Vrightsuch that Computer Graphics Volume 15, Number 3 August 1981 Xld ~ < ltright and 
v&#38;~ < Xright (1) (we have strict inequalities because interior intervals are assumed to be connected 
only when they touch along a side.) The interval (u,.L~h,,V~w~)on the line above must belong to the contour 
(it has a dif- ferent color from the interior). It would be connected to intervals of the same color 
in the line below if and only if one of the following inequalities holds i.e., it returns the address 
of the rightmost pixel to the left ofp which has the same color as p, and has the property that there 
is at least one pixel of different color between the two. (Figure 3b.) The implemen- tation of such procedures 
is straightforward and needs no further dis- cussion. The degrees of a node (interval) of the LAG containing 
the pixel p are found by the procedure LINK(p,v) which returns the vec-tor v defined as Uright ~ Xid 
~ or vl,,t~ <-- x,.igh~ (2) Both of them contradict (1), therefore the contour interval is not con- 
nected to any other intervals in the line below and thus the respective node in the c-LAG has below degree 
zero. Conversely, if a c-LAG node has below degree zero, then none if the inequalities (2) can be true. 
Their negation is the set of inequalities (1), and therefore we have a node of the i-LAG with above degree 
greater than one. A similar proof can be made for the other part of the proposition. Therefore instead 
of checking the degree of the nodes of the i- LAG we may check the degree of the nodes of the c-LAG. 
The practical advantage of such a choice is due to the fact that evaluation of the degree requires examination 
of three lines at a time. For the i-LAG these lines can be quite long while the contour intervals tend 
to be much shorter. Thus the algorithms described by Lieberman [5], Caspers and Denes [7], and Shani 
[8] require effort proportional to 3.1, where / is the area of the interior. The proposed algorithm requires 
effort proportional to / + 3.C, where C is the area of the contour pixelst. Thus, if one deals with objects 
where C is much smaller than 1, the proposed algorithm will be faster than the earlier ones. (See Section 
3.3.) 3. FILLING BY TRAVERSAL OF THE CONTOUR LINE ADJA- CENCY GRAPH 3.1. AUXILIARY PROCEDURES Before 
describing the main algorithm we outline certain pro-cedures used by it. We shall use the term pixel 
address to denote whatever way is used to describe the location of pixel. This can be a pair of x-y coordinates, 
a pointer to an array, etc. If p is the address of pixel A, the expression p-1 denotes the address of 
the pixel to its left (on the same horizontal line), and p+ 1 the address of the pixel to its right. 
LEFT(P) P LRIGHT (P) P (a) (b) t ................ (c) Figure 3 (a) Definition of LEFT(P). (b) Definition 
of LRIGHT(p). (c) Definition of pixels Pt, P2, el, and e 2. The argument p of the procedure LINK is any 
pixel in the shaded zone between P2 and Pl- The procedure LEFT(p) returns the address of the leftmost 
pixel of the fixed color interval containing p. (Figure 3a.) The procedure LRIGHT(P) is defined as LRIGHT(p 
) = LEFT(LEFT(P)-1)- 1, t These expressions count the number of times a pixel is addressed, not the number 
of times it is the "current pixel'" during the execution of the algo- rithm. v = (a, b. Pl, el, e2) 
where a and b are the above and below degrees and Pi and e i are pixel addresses as shown in Figure 3c. 
If any of these pixels is not defined, the returned address is zero. (If zero is a legitimate address, 
then the implementer can choose some other value to initialize e I and e2 inside LINK.) A detailed description 
of LINK is given in the Appendix. Because the seed may be given in the middle of a region, it is necessary 
to do the filling in both directions from it. Such bidirec-tional traversal is not necessary for any 
other pixel from the stack. Instead of having a special procedure for the traversal of the first region, 
it is best to pay some attention to the initialization. If the pixel seed,t,,,,.,,, directly above the 
seed, is not in the contour then LEFT(seed,h,,,~) should be placed in the stack with an indication that 
it should be used to start an upward scan. This is expected to be the common case in interactive graphics 
because a user is likely to provide as a seed a pixel far from the contour. If the pixel above the seed 
is on the contour, then we proceed as follows. First we examine the degree of that contour interval through 
a call to LINK. If it has degree (0,2), then the seed was chosen just below a maximum, so there is no 
need for an upward scan. Otherwise, the seed was chosen just beneath a nearly horizontal arc of the contour 
and we replace the seed by LEFT(seed) and call LINK for the contour interval to the left of the seed. 
el is then chosen as a seed for an upward scan. The algorithm can be described informally as follows. 
It fills horizontal lines by proceeding from left to right starting from a pixel immediately to the right 
of the contour. (This can be found in the first line by a call of LEFT(seed).) Let p be the value of 
that pixel. Before filling, a call to LINK(p-1) is performed to inves- tigate the c-LAG of the [eft contour 
interval. If a and b are both equal to one, then we have a simple arc and we can specify the start- ing 
location P,,e.,., for the next scan line. If the direction of vertical scan has the value "down", then 
we set p,,¢,, = e 2, otherwise Pne, = el-After saving that location we proceed filling horizontally starting 
from p until a contour pixel is found. Let Pr~gh, be the address of the last interior pixel. We call 
next LINK(pri8h,+l ) to investigate the c-LAG at that end. If it returns a and b both equal to one, then 
we have nothing else to do except advance to the next line. This is accomplished by replacing p by P,,e.~, 
and repeating the process. Clearly, as long as we are not at a maximum or a minimum of the contour the 
operation of the algorithm is very simple. Part of the cost of the calls to LINK would have been incurred 
in earlier versions of connectivity filling algorithms [5-8] as well because they must also search for 
a starting point on the next line. The interesting cases are where a and b are not both one at both ends. 
In particular if one of them is zero,'then we know that we have an extremum and we must either place 
a pixel in the stack, or terminate the filling in a particular direction. This is the essential part 
of the algorithm which is described formally in the next section. 3.2. DESCRIPTION OF THE MAIN ALGORITHM 
The algorithm is listed in Table I. Besides the stack S, it also uses a stack Sa to store the proper 
direction, when placing an address in S. The procedure POP(stack) removes the top element of a stack 
and returns its value. Step 3 examines the direction of the scan and sets up certain variables accordingly. 
In particular, u -1 means that el will be selected as the next pixel, while u = 2 will cause the selec- 
tion of e 2. The use of these variables eliminates the need to have separate code for upward and downward 
traversal. The loop consist- ing of steps 5-17 is the main part of the algorithm. Computer Graphics Volume 
15, Number 3 August 1981 Table I: Filling Algorithm by Connectivity Notation:p,,,,.~ is the pixel that 
will be used to start the filling on the new line. P,,h,,, can have the value P,,i,,,,,, or P~,,,i,,.. 
and is the pix- el lying directly above or below the current one. 0. Input is the image array, the address 
of an interior pixel seed and, possibly, a second pixel seed,,t,,,,~. If the second pixel is given, LEFT(seed,h,~e) 
is placed in the stack S and the direction up in the stack Sa. LEFT(seed) is placed in the stack S and 
the direction down in S,/. 1. While the stack S is not empty repeat steps 2-17 Begin  2. p ....... 
= POP(S), dir = POP(Sa), Pri~J,, = Xmax 3. If dir equals down, then set u = 2, other = below. Else u 
= 1, other = above. 4. Repeat steps 5-17 Begin 5. If p,,,,~, equals zero or if the addressed pixel is 
filled, then exit from the loop. 6. p = p,,,,,~ 7. LINK(p-1 ) {check left contour} 8. If [(dir equals 
down and a exceeds 1) or (dir equals up and b exceeds 1)] and p is to the right of Prlght then exit from 
the loop. 9. If both a and b exceed zero, then set p,,,,.,, = e,,.  Else do: Begin 10. Place in S 
the address LEFT(LRIGHT(p)) and in Sa the opposite of the value of dir, provided that the corresponding 
pixel is not already filled. 11. If (a equals zero and b does not and dir equals down) or (b equals 
zero and a does not and dir equals up), then set P,,~v~ = e,. 12. Else  If P,,a,e,-is not filled, then 
set p,,~.~, = LEFT(po~j,,,. ). Else set p ...... -0. End 13. Fill the line starting from p and let Pright 
be the last pixel before the contour. 14. LINK(p~,,eh,+ 1) {check right contour} 15. If either a or 
b equals 0 and if Pl is not filled, then  do: Begin 16. Placept in stack S 17. If a>O, then place 
up in Sa. Else place dou,n in S,~.  End End End Step 5 examines whether the next pixel is a legal address, 
or whether it has already been filled. In either case, the algorithm exits from the inner loop and it 
executes step 1. If the stack is not empty, a new pixel is popped and the whole process is repeated. 
If the next pixel is a legal address and not yet filled, then it becomes the current pixel (step 6). 
Step 7 evaluates the degrees of the c-LAG at the left end. Step 8 performs the first nontrivial operation: 
it checks whether we have reached the end of the interior in the direction of the scan. If the scan 
direction is downward and the above degree a exceeds one, we must have reached either a "bottom" as 
shown in Figure 4a, or one of the configurations shown in Figures 4aa and 4ab. The former case is detected 
by verifying that the current pixel p is to the right of Pright, and therefore outside the contour. Then 
filling in that direction is terminated and the algorithm exits from the loop. Other- wise we know that 
the current pixel is indeed in the interior. (If the scan direction is upward we have a similar test 
for checking whether we reached a "top".) Step 9 checks whether both the above and below degrees are 
nonzero and sets aside the starting point in the next line. We do not insist that both degrees be one, 
even though nodes with degree (1,2), or (2,1) are impossible for well formed contours since they may 
be present because of undersampling (as shown in Figure 2). For most pixels of the contour the conditions 
of this step will be true and the algorithm will proceed directly to step 14. If at least one of the 
degrees is zero, then we know that we must place a pixel in the stack (according to Proposition 1) and 
some caution is required in defining Pnexl. The placement in the stack is performed in step 10. Figure 
4b shows the pixel arrangement when a = 0. Note that the existence of the contour arc shown to the left 
is guaranteed if the contour is indeed closed. Otherwise, the pixels on the line above p will have the 
same color as p and will not be bounded to the left. Figure 4bb shows the relation between the c-LAG 
(full circles for nodes) and the i-LAG (open circles for nodes) in that case. The term "opposite of the 
value of dir" means that if dir equals dou'n, up is placed in the stack, and vice versa. Having either 
of the degrees zero means that we must be careful in defining the starting point for the next scan line. 
If we are proceeding downward and the above degree is zero but the below degree is not, then there is 
no problem. This check is performed in step 11 together with a similar check for the case of upward traversal. 
The configuration of the contours when the conditions in Step 11 are true during a downward traversal 
is shown in Figure 4d. Step 12 handles the case when there are no contour pixels in the next line (according 
to the scan direction) connected to the contour. This configuration is shown in Figure 4e for a downward 
direction. It examines the pixel directly above (,P,,b,,,.,,), or directly below (Pbe/,,,,) the current 
one. If it is not filled, then it selects as next pixel one on the same line which is immediatelly to 
the right of the contour. (See the comments in Step 10 regarding the existence of such a contour arc.) 
In the listing of the algorithm the notation Pother is used to denote either of these pixels. Depending 
on the direction of the scan the variable other is given the value above, or below. Step 13 does the 
actual filling of the line and it is the step which sees most of the pixels. For each pixel it checks 
whether it has the color of the contour, and if not it fills it with the appropriate color. Step 14 checks 
the degree of the c-LAG at the node correspond- ing to the right contour arc. If either of the degrees 
is zero, then the pixel on the other side of the contour arc (Pl in Figure 4c) is placed in the stack 
together with the appropriate direction. (Steps 15 and 16.) Figure 4cc shows the configuration in terms 
of the two LAGs. We observe that at most two nodes at a time are placed in the stack. However, all nodes 
connected to a given one are eventually either placed in the stack or filled. In the example of Figure 
5, let C be the first node visited during an "up" traversal. Then B and D are placed in the stack. Whenever 
D is visited again (either by' being pulled from the stack or by being reached from another node), E 
will be placed in the stack.  3.3. SPEED OF THE ALGORITHM The algorithm is characterized by the fact 
that it does most of the comparisons around extrema of the contour in the vertical direc- tion, points 
which are relatively rare. Thus in spite of its apparent complexity it can be very fast. Most of the 
time steps 10-12 and 15-17 are skipped. Also most of the time the conditions in step 8 are false so that 
only some of the comparisons take place. The typical sequnce of steps executed in the inner loop is 5, 
6, 7, 8(part), 9, 13 and 14. Except for the calls to LINK these involve very simple operations. Computer 
Graphics Volume 15, Number 3 August 1981  Pright ~/~ P I (aa) IPrighf~ Pright (a) (ab) LRIGHT(P) LRIGHT~ 
-~~ (bblLEFT(LRIGHT(P)} (~STACK) (b) IPright~ p I Pright~ pl (~STACK) (c) ~- Pnext (d] V/I/iliA Pne 
X i Pbelow (e) Figure 4 (a), (aa), and (ab) Configurations which when seen during a downward scan cause 
the condition in brackets([...]) of Step 8 to be true. However, only (a) indicates that a "bottom" has 
been reached and filling in that direction stops. (b) When the left contour is examined, and the shown 
configuration is found pixel p is placed in the stack. (bb) The two LAG's for configuration (b). (c) 
When the right contour is examined, and the shown configuration is found pixel p is placed in the stack. 
(ce) The two LAG's for configuration (c). (d) Configuration when the conditions of step 11 are true. 
(e) Configuration when the conditions of step 12 are true. Figure 5 If C is the first visited node, nodes 
B and D are placed in the stack. Node E will be placed only when D is visited again. The call to LINK 
is probably the most expensive operation although it does not involves much work either. If the contour 
has width equal to m pixels (note that usually m equals one) in three suc-cessive lines, then one need 
examine at most 5m + 5 pixels': m + 1 pixels in the current line and 2m + 2 on each of the lines above 
and below. It must look at all pixels directly above or below the current interval (m), m pixels while 
looking for the end of the lines so it can determine e I and e2, plus one more pixel in each line where 
it finds that it must stop. Figure 6 shows two configurations where the bound 5m + 5 is reached. 0 0 
0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 (a) 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 
0 0 0 0 (b) Figure 6 Contour configurations showing underlined the pixels that must be visited by the 
LINK procedure On a vertical contour arc consisting of only one pixel (m = 1) at each line the number 
of pixels examined equals 3m + 5 = 8. Thus for the fill of a rectangle of width 1, the algorithm will 
examine on each line 1 + 16 pixels. (The number 16 comes because the con-tour is examined at both ends.) 
An algorithm of the kind described in references [5-8] will be looking at the lines above and below for 
each pixel, so that its basic cost is 3.1. In addition it must look at extra pix- els on each side to 
find out when it should stop and which will be the starting point in the next line. It turns out that 
on the average only one pixel per side and line need be visited, so that the total number per line is 
3.1 + 6. (These calculations assume that we do not know that we have a rectangle. Otherwise the problem 
would have been trivial.) Then the new algorithm will be faster if 1 is greater than 5. For contours 
which are far from vertical as the examples shown in Figure 6 the cost of the new algorithm will be 1 
+ 30. (15 pixels on each side.) It will be faster than the old if I is greater than 12. Since the programs 
for both algorithms are equally complex (see [5] or [8] for example) the new algorithm is probably preferable 
for applications 33 Computer Graphics Volume 15, Number 3 August 1981 where the ratio of interior pixels 
over contour pixels is expected to be greater than 10. 4. COMPARISONS AND COMBINATION WITH A PARITY CHECK 
ALGORITHM It is natural to ask at this point how connectivity algorithms compare with parity check algorithms. 
In some ways the computa- tions performed by both are the same. In particular the procedure LINK mentioned 
in this paper is virtually the same as a procedure used by the author in [3] for a parity check algorithm. 
Connectivity algorithms have two major advantages: (a) the filling never "leaks" as long as the contour 
is connected. (b) The computation time is pro- portional to the area to be filled instead of the area 
of a rectangle cir- cumscribed around the contour as in the case with pixel based parity check algorithms. 
(However, scan conversion algorithms which use an edge list look only at the interior, even though they 
are based on parity check.) Their major disadvantages are: (a) The need to know a point of the interior, 
which in some applications may be difficult to find. (b) If a figure has a connected contour but a nonconnected 
interior, then only the part containing the seed will be filled. (A par- ity check algorithm will fill 
all parts.) (c) The interior is not traversed in raster order, which may cause excessive communication 
between host computer and controller of the graphic display. This point is relevant in the following 
context. It is often convenient to do the filling by displaying the contour on the refresh memory of 
the device and then fill the pixels directly there. If we cannot program the microprocessor, then we 
may read all the horizontal lines in sequence, keeping three of them in main memory at a time, and evaluate 
the degrees of the LAG there. In a parity check, once a line has been examined, it does not have to be 
examined again. This is not the case with a connectivity algorithm. Thus the actual computational cost 
may be higher because of the larger amount of I/O. Table II: Finding a Seed by a Parity Check 0. Input 
is the image array containing the contour. 1. Set the variable oldcount to zero. 2. For each horizontal 
line repeat steps 3-10.  Begin 3. Set the variable count to zero.  4. For all pixels p of the line, 
starting from the left, repeat steps 5-8.  Begin 5. If p belongs to the contour, then call LINK(p). 
Else skip the next three steps. 6. If both a and b equal 1, then do steps 7 and 8.  Begin 7. If count 
equals zero, then save in location Ps the address of the first pixel immediately after the contour interval. 
 8. Increment count.  End End 9. If the value of count is even and the value of oldcount is nonzero, 
then return the value Ps as the seed address. 10. Set oldcount = count. End Because both types of algorithms 
use the same basic procedure, LINK, it is possible to create a combined algorithm which fills some parts 
of an image by a parity check and others by seeding. This can be important in applications where it is 
difficult to determine a seed in advance. Such an initialization procedure is listed in Table II. Basically, 
it consists of a horizontal scan which proceeds until a line with an even number of nodes with degree 
(1,1) is found in the c-LAG. This by itself may give erroneous results if there is undersam- piing of 
the form shown in Figure 2. (Two such peaks would satisfy the condition!) It is possible to avoid the 
problem, if we label pixels in the way done in [3]. However, this may not be necessary for most applications 
and we may simply check whether the line above has a nonzero number of nodes of degree (1,1) in the c-LAG. 
(This is a very simple test and it eliminates a number of cases where undersam- piing will cause errors.) 
Then a seed pixel is chosen immediately to the right of the first contour. It is possible to relax the 
conditions on the degrees and require that they be greater or equal to one, rather than equal to one. 
(This relaxed form was used in the implementation which produced the examples of the next section.) Unless 
the contour has points of multiplicity greater than twot, the interior is well defined on such a line 
and therefore a seed pixel is chosen correctly. (Under some conditions the algorithm will work correctly 
even if there exist pixels of degree three,) Filling can then proceed by connec- tivity in order to avoid 
scanning outside the contour area. Conversely, we may choose parity check as the basic algorithm and 
then revert to connectivity when we come across lines where the interior is not well defined on the basis 
of parity. Such an algorithm will not miss parts of the interior which are not connected to the region 
containing the seed. Another interesting variation of such an algorithm is offered for applications involving 
multiple displays of the contour such as charac- ter fonts. One could perform the filling once and then 
save along with the contour information about the pixels that were placed in the stack and were actually 
used. Then steps 10-12 and 14-17 could be omitted for subsequent fills. Also step 7 would be simplified 
because it would be used only to find the new Pnext. It will look on the line below for about m pixels. 
Thus the number of pixels visited will be slightly above the number of interior pixels. If the output 
device does the filling by displaying vectors parallel to one of the coordinate axes, then the contour 
must be sorted, although differently than in scan conversion. Therefore the new method offers no advantage 
for such devices. On the other hand, if the output device has an addressable display buffer, then connectivity 
filling offers many possiblities. In particular, one may use contour descriptions other than polygons, 
splines being the prime alternative. Then we need give only the curve coefficients and a set of the seeds, 
a description which is by far more compact than anyone used by scan conversion. 5. DISCUSSION AND EXAMPLES 
The algorithm descibed in "l~ables I and II was implemented in the C Language under the UNIX operating 
system. It was run on a PDP11/45 computer with a Ramtek 9300 display. Figure 7 iUustrates the order of 
filling a region. The algorithm can be easily adapted to fill with patterns, rather than uniformly, by 
selecting a fill color according to the address of the pixels. Two such examples are shown in Figures 
8 and 9. Figure 10 shows three examples where contours were found by edge detection. Even though most 
of the space in this paper has been devoted in the discussion of a pixel based connectivity algorithm, 
the main point that I tried to make is that by presenting the contour filling problem in the proper theoretical 
framework one can come up with a variety of new algorithms. Shani [8] already demonstrated that simple 
graph theoretic techniques can be used to correct heuristically .derived algorithms. The close connection 
beween the i-LAG and the c-LAG suggests that one can traverse one of them by examining the degree of 
the nodes of the other. For some applications where the contours are thin compared to the interior, this 
will result in an increase in speed. A major motivation for this work has been the desire to built a 
variety of algorithms from certain common blocks. With the exeption of the recursive algorithm, all other 
"connectivity filling" algorithms must check the degree of the nodes of a LAG. This is the most expensive 
operation because the filling of line segments is very simple. One would like to optimize this operation 
by implementing the "t The discussions of pixel multiplicity and a proof of this statement is beyond 
the scope of this paper. See [3] or [14] for a discussion of these points. In practice, points of multiplicity 
greater than two occur only be- cause of severe undersampling: Figure 2 would have contained pixels with 
three arcs in them, or two arcs, but one of them being an extremum. * UNIX is a trademark of Bell Laboratories 
  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806787</article_id>
		<sort_key>37</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Shading of regions on vector display devises]]></title>
		<page_from>37</page_from>
		<page_to>44</page_to>
		<doi_number>10.1145/800224.806787</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806787</url>
		<abstract>
			<par><![CDATA[<p>Given an arbitrary simple polygon with N vertices we present an algorithm for shading the interior of the polygon with a set of parallel lines where the slope and the distance between lines are prespecified. If the number of shading line segments is M, the algorithm described in the paper runs in 0(N log N + M) time. The algorithm is generalizable to shade any region or regions of an arbitrary planar subdivision.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Cartography]]></kw>
			<kw><![CDATA[Computational complexity]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Line-drawing processing]]></kw>
			<kw><![CDATA[Polygons]]></kw>
			<kw><![CDATA[Shading]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP77032334</person_id>
				<author_profile_id><![CDATA[81100397910]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[D.]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Electrical Engineering and Computer Science, Northwestern University, Evanston, Illinois]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807434</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Brassel, K. E. and Fegeas, R., "An Algorithm for Shading of Regions on Vector Display Devices," Computer Graphics, Vol. 13, No. 2, Aug. 1979, pp. 126-133.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lee, D. T. and Preparata, F. P., "Location of a Point in a Planar Subdivision and Its Applications," Siam J.Comput., Vol 6, No. 3, Sept. 1977, pp. 594-606.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Shamos, M. I., Computational Geometry, Springer-Verlag, Berlin/New York, 1977.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>521463</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Knuth, D. E., The Art of Computer Programming, Second Edition, Vol. 1, Addison-Wesley, Reading, Mass., 1973.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Newman, W. M. and Sproull, R. F., Principles of Interactive Computer Graphics, Second Edition, McGraw-Hill, 1979.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Watkins, G. S., "A Real-Time Visible Surface Algorithm", Univ. Utah Comput. Sci. Dept., UTEC-CSc-70-101, June 1970.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SHADING OF REGIONS ON VECTOR DISPLAY DEVISES D.T. Lee Department of Electrical Engineering and Computer 
Science Northwestern University Evanston, Illinois 60201 Abstract Given an arbitrary simple polygon 
with N vertices we present an algorithm for shading the interior of the polygon with a set of parallel 
lines where the slope and the distance between lines are prespecified. If the number of shading line 
segments is M, the algorithm described in the paper runs in 0(N log N + M) time. The algo- rithm is 
generalizable to shade any region or regions of an arbitrary planar subdivision. Key words and phrases: 
computer graphics, poly- gons, shading, cartography, line-drawing processing, computational complexity 
 CR categories: 3.14, 3.30, 5.25, 8.2 I. Introduction The shading of regions by parallel lines is a 
task common in various applications of com- puter graphics. The region to be shaded is usually represented 
as a straight-line polygon consisting of a sequence of outline points whose x- and y-coordinates are 
known. The task requires .~ also as input parameters the angle (or slope) of shading lines and the distance 
between two consecutive parallel lines. Brassel and Fegeas [II present an elegant algorithm which does 
the shading in a way similar to the filling of an arbitrarily shaped container with water, where the 
influx pipe is at the lowest point of the container. The water fills the volumn labelled I (Figure 1) 
and then flows over into area 2. Whenever the water level reaches an insinuation point, the filling process 
is artificially inter- rupted because the computer cannot do the filling of the two areas separated by 
the insinuation point simultaneously as in the physical process. Permission to copy without fee all 
or part of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. &#38;#169;1981 ACM O-8971-045-1/81-0800-0037 
$00.75 Therefore, the left branch is selected arbi- trarily to be blocked and the right branch is filled 
first. The mechanism used for the block- ing is via a stack by pushing the left branch into the stack. 
The blocking is removed when either the right branch is entirely filled or if the water overflows the 
top of the island (to be defined later) and this is achieved by the popping operation on the stack. Although 
the shading algorithm is simple to understand and its correctness is easy to verify because of its analogy 
to the process of filling a water tank, its efficiency can still be improved upon. In this paper we shall 
present an algorithm for shading an N-edge polygon in time O(N log N +M) where M is the number of shading 
lines and in space O(N). The algorithm to be described below is an adaptation of an algorithm developed 
by Lee and Preparata [2] for point-location problem in computational geometry [3]. The technique employed 
in [21 is very similar to what Brassel and Fegeas proposed and as we shall show later, the approach is 
easily generalizable to shade any region or regions of a planar subdivision. We shall mainly compare 
our algorithm with Brassel and Fegeas' and therefore, the readers are referred to the references in [11 
for other related work. Let us introduce the notation to be used throughout the paper. A simple polygon 
P, represented as an ordered list (v0,vl,....,VN_l) consists of a sequence of vertices v0,vl,...,VN_ 
I such that (vi,vi+l), (I) for i = 0,1, ...,N-I, is a straight-line segment, called an edge, and no 
two nonconsecutive edges intersect. Each vertex v i is an ordered pair <xi,Yi >, where x i = x(v i) 
and Yi = Y(Vi) are its x- and y-coordi- nates in the plane for i = 0,I,...,N-I. The polygon P will divide 
the plane into two regions, interior and exterior. For convenience we shall assume that the vertices 
of P are ordered in counterclockwise direction so that the interior of P always lies to the left when 
the edges of ( and subtraction taken ].l. index addition are modulo N. Computer Graphics Volume 15, 
Number 3 August 1981 P ~re traversed. If a region R whose outer boundary is represented as a simple polygon 
P0 has holes, called islands whose boundaries are also represented as simple polygons P1,P2...,P h 
then to distinguish the interior of R from the exterior we will order the boundary vertices of PO in 
counterclockwise direction and of PI,P2...,Ph in clockwise direction in order to be consistent with 
our convention. Figure 2 illustrates the interior of a region R which is a path-connected point set. 
(2) In the following we shall assume that the region R to be shaded is a path connected point set and 
its boundary is composed of simple polygons Po,PI,...,Ph, where P0 is the outer boundary polygon and 
PI,P2,...,P h are islands. In the following discussion we shall assume that the slope of the shading 
line is zero, i.e., all the shading lines are horizontal, since we can always rotate the figure accordingly 
if the slope is not zero so that the shading lines become horizontal with respect to the new coordinate 
system. Given a polygon P, there are in general, three types of vertices with respect to the y-axis, 
stalactitic, regular and stalagmitic. A vertex v of P : .(Vo,Vl...,VN_ I) is called stalactiti~ (3) 
if both y(v k 1 ) and Y(Vk+ 1) are greater than y(v k) and is called stalagmitic if both Y(Vk_l)and 
Y(Vk+ 1) are less than Y(Vk). These two kinds of vertices are said to be non- regular. A vertex which 
is neither stalactitic nor stalagmitic is regular. In Figure 3a vertex r is stalactitic, vertex s is 
regular and vertex t is stalagmitic. In Figure 3b, only the two vertices, ones with maximum and minimum 
y-coordi- nates are nonregular. Such a special polygon is called monotone and can be shaded in linear 
 time. 2. The Alsorithm Let us first discuss the problem of shading a simple polygon, then consider 
how to handle a region with islands. Before we describe the algorithm let us make the following crucial 
obser- vations. First, every horizontal line which intersects the interior of a polygon is divided by 
the boundary edges into segments such that the segments that are exterior and interior to the polygon 
are alternating. To be more precise, suppose that Ul,U2,...,u k are the intersection points of the line 
with the boundary edges of the polygon such that x(u I) < x(u 2) < ... < x(u k ) Note that we have excluded 
the intersection points of the line with nonregular vertices and that k must be an even nonnegative 
integer. Then the segments (X(Ul),X(U2)),(x(u3),x(u4)), .... (X(Uk_l) X(Uk)) are interior to the polygon 
and the remaining segments (x(u2),x(u3)) , (x(u4),x(u5)), etc. are exterior. Second, the edges inter- 
secting a horizontal line can be ordered by the x-coordinates of the intersection points. That is, (s,t) 
< (u,v) if the intersection of edge (s,t) with the horizontal line lies to the left of that of edge (u,v) 
with the horizontal line. A horizontal line passing through a vertex is called a critical line. All the 
regions bounded by the boundary edges and two consecutive critical lines are either triangles or trapezoids 
and the regions bounded by the boundary edges and the two critical lines passing through con- secutive 
nonregular vertices are all monotone polygons. (Figure 4). With these observations in mind we now pre- 
sent the algorithm. For illustration purpose let us assume that no two vertices have the same y-coordinates 
and every vertex has a shading line passing through it. As an initialization step, we sort the ver- 
tices of the polygon P according to their y-coor- dinates in time O(N log N) and examine each of the 
vertices of P to determine whether it is stalactitic, regular or stalagmitic. We then do a linear scan 
of the vertices from bottom to top and maintain a list SL of segments of critical lines that are interior 
to the polygon and a doubly-linked list DL of edges, both of them being empty initially. The edges in 
DL are ordered from left to right according to the intersection points of these edges with the critical 
line as described earlier. When each vertex v is scanned we perform the following operations depending 
on its type. Let (u,v) and (v,w) be the two edges incident with v. Assume that the list SL is (Ul,U2),(u3,u4),...,(U2m_l,U 
m) and DL is (Vil'Vi2)'(vi3'vi4)'''''(vi2m-l'vi2m) when v is scanned. (i) v is regular. Assume that 
y(w)< y(v) < y(u). Find the intersection points of the edges in DL with the critical line whose equation 
is y - y(v) = O. Let them be u',u',...,uA , one i 2 Lm (~ of which is v. Output these m trapezoids'-" 
(Ul,U2,Ul,U 2)  ' ' Replace , , ,.. ,(U2m_l,U2m,U2m_l,U2m). the edge (w,v) with (v,u) in DL. (ii) 
v is stalactitic. In this case both y(u) and y(w)are greater than y(v). Find the inter- section points 
of the edges in DL with the crit- ical line y - y(v) = O. Let them be Ul,U2,...' U'2m '  (]~2.A point 
set Z is path-connected if any two points p and q in Z can be joined by a path in Z. (3)It is called 
an insinuation point in Brassel and Fegeas' terminology. (4)  Each trapezoid is represented by two 
pairs of horizontal segments (s,t) and (u,v) with the understanding that (s,u) and (t,v) are portions 
of two boundary edges of P. A triangle is repre- sented as a trapezoid except that one of these two segments 
is a point, denoted by (v,v). While we are obtaining the intersection points, we also find the positions 
u¶ and u' such that J j+l X(US) < X(V) < X(U~+I). Let the new list (ul,u~, tl 11 ' ' ...,u~ ) be denoted 
by (Ul,U2,  "'''uj'v'v'uj+l' Lm ...u"2m+l). Output the m trapezoids (Ul,U2; ' ' ' ' and update the 
Ul,U2),'-',(U2m_l,U2m,U2m_l,U2m) list SL to be ()" I, ,(u" u 'I ) Insert -Ul'U2-'''" 2m+l' 2m+2 " the 
edges (u,v) and (w,v) to DL at appropriate positions as indicated by the new list SL. (iii) v is stalagmitic. 
In this case since both y(u) and y(w) are less than y(v), both edges are present in DL. Therefore, when 
we find the inter- section points of the edges in DL with the criti- cal line y -y(v) = 0 we will find 
two occurrences of v in the list (u~,u~ ..... U~m). Let the new list after removal of the two occurrences 
of v be (u~,u~ ..... U~(m_l)).. Output the m trapezoids (Ul,UR;U~,U ~) ..... (URm_l,Umm;U~m_l,U~m) and 
 update the list SL to be " " (Ul,U2),...,(u"2m-3' U~m_2). Delete the edges (u,v) and (w,v) from DL. 
 The algorithm terminates when the topmost vertex is scanned (or when the doubly linked list DL becomes 
empty). The running time of this algo- rithm is O(N log N) + 0(M) where M is the total number of shading 
lines. Note that if every ver- tex has a shading line passing through, the algo- rithm after the initial 
sorting step runs in time proportional to the number of shading lines. To illustrate this idea let us 
consider the example shown in Figure 5. After vertex 11 is scanned, the list of segments TL = ((Ul,U2) 
, (u3,u4),(u5,u6)) and the list of edges DL = ((2,1),(2,3),(9,8),(11,12),(13,12),(13,14)). The next 
vertex to be visited is vertex 5 and is stalactitic. After we have computed the intersection points of 
the edges in DL with the critical line through vertex 5 and inserted the two edges (5,6) and (5,4) to 
DL we obtain the new lists SL : ((u~,5),(5,u~), (u~,u~), (u~,u~)) and DL = ((2,1),(5,6),(5,4),(2,3), 
(9,8),(11,12), (13,12),(13,14)). Note that we have also output 3 trapezoids for shading, i.e., trapezoids 
defined by (Ul,U2;U~,U~) , (u3,u4;u~,u ~) and (u5,u6; u~,u~). When the next vertex, vertex I, is visited, 
we output 4 trapezoids as indicated by the list SL and DL remains the same as the pre- vious one with 
edge (2,1) replaced by (1,19). When vertex 12 is visited, we output also 4 trapezoids but SL becomes 
((u~,u~),(u~,u~),(u~, u~)) and DL becomes ((1,19),(5,6),(5,4),(2,3), (9,8),(13,14)). What happens 
if the distance D between two shading lines is such that every vertex does not necessarily have a shading 
line passing through? This usually arises when the shading lines are not horizontal or vertical and a 
rotational trans- formation is required which makes the vertices of the polygon to have nonintegral coordinates. 
 Algorithm SHADING Input: A polygon P, represented as a list L = (v0,vl,...,VN_ I) of vertices of P 
in coun- terclockwise direction whose coordinates are <x(vi),Y(Vi)>, i = 0,I,...,N-I.  Output: A list 
TL of trapezoids (or triangles) for shading. Method: Step I: Sort the vertices according to y-coordinates. 
Let the resulting list be (v (o),v~l),...,V~N_l)) such that y(v (i)) < y(v (i+l)), i = 0,I,...,N-2. Let 
PRED(v i) and SUCC (v i) denote the predecessor and successor of v. in z L respectively. Note that PRED(v 
O) is VN_ 1 and SUCC (VN_ I) is v 0. Step 2: TL = ~; DL = ~; SL = ~. Step 3: For i = 0 to N-I do Let 
SL = (Ul, u2,...,U2m) For each edge e ~ DL find the intersection of e and the line y = Y(V~(i ) ) Let 
the intersections be Ul,U2,' ' ...,U~m. TL = TLU {(Ul,U2;U~,U~),...,(U2m_l , URm;U~m_l,U~m )} Case 
v (i) of Regular: Let s = PRED(v (i)), t = SUCC(v~(i)) if y(s)> y(t) then DL = DL-{(t, v~(i))} U { (v~(i),s)} 
else DL = DL-{(s, v~(i))} U { (v~(i),t)} SL = (Ul'U2'''''uX Lm ) Stalactitic: Let ej and ej+ 1 be edges 
in DL such that the intersections of ej and y = y( v(i)) is uj ' and of el+ 1 and y = y(v (i)) is u]+ 
1 and x(u]) < x(v (i)) < x(u]+ I) SL : (Ul,U2,...,u j ' ' ',v,v,u~3 +± .,...,u~ DL = DLU {(v (i),PRED(v 
(i))) , (v (i),SUCC(v (i)))} by inserting the two edges incident with v (i) in order between edges e. 
and J ej+l in DL. Stalagmitic: Let u~3 and u'j+l be ~(i) ' ' ,., U ! 11' .. ! SL = (Ul,U2, , j_l,~j+2, 
.,U2m ) DL = {(PRED(v (i)),v (i)) , DL - (SUCC(v~(i)),v~(i))}"  In this case the shading can be done 
in 0(N log N) + O(M) time after the initial sorting step. Computer Graphics Volume 15, Number 3 August 
1981 The algorithm for this case is essentially the same as before except for the following modifi- cations. 
First, the doubly-linked list DL for the edges is replaced by a height-balanced threaded binary tree 
(e.g., a threaded AVL-tree [4]). Second, if the next vertex to be visited is within D from the last shading 
line, then without doing the shading we simply perform an update on the binary tree in the same way as 
on the doubly-linked list DL. If the next vertex is regular or stalagmitic (an endpoint of some edge 
currently present in the tree) the update is trivially done via a certain bookkeeping of which edge in 
the tree the next vertex belongs to; if the next vertex is stalactitic (an endpoint of two edges that 
are to be inserted into the tree) then a point-location process is initiated by finding out which segment 
of the current criti- cal line the new vertex belongs to via the binary tree and then the two edges incident 
with the vertex are inserted there into the tree. To support the point-location in logarithmic time the 
tree must be rebalanced, if necessary, after each update which takes at most O(log N) time. Thus, the 
total amount of work for such updates is at most 0(N log N). In the previous case, the time for point-location 
process is completely absorbed in the time for shading, i.e., the point is located while the shading 
task is perfomed, resulting in an O(N log N) time difference in these two cases. One final remark about 
the algorithm is worth mentioning here. If we consider the number of trapezoids that are output for shading 
as overhead of the algorithm (because it may stand for the number of subroutine calls if the shading 
of a trapezoid is implemented as such), then such overhead involved in the algorithm may be as much 
as 0(N2). To reduce the number of trapezoids produced we, instead of "cutting" the polygon with the critical 
lines, cut the polygon with critical segments. A segment of each critical line is said to be critical 
if it is interior to the polygon and contains the current vertex. Figure 6a shows the decomposition of 
a polygon using critical segment approach and Figure 6b is one using previous approach. It can be easily 
seen that each vertex generates at most two trapezoids using critical segment approach. In fact, only 
stalagmitic vertices each can possibly produce two trapezoids. This modification will result in an algorithm 
which takes O(N log N) time for point-location and tree rebalancing operations plus 0(M) time for shading. 
Since tree rebalancing can be time consuming, even though it takes 0(log N) worst-case time for each 
update, and the point-location operation is needed only for stalactitic vertices, it may be cost effective 
on the average to maintain a doubly- linked list for the edges instead of a binary tree and perform sequential 
searches for the point-location process, especially when the number of stalactitic vertices is small. 
 The approach of decomposing the polygon into trapezoids by critical segments is exactly what Brassel 
and Fegeas proposed except that the algo- rithm described here to implement the idea is a lot more efficient 
than Brassel-Fegeas algorithm. 3.Generalizations The previous section dealt with regions which are 
simple polygons. We shall describe how the method is generalized to handle regions with islands and more 
generally, any region or regions in a planar subdivision. For the case when the region to be shaded 
is represented by a list of simple polygons P0,PI,...,Ph where PO denotes the outer boundary polygon 
and PI,P2,...,Ph denote the islands, we essentially perform the same operations as in the simple polygon 
case. Note that the vertices of PO and PI,P2,...,Ph must be arranged in appro- priate order. We classify 
as before the vertices of Po,PI,...,Ph to be stalactitic, regular or stalagmitic. Once the classification 
is done, we perform a linear scan in a bottom-up fashion as before. As for the general case when we 
are given an arbitrary planar subdivision as shown, for example, in Figure 7, we do the following. For 
each vertex v we divide the edges incident with it into two categories, incoming and outgoing. The edge 
(u,v) is considered incomin~ if y(u) y(v) and outgoing otherwise. In addition, the incoming (outgoing) 
edges incident with v are ordered so that the endpoints of the edges occur clockwise (counterclockwise) 
around v respec- tively. Vertices with no incoming (out-going) edges are considered stalagmitic (stalac- 
titic); vertices with at least one incoming and one outgoing edge are considered regular. All these operations 
including the initial sorting can be done in O(N log N) time, where N is the number of vertices. We then 
perform similar operations as before except that we do shading for the specified regions only. 4. Concludin~ 
Remarks We have presented a shading algorithm for an N-edge simple polygon in time O(N log N) + O(M) 
where M is the total number of shading line segments used. The second term can be considered as output 
time that is required for any such algo- rithm. The algorithm is generalizable to shade any region or 
regions of a planar subdivision. Furthermore, the algorithm is applicable for shading on raster graphics 
[5]. Particularly, each shading line can be viewed as a scan line on raster scan devices. The algorithm 
for the case in which each vertex is on a shading line becomes very similar to Watkins' scan-line algo- 
rithm [6] and can be implemented to run in time proportional to the output after the initial sorting. 
Notice that on raster display devices since the coordinate values of the vertices of a polygon are all 
integers (e.g. 0 through 1023), we can perform a bucket (radix) sort for the vertices of a polygon and 
need only 0(N) time instead of 0(N log N) to do the initial sorting.  Acknowledgements The author wishes 
to thank one of the reviewers who brought Watkins' algorithm to his attention and he also thanks Mr. 
Robert Chen for implementing the algorithm described in the paper. The author's research is supported 
in part by the National Science Foundation under grant MCS-7916847. References I. Brassel, K. E. and 
Fegeas, R., "An Algorithm for Shading of Regions on Vector Display Devices," Computer Graphics, Vol. 
13, No. 2, Aug. 1979, pp. 126-133. 2. Lee, D. T. and Preparata, F. P., "Location of a Point in a Planar 
Subdivision and Its Applications," Siam J.Comput., Vol 6, No. 3, Sept. 1977, pp. 594-606.  3. Shamos, 
M. I., Computational Geometry, Springer-Verlag, Berlin/New York, 1977.  4. Knuth, D. E., The Art of 
Computer Prosram- mins, Second Edition, Vol. I, Addison- Wesley, Reading, Mass., 1973.  5. Newman, W. 
M. and Sproull, R. F., Principles of Interactive Computer .Graphics, Second Edition, McGraw-Hill, 1979. 
 6. Watkins, G. S., "A Real-Time Visible Surface Algorithm", Univ. Utah Comput. Sci. Dept., UTEC-CSc-70-101, 
June 1970.  19 21 14 15 ~ I 13 12 --L 8 7 4 5  Figure I. MIN Illustration of water-filling algorithm 
of Brassel and Fegeas. Figure (b) 3. Two simple polygons. )ezo£ds monotone polygons Figure 4. Trapezoids 
and monotone polygons between Figure 2. A path-connected region with islands. critical lines.   Computer 
Graphics Volume 15, Number 3 August 1981 7 15 ~ 19 tl 18 17 15 16 14 U~ -- (a) __"__~__~ ...... lu_~-f/_u~ 
h ~'~ iu~ .......... ~'~.7~ ~k-~---~ ~ ~ --- Figure 5. Illustration of the shading algorithm. Figure 
6. (b) Decomposition of a simple polygon using two different approaches. 43  NORTHEASTERN ILLINOIS 
 MCHENRY COUNTY KANE COUNIY KENDALL COUNTY LASflLLE COUNTY GRUNDY COUNTY LnKE !,.! COUNTY COOK COU~ 
 DUPAGE COUNTY F J I L  HI:LL COUNTY KANKAKEE COUNTY Figure 7. A planar subdivision --a map.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806788</article_id>
		<sort_key>45</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Frame-to-frame coherence and the hidden surface computation]]></title>
		<subtitle><![CDATA[Constraints for a convex world]]></subtitle>
		<page_from>45</page_from>
		<page_to>54</page_to>
		<doi_number>10.1145/800224.806788</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806788</url>
		<abstract>
			<par><![CDATA[<p>Frame-to-frame coherence is the highly structured relationship that exists between successive frames of certain animation sequences. From the point of view of the hidden surface computation, this implies that parts of the scene will become visible or invisible in a predictable fashion. In this paper the frame-to-frame coherence constraints are identified and characterized for static scenes restricted to stationary, closed, convex, nonintersecting polyhedra. The animation derives from a continuous movement of the viewer. The mathematical analysis of the constraints is geometric, and leads to a characterization of the self-occlusion relationship over a single polyhedron; and to a characterization of the occlusion or change of occlusion relationship over two polyhedra. Based on these constraints, an algorithm is presented which generates successive frames in an animation sequence.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer animation]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Frame-to-frame coherence]]></kw>
			<kw><![CDATA[Visible surface algorithms]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Hidden line/surface removal</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Object hierarchies</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010244</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Hierarchical representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011066.10011067</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Development frameworks and environments->Object oriented frameworks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P330705</person_id>
				<author_profile_id><![CDATA[81100355855]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Harold]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hubschman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Vision and Graphics Laboratory, Department of Electrical Engineering, McGill University, Montreal, Quebec, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40037121</person_id>
				<author_profile_id><![CDATA[81100615306]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Zucker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Vision and Graphics Laboratory, Department of Electrical Engineering, McGill University, Montreal, Quebec, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>360354</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Clark, J.H., "Hierarchical Geometric Models for Visible Surface Algorithms" CACM Vol. 19, No. 10, (October 1975), p. 547.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807481</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Fuchs, H., Kedem, Z., Naylor, B., "On Visible Surface Generation by A Priori Tree Structures", Computer Graphics, Vol. 14, No. 3, (July 1980), p. 124.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Jones, C.B., "A New Approach to the Hidden Line Problem" Computer Journal, Vol. 14, No. 3; (August 1971), p 232.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807479</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Rubin, S., and Whitted, T., "A Three-Dimensional Representation for Fast Rendering of Complex Scenes" Computer Graphics, Vol. 14, No. 3, (July 1980), p. 110.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>908431</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Shamos, M., "Computational Geometry", Ph.D. thesis, Computer Science, Yale University, (1978).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356626</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I.E., Sproull, R.F., and Shumacker, R.A., "A Characterization of Ten Hidden Surface Algorithms", Computing Surveys, Vol. 6, (1974), p. 1.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 Frame-To-Frame Coherence and the Hidden Surface Computation: 
 Constraints For a Convex World Harold Hubschman Steven W. Zucker Computer Vision and Graphics Laboratory 
Department of Electrical Engineering McGill University Montreal, Quebec, Canada ABSTRACT Frame-to-frame 
coherence is the highly structured relationship that exists between successive frames of certain animation 
sequences. From the point of view of the hidden surface computation, this implies that parts of the scene 
will become visible or invisible in a predictable fashion. In this paper the frame-to- frame coherence 
constraints are identified and characterized for static scenes restricted to stationary, closed, convex, 
nonintersecting polyhedra. The animation derives from a continuous movement of the viewer. The mathematical 
analysis of the constraints is geometric, and leads to a characterization of the self- occlusion relationship 
over a single polyhedron; and to a characterization of the occlusion or change of occlusion relationship 
over two polyhedra. Based on these constraints, an algorithm is presented which gen- erates successive 
frames in an animation sequence. Key words and Phrases: Computer Graphics, Computer Animation, Visible 
Surface Algorithms, Frame-to-Frame Coherence. "A picture is worth a thousand words - and it costs" 
 J.C.R. Licklider 1. Introduction An animation sequence is a series of images sampled from a continuously 
changing scene, such as the movement of a viewer through a three dim- ensional world. In order to create 
the percep- tion of continuity, a high temporal sampling rate (e.g., 24 frames per second) should be 
used; as a result, any image in the sequence is almost i- dentical to its predecessor. This continuity 
is refered to as frame-to-frame coherence; i.e., the highly structured relationship that exists between 
successive frames of certain animation sequences. Despite this coherence, however, the only approaches 
available for producing animation regenerate each image in the sequence directly from the underlying 
scene model, making no use of the similarity between successive images [i,2,3,~]. Thelmage Synthesis 
techniques used in these ao- proaches are quite sophisticated, and the resultant Permission to copy 
without fee all or part of this material is granted provided that the copies are not made or distributed 
for direct commercial advantage, the ACM copyright notice and the title of the publication and its date 
appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy otherwise, or to republish, requires a fee and/or specific permission. @1981 ACM O-8971-045-1/81-0800-0045 
$00.75 animation is extremely realistic. However, these techniques are computationally very expensive 
and the cost of generating, individually, the thousands of frames required for a few minutes of animation 
often prohibits their use. One way to reduce this cost, while still being able to use realistic image 
 synthesis techniques, is to make use of the frame- to-frame coherence during the animation process. 
Computational effort could be saved if it could be concentrated on those parts Qf the image that change, 
with little (or no) effort expended on those parts that do not. Such isthe goal of the approach that 
will be investigated in this pape~ -a particular method of incorporating frame-to-frame coherence into 
the design of hidden surface removal algorithms for computer animation. A prior knowledge in the form 
of image or ob- ject coherence has always been used to speed up the hidden surface computation in static 
graphics ~]. A number of these techniques have been extended to animation as well. This is done by precomputing, 
into the scene model, certain properties of the scene that are independent of viewing position; for example, 
the hierarchical modeling techniques of Clark [1], and of Rubin and Whitted [4], and the binary space 
partitioning scheme of Fuchs, Kedem and Naylor [2]. This information is then available at runtime, and 
need not be recomputed for each image. However since the precomputation oflthis viewpoint independent 
data is many times more expensive than Computer Graphics Volume 15, Number 3 August 1981 the computation 
involved in generating a single i- mage, such an approach is only cost-effective when many images (such 
as in an animation sequence) are to be produced from the model. In other words, af- ter an initial computational 
investment, the cost of generating an animation sequence can be reduced by regenerating each image more 
quickly. No use is made of frame-to-frame coherence, which, as we at- tempt to show, can lead to even 
greater computation- al savings. The type of animation that we consider is an (effectively) continuous 
movement of a viewer's pos- ition with respect to a given static scene. As far as the hidden~surface 
computation is concerned, frame-to-frame coherence implies that what is visi- ble in one frame will, 
for the most part, remain vi- sible in the next. And, more importantly, parts of ~e scene cannot become 
visible (or invisible) in an arbitrary fashion, but are constrained to becom- ing ~isible in a predictable 
fashion. The main purpose of this paper is an identifi- cation and a characterization of the frame-to-frame 
coherence constraints within a restricted universe, and their subsequent use in a hidden surface algor- 
ithm. The universe consists entirely of stationary, closed, convex, nonintersecting polyhedra made up 
of convex, opaque, polygonal surface elements. There will be no relative motion of polyhedra in the scene; 
that is, their relative positions will re- main fixed. The only motion will be due to move- ment of the 
viewer's position. This restricted universe permits a clear characterization of the frame-to-frame coherence 
constraints. The mathe- matical analysis is geometric, and leads to two types of constraint: (i) those 
pertaining to the self-occlusion or change of self-occlusion relation- ship over a single polyhedron; 
and (ii) those per- taining to the occlusion or change of occlusion re- lationship over two polyhedra. 
A particular par- tioning of the scene is developed, with the proper- ty that a movement of the viewing 
position across partition boundaries results in either (i) a change in self-occlusion of a single polyhedron; 
(ii) a change in the occlusion of one polyhedron with re- spect to another ; or (iii) both. An encoding 
of these change of occlusion constraints into the scene data structure is then described which makes 
these otherwise implicit partition corssings explicit. Using this, it is possible to directly determine, 
for an incremental change in the viewing position, which polygons have become visible or invisible. 
Based on these constraints, an algorithm is presented that generates a frame in an animation sequence. 
It starts from a description of what was visible in the previous frame, and incremental- ly updates it 
to contain only what is visible in the current frame. The updating algorithm uses a list of visible or 
partly visible polygons, and either deletes polygons that have become invisible, or inserts polygons 
that have become visible. Poly- gons that are neither appended to nor deleted from this list are never 
examined. In short, space, or the explicit representation of the constraint in- formation for a particular 
scene, is traded against the time that is required for more traditional hid- den surface algorithms. 
 2. Frame-To-Frame Coherence Any polygon in a scene of closed convex poly- hedra will either be visible 
or invisible from a particular viewing position, and the visible poly- gons will be either completely 
or partly visible. These states define what we shall refer to as the visibility status of a polygon. 
In the case of a partly visible polygon, the visibility status also includes a description of which 
parts are visible. (A partly visible polygon is described by the edges of its visible sub-polygons.) 
The hidden surface problem thus becomes one of determining: (i) the visibility status of polygons in 
the scene for a particular viewing position; and  (ii) the change in visibliity status of polygons from 
one frame to the next due to an increment- al change in the viewing position.  Consequently, the constraints 
that we wish to ident- ify are those that pertain to the visibility status, or change in visibility status, 
of polygons in the scene. 2.1. Static Constraints The first two constraints refer to static pro- perties 
of the scene, in the sense that they are relevant to the computation of the visibility stat- us of the 
polygons in the scene for a single frame. 2.1.1. Constraint i: Self Occlusion of a Single Polyhedron 
 Silhouette edges are defined as those edges on the polyhedron at which two polygons meet, provided one 
polygon is forward facing (towards the viewer) and the other is back facing (away from the viewer; i.e., 
made invisible by the polyhedron itself.) Constraint 1 is that the visibility status of poly- gons comprising 
closed convex polyhedra changes only across silhoue£te edges. In other words, the silhouette edges of 
such a polyhedron form a bound- ary on the polyhedral surface, dividing it into two connected sub-surfaces. 
In one sub-surface, all the surface elements are visible, while, in the other sub-surface, all the surface 
elements are invisible. This constraint provides the basis for our frame-to- frame coherence approach. 
We will show that the computation necessary to go from one frame's des- cription to the next will involve 
considering only the visible silhouette edges, net the entire scene data base. 2.1.2. Constraint 2: 
Occlusion of Two Polyhedra In the case of occlusion of two polyhedra, the visibility status of a polygon 
on the occluded poly- hedron changes: (i) across one of its visible silhouette edges; or  (ii) across 
the projection of a visible segment of the silhouette edge of the occluding poly- hedron onto the occluded 
polyhedral surface.  This is illustrated in Figure 2.1. (An edge is said to project onto a polygon 
(or polyhedron) when the projection of the polygon (or polyhedron) onto the viewing screen is intersected 
by the projection of the edge onto the viewing screen. The edge may either be occluded by the polyhedron 
or be in front of the polyhedron.) This leads to the second constraint: that the visible portion of 
the surface of a closed convex polyhedron is contained inside a closed boundary composed of:  Computer 
Graphics Volume 15, Number 3 August 1981 (i) its own visible silhouette edges; and  (ii) the projection 
of visible silhouette edges of the occluding polyhedra onto its surface.  The visibility status of 
polygons comprising poly- hedra can only change when we cross such a bound- ary. This type of boundary 
will be refered to as a visible-area boundary, because it defines an area of a polyhedral surface which 
is visible in a particular frame.  2.2. Dynamic Constraints The remaining constraints deal with the 
dynamic properties of the animation process. These describe the kinds of changes that can occur in the 
visibil- ity status of the polygons in the scene when going from one frame to the next. 2.2.1. Constraint 
3: Change in Polyhedron Self- occlusion From a viewer's frame of reference, his motion with respect 
to a polyhedron manifests itself as a rotation of the polyhedron. This rotation causes some of the polygonal 
surfaces to rotate into view and others to rotate out of view. The third con- straint is that those polygons 
whose visibility status changes in this way will all be contained inside a boundary composed entirely 
of old and new silhouette edges of the polyhedron. This is il- lustrated in Figure 2.2.  2.2.2. Constraint 
4: Incremental Change in Active Occlusion Relationships For configurations involving more t/tan one 
polyhe- dron the motion of the viewer causes him to see one polyhedron uncovering (or covering) polygons 
on an- other, in addition to the percieved rotation of the individual polyhedra. In particular, polygons 
be- come uncovered (or covered) as the occluding poly- hedron's silhouette sweeps over them. The fourth 
constraint is that those polygons on the occluded polyhedron that have undergone a change in visibil- 
ity status as a result of the motion of the viewer are contained inside a closed boundary on the oc- 
cluded polyhedral surface. This boundary is com- posed of (i) the projection of the occluding poly- hedron's 
silhouette onto the occluded polyhedron, as seen from the initial viewing position; (ii) the projection 
of the occluding polyhedron's sil- houette onto the occluded polyhedron, as seen from the final viewing 
position; and (iii) the sil- houette edges of the occluded polyhedron that have become uncovered (or 
covered) as a result of the viewers motion. An example is shown in Figure 2.3. Such a boundary, as well 
as the boundary for con- straint 3, will be refered to as a "change of vis- ibility status boundary", 
or as a "change boundary". In short, the projection of the occluding poly- hedron's silhouette onto 
the surface of the occluded polyhedron is useful information. It will be re- fered to as the Occlusion 
relationship between the two polyhedra. An occlusion relationship not only indicates that one polyhedron 
occludes the other, but gives an exact description of how the viewer sees the occlusion. An active occlusion 
relation- ship refers to a situation in which one polyhedron partially obscures another, but in which 
both are visible. (This is to be contrasted with one poly- hedron being completely hidden behind another.) 
Constraints 4 and 5 characterize the possible changes in active occlusion relationships from one frame 
to the next. 2.2.3. Constraint 5: Creation of Active Occlusion Relationships In the previous constraint, 
the visibility status of polygons changed as a result of an in- cremental change in the active occlusion 
relation- ship between two polyhedra. Another, more extreme possibility is that the change in viewing 
position causes an occlusion relationship to become active from a configuration in which there was none, 
or vice versa. This corresponds to the case of the silhouette of one polyhedron projecting onto an- other 
polyhedron in one frame, while in the prev- ious frame it did not, or vice versa. There exists a partitioning 
of the scene with the property that a movement of the viewing posi- tion across a partition boundary 
results in an oc- clusion relationship becoming active or inactive. Consider Figure 2.4. This is a cross 
sectional view of two polyhedra. AS long as the viewing po- sition is in r~gion A, polyhedron 2 is completely 
invisible. If the viewing position crosses line L1 into region B, then polyhedron 2 becomes ~isi- ble, 
and the~occlusion relationship between poly- hedron 1 and 2 becomes active. Altogether there are four 
such possibilities, and these define our fifth constraint: (i) Crossing line L1 from region A into region 
B causes initially invisible polyhedron 2 to become partly visible, and causes the occlus- ion relationship 
between the two polyhedra to become active.  (ii) The sylr~etric case of moving from region B into A 
causes polyhedron 2 to disappear, and caused the previously active occlusion rela- tionship to become 
inactive. An example of these two possibilities is shown in Figure  2.5. (iii) Crossing line L2 from 
region C into B causes polyhedron 1 to begin occluding polyhedron 2. The occlusion relationship between 
the two polyhedra becomes active. (iv) The reverse case of moving from region B into C causes polyhedron 
2 to become completely unoccluded and visible. The occlusion rela- tionship between the two polyhedra 
becomes inactive. An example of these possibilities is shown in Figure 2.6. The importance of such a 
partitioning is that it indicates when a silhouette edgebegins to pro- ject onto a particular polyhedron, 
and when it no longer projects onto that polyhedron. For example, in case 1 of Figure 2.5, the movement 
of the view- ing position causes edge XY to begin projecting on- to polyhedron 2. The reverse movement 
causes edge XY to stop projecting onto polyhedron 2. The same holds for edge UV in Figure 2.6. 2.__~3Representation 
of Occlusion Relationships  In order to find those polygons whose visibil- ity status has changed, it 
is necessary to con~ struct the change of visibility status boundaries on each of the visible or previously 
visible poly- hedral surfaces. This requires that we compute the projection of each visible silhouette 
edge onto the polyhedron which it occludes. For an incremental Computer Graphics Volume 15, Number 3 
August 1981 change in an active occlusion relationship, the current projection of a silhouette edge 
onto a poly- hedron can be computed as an incremental change from the previous projection of that edge 
onto the polyhedron. In the case of an occlusion relation- ship only just becoming active, there is 
no such reference for the projection computation. However, an occlusion relationship becomes active or 
inact- ive when the viewing position crosses one of the scene partition boundaries. A method will now 
be presented for determining (i) when the viewing pos- ition crosses one of the scene partition boundaries; 
and (ii) for which silhouette-edge/polyhedron pair the occlusion relationship is becoming active or in- 
active. This'method involves encoding the change of occlusion constraints into the scene data struc- 
ture. To describe the encoding scheme, it is first necessary to define two geometric constructs: the 
viewing ray plane of a silhouette edge, and the sup- port planes of a convex polyhedron.  2.3.1. Viewin 
9 Ray Plane of a Silhouette Edge The viewing ray plane of a silhouette edge is the plane defined by 
the silhouette edge and the viewing position. The importance of this construct is that the perspective 
projection of a silhouette edge onto a polyhedron is the contour on the poly- hedral surface defined 
by the intersection of the viewing ray plane of the silhouette edge with the polyhedron. This is illustrated 
in Figure 2.7. The term "viewing ray plane" comes from the fact that the viewing rays which trace out 
the perspect- ive projection of the silhouette edge onto the polyhedral surface must lie in this plane. 
 An important property of the viewing ray plane is its parameterization. A line defines a family of planes 
which can be generated by rotating a plane through 360 degrees about an axis lying a- long the line. 
Using some fixed plane in this fam- ily as a reference, any plane in the family can be parameterized 
by its angle of rotation from the ref- erence plan. Since the silhouette edges are fixed to their respective 
polygons, a natural choice of reference plane for the viewing ray plane is the plane of one of the two 
polygons containing the sil- houette edge.  2.3.3. support Planes Given a convex polyhedron and a line 
not inter- secting the convex polyhedron, the support planes of the polyhedron are defined as the two 
planes, from the family of planes defined by the given line, which are tangent to the polyhedron. By 
tangent we mean that the plane can touch the polyhedron at a point, a line or at most a plane, but not 
intersect the polyhedron. This is shown in Figure 2.8, which is a cross sectional view. P is the line 
defining the support planes, shown here perpendicular to the page. L1 and L2 are the support planes of 
the poly- hedron. (This concept was motivated by a similar concept in the ~ield of Computational C~omet~_r, 
the supporting line of a convex hull [5].)  2.3.3. Change of Occlusion Description The partition boundaries 
across which the view- ing position must pass in order for the occlusion relationship between a silhouette 
edge and a poly- hedron to become active (or inactive) are precisely the support planes of the polyhedron 
that are de- fined by the silhouette edge. It is thus possible to encode the partition boundaries directly 
into the description of the silhouette edge. A support plane is represented by its angle parameter, de- 
fined relative to the same reference plane used to compute the angle of the viewing-ray-plane of the 
silhouette edge. In order to determine that the occlusion relationship has become active or inact- ive, 
it is only necessary to compare the angle of the viewing ray plane of the silhouette edge for the previous 
viewing position against the angle for the current viewing position. If a support plane angle lies between 
the previous and the current viewing-ray-plane angles, then the viewing position has crossed the partition 
boundary and the occlu- sion relationship has changed. This is illustrated in Figu/e 2.8. 3. The Hidden 
Surface Updating Algorithm   3.1. Introduction The central idea of frame-to-frame coherence is that 
by keeping a description of what is visible in the current frame, the next frame in the anima- tion sequence 
can be generated by incrementally up- dating this description. We will now indicate one description of 
the visible structure of a frame that is sufficient for characterizing certain frame-to-frame coherence 
constraints, as will be demonstrated by an algorithm for doing the hidden surface updating computation. 
 Given a particular viewing position, the poly- gons in the scene can be classified as invisible, partly 
visible or completely visible. The visible structure of a frame is described in terms of a list of visible 
or partly visible polygons. If a polygon is completely visible, then its own edges are sufficient to 
describe it. For a partly visi- ble polygon, the visible parts are themselves bounded by polygons, in 
general non convex. It is sufficient to describe a partly visible polygon by specifying the edges of 
its visible sub-polygons. These edges correspond either to segments of the polygons own edges, or to 
the intersection of the polyhedron by edges of a visible area boundary. This is illustrated in Figures 
3.1 and 3.2. An i- mage can then be produced by scan converting the visible polygons or sub-polygons 
in the list. The updating algorithm must do two things. Given, at the start of the frame computation, 
a list of the visible or partly visible polygons of the previous frame, it is necessary to: (a) find 
those polygons in the list which have be- come invisible and remove them, or those poly- gons not in 
the list which have become visible (or partly visible) and insert them; and (b) compute, for the partly 
visible polygons, the visible sub-polygon edges. We have seen that the area of a polyhedral sur- face 
which has just become visible or invisible is contained inside a closed boundary on the polyhed- ral 
surface, refered to as a change boundary. The change boundary is composed of segments of the poly- hedron's 
own silhouette edges, from both the cur- rent and the previous frames; and of segments of the occluding 
silhouette edges that project onto the surface in the current and previous frames. The polygons comprising 
this area are the ones that have undergone a change in visiblility status  Computer Graphics Volume 
15, Number 3 August 1981 (constraints 3 and 4). It is precisely these poly- gons that we wish to identify 
in step (a) above. There are three stages to the hidden surface updating algorithm: (i) Construct the 
visible area boundaries on all the visible polyhedra in the scene (con- straints 1 and 2).  (ii) Using 
the above visible area boundaries, com- pute the visible sub-polygon edges of the partly visible polygons. 
  (iii) Using the visible area boundaries from both the current and the previous frames, identi- fy 
the polygons contained within the change boundaries (constraints 3 and 4) on all of the currently and 
previously visible polyhed- ra in the scene. These polygons are then in- serted into, or removed from, 
the visible pol- ygon list, depending on the change in visi- bility status that the particular polygon 
has undergone.  3.2. Polyhedron Representation Each polyhedron in the scene is represented by a graph, 
which will be refered to as the polyhedron graph. A node in the polyhedron graph corresponds to one of 
the geometric primatives that make up the polyhedron. There are three types of nodes in a polyhedron 
graph, one for each type of geometric primative (vertex, edge, or polygon) in the poly- hedron. Arcs 
between nodes in this graph indicate structural relationships between the geometric primatives represented 
by the nodes. An edge points to its two vertices, and to the two poly- gons which define it. A vertex 
points to each of the edges emanating from it. A polygon points to each of the edges that make up its 
boundary. 3.3. Initializing the Scene Partitions We have seen, in section 2.3.3, that the part- ition 
boundaries across which the viewing position must pass in order for the occlusion relationship between 
a silhouette edge and a polyhedron to be- come active are the support planes of the poly- hedron that 
are defined by the silhouette edge. It is important to note that an active occlusion relationship between 
a silhouette edge and a poly- hedron is symmetric; it refers to the occlusion of the edge by the polyhedron, 
as well as the op- posite. The initialization of the scene partitions involves computing the support 
planes of each edge in the scene with respect to each polyhedron in the scene. However, only the support 
which do not intersect the polyhedron containing the edge are stored. The following information is comPuted 
for each support plane that will be stored: (i) The angle of the support plane relative to the reference 
plane used to compute the an- gles of the viewing ray plane of the sil- houette edge CSection 2.3.1). 
 (ii) A pointer to the vertex of the supported pol- yhedron where the support plane is tangent.  (iii) 
A flag indicating whether the supported poly- hedron is occluding the edge or is occluded by the edge, 
provided that the polygon rela- tive to which the support plane angle has been computed is forward facing. 
 The support planes defined by a particular edge are stored in a list associated with that edge. The 
list is ordered according to increasing angle va- lue (field (i) ). By encoding the partition bound- 
aries directly into the edge description, there is enough information associated with each silhouette 
edge to compute which parts of the silhouette edge are visible, and which polyhedron is occluded by each 
visible segment, without an exhaustive compar- ison against the entire scene database. In other words, 
the constraint information (i.e., the part- itions) is distributed directly to those parts of the scene 
model (i.e., the polyhedron edges) where it is used. 3.4. Representation and Updatin 9 of an Occlusion 
Relationship An occlusion relationship between a silhouette edge and a polyhedron describes the projection 
of the silhouette edge onto the polyhedral surface. Recall that the perspective projection of a sil- 
houette edge onto a polyhedron is the contour on the polyhedral surface defined by the intersection of 
the viewing ray plane of the silhouette edge with the polyhedron (Section 2.3.1). This is de- scribed 
by the two silhouette edges of the polyhe- dron that are intersected by the viewing ray plane of the 
projected silhouette edge, and by the points along the infinite line, (defined as the extension of the 
projected silhouette edge), that project on- to the intersected silhouette of the polyhedron. This is 
illustrated in Figure 3.3 The projection of edge P0-PI is described by the intersection of silhouette 
edges XY and UV, and by points TO and TI, which project onto XY and UV respectively. The Utility of this 
description is that it indicates precisely which part of the silhouette edge pro- jects onto the polyhedron 
(for example, segment T0-PI in Figure 3.3). In the event that the sil- houette edge is occluded by the 
polyhedron, this allows the visible segments of the edge to be in- expensively computed. One of the 
reasons that the two silhouette edges intersected by the viewing ray plane are kept as part of the description 
of the occlusion relationship is to facilitate the updating of the occlusion relationship from one frame 
to the next (constraint 4). Because of the incremental change in viewing position, the new viewing ray 
plane will usually intersect the same silhouette edges as the previous viewing ray plane. If it does 
not, the intersected silhouette edge will be close to the previously intersected silhouette edge. By 
using the previously intersected silhouette edge as a starting node, and examining the edges in the polyhedron 
graph in the visinity of this node, and examining the edges in the polyhedron gral~h in the vicinity 
of this node, the currently intersected silhouette edge can be found directly, without ex- amining all 
of the edges of the polyhedron. In similar fashion, the vertex where a computed sup- port plane is tangent 
(fiels (ii), Section 3.3) is used as a starting node in the initialization of the occlusion relationship 
that becomes active when the viewing position crosses that support plane.  3.5. Input to the Algorithm 
 The inputs to the algorithm, prior to  Computer Graphics Volume 15, Number 3 August 1981 processing 
a frame, are: (i) The list of visible polygons in the previous frame.  (ii) A list of all the silhouette 
edges visible in the previous frame. Associated with each sil- houette edge is a list of the edge's active 
occlusion relationships of the previous frame.  (iii) The visible area boundaries of the previous frame. 
 3.6. The Hidden Surface Updating Algorithm Stage One: Computing the visible area boundar- ies. The 
visible area boundary on a particular pol- yhedral surface is composed of visible segments of the polyhedron's 
own silhouette edges; and of the projection onto the polyhedral surface of visible silhouette edges of 
occluding polyhedra (constraints 1 and 2). Therefore it is only necessary to consi- der silhouette edges 
visible in the current frame. Most of these will already be in the visible sil- houette edge list of 
the previous frame. Those sil- houette edges not yet in the list can be located directly, by making use 
of the fact that the edges of the visible area boundary are connected. In this way it will not be necessary 
to exhaustively exa- mine every edge in the scene database; rather the algorithm will start the updating 
of frame by con- sidering only those edges in the visible silhouette edge list of the previous frame. 
Whenever a new visible silhouette edge is located, it will be ap- pended to the silhouette edge list. 
 The following five steps are executed for each list, including those edges which are inserted into the 
list during the current frame computation: (i) Check if the edge is still a silhouette edge, that is 
, check to see that, of the two polygons that define the edge, one is forward facing and the other is 
backfacing. If the edge is no long- er a silhouette edge, remove it from the list and go on to the next 
edge.  (ii) Compute the new viewing ray plane for the silhouette edge. Find those support planes, in 
the edge's list of stored support planes, that lie bet- ween the current and the previous viewing ray 
planes. These correspond to occlusion relation- ships that have just become active or inactive. Initiallize 
the newly active occlusion relation- ships (Section 3.4). Remove those that have be- come inactive from 
the silhouette edge's list of occlusion relationships.  (iii) Update the occlusion relationships that 
have remained active since the previous frame. For ex- ample, in Figure 3.1, edge CE has two active oc- 
clusion relationships, one with polyhedron 1 (seg- ment CD projects onto polyhedron i) and one with polyhedron 
3 (segment CB projects onto polyhedron 3). (iv) Determine which segments of the silhouette edge are 
visible and which are invisible. The in- visible segments of the silhouette edge are those which project 
onto polyhedra that occlude the sil- houette edge. If the silhouette edge is completely invisible, remove 
it from the silhouette edge list. Determine, for each visible segment, which polyhe- dron the segment 
projects onto. If a visible seg- ment projects onto more than one polyhedron, break the segment up into 
smaller segments such that each smaller segment projects onto only one polyhedron. In the case where 
this is not possible, because the projections onto two polyhedra overlap, the polyhe- dron closest to 
the edge is chosen. For example, silhouette edge CE in Figure 3.1 is divided into two segments. One is 
segment BD which projects on- to polyhedron i, and the other is segment BC which projects onto both polyhedra 
1 and 3. Polyhedron 3 is chosen for segment BC because it is the closer one. For a particular polyhedron, 
the visible seg- ments of its own silhouette edges and the visible silhouette edge segments which have 
been chosen to project onto it constitute the edges of the polyhe- drons visible area boundary. For polyhedron 
i, the boundary segments are AH, AB, BD which project onto it, and GH, GF, FD from its own silhouette 
edges. (v) Link up the individual boundary segments, com- puted in step (iv) above, into their respective 
vi- sible area boundaries. A visible area boundary edge is a segment of a silhouette edge. The end point 
of a segment occurs for one of two reasons:  (i) The silhouette edge end point is visible. For example, 
in Figure 3.4, point H is the end point of segment GH.  (ii) Two silhouette edges intersect. For example, 
end point B of segment BC results from the intersection of silhouette edge KC with sil- houette edge 
AH.   In each of these two cases, we have a pointer to the silhouette edge containing the neighbouring 
 boundary segment. In case (i), the neighbouring boundary segment to GH, connected at point H, is on 
 silhouette edge HI. We can find this silhouette edge by scanning the list of edges emanating from 
vertex H (Section 4.3.1.3). In case (ii), the neighbouring boundary segment to BC, at point B, is segment 
AB on silhouette edge AH. Silhouette edge AH is one of the two silhouette edges of polyhedron 3 which 
is intersected by the viewing ray plane of silhouette edge KC. (The other silhouette edge is AI; the 
intersection point is L). Pointers to these two silhouette edges were computed in step (iii). The 
above method of linking boundary segments also solves the problem of finding new silhouette edge list, 
a flag is set on the edge's descriptor, indicating that the edge is a silhouette edge. In the above linking 
process, when the silhouette edge that contains the neighbouring boundary segment is found, a check is 
first made to determine if it is already in the silhouette edge list. If it is not, it is appended to 
the list. In this way, all of the new silhouette edges of a parhicular boundary can be generated, starting 
from only one old sil- houette edge. Stage two: Compute the edges of the visible sub-polygons of each 
partly visible polygon. The partly visible polygons are precisely those poly- gons across which a boundary 
edge lies (Figure 3.2, boundary edge BD). This type of boundary edge re- sults from the projection o~ 
an occludin~ silhouette edge segment Onto the polyhedral surface containing the partly visible polygons 
(constraint 2). For each occluding silhouette edge segment, that poly- hedron which the segment occludes 
was determined in step (iv) above. Using this, the subsegments which intersect individual polygons can 
be computed.  Computer Graphics Volume 15, Number 3 August 1981 For example, in Figure 3.2, silhouette 
edge segment BD occludes polyhedron i, and intersects polygons 1 and 2. The subsegment which intersects 
polygon 1 is DX, and the subsegment which intersects polygons 2 is XB. The projection of such subsegments 
onto the polygon which they occlude, together with the visible segments of the polygon's own edges, form 
the edges of the visible sub-polygons of that poly- gon. The visible area boundaries were linked to- 
gether in order to speed up this computation. Us- ing this linked boundary, together with the struc- 
tural information in the polyhedron graph, the polygons on the polyhedral surface intersected by a particular 
visible area boundary segment can be directly computed. Consider the visible area bound- ary on polyhedron 
3, in Figure 3.5. Boundary seg- ment BC is part of silhouette edge BN. In Stage One, the intersection 
of silhouette edge KL by the viewing ray plane of edge BN was computed. Using KL as a starting point, 
it is found that polygon P1 is intersected by the viewing ray plane of edge BN, as is edge OQ. From the 
structure of the polyhedron graph, it is known that the polygon sharing edge OQ is polygon P2. This is 
continued until the polygons intersected by segment BC are encountered. Once this occurs, the visible 
area boundary is traversed, and for each boundary segment, the intersections of the segment with the 
polygons that the segment crosses is computed and stored on the polygon node. Stage Three: Updating 
the visible polygon list. At this point in the algorithm, enough in- formation has been computed to scan 
convert the vi- sible and the partly visible polygons. The final stage of the algorithm involves identifying 
the polygons contained inside the change boundaries on the currently and previously visible polyhedra, 
and either appending them to, or removing them from, the visible polygon list, depending on the change 
in visibility status that the particular polygon has undergone. The first case to consider is that of 
a change boundary resulting from the incremental change in the active occlusion of two polyhedra. Consider 
Figure 3.6, which represents two consecutive frames in an animation sequence. Edge A'B' is the projec- 
tion of edge AB for the viewing position of the previous frame. The motion of the viewer, shown by the 
arrow, has caused the polygons inside the solid boundary (the change boundary) to become vi- sible. In 
Stage Two, the algorithm determined that boundary .segment VY intersected edge PQ on polyhedron 2. Consequently, 
one of the two ver- tices of edge PQ must lie inside the change bound- ary on polyhedron 2. By comparing 
these two ver- tices against edges AB and A'B', vertex P is found to be the one inside the change boundary, 
since it is the one that lies between the two edges. Since vertex P is "above" the viewing ray plane 
of sil- houette edge AB, it is currently visible, and has therefore undergone a transition from invisible 
to visible. ("AbOve" is defined as in the direction of the normal to the viewing ray plane. The normal 
is chosen such that when the viewing ray plane is parallel to the polygon used as the reference, their 
normals are parallel. The normal to the ref- erence polygon points out of the polyhedron.) Con- sequently 
the contents of this change boundary are all visible polygons, and must be inserted into the visible 
polygon list, (if they are not already there). The polygons contained inside the change boundary are 
identified by traversing the subgraph contained inside the change boundary, starting from vertex P. The 
traversal of the subgraph terminates when either (i) a polyhedron edge is encountered that has been intersected 
by a visible area bound- ary segment in the current or previous frame (this information is encoded into 
the edge nodes of the polyhedron), or (ii) a silhouette edge of the cur- rent or previous frame is encountered. 
 The other case to consider is that of the change boundary resulting from the change in poly- hedron 
self occlusion. The same method that is described above can be used to traverse the sub- graph contained 
inside the change boundary. How- ever in this case the initial vertex is chosen in a different fashion. 
Starting from a new silhouette edge the polygon defined by this edge that has und- ergone a change in 
visibility status as a result of the viewers motion is determined. Using one of the non silhouette edge 
vertices of this polygon as a starting point, the sub-graph defined by the change boundary is traversed, 
and the polygons contained therein are inserted into the visible polygon list. These two methods are 
used to identify the contents of all of the change boundaries as well as to det- ermine the change in 
visibility status undergone by the polygons contained therein. After this has been completed for all 
of the change boundaries, the contents of the visible pol- ygon list can be scan converted to produce 
the fin- al image. 3.__~7. Computin~he First Frame of The Animation Sequence At the beginning of the 
computation of the first frame of the animation sequence, both the vi- sible silhouette edge list and 
the visible polygon list are empty. The initialization of the first frame involves examining every edge 
in the scene data structure, and inserting every silhouette edge into the silhouette edge list. Once 
this has been done, the updating algorithm can be used to compute the first frame. In Stage One, the 
silhouette edges that are invisible will be identified and removed from the list. From the visible silhouette 
edges, the visible area boundaries are then constructed. In Stage Three, all of the polygons visible 
in the first frame can be identified, by traversing the sub-graph contained inside the visible area bound- 
 ary. 4. Extension to More Complex Scene Domains m An obvious question still to be answered is how 
to make use of the current frame-to-frame co- herence techniques for objects which are more com- plicated 
than closed convex polyhedral surfaces. In order of increasing structural complexity there are: (i) 
Convex polyhedral surfaces with holes in them.  (ii) Closed non-convex polyhedral surfaces (that is, 
surfaces that have no holes in them).  (iii)Non-convex polyhedral surfaces with holes in them. In 
the above cases, the visibility status of polygons can change not only across the projection of silhouette 
edges, but also across the projection  Computer Graphics Volume 15, Number 3 August 1981 of edges which 
lie along the rim of holes in the surfaces. Also, the problem of self occlusion for a non-convex polyhedral 
surface is considerably more complicated. Due to the concavity of the pol- yhedral surfaces, silhouette 
edges can project onto their own polyhedra; and determining when sil- houette edges become visible or 
invisible is not as simple as in the convex polyhedron case. It is ne- cessary to identify the constraints 
which pertain to these cases. Another question is how to deal with higher order curved surfaces. While 
the same types of constraints as are defined in Chapter 3 hold for entities such as spheres or toruses 
(since the con- straints stem in part from the properties of the perspective projection, which remain 
the same for all types of surfaces), these constraints cannot be encoded into the scene database in the 
same manner. For polyhedra, the constraints are encoded into the description of each edge, and become 
active when the edge becomes a silhouette edge. For curved sur- faces, a silhouette edge is not a static 
property of the surface element. Rather, it is a contour on the surface which is determined after the 
view- ing position is specified. It is therefore neces- sary to find another way of encoding the constraint 
information into the description of the surface ele- ments. Finally, there is the question of how to 
hand- le a non-stationary scene (a scene in which objects are allowed to move). The same types of constraints 
as for the stationary case hold for the non station- ary case. However, the constraint information is 
no longer static, since the relative positions of the polyhedra are allowed to change. One possibil- 
ity is to make the region partitions (Section 3.2.2.3) dynamic, and update them for the parts of the 
scene which move. The continuity in time und- erlying the animation process requires that the change 
in the scene partition structure from one frame to the next be very small. Consequently, the computation 
involved in updating the partition structure might be sufficiently inexpensive to make this approach 
feasable for non-stationary scenes. 5. Summary We have shown how frame-to-frame coherence can be used 
to solve a hidden surface problem for three- dimensional computer animation. The notion of "visibility 
status" of a surface element has been introduced, and the hidden surface problem has been approached 
from the point of view of determining how the visibility status of surface elements can change from one 
frame to the next. The constraints which govern the change in visibility status have been identified 
and characterized within a restrict- ed universe. An algorithm for generating an anima- tion sequence 
has been presented, which uses the constraints to focus the computation on only those parts of the scene 
where there is a change. If the constraints governing changes in visibility status can be deterrmined, 
and these constraints can be in- corporated into a hidden surface algorithm, then frame-to-frame coherence 
could be used for more gen- eral scene domains as well. Acknowledjment: We would like to thank John 
L. Mohammed for his many helpful discussions and sug- gestions; and Cem Jak Eskenazi for his assistance 
 in drawing the diagrams. References i. Clark, J.H., "Hierarchical Geometric Models for Visible Surface 
Algorithms" CACM Vol. 19, No. 10, (October 1975), p. 547. 2. Fuchs, H., Kedem, Z., Naylor, B., "On Visible 
Surface Generation by A Priori Tree Struct- ures", Computer Graphics, Vol. 14, No. 3,  (July 1980), 
p. 124.  3. Jones, C.B., "A New Approach to the Hidden Line  Problem" Computer Journal, Vol. 14, No. 
3; (August 1971), p232.  4. Rubin, S., and Whitted, T., "A Three-Dimensional Representation for Fast 
Rendering of Complex Scenes" Computer Graphics, Vol. 14, No. 3,  (July 1980), p. ii0.  5. Shamos, M., 
"Computational Geometry", Ph.D.  thesis, Computer Science, Yale University, (1978).  6. Sutherland, 
I.E., Sproull, R.F., and Shumacker, R.A., "A Characterization of Ten Hidden Surface Algorithms", Computing 
Surveys, Vol. 6, (1974), p. i.  Figure 2.1 Occlusion of two polyhedra. The heavily drawn contour represents 
the visible area boundary of polyhedron 2. Figure 2.2 Change in polyhedron self occlusion. The heavily 
dashed line and the heavy solid line represent part of the polyhedron silhouette as seen from the initial 
and final viewing posi ~ tions respectively.  Computer Graphics Volume 15, Number 3 August 1981 1 2 
L4 L 3 L 2  Figure 2.3 Incremental change in an active occlusion relationship. The heavily dashed 
line and the heavy solid line represent the Figure 2.4 Partition boundaries for two silhouette of polyhedron 
1 from the initial polyhedra. and final viewing positions respectively. 0  Frame ;~ Frame b Frame a 
Frame b 2 2 1 y 2 1 Figure 2.5 Creation of an active occlusion Figure 2.6 Creation of an active occlusion 
 relationship. relationship. Figure 2.7 The viewing ray plane of edge P, for viewing position A. The 
projection of edge P onto the polyhedro n is heavily out- lined.  Figure 2.8 The support planes of polyhedron 
i, defined by silhouette edge P. tl and t2 are the angles of L1 and L2, relative to polygon SI. Computer 
Graphics Volume 15, Number 3 August 1981 F ~ DPolygon 1 F 1 Figure 3.1 The visible area boundary of 
poly- hedron i. Figure 3.3 Description of an active occlusion relationship. I ? (~ F M j K Figure 
3.5 Computing the edges of the visible sub-polygons of partly visible polygons.  Polyhedron~ ~'-"''"'~ 
 "~'", X ...... ~v ~x ,,i"y Polygon 2  Figure 3.2 Partly visible polygons are those across which a 
visible area boundary segment lies. 4 J H £ F ~"\ % Figure 3.4 Linking the visible area boundary 
segments together. B" B Figure 3.6 Identifying the polygons contained inside the change boundary of 
polyhedron 2.  54    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806789</article_id>
		<sort_key>55</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Parallel processing image synthesis and anti-aliasing]]></title>
		<page_from>55</page_from>
		<page_to>62</page_to>
		<doi_number>10.1145/800224.806789</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806789</url>
		<abstract>
			<par><![CDATA[<p>The continuing evolution of microelectronics provides the tools for developing new methods of synthesizing digital images by utilizing parallel processing architectures which hold the promise of reliability, flexibility and low cost. Beginning with the earliest real-time flight simulators, parallel processing architectures for image synthesis have been built, but "anti-aliasing" remains a problem. A parallel processing architecture is described and simulated which consists of a serial chain of processors which produces as output a depth sorted list of those objects which are at least potentially visible at each pixel. The lists are then filtered to provide the final shading at each pixel.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Aliasing]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Parallel processing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Parallel processing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Quantization</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31099176</person_id>
				<author_profile_id><![CDATA[81100638369]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weinberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, University of Minnesota, Minneapolis, MN]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>359869</ref_obj_id>
				<ref_obj_pid>359863</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Crow, Franklin, "The Aliasing Problem in Computer Generated Shaded Images," Communications of the ACM, November, 1977.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807360</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Catmull, Edwin, "A Hidden-Surface Algorithm with Anti-Aliasing," Computer Graphics, 1979, pp. 6-10.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807507</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Feibush, Eliot, Mark Levoy and Robert Cook, "Synthetic Texturing Using Digital Filters," Computer Graphics, Vol. 14, Number 3, July, 1980, pp. 294-301.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807459</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Kaplan, Michael and Donald Greenberg "Parallel Processing Techniques for Hidden Surface Removal," Computer Graphics, 1979, pp. 300-307.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Fuchs, Henry, "An Expandable Multiprocessor Architecture for Video Graphics (Preliminary Report)," University of North Carolina at Chapel Hill Technical Report TR-79-002, 1979.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807467</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Parke, Frederic, "Simulation and Expected Performance Analysis of Multiple Processor Z-Buffer Systems," Computer Graphics, Vol. 14, Number 3, July 1980, pp. 48-56.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Clarke, James, "A VLSI Geometry Processor for Graphics," Computer, July 1980, pp. 59-69.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Cohen, Dan, Presentation at SIGGRAPH 1980.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Schumacker, Robert, "A New Visual System Architecture," Second Interservice/Industry Training Equipment Conference, November 1980, pp. 94-101.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807486</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Jackson, J.H., "Dynamic Scan-Converted Images with a Frame Buffer Display Device," Computer Graphics, Vol. 14, Number 3, July, 1980, pp. 163-169.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Parallel Processing Image Synthesis and Anti-Aliasing Richard Weinberg Computer Science Department 
University of Minnesota Minneapolis, MN 55455 * ABSTRACT The continuing evolution of microelectronics 
provides the tools for developing new methods of synthesizing digital images by utilizing parallel processing 
architectures which hold the promise of reliability, flexibility and low cost. Beginning with the earliest 
real-time flight simulators, parallel processing architectures for image synthesis have been built, but 
"anti-aliasing" remains a problem. A parallel processing architecture is described and simulated which 
consists of a serial chain of processors which produces as output a depth sorted list of those objects 
which are at least potentially visible at each pixel. The lists are then filtered to provide the final 
shading at each pixel. KEYWORDS: Aliasing, Computer Graphics, Parallel Processing. ANTI-ALIASING AND 
PARALLEL PROCESSORS In image synthesis, one key problem is suppressing artifacts which result from the 
digital creation of the images, a process often referred to as "anti-aliasing." Under the general rubrick 
of "aliasing" comes several distinct problems including the appearance of jagged edges along object boundaries 
and small objects popping on and off the screen when they are smaller than the pixel size. [1,2,3] * 
Author's Current Address: Cray Research, Inc. 1440 Nor£hland Dr. Mendota Heights, MN 55120 Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by permission of the Association for Computing 
Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. &#38;#169;1981 
ACM O-8971-045-1/81-0800-0055 $00.75 In any anti-aliasing scheme, the contribution of all the visible 
objects (even small ones) within a certain distance from the center of the pixel being evaluated need 
to be taken into account. The final pixel color determined is then a combination of the visible colors 
or intensities. PARALLEL PROCESSING IMAGE SYNTHESIS In order to implement a real-time graphics system, 
special purpose architectures are employed to achieve the fast data rates needed to produce images in 
a cost effective manner. Parallel architectures attempt to meet the speed demand by partitioning the 
image generation problem among many processors (standard or custom) each operating concurrently. Recent 
work of Kaplan and Greenberg [4], Fuchs [5], Parke [6], Clarke [7], Cohen [8], and Schumacker [9] have 
dealt with parallel processing image synthesis systems, but only Schumacker's system deals with the problem 
of anti-aliasing. Four basic types of parallel processing architectures are apparent: i) partitioned 
object space architectures (e.g. Cohen), 2) partitioned image space architectures (e.g. Fuchs and Parke), 
3) partitioned operation architectures (Schumaker [9]), and 4) combinations of the above. Examples 
of partitioned object space systems include the G.E. system at NASA, the first real-time graphics system, 
~hich had a card for each edge in the environment. No anti-aliasing was attempted. A system now under 
development based on custom chips which implement a pixel by pixel in-or-out decision processor [8] is 
also based on partitioning the object space. Each processor scan-converts a polygon, and the processors 
are connected as a pipeline. A processor either passes on the pixel data it receives, or replaces it 
if its own pixel is closer to the viewer to effect hidden surface removal. Anti-aliasing is a problem 
since pixel coverage is not available at the appropriate time, and only one object Computer Graphics 
Volume 15, Number 3 August 1981 can be selected as the visible object at each pixel. Increased resolution 
would help, but at the expense of increased speed. This type of serially connected structure provides 
the basis for the architecture which is presented later but which includes anti-aliasing. Partitioning 
the image space means dividing the image generation task among processors which are each responsible 
for a region (scan lines, square regions of pixels, etc.) of the final image. Such an architecture has 
been described by Fuchs. In this system, essentially a distributed z-buffer, the screen is divided among 
a collection of processors, each of which is responsible for a group of pixels on the screen. Parke 
[6] has simulated a distributed z-buffer system also, and has examined various ways of distributing the 
objects to the processors. Anti-aliasing is difficult to perform in such systems. The z-buffer algorithm 
is not well suited to solving anti-aliasing problems. Since the polygons may arrive in any order, one 
cannot properly look at the interactions within the pixel which cause aliasing problems. If we try to 
increase the resolution to aid anti-aliasing, the memory and speed requirements grow rapidly. Several 
currently operating systems are based on operation partitioning. The main philosophy has been large scale 
pipeline systems with each processor performing an operation (sorting, shading, etc.) on a full frame's 
worth of data in a frame's time. Each of these processors is specialized for its particular task. These 
systems are not easily implemented in structured modular fashion as arrays of identical processors, but 
do generally include hardware to reduce the effects of aliasing. The system presented in the next section 
is based on partitioning both the object space and the image space. A SORTING CHAIN FOR ANTI-ALIASING 
 In this section a new architecture for high speed image synthesis is presented. Since a variety of systems 
have been built to transform points, perform perspective division, and clip objects to screen boundaries, 
we will not be concerned with those ~rocesses here, but will concentrate instead on visible surface determination, 
raster scan- conversion and anti-aliasing. The system has three basic components: i) object processors 
which concurrently scan- convert objects 2) comparators which accept inputs from the object processors, 
tie the system together in a serial chain, and maintain a depth sorted list of the objects which could 
be visible at each pixel and 3) a filter section which combines objects in the sorted list to derive 
final colors at each pixel. o o o o c--c--c--c--->f--f--f-->f->frame buffer key: o object processors 
c comparators f filters Fig. i. System Structure Each object processor in the system receives objects 
to scan-convert and outputs sections of the object to its own comparator. These sections represent the 
intersection of the object with a neighborhood (typically 2x2) of the current pixel. Each comparator 
in the serial chain receives a depth sorted list of potentially visible object pieces from the previous 
comparators, and if its own objects are potentially visible, it inserts them in the proper order. If 
the processor's objects totally hide previous candidate objects, the hidden objects are removed from 
the list. In Cohen's system, each processor passes a single color and depth as the current visible candidate 
at each pixel. Here, the system passes a list which may contain more than one object per pixel, namely 
those objects which may be visible (have not as yet been totally hidden by a closer or higher priority 
object). In addition to color and depth, shape information is also passed along. The first processor 
in the chain is used to generate a background color of the lowest priority at infinite depth to assure 
that every pixel has at least one visible object. At the end of the processor chain emerges a list of 
object sections which are sorted first by scan line (top to bottom), then by pixel (left to right), and 
then by depth (front to back or highest to lowest priority) within the pixel. The list then passes through 
a series of filter stages which scan the potentially visible objects and sum a precomputed filter function 
over the visible portions of the objects. The key elements of the system are the object processors (scan-converters), 
the comparators, and the filters, which are described in the next three sections. OBJECT PROCESSORS 
 An object processor is a device which receives a description of an object, scan-converts the object, 
and outputs sections of the object to a comparator. Scan-converting the object produces the color of 
the object at the pixel, the object's depth at the pixel, and the shape of the intersection of the object 
with the (rectangular) region  surrounding the pixel on a pixel by pixel basis. In the simulation a 
2 x 2 pixel region was used. The object processor is specialized to scan convert a particular type of 
object. Here, the objects to be displayed are convex polygons, which are decomposed into an intermediate 
trapezoid form, as in [i0] . This form was chosen to simplify the scan-conversion and filtering processes. 
A polygon which has been transformed to screen coordinates may be divided into (possibly degenerate) 
trapezoids with two horizontal edges (possibly of zero length) as shown below. Given a polygon with 
v vertices, at most v-i trapezoids are required to coVer the polygon. vl v2 v4 Fig. 2. A Polygon Decomposed 
into Trapezoids  The object processors each receive a trapezoid description (or more than one if they 
do not overlap in y). Included in the trapezoid description is the shape and location of the trapezoid, 
how its depth changes from pixel to pixel and line to line, and its priority. Each trapezoid has a single 
upper and a single lower edge parallel to the raster of the screen. Each has a single right and a single 
left edge which may be specified by the x coordinate of the intersection of the edge with the raster 
line and the change in x from one (sub) scan line to the next. For anti-aliasing, the position of the 
trapezoids must be specified to a resolution finer than the pixel size. The trapezoid data structure 
includes: top ( y at top of trapezoid) bottom ( y at bottom of trapezoid) xltop ( x at upper left 
of trapezoid) xrtop ( x at upper right of trapezoid) dxl ( change in x per sub-scan line on left size 
of trapezoid) dxr ( change in x per sub-scan line on right size of trapezoid) z (depth) dzpixel ( 
change in depth per pixel) dzline ( change in depth per line)  status r g b ( includes key, even/odd, 
(red) (green) (blue) priority, dud, covera ge) The object sections output by the object processors are 
essentially small trapezoids (a few pixels in size) as above, but with fixed depth. While the object 
received by the object processor contained enough information to track the object through the entire 
image, the object section is just that portion of the full object which intersects the current pixel 
region. The object processor generates status information for the comparator. The even/odd bit is a 
copy of the low order bit of the pixel number and is used to determine whether two object sections which 
are adjacent to each other in the sorted list are from the same or from neighboring pixels. The dud 
bit indicates whether the object intersects the pixel or not. The coverage bit indicates whether objects 
which intersect the pixel region cover the whole pixel region or not. COMPARATORS The comparators are 
devices which are used to tie the object processors together in a serial chain and properly sort object 
sections as they flow through the system. Each object processor has its own comparator. The comparators 
have two input registers (called "mysect" and "prevsect") and one output register (called "outsect"), 
each of which can contain a object section. The two input registers, mysect and prevsect, hold the object 
section created by that comparator's object processor,and the object section received from the previous 
comparator, respectively. Based on information contained in the status fields and the depth fields of 
mysect and prevsect, the comparator decides which of the two input object sections will be copied to 
the output register, and if both, in what order. object processor n mysect  from comparator n-i to 
comparator n+l Fig. 3. Object Processor/Comparator Connections When two object sections residing in 
mysect and prevsect are compared, one may totally hide the other, or both may be partially visible. 
In the algorithm which follows, we say that "A hides B" when A is closer in depth than B and A covers 
the entire pixel region; "A dominates B" when A is closer in depth than B but A only partially covers 
the pixel region. A sect-list is a contiguous list of object sections for a single pixel travelling through 
the system, and is always sorted in z or priority order. All objects in a sect-list have identical even/odd 
bits since they are for the same pixel. The following algorithm depicts the functioning of the comparators. 
 do forever: if mysect is empty receive object from object processor; if prevsect is empty receive object 
from predecessor; if (evenodd(mysect) = evenodd(prevsect)) /* i.e. same pixel */ switch(compare (mysect,prevsect)) 
 case mysect hides prevsect: send mysect to outsect; destroy sect-list; break; case prevsect hides 
mysect: destroy mysect; send sect-list to outsect; break; case mysect dominates prevsect; send mysect 
to outsect; send sect-list to outsect; break; case prevsect dominates mysect: send prevsect to outsect; 
 break;  else send mysect to outsect; end; FILTER SECTIONS Emerging from the last comparator is a 
sequence of object section sorted by y, x, and z. Given this front to back ordering of objects within 
each pixel, the filters can perform calculations for anti-aliasing, namely: final determination of visibility 
for each object at sub-pixel resolution, summation of a filter function over the visible portions of 
each object, and final color determination for each pixel. The method simulated here is one of a number 
of possible approaches. Within each pixel region (2 x 2 pixels), the filters determine which object 
is visible for each point on a high resolution rectangular grid. Each of these subpixel sample points 
has a precalculated weight. The final color is determined for each pixel by summing the filter function 
at those sub-pixel points within the boundary of each visible object, multiplying by the color of the 
 object, and summing over the visible objects in the pixel region. The final colors output to the frame 
buffer are: R =Z obj[k] .r xZ Z(mask[i,j] x filt[i,j]) k i j G =Z obj[k] .g xZ Z(mask[i,j] x filt[i,j]) 
k i j B =Z obj[k] .b xZ Z(mask[i,j] x filt[i,j]) k ij where k ranges over the visible objects for the 
given pixel, and i and j are indices of the subpixel grid columns and rows respectively. The sub-pixel 
mask is 1 at (i,j) if the object is visible at sub-pixel (i,j), otherwise it is 0. The right-hand double 
sums are calculated in the filter stages. The process of filtering the two-dimensional pixel region is 
broken down to a collection of one dimensional processes on "sub-scan" lines, with one filter section 
per sub-scan line plus three additional stages. A sub-scan line is a line through the image parallel 
to the raster lines, but at a finer resolution than the image raster. The filters will calculate which 
objects are visible at each "sub-pixel" on each sub-scan line, and filter the results to derive final 
colors. The filter sections are identical (except for contents of filter function lookup tables) and 
are connected as a pipeline. First consider the flow of data from  one filter section to the next. 
The input to the first filter section is the output of the final comparator in the chain of object processor/ccmparators, 
namely a depth sorted list of object sections. The first filter clips the incoming object section into 
two pieces: the top sub-scan line segment, and the rest of the object section. Each succeeding filter 
section clips off the top sub-scan line segment from the object section it receives, and sends the remainder 
to the next stage until the kth filter section (k is the number of sub-scan lines) receives the bottom 
sub-scan line segment. inputs outputs >  top I -i inside? bottom i -i dxhi ] > xhi + > xhi table 
 ] AND -AND -- ~xlo table J  xlo + > dxlo >  mask OR >already >(2) covered I~NOT--A new subpixels 
filter table ~ ~~(3) lookup  subline filter7 >(4) i  partial sum + > even/odd > I unit delay --=? 
reset already covered  red, green, blue > Fig. 4. One Sub-Scan Line Filter Section  Each filter section 
performs the following operations: l) receives input from previous filter stage (or from last comparator 
for the first filter) 2) clips off the top sub-scan line from the object and prepares the rest for the 
next stage, 3) generates a bit mask which indicates which subpixels are within the object's intersection 
with the sub-scan line, 4) determines which of those subpixels are visible, 5) looks up the summed filter 
function over the visible subpixels, 6) adds the filter contribution from the sub-scan line to the sum 
of the contributions from previous filter stages, 7) passes fixed information, namely even/odd status 
and color, and 8) outputs to the next filter stage. The filters operate in a concurrent pipelined fashion. 
 Two lookup tables are used to determine the mask of an object within a pixel region. If a scan segment 
extends from xlo to xhi, we do two table lookups xlotable[xlo] and xhitable[xhi], then AND the results 
to derive a mask of is where the subpixels are covered by the object, and 0s where they are not. Xlotable[xlo] 
is all 0s to the left of position x, otherwise is. Xhitable[xhi] is all 0s to the right of position x, 
otherwise is. These masks are then passed to the next stage and indicate which subpixels are covered 
by the object. An "already covered" subpixel mask with one bit per subpixel is used to determine which 
objects are visible at each subpixel. When the calculations for a new pixel begins, all of the bits in 
the mask are initially zeroed. Beginning with the object closest to the observer's eye (first in the 
depth sorted list) for the current pixel the filter sets those bits in the already covered subpixel mask 
which correspond to regions covered by the object. If an object covers any subpixels which are not already 
covered by another (closer) object, the already covered mask for those subpixels is set to i, and a hit 
for the object (at each newly covered subpixel) is output. The hits indicate that the new object is visible 
at those particular subpixel points, and provides an index for table lookup of filter functions at the 
next filter stage. The value in the table at that index is the sum of the filter function at each of 
the subpixel points which are visible for the object on the given sub-scan line. The size of the table 
is 2 ** m, where m is the number of bits of subpixel resolution. As the subpixel resultion goes up, 
the size of this table increases rapidly. This can be avoided by breaking the table into several sections, 
each section representing part of the sub-scan line. Instead of having one table with 2 ** 16 entries, 
we could use 4 tables each of which has 2 ** 4 entries and then sum Computer Graphics Volume 15, Number 
3 August 1981 the results in additional adder stages to obtain the final filter value for the sub-scan 
line. A balance must be struck between the size of the tables and the number of adders.  The output 
of the filter table lookup for the subscan line is then added to the sum from the previous sub-scan line 
filters. FINAL COLOR DETERMINATION  This model assumes a linear RGB color space. The output of the 
last filter stage is the sum of the filter function over the visible portion of the object under consideration. 
This output is one multiplicand for each of three color multipliers (red, green, blue) for multiplying 
the summed filter function by the color of the object. The other multiplicands (red, green, blue) arrive 
from the filters at the same time. If there is only one object per pixel, it must cover the entire pixel, 
so the filter function must sum to (normalized) unity. filter sum r.out x >+~ >R.final g.out x > +~G.final 
 b.out x >+~ ~>B.final even/odd  unit delay =? reset color adders Fig. 5. Final Color Determination 
 The output of the color multipliers becomes one input to each of the three (red, green, blue) color 
adders. The color adders accumulate the weighted colors for all objects visible in the pixel. When an 
object for a new pixel arrives, the color adders are zeroed. The output of the color adder contains 
the final color for the pixel, and is sent to the frame buffer for display. TIMING  Two phases of operation 
occur each frame or field: objects are loaded into the object processors, and then are scan converted 
and filtered to form an image. The basic data flow during scan conversion is from each object processor 
 to its associated comparator, and from one comparator to the next and then through the filter stages. 
 In a typical shift register, objects proceed from one stage to the next in synchronous fashion, and 
we always know that there will be an empty slot for each object (namely the next stage) at each data 
transfer. Here, however, since new objects may be inserted into and deleted from the stream, a comparator 
must signal its predecessor when it can accept an object and its successor when it has an object to output. 
The data transfer could then take place asynchronously at that point or synchronously. If we were only 
selecting one object per pixel, then we know the system requires (n x m) + p cycles to generate an image, 
where n x m is the size of the screen and p is the number of object processors. This allows the first 
object processor to generate all n x m pixels in n x m cycles, with an additional p cycles for the last 
pixel to travel through the chain. Here, however, we insert additional objects whenever they are partially 
visible. If edges are totally obscured by closer objects covering the entire pixel region at the time 
the object reaches its own comparator, then the edge does not get entered into the stream, and no additional 
time penalty is incurred. This indicates that an initial sorting of the trapezoids into a depth order 
would speed up the system. The number of cycles required is (n x m) ÷ p + q, where q is the number of 
additional objects inserted, q varies according to the image being generated, and is a function of the 
shape and location of the objects as well as how they hide one another. An approximate upper bound on 
q is the sum of the perimeters of the minimum size rectangle bounding the trapezoids in the scene. This 
assumption is based on each edge of each object intersecting a pixel, being visible and not covering 
the pixel totally. If the object totally covers the pixel region (as the interior of a large object would) 
then no additional objects are inserted over the interior. The number of objects added to the stream 
by a typical totally visible trapezoid would be the perimeter of the rectangle bounding the trapezoid. 
If its edges lie directly on pixel boundaries, additional objects are not added. For example, if n = 
m = p = i000 (i.e. i000 objects, I000 x i000 raster) and an average object is contained in a square 200 
x 200 pixels, then q = (4 x 200) 1000, or 800,000. Then the number of cycles is 1000 x 1000 + i000 + 
800,000 = 1,801,000, which is less than twice the number of cycles required with only one object per 
pixel.  SIMULATION To demonstrate the feasibility of the processing scheme, simulation programs were 
written by the author using the language "C" under the UNIX operating system on a DEC PDP-II/45 at the 
Image Processing Center, University of Minnesota. Film output was recorded on a Dicomed film recorder. 
 The purpose of the simulations was to exercise a functional model of the system and to produce images 
which would represent those created by the hardware if it were built. In this simulation, a single model 
of each of the three basic components (object processors, comparators, and filters) was programmed, and 
each operated sequentially on the data sets for the processors. It was assumed that the entire system 
operated in a shift register fashion during the computation of each frame. Beginning with the last stage 
in the system (the tail end of the filters), and progressing backward from the last to the first comparator 
and object processor, each piece of data advanced one stage through the processing system each cycle, 
providing that the data was present and the stages were free. DISCUSSION This system incorporates a 
form of anti-aliasing, which other parallel processing schemes lack for the most part. The system is 
suitable for VLSI implementation, since it depends on replicated processing units. The limitations include: 
There is no way to display more objects than we have space for in the object processors. Other systems, 
such as a z-buffer system, allow an arbitrary number of objects to be used, although the time to compute 
the image must exceed the frame time at some point; the objective here is real-time response with reasonably 
complex images and not the capability to generate arbitrarily complex images. Although not simulated 
here, this limitation could be overcome if each processor could store more than one object, provided 
that they did not overlap, and switch between them. The system operates at high speed: each processor 
runs at video rates or faster. The system could be improved in several respects. The object processors 
could return object sections over a larger region, thus reducing the number of transfers from one object 
processor to the next, but making the comparison problem harder. A number of different filter types could 
be developed, including a clipping filter which solves the hidden surface problem over the pixel region 
exactly and filters the resulting object sections as whole regions, not as finely sampled points. Finally, 
the number of object types supported should be expanded. ACKNOWLEDGEMENTS Many thanks go to: The Image 
Processing Center at the University of Minnesota, Evans and Sutherland Computer Company, and Cray Research. 
Thanks also to Lee Anderson for writing a polygon generator. REFERENCES [I] Crow, Franklin, "The Aliasing 
Problem in Computer Generated Shaded Images," Communications of the ACM, November, 1977. [2] Catmull, 
Edwin, "A Hidden-Surface Algorithm with Anti-Aliasing," Computer Graphics, 1979, pp. 6-10. [3] Feibush, 
Eliot, Mark Levoy and Robert Cook, "Synthetic Texturing Using Digital Filters," Computer Graphics, Vol. 
14, Number 3, July, 1980, pp. 294-301.  [4] Kaplan, Michael and Donald Greenberg "Parallel Processing 
Techniques for Hidden Surface Removal, Computer Graphics, 1979, pp. 300-307. [5] Fuchs, Henry, "An Expandable 
Multiprocessor Architecture for Video Graphics (Preliminary Report)," University of North Carolina at 
Chapel Hill Technical Report TR-79-002, 1979. [6] Parke, Frederic, "Simulation and Expected Performance 
Analysis of Multiple Processor Z-Buffer Systems," Computer Graphics, Vol. 14, Number 3, July 1980, pp. 
48-56. [7] Clarke, James, "A VLSI Geometry Processor for Graphics," Computer, July 1980, pp. 59-69. 
 [8] Cohen, Dan, Presentation at SIGGRAPH 1980. [9] Schumacker, Robert, "A New Visual System Architecture," 
Second Interservice/Industry Training Equipment Conference, November 1980, pp. 94-101.  [i0] Jackson, 
J.H., "Dynamic Scan-Converted Images with a Frame Buffer Display Device," Computer Graphics, Vol. 14, 
Number 3, July, 1980, pp. 163-169.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806790</article_id>
		<sort_key>63</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[A frame buffer system with enhanced functionality]]></title>
		<page_from>63</page_from>
		<page_to>69</page_to>
		<doi_number>10.1145/800224.806790</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806790</url>
		<abstract>
			<par><![CDATA[<p>A video-resolution frame buffer system with 32 bits per pixel is described. The system includes, in addition to standard features for limited zoom and pan, an arithmetic unit at the update port which allows local computation of many frequently-used pixel-level functions combining stored pixel values with incoming pixel values. In addition to the standard arithmetic and logical functions there are functions for sum to maximum pixel value and difference to minimum pixel value. Comparisons between incoming and stored data are used to implement conditional writes based on depth values for depth-buffer algorithms. Update and refresh ports are designed for a wide range of flexibility allowing simultaneous use by separate tasks and various functional rearrangements of the 32-bit pixel words. The memory architecture, refresh and update ports are described. Examples of widely divergent modes of operation are provided.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Bitmap and framebuffer operations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39070752</person_id>
				<author_profile_id><![CDATA[81100447047]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[F.]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Crow]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332185</person_id>
				<author_profile_id><![CDATA[81543037056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[M.]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Howard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807490</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Akland, B. and Weste, N., "Real-Time Animation Playback on a Frame Store Display System," Computer Graphics Vol. 14 (3) SIGGRAPH-ACM, (July 1980)]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>802893</ref_obj_id>
				<ref_obj_pid>800090</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Fuchs, Henry and Johnson, B., "An Expandable Architecture for Video Graphics," Proceedings of the Sixth Symposium on Computer Architecture, (April 1979).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807359</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Crow, Franklin C., "The Use of Grayscale for Improved Raster Display of Vectors and Characters," Computer Graphics Vol. 12 (2) SIGGRAPH-ACM, (July 1978).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Crow, Franklin C., "An Approach to Real-Time Scan Conversion," Proceedings of the National Computer Conference, (June 1979).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807486</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Jackson, J.H., "Dynamic Scan-Converted Images with a Frame Buffer Display Device," Computer Graphics Vol. 14 (3) SIGGRAPH-ACM, (july 1980).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Kajiya, James T., Sutherland, Ivan E., and Cheadle, Edward C., "A Random-Access Video Frame Buffer," Proceedings of the Conference on Computer Graphics, Pattern Recognition and Data Structures, IEEE Catalog No. 75CH0981-1C, (May 1975).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Kolb, William W., A Real-Time Raster Scan Conversion Processor, University of Texas, Austin, Texas (August 1980). Masters Thesis.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807467</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Parke, Frederick I., "Simulation and Expected Performance Analysis of Multiple Processor Z-Buffer Systems," Computer Graphics Vol. 14 (3) SIGGRAPH-ACM, (July 1980).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807418</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Shoup, Richard G., "Color Table Animation", Computer Graphics Vol. 13 (2) SIGGRAPH-ACM, (August 1979).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 A FRAME BUFFER SYSTEM WITH ENHANCED FUNCTIONALITY 
F.C. Crow M.W. Howard Ohio State University ABSTRACT A video-resolution frame buffer system with 32 
bits per pixel is described. The system includes, in addition to standard features for limited zoom and 
pan, an arithmetic unit at the update port which allows local computation of many frequently-used pixel-level 
functions combining stored pixel values with incoming pixel values. In addition to the standard arithmetic 
and logical functions there are functions for sum to maximum pixel value and dif- ference to minimum 
pixel value. Comparisons be- tween incoming and stored data are used to imple- ment conditional writes 
based on depth values for depth-buffer algorithms. Update and refresh ports are designed for a wide range 
of flexibility allow- ing simultaneous use by separate tasks and various functional rearrangements of 
the 32-bit pixel words. The memory architecture, refresh and update ports are described. Examples of 
widely divergent modes of operation are provided. INTRODUC TIC N A frame buffer is basically a multi-ported 
memory with one of its ports specialized and dedi- cated to refreshing a raster display. The other ports, 
used to update the contents of the memory, may be connected to processing power in a variety of ways. 
Originally, one port was con- nected to the I/O bus of a general-purpose pro- cessor (9,6). More recently, 
"smart" raster graphic displays have incorporated local processors dedi- cated to scan-conversion processes 
and other chores related to generating the display (1,5,8,2). Our frame buffer sits in a niche between 
the "dumb" and "smart" frame buffers by incorporating local arithmetic capability only. Permission to 
copy without fee all or part of this material is granted provided that the copies are not made or distributed 
for direct commercial advantage, the ACM copyright notice and the title of the publication and its date 
appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy otherwise, or to republish, requires a fee and/or specific permission. 01981 ACM 0-8971-045-1/81-0800-0063 
$00.75 We found that our display programs spent a great deal of their time computing simple functions 
of pixel intensities. Furthermore, programs which read, modified and then rewrote pixel values were I/O 
bound. Obviously we could improve things by distributing these calculations to a peripheral processor. 
However, an adequately powerful (bit- sliced) processor is a bit overcomplicated and expensive for these 
simple arithmetic tasks. By providing the frame buffer with an arithmetic-logic capability but no program 
storage or sequencing, we achieve the desired functions at adequate speed without the complexity of a 
full-blown processor. Incorporating arithmetic functions in this way also allows us to remove some computational 
load from a local processor, giving us an additional stage in an image production pipeline. Our fully 
configured system uses a DEC VAX 11/780 as the host processor, a 24-bit wide local graphics pro- cessor 
(bit-sliced, based on the 2900 series micro- processor chips) for local processing, and then the arithmetically 
potent update port as the final pro- cessing stage before the image memory (Fig. i). In the following 
sections we first describe and motivate the functions which may be applied to incoming pixel values at 
the update port. ~Are then provide a description of the update logic. The different possible display 
formats are then described, followed by a description of the refresh port logic. The architecture of 
the image memory system lying between the update and refresh parts offers a final glimpse into the innards 
before we launch into a description of some of the intended applications for the system, a topic of somewhat 
broader interest. PIXEL OPERATIONS Many display algorithms require merely a "write-only" capability 
of the display memory. A "painter's algorithm" for example assumes that the  Support for this work was 
provided by the National Science Foundation under Grants MCS-7920977 and MCS-7923670. 63  Computer 
Graphics Volume 15, Number 3 August 1981 VAX n/7~o k I Unlbus interface E i ....(640"512"32) l e Port 
(]-4 i ;h ] nits)l > Video Out B S Frame buffer Controller Figure 1 Frame Buffer System Configuration 
Note: ALU shown for byte 3, bytes 0, 1, 2 are identical. MSB 9 I i. _ ~ ~ Rio~ , o~orotto~' New -- Data 
I I r"----D- Carry out I -- --D L " L___~ ..... ~oro Never J Result to / SB te Same ag byte 3 above / 
Memory I Same as byte 3 above Same as byte 3 above [ 8// IB ~ "8/" 8/*8 "8i~q MSB "8ISB 32 32 External 
unction Old Data Figure 3 ALU and Operand Selection IDgic Detail 34 ~,Iemory 32 Q~ bus 32 -I .ALU eon~'ol 
reqtster 2 [ I 3z ALU and oper~nd ~/ Zx~mal funct%o, (~Iendtn~ unit. eJtc.) 3Z _ A Flq~e 2 Update eO~'t 
O~'qenlza~on current contents of the memory are written over whenever changes are made. However, this 
stifles interactive use of the display. ~Vithout additional special hardware, there is no way of displaying 
a cursor, for example. A simple cursor may be rep- resented by complementing a pattern of pixels suggesting 
the cursor loca Lion, a simple arithmetic operation. Many other useful graphical functions may also be 
implemented with simple arithmetic. Many intriguing possibilities in the realm of digital painting involve 
special "brushes" which modify the image rather than painting over it. Such operations can be used to 
achieve blends of colors and the appearance of partially transparent paint. Blends of pixel intensities 
are also import- ant to realistic imagery where transparent surfaces are represented or anti-aliasing 
techniques are employed. A blend of intensities is just a weighted average, a more complicated arithmetic 
opera Lion. The depth-buffer brute force hidden surface algorithm enjoys a large number of devotees 
lately. This algorithm involves nothing more than a con- ditional write based on a depth comparison. 
If the depth is stored in the frame buffer, the depth comparison is, again, a simple arithmetic operation. 
 Lines drawn on a raster display can be made £o appear smooth by judicious use of intensity levels (3). 
However, when two such lines cross, improper intensities result if the later line is merely written over 
the earlier one. For that reason the intensities must be combined, either by a weighted average or, 
more simply, by summing. But a simple sum may overflow, causing the wrong intensity to be displayed. 
Therefore, overflow must be prevented by "clipping" the sum to the maximum intensity. ~Ve have incorporated 
functions for h~ndllng all the above operations in our frame buffer. In the process we have tried to 
include enough generality to allow unforseen operations, which inevitably crop up. Sum, difference and 
all standard logic functions are available at an arithmetic-loglc unit (ALU) which combines the input 
word with the addressed pixel word in the frame buffer memory. Additiona i functions (e. g. the weighted 
average) may be applied to operands before they reach the ALU (Fig. 2). Thus such functions may be combined 
with ALU operations, if desired. See Table 1 for the selectable functions provided. THE UPDATE PORT 
 All changes to the image are made through data registers at a single update port. Address and control 
registers specify what is to be done with the data (Fig. 2). Two update modes are provided. (i)A simple 
write or read may be completed every 832 nanoseconds (nsecs) or (2) a read-modify-wrlte may be carried 
out every 1664 nsecs. The read- modify write cycle a11ows full use Of the pixel operations described 
above. Thus any updates to the memory may easily involve a combination of "new" (at the update port) 
and "old" (from the memory) pixel values. All input to the memory passes through four byte-wide ALUs 
where it may be combined with four bytes from memory (Fig. 2). To allow the ALU to be used flexibly, 
the functions for the 4 sections of the ALU are individually specified. Carries between the ALUs may 
be enabled to treat the 32- bit words as single integers, if desired. Other features allow the ALU output 
to be fed back to select an alternate ALU function. For example, the carry output of a single-byte ALU 
section indicates overflow when summing. Thus the carry can be used to force an alternate function (all 
ones) to "clip" the output to a maximum. Using the comparison function a11ows a contiguous set of 2 
or 3 bytes of new data to be compared with the corresponding bytes read from memory. Based on this comparison 
either the new or old data may be directed to memory. This imple- ments the depth-buffer hidden-surface 
algorithm directly. More general use of a comparison function is allowed since the comparison output 
of eachALU byte may be used to select between any two pre- specified ALU functions for thatbyte (Fig. 
3). Thus, it is easy to specify other operations such as selection of the greater or lesser of a new 
byte and an old byte or implementation of a positive difference function. In addition, the bytes of 
the input word may be directed through multiplexers to different parts of the memory word. The least 
significant byte may B~ Slice Frame Suffer Frocessor !n~.er:~ce Data ~om Memory l I ......... I I Ne 
w Data ~,atch ÷ Blendln~ Function I 1 I O~erand Select:on Logic F~g~e 4 Dam Pauhs For ~he B~t-$1ice 
Processor ~nO Blending Func:ion (All pa~s 3~ bi.~s wLce)  be directed to any byte; the least significant 
16 bits may be directed to either half of the memory word; and the whole 32-bit word may be treated as 
an entity. Additional functions may be implemented in an external arithmetic unit which may be selected 
by the multiplexers (Fig. 2). For example, an import- ant function which cannot be implemented using 
standard ALU chips is the blending function, which requires multiplication. Since this has proven to 
be such an important operation we have chosen to implement it with additional circuitry involving monolithic 
multiplier chips. The least significant byte of the input word or stored word may be used to form a weighted 
average of any or all of the other three bytes of the input and stored words. This re- quires three multiplier-accumulator 
chips. In two multiply-accumulate cycles the blending byte and its complement are multiplied by the input 
and stored bytes respectively. X and Y addresses within the memory are de- livered to the pixel address 
registers as 16-bit words (Fig. 2). This allows a scissor function to operate, inhibiting memory writes 
whenever the X or Yvalues lie outside the visible portion of the memory. The scissor function may be 
disabled to allow writes to non-visible portions of memory (used for refresh control, explained below). 
 The X and Y addresses may be automatically incremented or decremented after reads or writes. The increment/decrement 
may be one or 2 forY, allowing the two interlaced fields to be written independently. To allow a multi-cycle 
real-modify- write operation (under the control of the local or host processor) the auto-increment/decrement 
function is specified separately for read and write. A selectable wrap-around function allows the entire 
screen to be written without having to update the address registers. All the above functions are specified 
by three 32-bit control registers which may be written by either the host processor or local processor. 
The control register bits specify ALU functions, multi- plexer select bits, and address update modes 
(e.g. scissor, increment, and wrap-around). We have chosen to provide a large number of control bits 
for maximum flexibility. We expect setting up the control bits to be a formidable task akin to micro- 
coding. However, standard bit patterns will be provided in the interface software to handle normal applications. 
 Also attached to the update port is the local processor which was designed for an earlier pro- ject 
(4,7). It is a 24-bit wide, bit-slice processor optimized for incremental calculations among other things. 
The host processor may communicate inde- pendently with the frame buffer and local processor. The local 
processor loads the frame buffer's address and data through a queue, or first-in-first-out (FIFO) buffer, 
for write or read-modify-write oper- ations involving local processor computations (Fig. 4). The FIFO 
buffer is necessary since the 200 nsec cycle time of the processor allows some kinds of simple pixel 
generation at burst rates greater than the update rate permitted by the frame buffer. To complete the 
necessary communication F~qure 5 Refi'esh P~rt Organlza~n paths, the control registers may be written 
and 32- bit words from the frame buffer may be read. Additional devices may De attached to the up- date 
port. A digitizing camera and a second bit- slice processor to be pipelined with the first are among 
the intended additions. DISPLAY OPTIONS A refresh processor cycles through the memory at the refresh 
port to refresh a 640 by 486 pixel image at 30 Hz in the National Television Standards Code (NTSC) interlaced 
format. The 32-bit output word may be used in a number of different ways. Full color is available by 
using three bytes of the output word for red, green, and blue respectively. Translation tables at the 
output a11ow compensation for display nonlinearities and provide for pseudo- color display from any one 
of the four output bytes. Mask registers allow use of partial bytes for non- standard output formats 
(e.g. storing 4-bit images, 2 per byte). The translation tables, masks, and other output controls for 
zoom pan, and output word formatting are reloaded from the non-displayed portion of the image memory 
during the end-of-frame retrace for frame-synchronized update. 640 pixels per scanline allows the use 
of the standard 4 by 3 aspect ratio on a raster CRT with- out introducing "pixel distortion". Distorted 
pixels result in vertical and horizontal lines, one pixel wide, having different widths. In the pro- 
duction of software for very high-quality images, getting rid of the effect of pixel distortion can be 
difficult and expensive. It is much easier to pre- vent it in the hardware. Nine of the 26 lines of 
undisplayed memory are used to store control information specifying a number of output options. The output 
ma~ be zoomed by integer factors and panned in a limited way. Smooth pans are not possible but any part 
of the image may be inspected in a zoomed format. The 32-bit output word may be distributed in a number 
of ways to three 12-bit output translation tables, one for each of the primary colors. In addition, mask 
registers allow selecting any bit pattern from the output bytes without resorting to complicated translation 
table manipulations (Fig. 5). The translation tables, mask registers, and zoom and pan control bits 
are reloaded during the end-of-frame retrace. An interrupt to the host processor may be generated at 
the end of the reload operation. This allows the host processor (or local processor) to update the output 
control in a synchronized manner. Additional output ports may be added to the system, allowing more 
than one display to be driven. This allows multiple uses of the frame buffer memory either for comparisons 
of images (e.g. using different translation tables) or for shared access to the system by multiple users. 
The frame buffer can be time-shared by changing the update port control registers whenever a new process 
is granted access to the frame buffer system. THE REFRESH PORT The four bytes of the output word are 
mapped to the three masked output registers (Fig. 5) in a limited number of formats (limited to keep 
down wiring costs). Multiplexers select one of four mappings. (I) Any byte may be sent to all three outputs 
for pseudo-color output (Fig. 6a). This treats the memory as four separate images. (2) Three of the bytes 
may be sent to the three outputs respectively for full-color output (Fig. 6b). Finally, (3 &#38; 4) subfields 
from either 16-bit half-word may be sent to the outputs to produce full-color with five bits for red 
and blue and six bits for green (Fig. 6c). The mask registers may be used to select individual bits 
from the output registers before translation. This can be used to cycle through 32 single-bit images, 
for example. Masking doesn't provide any functions which couldn't be provided through translation table 
manipulations (9). How- ever, the mask registers allow such operations with greatly reduced overhead, 
since only a few bits must be cha ged as opposed to completely rewriting the translation tables. It remains 
to be seen how useful this function will prove to be. After masking, the three resulting bytes are used 
to address three independent translation memories. The contents of these memories are reloaded every 
frame with data in the non-displayed portion of the frame buffer memory. Each of the three memories provides 
12 bits of output data, which is currently routed to a ten-bit video digital-to-analog con- verted (DAO). 
The three DACs in turn drive the red, green, and blue guns of the display. Up to four output ports may 
be used in the system. These ports are independently controlled by information stored in the non-displayed 
portion of the frame buffer. Switches on the output boards control the location in the frame buffer where 
the board reads its control bits. Thus, one user may look at a 24 bit image while another views the remaining 
8 bit field, or one 16 bit and two 8 bit images may be simultaneously displayed, or 4 different 8 bit 
images may be viewed, etc. MEMORY ARCHITECTURE AND TIMING We chose to use memory boards supplied by 
the Ikonas company for reasons of convenience and previous experience. This entailed some compli- cations 
in timing since the boards were designed for a 512-pixel scanline. Twenty boards are used, each supplying 
64k bytes, for a total of 1.3 mega- bytes. An inevitable problem with any frame buffer design involves 
obtain ng sufficient bandwidth (48 megabytes per second) for the refresh port. To achieve this, the twenty 
memory boards do a simultaneous read cycle (display refresh cycle) every 1.66 microseconds to individual 
tri-state 3 1 ~ 3 1 Select Byte  Pixel 3 ~ g xMU I 8. ~ To red, green and blue Data translation tables 
 a) 8 bit pseudo color r'-~3~~i ~ k To red translation table Pixel ~32 w Data 8 ~ To green translation 
table ~- To blue translation table b) 24 bit RGB 31 Select Halfword Sc~(iS:ll ) to red translation 
table 0 ~ ~.~(4:0) tO blue translation table c) 16 blt RGB Note: data left Justified to 8 bits with zero 
low order fill Figure 6 Possible Mappings of Pixel Data to Translation Table Data ALU Functions CarryInput 
Source New byte 0 Old byte l New + Old Carry out from next lease New -Old significant ALU byte Old - 
New New Old Conditions for selection of New Old secondary ALU operation New + Old All 0's Never All l's 
Z compare true NewExclusive or Old CarryOUt of ALU byte = l All O's All l's Z Comparisons Possible (16 
or 24 bits) Always true New = Old AlwaFs false New / Old New Old New = Old New Old New -Old Other ControlFunctions/Modifiers 
Scissoring Automatic wrap around Fill displayed portion of memory with constant X index by -l, 0, or 
+I Y index by -2, -l, 0, +i, or +2 Table I List of Update PortOperation Primitives latches. Thus, twenty 
pixels (640 bits) are ob- tained at once. The refresh processor scans the 20 latches in sequence, obtaining 
a new pixel every 83.2 nanoseconds. Since the memory boards cycle in 416 nanoseconds, three additional 
memory accesses may take place in between the plxel refresh cycles. One of these cycles is dedicated 
to refreshing the dynamic RAMs, the other two are used by the update port for write cycles or a single 
read-modify-write cycle. Providing adequate memory bandwidth prevents problems in trying to decide which 
function should get access to the memory when, for example, the demands of refreshing the display conflict 
with the need to update the contents of the memory. By giving all necessary functions a slice out of 
every 1.66 microseconds, conflict is prevented. This is possible because the refresh processor may access 
the 20 output latches for refresh while the memory is occupied with the remaining three cycles until 
the output latches must be reloaded. The timing chain (all clocks are derived from a single oscillator) 
provides NTSC outputs and control signals to drive the memory and output ports. Be- cause the timing 
is locked to the NTSC standard the red, green, and blue video output signals may be sent to an external 
camera encoder to produce a broadcast quality signal for recording purposes. The timing chain may be 
modified by the zoom and pan parameters obtained from the frame buffer memory during vertical retrace. 
In this way a limited amount of animation may be obtained by displaying different sub-parts of the entire 
frame buffer in sequence. This capability may also be used to allow close examination of a picture region. 
 APP LICATIONS The frame buffer system was designed with the applications expected in our laboratory 
in mind. Obviously its design is somewhat skewed towards our notions of what is important. The following 
surveys some of the applications for which it is in tend ed. The primary purpose for the enhancements 
to the frame buffer is to speed up the process of picture generation. Currently, our installation runs 
in a time-shared mode on the VAX 11/780. Usually we have a background process computing high- quality 
images while various users, running at higher priority, work at designing animated se- quences, developing 
system software, or carrying out experiments in new picture-generation tech- niques. In this environment, 
the local processor and frame buffer allow greatly increased production of low and medium quality images 
for feedback on animation sequences. The depth-buffer computation at the update port frees the local 
processor to con- centrate on tiling polygons. This leaves the time- shared host processor with only 
the job of trans- forming vertices and calculating vertex colors. For quick turnaround on animation 
sequences, a i" videotape recorder is used. To record images, the tape must be backed up and accelerated 
to full speed, dropping in a frame buffer image as the proper spot on the tape is passed. Usually an 
image sequence is stored on disc for recording. The bottleneck in recording images from disk is the time 
to shuttle the tape back and forth. Furthermore, the magnetic videotape itself can only take so much 
abuse before it stretches or loses its oxide. The frame buffer can be used to store four full-frame pseudo-color 
images, 16 quarter-frame images, or even 64 16th-frame images. These can be presented to the videotape 
at full speed by synchronously up- dating the frame buffer output control. In this way recording time 
can be reduced by a factor of 4 at minimum. Attendant wear and tear on the recorder is similarly reduced. 
Limited-length sequences of crude images (e. g. 160 by 121 pixels) may be played back using only the 
frame buffer. Since 64 such images can be stored and the rate and order of image presentation can be 
synchronously controlled by the host or local processor, over ten seconds of animation may be presented 
at six frames a second, or 2 seconds at 30 frames a second. In a similar vein, the frame buffer may be 
treated as 32 single-bit images which may be played back in sequence (or 128 single-bit quarter-images, 
or 512 single-bit 16th images). In any time-shared graphics environment, demand for frame-buffer access 
exceeds any reason- able level of supply. Therefore, an important use for the 32-bit frame buffer is 
as four independent 8-6it frame buffers. The frame buffer memory may be time-shared through the host 
processor by up- dating the control bits before each change of user. The software device driver in the 
host processor handles contention for the frame buffer and updating of the control bits. Duplicating 
the output circuitry (requires duplicating one board and resetting a couple of switches) allows two separate 
images to be viewed simultaneously. In fact, any two (or more) combi- nations of output schemes may be 
used simultane- ously. One RGB image and one pseudo-color image may be simultaneously viewed, for example. 
Alternatively, the same image may be displayed with different translation tables. The arithmetic circuits 
in the update port are useful for a number of things. However, the most immediate use for them, since 
we don't use the system for interactive raster graphics very much is in representing smooth lines and 
transparent surfaces. Smooth lines on a raster display require careful blending of colors. When two such 
lines cross, the intersection must be treated correctly. Three ways of handling this situation are aided 
by the hardware. (i) The intensities of the two lines may be summed to a maximum (using circuits which 
force all bits to ones on overflow). (2) The maxi- mum of the two intensities may be stored (using the 
comparator circuit). (3) The closer line may be blended with the more distant (using the weighted average 
circuit). CONCLUSION We hope that our experience with the frame buffer will demonstrate the worth of 
many of the features of its design. We don't feel that the mix of fen tures offered here is proper for 
every environ- ment. However, many of the features of the design come virtually free once one of the 
other features is included in the design. For example, the ALU chips provide many functions of two inputs 
beyond those originally considered important. The authors wish to thank Win Bo who did the major portion 
of the chip layout and mechanical assembly of the system (including wire wrapping). REFERENCES 1. 
Akland, B. and Weste, N., "Real-Time Animation Playback on a Frame Store Display System," Computer Graphics 
Vol. 14 (3) SIGGRAPH-ACM, (1ulF 19801 2. Fuchs, Henry and Johnson, B., "An Expandable Architecture for 
Video Graphics, " Proceedings of the Sixth Symposium on Computer Archi- tecture, (April 1979).  3. Crow, 
Franklin C., "The Use of Grayscale for Improved Raster Display of Vectors and Characters," Computer Graphics 
Vol. 12 (2) SIGGRAPH-ACM, (luly 1978).  4. Crow, Franklin C., "An Approach to Real-Time Scan Conversion, 
" Proceedings of the National Computer Conference, (June 1979).  5. Jackson, I.H., "Dynamic Scan-Converted 
Images with a Frame Buffer Display Device, " Computer Graphics Vol. 14 (3) SIGGRAPH-ACM, (july 1980). 
 6. Kajiya, lames T., Sutherland, Ivan E., and Cheadle, Edward C., "A Random-Access Video Frame Buffer, 
" Proceedings of the Conference on Computer Graphics, Pattern Recognition and Data Structures, IEEE Catalog 
No. 75CH0981- IC, (may 1975).  7. Kolb, William V~. , A Real-Time Raster Scan Conversion Processor, 
University of Texas, Austin, Texas (August 1980). Masters Thesis.  8. Parke, Frederick I., "Simulation 
and Expected Performance Analysis of Multiple Processor Z-Buffer Systems, " Computer Graphics Vol. 14 
 (3) SIGGRAPH-ACM, (July 1980).  9. Shoup, Richard G., "Color Table Animation",  Computer Graphics 
Vol. 13 (2) SIGGRAPH-ACM, (August 1979).  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806791</article_id>
		<sort_key>71</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[A VLSI architecture for updating raster-scan displays]]></title>
		<page_from>71</page_from>
		<page_to>78</page_to>
		<doi_number>10.1145/800224.806791</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806791</url>
		<abstract>
			<par><![CDATA[<p>Interactive use of a display requires the capability to update the display rapidly. This paper describes an on-going project at Carnegie-Mellon University in which we are designing a frame buffer raster-scan display system which has the high performance typically required for interactive display applications. The system is intended to be a display for personal computers, computer generated graphic images, and image processing applications. Built using smart VLSI memory chips, the system will use parallel processing techniques to provide high performance.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Anti-aliasing]]></kw>
			<kw><![CDATA[Array processing]]></kw>
			<kw><![CDATA[Computer graphics,]]></kw>
			<kw><![CDATA[Frame buffers]]></kw>
			<kw><![CDATA[Gray-scale]]></kw>
			<kw><![CDATA[Image processing]]></kw>
			<kw><![CDATA[Parallel processing]]></kw>
			<kw><![CDATA[Raster displays,]]></kw>
			<kw><![CDATA[Raster operation]]></kw>
			<kw><![CDATA[Smart memory]]></kw>
			<kw><![CDATA[VLSI]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Grayscale manipulation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>B.7.1</cat_node>
				<descriptor>VLSI (very large scale integration)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Bitmap and framebuffer operations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010633</concept_id>
				<concept_desc>CCS->Hardware->Very large scale integration design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP94030109</person_id>
				<author_profile_id><![CDATA[81332502486]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Satish]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gupta]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Carnegie-Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P246551</person_id>
				<author_profile_id><![CDATA[81100302488]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Sproull]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Carnegie-Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P117524</person_id>
				<author_profile_id><![CDATA[81100265287]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ivan]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Sutherland]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sutherland, Sproull and Associates Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fuchs, Henry, and Poulton, John. PIXEL PLANES: a Cellular VLSI-oriented Design for a Raster Graphics Engine. Invention Disclosure: A VLSI oriented design for a Video Graphics System, November 26, 1980.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Gupta, Satish. 8&#215;8 Display Performance. VLSI Project Memo, Computer Science Department, Carnegie-Mellon University.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806783</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Gupta, Satish, and Sproull, Robert F. Filtering Edges for Gray-Scale Displays. Computer Graphics, August, 1981.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Roberts L.G. Machine Perception in Three-dimensional Solids. Optical and Electrooptical Information Processing, MIT Press, Cambridge, Massachusetts.: 159-165, 1965.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578095</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Rosenfeld, Azriel and Kak, Avinash C. Digital Picture Processing. Academic Press, 1976.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807428</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Sproull, Robert F. Raster Graphics for Interactive Programming Environments. Computer Graphics 13(2):83-93, August, 1979.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Sproull Robert F. Using Program Transformations to Derive Line-Drawing Algorithms. Technical Report, Carnegie-Mellon University, Computer Science Department, 1981.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Sproull R.F., Sutherland, I.E., Thompson A., Gupta, S., and Minter, C. The 8 by 8 Display. Technical Report, Carnegie-Mellon University, Computer Science Department, 1981.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A VLSI Architecture for Updating Raster-Scan Displays Satish Gupta Robert F. Sproull Computer Science 
Department Carnegie-Mellon University Ivan E. Sutherland Sutherland, Sproull and Associates Inc. Abstract 
 Interactive use of a display requires the capability to update the display rapidly. This paper describes 
an on.going project at Carnegie-Mellon University in which we are designing a frame buffer raster-scan 
display system which has the high performance typically required for interactive display applications. 
The system is intended to be a display for personal computers, computer generated graphic images, and 
image processing applications. Built using smart VLSI memory chips, the system will use parallel processing 
techniques to provide high performance. Keywords: Raster displays, Frame Buffers, VLSI, Smart Memory, 
Parallel Processing, Array Processing, Raster Operation, Computer Graphics, Anti-aliasing, Gray-scale, 
Image Processing CR Categories: 4.32, 6.22, 6.34, 6.35, 8.2 This research is supported by the Defense 
Advanced Research Projects Agency, Department of Defense, ARPA Order 3597, monitored by the Air Force 
Avionics Laboratory under contract F33615-78-C-1551. The views and conclusions contained in this document 
are those of the authors and should not be interpreted as representing the official policies, either 
expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by permission of the Association for Computing 
Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. &#38;#169;1981 
ACM O-8971-045-1/81-0800-0071 $00.75 1. Introduction Raster-scanned frame buffer displays offer the advantage 
of being able to display complex, flicker-free images and are becoming increasingly popular because of 
the decreasing costs of high-density random access memory. A raster-scan image is generated by plotting 
point-by-point the intensity of each pixel (picture element) on a two-dimensional raster or matrix of 
pixels. Complex images are constructed by creating appropriate patterns on the raster. The image is stored 
in a frame buffer, which is a large random access memory containing storage for a binary representation 
of the intensity of each pixel on the screen. A display controller reads this frame buffer and repeatedly 
scans a TV monitor on which the image is viewed. Interactive use of a display requires the capability 
to update the display rapidly. Updating a frame buffer often requires changing large amounts of information 
even for conceptually simple operations. This problem is best illustrated by examples. Scrolling a window 
of the frame buffer involves updating all the pixels in that window. For it to appear smooth it should 
happen in less than a frame time (e.g. 1/30 of a second). Hence for a display with 1024x1024 pixels we 
need a bandwidth of approximately 30 Megapixels/second to allow smooth scrolling. Scrolling is a common 
operation used for screen editing, or view very large VLSI checkplots, terrain maps, and other digitized 
images. Such images or arbitrary parts thereof may be scrolled in all directions, which means that hardware 
tricks cannot be used to allow scrolling without actually moving the contents of the frame buffer. BITBLT 
(standing for BLock Transfer of BITS), is a fairly general operator that provides the ability to move 
an arbitrarily sized rectangle of the image from one part of the screen.to another, and can be used to 
provide most of the higher-level operations required by the editing and browsing tasks. Typical vector 
displays have the ability to draw several thousand vectors during each refresh period. If we assume each 
vector to be approximately 100 points long (about 1/10 of.the screen), then a frame buffer display system 
needs the ability to up, late at least 3 Megapixels/second in order to emulate a vector display. Another 
strength of raster-scan displays is their ability to show natural images. Such images can be sampled 
on a grid of points and then digitized so that they can be processed using computers. One can then do 
a variety of things with these images. We can improve their quality using image processing techniques, 
compare them with other images using pattern recognition techniques, or try to extract their contents 
using image understanding techniques. Several of these techniques require interaction on a raster-scan 
display. This again requires both computing and modifying the whole image within a reasonable response 
time. 71 Computer Graphics Volume 15, Number 3 August 1981 The next section of this paper briefly discusses 
the disadvantages of conventional frame buffer designs and describes the "8x8 display", its architecture, 
and how it provides the capability to move the image around on the screen at extremely high speeds. The 
8x8 display does not provide the primitives required for graphics and image processing applications and 
Section 3 discusses how these applications can benefit from the fundamental ideas of the 8x8 display. 
Section 4 presents a VLSI smart memory architecture that can provide all the desired primitives for a 
high performance display system. 2. The 8 by 8 Display Conventional frame buffer designs are usually 
constrained by the high memory bandwidth required to refresh the screen. In order satisfy the high data 
rate required for refresh, the frame buffer memory is usually organized into words and each word contains 
more than one pixel. These pixels are made to lie along a scan line so that the video refresh controller 
can simply load them into a shift register and shift them out as video for the CRT. Updates to the frame 
buffer memory can occur only in between refresh accesses and during the CRT horizontal and vertical retrace 
periods. The word oriented memory organization influences the algorithms used to update the image. In 
order to achieve a. fast update of the whole image it is desirable to update several pixels at a time. 
The memory organization calls for these pixels to lie along a scan line. This leads to the use of what 
are commonly known as scan fine a/gorithms to create and update images. These algorithms usually generate 
the image such that successive pixels updated lie along a scan line. There exist scan-line algorithms 
for BITBLT, polygon filling, shading, hidden surface elimination, half-toning and most image processing 
tasks. These algorithms are also used in real time displays which do not store the image in a frame buffer 
memory but generate it during the raster scan of the screen. A disadvantage of the word organization 
is that it divides the screen into boundaries corresponding to the word boundaries in memory. If each 
word contains 8 pixels, updates can take place only on a 8xl grid aligned to the boundaries of the screen, 
or a subset thereof. If it is necessary to write 8 pixels into the frame buffer memory which are not 
aligned to the 8 pixel boundary on the screen, the write must be split up into two separate writes to 
adjoining words of memory. To load an 8x10 character would probably take 20 memory cycles instead of 
the 10 required in the aligned case.  ~0000000 O0000OO0 Ol ol 0 1 I  Ol Ol 000 E 64 Memory Chl~ 
The 8x8 display [8] attacks the problems of conventional frame buffers in an attempt to provide very 
high speed BITBLT. The key to achieving high speed BITBLT is parallelism. The 8x8 display can read or 
write 64 pixels in each memory cycle; the 64 pixels read or written by one memory cycle lie on an 8x8 
square on the screen. In this organization an 8x10 character can be transferred in 4 memory cycles. The 
complexity of line drawing is now symmetric with respect to the x and y axes of the screen. This symmetric 
organization effectively provides a low access time for image generation. Successive bits generated by 
most image generators (we have discussed characters and lines already) lie close to each other in any 
direction. The 8x8 organization allows them to be updated together, which is effectively providing them 
a low average access time. This unconventional organization does make screen refresh harder than usual: 
8 scan-lines are read from the memory and fed into one of two buffers. One of the buffers is used to 
generate the refresh video while the other buffer is being filled with the next 8 scan lines from the 
frame buffer. The 8x8 display provides a 1024x1024 bit-map frame buffer. It uses 64 16-Kilobit RAM chips. 
Each memory chip provides one bit for each 8x8 square of the screen and, if the memory array is viewed 
as an 8x8 square, then there is a one-to-one correspondence between the memory chips and the bits of 
the screen (see Figure 1). The top leftmost bit of each 8x8 square on the screen comes from the top leftmost 
chip of the memory array and so on. If each chip is identified by (column number, row number), then point 
(x, y) on the screen will be located in chip (x mod 8, y mod 8). The address provided to the memory will 
be determined by the particular 8x8 square on the screen that is desired. The 8x8 squares on the screen 
can be identified by a column number and a row number (there are 128 columns and 128 rows).This is particularly 
convenient since the memory chips require a column address and a row address. The column number of the 
8x8 square is used as the column address to the memory cl~ips aed the row number is used as the row address. 
I:-Ience, point (x, y) on the screen will be located in chip (x mod 8, y mod 8) at address (x div 8, 
y div 8) where x div 8 is the column address and y div 8 is the row address. The addressing scheme described 
above has the same boundary problem that we discussed before. It does not to allow access to an arbitrarily 
positioned 8x8 square on the screen in one memory cycle. However an arbitrary 8x8 square can be accessed 
in one memory cycle if we provide separate addresses to different parts of the memory chip array. As 
shown in Figure 2, if we want to access the 8x8 square outlined with the dotted lines then the >x  
I I1 I1 I I1 I Bits corresponding to lhe marked memory chip  Figure 1. Computer Graphics Volume 15, 
Number 3 August 1981 memory chips in region A will be addressed by (x, y), chips in B by (x+l, y), chips 
in C by (x, y+l), and chips in D by (x+l, y+l). The topmost left corner of the dotted square is located 
at (x*8+ i, y*8 + j). The most obvious implementation of this addressing scheme would require 64 sets 
of address wires and a large amount of multiplexing hardware to get the appropriate addresses on these 
wires. The 8x8 display uses a time-space trade-off to implement this addressing economically. As shown 
in Figure 3, it uses eight sets of address wires bussed down the columns of the memory chip array. Each 
address bus has seven wires to carry either the row or the column address. The RAS (Row Address Strobe) 
lines are bussed across the rows and the CAS (Column Address Strobe) lines are bussed down columns. Three 
address cycles are required to provide each chip with the correct address. To address the square located 
at (x*8+i, y*8+j), the first cycle places y on all the address busses and strobes the RAS lines on (8-j) 
rows from the bottom. The second cycle places y + 1 on all the address busses and strobes the RAS lines 
on j rows from the top. Now all the chips have the correct y address. The third cycle places x + 1 on 
the first (8-i) address busses and x on the rest of the i address busses and strobes the CAS lines to 
all the chips. This implementation uses only 8 address busses instead of 64 but has to use three cycles 
instead of the usual two to deliver a full address. The inner loop of BITBLT involves reading an 8x8 
square from one part of the frame buffer (the source) and writing it into another (the destination). 
Since neither the source nor the destination have to be aligned with 8x8 boundaries, pixels may not get 
written into the memory chips from which they were read. Therefore the data may have to be realigned 
before the write can take place. The situation is illustrated in Figure 4 which shows that a two-dimensional 
rotation can be used to align the two 8x8 squares. The overall memory system is shown in Figure 5. Both 
the input and output ports of the memory provide the mechanism to do the two dimensional rotation which 
is needed to access arbitrary 8x8 squares (a mechanism we nicknamed "swizzle".). These swizzles are controlled 
by the low order three bits of the row and column addresses. The mask inputs are connected to the write 
enable pins of the memory chips providing the capability to write into a part of the 8x8 square. The 
data paths through the pixel processor are only 8 bits wide. Hence it takes 8 cycles (actually 11 due 
to pipelining latency) for the data to move from the output of the memory to the input. This proves to 
be the main bottleneck in the performance of BITBLT [2]. Each memory cycle takes 550 ns while the pixel 
processor takes 2.5/~s in its 11 cycles. Together with microcode overhead, copying one 8x8 square takes 
4.2/Ls. The design of the swizzle mechanism is another time.space trade-off, and the most important one, 
because it determines the overall performance of the system. The 8x8 display performed the swizzle during 
the 8 cycles it took to pass the data through the pixel processor. The two-dimensional rotation of the 
8x8 array can be performed in one step using sixteen &#38;bit rotaters, eight for one dimension and eight 
for the other. It can be done in two steps by time-multiplexing between one set of 8-bit rotaters, but 
the cost of the multiplexing hardware makes this scheme as expensive as the previous one. In retrospect, 
the 8x8 display should have definitely used the single step scheme. The 8x8 display can achieve high-speed 
line drawing by updating an 8x8 segment of the line in each memory cycle. Precomputed strokes are used 
to generate straight lines [7]. From the slope of the desired line, a set of strokes is computed, which 
. I MemorYArrayC.,p Iic o ! y+l Screen Figure 2.  /~X+ I /kX att3 t" ,, //~+ I art2 Ay at tl f 
A£ DR ,7  ] RAS "~ 1, RAS it 2 RAS t tl CAS Y  V ts Figure 3. Addressing scheme for the memory array. 
 I A Sl_ IC =.mDI Screen L. source ~ Two-dimensionol !~ G ) Memory Array rotation "E  Figure 4. 
are then placed appropriately to produce the line. One approach usesa small number of strokes (2), but 
doubles the maximum pixel placement error from 1/2 to 1 screen unit. Techniques that produce optimal 
lines (1/2 unit error) require a great many different strokes (8), and their performance is dominated 
by the time required to compute strokes and stroke locations rather than by the time required to write 
a stroke into memory. 73 8 Swizzle ~AMD Output ,"---I 2901 t ~ixel Processor Input Swizzle Figure 5. 
The 8x8 display was built using off-the-shelf components only. It uses approximately 300 chips in four 
8"x13" wire-wrap boards, which may make it impractical and uneconomical for most applications. The particular 
parts of the design that proved to be excessively expensive are the memory and multiplexing needed for 
the video buffer and the registers and multiplexers needed to implement the addressing and swizzling. 
A redesign of the 8x8 display which removes the deficiencies of the first design, investigates the basic 
8x8 concept for other applications, and tries for a more feasible and economic implementation, is the 
goal of our current effort.  3. Gray-Scale Applications The underlying memory organization of the 8x8 
display provides a model of computation where updates to the frame buffer can take place on any 8x8 square 
of the display. This two-dimensional model of computation should be able to exploit the two-dimensional 
nature of almost all display applications and hence provide fast algorithms for them. The 8x8 design 
does not allow us to investigate this underlying symmetric organization for a large range of applications. 
It was primarily designed to "copy" pixels, and not to perform any sort of computing on them. Although 
it was designed so that the memory planes could be duplicated to extend the display for gray-scale or 
color, the design does not allow computing or manipulating gray-scale values within the display. Our 
new model shall assume the presence of independent processing at each of the 64 pixels of the 8x8 square. 
It shall also assume the presence of some sort of communication between the 64 pixels which can be used 
both to examine pixels at and relocate pixels to different parts of the 8x8 square. This section looks 
at two representative tasks, the algorithms that they can use in this model of computation, and the requirements 
that they impose on the redesign. 3.1. Oejagged Line Drawing On bit-map displays, lines are drawn by 
just turning on pixels that lie close to the line. Such lines look "jaggy" because of jumps that can 
be seen when the line goes from one scan-line to another. This problem can be removed in gray-scale displays. 
The jumps can be smoothed by turning pixels on at varying intensities. This process is called "anti-aliasing" 
The intensity of any pixel can be determined from the perpendicular distance from the pixel to the ideal 
line [3]. )X I , 'X'X~. X ' ' , i ~ -' r . , i i ;, ; ; ; X I X I Xl x I I I I I r 'iP-r-P-~-&#38;-~--~" 
 I I I , , ....... , .......... ~ ........ ~. ..... ~_÷__, ....... ,L XlX -t" F-L--.'--~-~,F,-~ ,,=.~-.-1 
...... 4 ....... = = = / p ~ ' __ i ,X ~ XlX ' I /I ' / -= | "1 , , r -. ~--.-.J--J--c--.-J. .... 4 
..... I-.... .--~f ~ ~ -,-,-- i . i ' I I r" "t r- I-.,...:__,__,._:_....,.~. ~"_,..L__,...,... ' -"--' 
-'-" ' ~J ....... .... - .~.J-.t--J._.~-J.- ', ; , , , , , , = i~i ^ ~ , , , , , ', - ........ ,.... 
 /: ', : , , I , ~ / ; ~ : I , ! : = ,xlx'T~ i i i i ~ I I i I I I I t I ' .-L-.t--'--L °J.-J.. , , 
, , , , , , l , I , i , i I I--t-~-~-r--;-T ...... ,~-,~-~-~_r.~. J ........ L .... J 1._-+I_'_~ ..... 
,_._,_.._,.,__,_,_,_. , , ,,, 1 ,, _.J._I._L_L_L_ _. ' , ; , ' I ' ' ] ' I ' , , -r ..... -,-,------.,-,.--..-.----,__,. 
. ' .... 7 I ' ~" . . . ,] l' '' L '--'l--i--r-~ ..... I i i i m I I I I i I ' i I I I I I I I Figure 
6. Pixels turned on by a line of thickness 1. Boundaries mark 8x8 squares. (Xa,Ya) [Xa+7)  d = ( y 
- ya)*COSO- ( X -Xa)'sin e (Ya+7) Figure 7a. The Initial 8x8 square. d = (y-yn)'COS e-( X-Xn)'Sin e+ 
derr n Figure 7b. Subsequent 8x8 squares. Suppose we want to draw a line of unit thickness between (Xa, 
ya ) and (x b, yb ). From Figure 6 we can make two observations. First, all pixels turned on are within 
the rectangle enclosed by the two end points of the line. Second, the intensity of all the pixels in 
this rectangle can be computed as if the line extended infinitely on both sides (This is not exactly 
true but will be assumed.). The following algorithm assumes that the line has a slope of less than one. 
For steeper lines the same algorithm can be used with the axes interchanged. The line drawing algorithm 
attempts to write 8x8 squares into the frame buffer one at a time. The objective then is to compute the 
intensities of a whole 8x8 square, write them into memory, and then move the 8x8 square along the line 
to its next position. Notice that we cannot place subsequent 8x8 squares 8 spaces apart. If we want to 
capture pixels like the one marked in Figure 6, we have to position the corner of the 8x8 square one 
pixel above the line. In order to do that the squares also have to be one over to the left. Hence consecutive 
8x8 squares have to be placed 7 pixels apart for unit thickness lines. 74 The first 8x8 square to be 
written into will be located at X 0 = Xa; Y0 = Ya and subsequent ones will be located at x n = 7*n + 
Xa; Yn = trunc(y(Xn)) - 1 for n > 0 Within each 8x8 square we will assume that each pixel can independently 
compute its perpendicular distance to the line. Figure 7a shows the situation for the initial 8x8 square. 
If Dy = Yb- Ya and gx = x b -x a then sin# = Dx/sqrt(Dx*Dx + Dy*Dy) and cos8 = Dy/sqrt(Dx*Dx + Dy*Dy) 
 Then the perpendicular distance dxy from a point (x,y) in the 8x8 square to the line is dxy = (y-ya)*COS8 
- (X-Xa)*Sin8 where 0 _< (X-Xa,y-ya) <_ 7. Figure 7b shows that for subsequent 8x8 squares, where the 
line does not pass through the corner, dxy = (y-yn)*COS8- (X-Xn)*Sin8 + derr n derr n is the initial 
perpendicular error at the corner of the nth 8x8 square and is given by derrn = (Yn -Ya )*cOs8 -(x n 
- Xa)*SinS. This algorithm gives us a method to implement line drawing. Each pixel in the 8x8 square 
will compute dxy. The other computations can be performed by the host computer and the results broadcast 
to the 8x8 array. If the display has k bits of gray. scale then we must compute d with at least k bits 
of accuracy. Hence derr must be that accurate and so must sin# and cosS. The computation of d involves 
two multiplications (each of which is the multiplication of a 3 bit number and a k bit number), and two 
k bit additions. Incremental methods could be used to speed up this computation or we could build a special 
purpose ALU which performs this particular computation to optimize line drawing.  3.2. Edge Detection 
Edge detection is used during image processing for picture segmentation [5]. It segments the picture 
into regions based on the detection of discontinuities in the gray level. Such a discontinuity is called 
an edge and there are several techniques used to detect such edges. Most of them use derivative operators, 
which compute the gradient of the image intensity, and hence give high values at points where the gray 
level of the picture is changing rapidly. One of the popular digital gradient approximations is due to 
Roberts [4], and computes the following function for each pixel (i, j) max(If(i,j) - f(i + 1,j + 1)1, 
If(i + 1,j)-f(i,j + 1)l) This function relies on the fact that differences in any pair of perpendicular 
directions can be used to compute the gradient. A parallel implementation of this function requires each 
pixel to be connected to its right, lower and lower rightneighbors. It can then read those three values 
and combine them with its own to compute Roberts' function. This parallel approach will work even with 
8x8 squares, the only problem being that pixels in the lower right edges cannot get access to their neighbors. 
This can be solved by overlapping successive squares by one column in the horizontal direction and one 
row in the vertical direction. The algorithm to compute Robert's function for an image follows. Place 
the initial 8x8 square at the top left corner of the image. All pixels in this square except the ones 
on the lower and right edges will compute the function by taking the picture values from their right, 
lower and lower right neighbors. Move the 8x8 square right by 7 pixels and recompute. Repeat until the 
right edge of the image has been reached. Now place the 8x8 square at the left edge 7 pixels below the 
previous left edge placement. Repeat these two nested loops until the whole, image has been processed. 
This as well as most other image processing algorithms rely on neighbor interconnections for their speed. 
 4. Smart Memory Architecture Our current design effort aims to combine the use of VLSI and the experience 
of the 8x8 display to implement a display architecture that can effectively provide high performance 
for a large range of applications. The use of special purpose VLSI will make the architecture economical 
in addition to providing some capabilities that would have been infeasible if the design was restricted 
to using commercially available parts. As discussed in the previous section, this architecture will provide 
independent processors at each of the 64 pixels of the 8x8 square The 8x8 memory architecture has 64 
pieces of random access memory, each of which can be accessed in parallel to update an arbitrary 8x8 
square array of pixels. Making a piece of memory local to each processor results in a smart memory architecture. 
The 8x8 display can now be built out of 64 smart memory chips where each smart memory chip contains RAM 
that can be used to access one pixel every memory cycle and the processing associated with it. A block 
diagram of such a chip is shown in Figure 8 and its various parts are described below. 1. For a 1024x1024 
pixel display, it contains 16 Kilopixels of RAM which takes a 14 bit address and can read or write only 
one pixel at a time. The RAM wilt contain the value of one pixel in each of the 16K 8x8 squares in the 
frame buffer. 2. The processor manipulates the data and addresses associated with the RAM. The processor 
will contain several registers to store temporary data and add resses. 3. The chip contains a video 
buffer to buffer scan lines between the frame buffer and the video monitor. The processor will store 
addresses related to the current position of the refresh.  75  Computer Graphics COMMANDS Video ~_~ 
  I'.~ I Volume 15, Number 3 August 1981 sub-micron technologies were available to fabricate this kind 
of a frame buffer. Fuchs et. al. [1] are working on one such scheme which uses extremely simple processors 
per pixel. A more practical scheme would be one in which we had an array of processors which could manipulate 
a sub-window of the whole screen. We would then move this window around and eventually update tl~e whole 
image. What this gives us is an array of Inter- PROCESSOR Processor Commun- I ication"~ Addre I (33 
I g  "1o I I I I I Smart Memory Chip Figure 8. Smart memory chip. 4. The chip communicates with the 
rest of the processor array using one of several possible communication mechanisms. The inter-processor 
communication is needed for BITBLT (the swizzle), and for image processing and pattern recognition operations 
which usually require neighbor communication. Current technology, however, might not allow fabricating 
all that is desired into one package. In that case, we could split up the memory and the processing into 
separate packages. Using technology available to CMU, we have to do that anyway because we cannot fabricate 
state-of-the-art RAM chips. This design approach is effectively putting the computation closer to the 
frame buffer memory. Imagine the degenerate case of a smart frame buffer where each pixel had a processor 
- associated with it. Updates could then take the form of global commands to all the processors. For 
example all of the processors could be given the end-points of a line or the edges of a polygon and each 
processor would determine whether the pixel with which it is associated lies on the line or within the 
polygon and hence determine the pixel's value. For the kind of processors we have in mind, this scheme 
would be expensive and impractical even if processors with a part of the frame buffer associated with 
each processor, with the restriction that each processor can update only one pixel associated with it 
at a time. This is exactly what our smart memory architecture implements. The next four sections discuss 
the various subparts of the smart memory chip and describe the first version of the design. 4.1. Processor 
The processor will provide the arithmetic-logic functions and the temporary storage required to manipulate 
the addresses and the data associated with the memory. This processor has a fairly standard architecture 
although the amount of temporary storage and power of the ALU will be determined by applications. For 
example, as we have seen earlier the ALU might have a three input adder to speed up line-drawing on the 
display. The processor will have input/output interfaces for the RAM and for the inter-processor communication. 
A simple version of the processor was designed and fabricated during the spring of 1980. An interesting 
observation from this design is that decoders and the wiring take much more space than the ALU or the 
registers. Hence the cost of providing additional functionality which might be desired for some particular 
application may not be very high. The applications should be analyzed carefully and the design modified 
appropriately.  4.2. Video Buffer One of the disadvantages of the 8x8 organization of the frame buffer 
is the large amount of video buffering required to refresh the screen. This can solved by providing an 
appropriately sized video buffer within the memory chips. A block diagram of the video buffer fabricated 
is shown in Figure 9. It consists of two dynamic shift registers which shift in opposite directions. 
Each is 128 bits long. The data in the Input Shift Register can be transferred in parallel to the Output 
Shift Register. Only the input of the Input Shift Register and the output of the Output Shift Register 
are accessible. The buffer is used by shifting in data serially into the Input Shift Register, transfer 
all the data to the Output Shift Register, and then shift it out in the reverse order. To Input Shift 
Register Data In > I Clock = = Logic ic,oc, Transfer 021 n I I( I I,c o, Data Out < I OE Output Shift 
Register Figure 9. Block diagram of Video Buffer. 76 use it as a video buffer we would shift in the 
scan line in reverse order, transfer it to the Output Shift Register, and read it out to the monitor. 
The reason that the shift registers run in opposite directions is that we can then use this chip to act 
as a buffer for displays which have-different screen sizes. If the screen size is smaller than the maximum_we_can 
just ignore the unused~part o-F-- the shift registers.  ~" " L L_J L_I L,_I c I-E] E-I 4.3. Extern'al 
Data Communication Each of the memory chips needs the ability to transmit and receive data to and from 
the rest of the world. This communication is primarily useful for three different types of applications 
each imposing different requirements. One of the most important requirements of any frame buffer is to 
be able to load and read image data as fast as possible. This data could be from or to the main or secondary 
memory of ihe host computer. This is especially important for small frame buffers where large images 
are viewed by looking at small sections at a time. Examples of such scenarios are windowing packages 
[6], checkplotting VLSI chips, and viewing large satellite images. The second class of applications that 
need the external communication are the BITBLT applications. As we have seen before, BITBLT of unaligned 
rectangles requires the moving of data between chips. Although the data from one chip always goes to 
only one other chip for each BLT, the complexity of the problem is much worse because it may go to any 
one of the other chips in the system. Figure 4 showed that the data movement in BITBLT can be performed 
by a two-dimensional rotation where the rotation count along each axis is the difference in alignment 
of the two rectangles along those axes. Other variations of BITBLT may require mirroring about either 
or both the axes and transposition of the axes. For the smart memory organization this problem is then 
the transformation of one 8x8 square into another using any combination of operations like rotation along 
either of the axes by rotation counts R and R mirroring along either of the x or y axes x . .y' (M, M 
), and transpos~t=on of the axes (T). Figure 10 shows two ^ y . . . . connechon mechanisms which could 
be used to =mplement these operations. The pipelined connection can move the data in lock- step in either 
of the four directions. This connection is very efficient for the two-dimensional rotation. An arbitrary 
rotation takes an average of four steps and a maximum of eight steps. In the case of the sequential connection 
mechanism, any row or column can be read on to the dark wires, and any row or column can be written from 
the dotted wires in one cycle. All operations now take eight cycles. Image processing and pattern recognition 
rely on neighbor communication to extract some properties of images. The edge detection example we looked 
at in the previous section required connecting each cell to its lower, right, and lower right neighbors. 
Figure 11 shows a connection mechanism where each chip is connected to 11 other chips. In one cycle the 
data can now be moved to either of the neighbors, to the x-mirror point or the y- mirror point, or to 
its transpose. The connection scheme used will determine the speed of the algorithm. It also determines 
the cost of the system by determining the number of pins each chip requires for external communication. 
This decision is hence just a cost/performance trade-off. FF E E-I E-1 c F E-1 E3 I MIRROR I  Figure 
lOa. Pipelined Swizzle. I I OutReg JRotate/Mirror I I I InReg 1  Figure lOb. Sequential Swizzle. LeftTopt~ 
TopI .~ght .  XMirrorLeft ~ " BoffomLeft BottomRight L YM irror Transpose Figure 11. Each cell can read 
any one of 1 1 neighbours. 77 5. Current Status and Goals The previous sections of this paper have introduced 
the basic framework of our research. This section will outline our goals for the performance of the display 
system and describe the status of the project at the time of the writing of this paper. Our system design 
is determined by a set of representative applications which are analyzed in detail. High speed BITBLT 
is one of the most important goals of the system. We have looked at its gray-scale generalizations and 
the different operations under these generalizations. The various scan conversion tasks that the system 
addresses are the drawing of dejagged lines, circles and parametric curves, filling in areas enclosed 
by any of these, interpolating shades in these filled areas, while also trying to achieve an appropriate 
combination with the existing image. We feel that anti-aliasing is a crucial necessity for all these 
scan conversion tasks. We are looking at the possibility of doing halftoning by table Iookups from the 
frame buffer memory. The design also attempts to provide the required hardware needed for low level image 
processing and QCR. We plan to build a single processor chip that can be used .to configure a display 
system. At the time of the writing of this paper we have a functional specification and simulation of 
this chip. The design is flexible and allows variations in memory organization, interconnection mechanism, 
memory technolbgy, and raster output device. The design is optimized for BITBLT and we expect the inner 
loop to transfer an 8x8 array of pixels every 1.2 Fs. This achieves an update rate of approximately 50 
Megapixels/second. The line drawing algorithm we described in Section 3 takes 5 p.s to update an 8x8 
square along the line. Since a square of this size will have approximately 24 pixels that get intensified, 
the update rate is of the order 4.8 Megapixels/second. 6. Acknowledgements Carl Ebeling, Ed Frank, and 
Hank Walker have been extremely helpful in this project both in the design of the system and the review 
of this paper. The chips were fabricated as part of the Multi-Project Chip effort organized by the Xerox 
Palo Alto Research Center and sponsored by the Advanced Research Projects Agency. The new design will 
be fabricated by MOSIS at USC-ISI. References [1] Fuchs, Henry, and Poulton, John. PIXEL PLANES : a Cellular 
VLSl-oriented Design for a Raster Graphics Engine. Invention Disclosure : A VLSI oriented design for 
a Video Graphics System, November 26, 1980. [2] Gupta, Satish. 8x8 Display Performance. VLSI Project 
Memo, Computer Science Department, Carnegie-Mellon University. [3] Gupta, Satish, and Sproull, Robert 
F. Filtering Edges for Gray-Scale Displays. Computer Graphics, August, 1981. [4] Roberts LG. Machine 
Perception in Three-dimensional Solids. Optical and Electrooptical Information Processing, MIT Press, 
Cambridge, Massachusetts. :159-165, 1965. [5] Rosenfeld, Azriel and Kak, Avinash C. Digital Picture Processing. 
Academic Press, 1976. [6] Sproull, Robert F. Raster Graphics for Interactive Programming Environments. 
Computer Graphics 13(2):83-93, August, 1979. [7] Sproull Robert F. Using Program Transformations to 
Derive Line-Drawing Algorithms. Technical Report, Carnegie-Mellon University, Computer Science Department, 
1981. [8] Sproull R.F., Sutherland, I.E., Thompson A., Gupta, S., and Minter, C. The 8 by 8 Display. 
Technical Report, Carnegie-Mellon University, Computer Science Department, 1981. 78  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806792</article_id>
		<sort_key>79</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[A system design revolution (Panel Session)]]></title>
		<page_from>79</page_from>
		<page_to>82</page_to>
		<doi_number>10.1145/800224.806792</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806792</url>
		<abstract>
			<par><![CDATA[<p>System design is undergoing radical changes that are significant both for the graphics community and for other design communities in which the principal hardware resources are those obtained via catalogues of parts. The drive to reduce system costs forces us to design at the lowest level possible, i.e., on silicon; only in this way can we reduce the number of packaged components, thereby reducing the number of interconnecting wires, board space and power requirements. This, in turn, presents a system design dilemma. The large cost of designing silicon systems, i.e. VLSI, conflicts with the need to design them to reduce cost. As a result, novel new design aids are being devised that actually make <underline>integrated</underline> system design <underline>easier</underline> than conventional system design; integrated systems can be thoroughly simulated because simple models of them exist at every level of representation.</p> <p>By providing the system designer with lower-level design primitives, significant new architectures, such as the Geometry Engine and Image Memory Processor, will be the result. Semiconductor companies cannot make system design decisions for every area of application; only the system designer understands the application area well enough to make the correct decisions. The traditional semiconductor industry must make a transition from offering packaged system building blocks to offering fabrication services to the designers who know how to design systems.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>B.7.1</cat_node>
				<descriptor>VLSI (very large scale integration)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Hidden line/surface removal</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010633</concept_id>
				<concept_desc>CCS->Hardware->Very large scale integration design</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P333471</person_id>
				<author_profile_id><![CDATA[81100302488]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sproull]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie-Mellon University]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39070428</person_id>
				<author_profile_id><![CDATA[81332493735]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Clark]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>907242</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Catmull, Edwin, A Subdivision Algorithm for Computer Display of Curved Surfaces, PhD thesis, University of Utah, 1974.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Clark, James H., and Hannah, Marc R., "Distributed Processing in a High-Performance Smart Image Memory", Lambda, vol. 1, no. 3, 1980.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Cohen, D., and Demetrescu, S., A VLSI Approach to Computer Generated Imagery, Technical Report, USC, 1979, oral presentation by Dan Cohen made at SIGGRAPH '80, Seattle, July 1980.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Eastman, Jeffrey, A Efficient Scan Conversion and Hidden Surface Removal Algorithm Computers and Graphics, Vol. 1, No. 2, Sept. 1975, p. 215.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>810237</ref_obj_id>
				<ref_obj_pid>800179</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Fuchs, Henry, "Distributing a Visible Surface Algorithm Over Multiple Processors", Proc. ACM Annual Conf., Seattle, Oct. 1977.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Gouraud, Henri, "Continuous Shading of Curved Surfaces", IEEE Trans. Cmptrs., C-20 (June 1971), 623.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807486</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Jackson, J. H., "Dynamic Scan-Converted Images with a Frame Buffer Display Device", Computer Graphics, 14,3, (July 1980), 163.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Myers, Allan J., An Efficient Visible Surface Algorithm, Report to NSF, Grant No. DCR 74-00768 A01, Computer Graphics Research Group, Ohio State University, July 1975.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807467</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Parke, Frederic I., "Simulation and Expected Performance of Multiple Processor Z-Buffer Systems", Computer Graphics, Volume 14, No. 3 (July 1980), pp. 48-56.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Parke, Frederic I., Performance Analysis of Z-buffer Convex Tiler Based Shaded Image Generation, Tech. Rep. CES 79-15, Case Western Reserve University, October 1979.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Roman, Grui-Catalin, and Kimura, Takayuki, "A VLSI Architecture for Real-Time Color Display of Three-Dimensional Objects", Proceedings of the Delaware Valley Microprocessor Conference, April 1979, pp. 113-118.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Schachter, Bruce, and Ahuja, Narendra, "A History of Visual Flight Simulation", Computer Graphics World, Vol. 3, no. 3 (May 1980) pp. 16-31.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>905548</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Watkins, G. S., A Real-Time Hidden Surface Algorithm, PhD. thesis, Univ. of Utah, 1970.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Whitted, T., "Hardware Enhanced 3-D Raster Display Systems", Canadian Man-Computer Communications Conference, June 1981.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Panel Introduction CUSTOM VLSI CHIPS FOR GRAPHICS Robert Sproull (chair) Carnegie-Mellon University 
 The flexibility of custom VLSI designs is beginning to be felt in computer graphics. Special-purpose 
chips are useful in many of the low-level operations required to generate or modify images: geometric 
calculations such as transformation and clipping, line drawing, polygon filling, BitBlt (aka RasterOp), 
and even hidden-surface elimination. In many cases, the advent of VLSI means not only graphics systems 
with fewer chips, but entirely new display architectures, especially for raster-scan displays. The panel 
represents several research projects leading to new display architectures and to new chip designs. These 
are experiments in design and architecture, usually requiring the fabrication of prototype chips to test 
out the ideas. Although the designs are carried out completely by the researchers, fabrication is usually 
done with a "multi-project chip" fabrication technique that combinines many designs on a single wafer 
to save time and expense. (Those interested in more information about multi-project chips and the burgeoning 
extension of VLSI design into universities and small companies should consult the magazine "Lambda," 
Redwood Systems Group, P.O. Box 50503, Palo Alto, Calif. 94303). There is also evidence of commercial 
interest in special-purpose graphics chips: NEC has introduced a part, the uPD7220, designed to control 
modest-performance frame buffer displays (Electronics, April 7, 1981, p. 153). Panel Introduction (continued) 
 A SYSTEM DESIGN REVOLUTION James H. Clark Stanford University System design is undergoing radical 
changes that are significant both for the graphics community and for other design communities in which 
the principal hardware resources are those obtained via catalogues of parts. The drive to reduce system 
costs forces us to design at the lowest level possible, i.e., on silicon; only in this way can we reduce 
the number of packaged components, thereby reducing the number of interconnecting wires, board space 
and power requirements. This, in turn, presents a system design dilemma. The large cost of designing 
silicon systems, i.e. VLSI, conflicts with the need to design them to reduce cost. As a result, novel 
new design aids are being devised that actually make intearated system design easier than conventional 
system design; integrated systems can be thoroughly simulated because simple models of them exist at 
every level of representation. By providing the system designer with lower-level design primitives, 
significant new architectures, such as the Geometry Engine and Image Memory Processor, will be the result. 
Semiconductor companies cannot make system design decisions for every area of application; only the system 
designer understands the application area well enough to make the correct decisions. The traditional 
semiconductor industry must make a transition from offering packaged system building blocks to offering 
fabrication services to the designers who know how to design systems. In this presentation, we will 
outline the history of the design of the Geometry System and describe its characteristics. References 
 Clark, J. H., "A Revolution in Hardware System Design and its Implications for the Graphics Community", 
invited paper, EuroGraphics 81, to appear, September, 1981. Clark, J. H. and Hannah, M. R., "A High-performance 
Smart Image Memory", LAMBDA, 3rd Quarter, 1980. Clark, J. H., "A VLSI Geometry Processor for Graphics", 
Computer, July, 1980. Hennessy, J. L., "SLIM -Stanford Language for Implementing Microcode", Technical 
Report, Computer Systems Lab, Dept. of Electrical Engineering, Stanford University. Panel Introduction 
(continued) PIXEL-PLANES: A VLSI-ORIENTED DESIGN FOR A RASTER GRAPHICS ENGINE Henry Fuchs and John Poulton 
 University of North Carolina at Chapel Hill We will discuss a VLSI-oriented design for a special-purpose 
graphics engine capable of rapidly rendering shaded three-dimensional images from a polygonal data base 
on a raster-scan color video display. The system achieves its speed by performing the most time-consuming 
calculations with special hardware at each pixel memory cell. These calculations (for visibility and 
color rendering) are achieved in a distributed fashion which requires only a pair of one-bit adders and 
a one-bit storage element at each pixel. In our design, this circuitry is combined with the storage elements 
required for a frame buffer, with only a small penalty in additional silicon area over a conventional 
memory design. Visibility and painting calculations generally place a great burden on the computational 
resources of a graphics system, entailing separate calculations for the 2"'18 to 2**20 pixels in the 
image. Other portions of the computation required for scene rendering, including coordinate transformations, 
clipping, perspective scaling, and lighting, require calculations increasing in number only as the number 
of polygons in a scene. These calculations can readily be carried out in real time by many current graphics 
systems, and such a system will serve as a host processor for our graphics engine. The Pixel-planes 
system consists of two parts: i) a special-purpose computer (the "pre-processor") which converts polygon 
data from the host into a form suitable for transmission to 2) a set of identical "smart" memory chips. 
The system performs visibility calculations using the depth-buffer (z-buffer) algorithm and can also 
execute a Gouraud-like shading algorithm. Important features of the system are: * Visibility and painting 
calculations are performed polygon-by-polygon rather than in scanline order.  * The expected polygon 
processing time during image generation is easily as fast as current real-time line-drawin~ systems. 
 * Processing time for a polygon is independent of size and orientation of that polygon, and increases 
only linearly with the number of vertices. Convex polygons with any number of vertices can be processed. 
 * The "smart" frame-buffer memory, implemented in a number of identical chips, is easily expandable 
to accommodate increased screen resolution.  * The design retains the regularity and simplicity of 
conventional memories and can in order take advantage of well-developed to make layout, testing, and 
communication techniques for memory system design relatively simple. The chips of experiences this system 
have recently with them. been fabricated; we will describe our BIBLIOGRAPHY Clark, J. H., "A VLSI Geometry 
Processor for Graphics", Computer, July, 1980. Clark, J. H. and Hanna, M. R., "A High-performance Smart 
Image Memory", LAMBDA, 3rd Quarter, 1980. Fuchs{ H. and Johnson, B., "An Expandable Multiprocessing 
Architecture for Video Graphics", Proceedings of the Sixth Annual ACM-IEEE Symposium on Computer Architecture, 
April, 1979. Clark, J. H., "Distributed Processing in a High-Performance Smart Image Memory," LAMBDA, 
Vol. I, No. 3, 1980. Gupta, S., Sproull, R. F. and Sutherland, I. E., "A VLSI Architecture for Updating 
Raster-Scan Displays," Computer Graphics, Vol. 15, No. 3, 1981 (this volume). Panel Introduction (continued) 
LSI-ENHANCED 3-D RASTER DISPLAY SYSTEMS Turner Whitted Bell Laboratories Shaded display of three dimensional 
objects has been accomplished in a variety of hardware and software systems. Special hardware has typically 
been developed to satisfy a requirement for real-time performance, while most display software is at 
least two orders of magnitude slower. There are applications for which performance of 1/30th or 1/60th 
of real time (i.e. one frame per second) gives a sufficient level of interaction to be useful. Attempts 
to provide this medium level of performance have used special purpose microcode for conventional computers, 
or bipolar microprocessors linking the host computer with the display device. An extension of this approach 
is to directly convert time consuming portions of the display code into special purpose LSI chips. 
One such chip executes the scan line visibility tests associated with the well known z-buffer algorithm, 
removing all of the pixel-by-pixel operations from the host. An identical chip is used to interpolate 
intensity values for smooth shading. A circuit containing both chips can be incorporated into a frame 
 buffer memory. Statistics gathered from a software simulation of a system using this circuit show that 
the chip can reduce execution times by as much as i0:i when the average projected area of polygons in 
a scene is high compared to the number of polygons. The system's performance is detailed in reference 
[14]. References [i] Catmull, Edwin, A Subdivision Algorithm for Computer Display of Curved Surfaces, 
PhD thesis, University of Utah, 1974. [2] Clark, James H., and Hannah, Marc R., "Distributed Processing 
in a High- Performance Smart Image Memory", Lambda, vol. l, no. 3, 1980. [3] Cohen, D., and Demetrescu, 
S., A VLSI Approach to Computer Generated Imagery, Technical Report, USC, 1979, oral presentation by 
Dan Cohen made at SIGGRAPH '80, Seattle, July 1980. [4] Eastman, Jeffrey, A Efficient Scan Conversion 
and Hidden Surface Removal Algorithm Computers and Graphics, Vol. i, No. 2, Sept. 1975, p. 215. [5] 
Fuchs, Henry, "Distributing a Visible Surface Algorithm Over Multiple Processors", Proc. ACM Annual Conf., 
Seattle, Oct. 1977. [6] Gouraud, Henri, "Continuous Shading of Curved Surfaces", IEEE Trans. Cmptrs., 
C-20 (June 1971), 623. [7] Jackson, J.H., "Dynamic Scan-Converted Images with a Frame Buffer Display 
Device", Computer Graphics, 14,3, (July 1980), 163. [8] Myers, Allan J., An Efficient Visible Surface 
Algorithm, Report to NSF, Grant No. DCR 74-00768 A01, Computer Graphics Research Group, Ohio State University, 
July 1975. [9] Parke, Frederic I., "Simulation and Expected Performance of Multiple Processor Z-Buffer 
Systems", Computer Graphics, Volume 14, No. 3 (July 1980), pp. 48-56. [i0] Parke, Frederic I., Performance 
Analysis of Z-buffer Convex Tiler Based Shaded Image Generation, Tech. Rep. CES 79-15, Case Western Reserve 
University, October 1979. [ii] Roman, Grui-Catalin, and Kimura, Takayuki, "A VLSI Architecture for Real- 
Time Color Display of Three-Dimensional Objects", Proceedings of the Delaware Valley Microprocessor Conference, 
April 1979, pp. 113-118. [12] Schachter, Bruce, and Ahuja, Narendra, "A History of Visual Flight Simulation", 
Computer GraPhics World, Vol. 3, no. 3 (May 1980) pp. 16-31. [13] Watkins, G.S., A Real-Time Hidden 
Surface Algorithm, PhD. thesis, Univ. of Utah, 1970. [14] Whitted, T., "Hardware Enhanced 3-D Raster 
Display Systems", Canadian Man- Computer Communications Conference, June 1981. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806793</article_id>
		<sort_key>83</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Effective use of color in computer graphics]]></title>
		<page_from>83</page_from>
		<page_to>90</page_to>
		<doi_number>10.1145/800224.806793</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806793</url>
		<abstract>
			<par><![CDATA[<p>Color is a significant component of computer aided visualization of information, concepts and ideas. The use of color in all applications of computer graphics enhances the image, clarifies the information presented, and helps distinguish features that are obscure in black and white pictures. Color is used to differentiate elements in the diagrams so that the comparative information is read and understood rapidly and accurately. Color visualization techniques increase the amount of information that can be integrated into the visual message or picture, and thus create layers of information. It is clear that research and discovery can be supported and enhanced by color application, or inhibited by ineffectual use of color. In order to effectively utilize color in the visualization of ideas, information, or concepts, the role of color in various applications must be examined, and the perceptual behavior of color must be delineated. The basic principles of color theory have been discussed to provide a fundamental understanding of the characteristics of color and the manner in which colors interact with one another. Based on these principles, the user will be better able to utilize color in an effective manner relative to specific applications in color graphics.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P331414</person_id>
				<author_profile_id><![CDATA[81332532476]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Joan]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Truckenbrod]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Art, Northern Illinois University, DeKalb, Illinois]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Albers, Josef. Interaction of Color. Yale University Press, New Haven, Connecticut, 1963]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Birren, Faber (Ed.). Itten, The Elements of Color. Van Nostrand Reinhold, New York, 1970.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Birren, Faber (Ed.). Munsell, A Grammer of Color. Van Nostrand Reinhold, New York, 1969.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Birren, Faber (Ed.). Ostwald, The Color Primer. Van Nostrand Reinhold, New York, 1969.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Itten, Johannes. The Art of Color. Van Nostrand Reinhold, New York, 1961.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Jacobson, Egbert. Basic Color. Paul Theobald, Chicago, 1948.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Levitan, Eli L. Electronic Imaging Techniques. Van Nostrand Reinhold, New York, 1977.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Martin, Michael &amp; John Chesterman. The Radiant Universe: Electronic Images From Space. Macmillan Publishing Co., Inc., New York, 1980.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807502</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Meyer, Gary W. &amp; Donald P. Greenberg. "Perceptual Color Spaces for Computer Graphics." Computer Graphics, Vol. 14, No. 3. Siggraph '80 Conference Proceedings, J. Thomas (Ed.). Association for Computing Machinery, New York, July 1980.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Munsell, A. H. A Color Notation (12th Edition). Munsell Color Company, Baltimore, Maryland, 1971.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807361</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Smith, A. R. "Color Gamut Transform Pairs." Computer Graphics, Vol. 12, No. 3, August 1978.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 EFFECTIVE USE OF COLOR IN COMPUTER GRAPHICS Joan 
R. Truckenbrod Department of Art Northern Illinois University DeKalb, Illinois ABSTRACT Color is a 
significant component of computer aided visualization of information, concepts and ideas. The use of 
color in all applications of computer graphics enhances the image, clarifies the information presented, 
and helps distinguish features that are obscure in black and white pictures. Color is used to differentiate 
elements in the diagrams so that the comparative informa- tion is read and understood rapidly and accurately. 
Color visualization techniques in- crease the amount of information that can be integrated into the visual 
message or picture, and thus create layers of information. It is clear that research and discovery can 
be supported and enhanced by color application, or inhibited by ineffectual use of color. In order to 
effectively utilize color in the visualization of ideas, information, or concepts, the role of color 
in various applications must be examined, and the perceptual behavior of color must be delineated. The 
basic principles of color theory have been discussed to provide a fundamental understanding of the characteristics 
of color and the manner in which colors interact with one another, Based on these principles, the user 
will be better able to utilize color in an effective manner relative to specific applica- tions in color 
graphics. i. INTRODUCTION Color is playing a dynamic role in the applica- tion of computer graphics 
to experimentatio~ and research in the Sciences as well as in the Arts. Computer aided visualization 
and image proces- sing are significantly enhanced with the inte- gration of color technology. Historically, 
the evolutionary development of optical devices such as microscopes, electron microscopes, tele- scopes 
and cameras have enabled us to contin- ually expand our knowledge and understanding of Permission to 
copy without fee all or part of this material is granted provided that the copies are not made or distributed 
for direct commercial advantage, the ACM copyright notice and the title of the publication and its date 
appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy otherwise, or to republish, requires a fee and/or specific permission. &#38;#169;1981 ACM O-8971-045-1/81-0800-0083 
$00.75 the world. Computer graphic systems are a natural extension of these optical systems and facilitate 
the discovery of additional layers of information that have been previously inaccessible. Elec- tronic 
imaging systems, working in conjunction with digitizers, scanners and digital image processing systems, 
provide a valuable tool for enhancing images, and for visualizing structures and phenomena normally expressed 
in numeric formulas or algebraic expressions. In black and white photographs we can only distinguish 
the difference between thirteen levels of gray from black to white. We can, however, distinguish between 
a very large number of colors. Conse- quently, color extends the amount of perceptable information that 
can be injected into, or ex- tracted from, a visual image. The intrinsic nature of color adds a new dimension 
to visual imaging techniques. The varied character of color provides the potential for creating unlimited 
 effects with color that can accurately describe and delineate various aspects of a visual image. Color 
is a major element in visual images or compositions. A composition consists of a set of parts, figures 
and spaces, and the interaction between these components. These elements, in- cluding color, are related 
to one another in a manner than creates a sense of balance, harmony, rhythm and proportion. Visual dynamics 
such as motion and directionality result from the inter- action of the compositional components and their 
structure. Compositional elements have various characteristics that differentiate them from one another. 
In black and white pictures, distinc- tions are made between these elements according to the visual cues 
of size, shape, position, orientation and gray value. These visual cues allow the viewer to organize 
and comprehend the image. Color adds an entirely new dimension to a composition and provides information 
beyond that encoded in the normal visual cues. Color expands the capability of the visual composition 
in communicating ideas by providing more detailed information. In order to effectively utilize color 
in the visualization and enhancement of images, the role of color in visual compositions must be examined 
and the perceptual behavior of color delineated. In many instances, color used in computer graphics has 
been chosen arbitrarily. An awareness of the principles of color theory provides a fundamental understanding 
of the visual characteristics of color and the manner in which Computer Graphics Volume 15, Number 3 
August 1981 they interact with one another. This knowledge visible, irregardless of the difference between 
will optimize the use of color in color graphics. the original colors. 2. BASIC CHARACTERISTICS OF COLOR 
2.2 Value Major color theorists such as Munsell and Ostwald have developed three-dimensional models 
that organize "all possible colors" into a system, catagorizing colors by three basic color charac- teristics; 
hue, value and chroma. These models, or color solids, provide a classification system that facilitates 
the specification of any color based on these three clearly distinguishable aspects of color. Hue, value 
and chroma can be used to clarify elements in a composition as well as to provide more detailed information. 
For instance, blue can be used to identify specific elements, while variations in the lightness or darkness 
of the blue can be used to give more information about the elements. 2.1 Hue  Hue refers to the specific 
color family such as red, blue, green or yellow. Munsell, in his book, A Color Notation, defines hue 
as the distinctive characteristic of any chromatic color that distinguishes it from other hues. Ostwald 
describes hue as a distinct color sensation such as redness or blueness. Hue scales illustrate the transition 
from one hue to another as the proportion of each hue is increased or decreased. Gradual changes in hue, 
as shown in Figure i, create a subtle blending of hues in which boundries between the hues are obscured. 
A sharper contrast in the hues chosen for the hue progression in Figure 2, creates a clear transi- tion 
in the density of each hue. Depending on the degree of similarity of difference between the hues chosen 
for the hue progression, the effect of three-dimensional spaces and figures can be created. This gradation 
technique is similar to shading techniques used with paint or pencil. All of these illustrations were 
created on a Tektronix 4027 color display terminal in the process of exploring the potential of this 
system for creating various color effects. These scales consist of thirteen steps of incre- mental color 
mixing. Two hues are progressively "mixed" together by sequentially increasing the density of color dots 
of Hue A on a background of Hue B. On the Tektronix terminal, this is accomplished by defining patterns 
with a back- ground hue and a specified distribution of dots of the foreground hue. By using this technique 
of varying the density of different color dots on the display screen, new hues are created by optical 
mixing, similar to the technique used in painting by Seurat and other French Impressionist painters. 
The series shown in these illustrations begin with a square of background color and pro- gresses in twelve 
steps to a 50% mixture of fore- ground and background hue. When using this technique to create color 
gradation, the dots may or may not be apparent to the viewer, depending on the similarity between the 
foreground and background colors. However, at the midpoint of the color progression when the distribution 
of colors is equal, the foreground dots are not The value of a color specifies its lightness or darkness 
in relation to black or white. Munsell describes value as the brightness, brilliance or luminosity of 
a color in which a color may be of dark, light or middle value. Ostwald discusses value in terms of a 
percentage of white and/or a percentage of black that is present in a color. Itten, a more contemporary 
theorist, discusses value in terms of the contrast between light colors and dark colors, in which this 
contrast represents some of the fundamental polarities in life such as the difference between night 
and day. The value scales in Figures 3 and 4 illustrate gradual and dynamic changes in value. This change 
in value is created by changing the proportion of light to dark dots. Contrasts in value can be used 
to delineate differences as well as to create spatial effects. Light areas in a composition imply a source 
of light or lightness, while dark areas imply a sense of depth or weight.  2.3 Ehroma Chroma refers 
to the saturation of a color in terms of its purity. Munsell defines chroma as the strength or weakness 
of a color. In regard to chroma, a color can be described as weak, moderate or strong in intensity, such 
as dull red or vivid red. In the Ostwald color system, each color is specified by the percentage of white, 
the percentage of black, and the percent- age of full color. The higher the percent of full color, the 
higher the chroma. Itten refers to chroma as the saturation of a color. Accord-ing to Itten, a contrast 
in saturation is the difference between a color in its pure, intense state and a color in its diluted 
state. The chroma or intensity of a color is to be distin- guished from the value or lightness/darkness 
of a color. The value of a color is changed by the addition of white (light colors) or black (dark colors). 
The chroma of a color can be reduced in two ways. First, the intensity of a color can be reduced by adding 
a gray of the same value. This method creates a very drab color. Another method of chroma reduction is 
the addi- tion of a complementary color of a similar value. The chroma scale in Figure 5 illustrates 
a reduction of chroma in the red achieved by pro- gressively increasing the proportion of green, the 
complement of red. This method is preferred because a richer color is maintained as the chroma is reduced. 
The chroma scale in Figure 6 uses colors in which the hue and value specifi-cation on the Tektronix system 
were kept constant while the chroma (saturation) specification was changed from 50% to 100% in creating 
the scale. Variation in chroma can be used to differentiate elements in a composition as well as to create 
the effect of three-dimensional figures in space. 2.4 Tektronix Color Specification System Tektronix 
has adopted Ostwald's color system for color selection and specification on their 4027  Computer Graphics 
Volume 15, Number 3 August 1981 color display terminal. This color solid con- sists of a double cone 
with white located on the top apex of the cone and black at the opposite apex. The gray value scale is 
located on a vertical axis in the center of the solid that connects the two apexes. Consequently, equal 
value colors are located on horizontal planes in this color solid. The chroma of a color is determined 
by the horizontal distance from the central axis. Dull colors with low chroma are located near the center 
of the solid, while more intense colors with high chroma are located towards the outer edge of the solid. 
The high- est chroma colors are located on the outer edge of the solid at the equator. This organization 
of colors, with the most intense hues located on the same horizontal plane at the equator, implies that 
the highest chroma hues all have the same value, or lightness/darkness. In reality, however, we find 
that high chroma, intense yellow is lighter than a high chroma blue and, consequently, yellow should 
be located closer to the top of the solid and blue closer to the bottom of the solid. Bright orange is 
lighter in value than bright purple, while intense red is the same value as intense green. Conse- quently, 
red and green are appropriately positioned on the equa tor of the solid. It is important to be aware 
of this phenomenon as the use of intense yellow and blue will create a contrast in value that is not 
present if you use bright yellow and orange, or bright purple and blue. 3. COLOR IN COMPOSITION The 
integration of color into the visual imaging process expands our capability of visualizing complex mental 
constructs and communicating sophisticated ideas. A poor choice of color can obscure the essential information 
or idea. Color choices and placement must be congruent with, and supportive of, the objectives of the 
visualization. 3.1 Color Relativity The appearance of a color is relative and is dependent o~the color 
of its background. The manner in which the hue, value and chroma of color are perceived varies in accordance 
with the character of the surrounding colors. Colors are dynamically perceived as they con- tinuously 
interact with one another. Josef Albers delineates this phenomenon in his book, Interaction of Color. 
It is important to be aware of the relativity of color since the interaction between colors can be used 
to create the appearance of subtle differences be- tween design elements, as well as to create the appearance 
of more colors in a color display than actually exist. If it is necessary to main- tain the appearance 
of a color throughout a picture, surrounding colors must be chosen with care to avoid changes in color 
appearance pre- cipitated by the color environment. The back- ground color can influence the appearance 
of the foreground color(s) in various ways. The most obvious change in the character of a color occurs 
with the hue. Figure 7a shows a small purple square on a blue background and on a red back- ground. 
Close observation reveals that the fore- ground purple appears more as a blue purple on the red background 
and more as a red purple on the blue background. Even though there are only three colors used in this 
diagram, it appears that there are four colors. This illustrates the phenomenon in which the background 
color appears to subtract itself from the foreground color. The adjacent set of squares in Figure 7b 
 shows that the center color on both squares is identical. If the foreground and background colors are 
located opposite one another on the color wheel, the foreground color takes on characteristics of the 
complement of the back- ground. For example, in Figure 8 the foreground orange is intensified when placed 
on blue or purple. The value level or lightness/darkness of the background color can also have an effect 
on the appearance of the foreground color. A dark background makes a color tend to appear lighter than 
it really is and a light background makes a color appear darker. Figure 9 illus- trates the relativity 
of color in relation to the lightness or darkness of the background color. The saturation, or chroma, 
of the back- ground color has a similar effect. A medium bright color can be manipulated to appear more 
intense by placing it on a very dull background. It can also be manipulated to appear less in- tense 
by placing it on a more intense, more vivid background. Figure 10 illustrates an increase in the intensity 
of the foreground color on the dull background (left) and an appearance of a duller foreground color 
(right) on a brighter background. In order for these color changes to occur, the background area must 
be considerably larger than the foreground figure.  3.2 Color Contrasts In choosing colors for visualization, 
a color wheel is a helpful reference. Colors can be chosen for their varying degrees of similarities 
or differences to meet the needs of the image. High contrasts clearly delineate shapes by creating sharp, 
clearly discernable boundries. High contrasts imply a difference between ele- ments. In situations where 
false coloring is used to differentiate previously unclear grada- tions, the technique of using high 
contrast is successful. Low contrast in colors is used in blending and creates a sense of harmony and 
continuity. Color blending can be used to give figures a three-dimensional character. Spatial effects 
can be created by employing different levels of contrast. Johannes Itten discusses seven types of color 
contrasts in his book, The Elements of Color. One type of color contrast is that between warm and cool 
colors. Warm colors, reds and oranges, tend to advance in space and take a frontal position in a composi- 
tion. On the other hand, cool colors, blues and greens, tend to receed in space and take a back- ground 
position. Spatial effects can be created with warm/cool contrasts. Forms may even appear larger when 
they are rendered in red as opposed to being colored green or blue. Bright reds and oranges seem to radiate 
warmth and light and are thus appropriate colors for depicting light, warm spaces or objects. Blues and 
greens pro- ject a sense of coolness or a cool temperature.  Computer Graphics Volume 15, Number 3 August 
1981 These cool colors are passive and seem to imply a comfortable stability. Reds and oranges are very 
dynamic, active colors and seem to energize spaces and objects. Another type of color contrast to be 
explored is that between colors of different saturation or intensity. A sense of depth in space can be 
created by using variations in the degree of saturation. In landscape paintings, the fore- ground figures 
are commonly rendered in bright colors and the background colors progressively reduced in saturation 
as you move into the distance. This phenomenon can also be created on a color graphics display as shown 
in Figures 5 and 6. In Figure 5, red is made progressively duller by the addition of its complementary 
color green. Contrast in saturation can also be used to indicate the three-dimensional character of an 
object as the color of the object is most intense or bright where the light source hits it directly, 
and diluted as you move away from this direct light. Gray can also be directly mixed with a color to 
reduce its intensity. However, this latter process dilutes the color much more quickly. The purity of 
a color also changes as you add white or black. However, adding white or black also changes the value 
of the color as it becomes lighter or darker. As with contrasts in saturation, a contrast in value can 
also create a sense of depth in space. When working with a color graphics display such as the Tektronix, 
it is essential to recognize that all colors with a specified value of 50 do not necessarily have the 
same lightness or darkness on the screen. Variations in the three basic characteristics of color; hue, 
value and chroma can be used to add detailed information to a figure or to clarify an image, as contrasts 
in hue, value and/or chroma can each be used simultaneously to denote specific aspects of a figure 
in composition.  3.3 Color Harmony Harmony is a sense of continuity that is created in a composition 
by establishing a relationship between the compositional elements. In choosing harmonious color sets, 
the colors should relate to one another in a given manner. This rela- tionship can be established based 
on the rela- tive positions of the colors on the color wheel. Two techniques for choosing harmonious 
color sets are the use of a series of adjacent colors on the color wheel or the use of a pair of opposite 
(complementary) colors. Adjacent or neighboring colors have close similarities such as red and orange, 
while complementary colors exhibit a high degree of contrast in hue, such as blue and orange. Another 
technique for choosing harmonious color sets is the choice of colors that are found at equal intervals 
on the color wheel. This can be done by superimposing a square, rectangle or triangle (isosceles or equilateral) 
on the color wheel and then choosing the colors that appear at the vertices of one of the polygons. These 
polygons can be turned on the color wheel to aid in the determination of numerous color sets. A sense 
of harmony can also be created within a set of colors by maintaining equal value and/or chroma levels. 
 3.4 Color Balance Predominant components of an image can be given visual dominance by rendering them 
in a bright intense color on a duller, less bright back- ground. Light figures tend to stand out on a 
dark background. If visual balance is desired, attention must be paid to the quantitative proportion 
between two or more colors. Generally, a small area of intense color is balanced by a larger area of 
less intense color. Also, a small proportion of a light color, such as yellow, is balanced by a larger 
area of dark color, such as purple. Specific formulas for determining the appropriate proportions of 
colors are outlined in Munsell's color system and in Itten's study of color contrasts.  3.5 Visual Dynamics 
of Color Color choices are a major determinant in the manifestation of the visual dynamics of a composition 
such as rhythm, proportion, balance, directionality, and motion. Color applied in different ways to the 
same composition can create different focal points, different direc- tional flows, various even and uneven 
rhythms, and a change in predominance of figure/ground relationships. The choice and distribution of 
color in a visual image has a significant impact on the aesthetics and the manner in which an image is 
understood and interpreted. To dem- onstrate this phenomenon, a pattern was created on the Tektronix 
4027 color terminal and numerous color sets were then applied to the pattern to explore the influence 
and effects of color in the visual communication of the image. The basic elements in this pattern are 
a series of geomet- ric shapes that progressively transform from a square to a triangle, back to a square, 
and then to a parallelogram. The background fades from Color A to Color B to Color C. The appearance 
of the pattern changes dramatically as the colors change. The pattern in Figure II has a predom- inance 
of yellow which gives it a sense of light- ness. It appears to be a flat pattern in con- trast to Figure 
12 which has a sense of depth. Elements are visually grouped together based on their color assignment. 
Consequently, Figure ii has a series of bands radiating upward from the base which are broken up in other 
examples. The contrast between colors in each pattern deter- mines the focal point. As the color distribution 
changes, different shapes are emphasized and in some instances, the shape transformation is even obscured. 
Due to color choices and the amount of contrast, some of these patterns appear to be more active and 
vibrant than others. The similarity in color in Figure 12 creates a subtle modulation of the surface, 
while the differences in color in Figure 13 create a more dynamic surface vibration. The atmosphere created 
in Figure 14 reflects the character of blue color as well as the monochromatic color scheme. The elements 
in Figure 14 appear well integrated, while those in Figure 15 are very separate due to the diversity 
of colors used. Figure 16 illustrates the complementary colors of red and green with the addition of 
blue. The contrast of red and green does not have the same type of impact found in Figure 15, although 
it is a more  Computer Graphics Volume 15, Number 3 August 1981 pleasing color scheme. The color progressions 
in the background imply a sense of motion with a given directional thrust. These examples clearly show 
that color choices and color distribution in a visual image or picture have a significant impact on the 
presentation of the image.  4. CONCLUSION The choice of color in a visual image and the application 
of color to various areas affect the manner in which the image is perceived, and consequently effect 
the clarity and accuracy of the information in the image. Pictures of the natural world from devices 
such as telescopes, microscopes and optical scanners are generally very beautiful and have a sense of 
aesthetics. Artists and scientists who work with color graphics systems are continually faced with decisions 
that affect the aesthetics of the image and thus the communication of the ideas. It is thus essential 
that color be used effectively in order to present clear, accurate and well organized information in 
a visually pleasing manner. Color plays a major role in the manifes- tation of visual dynamics which 
contributes to the effectiveness and success of the visual image. In understanding the behavior of color 
in composition and the impact it has on the aesthetics of an image, color can be used effec- tively and 
sensitively in developing and enhancing color graphics displays. ACKNOWLEDGEMENTS The author wishes 
to acknowledge the Northern Illinois University Graduate Fund and Tektronix Corporation for their support 
of this project. REFERENCES Albers, Josef. Interaction of Color. Yale University Press, New Haven, 
Connecticut, 1963 Birren, Faber (Ed.). Itten, The Elements of Color. Van Nostrand Reinhold, New York, 
1970. Birren, Faber (Ed.). Munsell, A Grammer of Color. Van Nostrand Reinhold, New York, 1969. Birren, 
Faber (Ed.). Ostwald, The Color Primer. Van Nostrand Reinhold, New York, 1969. Itten, Johannes. The 
Art of Color. Van Nostrand Reinhold, New York, 1961. Jacobsen, Egbert. Basic Color. Paul Theobald, Chicago, 
1948. Levitan, Eli L. ~Electronic Imaging Techniques. Van Nostrand Reinhold, New York, 1977. Martin, 
Michael &#38; John Chesterman. The Radiant Universe: Electronic Images From Space. Macmillan Publishing 
Co., Inc., New York, 1980. Meyer, Gary W. &#38; Donald P. Greenberg. "Percep- tual Color Spaces for 
Computer Graphics." Computer Graphics, Vol. 14, No. 3. Siggraph '80 Conference Proceedings, J. Thomas 
(Ed.). Association for Computing Machinery, New York, July 1980. Munsell, A. H. A Color Notation (12th 
Edition). Munsell Color Company, Baltimore, Maryland, 1971. Smith, A. R. "Color Gamut Transform Pairs." 
Computer Graphics, Vol. 12, No. 3, August 1978.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806794</article_id>
		<sort_key>91</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Tablet-based valuators that provide one, two, or three degrees of freedom]]></title>
		<page_from>91</page_from>
		<page_to>97</page_to>
		<doi_number>10.1145/800224.806794</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806794</url>
		<abstract>
			<par><![CDATA[<p>The ability of the user of a graphics system to interactively control the motions of 3-D objects enhances his or her spatial perception and comprehension of those objects. This paper describes several logical devices based on the tablet, each particularly suited to control some type of interactive manipulation of 2-D and 3-D objects in real time. The &#8220;turntable&#8221; and the &#8220;stirrer&#8221; convert rotary motion of the tablet pen or puck into values for one-axis rotational control; the &#8220;rack&#8221;, a two-axis device, is used for scaling control; for three-axis rotational control, the tablet-based three-axis trackball is particularly suited.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[3-D graphics]]></kw>
			<kw><![CDATA[Graphics tablet]]></kw>
			<kw><![CDATA[Input devices]]></kw>
			<kw><![CDATA[Interactive graphics]]></kw>
			<kw><![CDATA[Light handle]]></kw>
			<kw><![CDATA[Rack]]></kw>
			<kw><![CDATA[Real-time graphics]]></kw>
			<kw><![CDATA[Stirrer]]></kw>
			<kw><![CDATA[Tablet-based trackball]]></kw>
			<kw><![CDATA[Turntable]]></kw>
			<kw><![CDATA[Virtual devices]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Input devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010391</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P160029</person_id>
				<author_profile_id><![CDATA[81332497661]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kenneth]]></first_name>
				<middle_name><![CDATA[B.]]></middle_name>
				<last_name><![CDATA[Evans]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Research Council Of Canada, Ottawa, Ontario, K1A 0R8]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P225388</person_id>
				<author_profile_id><![CDATA[81100035891]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Tanner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Research Council Of Canada, Ottawa, Ontario, K1A 0R8]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39085248</person_id>
				<author_profile_id><![CDATA[81100294917]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Marceli]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Research Council Of Canada, Ottawa, Ontario, K1A 0R8]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baecker R.M., Tilbrook D.M., Tuori M.I., McFarland D. NEWSWHOLE. SIGGRAPH '79 Videotape.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807394</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Britton E.G., Lipscomb J.S., Pique M.E. Making nested rotations convenient for the user. Computer Graphics 12(3):222-227, 1978.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988498</ref_obj_id>
				<ref_obj_pid>988497</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Status report of the Graphics Standards Planning Committee. Computer Graphics 13(3), 1979.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Newman W.M. A graphical technique for numerical input. Computer Journal 11(1):63-64, 1968.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Newman W.M. Markup. Videotape, Xerox PARC, 1976.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807385</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Staudhammer J. On display of space filling atomic models in real-time. Computer Graphics 12 (3):167-172, 1978.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Stone M. The Griffen demonstration tape, Videotape, Xerox PARC, 1981.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Tanner P.P. Dynamic graphical display of data. Proceedings 4th Canadian Man-Computer Communications Conference, 1975.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Tanner P.P., Evans K.B., Wein M. A survey of graphics tablet techniques. In preparation, 1981.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Tilbrook D.M. A newspaper pagination system. M Sc Thesis, Dept of Computer Science, University of Toronto, 1976.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807430</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Thorton R.W. The number wheel: a tablet-based valuator for interactive three-dimensional positioning. Computer Graphics 13 (2):102-107, 1979.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Wein M., Evans K.B., Tanner P.P. Videotape showing a range of interaction with 3-D picture segments. National Research Council of Canada, 1980, updated 1981.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 TABLET-BASED VALUATORS THAT PROVIDE ONE, TWO, OR THREE 
DEGREES OF FREEDOM Kenneth B. Evans, Peter P. Tanner, and Marceli Wein NATIONAL RESEARCH COUNCIL OF 
CANADA OTTAWA, ONTARIO K1A 0R8 ABSTRACT The ability of the user of a graphics system to interactively 
control the motions of 3-D objects enhances his or her spatial perception and comprehension of those 
objects. This paper describes several logical devices based on the tablet, each particularly suited to 
control some type of interactive manipulation of 2-D and 3-D objects in real time, The "turntable" and 
the "stirrer" convert rotary motion of the tablet pen or puck into values for one-axis rotational control; 
the "rack", a two-axis device, is used for scaling control; for three-axis rotational control, the tablet-based 
three-axis trackball is particularly suited. Key Words and Phrases: input devices, graphics tablet, 
interactive graphics, virtual devices, 3-D graphics, real-time graphics, turntable, stirrer, light handle, 
rack, tablet-based trackball° CR Categories: 4.41, 6.35, 8.2. 1.0 Introduction Originally conceived 
as a device for X,Y  position digitizing, tracing, and drawing, the tablet has proven to be a versatile 
device which can simulate many special purpose interactive devices very effectively and inexpensively. 
In recent years the tablet has taken on the job of all six interactive virtual devices described in 
the Status Report of the Graphics Standards Planning Committee [GSPC79]. These include the locator 
(the traditional use of the tablet), the pick (to select picture elements on the screen), the button 
(for selecting system defined parameters or functions), the stroke (for providing a series of location 
 values), the keyboard (alphanumerics), and the valuator (to determine a single real or integer value). 
Tablet-simulated valuators may take a variety of forms, just as there are many types of physical valuators 
such as joysticks and trackballs. Each of these tablet valuator techniques provides a unique style 
of interaction that is appropriate to a particular range of interaction requirements. Permission to 
copy without ~e all or part of this material is granted provided that the copies are not made or distributed 
~r divot commercial advantage, the ACM copyright notice and the title of the This paper describes several 
tablet-based valuator devices, each particularly suited to some aspect of interactive manipulation of 
3-D objects in real time. The turntable and the stlrre~r, one-axis devices, are useful for rotating objects 
around an arbitrary axis. The rack is a device for manipulating the scale of an object in two axes. The 
tablet-based trackball with three orthegonal motion sensors allows a very fluid interactive three-axis 
rotation. The physical characteristics and applications of each of these devices will be described, along 
with comments on the style of interaction resulting from their use. 2.0 Definitions and Terminology 
A few notes are needed at this point to clarify some of the terminology used in the paper. The term "tablet-based 
device" will refer to examples of different types of input techniques all implemented on a graphics tablet. 
These may or may not be analogous to a physical device. In addition, a given physical device, such as 
a trackball, may be simulated by a class of tablet-based logical devices, each with its own characteristics 
and each appearing to the user as a distinct device. The tablet maps the position of either a tablet 
stylus (pen) or flat button-pad (puc k ) into an X,Y location. Additional information is available as 
to whether or not the pen has been depressed on the tablet surface or a puck button has been pushed. 
The logical devices described in this paper are essentially mappings from the X,Y position into a single 
value, an ordered pair, or an ordered triple, depending on whether the device is to control one, two, 
or three axes. The mapping may also use information from previous X,Y readings. A device may operate 
in either rate or displacement mode. In the displacement mode, the outputs are used to control the magnitude 
of the parameter which is being manipulated (angle, scale factor, position). For example, when rotating 
in displacement mode, the image rotates only if, as a result of tablet pen movement, there is a change 
in the output value of the tablet. In the rate or velocity mode, the outputs are used to control the 
 publication and i~ date appear, and notice is given that copying is by permission of the Association 
~r Computing Machinery. To copy otherwise, or to republish, requires a ~e and/or specific permission. 
Computer Graphics Volume 15, Number 3 August 1981 rate at which an image moves in a manner analogous 
to the use of a rate controlled joystick. In this way continuous transformations can be set up and left 
running. Rate control is particularly useful for rotations where a continuous slow rotation is a helpful 
aid in understanding 3-D structures. Specific interactions for performing rate and displacement transformations 
are detailed in later sections. Whether in rate or displacement mode, the tablet can be operated either 
by sensing the absolute position of the puck (position sensin~ mode) or by sensing only the motion of 
the puck (motion sensin~ mode or mouse mode). Position mode is the familiar use of the tablet; the tablet 
coordinates are used directly. When in mouse mode, the tablet values are only changed when the puck is 
moved with a button pushed, hence only relative motion of the puck is significant. This is described 
in more detail in section 5. Often a graphic manipulation requires the use of the tablet for several 
different operations, necessitating some method of switching from one method of interaction to another. 
The sets of buttons found on most tablet pucks are convenient for signalling to the system that the user 
wishes to switch from one operation to another, or from one stage of an operation to the next. In this 
paper, the terms X, Y, and Z axes will refer to the horizontal screen axis, the vertical screen axis, 
and an axis normal to the screen, respectively. the viewing plane. If selecting a preset centre of rotation, 
the user will merely engage in some form of menu selection process and then proceed to stage two. If, 
however, he wishes to place the centre of rotation at an arbitrary screen location, then a centre of 
rotation cue must be provided (Figure 2). The centre of rotation cue (the + symbol in Figure 2) is "attached" 
to the tablet puck so that the user may "drag" it while the appropriate puck button is pushed. When the 
tracker is appropriately positioned, pushing a second puck button fixes its location. Figure 1 A physical 
analogue of the turntable device.  3.0 The Turntable The turntable is a graphic logical input device 
designed for user control of the two-stage process of rotating an object about a single axis. The turntable 
facilitates the definition of the centre of rotation and subsequently gives the user direct control of 
the angle of rotation. Forms of this device have been used in various systems [Tann75], [StonSl]. In 
practice, the use of the device is analogous to positioning a turntable under the object which is to 
be rotated, so that the object rests on the surface of the turntable. The turntable can then be rotated, 
causing the object to rotate about the turntable centre (Figure I). As mentioned above, activation of 
the turntable is a two stage operation - positioning the turntable or central axis, and then rotating 
the object. Corresponding to these steps are two interactive dialogues or procedures, one to establish 
the axis and the second to control either the angle or the rate of rotation. The first stage, positioning 
the axis, is accomplished in one of several ways, depending upon whether the user is picking a preset 
origin for the rotation (such as the screen centre, the centroid of the object, or a previously tagged 
point within the object) or is visually selecting an axis which passes through an arbitrary point and 
is normal to l I I + Figure 2 Turntable -Phase 1. Positioning the centre of rotation cue (+). In 
the second stage the user is presented with a pair of vectors (Figure 3) - a fixed horizontal reference 
vector (BA) which shows the zero degree position, and a rotating vector (BC) which shows the total angular 
displacement of the object. While the appropriate puck button is depressed, changes in the angular displacement 
of the tracker position (*) about the centre cue (+) are used to rotate the object. The vector BC is 
rotated to reflect this change in the angle of rotation of the object. The user then sees the rotation 
of the selected object as well as a display of the two vectors BA and BC which gives him a direct visual 
feedback of the angular displacement applied to the object.  Computer Graphics Volume 15, Number 3 August 
1981 S ,11 -Oj +  B A / Figure 3 Turntable -Phase 2. Rotating the object. (*) is the tablet puck position, 
CBA is the angle of rotation. One full rotation of the tracker around the centre cue, in response to 
a similar motion of the puck, causes THETA to go through 360 degrees. Normally, THETA directly determines 
the object rotation. Two simple techniques are available for vernier motion, allowing the user to make 
fine adjustments. An automatic vernie~ effect results from varying the distance between the puck position 
 (*) and the centre cue (+); the greater thls distance, the more the puck must move to produce a given 
THETA. Alternatively, a sensitivity button may be used to change the mapping between THETA and the angle 
of rotation. Thus the object may be made to rotate N times faster or slowe~ than the puck. In the normal 
situation above, the user wishes to control the angular orientation of an object on the screen. There 
are situations - as, for example, those encountered in viewing complex 3-D molecules [Stau78] - where 
it is desirable to control the rate of rotation of the object so that the object moves continuously at 
a rate governed by the value of THETA. The automatic vernier effect here is useful, as moving the puck 
further from the centre of rotation cue, just above or below the zero rotation llne, will cause the object 
to rotate very slowly. The turntable has been discussed in terms of controlling rotations about the 
Z axis. The same technique is applicable to producing rotations about the X, Y, or any arbitrary axis. 
When positioning the centre of rotation cue for a rotation about an axis that lles in a plane parallel 
to Z=0, the system can be set to make a default assumption as to the depth or Z component of the axis, 
or provide a procedure for depth positioning. For many purposes, setting Z equal to the plane of projection 
is a natural default. There are a number of distinct interactive procedures for specifying the orientation 
of an axis in 3-D [Tann81], but these are outside the scope of this paper. Once the axis has been selected, 
by whatever  means, the rotation is controlled as if it were about the Z axis. The user still moves 
his pen around the centre of rotation; however, THETA is mapped into a rotation about the chosen axis. 
As the user moves the tracking cue (*) around the centre cue (+), where both cues remain in the plane 
 of the screen, the object rotates about the specified axis. Regardless of the orientation of the axis 
of rotation, the resulting interaction is natural and direct. The turntable improves upon the standard 
use of the number wheel for rotation by offering the ability to select an axis of rotation, by including 
a vernier capability, and by providing two forms of visual feedback: a perspective view of the object 
rotating about a selected axis, and a face-on view of the turntable display which shows the exact angle 
of rotation. 4.0 The Stirrer This device, also used primarily for single axis rotation control, converts 
circular motions of the pen into a series of values that can be used to control the rotation of an object. 
The user, by making stirring movements with the pen, can move an object around an axis (chosen as in 
section 3) in the direction of his hand motion (a variation of Newman's light handle [Newm68]). The technique 
uses the last three puck positions (Figure 4) to calculate a value for PHI, the change of direction at 
time i. The angle of rotation is then controlled by a function of PHI. A  \ "X J Figure 4 Extraction 
of curvature from pen motion. The stirrer is easier to use than the turntable as the user need not be 
concerned with moving the tracker around a centre point and is free to concentrate on the image he is 
manipulating. He has only to move the puck in an approximately circular manner to effect the rotation. 
When the user draws small tight circles, the rotation is quick; gentle curves result in slow object rotation. 
It does not matter w~ere the tablet puck is on the tablet surface, all that matters is the direction 
- clockwise or counterclockwise - and the instantaneous curvature as calculated from the last three points. 
Consequently, the user can keep his eyes on the rotating object and he needs little manual dexterity 
to control the device. If the user needs to see the current angle of rotation, that value can be included 
in the status area of the screen or the pictorial display from the turntable method can be invoked (Figure 
3). Computer Graphics Volume 15, Number 3 August 1981 5.0 Two-axis Trackball f Various authors have 
described the use of the tablet as a single number wheel or a pair of number wheels [Newm76], [Thor79]. 
Briefly, the plane of the tablet is regarded as having two orthogonal axes X and Y, parallel to the 
borders of the tablet. All pen motion (Figure 5) such as that from P(i-l) to P(i) (where P(i) is the 
current position of the pen at time (i)) is resolved into components dX and dY. These values can then 
be used to control a variety of transformations. Figure 6 shows,Thornton's use of a two-axis valuator 
to control rotation about the X and Y axes [Thor79]. This number wheel pair is equivalent to a two-axis 
trackball or a Stanford Mouse. This is a motion sensing technique that forms a basis for the stretching 
device (section 6) and is a component of the 3-D trackball (section 7). Pi Pi-I dx J Figure 5 Extracting 
tablet pen information to control two number wheels. -i Figure 6 Two number wheels for controlling two-axis 
rotation. F Figure 7 Setting the centre of the scaling operation. The centre of the + symbol will be 
the scale centre.  6.0 The Rack The rack, a generalization of a technique used in NEWSWHOLE [Tilb76], 
[Baec79], is a modification of the two-axis trackball and is designed to facilitate the scaling or stretching 
of objects. Scaling is a two-step operation. First the centre or origin of the scaling operation must 
be determined and then the magnitude of the scale factors can be established. To locate the scaling origin, 
the user, with the tablet in a select mode, picks a point on the screen. Once the user has chosen an 
origin (+) (Figure 7) he depresses a button on the number pad which fixes it and advances him to the 
next state. This new state is identified to the user by the appearance of the "identity cue" (.) representing 
a scale of 1.0 in both X and Y, and the tablet tracker (*). When the tablet tracker is superimposed on 
the identity cue, an identity scale factor results, and the resulting object is the same as the original. 
 The transformation process is applied to both X and Y. The X component is determined by XSCALE = (Xtracker-Xorigin) 
/ (Xidentity-Xorigin) As the tracker is moved away from the identity cue in an X direction the X scale 
increases (Figure 8). As the tracker moves back towards the origin, the X scale diminishes to 0 (Figure 
9). As the tracker moves past the origin in a negative direction, the result is a negative scale increasing 
in magnitude (Figure i0). This is equivalent to reflection about the Y axis followed by a scaling by 
the absolute value of XSCALE. In this technique, the user's hand actions are tightly coupled to the scaling 
in the two directions. A fluid and easily controllable interaction results. If the scale factor required 
would force the tracker to run off the screen, then the user has two options: to perform the scaling 
operation in two or more stages; to perform a menu selection which changes the sensitivity of the scaling 
operation.  Computer Graphics Volume 15, Number 3 August 1981 Variations of this technique are available 
that are essentially constraints to simplify the scaling operation, restricting the scaling to only the 
X axis or only the Y axis or coupling the two axes for distortionless scaling.   I.* I + I j Figure 
8 Increasing the X scale factor. S l J Figure 9 Reducing the X scale factor towards zero. f J Figure 
10 A negatt've X scale factor. Z Figure 11 Three-axis trackball. 7.0 The Three-axis Trackball The tablet-based 
three-axis trackball emulates a commercially available device -the two-axis trackball -to which has been 
added a third sensor so that rotation can be detected about each of three orthogonal axes simultaneously. 
The resulting physical three-axis trackball detects motion about the X axis (rolling the ball up and 
down), the Y axis (rolling the ball sideways), and the Z axis (twisting the ball clockwise or counterclockwise) 
(Figure ii). The full tablet-based three-axis trackball, which is described below, exhibits most of 
the features of the physical device, but in some ways it is easier to use. The advantage is principally 
in the improved simultaneous control of the three axes of rotation. It is awkward to combine rotations 
about Z with rotations about the other axes on the physical device; a rotation about an axis in the XY 
plane takes one finger to control; a rotation about the Z axis requires that one grip the ball with several 
fingers in order to twist it. This is because only a small portion of the sphere projects above the surface 
of the device. The tablet-based device does not have this problem. The puck buttons also favour the tablet-based 
device over the physical device. Buttons can be added to a trackball chassis, but they are more distant 
from the user's finger tips and hence take fractionally longer to use. As implied above, even with a 
three-axis trackball, the user often performs manipulations by alternating between rolling (XY) and twisting 
(Z) motions. Consequently there are two distinct ways to simulate a three-axis trackball: i. provide 
adjacent buttons on the number pad to allow instant alternations between XY and Z rotations 2. provide 
full simultaneous control of XYZ axes  Computer Graphics Volume 15, Number 3 August 1981 The alternation 
from XY to Z rotation modes is accomplished by pressing either the "XY" or the "Z" button on the number 
pad of the puck. The XY mode is simply the two axis trackball rotation about the X and Y axes as described 
in section 5. The Z mode uses the stirrer of section 4.  The true simultaneous control of the XYZ axes 
of rotation is accomplished by combining the two-axis traekball with the stirrer such that the former 
controls X and Y, while the latter controls Z rotation. In order to provide the resulting device with 
the "feel" of the physical device, it is necessary to ensure that relatively straight line motions of 
the puck produce no Z rotations and that conspicuous attempts to "stir" the puck produce only Z rotation. 
In short, the outputs of the two component techniques require modification so that the X and Y rotations 
are reduced when PHI (Figure 4) is large, and the rotation about Z is made negligible when PHI is small 
[TannSl]. The result is that regardless of where the user locates the puck, motions in a straight line 
cause rolling about an axis parallel to the plane of the screen and orthogonal to the direction of motion; 
motions which are basically circular produce rotations about an axis normal to the screen. It is also 
easy to mix these two types of hand motion to produce rotation about all 3 axes at once, because the 
technique exhibits what Britton et al [Brit78] call "kinesthetic correspondence": there is a direct and 
natural relationship between each of the three distinct hand motions and the corresponding rotations 
about the three axes. Several minor advantages accrue to this device: I. true simultaneity is easier 
 2. Z motions are easily made continuous  3. the surface of the tablet-based trackball is much larger 
and hence longer "finger strokes" result  4. precise location of the manipulating hand is less critical 
because control motions can be made anywhere on the tablet surface  5. changing logical devices or modes 
within such a device does not require that the user move his hand, let alone swap gadgets   In practice, 
the most versatile approach to the tablet-based trackball is to provide (among other possible functions) 
the following six buttons so that the user can switch instantly into whatever mode is convenient: -XYZ 
(full three-axls control) - XY (two-axis traekball for X and Y) -Z (stirrer for Z control) -X (one-axls 
trackball for X) - Y (one-axis traekball for Y) -VELOCITY/DISPLACEMENT (toggle) The X and Y buttons 
simulate a trackball in which software has disabled the Z and Y or X sensors. X axis and Y axis control, 
when available, is always achieved by means of the same hand motions, regardless of which mode is used; 
the same is made true for Z. The last button, "VELOCITY~DISPLACEMENT', switches modes as described in 
section 2.  8.0 Summary The ability of the tablet to mimic or emulate physical devices and to provide 
novel logical devices matched to specific applications renders it the most general purpose and cost effective 
graphic input device available. In most cases all other physical input devices, excepting the keyboard, 
can be eliminated. A tablet simulaton of a device must perform at least as well as the device it is replacing. 
This paper describes several techniques that meet this criterion and improve real-tlme interactive control 
of 3-D object manipulation, namely: the turntable and the stirrer for one-axis rotation, the two-axis 
trackball for two-axls rotation, the rack for two-axis scaling, and the three-axls trackball for three-axls 
rotation. With the tablet, the user need not turn his attention from the task at hand to press buttons, 
turn knobs, or manipulate a trackball. Consequently, when used to its full capability it is less tiring, 
causes fewer of the momentary breaks required to search for the appropriate physical device, and provides 
a less cluttered, less intimidating environment than any equivalent set of input devices. Finally, it 
should be mentioned that the handling of one device, rather than an array of physical devices, makes 
easier tasks of programming, debugging, and learning an interactive graphics system. 9.0 Acknowledgements 
Many of the techniques in this paper have, in one form or another, been in use for many years at the 
National Research Council of Canada, and at a number of other locations, for example Xerox PARC and the 
University of Toronto. We would llke to acknowledge the assistance of the members of both of these institutions 
who helped in the preparation of this paper. Tablet techniques have not been widely published as they 
are often cumbersome to convey in print. A full understanding is best acquired by live demonstrations, 
film or video-tape [Baec79], [Newm76], [Wein81], [Ston81].  10.0 References i. [Baec79] Baecker R.M., 
Tilbrook D.M., Tuori M.I., McFarland D. NEWSWHOLE. SIGGRAPH "79 Videotape. 2. [Brit78] Britton E.G., 
Lipseomb J.S., Pique M.E. Making nested rotations convenient for the user. Computer Graphics 12(3):222-227, 
1978.  ~6 Computer Graphics Volume 15, Number 3 August 1981 3. [GSPC79] Status report of the Graphics 
Standards Planning Committee. Computer Graphics 13(3), 1979.  4. [Newm68] Newman W.M. A graphical technique 
for numerical input. Computer Journal 11(I):63-64, 1968.  5. [Newm76] Newman W.M. Markup. Videotape, 
Xerox PARC, 1976.  6. [Stau78] Staudhammer J. On display of space filling atomic models in real-time. 
Computer Graphics 12(3):167-172, 1978.  7. [Ston81] Stone M. The Grlffen demonstration tape, Videotape, 
Xerox PARC, 1981.  8. [Tann75] Tanner P.P. Dynamic graphical display of data. Proceedings 4th Canadian 
Man-Computer Communications Conference, 1975.  9. [TannSl] Tanner P.P., Evans K.B., Wein M. A survey 
of graphics tablet techniques. In preparation, 1981.  i0. [Tilb76] Tilbrook D.M. A newspaper pagination 
system. M Sc Thesis, Dept of Computer Science, University of Toronto, 1976. ii. [Thor79] Thorton R.W. 
The number wheel: a tablet-based valuator for interactive three-dimensional positioning. Computer Graphics 
13(2):102-107, 1979. 12. [Wein81] Wein M., Evans K.B., Tanner P.P. Videotape showing a range of interaction 
with 3-D picture segments. National Research Council of Canada, 1980, updated 1981.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806795</article_id>
		<sort_key>99</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[A methodology for the specification of graphical user interface]]></title>
		<page_from>99</page_from>
		<page_to>108</page_to>
		<doi_number>10.1145/800224.806795</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806795</url>
		<abstract>
			<par><![CDATA[<p>We present a methodology for the design of interactive user interfaces. This methodology provides the designer with a number of steps to be followed in the design of a user interface. Examples of a formal notation for describing user interfaces are presented. This methodology also provides the designer with a number of techniques for evaluating his design. We present a list of desirable properties for a user interface design methodology. Our design methodology has all of these properties.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Human factors]]></kw>
			<kw><![CDATA[Man-machine interfaces]]></kw>
			<kw><![CDATA[Software engineering]]></kw>
			<kw><![CDATA[Software specification]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>D.2.1</cat_node>
				<descriptor>Methodologies (e.g., object-oriented, structured)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Graphical user interfaces (GUI)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010865</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Graphical user interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011074.10011081.10011082</concept_id>
				<concept_desc>CCS->Software and its engineering->Software creation and management->Software development process management->Software development methods</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39077139</person_id>
				<author_profile_id><![CDATA[81339501983]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Green]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CSRG, University of Toronto, Toronto, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Boehm B. W., J. R. Brown, H. Kaspar, M. Lipow, G. J. MacLeod, M. J. Merrit, Characteristics of Software Quality, North-Holland, Amsterdam, 1978.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Green M., "A Specification Language and Design Notation for Graphical User Interfaces", TR 81-CS-09, Unit for Computer Science, McMaster University, 1981.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Heninger K. L., "Specifying Software Requirements for Complex Systems: New Techniques and Their Application", in Specification of Reliable Software, IEEE Computer Society, Long Beach CA, 1979.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Liskov B. H., V. Berzins, "An Appraisal of Program Specifications", in Directions in Software Technology (ed. P Wegner), The MIT Press, Cambridge Mass., 1979.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Liskov B. H., S. N Zilles, "Specification Techniques for Data Abstractions", IEEE Transactions on Software Engineering, SE-1, no. 1, p.7, 1975.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Newman W. M., R. F. Sproull, Principles of Interactive Computer Graphics, 2nd Edition, McGraw-Hill, 1979.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>361309</ref_obj_id>
				<ref_obj_pid>355602</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Parnas D. L., "A Technique for Software Module Specification With Examples", CACM, vol. 15, no. 5, p.330, 1972.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Reisner P., "Formal Grammar and Human Factors Design of an Interactive Graphics System", IEEE Transactions on Software Engineering, SE-7, no. 2, p.229, 1981.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>359483</ref_obj_id>
				<ref_obj_pid>359461</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Robinson L., K. N. Levitt, "Proof Techniques for Hierarchically Structured Programs", CACM, vol. 20, no. 4, p.271, 1977.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578504</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Zelkowitz M. V., A. C. Shaw, J.D. Gannon, Principles of Software Engineering, Prentice-Hall, New Jersey, 1979.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 A Methodology for the Specification of Graphical User 
Interface Mark Green* CSRG, University of Toronto Toronto, Canada Abstract We present a methodology 
for the design of interactive user interfaces. This methodology provides the designer with a number of 
steps to be followed in the design of a user interface. Examples of a formal notation for describing 
user interfaces are presented. This methodology also provides the designer with a number of techniques 
for evaluating his design. We present a list of desirable properties for a user interface design methodology. 
Our design methodology has all of these properties. Keywords: computer graphics, man-machlne interfaces, 
human factors, software engineering, software specification CR Categories: 8.2, 4.6 i. Introduction 
 The design of good interactive user interfaces is an important yet poorly understood process. At the 
present time the designer of interactive user interfaces has very few tools to draw upon for this task 
[6]. The purpose of this paper is to present a methodology for the design of user interfaces. This methodology 
suggests a sequence of steps to be followed in the design of user interfaces and provides the designer 
with a notational tool for describing and analyzing his designs. The field of software engineering deals 
with the efficient production of reliable ........................ *) Author's Current Address: Unit 
for Computer Science, McMaster University, Hamilton, Ontario, Canada, L8S 4KI. Permission to copy without 
fee all or part of this material is granted provided that the copies are not made or distributed for 
direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, 
and notice is given that copying is by permission of the Association for Computing Machinery. To copy 
otherwise, or to republish, requires a fee and/or specific permission. @1981 ACM O-8971-045-1/81-0800-0099 
$00.75 software. The work in this field is a logical starting point and source of inspiration for the 
 development of a user interface design methodology. Researchers in software engineering have suggested 
that software development should consist of six phases [I0]. These phases are: I) Requirements -In 
this phase a formal statement of the problem to be solved is developed. This statement includes the environment 
in which the program is tooperate and any constraints on its performance. Requirements are usually expressed 
in an informal manner (though formal requirements languages do exist [3]) and are developed in consultation 
with the user. The requirements definition states the properties that a system must have in order for 
it to be acceptable. Many programs may satisfy these requirements. 2) Specification -In this phase a 
formal description of the external behavior of the program is produced. This description includes the 
valid input to the program, the output produced in response to this input, error conditions, the modules 
in the program, and their interfaces. The specification does not mention the algorithms or data structures 
used in the program. The specification describes the particular program that is to be implemented. 3) 
Design -In this phase the internal description of the program is produced. This description covers the 
algorithms and data structures used in the program. 4) Implementation -In this" phase the modules designed 
in the previous phase are implemented (coded). 5) Testing -The individual modules and the complete program 
are tested in this phase. 6) Maintenance -This phase deals with any changes which must be made to the 
program after it has been released. The above sequence of phases is known as the software development 
cycle or the software life cycle. The methodology presented here deals mainly with the first three phases 
of the software development cycle. Several studies Computer Graphics Volume 15, Number 3 August 1981 
have shown that these are the most important phases from the viewpoint of reducing the number of errors 
made and the cost of these errors [i]. The specification phase is the earliest phase in the software 
development cycle that has a strong formal basis. Because of this our methodology will concentrate on 
this phase. Program specification involves creating an abstraction of the program being designed. This 
abstraction deals with one aspect of the program. The main reason for doing this is to reduce the amount 
of detail the designer must deal with at any one time. Two main abstractions have appeared in the software 
 engineering literature. The functional abstraction deals with the function performed by a program and 
its subroutines. This abstraction only deals with what the subroutines do and not with the algorithms 
or data structures used. The data abstraction deals with the semantics of and operations applicable to 
the data types in the program. Both of these abstractions are mainly concerned with precisely defining 
the computation performed by the program. More details on the techniques used for specifying these abstractions 
can be found in [4,5]. In designing an interactive user interface we are more interested in the interaction 
between the user and the program than in the computation performed by the program. Because of this the 
techniques developed by others for the specification of functional and data abstractions are not particularly 
well suited to the design of interactive user interfaces. We introduce a new type of abstraction, called 
the user abstraction, to cover the interaction between the user and the program. The user abstraction 
deals with such things as command languages, graphical input techniques, interactive graphical displays, 
and user models. To completely define a program all three types of abstractions must be specified. What 
are the benefits of using a formal design methodology? The obvious answer to this question is better 
user interfaces. This answer doesn't give us much guidance in the development of the design methodology. 
Instead we suggest three less ambitious goals. These goals indicate features that any design methodology 
for interactive user interface should possess. The three goals are: I) The methodology should provide 
a formal notation for describing user interfaces. This is the major contribution of any software design 
methodology. The act of constructing a complete description of a user interface will often highlight 
important decisions and indicate areas where the design is incomplete. A formal notation can be used 
for communications between designers. The description of the user interface can be used as a reference 
in design discussions. It can also be used as a reference for the programmers who must implement the 
user interface. Programmers should not make interface design decisions, therefore, a complete description 
of the interface must be available before implementation starts. The fact that it is a formal notation 
is important for two reasons. First, it ensures that all users of the notation will agree on the interpretation 
of a description of a user interface. Second, it allows for the contruction of programs which process 
the descriptions of user interfaces. In this paper we will not formally define a design notation. Due 
to the importance of this topic it warrants a separate paper. 2) The methodology should provide a mechanism 
for determining the "correctness" of a design before it is implemented. In practice it will be hard if 
not impossible to show that a design is totally correct. The best we can hope for is to eliminate the 
majority of the major bugs before the user interface is implemented. 3) The methodology should provide 
some mechanism for evaluating the effectiveness of a user interface for a particular class of users. 
This goal deals with the quality of the user interface. At the present time very little is known about 
what factors effect the quality of a user interface. The methodology does not provide a means of determining 
whether a particular factor contributes to the quality of a user interface. It only provides a means 
of measuring the quality once a set of quality standards have been established. In the rest of this 
paper we will present our design methodology and show how it satisfies these goals. 2. The Design Methodology 
 2.1 Outline of the Methodology Our design methodology is divided into two components. The first component 
is a formal description of the user or group of users who will use the program. This formal description 
is called the user model. The user model contains a description of how the user views the task domain 
(the problem to be solved). It also ~ontalns the user's conceptual model of how the user interface operates. 
In general each user will have a different conceptual model of the tasks to be performed and the operation 
of the user interface. In practice the user co--unity will be divided into a small number of groups. 
A user model must be developed for each of these groups. The second component of the methodology is a 
formal specification of the user interface. This component of the methodology does not start until after 
the first few steps of the user model have been completed. A special specification language is used for 
writing the specification of the user interface. Parts of this language and the general approach to specification 
will be presented in this paper. Computer Graphics Volume 15, Number 3 August 1981 The division of the 
design into two components has three advantages. First, there is usually more than one group of users 
and more than one user interface will satisfy their needs. Developing these two components separately 
requires less effort than developing all the user model user interface combinations. The second reason 
is that there is a natural division between the user model and the specification of the user interface. 
We should take advantage of this division to modularize the design process and reduce the amount of 
detail that must be considered at any one time. The third reason is that the user model is used as a 
control in our design methodology. By incorporating information about the operation and desirable features 
of the user interface into the user model we have a source of information which can be used in determining 
the correctness of the design. Any source of redundancy helps in determining the correctness (or at least 
consistency) of the design. To illustrate the steps in our methodology the design of a simple user interface 
will be presented. The purpose of this user interface is to aid the user in arranging a number of simple 
geometrical shapes on a two dimensional plane (work area). The shapes used are circle, square, and triangle. 
The design of an interface to solve this problem will be elaborated in the following two sections. 2.2 
User Model The starting point in any user interface design process is a study of the problem the user 
is trying to solve. The purpose of the user interface is to help the user in solving this problem. Most 
(if not all) problems can be decomposed into a number of tasks which when combined in the proper order 
solve the problem. These tasks are atomic (not decomposable) from the user's point of view. There are 
many possible task decompositions for any given problem. Each decomposition will give rise to a different 
user model. The process of decomposing a problem into a number of tasks is called task analysis. The 
first component of the user model is a formal description of the atomic tasks in the problem. This formal 
description is called a task model. The object of this step is to convert the informal task analysis 
into a formal description of the user's view of the problem. This formal description is in terms of objects 
and operators. Objects are the entities in the problem to be solved. They are the things the user wants 
to manipulate. The operators are the operations which can be performed on the objects. Usually there 
is a one-to-one correspondance between operators and tasks. The development of the task model serves 
two purposes. First, it supplies a notation which the designer can use when performing the task analysis 
and describing the problem the user is trying to solve. Second, it is the starting point for the rest 
of the formal notation. The notation we use for the task model has three components; object definitions, 
operator definitions, and invarlants. The object definitions state the properties of the objects in 
the task analysis. They are simillar to data structure definitions. The definition of an object consists 
of a list of attributes. Each attribute is either one of the primitive data types in the notation or 
another object in the model. The primitives in the notation include integer and real numbers, sets, booleans, 
points, and extents. Operator definitions state the conditions under which the operator can be applied 
and the effect it has on the objects in the model. An operator definition consists of a header and a 
body. The header contains the name of the operator and declarations of its operands. The body of the 
definition is divided into two sections. The first section contains a llst of logical statements giving 
the conditions under which the operator can be applied. These statements are called pre-condltlons. 
The second section contains a llst of logical expressions stating the effects of the operator. The variables 
in these expressions are the objects in the model. An operator can return a value. These values are used 
in the invarlants. Invariants are relations which must hold between the operators and objects in the 
model. These relationships express the user's view of how the problem domain operates. Invarlants may 
vary greatly from user to user. An example of the use of this notation can be found in appendix I. 
One task analysis of our example user interface suggests three atomic tasks. The first task creates a 
new geometrical object and places it in the work area. The second task moves an existing geometrical 
object to a new position. The third task removes a geometrical object from the work area. This task analysis 
suggests that there will be two types of objects and three operators in the task model. The two objects 
are the geometrical objects and the work area. Each geometrical object has three attributes; its shape, 
its position in the work area, and the area of the work area it covers. The work area object has two 
attributes; its size, and a llst of the geometrical objects that have been placed on it. The defintlons 
of these two objects can be found in appendix I. The three operators in the model are "place", "move", 
and "remove". They correspond to the three tasks in the task analysis. The pre-condition for the "place" 
task states that the point where the user wants to place the new object must be within the work area. 
The post-condltlons for this task state that the new object is included in the work area at the position 
indicated by the user. There is one invariant in this task model. This invariant states that all geometrical 
objects in the work Computer Graphics Volume 15, Number 3 August 1981 area must be physically inside 
the work area. That is, no part of the object can lie outside of the work area. It should be noted that 
the user will not normally be exposed to the formal description of his task model. This raises the question 
of how do we determine if the task model is accurate? There is no exact answer to this question. The 
best the designer can do is interview the user, and use his intuition and design experience to create 
the task model. The user will often not know what he wants and when he does it may not be the best thing 
for him. The second component of the user model, called the control model, is a description of how the 
user views the operation of the user interface. This is the first step in the actual design of the user 
interface. The major purpose of this step is to design a consistent model of how the user interface operates 
[6]. This model will be used by the user to predict the effect of a command or sequence of commands. 
The user will not see this model but will infer it from his experience with the user interface. The 
control model describes all the actions that the user can perform. These actions are described in terms 
of the objects in the task and control models. The structure of the control model is similar to that 
of the task model and the same notation is used for describing both models. The major difference between 
the two models is that the task model describes the tasks to be performed and the control model describes 
the user's view of the commands that are available to perform them. Like the task model the control model 
is made up of object definitions, operator definitions, and invariants. All the objects in the task model 
are implicitly included in the control model. The objects defined in the control model are the ones introduced 
by the user interface. This includes things like menus, tracking crosses, and digitizing tablets. The 
operators in the control model define the commands and actions provided by the user interface. These 
commands and actions might include selecting items from menus, dragging objects, selecting operands, 
and tracking the tablet cursor. The invariants in the control model state relationships which hold between 
the objects and operators in the model. The control model is a high level description of the user interface 
which will be specified in the next step of the methodology. An example control model can be found in 
Appendix 2. After the control model has been developed it should be checked for consistency with the 
task model. This is accomplished by constructing a sequence of control model operators which perform 
the same function as a task model operator. This sequence of operators is called the control level mapping 
for the given task model operator. If a control level mapping can be constructed for each operator in 
the task model then we say that the control model is consistent with the task model. One possible measure 
of the quality of a user interface is the complexity of the control level mappings. It would be expected 
that the more complex the mappings the harder it would be to use the user interface [8]. For our example, 
the user interface is based on the screen layout shown in fig. I. On the right side of the screen is 
a menu with the geometrical shapes available to the user. The left side of the screen is the work area. 
A digitizing tablet is used to select geometrical objects and drag them to the desired position in the 
work area. There are three com~aands in this user interface. The "place" command corresponds to the place 
operator in the task model. The user selects one of the shapes from the menu. A copy of the selected 
shape becomes the tracking cross and the user can drag it to any position in the work area. The user 
can deposit the geometrical object at the current cursor position by pressing a button on the tablet. 
The "move" command corresponds to the move operator in the task model. The user selects one of the geometrical 
objects in the work area and drags it to another position. The user then presses the tablet button to 
deposit it at that position. The "remove" command is used to remove geometrical objects from the work 
area. The user points at the objects he wants to remove and drags it off the work area. He then presses 
the tablet button to signal the end of the command. The control model for this user interface is in 
Appendix 2. Four new objects are introduced by the user interface. These objects describe the menu, tracker, 
and digitizing tablet. There are four operators in this model. The first two operators describe the action 
of selecting a geometrical object from the menu or work area. The next operator deposits the selected 
geometrical object in the work area. The last operator deletes a geometrical object from the work area. 
There are two invariants in this control model. The first states that all the items in the menu must 
lie within the menu. The second invariant states that the tracking symbol follows the movement of the 
tablet cursor /h I &#38;#169; Fig. 1 Computer Graphics Volume 15, Number 3 August 1981 2.3 User Interface 
Specification The control model provides a high level description of the user interface. The next step 
in the methodology is to produce a detailed description of the user interface. This detailed description 
will be refered to as the specification of the user interface. How detailed and complete should the 
specification of the user interface be? We could produce a specification which includes every last detail 
of the interface design. This may not be desirable since the excess detail may hide important design 
decisions. Also the larger the specification the harder it is to change it when a mistake is discovered. 
If not enough detail is supplied then two problems will occur. First, the programmer who is implementing 
the interface will be forced to make interface design decisions. Second, it will be impossible to evaluate 
the interface before it is implemented. For our purposes, the specification has enough detail when the 
following two conditions are met: I) All major design decisions have been incorporated into the specification. 
2) It is possible to evaluate the correctness and quality of the design. The first condition implies 
that the specification is at least as detailed as the control model. Methods for evaluating the correctness 
and quality of the user interface are discussed in the next section. The technique for developing an 
acceptable specification starts with producing a specification which covers all the details in the control 
model. The evaluation techniques discussed in the next section are then applied to this specification. 
If the evaluation is not successful then the specification is revised to include more detail and the 
evaluation is repeated. The specification process is complete when the specification can be evaluated 
successfully. In this section we will briefly outline the structure of our specification language (more 
details can be found in [2]). Our specification language is based on the state machine model [7]. It 
has been influenced by the SPECIAL language developed at SRI [9]. Specifications written in this type 
of specification language are made up of a number of state machines. Each state machine is represented 
by a collection of V functions and 0 functions. The V functions represent the state of the machine. The 
0 functions are the operations which can be used to change the machine's state. Some of the V functions 
are callable from outside of the machine definition, they represent the external state of the machine. 
 This specification technique was chosen for two reasons. First, it is the only major specification technique 
which can handle side effects. This is important since a large number of interaction techniques involve 
side effects. The second reason is that it can handle exception conditions. The state machines in our 
specification language are called modules and are similar to abstract data types. There can be more than 
one instance of a given module and these instances can be bound to identifiers in the specification. 
These identifiers are similar to variables in programming languages. A module may be parameterlzed. A 
function in another module can be referenced by appending a period and the function's name to the name 
of the module. A module definition consists of four sections. The first section contains the declarations 
of the module's parameters. A parameter declaration consists of the name of the parameter, a colon, and 
the type of the parameter. The parameter type may be the name of another module. The next section, called 
the declarations section, contains the declarations of the module's local variables. These variables 
are used to reference module instances created inside this module. Local variable declarations have the 
same format as parameter declarations. The definitions section contains the definitions of the syntax 
macros used in this module. These macros are used to make the module definition more readable. The last 
section in the module definition is the function section. This section contains the definitions of the 
module's V and 0 functions. There are two kinds of V functions; primitive and derived. The definition 
of a primitive V function states the initial value of the function, its parameters, and the conditions 
under which it can be called. The values of the primitive V functions are changed by the O functions. 
The value of a derived V function is derived from the values of the primitive V functions in the module. 
Derived V functions provide a cleaner interface to the other modules in the specification. The definition 
of a derived V function contains its derivation along with parameter declarations and the conditions 
under which it can be called. The definition of an 0 function consists of its parameters, a set of pre-conditlons, 
and a set of post-condltions. The pre-condltions are logical expressions which must hold when the function 
is called. The post-condltions are logical expressions which state the effects of the 0 function. These 
logical expressions can involve the V functions, the variables in the module, and the 0 functions of 
other modules. Appendix 3 contains the specification of the example user interface. Computer Graphics 
Volume 15, Number 3 August 1981 The definitions of the modules in Appendix 3 use modules which are defined 
in a standard library. These modules include the definitions of raster and vector displays and simple 
concepts from geometry. These library modules are not necessary for understanding most aspects of this 
specification. Their main purpose is to supply details necessary for the evaluation of the specification. 
These modules are kept on a library so they do not have to be redefined each time a new interface is 
specified. 3. Evaluation of the Specification Once the user interface has been specified we need some 
method for determining the correctness of the specification. What does the correctness of the specification 
of a user interface mean? First of all the specification must be consistent with the control model for 
the user interface. The specification is consistent with the control model if each operator in the control 
model can be implemented by the functions in the specification. This notion of consistency is similar 
to the notion of the control level mapping. The consistency checks insure that all the descriptions describe 
the same user interface. More details on the consistency checking process can be found in [2]. The 
second aspect of the correctness of the specification is that all the invariants in the task and control 
models hold for the specification. The invariants represent the user's view of how the problem domain 
and interface should operate, therefore, they most be reflected in the specification of the user interface. 
 There are three invarlants in the task and control models for our example user interface. The first 
invariant, which is from the task model, states that all the geometrical objects in the work area must 
be completely contained within the work area. We can see that this invarlant holds by examining the 0 
function "include" in the module "work area". Before this function adds a geometrical object to the list 
of objects in the work area it checks to see if the geometrical object is completely with the work area. 
The second Invariant, which is from the control model, states that all the items in the menu must be 
completely within the menu. By examining the geometrical object and menu modules we can see that this 
Invarlant holds. The third invariant deals with the tablet and tracker modules. It states that the tracker 
symbol follows the motion of the tablet cursor. Each time the tablet cursor is moved the function change_posltion 
in the module tablet is invoked. This function in turn invokes the function move in the tracker module. 
The move function moves the tracker symbol to its new position. Thus we see that this invarlant holds. 
 The third and final aspect of the correctness of the specification is determining how the user interface 
will behave under particular conditions. These conditions usually include errors and situations where 
the response of the user interface may be ambiguous or undefined. It is the job of the designer to determine 
what these conditions are (possibly before the specification is produced) and test them. An example 
of one of these conditions in our example user interface is when two or more geometrical objects overlap. 
First, is it possible to place one geometrical object on top of another? By examining the modules in 
Appendix 3 we can see that there is nothing to stop the user from placing one geometrical object on top 
of another. What happens when the user points to a position where two objects overlap? By examining the 
"hit" and "decode" functions in the "work_area" module we see that one of the two objects will be selected 
but which one is not specified. This indicates that the current specification is incomplete. By testing 
all three aspects of the correctness of the specification our confidence in its correctness will be increased. 
Using the above procedures we can never be sure that the specification is totally correct but we can 
increase our confidence in it to an acceptable level. 4. Conclusions In this paper we have presented 
a methodology for the design of interactive user interfaces. This methodology is made up of a number 
of steps, which are: I) task analysis 2) construction of a task model for each user group 3) construction 
of a control model for each user group 4) constructing a control level mapping 5) constructing a formal 
specification of the user interface 6) evaluating the correctness of the specification While this 
design methodology will not guarantee the success of the user interface it does give the designer a set 
of tools for organinzlng and evaluating his design. Acknowledgements The Computer Systems Research 
Group at the University of Toronto provided a stimulating environment for this research. In particular 
the assistance of R. Baecker, B. Buxton, B. Reeves, and D. Wortman is greatly appreciated. I would also 
llke to thank the referees for their many helpful suggestions. References i. Boehm B.W., J.R. Brown, 
H. Kaspar, M. Lip- ow, G.J. MacLeod, M.J. Merrlt, Character- istics of Software Quality, North-Holland, 
 Computer Graphics Volume 15, Number 3 August 1981 Amsterdam, 1978. END; 2. Green M., "A Specification 
Language and OPERATOR move(ob : geometrlcal_object; Design Notation for Graphical User Interfa- p : 
point); ces", TR 81-CS-09, Unit for Computer Scien-PRE ce, McMaster University, 1981. p in work area.extent; 
 ob on work area.contents;  3. Heninger K.L., "Specifying Software Requir-POST ements for Complex Systems: 
New Techniques ob.where = p; and Their Application", in Specification of p in ob.extent; Reliable Software, 
IEEE Computer Society, END; Long Beach CA, 1979.  OPERATOR remove(oh : geometrlcal_object);  4. Liskov 
B.H., V.Berzlns, "An Appraisal of PRE Program Specifications", in Directions in ob on work_area.contents; 
Software Technology (ed. P Wegner), The MIT POST Press, Cambridge Mass., 1979. NOT ob on work area.contents; 
 END;  5. Liskov B.H., S.N Zilles, "Specification Techniques for Data Abstractions", IEEE INVARIANT 
Transactions on Software Engineering, SE-I, FORALL g:geometrical_obJect I no.l, p.7, 1975. g on work 
area.contents  {  6. Newman W.M., R.F. Sproull, Principles of g.extent in work_area.extent; Interactive 
Computer Graphics, 2nd Edition, }; McGraw-Hill, 1979. END TASK MODULE exampletask; 7. Parnas D.L., 
"A Technique for Software Mod- ule Specification With Examples", CACM, vol.15, no.5, p.330, 1972.  
Appendix 2 Control Model for Example Interface 8. Relsner P., "Formal Grammar and Human Fact- ors Design 
of an Interactive Graphics Syst-CONTROL MODEL example__interface; em", IEEE Transactions on Software 
Enginee- ring, SE-7, no.2, p.229, 1981. OBJECT menultem;  ATTRIBUTE symbol : geometrical_object;  9. 
Robinson L., K.N. Levitt, "Proof Techniques ATTRIBUTE extent : Extent; for Hierarchically Structured 
Programs", END; CACM, vol.20, no.4, p.271, 1977.  OBJECT menu;  i0. Zelkowltz M.V., A.C. Shaw, J.D. 
Gannon, ATTRIBUTE extent : Extent; Principles of Software EngineerlnK, Prentl-ATTRIBUTE items : LIST 
OF menu_item; ce-Hall, New Jersey, 1979. END; OBJECT tracker; Appendix 1 Task Model for Example Interface 
ATTRIBUTE tracker symbol : geometrical object; TASK MODEL exampletask; ATTRIBUTE where : point; END; 
OBJECT geometrlcal object; ATTRIBUTE kind : (triangle, circle, OBJECT tablet; square); ATTRIBUTE button__push 
: boolean; ATTRIBUTE where : point; ATTRIBUTE position : point; ATTRIBUTE extent : Extent; END; END; 
OPERATOR menu select -> geometrical object; OBJECT work area; PRE ATTRIBUTE extent : Extent; tracker.where 
in menu.extent; ATTRIBUTE contents : LIST OF tablet.button push; geometrlcalobject; POST END; LET y:menu_item 
I tracker.where in y.extent; OPERATOR place(oh : geometrlcal_object; menu__select = y; p : point); tracker.tracker 
symbol = y; PRE END; p in work_area.extent; POST OPERATOR work_select -> geometrlcal_obJect ; oh.where 
= p; PRE ob on work_area.contents; tracker .where in work area.extent; p in ob.extent; tablet .button__push; 
 Computer Graphics Volume 15, Number 3 August 1981 POST LET y:geometrical__object I (tracker.where in 
y.extent AND y in work area.contents);  work select = y; tracker.tracker_symbol = y; END ; OPERATOR 
release(ob : geometrical_object) ; PRE tracker.where in work area .extent; tablet .button__push ; POST 
ob.where = tracker.where; tracker.where in ob. extent ; ob on work area.contents; tablet.button_push 
= FALSE; tracker.trackersymbol = tracking_cross ; END ; OPERATOR delete ; PRE NOT tracker .where in 
work area.extent; tablet .button_push; POST NOT tracker.tracker_symbol on work_area, contents ; tracker.tracker 
symbol = tracklng cross; END; INVARIANT FORALL m:menu item I m on menu.items { m.extent in menu.extent 
; }; INVARIANT tracker.where = tablet.posltlon AND tracker, tracker symbol.where = tracker .where; 
 END CONTROL MODEL example_interface; Appendix 3 Interface Specification MODULE geometrlcal_object 
; FUNCTIONS VFUN size -> real; IN IT IALLY size = ?; END; VFUN where -> point; INITIALLY where = ?; 
END ; VFUN extent -> Extent; INITIALLY extent = appear.extent; END ; VFUN in(p:polnt) -> boolean DERIVED 
in = (p - where) in appear.extent; END; VFUN appear -> ONE OF (circle, square, triangle, cross); INITIALLY 
appear = ?; END; VFUN ob type -> string; INITIALLY obtype = ?; END; OFUN init(type:string; p:point; 
s:real); POST IF type = "circle" THEN appear = circle(s); IF type = "square" THEN appear = square(s); 
 IF type = "triangle" THEN appear = triangle(s); IF type = "tracker" THEN appear = cross(s); ob type 
= type; where = p; size =s; END; OFUN new positlon(p:polnt); POST where = p; END;  END MODULE geometrical_object; 
 MODULE menu; PARAMETERS pos:polnt; xslze,yslze:real; DECLARATIONS gl,g2,g3:geometrlcal object; FUNCTIONS 
VFUN extent -> Extent; INITIALLY extent = ?; END; VFUN mlist -> LIST OF geometrical_object; HIDDEN INITIALLY 
 mlist = ?; END; VFUN hit(p:polnt) -> boolean; DERIVED hit = EXISTS g:geometrical__obJect I g on mlist 
{ p in g.extent; }; END; VFUN decode(p:point) -> geometrlcal_object; DERIVED LET g:geometrical_obJect 
p in g.extent; decode = g; END;  Computer Graphics Volume 15, Number 3 August 1981 OFUN init; POST 
gl.init("circle",bottom, 0.25*xsize); g2.1nit("square",middle, 0.25*xsize); g3.init("trlangle",top, 0.25*xslze); 
gl on mllst; g2 on mlist; g3 on mlist; extent = Extent(pos,p+(xsize,ysize)); END; END MODULE menu; 
 MODULE work area; PARAMETERS pos:polnt ; xsize, yslze : real ; FUNCTIONS VFUN extent -> Extent; INITIALLY 
extent = ?; END; VFUN dlist -> LIST OF geometrlcal object; HIDDEN INITIALLY dlist = ? ; END ; VFUN 
hit (p :polnt) -> boolean; DERIVED hit = EXISTS g:geometrlcal object I g on dllst { p in g.extent; }; 
END ; VFUN decode (p :polnt) -> geometrical_obJ ect ; DERIVED LET g:geometrlcal_object I p in g.extent; 
decode = g; END; OFUN include (g: geometrical__obj ect ) ; PRE g.extent in extent ; POST  g on dlist; 
END; OFUN remove(g:geometrleal_obJect) ; PRE g on dlist; POST NOT g on dllst; END; OFUN init ; POST 
extent = Extent (pos,pos+(xsize,yslze)) ; END ; END MODULE work_area; MODULE tablet; FUNCTIONS VFUN 
button_state -> ONE OF (up,down); INITIALLY button_state = up; END; VFUN position -> point; INITIALLY 
position = ?; END; OFUN button__push; POST button state = down; tracker.select(posltlon);  END; OFUN 
button release; POST button_state = up; END; OFUN change__positlon(p : point); POST position = p; tracker.move(position); 
 END; END MODULE tablet; MODULE tracker; DECLARATIONS cross : geometrlcal object; menu : menu; work 
area : work area; FUNCTIONS VFUN cross_dlsplayed -> boolean; HIDDEN INITIALLY cross_dlsplayed = TRUE; 
END; VFUN tracker symbol -> geometrlcal object; INITIALLY tracker_symbol = cross; END; VFUN where -> 
point; INITIALLY where = ?; END; OFUN move(p : point); POST where = p; tracker_symbol.where = p;  
END; OFUN select(p : point); POST Computer Graphics Volume 15, Number 3 August 1981 CASE p in menu.extent 
AND menu.hit(p) { LET g : geometrical_object I g = menu.decode(p); current_cross = g; tracker__symbol 
= FALSE; }; CASE cross_displayed AND work area .hlt(p) { LET g : geometrical__object I g = work_area.decode(p) 
; tracker_symbol = g; cross__dlsplayed = FALSE; }; CASE p in work_area.extent AND NOT cross__displayed 
{ work area.include (tracker. symbol) ; tracker symbol = cross; cross_dlsplayed = TRUE ; }; CASE NOT 
cross_dlsplayed AND NOT p in work area.extent { work_area, remove (t racker symbol) ; tracker symbol 
= cross; cross_displayed = TRUE;  }; END ; END MODULE tracker; 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806796</article_id>
		<sort_key>109</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Gaze-orchestrated dynamic windows]]></title>
		<page_from>109</page_from>
		<page_to>119</page_to>
		<doi_number>10.1145/800224.806796</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806796</url>
		<abstract>
			<par><![CDATA[<p>Consider a large-format display before the user, bearing a multiplicity of &#8220;windows,&#8221; like little movies, the majority dynamic and in color. There are upwards of 20 windows, say, more than a person can ordinarily absorb at once. Some of the windows come and go, reflecting their nature as direct TV linkages into real-time, real-world events. Others are non-real-time, some dynamic, others static but capable of jumping into motion.</p> <p>Such an ensemble of information inputs reflects the managerial world of the top-level executive of the not too distant electronic future: a world of brevity, fragmentation, variety, above all one of an overwhelming onslaught of events.</p> <p>The multiplicity and simultaneity of such a display situation ordinarily would make coping with it untenable. The intent of the reported research is to introduce order and control, through the creation of a dynamic, <underline>gaze-interactive</underline> interface.</p> <p>Making the behavior and reactivity of the &#8220;windows&#8221; contingent upon measured eyemovements - the point-of-regard of the observer - aims both to help the observer to cope with the onslaught of events on the one hand, yet enable on the other hand continuing close contact with that everchanging ensemble.</p> <p>A simulation of such a world is described and demonstrated in the composite medium of computer, videodisc, and video special effects. Eye-tracking technology, integrated with speech and manual inputs, controls the display's visual dynamics, and orchestrates its sound accompaniments. All elements are combined to form a testbed for the conception generally, and to explore the associated human factors and stagecraft.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Eye-tracking]]></kw>
			<kw><![CDATA[Graphical interface]]></kw>
			<kw><![CDATA[Man-machine interfaces]]></kw>
			<kw><![CDATA[Videodisc]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Windowing systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011053</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Window managers</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P241484</person_id>
				<author_profile_id><![CDATA[81100653961]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Bolt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Architecture Machine Group, Massachusetts Institute of Technology, Cambridge, Massachusetts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[FOULDS, R. A. Studies in the use of visual line of gaze as a selection technique in nonvocal communication. Proposal to National Science Foundation by Tufts-New England Medical Center, Department of Rehabilitation Medicine, Richard A. Foulds, Principal Investigator, July, 1980.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[KAHNEMAN D. Attention and effort. Englewood Cliffs, N.J.: Prentice Hall, 1973.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[KEEN, P. G. W. &amp; SCOTT-MORTON, M.S. Decision support systems: an organizational perspective. Reading, MA: Addison-Wesley, 1978.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[MARKS, L. E. Multimodel perception. In E. C. Carterette &amp; M. P. Friedman, Eds., Handbook of Perception, Vol. VII (Perceptual Coding), New York: Academic Press, 1978.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[MONTY, R. A. &amp; SENDERS J. W. (Eds.) Eye movements and psychological processes. Hillsdale, N.J.: Erlbaum Associates, 1976.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[NEISSER, U. &amp; BECKLEN, R. Selective looking: attending to visually specified events, Cognitive Psychology, 7(4), 480-494, 1975.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[RABB, F. H., BLOOD, E. B., STEINER, T. O. &amp; JONES, H.R. Magnetic position and orientation tracking system. IEEE Transaction on Aerospace and Electronic Systems, Vol. AES-15, No. 5, September 1979, 709-718.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[RINARD, G. A. &amp; RUGG, D. E. Current state of development and testing of an ocular control device. 4th Conference of Systems and Devices for the Disabled, Seattle, WA, June 1977.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[RINARD, G. A. &amp; RUGG, D. E. An ocular control device for use by the severely handicapped. 1976 Conference on Systems and Devices for the Disabled, Boston, MA, 1976.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[SENDERS, J. W. FISHER, D. G. &amp; MONTY, R. A. (Eds.) Eye movements and higher psychological processes. Hillsdale, N.J.: Erlbaum Associates, 1978.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 GAZE-ORCHESTRATED DYNAMIC WINDOWS Richard A. Bolt 
Architecture Machine Group Massachusetts Institute of Technology Cambridge, Massachusetts 02139 ABSTRACT 
 Consider a large-format display before the user, bearing a multiplicity of "windows," like little movies, 
the majority dynamic and in color. There are upwards of 20 windows, say, more than a person can ordinarily 
absorb at once. Some of the windows come and go, reflecting their nature as direct TV linkages into real-time, 
real-world events. Others are non-real-time, some dynamic, others static but capable of jumping into 
motion. Such an ensemble of information inputs reflects the managerial world of the top-level executive 
of the not too distant electronic future: a world of brevity, fragmentation, variety, above all one of 
an overwhelming onslaught of events. The multiplicity and simultaneity of such a display situation ordinarily 
would make cop- ing with it untenable. The intent of the reported research is to introduce order and 
control, through the creation of a dynamic, gaze-interactive interface. Making the behavior and reactivity 
of the "windows" contingent upon measured eyemovements -the point-of-regard of the observer -aims both 
to help the observer to cope with the onslaught of events on the one hand, yet enable on the other hand 
continuing close contact with that everchanging ensemble. A simulation of such a world is described 
and demonstrated in the composite medium of computer, videodisc, and video special effects. Eye-tracking 
technology, integrated with speech and manual inputs, controls the display's visual dynamics, and orchestrates 
its sound accompaniments. All elements are combined to form a testbed for the conception generally, and 
to explore the associated human factors and stagecraft. Key Words: Eye-tracking; videodisc; man-machine 
interfaces; graphical interface The work reported herein has been supported by the Cybernetics Technology 
Divlsion of the Defense Advanced Research Projects Agency, under Contract No. MDA-903-77-C-0037. Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by permission of the Association for Computing 
Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. &#38;#169;1981 
ACM O-8971-045-1/81-0800-0109 $00.75 Computer Graphics Volume 15, Number 3 August 1981 INTRODUCTION 
The managerial world of the top-level executive or commander is one of brevity, frag- mentation, and 
variety. They spend little time on any one activity, deal with a great number of problems in the course 
of a day, and the problems, and decisions made about them, are of many types (Cf. Keen &#38; Scott-Morton, 
1978). In contemplating what might be a graphical information systems interface corresponding to such 
a management world, we envisaged a large-format display whereon are upwards of twenty, at times even 
up to fifty, simultaneous mini-displays confronting the user, the majority of them dynamic and in full 
color. The multiplicity and simultaneity of the displays would make coping with such a visual situation 
untenable. Our goal would be to introduce order and control, primarily through gaze-directed commands. 
 The dynamic, gaze-interactive interface we wanted to assemble would function both to enable and protect. 
We aimed to help the observer to keep up with the onslaught of events on the one hand, performing a "filtering" 
function -or, more accurately, making it possible for the observer to protect himself -but, on the other 
hand, to enable the observer yet to keep in touch with a complex world of on-going events. The display 
situation described below, which we have dubbed "Gaze-Orchestrated Dynamic Windows," offers to the observer 
many visual events, while at the same time permitting built-in modes of selective attention to operate. 
In the midst of the welter of impres- sions and events, the mode par excellence of directing attention 
is vision, specifically, where you are looking (Cf. Marks, 1978). Eye-tracking, the knowledge of where 
the observer is looking, has been the object of much study (Cf. Senders et al, 1978; Monty &#38; Senders, 
1976). However, eye-tracking has been done primarily in the context of behavioral and psychological studies, 
con- trolled and in the laboratory context, rather than using an eye-tracker as a system component, 
with the derived an interactive system should point-of-regard have concerning as the human part of the 
user. essential information that Think of it as "eyes as outp ut." TRACKING TECHNOLOGIES Of eye-tracking 
techniques commercially available the technique least obtrusive to the observer is the "pupil-center, 
corneal-reflection distance" method (Young &#38; Sheena, 1975). This method of tracking measures the 
fixation point of the observer, the point- of-regard as it falls in the surround, not simply the position 
of the eye relative to the head. In this method, advantage is taken of two features of the eye that 
only change with rotation of the eye, but not with lateral or vertical displacement: corneal-reflection; 
center of the pupil. The distance between the two as measured are monotonically related to the observer's 
point-of-regard. An invisible infra-red light source for the corneal-reflection is used together with 
a TV camera sufficiently sensitive in the infra-red region to detect this light easily. The TV image, 
which is zoomed-in close upon the pupil of the eye, is computer analyzed; co-ordinates of the pupil center 
and of the infra-red spot reflect ion are determined by examining the timing signals of the video scan. 
 The infra-red spot is not at all annoying to the person being monitored, and the small TV monitor and 
associated apparatus can be situated about a meter distant from the observer, or even further away, depending 
upon the configuration and the use of special lenses. To a considerable extent, the observing TV camera 
and the infra-red light source can be so housed and/or "dispersed" that only minimal parts of the apparatus 
are directly in the presence of the person being tracked. Currently, there are two commercially available 
systems which use this method: the Honeywell Occulometer Mark II, which allows a cubic inch of head motion, 
and Mark III which allows a cubic foot of head motion; and the Gulf &#38; Western Model 1998S, which 
al- lows up to a cubic foot of head motion. The Honeywell unit tracks +30 ° horizontally, -i0 ° to +30 
° vertically, to +I° accuracy. The Gulf &#38; Western traces +20 ° horizontally, +15 ° vertically, also 
to + 1 ~ accuracy. (Cf. Young &#38; Sheena, 1975; vendor literature). Computer Graphics Volume 15, Number 
3 August 1981 Both units provide pupil diameter as an output as well as point-of-regard information. 
(Cf. Kahneman, 1973, for a discussion of the relevance of this measure of eye activity) Another promising 
approach to point-of-regard tracking is the following combination: I) a miniature corneal-reflection 
tracking system housed within or on conventional glass- es frames (Cf. Rinard &#38; Rugg, 1976, 1977), 
combined with 2) small magnetic space-sensing instrumentation (Cf. Rabbet al, 1978). The corneal-reflection 
tracking system provides a measure of the orientation of the eye vis-a-vis the glasses, and the space-sensing 
instrumentation mounted to the glasses frame provides a measure of the orientation of the glasses to 
the environment; the two measures combined provide the observer's point- of-regard re the environment. 
Such a tracking set-up is described by Foulds (1980). While a glasses-mounted system is much less unobtrusive 
than the remote corneal-reflec- tion pupil-center distance measuring systems, it does have the advantage 
of unrestricted vertical and horizontal range, and free subject motion in z, as well as a substantial 
price advantage. We are currently investigating advantages/disadvantages of both systems for our purposes. 
But the chief point now is the presence of relatively unobtrusive eye-tracking in place at the interface 
to capture the user's point-of-regard: an interface that knows where you're looking. Which in turn implies 
a graphic interesting and rich enough to be worth looking at, and where direction of gaze means something 
in terms or its orchestra- tion. VIDEODISC GRAPHICS: the "WORLD OF WINDOWS" The actual physical premises 
for our large-format display is a special room at MIT's Architecture Machine Group's laboratory, which 
we have dubbed the "Media Room." The room is about the size of a personal office. An entire wall at one 
end of the room is actually a large screen, thirteen feet on diagonal, which can be back-projected upon 
by a light-valve television projection system stationed in an adjoining room (Cf. Figure i) '.._.9 ii'I 
t ~,{I a ~.J L._~J" /.--~ ~.;~ , q-_J l I I I I I I II E / I I i '/, Ii Ii If Figure i  Computer 
Graphics Volume 15, Number 3 August 1981 In any case, the result of the collection of images is a realtime 
stream of parallel audio-visual "events" laid out in cross-section for the observer. The broad frontal 
display in the x,y plane of the large-screen is what offers multiplicity to the observer. The observer 
selects from amidst this multiplicity by where he is looking. The Videodisc Simulator We have made 
a videodisc which simulates such a real-world timestream of simultaneous on-going events: -conversations 
 -groups of people -large-scale events: e.g., military maneuvers, sports events -newscasts -dynamic 
graphs -radar maps and the like. Additionally, there are a few windows into non-real-time events: "movies" 
and "slides." While the videodisc material was intended to impersonate a welter of live, real-time 
remote channels into ongoing events, in a more general sense, the format of the disc represents a collection 
of events that occur in parallel mode, juxtaposed, at a reduced scale. The events could be actually 
real-time events, or not, or some combination of both classes of events. In addition to being represented 
on the disc at a small and "simultaneous" scale, each event was to be stored on disc as an independent 
single event at its own level at full-screen scale. From a graphics point of view, the structure of 
the disc to simulate the real-time WOW is of more interest and importance than the content of the images 
as such. From the point of view of interacting with videodisc, the focus becomes: i) how the disc is 
used to support the necessary illusion of real-time eye-interaction, and 2) how the eye- graphics interaction 
itself proceeds. First, a description of the disc, its layout, and then a description of how simulating 
 the "World of Windows" proceeds. Disc Layout As a first step, the collection of movie snippets (on 
videodisc and videotape) comprising the WOW was selected. The briefest snippet was about 27 seconds, 
the longest was five minutes. Total running time for all snippets, end-to-end, was about 72.5 minutes. 
 All of these movie snippets, plus some stills for "slide shows" had to go on the disc at full-frame 
size so that the observer could be zoomed-in upon them from their little image in the WOW collage. 
The length of time for the parallel play of the collage of little versions of the snippets was to be 
5 minutes. This 5 minute collage section thus would contain in a spatio-temporally overlapping pattern 
all of the 72 minutes of the individual full-size snippets. As one side of a videodisc (MCA Discovision) 
holds 30 minutes of play, clearly 2 discs at minimum were necessary. However, 3 discs were decided upon, 
to allow certain "stage- management," to be described below. The layout of the 3 discs is depicted in 
Figure 3. All 3 dis~s have identical 5 minute sections of the small window collage. Then, the approximately 
72.5 minutes of material was distributed on the discs, about one-third of the material on each. Only 
one full- frame version of any snippet appeared somewhere on one of the three discs. Computer Graphics 
Volume 15, Number 3 August 1981 5< minute~ ~< Disc A: ~ w-o-~ Disc B: ~-o-~ I< Disc C: I" Frame Final 
Frame ) Figure 3 Each videodisc, A, B, and C has the 5-minute collage of the "World-of-Windows" laid 
o~t near the center of the disc. The rest of the space on the three discs is used to'store the full-frame 
size versions of the video episodes (72.5 minutes in all). At left: % ~¢ e / I/ I'/I Spatial/temporal 
~ layout of "World- o..ow ~ / InSet a-nt an~e°t h rough 5-minute O inl: o:' on ( eybo.. Figure The 
editor monitor, with keyboard and graphics tablet, permitted editing into place of video snippets comprising 
the "World-of-Windows." Stipulating the spatio-temporal layout of the collection of snippets onto the 
5-minute collage or composite block was greatly aided by an editing program which could display on a 
frame-by-frame basis what any time-slice through the 5-minute WOW segment would look like in terms of 
general spatial layout. The editor could insert, delete, and move about small color-coded and numbered 
rectangles, representing any snippet. These rectan- gles were scaled to the desired relative size. The 
editor could also "play" the spatial mockup to show off the "look" of the layout as editing progressed, 
allowing on-the-spot changes and corrections. (Cf. Figure 4) The little rectangles were, as noted, assigned 
code numbers associated with their corres- ponding snippets so that, upon completion of the spatio-temporal 
layout editing, a "reci- pe" for assembling the snippets at proper relative size and at proper spatial/temporal 
juxtaposition could be output from the editor. Pre-mastering Assembly Each source video snippet was 
then reduced to its proper relative size, and "printed" individually on videotape in its proper relative 
x,y position. At the conclusion of this step, each snippet was in place and sized properly, but existing 
each by itself on a separate tape. The next step was to merge the separate tapes into one. The strategy 
was to do a pair- by-pair generational merge, to minimize image degradation by minimizing the number 
of generations necessary. In essence, single episodes A and B were merged with proper temporal offset, 
to form tape A', episodes C and D to form B', E and F to form C' etc. Then, in generation 2, tapes A' 
and B' were merged, again with proper temporal offset, to form tape A", C' and D' to form B", etc., and 
so on until through about five actual copying generations all the tapes were merged to a single five-minute 
tape holding all the tiny spatially-deployed and temporally-staggered episodes. From this 5-minute composite 
WOW tape, plus the aggregate 72.5 minutes of material at full-frame scale, premastering tapes were made 
as input to videodisc mastering. Computer Graphics Volume 15, Number 3 August 1981 Playback Strategy 
 As noted above, 3 discs were made. The reason for this is due to the effects desired when the WOW is 
being observed. Real-time windows are dynamic: the images are of people talking, gesturing, walking 
about, of planes diving, cars careening, etc. The reactivity to these dynamic windows to looking is to 
"freeze" when you look at them, then resume action. The "freeze" is feedback that they know you are looking 
at them. The non-real-time windows e.g., a "movie", in contrast, start up when you look at them from 
a stopped position. You may look away and then come back; the movie resumes where you left off looking. 
Unlike the supposedly real-time win dows, the timestream of the event waits for you. Now, at the commencement 
of a demonstration of the WOW, necessarily a five-minute sample of what the experience is like, all 3 
discs are played together, each 5-minute WOW segment being played in precise synch. Should the observer 
look at some particular small window for a "supra-threshold" dura- tion of time, we want to be able to 
freeze that window, while not the others. How?? Suppose the WOW image currently seen on the large-screen 
is coming off disc player A, and further that the full-screen size version of the episode resides on 
disc B on another player. What we do then is keep exhibiting the video stream from disc A, but matte 
through a freeze-framed image of the looked-at window from disc C. We do not take this freezed window 
from disc B as we want now to begin on B a "seek" to the full-size ver- sion of the episode in question, 
in particular to its start frame, plus an offset re- flecting how much of the episode has already, in 
miniature amidst the WOW, gone by. Now if the observer acts such as to cause a zoom-in to the episode 
at full size, the discs (especially disc B) are set up to do so. In general then, there are 3 discs 
having a central section holding the 5-minute WOW composite so that: i) we are able to freeze (or start 
up, in the case of stills) some eye-selected episode by matting-through from some second disc, and 2) 
we can perform the 2-disc matte and keep free for "seeking" on whatever disk has the full-scale version 
of the selected episode for zooming-in, if required. THE "PERSONALITY" OF THE SYSTEM Interaction with 
Windows Before getting into discussion of the visual side of reactivity, we should note that an important 
aspect of the reactivity of windows is rooted in sound. One scenario is that only the looked-at window 
can give off its associated soundtrack, or that the soundt~ack of the looked-at window is markedly louder, 
against a muted background of the composite sound of the other windows. Another scenario re sound is 
to play the track of that window currently nearest the observer's point-of-regard, plus those of its 
immediate neighbors in stereo (with appro- priate fall-off for distance). This tactic would make the 
observer's auditory locus the looked-at spot in the x,y plane. This procedure is, for the present, untenable 
due to videodisc addressing considerations arising out of the way we have organized our simulation. 
 An alternative rendering of "neighboring" at least 2 sounds, however, is to construe proximity not on 
a spatial basis, but on a temporal basis: images looked at most re- cently are juxtaposed auditorially. 
For example, you are looking at a newstape of President Reagan. His soundtrack amplifies, the others 
attenuate. You then glance over at a newsflash about the Near East. That soundtrack loudens, but you 
still hear a relatively loud Reagan soundtrack (the window you just left). If you return to the Reagan 
newsclip, then that soundtrack restores to dominant volume; but, if you stay at the Near East window, 
the Reagan soundtrack gradually attenuates to "background" lev- el. ("Background" can be either silence, 
or the muted composite of all the other tracks together.) Now, where and how you look will get you 
"zoomed-in" upon some window of interest. Computer Graphics Volume 15, Number 3 August 1981 Zooming 
Techniques At first pass, we simply "cut" to the fu$1-screen view: a "degenerate" form of zoom.Next, 
we plan to have a "special-effects" type of zoom, where the frame of the little dynam- ic window expands 
to full-screen scale. The little image itself does not expand, but stays in place at its original size 
(Figure 5). In the "wake" of the expanding frame, the full-size version of the contents of the little 
screen is revealed. The image con- tents of the little window at the same time fades, either to disappear 
entirely, or remain at the same general level of background brightness as the WOW doubled-exposured 
with the zoomed-in-upon full-screen image (See section below on "Double Exposure"). With appropriate 
new hardware, we will go into "squeeze- zooming," where the little scene in the zoomed-in-upon window 
expands as well, not just the frame moving outward. This is the style of zooming commonly seen on Saturday 
football TV. Tiny image of, e.g., General Jones, speaking amidst the "World of Windows": :: Gen. Jones 
of other windows For example, as the full-screen episode proceeds, we see a composite double-exposure: 
we see both the full-scale image of General Jones speaking before us, plus a ghost-like image of the 
World-of-Windows video-mixed in. The particular little window from whence we came (the tiny image of 
Gen. Jones) will be among the fainter set of windows, yet distinctively high- lighted to set it off from 
the rest. Figure 5 Zooming in on a Window There are at least two competing philosophies about what 
should cause you to be "zoomed- in" upon some window you are looking at: i) zoom-in automatically, based 
upon some timing-out of how long you look at ("stare at") some certain window; 2) zoom-in upon the window 
you are eye-addressing contingent upon a ~eliberate action via an independent modality (e.g., joystick 
action; or a word spoken to the system via our Nippon Electric Company (NEC) DP-100 Connected Speech 
Recognizer). There is appeal to both routes. The triggering of zoom by "stare" conjures up ultra- automaticity. 
There is a trade-off involved: the system wants to wait to be "sure" that you want to do in, yet it shouldn't 
hesitate too long. However, it can be argued that while the selection of what window you want to enter 
is properly given in the x,y plane by the eye's point-of-regard, any decision by the system to now zoom-in, 
as opposed to simply letting you view the window from afar, is best giv- en by a specific indication 
on your part in an independent modality. That is, "staring" to effect some action may not be a good assignment 
of function; it is not a use of the eyes, but a non-use over time that ties them up. We are trying alternative 
combinations to see what seems best. One combination will be: if you keep looking at a window "long 
enough" (e.g., 4 -5 Computer Graphics Volume 15, Number 3 August 1981 seconds), you hear its soundtrack 
only, not the general sound hub-bub. Then, if you look longer, you begin to be zoomed-in. You can abort 
the zoom-in (if you decide you only want to listen-at, not zoom-in-upon, the window) by pulling back 
on the joystick of our Media Room's chair. If you decide that now you want to zoom-in, then a push of 
the joystick is required to over-ride your previous over-ride of the zoom-in. If, for any window, you 
immediately know you want to go zooming-in, you needn't just tolerate the system's timing-out of your 
looking. You push forward on the joystick to say you want to zoom in. This general scenario for zooming 
would seem to be compatible with a "browsing" or per- haps moderately leisurely stance or frame-of-mind 
with regard to the set of windows. In a more "active" or very quick-and-urgent mode, the overall system 
response might want to be one where you get immediately zoomed-into the window of your choice (by looking) 
unless you keep holding back on the joystick. The continuous holding back on the joystick to prevent 
you popping in, or to pop you in there by a simple "release" action, corres- ponds to alert, "up" stance 
generally. The search for a comfortable and compatible protocol for zooming (and not zooming) is part 
of the research, though, and the above scenarios represent explorations, not con- clusions. Leaving 
a Window There are two ways in which you "leave" a window you are currently zoomed into: i) for a moment 
(temporarily), for some reason irrelevant to the display, e.g., you turn away to greet someone who has 
entered the room; 2) when you leave some window for the display- at-large, or for some other window. 
 Whether you are looking away from the entire display, or away and at another window can, of course, 
be detected by eye-tracker, and the system's action should be appropriate to either case. If you look 
away from the display, or if looking is non-specific (i.e., "meandering about"), a reasonable guess is 
that you are somehow elsewhere (and thus elsewise) engaged, and the display should not change state. 
Thus, if you had been zoomed-in upon some particular window, the zoom should hold so that you may resume 
looking at that window without having again to effect a zoom-in upon it. Or, if you are tuned into some 
specific window's soundtrack, but not yet visually zoomed- in, then that sound status should be held 
until and if you return, as indexed by eye- tracker, from beyond the display back to the same window. 
Of course, if you return to the World-of-Windows under such circumstances, but after some seconds (number 
to be determined) are not refastened to that specific window which was not zoomed-in on, but whose sole 
soundtrack was playing, then the soundtrack reverts to the general melange of n sounds. In sum, the 
simple fact that you have left the currently attended window does not neces- sarily mean that it is to 
be "closed down" (zoomed-out from; or, if not zoomed-in, its sound replaced by all the sounds or silence). 
It depends upon where your eyes go: yes, closed, if you go to another window, or to the display-at-large 
(non-specifically about the display area); but not closed if you leave the display entirely (but temporarily) 
- it depends upon what you do upon your return, as discussed above. Zooming-out Because the observer 
can leave a window temporarily for some reason, and wish to stay zoomed-in for when he comes back, we 
need a way independent of the sheer fact of looking away to get the observer zoomed-out when he really 
wants to zoom-back to return to the overall set of windows. This is by pulling back on the joystick. 
(Or by voice command.) The visual action need not be an explicit zoom-out, the reverse visually of zooming-in, 
but could be a direct return to the World-of-Windows (perhaps via a "fade"). The precise form that seems 
best (read: most visually plausible and pleasing) is an object of our research. "Double Exposure" When 
zoomed-in upon some window, now at full-screen scale, it will be possible yet to see the WOW in its entirety 
via a video "double-exposure" mix, variable from 100/0%, through 50/50% to 0/100%. The degree of mix 
is an independent parameter, controllable by the Computer Graphics Volume 15, Number 3 August 1981 observer 
via the remaining joystick of the Media Room chair. Thus, the observer, when in close upon some window, 
can yet be in touch, in at least a "pre-attentive" way, with the World-of-Windows at large: the observer 
pays attention to the large scene, now to the WOW, then back to the large scene. (Cf. Neisser &#38; Becklen, 
1975.) In this double-exposure situation the observer will be able to zoom-in upon some little episode 
seen in the faint view of the World-of-Windows by conjointly looking at it and pressing forward on the 
zooming joystick. A press on the joystick is mandatory in this context to avoid the situation where an 
active World-of-Windows episode just happens to be lined up on the screen on the same spot where the 
observer is intently looking in the large image (the other half of the double-exposure). In this diving-through-the-double-exposure 
situation, the action will first be the cutting out of the big image to leave the World-of-Windows only, 
then a zoom-into the new episode. The new episode will itself be double-exposed with the World-of-Windows 
at the same degree of image mix, unless the observer resets the mix level. Non-real-time Windows The 
primary difference in observer interaction with this class of window is that the event starts up when 
looked at. The features of zooming-in, where you first hear only that sound from the window you are 
looking at, then get the full-screen picture, are the same as for real-time windows. However, if you 
look away from the full-size window of one of these events, what occurs depends on which sort of two 
general classes of non-real-time event it is. If it is a "slide show" type, where the slides have been 
changing all along, then they will go on changing independent of whether or not you are looking at them. 
(This is an arbitrary decision; there is no reason why they might only "flip" when you go in, and then 
only when you look at them . .). If it is a "movie" type, then it only proceeds when you are looking 
at it; that is, it always waits for you. CONCLUSIONS The world of interactive videographics has been 
dramatically enlarged by the advent of the optical videodisc. The planning, layout, and formatting of 
the videodiscs to support the simulation of a "World-of-Windows" described herein conveys the sense 
of programming in images. At the user/observer interface level, interactivity is extended to incorporate 
where the user is looking, making the eye an output device. While the current application of eyes at 
the interface is in a sense a special case, eyes-as-output has a promising future in interactive graphics, 
especially where the graphics contemplates the possibilities arising out of the orchestrated use of videodiscs. 
 ACKNOWLEDGEMENTS Thanks are due to Chris Schmandt and Eric Hulteen, who made substantial contributions 
to the concepts described as well as implementing them. Thanks are due also to Mike Naimark, who selected 
and edited the materials for our World-of-Windows videodisc, and guided it through its pre-mastering 
phase. REFERENCES FOULDS, R.A. Studies in the use of visual line of gaze as a selection technique in 
nonvocal communication. Proposal to National Science Foundation by Tufts-New England Medical Center, 
Department of Rehabilitation Medicine, Richard A. Foulds, Principal Investigator, July, 1980. KAHNEMAN 
D. Attention and effort. Englewood Cliffs, N.J.: Prentice Hall, 1973. KEEN, P.G.W. &#38; SCOTT-MORTON, 
M.S. Decision support systems: an organizational perspective. Reading, MA: Addison-Wesley, 1978. }r~RKS, 
L.E. Multimodal perception. In E.C. Carterette &#38; M.P. Friedman, Eds., Hand- book of Perception, Vol. 
VII (Perceptual Coding), New York: Academic Press, 1978. MONTY, R.A. &#38; SENDERS J.W. (Eds.) Eye movements 
and psychological processes. Hillsdale, N.J.: Erlbaum Associates, 1976. NEISSER, BECKLEN, U. &#38; R. 
Selective looking: Psychology, 7(4), attending to visually 480-494, 1975. specified events, Cognitive 
RABB, F.H., BLOOD, E.B., STEINER, T.O. JONES, H.R. &#38; Maqnetic position and orientation trackinq on 
Aerospace and Electronic Systems, Vol. 709-718. system. AES-15, No. IEEE Transaction 5, September 1979, 
 RINARD, G.A. &#38; RUGG, D.E. Current state of development and testing of an ocular control device. 
4th Conference of Systems and Devices for the Disabled, Seattle, WA, June 1977. RINARD, G.A. &#38; RUGG, 
D.E. An ocular control device for use by the severely handicapped. 1976 Conference on Systems and Devices 
for the Disabled, Boston, MA, 1976. SENDERS, J.W. FISHER, D.G. &#38; MONTY, R.A. (Eds.) Eye movements 
and higher psychological processes. Hillsdale, N.J.: Erlbaum Associates, 1978. 119 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806797</article_id>
		<sort_key>121</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Video Panel Session on Interactive Systems]]></title>
		<page_from>121</page_from>
		<page_to>122</page_to>
		<doi_number>10.1145/800224.806797</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806797</url>
		<abstract>
			<par><![CDATA[<p>SIGGRAPH has long provided a forum for the presentation, discussion, evaluation, and appreciation of beautiful algorithms and beautiful pictures. The Video Panel Session on Interactive Systems is a new SIGGRAPH event, affording us the opportunity to look at beautiful systems.</p> <p>The design of interactive systems and of their user interfaces is an art. We possess no systematic design methodology, no effective and universal method of evaluation. We speak in earnest praise of the importance of power, generality, flexibility, ease of use, ease of learning, robustness, and a dozen other universally admired attributes, but we do not know how to construct systems so that they are guaranteed to have these attributes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944.10011122.10002945</concept_id>
				<concept_desc>CCS->General and reference->Document types->Surveys and overviews</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P248880</person_id>
				<author_profile_id><![CDATA[81100448368]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ronald]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baecker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Human Computing Resources Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baecker, R., "Human-Computer Interactive Systems: A State-of-the-Art Review," appears on pp. 423-443 of Kolers, P.A., Wrolstad, M. E. &amp; Bouma, H. (Editors), Processing of Visible Language 2, Plenum Press, 1980.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Baecker, R., "Towards an Effective Characterization of Graphical Interaction", appears on pp. 127-147 of Guedj, et. al. (see below).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Baecker, R., Buxton, W. &amp; Reeves, W., Towards Facilitating Graphical Interaction: Some Examples from Computer-Aided Musical Composition, proceedings of the 6th Man-Computer Communications Conference, Ottawa, May 1979, pp. 197-207.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807394</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Britton, E.G., Lipscomb, J. S. &amp; Pique, M. E., Making Nested Rotations Convenient for the User, Computer Graphics, Vol. 12, No. 3, 1978, pp. 222-227.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Brooks, F. P., Jr., The Computer "Scientist" as Toolsmith: Studies in Interactive Computer Graphics, Proceedings of the IFIP Conference, 1977, pp. 625-634.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Buxton, W., Design Issues in the Foundation of a computer-based Tool for Music Composition, Technical Report CSRG-97, Computer Systems Research Group, University of Toronto, Toronto, 1978.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Buxton, W., Sniderman, R., Reeves, W., Patel, S. &amp; Baecker, R., The Evolution of the SSSP Score Editing Tools, Computer Music Journal, Vol. 3, No. 4, 1979, pp. 14-25.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Buxton, W., Reeves, W., Fedorkow, G., Smith, K. C. &amp; Baecker, R., A Microcomputer-based Conducting System, Computer Music Journal, Vol. 4, No. 1, 1980, pp. 8-21.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358895</ref_obj_id>
				<ref_obj_pid>358886</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Card, S. K., Moran, T. P. &amp; Newell, A., The Keystroke-Level Model of User Performance Time with Interactive Systems, Communications of the ACM, Vol. 23, No. 7, 1980, pp. 396-410.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Foley, J.D. &amp; Wallace, V. L., "The Art of Natural Graphic Man-Machine Communication", Proceedings of the IEEE, Vol. 62, No. 4, 1974, pp. 462-471.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Guedj, R. A., tenHagen, P. J. W., Hopgood, F. R. A., Tucker, H. A. &amp; Duce, D.A. (Editors), Methodology of Interaction, IFIP Workshop on Methodology of Interaction, Seillac, France, May 1979, North-Holland Publishing Company, 1980.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>574884</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Martin, J., Design of Man-Computer Dialogues, Prentice-Hall, 1973.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Moran, T. P. (Editor), Special Issue on The Psychology of the Computer User, Computing Surveys, Vol. 13, (in press), 1981.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Newman, William M., Towards the Integrated Interactive System, proceedings of Eurographics, 1980, pp. 1-9.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Newman, William M. and Sproull, Robert F., User Interface Design (Chapter 28), Principles of Interactive Computer Graphics, Second Edition, McGraw-Hill, 1979.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1024286</ref_obj_id>
				<ref_obj_pid>1024273</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Nickerson, R. S., "On Conversational Interaction with Computers", appears on pp. 101-113 of Treu, S. (see below).]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1097028</ref_obj_id>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Schneiderman, Ben, Software Psychology: Human Factors in Computer and Information Systems, Winthrop Publishers, 1980.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1024273</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Treu, Siegfried (Editor), User-Oriented Design of Interactive Graphics Systems, Based on ACM/SIGGRAPH Workshop Conducted October 14-15, 1976 in Pittsburgh, Pa., Association for Computing Machinery, 1977.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Tsernoglou, D., Petsko, G. A., McQueen, J. E., Jr. &amp; Hermans, J., Molecular Graphics: Application to the Structure Determination of a Snake Venom Neurotoxin, Science, Vol. 197, No. 4311, 30 September 1977, pp. 1378-1381.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 VIDEO PANEL SESSION ON INTERACTIVE SYSTEMS Ronald 
Baecker Human Computing Resources Corporation SIGGRAPH has long provided a forum for the presentation, 
discussion, evaluation, and appreciation of beautiful algorithms and beautiful pictures. The Video Panel 
Session on Interactive Systems is a new SIGGRAPH event, affording us the opportunity to look at beautiful 
systems. The design of interactive systems and of their user interfaces is an art. We possess no systematic 
design methodology, no effective and universal method of evaluation. We speak in earnest praise of the 
importance of power, generality, flexibility, ease of use, ease of learning, robustness, and a dozen 
other universally admired attributes, but we do not know how to construct systems so that they are guaranteed 
to have these attributes. But many of us recognize a good interface when we see it. We bubble with admiration 
and enthusiasm, then return home and try without real success to describe with words and waves of the 
hand what we have seen that was so exciting. A few among the cognoscenti trade video tapes, but these 
are not widely circulated. SIGGRAPH now has a Video Review, but there have been few submissions. Print 
media still enjoy a monopoly on publication. We have therefore invited three of the leading practitioners 
of the design of powerful interactive systems with congenial user interfaces to present their work and 
comment upon it. Prof. Fred Brooks is Professor and Chairman of the Department of Computer Science at 
the University of North Carolina in Chapel Hill. Dr. William Newman is Manager of Research and Advanced 
Development at Logica VTS Limited, a London-based British manufacturer of office equipment. Mr. William 
Buxton is Lecturer in the Department of Computer Science of the University of Toronto, Canada. Almost 
a decade ago, Prof. Brooks, his colleagues, and students began the development of a system for biochemical 
crystallography. Crystallographers use the GRIP-75 system to determine the shape of large molecules. 
They fit a three-dimensional stick-figure molecule into a contour map of its electron density, whose 
shape they have found through X-ray diffraction. They fit the molecule one small substructure at a time, 
by translating and rotating each as a rigid unit and twisting its internal bonds. Molecular investigators 
have come from distant laboratories to use the system, and have published their results in over twenty-five 
scientific papers. One of the most thoughtful and valuable discussions of principles of user interface 
design was presented by William Newman in Chapter 28 of the Second Edition of Newman and Sproull's Principles 
of Interactive Computer Graphics. Dr. Newman was for a period in the latter half of the 70s one of a 
number of researchers experimenting with user interface design at the Xerox Palo Alto Research Center. 
Although much of this work was never published, those of us who had the opportunity to visit the lab 
were delighted with the sight of raster displays performing in some highly responsive and novel ways. 
Dr. Newman will present a sampler of some of his work and that of his colleagues in using interactive 
graphics for document illustration and office automation. William Buxton is a composer, performer, instrument 
designer, teacher, and researcher who has been involved in electroacoustic music since 1970. In 1976 
he formed the Structured Sound Synthesis Project (SSSP) to investigate the application of digital technology 
to the composition and performance of music. The work has been structured to be both an investigation 
into representations of musical data and process and a study in the use of computer technology by people 
who were experts in their own field but had little or no knowledge of computers. User interface design 
has therefore been a central concern of the project. The programs to be demonstrated have been used for 
four years by professional composers and performers from around the world in the creation of original 
works and in live performances. The three systems illustrate a broad spectrum of design problems and 
solutions. They are used for scientific, presentational, and artistic purposes, respectively. The Xerox 
systems are intended for casual use by any intelligent office worker, while the GRIP-75 and SSSP systems 
are aimed at making specialists in other disciplines expert in their skilled use. GRIP-75 employs three-dimensional 
Vector graphics, the Xerox efforts two- dimensional raster graphics, and the SSSP system two-dimensional 
vector graphics. The GRIP-75 system makes use of a wide variety of special purpose input devices, whereas 
the other two employ a single device, the mouse in one case, the tablet in the other, for almost all 
actions. Members of the audience at the panel session will be able to compare and contrast the systems 
at a far deeper level after seeing them in operation. 121 Computer Graphics Volume 15, Number 3 August 
1981 It is important to remember that in each case the systems presented are part of a stream of ongoing 
technical development, in which interfaces are designed, implemented, used, evaluated, and discarded, 
and the lessons learned incorporated into the design of the next generation. Bibliography Baecker, 
R., "Human-Computer Interactive Systems: A State-of-the-Art Review," appears on pp. 423-443 of Kolers, 
P.A., Wrolstad, M.E. &#38; Bouma, H. (Editors), Processina 9~ Visibl@ Lanauaue ~, Plenum Press, 1980. 
 Baecker, R., "Towards an Effective Characterization of Graphical Interaction", appears on pp. 127-147 
of Guedj, et. al. (see below). Baecker, R., Buxton, W. &#38; Reeves, W., Towards Facilitating Graphical 
Interaction: Some Examples from Computer-Aided Musical Composition, proceedings of the 6th Man-Computer 
Communications Conference, Ottawa, May 1979, pp. 197-207. Britton, E.G., Lipscomb, J.S. &#38; Pique, 
M.E., Making Nested Rotations Convenient for the User, Computer Graphics, Vol. 12, No. 3, 1978, pp. 222-227. 
 Brooks, F.P., Jr., The Computer "Scientist" as Toolsmith: Studies in Interactive Computer Graphics, 
Proceedina$ of the IFIp Conference, 1977, pp. 625-634. Buxton, W., Design Issues in the Foundation of 
a Computer-based Tool for Music Composition, Technical Report CSRG-97, Computer Systems Research Group, 
University of Toronto, Toronto, 1978. Buxton, W., Sniderman, R., Reeves, W., Patel, S. &#38; Baecker, 
R., The Evolution of the SSSP Score Editing Tools, Computer Music Journal, Vol. 3, No. 4, 1979, pp. 14-25. 
 Buxton, W., Reeves, W., Fedorkow, G., Smith, K.C. &#38; Baecker, R., A Microcomputer-based Conducting 
System, Computer Music Journal, Vol. 4, No. i, 1980, pp. 8-21. Card, S.K., Moran, T.P. &#38; Newell, 
A., The Keystroke-Level Model of User Performance Time with Interactive Systems, Commun~catioDs of the 
ACM, Vol. 23, No. 7, 1980, pp. 396-410. Foley, J.D. &#38; Wallace, V.L., "The Art of Natural Graphic 
Man-Machine Communication", Proceedinas of the IEEE, Vol. 62, No. 4, 1974, pp. 462-471. Guedj, R.A., 
tenHagen, P.J.W., Hopgood, F.R.A., Tucker, H.A. &#38; Duce, D.A. (Editors), Methodoloav of Interaction, 
IFIP Workshop on Methodology of Interaction, Seillac, France, May 1979, North-Holland Publishing Company, 
1980. Martin, J., Desiun of Man-Computer Dialogues, Prentice-Hall, 1973. Moran, T.P. (Editor), Special 
Issue on The Psychology of the Computer User, Comput~na Survevs, Vol. 13, (in press), 1981. Newman, 
William M., Towards the Integrated Interactive System, proceedinas of ~//.Q~KA~, 1980, pp. 1-9. Newman, 
William M. and Sproull, Robert F., User Interface Design (Chapter 28), Principles of Interactive Computer 
Graphics, Second Edition, McGraw-Hill, 1979. Nickerson, R.S., "On Conversational Interaction with Computers", 
appears on pp. 101-113 of Treu, S. (see below). Schneiderman, Ben, Software Psvchology: Human Factors 
in Computer and Information SysteMs, Winthrop Publishers, 1980. Treu, Siegfried (Editor), User-Qriented 
Desian of Interactive Graphics Sysgems, Based on ACM/SIGGRAPH Workshop Conducted October 14-15, 1976 
in Pittsburgh, Pa., Association for Computing Machinery, 1977. Tsernoglou, D., Petsko, G.A., McQueen, 
J.E., Jr. &#38; Hermans, J~, Molecular Graphics: Application to the Structure Determination of a Snake 
Venom Neurotoxin, Science, Vol. 197, No. 4311, 30 September 1977, pp. 1378-1381. 1_22
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806798</article_id>
		<sort_key>123</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[The George Washington University Core System implementation]]></title>
		<page_from>123</page_from>
		<page_to>131</page_to>
		<doi_number>10.1145/800224.806798</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806798</url>
		<abstract>
			<par><![CDATA[<p>A full implementation of the proposed standard Core System graphics subroutine package, including raster extensions, is described. Emphasis is placed on the internal structure of the implementation, and on critical design decisions. Salient features of the implementation include separate 2D and 3D viewing pipelines, a centralized dispatcher to control the flow of information between various parts of the Core, a strong separation between the device-independent and device-dependent parts of the Core, and either device-independent or device-dependent simulation of missing hardware capabilities.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>D.3.2</cat_node>
				<descriptor>Core</descriptor>
				<type>P</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011009</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language types</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Languages</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P132223</person_id>
				<author_profile_id><![CDATA[81100302796]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Foley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Electrical Engineering and Computer Science, The George Washington University, Washington, D.C.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332795</person_id>
				<author_profile_id><![CDATA[81332535158]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Patricia]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Wenner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Electrical Engineering and Computer Science, The George Washington University, Washington, D.C.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>356748</ref_obj_id>
				<ref_obj_pid>356744</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bergeron, D., Bono, P., Foley, J.D., "Graphics Programming Using the Core System", Computing Surveys, 10(4), December 1978, pp. 389-443.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Cahn, D., W. Johnston, and N. Johnston, GRAFPAC, UCID 8094, 1979.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356750</ref_obj_id>
				<ref_obj_pid>356744</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Carlbom, I., and J. Paciorek, "Geometric Projection and Viewing Transformations," Computing Surveys 1(4), 1978, pp. 465-502.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563878</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Caruthers, L., van der Bos, J., van Dam, A., "A Device-Independent General Purpose Graphic System for Stand-Alone and Satellite Graphics", Proceedings of SIGGRAPH '77, published in Computer Graphics 11(2), Summer 1977, pp. 112-119.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Chappell, G., "Implementations of the CORE," Computer Graphics 13(4), February 1980, pp. 260-278.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Ewald, R.H., and R. Fryer (eds), "Final Report of the GSPC State-of-the Art Subcommittee," Computer Graphics, 12(1/2), June 1978.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807420</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Foley, J.D., Templeman, J., and Dastyar, D., "Some Raster Graphics Extensions to the Core System," SIGGRAPH '79 proceedings, published as Computer Graphics 13(2), August 1979, pp. 15-24.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>6684</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Foley, J.D., and van Dam, A., "Fundamentals of Interactive Computer Graphics," Addison-Wesley, 1981.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988498</ref_obj_id>
				<ref_obj_pid>988497</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA["Status Report of the Graphics Standards Planning Committee", Computer Graphics, 13(3), August 1979.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Hanlon, J., "Implementation and Evaluation of Input Functions for the 1979 Core Graphics System", M.S. Thesis, EE/CS Dept., The George Washington University, May 1981.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807473</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Laib, G., Puk, R., and Stowell, G., "Integrating Solid Image Capability into a General Purpose Calligraphic Graphics Package," SIGGRAPH '80 proceedings, published as Computer Graphics 14(3), July 1980, pp. 79-85.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356749</ref_obj_id>
				<ref_obj_pid>356744</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Michener, J., and Foley, J.D., "Some Major Issues in the Design of the Core Graphics System," Computing Surveys, 10(4), 1978, pp. 445-464.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356747</ref_obj_id>
				<ref_obj_pid>356744</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Michener, J., and van Dam, A., "A Functional Overview of the Core System with Glossary," Computing Surveys, 10(4), 1978, pp. 381-388.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356746</ref_obj_id>
				<ref_obj_pid>356744</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Newman, W.M., and van Dam, A., "A Brief History of Efforts towards Graphics Standardization," Computing Surveys, 10(4), 1978, pp. 365-380.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807406</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Warner, J., Polisher, M., and Kopolow, R., DIGRAF - A FORTRAN Implementation of the Proposed GSPC Standard," SIGGRAPH '78 proceedings, published as Computer Graphics 12(3), August 1978, pp. 301-307.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Wenner, P., et al., Design Document for the George Washington University Implementation of the 1979 GSPC CORE System, Technical Report GWU-EE/CS-80-06, The George Washington University, 1980.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 The George Washington University Core System Implementation 
 James D. Foley Patricia A. Wenner Department of Electrical Engineering and Computer Science The George 
Washington University Washington, D.C. 20052 ABSTRACT A full implementation of the proposed stand- 
ard Core System graphics subroutine package, including raster extensions, is described. Empha- sis is 
placed on the internal structure of the implementation, and on critical design decisions. Salient features 
of the implementation include separate 2D and 3D viewing pipelines, a central- i2ed dispatcher to control 
the flow of infor- mation between various parts of the Core, a strong separation between the device-independent 
and device-dependent parts of the Core, and either device-independent or device-dependent simulation 
of missing hardware capabilities. 1.0 Introduction The Core System is a proposed standard graph- ics 
subroutine package developed by the Graphics Standards Planning Committee (GSPC) of the ACM's Special 
Interest Group for Graphics (SIGGRAPH). An initial definition of the Core was published in 1977 [GSPC77] 
and an updated definition plus extensions for raster graphics was published in 1979 [GSPC79]. Many of 
the raster extensions used in the update were based on a design devel- oped earlier by Foley, Templeman, 
and Dastyar [FOLE79]. The Core System definition has formed the starting point for the formal standard 
being developed by the X3H3 committee of the American National Standards Institute (ANSI). The g~neral 
functional capabilities found in the Core System are of six types: 1. Output primitives, both 2D and 
3D, used to define the objects to be displayed. The primi- tives, which are lines, character strings, 
polygons, and markers, are specified in the application (world) coordinate system. Permission to copy 
without fee all or part of this material is granted provided that the copies are not made or distributed 
for direct commercial advantage, the ACM copyright notice and the title of the publication and its date 
appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy otherwise, or to republish, requires a fee and/or specific permission. @1981 ACM O-8971-045-1/81-0800-0123 
$00.75 2. Output primitive attributes, such as line color, line style, intensity, character font, polygon 
interior index, and polygon border index.  3. Segments (disjoint collections of output primitives) which 
form the unit of modification for interactive applications, and can be created, deleted, transformed, 
blinked, made temporarily invisible, and picked using an input device.  4. Viewing capabilities to create 
an image of an object in a viewport on a view surface. Any planar geometric projection can be used for 
view- ing. A window is used to bound the part of world coordinates displayed in the viewport.  5. Operator 
interaction, using logical pick, loeator, button, text, stroke and valuator input devices. Thephysical 
prototypes of these devices are the light pen, tablet, programmed function keyboard, alphanumeric keyboard, 
tablet and potentiometer, respectively.  6. Overall control, to enable/disable input devices, select 
the view surface on which objects will be displayed, establish default attributes, and enable/disable 
clipping.  Readers interested in learning more about the Core System are referred to the December 1978 
Special Issue of Computing Surveys on Graphics Standards [BERG78, CARL78, MICH78a, MICH78b, NEWM78]. 
A list of Core System implementation efforts is given in CHAP80, and a detailed com- parison of the (1977) 
Core System and eight other graphic subroutine packages is in EWA~78. The GWU Core System implementation 
began in the Fall of 1977 as a course project and has con- tinued since then, partially sponsored by 
several research projects and serving as a base for several independent study projects and an MS thesis. 
 The implementation will shortly be complete (out- put level 3, input level 3, dimensionality 3D), 
including all raster extensions except hidden- surface removal. There are input and output drivers 
for the Tektronix 401x and Ramtek 9400. This work was partially supported by Department of Energy, Basic 
Energy Sciences grant DE-AS05-79ER 10521 and NASA-Langley grant NSG 1508.  Computer Graphics Volume 
15, Number 3 August 1981 The input drivers support at least the minimum set of input devices required 
by the specification. This paper describes our implementation with empha- sis on design decisions, the 
internal structure and the device-independent/device~dependent inter- face. Other papers reviewing the 
implementation of Core or Core-like systems are [CARU77, LAIB80, and WARN78].  2.0 Device-independent/device-dependent 
Interface The Core System, like any other device- independent graphics package, defines a set of abstract 
graphics capabilities. The Core imple- mentation must bind the abstractions to real hardware. This is 
done in two steps: the first is device-independent; the second, device-depender~. For example, in the 
device-independent phase, the viewing transformation is used to transform a line output primitive into 
normalized device coordi- nates (NDC) in the device-independent phase. In the device-dependent phase, 
normalized device coordinates are transformed to device coordinates, and device commands are constructed 
to display the line. Our implementation, like other device- independent package implementations, consists 
of a single device-independent part plus a device- dependent driver for each different graphics de- vice 
(Figure i). Thus adding support for a new display only requires writing a new device-depen- dent driver. 
 The design of the device-independent to device-dependent interface (the DI/DD interface) is a first 
major step in any implementation. DEVICE INDEPENDENT GORE DI/OO INTERFACE  DEVICE DEPENDENT DRIVERS 
 FIGURE 1 Just as the Core System itself defines abstract graphics capabilities, so too the DI/DD interface 
defines a virtual graphics station, one which presumably has fewer capabilities than that de- fined by 
the Core System itself. The DI/DD interface is the same concept as the P-codes used in many PASCAL compilers, 
but a standard inter- face has not been adopted across Core implemen- tations. Three major objectives 
guided the design of the DI/DD interface: i) complete functionality, 2) sharing of device and drivers 
with GRAFPAC, 3) clear partitioning between device- independent and device-dependent. Because we are 
implementing the full Core System and because we want to take maximum advantage of powerful display devices, 
our first objective was an interface with fairly complete functionality. It accepts transformed output 
primitives in 3D NDC space, handles most output primitive attri- butes, can create segments and modify 
their attributes (including 3D image transformations), and (for an interactive display) provides all 
the logical input devices. It does not perform view- ing transformations, deal with stroke or character 
precision text, maintain an event queue, nor do error checking or reporting. with such a high-level 
DI/DD interface, the task of writing drivers for simple devices such as plotters, DVST displays, and 
some raster displays becomes quite formidable, if all the functions must be provided in the device driver. 
A pseudo- display file (PDF) must be maintained for retained segments, and output primitive attributes 
and seg- ment attributes not supported by the hardware must be simulated. Rather than force each driver 
to do this, and duplicate code, we allow the driver to report, when initialized, which functions it actu- 
ally supports. The device-independent Core System simulates the capabilities not provided by the driver, 
thereby considerably simplifying driver implementation. More details of the simulations are given later 
in this paper. A device driver is not precluded from simu- lating capabilities which its device does 
not provide, perhaps to optimize the performance of a particular simulation. For instance, a driver for 
a dot-matrix printer can trivially build line styles into its line scan-conversion algorithm, while simulating 
line style at the device-inde- pendent level by decomposing lines into a series of short line segments 
would be relatively more expensive. A second objective of the design was to allow the drivers to be 
used with a new implementation of the GRAFPAC [CAHN79] package developed at Lawrence Berkeley Laboratory. 
This led to the decision that NDC coordinates be represented in floating point rather than integer 
form to sim- plify the use of large, high-resolution output devices.  Computer Graphics Volume 15, 
Number 3 August 1981 Another design objective was to keep the actual DI/DD interface mechanism itself 
as simple as possible, with a clean partitioning between the two parts. This was desired in part so that 
a device driver could in fact run on a separate processor. Therefore, no variables are shared across 
the interface: all parameters are passed in an array through a single entry point, along with an op-code 
and length to allow the driver to know what function to perform, and thus to know how to interpret the 
information in the array. There are about 70 op-codes for output functions and 35 for input, listed in 
Table i, but a simple output-only driver which depends on the device- independent Core for simulations 
need deal with only 20 of the op-codes.  TABLE l: DI/DD INTERFACE FUNCTIONS Output Functions Polymarker_3 
Polymarker_2 Polyline3 Polyline 2 Polygon_~ Polygon_2 Text 3 Set__segmentlmagetransformation Set_segment__highlighting 
Set visibilities Set_segment__detectability New frame Del~te_segment Delete all segments Set immediate 
visibility Make_picture__current Beginbatch of updates End batch of updates Create_retained_segment Createtemporary_segment 
Close retained segment CloseZtemporar~segment Rename segment Initialize DD Terminate DD Initializ~ view 
surface Terminate View ~urface Set NDC space Inquiredevice_capabilities Set color Setbackgroundcolor 
Setintensity Set_background_intensity Set index Set,background index Define color i~dices Define--inte~ityindices 
Inquire color indices InquireZinten~ityindices Set linestyle Inquire__linestyles Set linewidth Set_pen 
Inquire_density Set font Set charsize Set__charspace Set_string__rotation Set_charpath Set charjust Inq~ire__char__extent 
 Set__marker__symbol Set pick id Set,primitive attributes Set polygon edge style Set~polygon~interiorstyle 
 Set_pixel array colors Set__pixelarray__intensities Set pixel array indices Write raster colors Write--raster--intensities 
 Write--raster--indices Read ~aster ~olors Read--raster--intensities Read--raster--indices Writ~ raste7 
indices batch Write--raster--colorsbatch Write--raster--intensities batch Escape Inquireescape Input 
Functions Initiaiize specified__device Enable specified device Disabl~specified_device Terminatespecified__device 
Read a 2D locator Read a 3D locator Read a valuator Wait for an event from an enabled device Freeze the 
queue Unfreeze the queue Associate_a_sampleddevice with an event device Disassociate__a_sampleddev~ce__from 
an ev~nt__device Disassociate a device from all associations Delete all existing associations Set__echo__type 
Set__echo_segment Set echo position SetZpickZaperture Set__keyboard Set button Set all buttons Set stroke 
device Set--2D loc~tor device Set--3D--locator-- Set 2D locport Set 3D locport Set valuator Set echo 
surface Inq~ire__~nputcapabilities Inquireinput__devicecharacteristics  Computer Graphics Volume 15, 
Number 3 August 1981 3.0 Device-independent Structure - Output The design features of the output part 
of the device-independent Core are the viewing pipe- line, dispatcher, pseudo-display file, and simu- 
lations. Their relationships are shown in Figure 2, and each is discussed in the following sub- sections. 
 3.1 Viewing The viewing pipeline uses the viewing trans- formation to operate on output primitives 
to clip, project, and convert them into normalized device coordinates. The viewing transformation is 
de- rived from the viewing specification given by the view plane normal, view reference point, center 
of projection, window, viewport, etc., using mathe- matics developed in [FOLE81~. The viewing trans- 
formation is calculated whenever a segment is created if and only if part of the viewing speci- fication 
has been changed since the previous segment was created. This saves considerable time for applications 
which create many segments using the same viewing transformation. As is the case with DIGRAF [WARN78], 
sepa- rate paths through the viewing pipeline are provided for 2D and 3D output primitives, saving computation 
time for 2D applications. The code for the 2D primitives is simple, little memory is required so the 
cost is minimal. Although we do not do so, selective linking of the 2D and 3D pipelines could be used 
to slightly reduce link- ing time. The actual code for the extra 2D pipe- lines is so simple that its 
memory use is negligible.  VIEWING When a segment is created, a 2D/3D flag is set to 2D if all of the 
following conditions are true:  l) no modelling transformation is applied, 2) view plane normai is (0.0,0.0,1.0) 
or (0.0,0.0,-i.0), 3) view up vector is (0.0,i.0,0.0), 4) depth clipping is off, 5) parallel projection 
is specified, and, 6) 2D NDC space has been specified. Whenever the flag is set to 2D, output primitives 
are sent through the 2D pipeline. This adaptive behavior of the viewing pipeline, implemented at the 
trivial cost of the above six tests each time a segment is opened plus a single test for each output 
primitive, demonstrates that a 3D graphics package can be efficiently used for 2D applica- tions. This 
has in the past been a frequently- debated point. The output of the viewing pipeline is placed into 
a communication packet (implemented as an array), and passed on to the dispatcher. There are packet formats 
for each type of output pri- mitive: marker, text, line, polymarker, poly- line, and polygon. The format 
is op-code, length, and then data - NDC coordinates, character codes, marker types, etc. The data in 
the packet is organized in the same format as is passed across the DI/DD interface.  OUTPUT SEGMENT 
SPECIFICATIONS PRIMITIVES OPERATIONS WORLD COORDINATES VIEWING PIPELINE CLIPPED, NORMALIZED OEVICE COORDINATES 
SIMULATIONS I IDISPATCHER t t PSEUO0 DISPLAY FILE SYSTEM  DI/DD INTERFACE FIGURE 2  Computer Graphics 
Volume 15, Number 3 August 1981 There is no packet for a "move" output pri- mitive, because all the packets 
contain their own starting point. This was done in planning ahead to hidden surface/hidden edge removal, 
when each output primitive must be completely represented in its own right. A polyline is transmitted 
as a single packet rather than as a sequence of line packets for efficiency, both to minimize the number 
of packets created and to allow devices with polyline instructions to make use of them.  This is important 
to save space in the refresh list and to save time. For example, the Ramtek 9400 has a 2-5 millisecond 
set-up time for a line instruction, but takes only an additional 70 micro- seconds of set-up time for 
each additional line segment of a polyline. 3.2 Dispatcher  The dispatcher is the traffic control point 
of the output subsystem of the device-independent Core, as depicted in Figure 2. It decides which information 
packets should be sent across the DI/ DD interface to the currently selected device driver (our implementation 
does not support multi- ple selected devices), and which should be stored in the PDF for later use whenever 
a new-frame action is requested. The decision logic is com- plex, and takes into consideration: -segment 
type -retained or temporary, -segment visibility, -batch of updates -on or off, -device type -buffered 
or unbuffered, -attribute type -dynamic or static, and -device defer type -defer retained, defer deletions, 
or defer all.  Based on the primitive attributes and knowledge of device capabilities as reported by 
the driver when the view surface was selected, the dis- patcher also decides if a simulation is necessary 
and calls the appropriate simulation routines, as described in Section 3.4. Two dispatcher entry points 
are provided so that packets which always go to the device driver, regardless of the above control considerations 
are not delayed and are passed almost directly to the device driver. These include echoes from the input 
subsystem and segments being redrawn as the result of a new- frame action.  Several device drivers 
can be linked to the device-independent Core: output primitives are directed to the currently-selected 
driver (i.e., view surface). However, many applications use a single view surface per session, but may 
need to be used with different view surfaces at different times. All device drivers are loaded to permit 
run-time view surface selection. Each view sur- face is assigned a numeric identifier. Alter- nately 
to avoid the overhead of linking multiple drivers, link-time selection is provided, with only a single 
driver linked.  3.3 Pseudo-display file  A pseudo-display file (PDF) is maintained for display devices 
which report to the device-inde- pendent Core that they are unbuffered (i.e., have no display file). 
Logically, the PDF is a set (unordered collection, no replication) of segments and their attribute 
values. Each segment is a sequence (ordered collection, replication allowed) of output primitives and 
output primitive attribute values in effect when the segment was opened. This is necessary to maintain 
the independence of each segment in the PDF. The dispatcher accesses the PDF through a set of procedures 
which isolate the physical stor- age structure of the PDF from the logical struc- ture. The physical 
structure is two large arrays, one for the set of segment names and attributes, the other for the sequences 
of output primitives and their attributes. Other physical structures, such as a direct-access file for 
the segment con- tents, can be readily substituted. Packets of information in the same format as passed 
to the dispatcher are sent to the PDF system, which examines only the length field in the process of 
storing the packet into the PDF. For buffered display devices, segment names and attributes are kept 
in the PDF, but segment ~ontents are not. For unbuffered devices, segments are redrawn out of the PDF 
in response to all direct new-frame actions and to all segment operations (such as dynamic attri- bute 
changes) which in turn cause a new-frame action. As an example of a dynamic attribute change, when an 
image transformation is set, the affected segment is read from the PDF, multiplied by the image transformation 
matrix and sent to the device. The segment in the PDF is not modified, because image transformations 
are not cumulative. Our Tektronix 401x driver, which is unbuffered, has the device-independent Core perform 
its transfor- mations. In contrast, the Ramtek 9400 driver maintains its own buffer and therefore does 
its own image transformations.  3.4 Simulations  In order to maintain device independence, simulation 
of certain primitive attributes is provided: line style, line width, pen, polygon fill, polygon edge 
type, and intensity on a color device used as a monochrome device. The segment attributes of highlighting 
and image transformation can also be simulated. The simulation routines are invoked by the dis- patcher 
just before the primitives are sent across the DI/DD interface. Packets of information can come thru 
the dispatcher from two sources: the viewing pipe- line or the PDF. The dispatcher records the at- tributes 
of the segment currently being processed, and knows which attributes need to be simulated on the currently-selected 
view surface. By using this information and the op-code of each packet, the dispatcher decides which 
simulations to invoke. Computer Graphics Volume 15, Number 3 August 1981 VIEWING OUTPUT SEGMENT SPECIFICATION 
PRIMITIVES TEXT OPERATIONS  >[ VIEWING H STRING  PIPELINE STROKE CHARACTER ,~ PSEUDO  SIMULATIONS 
t DISPATCHER OISPLAY FKE SYSTEM  DI/DD INTERFACE FIGURE 3  3.5 Text  Figure 3 shows that stroke and 
character pre- cision text are provided by the device-independent Core. For stroke precision text, a 
series of moves and lines is decoded from font tables and sent thru the viewing pipeline. Four fonts 
are currently pro- vided, along with three spacing modes - mono, spread and variable. The spacing parameters 
applicable to stroke precision text are applied by the text rou- tines before the moves and lines are 
sent to the viewing pipeline. For character precision text, a string of characters is decomposed into 
a series of single- character text string op-codes, each preceded by a position calculated from the text 
spacing and orientation attributes. The device driver, which is required to provide simulated or hardware 
string-precision text, then displays each char- acter at the appropriate position. 4.0 Device-independent 
Structure -Input For the input part of the Core, shown in Figure 4, the design features are the event 
queue, synchronous and asychronous event handling, echo and association handling and the use of the pseudo-display 
file. Details are reported in [HANLSI].  4.1 Event Queue The event queue is maintained by the device- 
independent Core as a one-dimensional array. The event queue is a set of event reports returned from 
the device drivers, including information from any associated sampled devices. Each report has a header 
which contains identifying information (device class, device number).  4.2 Associations Associations 
exist to link sampled devices to event devices. For example, a valuator could be associated with a certain 
button. When that button is pressed, the valuator is read and its value in- cluded with the button event 
report. The asso- ciation map (Fig. 4) contains an entry for each event device ho indicate which of the 
locator and valuator sampled devices are associated with the event device. Natural associations are 
reported to the device-independent Core by an input device driver when it is enabled. This type of association 
is inherent to specific hardware, such as the Tektronix crosshair cursor, a locator, being asso- ciated 
with a button event. Then if the natural association is established by the application pro- gram, the 
device driver returns the associated locator or valuator data with the event report. Otherwise the device-independent 
Core, once noti- fied of an event by the driver, requests the driver to sample the associated valuators 
and locators. This is in essence another simulation -the device-independent Core does whatever the driver 
reports it does not do.  Computer Graphics Volume 15, Number 3 August 1981 4.3 Asynchronous input 
The asynchronous input capabililities of the Core System may be implemented either synchro- nously or 
asynchronously, depending on the capa- bilities of the input device/driver. The appli- cation program 
using the Core is unaffected by which implementation is emploved. A device driver which operates in synchronous 
mode returns event data only when AWAIT EVENT is called. An asynchronous device driver has an interrupt 
handler which places events on the event queue as they occur by calling a queue manager routine in the 
device-independent Core. For the synchronous implementation the event queue is always empty. When the 
application pro- grammer calls AWAIT__EVENT, control is passed to the device driver, which starts a polling 
loop to alternately wait and read until a response is received from any of the enabled event devices. 
The device driver then returns the event report to the device-independent Cpre, which makes it the current 
event report and returns from the AWAIT EVENT call. In the asynchronous implementation, events may occur 
any time one or more event devices are enabled. An asynchronous interrupt handler in the device driver 
accepts the event and samples any naturally associated devices. The event queue is then locked so that 
the device-inde- pendent routines can not access it, the event report is placed on the event queue, additional 
associated devices are read by the device- independent Core, and the event queue is un- locked. When 
the application programmer calls AWAIT EVENT, there may already be something on the queue because the 
asynchronous interrupt handler acts independently of the application. If the event queue is not empty, 
the queue is locked, the event report is removed from the queue, and the queue is unlocked. If the queue 
is empty, the application program is idled until an event occurs or the interval given an AWAIT EVENT 
has passed. The Core returns the first event on the queue to the appli- cation program as the current 
event report. The application program can then call the appropriate routines to obtain the data in 
the event report.  4.4 Synchronous input  Input level 3 of the Core defines both synchronous and asynchronous 
capabilities. The design decision here is whether to build the synchronous capabilities "on top of" the 
asyn- chronous, or separately. We chose the former for ease of imolementation. Therefore the synchronous 
input functions also use the asynchronous input capabilities. In response to one of the synchro- nous 
functions, the device-independent Core en- ables the appropriate device and calls AWAIT EVENT. The resulting 
event report is returned directly to the application programmer, as required by the synchronous function, 
and the device is disabled.  4.5 Interaction between input and output  There are several interactions 
between the input and output subsystems of the Core. The first involves the pick event, which returns 
a pick-id and segment name for a buffered device. For an unbuffered device, the device-independent input 
subsystem must access the PDF to extract this infor- mation. If, as is usually the case, the pick is 
simulated with a position input followed by a scan of the PDF, then the input position which is returned 
in the event report in place of a segment name and pick-id must be transformed by the inverse of a segment's 
image transformation before ex- amining the PDF to locate the segment being pick- ed. This is the only 
situation where a logical input device is simulated by the device-indepen- dent Core. We do not maintain 
pick or segment bounds to simplify picking, as DIGRAF does [WARN78], but intend to add this feature. 
 INPUT [~ ECHO IOUTINES~ TO,.  ~ OgTru= INPUT T f J ";I;"" I OEVICE HADADER " ] OIHULATIOD .L DI/DO 
INTERFACE  < INPUT I VIEWIR6 OUTPUT TEXT $EOHENT I SPECIFICATION PDIHTIVES OPERATIONS I l i I.... 
VIEWIN6 I PIPELINE  I t H ''o'' I I 1,[ 4, H . PSEUDO JSIHULATIONS I_ J OISPATCHEA L DISPLAY F ] FILE 
$YOTEH I-"i I Ot/DD INTERFACE OUTPUT I > FIGURE 4  Computer Graphics Volume 15, Number 3 August 1981 
Another interaction is echo. The input sub- system generates some output, perhaps highlighting or color 
change. It must do this without affecting the output control status, the currently open seg- ment, etc. 
User level image transformation rou- tines are called for the echo types which require them. Temporary 
segments are used to display the other echo types. The input subsystem sends these to the output dispatcher 
along with the control information necessary to create a temporary seg- ment while the output subsystem 
has a retained segment open.  5.0 Error Handling All errors are reported to the user through a standard 
error handling facility. The name of the called subroutine, the parameters of the call, and an error 
code (keyed to the error number in GSPC79 and augmented as required for additional errors) are passed 
to an error reporting module. The subroutine name, parameters, error number, error message (extracted 
from a file of error messages), and severity code are reported on a user-specified surface (or device). 
For example, the following error message would be printed if a line primitive were issued and no segment 
is open: THERE IS NO OPEN SEGMENT. ERROR NUMBER 201 WAS RECEIVED FROM ROUTINE LINA2, SEVERITY=6 CALL 
LINA2(0.10000E+01, 0.00000E+00) Our implementation provides only the simplest error handling, invoking 
the LOG ERROR function. It attempts to recover and continue, but the graphical result cannot be guaranteed. 
It is the responsibility of the application program to inter- pret the severity of errors and stop execution 
as necessary, else a run-time error may halt the process. Error checking is a time-consuming activity. 
We would like to add a function which disables all error checking for use in situations where the application 
programmer is confident of the re- sults. There are a group of er£ors concerning attri- butes, numbered 
202-207 and 211 in GSPC79, which state that an attribute is not supported on a selected view surface. 
We do not report these errors, because it limits device independence. An application shifted from a "rich" 
device to a "plain" device would suddenly receive many error messages. If the attribute is not provided 
on the new device, a simulation is done if possible. If a simulation is not possible, the Core uses a 
 reasonable default attribute value.  6.0 Extension of Color and Intensity Specifications An extension 
which we have provided is direct specification of color and intensity. When a device is initialized 
using INITIALIZE VIEW SURFACE, one parameter is used to indicate whether the spec- ification will be 
direct or indexed, and another to specify whether intensities or colors will be used. If direct, it 
is the responsibility of the device driver for a raster display with a look-up table to pre-load the 
table in a methodical way. This allows the device to easily convert a color spec- ification into an index, 
which is then loaded into the refresh buffer. This extension to direct specification of color and intensity 
requires about 30 additional user level routines. Any routine in GSPC79 which specifies index values 
has counterparts to specify color and intensity directly. 7.0 Device-dependent Structure The device 
drivers have a top level module which is essentially a case statement, based on the input or output function 
op-code passed to the driver as part of its arguments. Any func- tion which is not supported is treated 
as a null case, and is ignored. This top-level case state- ment is necessary because the DI/DD interface 
consists of a single procedure call to simplify run-time device binding and distributed proces- sing. 
Because the device-independent Core is responsible for providing simulations of functions which the device 
driver does not provide, a simple output driver is quite straightforward: the Tektronix 401x driver is 
about 700 actual lines of FORTRAN, most of which is code generation and buffer management. A buffered 
driver maintains a segment table and display list. The device driver for a raster display with a look-up 
table is responsible for loading the table and, if necessary, remembering what has been loaded in the 
table: no color tables of any kind are maintained in the device- independent Core. Buffered drivers for 
color raster displays can be complex: the output driver for the Ramtek 9400 is about 3700 lines of C. 
 Status information is returned from the device driver in the communication packet. In addition, an error 
code is returned if any un- resolvable condition, such as hardware error or table overflow, is detected 
in the device driver. The device drivers maintain their own status tables independent from each other 
and the device- independent Core.  8.0 Software Engineering Issues Our programming environment influenced 
some design decisions. The project started in 1977 using FORTRAN on the HP3000, with a 32K byte pro- 
gram module limit, so segmentation was necessary and space was at a premium. We switched to the VAX 11/780 
in spring 1979 and that summer ini- tiated a major redesign effort, based upon the 1979 GSPC Status Report 
and the new virtual mem- ory environment. Concern with space limits vanished, while additional emphasis 
was placed on implementation ease, modularity, and maintain- ability. A design document [WENN80] was 
prepared, as were module specifications for all 170 user- level and 60 internal modules. The language 
for the device-independent part remained FORTRAN, for portability of the imple- mentation. Some modules 
use FORTRAN77 features (IF-THEN-ELSE, CHARACTER), somewhat limiting  Computer Graphics Volume 15, Number 
3 August 1981 portability. As we knew when we started, FORTRAN is an unsatisfactory implementation language. 
The FORTRAN 77 extensions provide some relief in flow of control, but none whatsoever in data structuring. 
If we were starting over today, our choice would be from PASCAL, ADA, PL/I, or C. Computer/compiler 
dependencies are isolated to just a few subroutines so that changes neces- sary for different word lengths 
or different run- time libraries and system services are confined. Also our module level documentation 
indicates such dependencies.  9.0 Implementation Status The output part of the Core was released to 
the National Energy Software Center in September 1980. There are 22000 lines of source code, of wh{ch 
approximately 35% are comments. The re- lease contains all output functions and raster extensions except 
hidden line and surface re- moval, shaded and patterned polygons. There is a FORTRAN driver for the Tektronix 
401x and a C driver for the Ramtek 9400. The integration of the input and output subsystems is going 
on during fall 1980, with release planned in spring of 1981. Our implementation is the result of about 
three person-years of work. The current implementation is available from the authors. As our implementation 
was developed as a research tool, we are interested in what amount of overhead is involved with complete 
function- ality. Benchmarking of size and speed is being pursued. We expect to make these measurements 
available separately. We are also pursuing the question of ease of use of the input devices as defined 
by the Core and the adequacy of the -raster functions in providing full access to the capabilities of 
available raster devices. i0.0 Acknowledgments Many people have made important contri- butions to the 
design and coding of our imple- mentation: Charles Andrews, who implemented the image transformation 
simulation; Terry Bleser; Paul Brewster, who implemented the stroke and character precision text; Michael 
Callahan; Dara Dastyar; Patty Denbrook; Rob Gurwitz of Brown University, who implemented part of the 
Ramtek output driver; James Hanlon, who designed the input part of the DI/DD interface and the device-independent 
input subsystem; Tom Herbert; Aragam Nagesh, who implemented the 2D and 3D viewing pipelines; George 
Rogers; Jim Templeman; George Winkert, who implemented the input part of the Ramtek driver; and Albert 
Yen of Lawrence Berkeley Laboratories, who designed the output part of the DI/DD interface and implemented 
much of the Ramtek output driver. BERG78 Bergeron, D., Bono, P., Foley, J.D., "Graphics Programming 
Using the Core System", Computing Surveys, 10(4), December 1978, pp. 389-443. CAHN79 Cahn, D., W. Johnston, 
and N. Johnston, GRAFPAC, UCID 8094, 1979.  CARL78 CARU77 CHAP80 EWAL78 FOLE79 FOLE81 GSPC79 
HANL81 LAIB80 MICH78a MICH78b NEWM 78 WARN78 WENN80 Carlbom, I., and J. Paciorek, "Geometric 
Projection and Viewing Transformations," Computing Surveys 1(4), 1978, pp. 465-502. Caruthers, L., 
van der Bos, J., van Dam,   A., "A Device-Independent General Purpose Graphic System for Stand-Alone 
and Satel- lite Graphics", Proceedings of SIGGRAPH '77, published in Computer Graphics 11(2), Summer 
1977, pp. i12-i19. Chappell, G., "Implementations of the CORE," Computer Graphics 13(4), February 1980, 
pp. 260-278. Ewald, R.H., and R. Fryer (eds), "Final Report of the GSPC State-of-the Art Sub- committee," 
Computer Graphics, 12(1/2), June 1978. Foley, J.D., Templeman, J., and Dastyar, D., "Some Raster Graphics 
Extensions to the Core System," SIGGRAPH '79 proceedings, published as Computer Graphics 13(2), August 
1979, pp. 15-24. Foley, J.D., and van Dam, A., "Fundamentals of Interactive Computer Graphics," Addison- 
Wesley, 1981. "Status Report of th~ Graphics Stand- ards Planning Committee", Computer Graphics, 13(3), 
August 1979. Hanlon, J., "Implementation and Eval- uation of Input Functions for the 1979 Core Graphics 
System", M.S. Thesis, EE/CS Dept., The George Washington University, May 1981. Laib, G., Puk, R., 
and Stowell, G., "Integrating Solid Image Capability into a General Purpose Calligraphic Graphics 
Package," SIGGRAPH '80 proceedings, pub- lished as Computer Graphics 14(3), July 1980, pp. 79-85. 
Michener, J., and Foley, J.D., "Some Major Issues in the Design of the Core Graphics System," Computing 
Surveys, 10(4), 1978, pp. 445-464. Michener, J., and van Dam, A., "A Functional Overview of the Core 
System with Glossary," Computing Surveys, 10(4), 1978, pp. 381-388. Newman, W.M., and van Dam, A., "A 
Brief History of Efforts towards Graphics Standardization," Computing Surveys, 10(4), 1978, pp. 365-380. 
 Warner, J., Polisher, M., and Kopolow, R., DIGRAF - A FORTRAN Implementation of the Proposed GSPC Standard," 
SIGGRAPH '78 proceedings, published as Computer Graphics 12(3), August 1978, pp. 301-307. Wenner, p., 
et al., Design Document for the George Washington University Imple- mentation of the 1979 GSPC CORE System, 
Technical Report GWU-EE/CS-80-06, The George Washington University, 1980.     
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806799</article_id>
		<sort_key>133</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[GRAMPS - A graphics language interpreter for real-time, interactive, three-dimensional picture editing and animation]]></title>
		<page_from>133</page_from>
		<page_to>142</page_to>
		<doi_number>10.1145/800224.806799</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806799</url>
		<abstract>
			<par><![CDATA[<p>GRAMPS, a graphics language interpreter has been developed in FORTRAN 77 to be used in conjunction with an interactive vector display list processor (Evans and Sutherland Multi-Picture-System). Several of the features of the language make it very useful and convenient for real-time scene construction, manipulation and animation. The GRAMPS language syntax allows natural interaction with scene elements as well as easy, interactive assignment of graphics input devices. GRAMPS facilitates the creation, manipulation and copying of complex nested picture structures. The language has a powerful macro feature that enables new graphics commands to be developed and incorporated interactively.</p> <p>Animation may be acheived in GRAMPS by two different, yet mutually compatible means. Picture structures may contain &#8220;framed&#8221; data, which consist of a sequence of fixed objects. These structures may be displayed sequentially to give a traditional frame animation effect. In addition, transformation information on picture structures may be saved at any time in the form of new macro commands that will transform these structures from one saved state to another in a specified number of steps, yielding an interpolated transformation animation effect.</p> <p>An overview of the GRAMPS command structure is given and several examples of application of the language to molecular modeling and animation are presented.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Graphics language interpreter]]></kw>
			<kw><![CDATA[Picture editor]]></kw>
			<kw><![CDATA[Real-time animation]]></kw>
			<kw><![CDATA[Vector display list processor]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>D.3.1</cat_node>
				<descriptor>GRAMPS</descriptor>
				<type>P</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003766</concept_id>
				<concept_desc>CCS->Theory of computation->Formal languages and automata theory</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011039</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Formal language definitions</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Languages</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14067123</person_id>
				<author_profile_id><![CDATA[81100162542]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[T.]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[O'Donnell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Resource for Computation in Chemistry, Lawrence Berkeley Laboratory, University of California, Berkeley, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14072432</person_id>
				<author_profile_id><![CDATA[81100178453]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Arthur]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Olson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Resource for Computation in Chemistry, Lawrence Berkeley Laboratory, University of California, Berkeley, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Gund, P., Andose, J.D., Rhodes, J.B., Smith, G.M. "Three-Dimensional Molecular Modeling and Drug Design." Science, 208, 1425-1431, (1980).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tsernoglou, D., Petsko, G.A., McQueen, J.E., and Hermans, J. "Molecular Graphics: Application to the Structure Determination of a Snake Venom Neurotoxin". Science, 197, 1378-1381, (1977).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Newman, W.M., and Sproul, R.F. Principles of Interactive Computer Graphics, 2nd Edition, McGraw-Hill Book Company, New York, (1979)]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bernstein, F.C. et al. "The Protein Data Bank: A Computer Based Archival File for Macromolecular Structures." J. Mol. Biol., 112, 535-542 (1977).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Allen, F.H., Kennard, O., Motherwell, D.S., Town, W.G., Watson, D.G. "The Cambridge Crystallographic Structural Data File." J. Chem. Doc., 13, 119 (1973).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Defanti, T. "The Digital Component of the Circle Graphics Habitat." Proceedings National Computer Conference, (1976).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Burtnyk, N. and Wein, M. "Computer Generated Key Frame Animation." J. Soc. Motion Picture and Television Engineers, 80, 3, 149-153, (1971).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Harrison, S.C., Olson, A.J., Schutt, C.E., Winkler, F.k., and Bricogne, G. "Tomato Bushy Stunt Virus at 2.9 Angstroms Resolution." Nature, 276, 368-373, (1978).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807491</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Badler, N.I., O'Rourke, J. and Kaufman, B. "Special Problems in Human Movement Simulation." Computer Graphics, 14, 3, 189-197, (1980).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Muybridge, E. The Human Figure in Motion, Dover Publications, New York, (1955).]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360357</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Burtnyk, N. and Wein, M. "Interactive Skeleton Techniques for Enhancing Motion Dynamics in Key Frame Animation." Communications of the ACM, 19, 10, pp. 564-569, (1976).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 GRAMPS - A Graphics Language Interpreter for Real-Time, 
Interactive, Three-Dimensional Picture Editing and Animation* T.J. O'Donnell Arthur J. Olson National 
Resource for Computation in Chemistry Lawrence Berkeley Laboratory University of California Berkeley, 
CA 94720 Abstract GRAMPS, a graphics language interpreter has been developed in FORTRAN 77 to be used 
in conjunction with an interactive vector display list processor (Evans and Sutherland Multi-Picture-System). 
Several of the features of the language make it very useful and convenient for real-time scene construction, 
manipulation and animation. The GRAMPS language syntax allows natural interaction with scene elements 
as well as easy, interactive assignment of graphics input devices. GRAMPS facilitates the creation, manipulation 
and copying of complex nested picture structures. The language has a powerful macro feature that enables 
new graphics commands to be developed and incorporated interactively. Animation may be acheived in GRAMPS 
by two different, yet mutually compatible means. Picture structures may contain "framed" data, which 
consist of a sequence of fixed objects. These structures may be displayed sequentially to give a traditional 
frame animation effect. In addition, transformation information on picture structures may be saved at 
any time in the form of new macro commands that will transform these structures from one saved state 
to another in a specified number of steps, yielding an interpolated transformation animation effect. 
 An overview of the GRAMPS command structure is given and several examples of application of the language 
to molecular modeling and animation are presented. Keywords: Graphics language interpreter, Vector display 
list processor, Picture editor, Real-time animation. I. Introduction Computer graphics usage within 
the scientific community has been growing dramatically in recent years. Applications in chemistry range 
from the production of publication graphics to sophisticated interactive real-time utilization in macromolecular 
structure determination and drug design [1,2]. As chemical systems under study become more complex there 
will be an even greater need for computer graphics tools to aid the researcher in visualizing large amounts 
of data, in constructing and evaluating models and in presenting results to other scientists and to the 
public. To this point chemical graphics software has been developed principally in an ad hoc manner, 
with a single graphics task in mind. Our motivation for undertaking the work described here was to develop 
an easily usable interactive graphics interpreter to form the basis for a multitude of differing chemical 
applications at the National Resource for Computation in Chemistry. Areas of chemistry with growing need 
 *This work was supported by the Director, Office of Energy Research, Office of Basic Energy Sciences, 
Chemical Sciences Division of the U.S. Department of Energy under Contract No. W-7405-ENG-48 and under 
a grant from the National Science Foundation Grant No. CHE-7721305. for interactive graphics include 
x-ray crystallography, reaction dynamics, statistical mechanics and quantt~n chemistry. OUr criteria 
for graphics system development were: it should be flexible and easy to use, it should support real-time 
dynamic graphics interaction, and it should be usable as soon as possible. With these goals in mind GRAMPS 
(GRAphics for the Multi-Picture System) development was initiated, and rudimentary utilization followed 
almost immediately. At first only the very basic capabilities were incorporated. GRAMPS has developed 
as usage and demand for additional capabilities have dictated. We have found that GRAMPS has proven quite 
useful in a variety of chemical research problems, and that its capabilities have reached beyond, to 
the broader applications of general 3-D picture editing and real-time animation. GRAMPS development is 
still on-going. This paper serves as a progress report, chronicalling the first 8 months of work. 2. 
The GRAMPS System and Language Interpreter GRAMPS is a graphics software system consisting of about 
70 subroutines written in VAX FORTRAN IV and makes use of the structured programming and character data 
enhancements available in the 1977 standard description of FORTRAN. The computer system on which GRAMPS 
development has taken place is a VAX 11/780 with 2.5 M bytes of main memory and two 167 M byte disks 
for file storage. The graphics processor and display are the Evans and Sutherland Multipicture system 
(MPS) with 48 K words of picture memory. All programming has been done in VAX FORTRAN operating under 
Digital Equipment Corporation's Virtual Memory System (VMS) and the Evans and Sutherland supplied MPS 
driver and associated FORTRAN callable subroutines. While GRAMPS is dependent on the E&#38;S subroutine 
library, it is not inherently dependent on the MPS. GRAMPS' code occupies about 53 K bytes of memory. 
Since the VAX is a virtual memory machine, no overlaying was done. At the NRCC, the VAX operates in a 
multi-user environment with many processes running concurrently. GRAMPS is run as a background subprocess 
at normal or lower priority. It offers good interactive response with minimal impact on other processes 
running on the VAX. Figure I shows a diagram of GRAMPS' parts and their interaction with other graphics 
system components as discussed below. r .................... ~ r .................... -i ', Nu| t, -P,cture-Syltl= 
" ', r.~ AMPS eo(telrl ' ................... : ............. Umer'= L ] MoLlbow p,oQ~,, I "I I Figure 
I. Diagram of GRAMPS' parts and their interactions with other components of the graphics system.  GRAMPS 
consists of two interacting but independent parts. One part manages the update cycle of the graphics 
data base. This includes, for example, transformation matrix updates based on interactive devices assigned 
to objects' transformations. This part also manages the nested tree structures (GROUPs) of objects. Input 
to this part of GRAMPS is accomplished through the use of the graphics input devices, such as dials, 
joysticks and tablet. Input also comes from the second part of GRAMPS.  The second part is a command 
language interpreter through which the user creates or modifies the graphics data base. In addition, 
 the command language interpreter allows the user to alter the way that the graphics input devices are 
used for update cycle functions. Communication with the GRAMPS command language interpreter is accomplished 
through the VAX/VMS mailbox system for interprocess communication. Commands are "mailed" to the GRAMPS 
interpreter by any other process running on the VAX. Most often, the user runs a simple program which 
reads commands from the terminal and sends them to GRAMPS. It is also possible, however, to run any other 
program which constructs GRAMPS commands and sends them off to be interpreted. This latter mode of operation 
is quite general and powerful for some applications, while the former mode is more useful for interactive 
experimentation with images. Switching modes of operation is done simply by stopping one program and 
starting another, while leaving GRAMPS running in the background. In the following sections the GRAMPS 
co~and language will be described. This will include the syntax of the language, the macro features of 
the language, and the data types supported.  2.1 Syntax of GRAMPS The syntax of the GRAMPS command 
language is of the type: VERB followed by KEYWORDs and ARGUMENTs (see [3] p.454). Every GRAMPS command 
must begin with a VERB which specifies the nature of the operation which is to be carried out. Some examples 
of VERBs are SAVE, ROTATE and DRAW. A COMMAND consists of at least a VERB, which may be modified by SWITCHes. 
In addition, most COMMANDs accept or require one or more KEYWORDs or ARGUMENTs. Blanks or commas are 
recognized as delimiters between ARGUMENTs and KEYWORDs. The following are examples of complete GRAMPS 
commands. GET CUBE ROTATE CUBE X 16384 SCALE CUBE XYZ .+2000 GET SQUARE GROUP CUBE SQUARE FIGURES TRANSLATE 
FIGURES XY -20000<D6<20000 FREEZE/GROUP FIGURES GRAMPS commands fall, roughly, into three categories: 
picture creation commands (GET, DRAW and TEXT), picture transformation commands (ROTATE, TRANSLATE, SCALE, 
PERSPECTIVE, and MOVE) and system utility commands (such as SHOW, SET STEREO, SAVE and ADVANCE). Table 
I lists all GRAMPS commands and briefly describes their functions. Table I. GRAMPS Command Names and 
Descriptions ADVANCE Advance display one cycle and trigger camera. BLANK Discontinue display of an object. 
BLINK Start/stop blinking an object. CHANGE Change the device assignment of an object. CONDENSE Condense 
a group "structure into a simple object. CONTINUE Allow continuous transformations. COPY Make a copy 
of an object. DASH Cause an object's line texture to be dashed. DEASSIGN Free a device from all previous 
assignments. DO Interpret a macro VERB file. DRAW Accept picture input from the tablet. EXIT Exit from 
the GRAMPS program. FIX Fix an object's transformations at their current values. FREEZE Deactivate device 
assignments for an object. FRAME Specify which frame of a framed object is to be displayed.  Table I. 
(continued) GET Retreive a object from the disk. GETFRAME Retreive a framed object from the disk. GROUP 
Create a group/tree structure of objects. HELP List GRAMPS command names and object names. INSERT Insert 
an object into a group. INTENSITY Specify the intensity of an object. MOVE Specify a post-perspective 
translate of a world. PAUSE Disallow continuous transfor- mations. PERSPECTIVE Specify a perspective 
window for a world. PICKAXIS Choose and store an arbitrary axis. RELEASE Reactivate devices assigned 
to an object. REMOVE Remove an object from a group. RENAME Give an object a new name. RESET Reset an 
object's transformations to default values. RESTART Reinitialize the GRAMPS system. ROTATE Specify the 
rotation of an object. SAVE Construct and save GRAMPS transformation commands necessary to transform 
an object to its current postion. SCALE Specify the scale of an object. SET Change values of various 
system parameters: -STEREO Initiates stereo viewing mode; activates RWORLD. -VERIFY Echoes GRAMPS commands 
as they are interpreted. -DEFAULT Sets default directory for file transfers. -SENSITIVITY Modifies the 
sensitivity of input devices. -WATCH Allows display update during macro interpretation. -COLLECT Tells 
GRAMPS to collect command lines into a file. -FILMPAUSE Sets time for pause between display cycles. 
-BUFFERED Allows display lists to be buffered. SHOW Show the current transformation values of an object. 
SYNONYM Create a synonym of an object. TEXT Create an object containing text data. TRANSLATE Specify 
the translation of an object. TREE Show the tree structure of all grouped objects. USEAXIS Use a previously 
picked axis on an object. Picture input in GRAMPS is normally accomplished by use of the GET command. 
This command retrieves a file from the disk which contains Cartesian coordinates of the endpoints of 
the lines making up the picture. When GETting m picture, a set of transformation matrices is written 
into the display list in host computer memory along with the coordinate data. The picture input files 
are normally created using a set of utility programs available at the NRCC, from structural coordinate 
data available in the chemical literature [4,5], or from more complex programs which perform theoretical 
chemical calculations. It is also possible to input pictures through the tablet, using the DRAW command. 
A facility for drawing three-dimensional objects is available in the DRAW command. Finally, the TEXT 
co~and allows creation of pictures containing references to E&#38;S character memory. Once a picture 
has been created or input to GRAMPS, the transformation matrices which are a part of every GRAMPS picture 
may be readily modified using one of the transformation commands. The syntax of GRAMPS transformation 
commands is: VERB NAME AXIS FUNCTION where VERB is one of the transformation commands, NAME is the 
name of a previously input picture, AXIS is one of the Cartesian or arbitrary axis names, and FUNCTION 
may be some value, a graphics input device or a more complicated function. In general, when a graphics 
input device is given as the FUNCTION in a transformation command, the device is used to dynamically 
control the transformation specified in the command. When a value is given as the FUNCTION, then the 
transformation immediately takes on that value. It is also possible to specify bounds for FUNCTIONs and 
to interpolate smoothly between transformation values. FUNCTIONs are discussed in section 3 describing 
the use of GRAMPS more fully. The third class of GRAMPS commands contains a wide range of inquiry and 
manipulation functions that enhance overall performance. For example, the CONDENSE command operates 
on a GROUP, condensing each group members' transformations into a single transformation matrix. In 
cases where the coordinate data is ~nall compared to the transformation data, CONDENSE can reclaim 
memory and increase interactive response once the need for dynamically changing transformations within 
a specified group is completed. Two other useful commands are FREEZE and RELEASE. A picture object 
may be frozen, in which case the input devices used to dynamically update its transformations are suspended 
until it is RELEASEd. One useful application of FREEZE and RELEASE involves setting up a "template" 
of transformations which are applied to many objects. Individual objects may then be activated (RELEASE) 
or deactivated (FREEZE) as desired. Making a transformation "template" is an example of the use of 
macro VERBs, which are discussed in the next section.  2.2 Macro VERBs It is possible to collect 
together sets of GRAMPS commands into disk files and to use the files as macro VERBs. These will then 
be expanded and interpreted by GRAMPS when the file name is used as a VERB. Any ARGUMENTs on a command 
line that begins with a macro VERB will be saved for use within the macro definition. This feature of 
GRAMPS thus allows for user defined commands. For example, a "template" of  transformations may be created 
as a macro VERB. The file defining the macro VERB would contain transformation commands which named a 
parameter ("$I", for example) in the place of the object NAME required for transformation commands. The 
macro VERB could then be used as a command whose argament is the picture name to be transformed according 
to the template. Macro VERBs can be created as ASCII files of GRAMPS co~ands. Aside from independently 
editing macro VERB files, GRAMPS provides two useful ways for the user to define new commands. On user 
request, GRAMPS will collect each command line typed at the terminal into a file which may then function 
as a macro VERB. GRAMPS also includes a powerful SAVE command which will create the linear transformation 
commands (ROTATE, PERSPECTIVE, SCALE, etc.) necessary to transform any picture to its current position. 
These commands are output to a file, in correct GRAMPS syntax, for later use as a macro VERB. Examples 
of the usefulness of these features will be given in a later section.  2.3 Data Types The Evans &#38; 
Sutherland Multi Picture System is, in part, a display list processor. GRAMPS, then, is a display list 
manager. There are five data types supported by GRAMPS which facilitate the management of graphical data. 
These data types are the simple object, the world, the group, the framed object, and the synonym. In 
the following discussion, the word "object" is used to refer to any of the above data types. If one of 
the data types is referred to specifically it is named as a "simple object", "framed object", etc. The 
word "picture" is also used to mean a general object of any data type. The basic graphical data type 
in GRAMPS is the simple object. This type of object consists of a set of variable transformation matrices 
and the Cartesian coordinates of the endpoi~ts of the lines describing the object. It is only the coordinates 
that must be explicitly given for each object. The set of variable transformation matrices is fixed in 
size and contains rotation, scaling and translation with respect to the Cartesian coordinate system axes 
as well as rotation, scaling and translation with respect to an arbitrary set of three mutually orthogonal 
axes. These transformation matrices may be dynemically changed with the use of GRAMPS transformation 
commands. The second data type in GRAMPS is the  WORLD. There are only two of these, called LWORLD 
(left world) and RWORLD (right world). The names left and right arise from the stereo viewing capabilities 
in GRAMPS which utilize these WORLDs. The WORLDs contain only transformation matrices. The set of WORLD 
transformations consists of the set used for simple objects as well as a perspective and a move (post-perspective 
translation) linear transformation matrix. The transformations of the WORLD are concatenated with the 
transformations of every other data type before the data are displayed. Thus the WORLD is a simple tree 
of depth=1 and contains every other object. More complex tree structures can be created  and manipulated 
in GRAMPS. The GROUP data type contains only transformation matrices which apply to a group of one or 
more other objects. These group transformations are concatenated with the objects' transformations before 
the display of each object. GROUPs may contain other GROUPs as well as other objects thus allowing the 
creation of tree structures of arbitrary (up to a maximum) depth. These GROUP tree structures can be 
manipulated with the GRAMPS REMOVE and INSERT commands. Examples of the use of nested tree structures 
in GRAMPS are given later.  The fourth data type in GRAMPS is the framed object. This data type contains 
a set of transformation matrices identical to that contained by the simple object. However, the coordinate 
data for the framed object is variable unlike the fixed coordinate data of the simple object. A framed 
object is created as a set of frames of coordinate data, any one of which may be displayed at one given 
time. Thus the frames of a framed object may be sequentially displayed to give a traditional animation 
effect. Because display lists are maintained in the host computer, very large data bases may be quickly 
and effectively handled using the framed object data type. Examples of this type of use are given later. 
 The final data type supported in GRAMPS is called the synonym. A synonym contains transformation matrices 
like a simple object, but in the place of coordinate data synonyms contain a pointer to another object's 
coordinate data. A useful application of synonyms is demonstrated in the concluding examples, in which 
a virus particle of symmetry related motifs is constructed. Each symmetry position in the particle is 
occupied by a synonym of an original motif. When the original motif is transformed, the transformation 
is automatically apparent at all other symmetry related positions. The use of synonym is identical to 
the use of "instances" as discussed in reference [3]. Many graphics systems allow creation and manipulation 
of nested tree structures of pictures (see, for example, [6]). In GRAMPS every data type is fully integrated 
into the language and every operation on each data type is fully supported. For exemple, once a GROUP 
is created, the entire GROUP along with its tree structure, transformations and data can be duplicated 
with one COPY or SYNONYM command. Likewise, the fremed object data type is fully supported and can be 
easily COPYed, GROUPed and transformed as readily as a simple object. 3. GRAMPS System Use  GRAMPS 
is intended as an interactive user-oriented graphics system. This is reflected in the simple command 
structure and in the powerful data types described above, which free the user from many concerns of graphical 
data base management while still allowing much control over the data base. The interactive use of GRAMPS 
is facilitated by the graphics input devices (8 dials, a joystick and a tablet) which can be freely assigned 
and used to control any picture transformation of any of the data types described above. There are two 
classes of transformations in GRAMPS. One involves the use of interactive input devices, such as dials, 
joysticks or a tablet. The other type involves transformations using number values. 3.1 Transformations 
on Devices Association of a device with a transformation is done by naming the device as the FUNCTION 
in any transformation command. Thus, the command: SCALE SPHERE XY D7  will allow the previously created 
picture named SPHERE to scale with respect to the X and Y axes using a scale factor dynamically controlled 
on dial 7. The frames of a framed picture named VIRUS could be shown under the control of a dial with 
the following command: FRAME VIRUS N -32767<D3<0  In the above example, a bounded FUNCTION of dial 
3 was used. The bounded FUNCTION is used to limit the values read from a dial to be within specified 
bounds. In addition to the FUNCTIONs described above, there are several other FUNCTIONs which can be 
used to control picture transformations. The bounded FUNCTION is very useful in translation commands 
in order to ensure that the object does not jump to the opposite side of the display screen due to integer 
wrap-around. A bounded FUNCTION can be specified by any of the syntax examples given below. -I0000<D4<20000 
or 20000>D4>-I0000 I0000<D8 or D8>I0000 DI<-17000 or -17000>DI  B~pered FUNCTIONs are like bounded FUNCTIONs 
except that the transformation is reversed when the bound is encountered. For example, a sphere could 
be made to bounce inside a cube under the control of dial 4 with the single GRAMPS command: TRANSLATE 
SPHERE XYZ [-20000<D4<20000]  Bumpered FUNCTIONs are specified with their bounds as in bounded FUNCTIONs, 
but the entire specification is enclosed in square brackets. For example, ROTATE SPHERE X [15000>-D2>0] 
 In this example the device is -D2. This introduces another FUNCTION of devices. Wherever a device is 
required in a FUNCTION, the negative of a device may be specified as in the above example. Thus if two 
objects are transforming on the same device, but if one is using the negative device FUNCTION, then the 
objects will dynamically transform in opposite directions. A dividing factor may also be specified, such 
as D4/3. More complicated functions of dials are not yet possible.  3.2 Transformations on Values: Animation 
 devices, but rather uses a number value to transform the object. The simplest type is a FUNCTION which 
gives only a value. The transformation to which the FUNCTION is applied immediately takes on that value. 
The values in GRAMPS are signed 16 bit integers. The lower extreme value (-32767) always corresponds 
to one extreme of transformation (-180 degrees for rotations, left edge of the screen for translations, 
etc.) while the upper extreme value (32767) corresponds to the opposite extreme of transformation. This 
consistent value range, which is also used for coordinate specification, makes the use of absolute values 
in GRAMPS much easier for the average user. The choice of 16 bit integers was made to facilitate and 
speed interaction with the MPS. Another choice could be implemented if GRAMPS were converted to use 
another graphics output device. Another type of FUNCTION in the value class is the delayed update function. 
In this type of FUNCTION the value to be applied to the transformation does not occur instantly, but 
rather over a specified number of steps. The syntax of this type of FUNCTION is: value:steps  where 
the colon serves to separate the destination value from the number of steps to take in reaching the destination, 
and to identify the FUNCTION as the delayed update FUNCTION. When this type of FUNCTION is used, the 
object's transformation value is linearly interpolated from its current value until it reaches the destination 
value. It is also possible to specify a sinusoidally varying interpolation stepsize, which gives a smoother 
transformation to the destination value. These types of FUNCTIONS are very useful in computer animation 
and were first described by DeFanti [6] as time-based variables. Using the delayed update FUNCTION, 
real-time interactive animation can be accomplished by transforming the object(s) to desired key positions 
and saving the transformation values corresponding to those positions using the SAVE command described 
above. One can then use the update FUNCTION to interpolate to intermediate positions. It is important 
to distinguish this type of animation from key-frame animation [7] in which line segments or endpoints 
are interpolated between the key frames. Examples of computer animation using the delayed update FUNCTIONs 
are given later. A particularly useful combination for computer animation is the framed object data 
type used in conjunction with delayed update FUNCTIONs. The framed object data type allows dynamic variation 
of the coordinate data displayed for an object. Coupled with the transformation variations allowed in 
delayed update FUNCTIONs, very complex changes in the display can be made quickly. Most importantly, 
these animations can be played in real-time in order to see if they give the desired effect. After the 
sequence is seen to be correct, the playback may be slowed down and synchronized with a camera for film 
recording. In summary, the user orientation of GRAMPS The other class of FUNCTIONs does not use allows 
for quick and easy production of graphical images. The convenience features (especially the SAVE feature) 
encourage experimentation in the creation of new images without the threat of losing what has already 
been accomplished. Finally, the animation commands in GRAMPS allow easy set-up of animation sequences 
which can be played back in real-time or slowed down for synchronization with a camera. 4. System Performance 
 GRAMPS is not run on a dedicated computer nor is it run as a high priority process on the host computer. 
It is run as a intermediate priority subprocess (one level below interactive terminal processes) in a 
time-sharing environment. Yet, the response of GRAMPS is well within the range that classifies it as 
a real-time, interactive graphics system. This response is due partially to the fast hardware of the 
Evans &#38; Sutherland Multi Picture System, partially to the fast CPU and large memory size of the VAX 
11/780, and partially to the organization and optimization of GRAMPS software. Consider, for example, 
the UPDATE and DISPLAY subroutines of GRAMPS. The UPDATE subroutine updates objects' display lists with 
new transformation matrices dependent on the FUNCTIONs specified in transformation commands. Rather than 
tightly structure this subroutine so that simple FUNCTIONs could be considered as subsets of a more general 
FUNCTION, each FUNCTION has its own section of FORTRAN code for the calculation of a new value. In this 
way speed is gained (less structure to be branched through) at the expense of code size. The DISPLAY 
subroutine examines the tree structure of objects and creates a single, large display list reflecting 
this structure. Thus each display list is duplicated, once as the object and once in the buffered display 
list. Once this buffer is created, however, only one set-up for direct memory access (DMA) from the VAX 
to the E&#38;S need be done. In addition, the VAX operations are overlapped with the E&#38;S operations 
so that a new buffer may be created while the old one is being displayed. In the planning stages, much 
attention was paid to methods of maximizing the interactive response of GRAMPS. A general rule was that 
GRAMPS time for setting up a transformation or animation sequence was more expendable than time spent 
performing the transformation updates. Thus many tables and pointers are set-up and maintained for each 
transformation in order to increase the efficiency and speed of the update cycles performed by GRAMPS. 
For example, in the delayed update FUNCTION using sinusoidally varying interpolation step sizes, two 
auxiliary table entries are calculated when such a FUNCTION is set-up. This saves two calculations in 
each display update cycle. In general, the interactive response of GRAMPS is quite good. In test cases 
examined at the NRCC, special purpose programs were written whose response was better than GRAMPS' response 
for the same function. However, considering the general-purpose nature of GRAMPS and the ease of GRAMPS 
programming, it was considered more advantageous to use GRAMPS than a special purpose program. Even 
when performing many calculations per display update cycle, the interactive response of GRAMPS is good 
and the impact on other VAX users is minimal. In addition, since GRAMPS is written in FORTRAN is is more 
easily modified or extended by less experienced users. 5. Applications As discussed above, GRAMPS is 
designed as a user-oriented, interactive, general purpose graphics system. At the NRCC the emphasis is 
on chemical problems; thus GRAMPS has been used primarily as a tool in presenting and understanding chemical 
ideas. We present here four applications of GRAMPS which demonstrate its usefulness, while at the same 
time explain some of its features by example. These examples are models of: I) the icosohedral geometry 
of the TBSV particle 2) the unwinding of double helical DNA 3) a four-dimensional potential energy surface 
4) a 22-jointed human figure  5.1 Tomato Bushy Stunt Virus (TBSV) particle X-ray crystallography has 
reached the point where very large structures, composed of many protein subunits, can be examined at 
atomic level resolution. Many of these supra-molecular assemblies are arranged in symmetrical fashion. 
The structure of such an icosahedrally symmetric assembly, the tomato bushy stunt virus, has been solved 
crystallographically to 2.9 angstroms resolution [8]. Work is underway at the NRCC to interpret features 
of the structure in order to understand the nature of protein-protein interactions, and of protein flexibility 
in virus assembly and disassembly. GRAMPS has proven to be a useful tool for examining and modeling many 
aspects of the TBSV structure. The application that we detail here involves the generation of symmetrically 
related structural elements. In order to examine the packing relationships of the protein subunits comprising 
the icosahedral viral shell, it is desirable to have the ability to manipulate a single subunit in the 
shell and see how the subunit interactions change as a result of the imposed particle symmetry (see figure 
2). To this end a small FORTRAN program was written to generate appropriate GRAMRS commands. Input to 
this program are the descriptions of the symmetry axes of an arbitrary point group. In the case of the 
icosahedral point group a macro VERB GRAMPS command was generated that had the following structure: 
GET W (the subunit motif) (pick symmetry axes) PICKAXIS I O, 14562, 23561, O, O, 0 (5-fold) PICKAXIS 
2 O, O, 23561, O, O, 0 (2-fold) PICKAXIS 3 O, 23561, O, O, O, 0 (2-fold) PICKAXIS 4 23561, 23561, 23561 
O, O, 0 (3-fold) SYNONYM W WI GROUP Wl WI GIWI SYNONYM W W2 GROUP W2 W2 GIW2   The complete construction 
of ADAM from the silhouette drawing to the end assembly of all parts was done in one graphics session 
in about 3 hours. Because of the manner in which ADAM's parts are grouped into a hierarchical tree 
(see table 2), all body pieces follow the motion of those pieces which are closer to the root segment 
of the tree. For example, the torso is a group containing five body segments plus the arms and head. 
The body segments themselves are grouped from the waist up, each segment being grouped with those above. 
This structure not only ensures the proper following of all the appropriate body parts when, for example, 
the torso is bent from the waist, but also (in a similar fashion to the DNA example) uses nested rotations 
to emulate complex spinal bending and twisting. Table 2. Diagram of the nested tree structure of ADAM's 
parts. ADAM --LOWBOD h LCALF LFOOT k LHEEL LTOES --RLEG ~ RTHI RLLEG k RCALF R~OOT T---RHEEL L--RTOES 
--HIPS ---TORSO ~ WAIST ORI -RIBI OR2 kT RIB2 Oit3 ~ RIB3 UPTORSO J}---SHOULDER ~L~LBICEP ! u--12ORARM 
 I I    LwRIST | U--LHAND k LPALM LFINGS --RARM k RBICEP RFORARM ---tNRIST U--RHAND ~ RPALM 
RFINGS --HF~D Many other GRAMPS features are easily demonstrated with ADAM. Certain common body motions 
and motion limits can be assigned to dials using bounded functions and the macro VERB capabilities. Thus 
a simple leg lift with  concurrent knee bend can be accomplished by a macro VERB named LEGLIFT defined 
by: ROTATE $1$LEG X 0<D6<16000 ROTATE $1$LLEG X -16000<-D6<0  Once the definition is made, lifting 
a leg within proper body motion limits can be assigned to dial 6 by simply typing "LEGLIFT R" or "LEGLIFT 
L" for the right or left leg respectively. More complex motion templates may also be assigned to dials 
in a similar manner. For instance the posture change accompanying a shift of weight from one leg to the 
other may be emulated on a dial by the following macro VERB: ROTATE TORSO Z -1500<D4<1500 ROTATE TORI 
Z -1500<D4<1500 ROTATE TOR2 Z -1500<D4<1500 ROTATE TOR3 Z -1500<D4<1500 ROTATE SHOULDER Z -1500<D4<1500 
 ROTATE LOWBOD Z -1500<-D4<1500 ROTATE RLEG Z -1500<D4<1500 ROTATE RLEG X 0<D4<1500 ROTATE RLLEG X -2000<-D4<0 
ROTATE LLEG Z -1500<D4<1500 ROTATE LLEG X 0<-D4<1500 ROTATE LLLEG X -2000<D4<0 ROTATE RFOOT X I#384<-D4<16384 
ROTATE RTOES X 0<D4<1500 ROTATE LFOOT X 14384<D4<16384 ROTATE LTOES X 0<-D4<1500  It should be emphasized 
that all specified axes refer to the internal coordinate system of the object being NAMEd, and therefore 
the axes remain independent of transformations on any groups closer to the root segment. The real-time 
animation features of GRAMPS are handily demonstrated using ADAM. It should be noted that the use of 
GRAMPS in creating and manipulating ADAM does not have as its objective the simulation of human motion 
as has been undertaken by groups such as Badler and coworkers [9]. Rather, the task performed is the 
much simpler one of manipulating, as would a puppeteer, a jointed model. From this basic approach, however, 
the goal of animating human movement becomes quite feasible with the transformation saving capabilities 
of GRAMPS. By manipulating the human model into various key positions and saving the transformations 
that produced each, a sequence can be set up that allows the animator to advance the model's motion in 
any specified number of steps between the key positions. As a demonstration of this facility we have 
manipulated ADAM into various human motion positions as photographically doc~nented by Eadweard Muybridge 
about one hundred years ago [10], and have reanimated his subjects back into three dimensions. Using 
templated dial assignments to assist in positioning the body, 12 key frames of a complex motion such 
as "man performing standing broad jump" (see figure 6 and [10] Plate 30) or '~an lying down" ([10] Plate 
91) can be graphically input in approximately one hour. Such sequences can then be played back with any 
degree of interpolation, and viewed from any position. There are several quite striking advantages to 
saving the transformation values rather than  the actual vector positions of the key frames, First, 
the amount of information that is saved is in general much smaller, and thus more quickly stored and 
retrieved. Second, scaling as well as rotation and translation transformations can be specified, allowing 
a continuous distortion of shape from, say, ADAM to EVE (Equally Viable Entity). Third, such a scheme 
allows for different object descriptions to be manipulated by the same saved sequences. For example, 
EVE will lie down using the same transformations as ADAM. Additionally, framed objects, which themselves 
may be composed of the traditional key-frame interpolants may be further animated, GRAMPS, thus~ enables 
several features suggested by Burtnyk and Wein as extensions to their key-frame animation system [11]. 
 Figure 6. A view showing a sequence of ADAM executing a standing broad jt~p, reconstructed from Muybridge 
[10]. For clarity only 7 of the 12 key positions are shown. 6. Concluding Remarks The graphics language 
interpreter, GRAMPS, that has been described in this paper is serving as a basis for chemical graphics 
applications at the National Resource for Computation in Chemistry. It is also serving as a pilot project 
for future graphics development in the chemical community. Our experience, so far, has shown that GRAMPS 
has been applied with considerable ease to several problems in chemistry, and, in fact, has aided researchers 
at the Lawrence Berkeley Laboratory in such non-chemical areas as the 3-D reconstruction of medical imagery 
derived from CAT scans. We have recently distributed a preliminary version of GRAMPS to several sites 
with similar hardware configurations, and have received initial positive response. We await more complete 
feedback on its use at these sites. As work proceeds, both language capabilities and applications are 
expanding. Two major goals of the project are: I) to provide a generally usable tool for chemists to 
fashion their own graphics applications without the burden of graphical data management and 2) to provide 
a testing ground for useful graphics features that  can be incorporated into a portable package for 
distribution in the chemical community. To these ends we will continue to develop GRAMPS in a modular 
structured fashion, isolating all machine and system dependent parts of the code, and we will extend 
the language in the direction of user convenience and flexibility. Although the present implimentation 
of GRAMPS uses the subroutine package supplied by Evans and Sutherland for their Multi-Picture System, 
the GRAMPS interpreter itself exists as a higher level language, and thus could be applied to any nt~nber 
of hardware configurations with sufficient real-time graphics capability. 7. References I. Gund, P., 
Andose, J.D., Rhodes, J.B., Smith, G.M. "Three-Dimensional Molecular Modeling and Drug Design." Science, 
208, 1425-1431, (1980). 2. Tsernoglou, D., Petsko, G.A., McQueen, J.E., and Hermans, J. "Molecular Graphics: 
Application to the Structure Determination of a Snake Venom Neurotoxin". Science, 197, 1378-1381, (1977). 
 3. Newman, W.M., and Sproul, R.F. Principles of Interactive Computer Graphics, 2nd Edition, McGraw-Hill 
Book Company, New York, YX~Y7.  4. Bernstein, F.C. et al. "The Protein Data Bank: A Computer Based Archival 
File for Macromolecular Structures." J. Mol. Biol.,  112, 535-542 (1977).  5. Allen, F.H., Kennard, 
0., Motherwell, D.S., Town, W.G., Watson, D.G. "The Cambridge Crystallographic Structural Data File." 
J. Chem. Doc., 13, 119 (1973).  6. Defanti, T. "The Digital Component of the Circle Graphics Habitat." 
Proceedings National Computer Conference, (1976).  7. Burtnyk, N. and Wein, M. "Computer Generated Key 
Frame Animation." J. Soc. Motion Picure and Television Engineers, ~_~_,  3, 149-153, (1971).  8. Harrison, 
S.C., Olson, A.J., Schutt, C.E., Winkler, F.K., and Bri6ogne, G. "Tomato Bushy Stunt Virus at 2.9 Angstroms 
Resolution." Nature, 276, 368-373, (1978).  9. Badler, N.I., O'Rourke, J. and Kaufman, B. "Special Problems 
in He,nan Movament Simulation." Computer Graphics, 14, 3,  189-197, (1980). 10. Muybridge, E. The 
Human Figure in Motion, Dover Publications, New York, (-1955-~. --  11. Burtnyk, N. and Wein, M. "Interactive 
Skeleton Techniques for Enhancing Motion Dynamics in Key Frame Animation." Communications of the ACM, 
19, 10, pp.  564-569, (1976).  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806800</article_id>
		<sort_key>143</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[An attribute binding model]]></title>
		<page_from>143</page_from>
		<page_to>151</page_to>
		<doi_number>10.1145/800224.806800</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806800</url>
		<abstract>
			<par><![CDATA[<p>An attribute binding model is presented and discussed to illustrate the use of such reference models in the ANSI X3H3 graphics software standardization effort. The model helps in defining and illustrating issues, and in explaining the proposed standard to those who will accept and use it. Using the model components and attributes can be categorized and characterized.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Standards</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011017</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Domain specific languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010919.10010177</concept_id>
				<concept_desc>CCS->Computing methodologies->Distributed computing methodologies->Distributed programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Standardization</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P332439</person_id>
				<author_profile_id><![CDATA[81100340455]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Garrett]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Applied Graphics, Inc., Falls Church, Virginia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>909575</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Garrett, M.T., A Unified Nonprocedural Environment for Designing and Implementing Graphical Interfaces to Relational Data Base Management Systems, Ph.D. Dissertation, The George Washington University, Dept. of EE &amp; CS, Report GWU-EE/CS-80-13, September 1980.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[GKS6.2 "Graphical Kernel System" ANSI X3H3 document number 80-55, 11 July 1980.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988498</ref_obj_id>
				<ref_obj_pid>988497</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA["Status Report of the Graphics Standards Planning Committee", Computer Graphics, vol. 13, no. 3, August 1979.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA["First Report of the Reference Models Task Group", ANSI X3H3 document number 80-36, 21 March 1980.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 AN ATTRIBUTE BINDING MODEL by Michael T. Garrett 
Applied Graphics, Inc. Falls Church, Virginia Abstract An attribute binding model is presen- ted and 
discussed to illustrate the use of such reference models in the ANSI X3H3 graphics software standardization 
effort. The model helps in defining and illus- trating issues, and in explaining the proposed standard 
to those who will accept and use it. Using the model components and attributes can be categorized and 
characterized. i. Introduction The American National Standards In- stitute committee for graphics software 
standardization (ANSI X3H 3) is currently considering and extending the ACM Graphics Standards Planning 
Committee's proposed standard Core System (GSPC79). The project is a difficult one for several reasons. 
One is that, when an issue is generated, it must be examined for "ripple" effects on the entire draft 
~tandard proposal. Seldom does a proposed change or addition affect only one part, in spite of efforts 
to keep those parts orthogonal. Another is that, because it is a large effort, committee members must 
be expected to maintain detailed knowledge of all areas. However, some knowledge of all areas is necessary 
if one is to see the forest for the trees. Another difficulty is the requirement that the standard be 
explained to those who will accept and use it. Explanations are expected to be clear, concise, and consistent. 
 Permission to copy without fee all or part of this material is granted provided that the copies are 
not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the 
publication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 
@1981 ACM O-8971-045-1/81-0800-0143 $00.75 The ANSI X3H3 committee, in an- ticipation of these problems, 
created the Reference Models Task Group, commissioned to produce models which illustrate and clarify 
the issues of graphics software. These models are intended to drive and reflect the standardization effort, 
and are usually logical, conceptual models rather than physical models. The Reference Models committee 
initially produced a report (RMTGSO) which identified areas to be modelled, types of models and modelling 
techniques, and specific proposals to be modelled. One of the areas is attributes and at- tribute binding. 
In this paper, I will illustrate the application of models with an attribute binding model. This model 
is based on work done in graphical inter- faces to data base management systems by the author and Professor 
James D. Foley of George Washington University (GARRSO). 2. Conventional Attribute Models An attribute 
makes little sense by itself. It must be the attribute of something; an object. Therefore, before we 
can study attributes, we must decide what the objects are. In some environ- ments, the value of an attribute 
of an object is itself an object. Most people would agree that a line, as a graphical primitive, is an 
object and a segment of an image is also an object. However, is a primitive an attribute of a segment 
or is the segment to which a primitive belongs the value of its "segment name" attribute? If you think 
that one is easy to answer, try substituting piek_id for segment, where pick id is the iden- tifier 
produced when a piece of a seg- ment is chosen with a pick input device (e.g., a light pen). Once 
we have decided what the ob- jects and attributes of a system are, then we must decide how the attributes 
are to be assigned values, and how the objects are to be created. Rather than assign values explicitly 
to each at- tribute of each object as it is created, we usually assign values modally. That is, there 
are attribute value "holders" which can have their values changed, and whatever values they have at 
the time of creation of an object are the values assumed by the ob- ject's attributes. In the Core System, 
for example, the pick_id attribute of a prim- itive is given a value modally, as is one of the endpoints 
via the "current position" (CP) holder. The other endpoint is spec- ified explicitly at the time of 
the creation of the primitive, and at the same time the CP holder is given a new value. If the seg- ment 
name is considered to be an attribute of a primitive, it is given a value at the time of creation of 
the primitive from the "open segment" holder. Attribute values may be static or dy- namic. If the value 
of an object's at- tribute can be changed after the object's creation, the attribute is dynamic; other- 
wise it is static. However, we must still address the problem of changing the sys- tem's interpretion 
of a value. For example, the color of a primitive may be a static attribute, but if we change the video 
look- up table of a device, the primitive may appear with any color. When and where is a logical attribute 
value bound to a physical value? This is the attribute binding problem. If some attributes can be table 
driven then must we be consistent and allow all attributes to be so driven? Another problem of attributes 
is "inquirability". Can one inquire the value of an attribute? Note that this is dif- ferent from inquiring 
the value of a modal value holder. Hand in hand with this prob- lem is that of identification: Hew do 
we identify the object (or objects) for which we want the attribute's value? This is really a small problem 
unless the system allows very general, consistent attribute value inquiry. Structuring of images further 
com- plicates attribute binding. An object, in an image, may be composed of other objects and may, itself, 
be a piece of a larger object. Each object may have the same set of attributes. However, in dif- ferent 
instances, we may want the values of an object's attributes to be inherited from its owner, or we may 
wish to specify the values explicitly, or allow some com- bination of the explicit and implicit values 
to produce a new value. The latter is usually the case, for instance, for image transformations, while 
color may be inherited or specified explicitly. In the next section, a model is presented which illustrates 
these concepts, and then the model is applied to some systems. 3. A Binder Model Primitives are the 
non-decomposable units of images. We define two types of primitives. World primitives are spec- ified 
by user programs to graphics sys- tems. Bound primitives are specified by graphics systems to primitive 
inter- preters. Primitive interpreters are sys- tems which convert bound primitives into images with 
no further binding of at- tributes. Note that a primitive inter- preter is usually part, but not all, 
of a graphies device. Graphics devices of- ten de some binding of attribute values. In this conceptual 
model, all image attributes, handled by the graphics sys- tem, are considered to be attributes, directly 
or indirectly, of world prim- itives. Thus, segment name is an at- tribute of a world primitive whose 
value indicates which segment it is in. In a system which allows structured images, the segment name 
would be a complex, rather than single valued attribute. The task of the graphics system is to transform 
world primitives into bound primitives. The appearance on a view surface of the image of a world prim- 
itive, is affected by the values of its attributes. These attribute values are supplied by the user program 
when calls are make to a graphics package. A bound primitive's attributes are used by prim- itive interpreters 
to produce images. For this conceptual model, we define a new object type: binder. A binder is a hardware 
or software system whose in- puts are either world primitive attribute values, supplied by user programs, 
or the outputs of other binders. Binders may, in some cases, be broken down into "sub" binders, each 
of which binds some subset of the inputs to the higher level binder, or which are applied in series to 
the same attribute. In the extreme, a graphics system can be considered to be one big binder. In our 
examples, how- ever, we may choose the level of detail which illustrates some issue or point. A few examples 
of binders in the Core System are: - coordinate binder(s) -world to bound 6oordinates - font binder 
-world codes to other codes or sets of line segments - view_surface_list binder - linewidth/linestyle 
binders -highlighting binder -color/intensity binder -pen binder 144 Computer Graphics Volume 15, 
Number 3 August 1981 Binders, as objects, have attributes. The bound primitive's attribute value depends, 
not only on its corresponding world primitive's attribute values, but also on those of the attributes 
of the binders at the time that the world prim- itive is bound. We shall associate with each primitive 
a set of attributes whose values are to be those of the attributes of the binders at the time of binding. 
These shall be called binder setting attributes. Conceptually, when a world primitive is processed, the 
binder's at- tributes are set to the values in the world primitive's corresponding binder setting attributes, 
and then the approp- riate world primitive attributes are passed through the binders to produce the bound 
primitive attributes. TO illustrate the difference between binder setting attributes and primitive attributes, 
we use some attributes implied by the Core System for line world prim- itives: WORLD LINE PRIMITIVE 
ATTRIBUTES: STATIC: - xl, yl, zl, x2, y2, z2, - linewidth, linestyle, - color1, color2, color3, intensity, 
pen, (color_index, ana other indices), -pick_id, segment_name DYNAMIC: visibility, highlighting BINDER 
SETTING ATTRIBUTES: STATIC: - pen_table, - linewidth table, /* determine mapping from system */ - 
linestyle table, /* codes-to internal codes */ - (other index tables) - view surface_table, - segment_name_table, 
 -x_ref, y_ref, z_ref, - dx_norm, dy_norm, dz_norm, - view-distance, - front_distance, back distance, 
 - projection_type, dx_proj, dy_proj, dz_proj, - umin, umax, vmin, vmax, - dx_up, dy_up, dz_up, -width, 
height, depth, - xmin, xmax, ymin, ymax, zmin, zmax, -windowclipping, depth_clipping, coord_sys_type 
 DYNAMIC: z (video lookup_table), - scale x, scale y, scale z, /* m~trix for-coordinaTe binder */ 
- angle x, angle_y, angle_z, - trans_x, tr~s_y, trans_z A neutral display file (NDF) is a collection 
of world primitives. The fact that such a file may have attributes specified or stored modally is incon- 
sequential to this conceptual model. The values of a world primitive's attributes, including the binder 
setting attributes, are determined at the most recent point in the file at which that valuewas set. We 
may consider each world primitive to be an independent unit of information with its own value for each 
attribute, even though that value is the same for all the other world primitives which follow the setting 
of that modal attribute value holder. It may be selected by the values of some of its attributes (e.g., 
segment name or pick id). Primitives may be created or des- troyed, at which times their images must 
 be created or destroyed. A line prim- itive, for example, is created when the program calls the line 
drawing function of a graphics package. The values of attributes of a world primitive (in- cluding 
binder setting attributes) are determined at the time that the primitive is created. Some are supplied 
by user programs; others are suoD±i~d by the sys- tem. Again, the fact that the values of attributes 
may be specified modally by application program invoked functions, is inconsequential. The latest specified 
 value is kept by the system and used when the primitive is created to determine a world primitive attribute 
value. Some of the values of world primitive attributes can be changed after primitive creation. These 
are called dynamic attributes. (Notice that being able to change the value of, say, the window clipping 
mode, does nal mean that the value of the window clipping attribute for a particular world primitive 
can be changed; merely that the holder value for primitives yet to be created can be set.) Those attributes, 
which cannot be changed for existing primitives, are called static attributes. Finally, attributes' 
values may be inquirable or not, independent of whether they are static or dynamic. Note the difference 
between inquirability of a primitive's attribute value and the in- quirability of the current modal value 
of the attribute,, which is to be given to subsequently generated primitives. An Computer Graphics Volume 
15, Number 3 August 1981 attribute of world primitives is inquirable if its value can be retrieved for 
an iden- tified set of existing world primitives. For example, although in the Core System the current 
modal value of the color at- tribute can be inquired, the color of a particular existing primitive can 
not be inquired. On the other hand, the vis- ibility attribute value of a particular existing world primitive 
can be inquired by using its segment name. In this model, the set of binders is fixed; they are neither 
created nor des- troyed. A binder might, however, have a "by-pass" attribute, whose value may in- dicate 
that the bound value out, is to be the same as the unbound value in; i.e., the binder is to have no effect. 
For example, the clipping binder may be by- passed because its CLIPPING STATUS at- tribute has a value 
of FALSe. When a prim- itive is c~cat~d, its binder setting at- tribute, CLIPPING_STATUS, is conceptually 
set to the current modal value. This, then, is the value assumed by the clipping binder when the world 
primitive is bound. When a world primitive has one of its dynamic attributes' values changed, it must 
again, conceptually, be bound by the binders involved with that attribute (e.g., coordinate binders when 
image transformations change). When dynamic world primitive attri- butes are changed by the user program, 
the primitives to be affected must be identified. In a completely general sys- tem, the identification 
could be accom- plished by the expression of some asser- tion in terms of the values of certain attributes 
of the primitives. For example, "change the visibility to OFF for all mar- kers whose x coordinate is 
less than the y coordinate of some other marker". In most useful common cases, the assertion would be 
that a certain attribute has a certain value. For example, in the Core System, to change the transformation 
of a set (segment) of world primitives, the assertion is that the primitives' segment name attribute 
has a certain value (the specific segment name). As another example, suppose the normalized device coordinate 
(NDC) space was a dynamic attribute of primitives (which in the Core System, it is not). This binder 
setting attribute will have the same value for all world primitives. Therefore, to identify these primitives, 
the assertion would simply be "TRUE". Notice that some of the attributes, which are considered to be 
segment at- tributes, (e.g., transformations), are, in this model, attributes of primitives which determine 
binder attribute values, i.e., binder setting attributes. Description of General Model Diagrams Figure 
1 illustrates the application program view of the model. The appli- cation program specifies a value 
for each possible world attribute, either directly or indirectly, by means of a storage location holder 
which is filled modally. At the time of creation of a primitive, certain attributes may be given values 
explicitly. Others are supplied by the system from modal value holders. Thus, the application program 
is conceptually involved with setting modal attribute values and then at various points creating a primitive 
which takes those modal values. We shall later illustrate the inquiring and changing of world primitive 
attribute values, which are the other conceptual functions of the application program. I-Application 
Program ,,.1 i i 1M°dal I I Modal [,S tor age S to rage / Primitive Primitive Primitive @ 0 Attribute 
Attribute Attribute World Primitive Figure Figure 2 is an illustration of the graphics system of the 
conceptual model discussed. When a world primitive is to be bound, i.e., when it is created or when one 
of its dynamic attribute values is changed, the binder attributes are first set to the binder setting 
attribute values of the primitive. Then the other attributes are passed through the binders to form the 
bound primitive attributes. This bound primitive is, then, conceptually sent to a bound primitive interpreter. 
 146 Computer Graphics Volume 15, Number 3 August 1981 World Primitive  InOer ett n l I attribute 
1 attribute 2 attribute 1 attribute 2 ... I I J binder 1 I binder 2  ,L ,[ Bound Primitive I primitive 
primitive "" attribute 1 attribute 2 I Ii  Bound Primitive Interpreter  Fiaure 2 Figure 3 is an 
illustration of the world primitive attribute setting, changing, and inquiring mechanism of the conceptual 
model. It is, essentially, an associative memory. A primitive is created by making an entry in the memory, 
into which go the current values of all the attributes. Inquiry is done by specifying an assertion Which 
identifies the primitives, for which the attribute value is required. This would be the same assertion 
by which the value might be changed, so that all identified primitives would receive the same value. 
A mask could be used to prevent static attributes from being changed. (In an actual system, there would 
simply be no functions by which they could be changed.) Such masks may be in effect only at cer- tain 
times, such as only when a segment is open. Another mask prevents un- inquirable attributes from being 
read. (Again, in an actual system, functions for inquiry of these attributes would not exist.) Primitives 
which are entered into the memory, or which have their values changed, are bound by the binders to 
produce the corresponding bound prim- itives, which are converted by bound primitive interpreters into 
images. Suppose, in this conceptual model, we wish to store an image in a "m@tafile" We can store world 
primitives, in which case, the entire set of binders must be used to interpret the metafile. We can store 
bound primitives, in which case, no binders are required. Finally we can store some bound primitive attribute 
values, in which case some binders are required and others are not. This last situation is the practical 
situation, because some devices, as we have pointed out, do some binding (e.g., video lookup tables). 
 Computer Graphics Volume 15, Number 3 August 1981 Neutral Display File (World Primitives) I ,, ,L 
l I New or retrieved values Assertion Tester  Figure ] Coordinate Binder Configuration for Core System 
World Primitive dynamic &#38; static static static inquirable by uninquirable uninquirable uninquirable 
segment name world coord I primitive segment I other world viewing 1  parameters matrix I coordinates 
transformation attributes $ binder 1 -4, i ,J c~ipping T 't bi~der I binder 2J<  Bound Primitive 
$ I primitive I other @QD9 .... coordinates attributes J Bound Primitive Interpreter Figure Computer 
Graphics Volume 15, Number 3 August 1981 4. Examples The most obvious binders in the Core System and 
the Graphical Kernel System (GKS79) are the coordinate binders (see figures 4 &#38; 5). These take the 
coordin- ates of a particular world primitive and produce the bound coordinates. For lines, there are 
basically three binders: a pre- clipping linear transformation, a clipper, and a post-clipping linear 
transformation.  Coordinate Binder Configuration for Graphical Kernal System World Primitive  dynamic 
dynamic &#38; set by WS ID static static inquirable by uninquirable uninquirable uninquirable segment 
name WS viewing I GKS viewing primitive I segment I other world parameters parameters coordinates transformation 
attributes linear binder 1 I I .. I clipping i binder lilinear I binder 2 4,, Bound__ Primitive 
I primitive other I ' '  " coordinates attributes "'" Bound Primitive Interpreter Figure 5 Computer 
Graphics Volume 15, Number 3 August 1981 Pen and Video Lookup Table Binders for Core System World Primitive 
 Attribute Register  dynamic &#38; static static static inquirable by uninquirable uninquirable uninquirable 
surface name I color, etc. I pen definition I pen index video lookup I other world world values table 
values table values attributes I pen binder  l color binder p Bound Primitive Attribute Register 
 color, etc. other I  bound values attributes .., Bound Primitive Interpreter Figure Figures 6 and 
F show binder con-- What binders are to exist? figurations for some indexed attributes in the Core System 
and in GKS. - Which binder attributes are to be static and which dynamic? Some questions concerning 
pen binding, color binding, and other in- -Which static attributes are to dexed attributes are restated 
in this be inquirable? model as: If dynamic, how are affected  - What index attributes are to exist? 
- primitives identified when binder attribute values are to be -Which index attributes a~e to be changed? 
In the present form of static and which dynamic? the standard, this question be- comes: are the binder 
attributes -If dynamic, how (with what iden- associated with the system, with tifying assertion) are 
affected individual segments, or with  primitives to be identified when something else? index attribute 
values are to be changed? Computer Graphics Volume 15, Number 3 August 1981 Pen and Video Lookup Table 
Binders for Graphical Kernal System World Primitive Attribute Register dynamic &#38;  inquirable 
by static work station uninquirable pen definition pen index other world table values attributes binder 
Bound Primitive Attribute Register color, etc. other bound values attributes I . . .  Bound Primitive 
Interpreter Figure REFERENCES GARR80 Garrett, M.T., A Unified Non- procedural Environment for Designing 
and Implementing Graphical Interfaces to Relational Data Base Management Systems,  5. Conclusion Ph.D. 
Dissertation, The George Washington University, .Dept. of The intent of this paper was to EE &#38; CS, 
Report GWU-EE/CS-80-13, present a model of attribute binding as September 1980. an example of the usefulness 
of reference models in defining and illustrating is-GKS79 GKS6.2 "Graphical Kernel System" sues. Dther 
more specific attribute ANSI X3H3 document number 80-55, binding models can fit into its frame- 11 July 
1980. work. In the model, all image attributes are associated with primitives and the GSPC79 "Status 
Report of the Graphics graphics system is considered to be a set Standards Planning Committee", of hardware 
or software binders. Using Computer Graphics, vol. 13, no. 3, this model, components and attributes, 
in August 1979. any graphics package, can be categorized and characterized. It is, however, a RMTC80 
"First Report of the Reference conceptual model and should not be car-Models Task Group", ANSI X3H3 ried 
too far in hope of explaining other document number 80-36, aspects of graphics packages. 21 March 1980. 
  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806801</article_id>
		<sort_key>153</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[An application of color graphics to the display of surface curvature]]></title>
		<page_from>153</page_from>
		<page_to>161</page_to>
		<doi_number>10.1145/800224.806801</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806801</url>
		<abstract>
			<par><![CDATA[<p>In developing a mathematical representation for a surface, designers currently must use line drawing graphics to examine the curvature of a line in a plane, a two-dimensional analysis. By combining a result from differential geometry with the use of color raster graphics, the method described in this paper provides a means for the designer to examine surface curvature, a three-dimensional analysis. In particular, a formulation for the Gaussian and average curvatures is given and it is shown how these indicate the presence or absence of protrusions, hollows, etc. in a surface, i.e., how, where, and by how much the surface curves.</p> <p>Showing a fourth variable, curvature in this case, over a three-dimensional surface is difficult, if not impossible with traditional line drawing computer graphics. The method described solves this problem by using color as a fourth dimension. Examples are given, including both known shapes (torus) and automotive parts (hood, fender).</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Computer-aided design]]></kw>
			<kw><![CDATA[Raster graphics]]></kw>
			<kw><![CDATA[Sculptured surfaces]]></kw>
			<kw><![CDATA[Surface curvature]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP77039411</person_id>
				<author_profile_id><![CDATA[81409591991]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Dill]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, General Motors, Research Laboratories, Warren, Michigan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807453</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Forrest, A.R. "On the rendering of surfaces." SIGGRAPH '79 Proceedings 13(2), August 1979.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>905524</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Jones, J.I. "A system for designing and approximating aesthetically smooth curves with interactive graphic controls." Ph.D. Thesis, U. of Detroit, 1970.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Gordon, W.J. "Spline-blended surface interpolation through curve networks." J. Math. &amp; Mechanics 18(10), 931-952, 1969.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Struik, D.J. Differential Geometry. Addison-Wesley, Cambridge, 1950.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Christiansen, H. "Application of continuous tone computer generated images in structural mechanics." In: Structural Mechanics Computer Programs - Surveys, Assessments and Availability (Pilkey, Saczalski and Schaefler, eds.), U. Press of Virginia, 1974.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Rodriguez, R.N. "Multivariate Burr III distributions. Part I. Theoretical properties." Research Publication GMR-3232, General Motors Research Laboratories, Warren, Michigan, 1980.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Dill, J.C. "An overview of CADANCE - a computer graphics system at General Motors." Research Publication GMR-3150, General Motors Research Laboratories, Warren, Michigan, 1980.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 AN APPLICATION OF COLOR GRAPHICS TO THE DISPLAY OF 
SURFACE CURVATURE John C. Dill Computer Science Department Genera] Motors Research Laboratories Warren, 
Michigan 48090* ABSTRACT analysis the designer works with variations in the line's curvature with arc 
length, and must In developing a mathematical representation for a repeat this for many such lines. 
surface, designers currently must use line drawing graphics to examine the curvature of a line in a 
Since curvature of a line in a plane is a one- plane, a two-dimensional analysis. By combining a dimensional 
quantity, it would be desirable to result from differential geometry with the use of provide a tool 
for analysis of two-dimensional color raster graphics, the method described in this curvature, particularly 
since up to 25% of the paper provides a means for the designer to examine overall computer-aided design 
effort is spent in surface curvature, a three-dimensional analysis. constructing mathematical surfaces 
from the clay In particular, a formulation for the Gaussian and data. Especially desirable would be 
a new average curvatures is given and it is shown how approach both to simplify this effort and to pro- 
these indicate the presence or absence of protru- vide an improved analysis tool. sions, hollows, etc. 
in a surface, i.e., how, where, and by how much the surface curves. As pointed out in [l] differential 
geometry deals with local curvature of surfaces, and in particu- Showing a fourth variable, curvature 
in this case, lar shows that the (local) nature of a surface can over a three-dimensional surface is 
difficult, if be characterized by its average and Gaussian cur- not impossible with traditional line 
drawing vatures. These scalar valued functions can be computer graphics. The method described solves 
 computed for a variety of surfaces and can be this problem by using color as a fourth dimen- used to 
indicate bumps, holl'ows, saddle points, sion. Examples are given, including both known etc. in the 
surface. shapes (torus) and automotive parts (hood, fender). Graphical representation of this information 
in a form easily understood by a designer is, however, KEY WORDS AND PHRASES: Computer graphics, raster 
 very difficult with traditional line drawing graphics, computer-aided design, sculptured computer graphics 
hardware (e.g., DEC GT62 or IBM surfaces, surface curvature. 3250 vector refresh displays). The solution 
to this difficulty is the use of color raster CR CATEGORIES: 8.2 , 3.2 graphics where color with solid 
shaded images can show a designer how curvature varies over a three- INTRODUCTION AND BACKGROUND dimensional 
surface. cer- lies in the smooth blending of highlights and tain results from differential geometry 
with the shadows. This, in turn, is determined by the Part of the attractiveness of a car's appearance 
This paper describes a method for combining use of color raster graphics to display surface shape of 
the surface. Thus a major part of the curvature of three-dimensional surfaces. Follow- computer-aided 
design of vehicle outer surfaces ing sections contain: is construction of mathematically smooth, esthetically 
pleasing surfaces from raw clay -description of the curvature problem data. A key element of this process 
is analysis -method to obtain surface curvature of the lines formed by intersecting the surface -results 
of applying this method to known with planes (surface cross sections). In this shapes and automotive 
parts -discussion and conclusions Permission to copy without fee all or part of this material is granted 
THE PROBLEM OF SURFACE CURVATURE provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the Design of outer surfaces is an important use 
of publication and its date appear, and notice is given that copying is by computer-aided design. At 
General Motors, for. permission of the Association for Computing Machinery. To copy example, approximately 
25% of this effort is in otherwise, or to republish, requires a fee and/or specific permission. *Present 
address: Corne11 University, Ithaca, New York 14853 &#38;#169;1981 ACM O-8971-045-1/81-0800-0153 $00.75 
 Computer Graphics Volume 15, Number 3 August 1981 the construction of a mathematical description of 
the sampled data from the clay model. The result must be mathematically smooth (continuous in the second 
derivative) and esthetically pleasing, i.e., have smoothly flowing highlights and shadows. In brief, 
the procedure for accomplishing this is (see [2] ) : l. The surface of the clay model is sampled to 
produce an array of (x y z) coordinate triples. The samples are usually taken along cross sections at 
10-cm intervals, and along various key lines. 2. For each cross section the designer fits a mathematically 
smooth line (i.e., cubic polynomial) through the raw clay data points.  3. To obtain an esthetically 
pleasing result, the designer then works with the curvature of the resulting lines, using displays like 
that in Figure l [2]. The goal is to split the line into regions where the curvature changes monotonically 
and smoothly, with the resulting line sufficiently close to the original data.  4. Lines from perpendicular 
sets of cross sections are then interpolated to form surfaces, usually so-called Gordon surfaces [2,3]. 
  2.2 0.0 L___ Y  -2.3 CURVATURE VS. ARC LENGTH DEVIATIONS  .04 0.0 -0.3 Figure I. Curvature 
analysis plots. Curve "A" is the mathematical approximation to the raw data, "B" is the deviation of 
"A" from the data, and "C" is the curvature of A. There are several difficulties with this method. First, 
the curvature plots are made at only a finite number of sections, i.e., the curvature is "sampled" and 
not examined in the intervening intervals. Second, it is a one-dimensional curva- ture, that of a line 
in a plane, rather than of a surface in 3-space. Third, with line drawing graphics, the designer must 
rely on cross sections through the resulting Gordon surface to examine overall flow and smoothness. A 
potential solu- tion might be a suitably realistic computer rendering of the surface. However, even 
with this, ambiguities can result [l]. What is desired then is an objective measure of surface curvature. 
 APPROACH The subject matter of differential geometry addresses, among other topics, the issue of sur- 
face curvature [1,4], and as we will see, supplies an objective measure of this curvature. We first discuss 
the mathematical basis of surface curva- ture on an intuitive basis; a detailed derivation is given 
in an appendix. We then describe a method for graphically presenting this informa- tion to a designer. 
 Surface Curvature In Figure 2, the plane (P) cutting the surface (S) contains the surface normal (N--) 
at the point A on the surface. At the point A, suppose the line of intersection has curvature K. As P 
is rotated about E, K changes with the angle of rotation, 0: K = K(0). As described in [4], Euler proved 
there are unique directions for which K reaches a minimum and maximum. Curva- tures in these directions 
are called the princi- pal curvatures, K m and K M. Further., two combina- tions of these characterize 
the local nature of the surface, the average and Gaussian (often referred to as Total) curvatures K 
and K . a g K a = (K m + KM)/2 Kg = K'K M Figure 2. Surface curvature illustration.  To see how ~o 
compute these, consider first the two-dimensional case of a circle as shown in Figure 3. From elementary 
calculus, the curva- ture K is K "- Y" (l + y,2)3/2  At the origin, y' = O and K = y" . Expanding 
in a Maclaurin series, since y(O) = y'(O) = O, Computer Graphics Volume 15, Number 3 August 1981 Ay 
= 1/2Ax2y '' + O(Ax 2) and lim 2A~ y" = K = Ax+O Ax z  describes the curvature at the origin. Y Ax 
 Figure 3. Curvature of a circle.  Surfaces of interest are bivariate param___etric surfaces, ~.g., 
~(u,v). In Figure 4, AS is the change in S as we move a short distance along the intersection line. The 
parametric analog of ~y is the projection of AS on N, the dot product AS.N. Thus we are led to the following 
expression for K for a parametric surface. where (A B C) = Q'(Lu S SVV) "Q = ('Su x ~) UV ' a normal 
vector, and Su, Svv , etc. are partial derivatives of ~(u,v). What is it that these values tell us about 
the nature of the surface? A local protrusion or bump will have K > 0 while an indentation will a have 
K < 0, with the magnitude of K indicating a a  the sharpness of the bump or hollow. But if K m and 
K M differ in sign or are near zero, K a tells us little. K , on the other hand, nicely charac- g terizes 
the nature of the local curvature by dis- tinguishing among three cases, as illustrated in Figure 5. 
The cases depend on whether K m and K M have the same or opposite sign, or are near zero. Shape K m 
K M K g ell iptical same > 0 (bump, hollow) sign K = l im 2N.AS As 0 Figure 4. Curvature of parametric 
surface.  A bivariate Taylor expansion yields an expression for K(Au, Av) which can be differentiated 
to obtain extrema of K. From this can be derived equations for K a and Kg, and from these, K m and K 
M can be computed (see Appendix). K a = (A I\1 2 -2BSu'Sv + CI Su 12)/2]EI 3 K = (AC -B)/[~I 4 g hyperbolic 
opposite < 0 (saddle point) sign cyl indrlcal one or = 0 (ridge, hollow, plane) both = 0 Figure 5. Characterization 
of surface elements by K . g K then readily distinguishes elliptlcal, hyper- g bolic and cyllndrlcal 
cases; K a can be used subse- quently to separate bumps and hollows, ridges and hollows, etc. Thus K 
and K serve to character- a g [ze the local shape of the surface, providing just the objective measure 
of surface curvature that we sought. Described in the next section, these scalar values may be computed 
at any point of the surface and tell us (locally) if there is a bump, hollow, etc. along with the magnitudes 
of each. Graphical Representation Though we now have an objective measure of surface  Computer Graphics 
Volume 15, Number 3 August 1981 curvature, the problem of presenting this to a designer, i.e., showing 
the variation in curva-ture over a surface in 3-dlmensional space, remains. The difficulty is the lack 
of a fourth spatial variable with which to represent curvature. The same difficulty, of course, applies 
to any scalar, e.g., pressure, temperature, etc. Although various solutions have been attempted with 
line drawing graphics, none have proven satisfactory: -numerical values on surface: crowded, hard to 
read and distinguish -contour lines: computationally demanding, subject to misinterpretation -hedgehog 
diagrams: (surface normals with length proportional to value) difficult to interpret. Use of the color 
spectrum, as first pointed out by Christiansen [5], provides the needed extra dimen- sion, superimposing 
this color variation on the three-dimensional image, as described below. As an example, consider how 
K might be shown. Areas g where K is positive are red, areas where it is g negative are blue, and areas 
where it is near zero are yellow. Superimposing these colors on a solid shaded image (as opposed to a 
wire-frame line drawing) will show the user immediately and quantl- tatively how curvature varies over 
the surface. IMAGE GENERATION This starts with the construction of a polygonal approximation formed by 
sampling the parametric surface along flow lines (constant u and v). The designer may specify either 
the number of samples or a maximum chordal deviation between samples. All four curvature values (Km, 
K M, Ka, and Kg) are computed at each vertex or sample point. A set of values for Km, etc. is assigned 
to each polygon by averaging the values at the vertices. To remove hidden portions, back-facing polygons 
are optionally culled; the remainder are colored and shaded as described below and sent to a frame buffer 
for display in reverse order of their distance from the viewpoint (painter's algorithm). Polygons are 
assigned a color either by the user interacti~ely specifying a color for the entire surface, or by using 
a color-encoded curvature value, described below. Shading uses a simple constant-plus-cosine-squared 
rule (ambient plus Lambertian reflection), assuming a single light source at the viewpoint. Each polygon 
has a uniform shade since no attempt was made to produce smooth shading. To color-encode the curvature 
(or any scalar value), the entire scalar range is divided into five inter-vals (initially) with each 
interval assigned a different color. The initial color assignment for a curvature range of -.07 to +.03 
is shown in Figure 6. Interactive commands support modifica- tion of this assignment as well as specification 
of which curvature to use for the encoding. curvature (r g b) value values color .01 to .03 l 0 0 red 
-.01 to .O1 I l 0 yellow -.O3 to -.Of 0 1 O green -.O5 to -.03 0 1 1 turquoise -.07 to -.05 0 O l blue 
 Figure 6. Default color assignments. The surface sampling and curvature computations were written in 
PL/I and added as a user option to GM's CADANCE system [7] which runs on a large IBM system. The results 
are transmitted as geom- etry and curvature files to a DEC VAX ll/7BO where images are generated and 
manipulated. The display device is a Ramtek 9300 (640 by 480 resolution, lO bits per pixel). RESULTS 
To test the theory and implementation, several known shapes, all mathematically well defined, were displayed. 
Consider, for example, a torus, illustrated in Figure 7 below. On the outside, K M and K m have the same 
sign (Kg > 0), while on the inside they have opposite signs (Kg < 0). On the rim, K will be zero yielding 
K = O. The m g default color assignment results in the image shown in Figure 7, as predicted. Consider 
a sur- face element on the "outside" of the torus. Here both K m and K M curve towards the center of 
the torus. Both have the same slgn, hence K is posi- g rive and outside areas are colored red. Similarly, 
for an "inner" element, K m and K M have opposite signs, K is negative and the inner areas are blue. 
g A second interesting example is the catenold, a catenary of revolution. One result of this being a 
"minimal energy" surface is that the average curvature is everywhere zero. Thus, as demonstra-ted in 
Figure 8, K says nothing about the shape. a K on the other hand varies from -i.O at the center g to zero 
at the ends, illustrating the change in shape. Basically this says every point on the catenold is a saddle 
point, with the sharpest curvature at the center. m and K M tend Both ...K toward zero with increasing 
dlstance from the center. Automotive Components Following testingwith known shapes, the system was applied 
to several automotive parts. Illustrated in Figure 9 is the right half of a 1980 Pontiac "B" body hood, 
in both normal and curvature colored forms. Observe that the normal colored image (Figure 9a) says little 
about the detailed shape, Even the longitudinal crease is visible only as a faint darkening. The Gaussian 
curvature colored image (Figure 9b) gives much more information, and more clearly. As expected, most 
of the hood is relatively flat (red area), with high values of   Computer Graphics Volume 15, Number 
3 August 1981 curvature along the longitudinal crease and at the front (yellow, green). Unexpected, however, 
was the high value at the back outside corner. An observer from Design Staff on seeing this was moti- 
vated to examine the hood of a production vehicle, and in fact, discovered a minor "flat" spot in just 
this area. Figure IO shows a portion of a front fender from a 1980 Oldsmobile "B" body (Delta 88). Here 
four separate Gordon surface patches are shown. It will be seen that the patches blend together in an 
overlapping fashion. This is indicated by, for example, the small green areas in the blue part and vice 
versa in Figure lOa where the parts were assigned different colors and colored normally. In this design, 
each part has been extended a little beyond the original boundaries to get a better blend. Thus, where 
the two parts overlap, they are very close in "Z" value. Due to the particular hidden surface algorithm 
then, some polygons of each part are shown on top of the other in essentially random locations because 
this algorithm does not handle intersecting surfaces correctly. Figure IOb shows the average curvature 
for the same part. Observe the relative constant curva- ture (desirable for a "smooth" surface) except 
in the region where the blade of the scythe meets the handle. DISCUSSION As the example of the torus 
illustrates, Gaussian curvature does show the nature of surface curva- ture. Further, the superimposition 
of color graphically shows the variation of curvature over a surface in 3-space. Other examples, not 
illus- trated, including a sine wave of revolution and a sphere, confirmed both the use of Gaussian and 
average curvature to understand how a surface behaves and the use of color as the mechanism for illustrating 
it. The example also shows that these ideas can be applied to real automotive components. The point 
to be made from the "flat spot" example is the qualitative use of this approach, that is, its utility 
in providing a complete view of curvature over theentire surface as well as providing a means for the 
designer to get a qualitative "feel" for how the surface behaves.. As an example of this in a quite 
different context, during early testing, a display of surface curvature of a bivariate probability distrlbution 
[6] was made. According to the analyst, had it not been for this display he would have missed a certain 
beak in the distribution. In sum, potential problem areas are made imme- diately apparent to the designer, 
in a single image, an achievement not possible using a line drawing display. Thus, this approach holds 
poten- tial for improving design quality while reducing the time required, and may help move the applica- 
tion of computer-aided design to earlier stages in the overall design process. The examples also illustrate 
the potential for the use of color as a quantitative measure of a fourth dimension. To be sure, the precision 
is not high. However, we believe it is certainly adequate as a "school grade" (i.e., A B C D E) measure 
which suffices in many applications. CONCLUSIONS Gaussian and average curvature can be used to indicate 
the shape of local elements of a three- dimensional curved surface. The method described here, by providing 
a means for color-encoding these scalar functions and superimposing the result on a three-dimensional 
image, provides a useful, effective means for displaying surface curvature to a designer. While further 
work is required, the examples indicate this approach holds much potential as a tool for surface design 
and analysis. ACKNOWLEDGEMENTS The writer is indebted to W. Weston Meyer of the Mathematics Department, 
GM Research Laboratories, for showing him the derivation given in the Appen- dix for curvature of parametric 
surfaces, and for discussions relating to surface curvature. The writer also wishes to thank G. Werschler, 
GM Design Staff, and N. Oliver, Computer Science Department, GM Research Laboratories, for program- ming 
the CADANCE sampling operator and curvature computation subroutine, respectively. REFERENCES I. Forrest, 
A.R. "On the rendering of surfaces." SIGGRAPH '79 Proceedings 13(2), August 1979. 2. Jones, J.I. "A 
system for designing and approximating aesthetically smooth curves with interactive graphic controls." 
Ph.D. Thesis, U. of Detroit, 1970.  3. Gordon, W.J. "Spline-blended surface inter- polation through 
curve networks." J. Math. &#38; Mechanics 18(IO), 931-952, 1969.  4. Struik, D.J. Differential Geometry. 
Addison- Wesley, Cambridge, 1950.  5. Christiansen, H. "Application of continuous tone computer generated 
images in structural mechanics." In: Structural Mechanics Computer Programs -Surveys, Assessments and 
Availability (Pilkey, Saczalski and Schaefler, eds.), U. Press of Virginla,'1974.  6. Rodrlguez, R.N. 
"Multivariate Burr III dis- tributions. Part I. Theoretical properties." Research Publication GMR-3232, 
General Motors Research Laboratories, Warren, Michigan, 1980.  7. Dill, J.C. "An overview of CADANCE 
-a computer graphics system at General Motors." Research Publication GMR-3150, General Motors Research 
Laboratories, Warren, Michigan, 1980.  Computer Graphics APPENDIX Curvature Derivation The curvature 
K of a parametric surface ~(u,v) = (X(u,v) Y(u,v) Z(u,v)) is (see APPROACH): lim 2N .AS K = -- As+o 
I~1 ~ Expanding X(u,v) in a bivariate Taylor series, F Au2 7  AX = X u +× v + 1 iXuu x uv Xvvl i 
2- L Av2 J with similar expressions for AY and AZ (subscripts u, v denote differentiation with respect 
to u and v). In vector form, AS = (AX AY AZ)  IX uu Xuv Xv!l rut = LAu + ~'vAV + l /u ,uv ,vv I z 
z LAv~ .J L UU UV V Since N is perpendicular to U and Sv, N.S U = N'S = O, and V X X f Au2 7 N-AS = 
~ I (N x Ny N z) Yuv Y AuAv I UU UV VV zUU vv Z Z Av2 ,_,I L UU UV V = ~1 (N x Ny Nz) " (S'uu S uv Svv) 
AuAv I Av 2 _J Letting A : N-Suu, B : N.Suv and C = N.Svv, we have N.~S : 1 (AAu 2 + 2BAuAv + CAv 2) 
and since I~1 : I~Au + ~vAVl, AAu 2 + 2BAuAv + CAv 2 K(Au, Av) =  l~ul~u~ ÷ 2~u ~v~U~V ÷ I~vl2Av 
2 Volume 15, Number 3 August 1981 To see how K varies wlth Au and Av, we divide by Av 2 and substitute 
t = Au/Av. Letting E = 12' G = vlSI2' F = S -S v, we have 'u'IS- U g(t) : (At 2 + 2Bt + C)/(Et 2 
+ 2Ft + G) Recasting this as an extremum problem with the implicit function H(g(t), t) = O, we see that 
 H(g(t), t) = (A -gE)t 2 + 2(B -gF)t + (C -gG) Conditions for an extremum are H(g(t), t) = O, and aH/at 
= O. The second of these yields t = (gF -B)/(A -gE), which, when substituted into the first, gives 
 g2(F2 -EG) + g(AG -2FB + EC) + B 2 -AC = 0 Under this condition then, we have extrema. In effect, we 
do not care what g looks like as long as we can find values which produce roots. Recalling that the sum 
and product of the roots of a quadratic are -b/a and c/a, we have from the above equation, AG -2FB + 
EC K = a 2(EG -F) 2  AI~I 2 -2B~u " ~v + CI~ 12 2(EG -F) 2 AC -B K - g (EG -F) 2 Since EG- F2 = 
l~uI~ I~v12- %.Sv)~= l~u× ~vI2,  Al~v 12 - 2BSu- Sv + cl~ 12 K a AC -B K  g 1\ x\i2  Using Q 
= S x S instead of the unit normal N, U V  and letting (A' B' C') = Q" (Lu Suv Svv)' we obtain the 
computationally more convenient Computer Graphics Volume 15, Number 3 August 1981 A' I~I ~ -2B'~- ~ 
. c'l\l~ U V K  a 21~I 3 A' C' B' K - g I~1 ~ Letting b = ~Kg -K a = (K M -Km)/2, the maximum and 
minimum curvatures are K M : K a -b K :K +b m a   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806802</article_id>
		<sort_key>163</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[A computer aid for man machine modelling]]></title>
		<page_from>163</page_from>
		<page_to>169</page_to>
		<doi_number>10.1145/800224.806802</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806802</url>
		<abstract>
			<par><![CDATA[<p>Computer aids for design ignore, in general, one of the critical areas in present day society. This is the workplace, where men and machines interact to produce goods or generate services with a mixture of social, economic and industrial objectives.</p> <p>This paper describes the SAMMIE (System for Aiding Man Machine Interaction Evaluation) Computer Aided Design System, which provides the designer with a wide range of aids including ones specifically for the evaluation of ergonomic criteria. A three-dimensional workplace model may be displayed on the screen of a graphics terminal linked to a minicomputer. To this workplace model, a Man Model of variable anthropometry can be added, enabling the workplace to be evaluated against criteria such as those relating to the operator's reach, vision, and fit capabilities.</p> <p>The system has been used in a wide range of applications including vehicles and aircraft, materials handling equipment, large assembly jigs, commercial and industrial interiors. Examples from some of these are shown illustrating the facilities of the system.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.6</cat_node>
				<descriptor>Computer-aided design (CAD)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering->Computer-aided design</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010472.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Architecture (buildings)->Computer-aided design</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Management</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P330011</person_id>
				<author_profile_id><![CDATA[81100345361]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[E.]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Kingsley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Compeda Limited, Stevenage, England]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332578</person_id>
				<author_profile_id><![CDATA[81100286647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[N.]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Schofield]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Compeda Limited, Stevenage, England]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14067916</person_id>
				<author_profile_id><![CDATA[81361599310]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[K.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Case]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dept. of Engineering Production, Loughborough University of Technology, England]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bonney, M.C., Blunsdon, CaA., Case, K and Porter, J.M.: Man Machine Interaction in Work Systems. International Journal of Production Research, Vol. 17, No. 6, 619-629, 1979.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Case, K and Porter, J.M: SAMMIE, A Computer Aided Ergonomics Design System. Engineering, January 1980.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Case, K., Porter, J.M., and Bonney, M.C.: Design of Mirror Systems for Commercial Vehicles. Applied Ergonomics, Vol. 11, No. 4, 199-206, 1980.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Porter, J.M., Case, K., and Bonney, M.C.: A Computer-Generated Three-Dimensional Visibility Chart. in: 'Human Factors in Transport Research' edited by D.J. Oborne and J.A. Levis, Vol 1, 365-374, 1980.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Bonney, M.C., Kase K., Dooner, M., and Porter, J.M.: Using SAMMIE Computer Aided Design System for Workplace Design. Paper presented at the I.M.S. Summer School, St. Johns, Cambridge, July 1980.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 SM'RdlE A Computer Aid for Man Machine Modelling E.C. 
Kingsley*, N.A. Schofield*, and K. Case* * Compeda Limited, Stevenage, England Department of Production 
Engineering and Production Management, University of Nottingham, England (now at Dept. of Engineering 
Production. Loughborough University of Technology, England) ABSTRACT Computer aids for design ignore, 
in general, one of the critical areas in present day society. This is the workplace, where men and machines 
interact to produce goods or generate services with a mixture of social,economic and industrial objectives. 
This paper describes the SAI.~IE (System for Aiding Man Nachine Inter ction Evaluation) Computer Aided 
 Design System, which provides the designer with a wide range of aids including ones specifically for 
 the evaluation of ergonomic criteria. A three- dimensional workplace model may be displayed on the screen 
of graphics terminal linked to a minicomputer. To this workplace model, Man Model of vari ble anthropometry 
can be dded, enabling the workplace to be evaluated against criteria such as those relating to the 
operator's reach, vision, and fit capabilities. The system has been used in wide range of applications 
including vehicles and aircraft, materials h ndling equipment, large assembly jigs, co~mercial and industrial 
interiors. Examples from some of these are shown illustrating the facilities of the system. I. INTRODUCTIOI~ 
Computer aided design methods are increasingly being used by designers. They provide the designer with 
 degree of flexibility which does not exist with conventional techniques. However, most CAD methods have 
no way of considering the inter-relationships between the equipment being designed and the operator 
who has to use it. The increasing complexity of equipment and social, economic and legislative pressures 
are forcing Ergonomics (i.e. the scientific study of the relationship between man and his working environment) 
to assume its rightful place early in the design process. Permission to copy without fee all or part 
of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. &#38;#169;1981 ACM O-8971-045-1/81-0800-0163 
$00.75 The SAMMIE package (System for Aiding Man-Machine Interaction Evaluation) has evolved with this 
in Wind and enables a designer to make judgements on the ergonomic implications of design at the same 
time as the functional aspects are being considered. SA~IE is a highly interactive system which is particularly 
suitable for use in designing trans-portation equilmnent , but is sufficiently general purpose to be 
useful in other fields such as industrial and military workplaces and interior design. The system is 
inter ctive as it is intended s design aid rather than aesign system. It requires the hnowledge ano 
experience of the designer in his own field but attempts to minimise the requirement for knowledge Of 
computing techniques. The system is intended as an aid to the professional ergonomist or designer rather 
than a means of 'automating' the assessment of Human Factors or the design process. In general, SA/~IE 
provides the user with the following facilities: * three-dimensional modelling of equipment nd workplaces; 
 * a m n model which c n be associated with the workplace model nd used for ergonomie evaluations; 
 * model-viewing facilities allowing for prespectives, projections, man's view, mirrors etc.,  * an 
easy-to-le rn method of communicating with the model so that it can be amended or ev luated.  2. SYSTEM 
COMPONENTS The two essential features of SM@IIE are its facilities for building three-dimenslonal models 
of workplaces nd its anthropometric and bio- mechanical man model. Use of the workplace mode] on its 
own can provide valuable design information, whilst the addition of the man model produces an Ergonomics 
or Human Factors evaluation system. The basis of the SM~HIE system is a hierarchical structure which 
llows a completely three-dimen- sional full-scale representation of the equipment to be created, stored, 
retrieved and displayed. This data structure may be manipulated to: * build a three-dimensional mode] 
of the work- place;  * place man model within the workplace;  163 Computer Graphics Volume 15, Number 
3 August 1981 * display and view the model in a variety of useful ways; a interact with the model to 
amend it or to carry out evaluations involving man and/or machines in the workplace. 2.! The Workplace 
llodel The workplace model is normally specified by data input prepared off-line from working drawings, 
although it is also possible to create model items interactively during a design session. Complex three-dimensional 
models can be built from a range of primitive shapes. Primitive shapes are those which can be defined 
parametrically such as cuboids, cylinders and prisms. For example, a cubold can be geometrically defined 
by statement of its three principal dimensions (length, width, and height). Complex shapes can be generated 
by the specification of the Cartesian coordinates of the vertices and the topological relationships between 
them (i.e. pairs of vertices are formed into edges which in turn are chained together to form faces). 
In addition to defining the geometry, the data input mechanism also enables the spatial and logical relationships 
between items to be speci- fied. The logical relationships are particularly important as they enable 
certain mechanical func-tions of the equipment to be demonstrated. For example. set of controls can 
be associated and manipulated logically as group without losing the ability to manipulate single member 
of the group. By careful construction of the model this facility can be used to simulate some functions 
of the equipment being designed, for example the raising and lowering of the forks of a fork-lift truck 
(see figure I). d The art of model building using the system is to build a model which contains the 
essential detail but excludes the clutter. A simple model is, of course, easier to construct and quicker 
to display and so one will try to use as simple a model as is possible. However, if greater detail is 
required for display purposes, then this may be included. A facility which helps to avoid clutter is 
that it is possible to selectively switch off the display of parts of the workplace so that the designer 
may concentrate on the parts in which he is currently interested. 2.2 The Han Hodel Although the three-dimensional 
modelling facility in SADIE has proved useful in its own right, the SAMMIE system differs from other 
modelling systems by having the additional feature of a man model provided for the purpose of ergonomic 
evaluations. The Man Model represents the human body as a mechanical system of 19 links connected at 
points corresponding with bone joints such as the hips, elbows, ankles etc. The visible aspect of the 
model is a three-dimensional mannikin which can be displayed in either of two forms: as a stick man in 
which just the links appear, or as a fleshed man in which each link is replaced by a correspond-ing flesh 
envelope. The dimensions of the rigid links may be varied to represent either individual people or a 
sample from a population. Full facilities are provided for changing this anthro-pometric information 
so that the user can explicit-ly specify one or a number of link lengths in a variety of ways; alternatively 
a percentile can be requested from population data. The system supplies the user with a particular population 
database although this can be easily substituted by any other database as required. The flesh contours 
are also variable so that for example a range of shapes from fat to thin men can be modelled. Figure 
2 illustrates some of the varieties of human form that can be generated in this way. Figure 2 Figure 
1   Computer Graphics Volume 15, Number 3 August 1981 The man model is constructed in a manner complete- 
ly analogous with workplace models in that it con- sists of a set of geometric, spatial and logical relationships 
maintained in a hierarchical data structure. These logical relationships are included so that, for example, 
when the upper arm moves, the lower arms and hand move in the way to be expected. All movements of the 
man model's links occur in a manner consistent with movements available to human beings I it is impossible 
to move limbs so that human joint constraints are exceeded. As with the limb lengths data, the normal 
and maximum joint constraints data can be altered according to available information. SAMMIE is primarily 
concerned with the physical attributes of the human being and has not been designed to assess environments, 
psychological or physiological characteristics. 3. OPERATING THE SYSTEM A feature of SAMMIE upon which 
considerable emphasis has been placed, is its usability by designers rather than computer experts. It 
is extremely difficult to remove completely the necessity for computing knowledge, as initial access 
to the computer must be obtained and data prepared. However, during the actual running of SAMMIE a menu 
system of control is used to present command alternatives to the user. A menu is a list of commands presented 
on the graphics screen alongside the picture of the model and represents all the alternatives currently 
available to the designer. The commands are activated by a typed abbrevia- tion of the command word 
or by the use of a lightpen. The careful choice of easily under- stood command words means that inexperienced 
users can become proficient in the use of the system within a few days. More importantly, however, such 
a system enables the designer to remain in control of the system throughout its use so that it is supplementary 
to his own skills rather than a replacement design metho- dology. There are over 35 menus of which the 
most important are: * WORKPLACE MEh"O for interactively position- ing models or component parts of models 
in the workplace; * DISPLAY MENY for selectively displaying only part of a model at a time;  * SEQUENCE 
I~NU for running a series of discrete movements of a model into an animated sequence;  * MAN HEI~ for 
changing the anthropometry and posture of the man model;  * MAN'S VIEW HENU for displayin E the view 
seen by the man model.  Certain commands merely enable a different menu to be displayed, whereas others 
will cause some change to the model or the way in which it is displayed. Although a large number of 
menus are available, typically only a small selection will be required for any particular design problem. 
In certain circumstances it is possible for the user to interact with the model more directly than through 
menus. For example, it is possible to re-position items on the screen by the use of a lightpen or cursor. 
 Con~nunication in the other direction, from the system to the user, is primarily graphical, most of 
the information being given to the user in the form of pictures of the model. However, at certain points, 
information is given directly to the user in the form of messages indicating error conditions, constraint 
violations, values of important para- meters, alternative ways of progressing etc. Permanent records 
of the design can be obtained either directly from the screen if a hard copy unit is available, or alternatively 
by creation of a disc describing the picture and user-supplied annotation. This file file can be used 
to re-create the picture on the screen of any suitable graphics terminal or to produce a plot of a digital 
plotter. A record of the design process in the form of a logging file is automatically maintained on 
disc whilst SAMHIE is being used. This file contains all input and output to the terminal with the exception 
of graphical output. Hence all messages and menu selections are logged along with other information which 
may be useful to the designer. 4. FACILITIES 4.1 General Facilities 4.1.I Viewing Three basic methods 
are available for viewing the full scale three-dimensional model of the work- place and man models on 
the two-dimensional screen of a graphics terminal: * plane parallel projection  * perspective  * simultaneous 
display of up to four views of the model (typically three projected views in plane parallel projection 
and an oblique view in perspective).  In all cases the model is viewed by the specifica- tion of a 
ICentre of Interest' and a 'Viewing Point'. These may be specified in a number of ways which are useful 
to the designer, so that for example the Viewing Point can be made to coincide with the man model's eyes 
giving the view as seen by by the man model (see section 4.2.3 'Assessment of Visual Factors'). Similarly 
the Centre of Interest can be specified as an object in the workplace or three-dimensional coordinates 
in the model space. By suitable manipulation of the viewing parameters the designer can obtain three-dimensional 
views as he 'walks around' or 'enters the model, or alternatively the more familiar plan, front, or side 
elevation can be displayed. At any stage the user can 'zoom' in to incus on particular parts of the model. 
 The model is normally displayed in 'wire-frame' form with all parts of the model visible, but when required 
the 'hidden lines' of obscured objects can be removed. Figure 3 illustrates a simple model in both wire 
frame and hidden line versions. Hidden line removal is not normally undertaken at every regeneration 
of the picture, since it is time consuming and with experience the facility is only required to clarify 
visual ambiguities or 165  Computer Graphics Volume 15, Number 3 August 1981 or prepare hard copies. 
% Figure 3 Finally, there is a facility in SAI~!IE for speci-fying up to sixteen mirrors in the model. 
These are defined by nominating a particular surface as the mirror and associating with it a focal length. 
The focal length may be such as to define a plane mirror (very large focal length), a convex mirror (negative 
focal length), or a concave mirror (positive focal length). On drawing the model all lines obscured 
behind the mirror are removed and any reflected images are placed within the perimeter Of the mirror. 
Images will be inverted, enlarged, or reduced as appropriate to the position of the object and the focal 
length of the mirror. Figure 4 illustrates the driver's eye view of a bus mirror with a car in the process 
of overtaking (both bus and car are in wire-frame form). 4.].2 Model Manipulation In the early stages 
of building a model, inter-active facilities are useful as an aid to formu- lating the model correctly. 
For example, difficulty can be experienced in expressing the precise orientation of objects when preparing 
input data. As an alternative approach, SA}~IE ~nables the' object to be orientated approximately on 
input and then interactively moved into position. Manipulation of the model may also be required during 
the evaluation stages of the design. For example, the same object may be placed in a number of different 
positions so that various tests can be carried out to determine the most suitable location. At a more 
complex level the simulation of the actions of machinery may require a large number of objects to move 
in an inter- r~lated manner. Having located the object that it is desired to move, there are five basic 
operations that may be carried out on it. It may be ROTATED, SHIFTED, Figure 4 DRAGGED, LOCATED, or ORIENTATED. 
SAI~IE also features a facility for altering the logical relationships between items in the workplace 
such that objects assigned to a particular other object on data input can be re-assigned to another 
object interactively.  4.1.3 Modelling of Movements Operations such as the raising and lowering of 
the forks of e fork-lift truck (see section 2.1) can be carried out interactively under the direct control 
of the designer, or alternatively the permissible forms of movement can be specified on input. These 
pre-determined movements are known in the system as 'Modifications' end are useful for three reasons. 
 Firstly the specification of movements can cause the user some problems if this has to be done in terms 
of the model's three-dimensional axis system. Pre-speciflcation, although it, too, requires knowledge 
of the axis system, enables a symbolic name to be given to the movement. The movement can then be initiated 
by a single co,and giving the appropriate name such as RAISE FORKS. The need for an intimate knowledge 
of the axis system and data structure is thus removed and this may be an important feature if the user 
of the model is different from the person who initially specified the data. Also, modifications become 
a part of the data structure in much the same way as geometric data I and can be retained for use at 
a future date. Secondly, modifications enable movements to be constrained to those available in the real 
 equipment, e.g. in the case of the fork-lift truck it can be arranged, so that it is impossible to 
exceed the available travel in the upwards or downwards direction. Thirdly, and perhaps most importantly, 
modifies-   Computer Graphics Volume 15, Number 3 August 1981 tions can be linked together to form 
Sequences of movement. In this way for example a complete task such as the loading of a container would 
be simula-ted by the issuing of a single command. This may be important where the same operation is to 
be evaluated under a number of different conditions (e.g. with different dimensioned equipment or different 
sized operators). In some senses Sequences provide a mechanism for animatin~ the model within the limitations 
imposed by the time taken to redraw the model or part of it i.e. only in the simplest models will the 
movements displayed be at all 'smooth'. Sequences of movements can be specified for the man model just 
as they can for the workplace model. Thus, for instance, the sequence of postures in-volved in the access 
of a vehicle could be demonstrated. 4.2 Ergonomic Evaluation Facilities Ergooomic evaluations that can 
be carried out using the man model in S/Off.HE include: * tests on the model's ability to reach items 
defined in the workplace, or teats to display complete envelopes of reach; * tests of the ability of 
the operator to fit into the available space within the equipment. The working postures adopted can be 
evaluated for feasibility and comfort by reference to the degree of joint extension; * display of the 
view as seen by the man model in his working environment and evaluation using visibility plots to enable 
'blind spots' to be quantified;  * demonstration of sequences of movement simulating work tasks.  
 4.2.1 Reach Tests When testing the ability of the man model to reach the objects in the workplace model 
a posture for the appropriate arm or leg is predicted by the system and tested for feasibility against 
the joint angle constraints. The user can specify a point to reach by a number of means: * the reach 
point may be specified in terms of three-dimensional coordinates of the workplace model i.e. the global 
coordinates; * an object in the workplace model may be named and the 'anchor point' of the object becomes 
the reach point;  * the current position of the man model's hand or foot may be incremented away from 
its current position towards the object to be reached.  If a point is out of reach because the sum 
of the links lengths is inadequate an error message is given which indicates by how much the point is 
out of reach. If the point cannot be reached because of joint angle constraints an appropriate message 
is given. The reach algorithm predicts a single feasible posture which is''good' with respect to extension 
of limbs about joints, but cannot take into account other factors such as the type of operation being 
carried out, loads being carried, the time for which the posture ia to be held etc. Within the reach 
menu there is the facility for creating special reach paths. These are ordered series of reach points 
linked to form regular grids. The grid spacing is under the control of the user so that a fine or coarse 
grid of points can be created. A reach test on the path will cause each point on it to be tested with 
the indicated hand or foot. On completion of testing of every point the user may inspect the results 
in one of three ways: * Maximum geometric reach contours indicate the area or volume which can be reached 
by the links without joint angle constraints being considered; * maximum reach contours show the area 
that can be reached within the restriction of the joint angle constraints; * normal reach contours 
show the maximum area that can be reached 'comfortably' i.e. without extending the limbs to their maximum. 
Figure 5 illustrates the results of a reach test on a panel in an aircraft cockpit. Figure 5  4.2.2 
Fit Tests The ability of a range of operators to fit into available workspaee can be assessed visually 
using the facilities ~or specifying men of vary- ins size and shape and the facilities for viewing the 
model in a variety of ways. These tests are particularly useful for the assessment of access to workplaces 
through re-stricted spaces and in the assessment of necessarily confined workspaces. Hodei manipu- lation 
can be made to make best use of the available space and the revised layouts can be assessed in terms 
of the feasible maximum stature and somatotype (fatness/thinness) limits  Computer Graphics for the 
users of the equipment being designed. 4.2.3 Visual Factors Assessments Views of the model as seen by 
the man model re gener ted by the system in the same way that the normal perspective views re produced 
except that the viewing point is set t either the man's left eye, tight eye, or mean position between 
the two. On iniiial entry to this viewing mode. the line of s~ght is for the eyes looking straight ahead, 
although ny movement of the head away from the straight ahead position is ccounted for. From this position 
either the eyes or the head may be moved to give a new view. These movements may be made in number of 
ways: * within the range of movement available to the eyes and head, they may be moved away from the 
current position either UP, DOWN, to the RIGHT or to the LEFT. When constraint to movement is encountered, 
a suitable message is given and no furhter movement in that direction is permitted; * the man may be 
made to look directly at an object by making some point on the object the centre of interest; * by a 
method analogous to reach paths (see  section 4.2.1) an area or volume can be tested for sight by the 
man model. In the latter method a grid of points to be viewed is placed over the surface or through 
the volume, and attempts are made to move the head or eyes so that each pdint in turn becomes the centre 
of interest. The view of each point is not displayed, but two m~ore paths re automatically created. One 
of these paths displays those points that can be made the centre of interest without exceeding any constraints 
on head or eye movement while the other path displays the area that can be been 'comfortably' (i.e. with 
something less than maximum head or eye movements). Qu ntification of man's views in terms of areas 
obscured by door pillars, mirrors etc. is often aided by the provision of horizontal or vertical grids 
of known size. Figure 6 shows the applica- tion of ground level horizontal grid to the view from the 
cab of a straddle carrier with nother straddle carrier (used for tr nsporting containers around docksides) 
in the distance. A development of this technique into a three-dimensional visibility chart enables the 
complete visibility characteristics of a vehicle to be represented on a series of two-dimensional annotated 
ch rts. 4.2.4 Stored Postures In many evaluation situations it is useful to have commonly used postures 
prespecified so that they can be called up on command. The Posture menu provides some standard postures 
such as STAND, SIT, PRONE, SUPINE, CRAWL, and CROUCH which will move the man model into the indicated 
posture automatically. These standard postures provide starting points for creating the exact desired 
postures. For ex mple, the SIT posture places the m n in a normal sitting position, but the user may 
wish to vary this for different seat back and squ b angles.  Volume 15, Number 3 August 1981 ~S-5:~"."/.' 
:': ~ ',", \ ",. ._- .-, .-.-,' ,; ,,,,,' ", ", ". /%Y V I I /% / ./\v,,',' \ Figure 6 The SIT posture 
is in fact so commonly used that, part from the default posture, it is also possible to locate a man 
model on seat directly. Thus by specifying the dimensions and location of the seat, the man will assume 
 posture on the seat so that his feet re on the floor and his back is against the seat back. This kind 
of 'parametric' specification of a posture is only available for sitting but all the other basic postures 
can of course be modified by the movement of individual joints. Some design problems require postures 
that could be considered as unusual. For instance, in the weightless conditions found in spacecraft 
working postures re adopted which would not otherwise be used in normal conditions. These kinds of postures 
can be created interactively and stored on disc file for future use. All postures, whether they be retrieved 
from disc file or standard postures, simply describe the tel tire orient tion of the body segments. 
Hence they re not specific to a particular set of limb dimensions or flesh shape. 5. BENEFITS OF THE 
S~E SYSTEH To date, S~@4IE has been used for a wide range of design studies in number of fields: Transportation: 
Car, Bus, Truck nd Aircr ft Materials Handling: Cr ne, Fork-Lift, Straddle Carrier Plant Design: Robot 
Work Stations, Machinery Layout in LonEwall Hining Operations, Aircraft Assembly Jig Interior Design: 
Office, Control Room, Bank Service Counter, Kitchen. In tackling work in these areas a number of adv 
ntages over tr ditional methods have been identified.  Computer Graphics Volume 15, Number 3 August 
1981 Firstly, by providing a flexible means of three- dimensional model building it enables initial design 
decisions to be taken without recourse to 'mock-ups' which are both expensive and time consuming. By 
virtue of the system's inbuilt evaluation facilities and the ease with which design changes can be made, 
SAMMIE allows a rapid optimisation of the design. Comparisons can be made between different design solution, 
new possibilities can be easily ~nvestigated, and 'no-hope' designs can be quickly eliminated. Again, 
by reducing pre-concept design time, design costs are reduced and new products can be brought to the 
production stage in less time than with traditional methods. The three-dimensional representation that 
SAi~IIE produces makes it easy for designers to communicate their work and with the facility for hard 
copies and plots, reports can be produced for progress meetings at any stage in the design process. Finally, 
the system provides the designer with a means of incorporating Ergonomic considerations into the design 
at an earlier stage in the design/ development/manufacture process than is normal using traditional Ergonomics 
evaluation methods. These are typically only applied at the later prototype stage when suggested modifications 
can be prohibitively expensive. Thus, overall the system allows the production of cheaper, quicker and 
ergonomically superior designs. A number of further developments are currently being implemented including 
a means of automatic interference detection between the man model and objects in the workplace, as well 
as an improved reach analysis facility. Further developments will include the integration of SAMMIE with 
other CAD systems, a strength analysis facility, and the capability for modelling curved surfaces. 6. 
SYSTEII AVAILABILITY SAMHIE is available internationally from the Compeda Group of Companies whose headquarters 
and development centre is in Stevenage, England. The Company specialises in the marketing and support 
of engineering applications software. Compeda will take full responsibility for the implementation of 
S~R~E on a user's, own computer system. Continuous development ensures that the technical quality of 
the Company's products is constantly being improved to keep pace with industrial requirements. Total 
technical support for all products is provided at all times. Users have automatic access to the latest 
version of the program as development progresses. Compeda has full training facilities and advisory 
 services to enable users to derive the maximum benefit from the software gystems. The Company will, 
if required, provide advice regarding computer hardware and terminals. Acknowled~ement~ Acknowledgements 
are due to the Science Research Council for their support of the S~IE project at Nottingham Dniversity 
over many years. Bibliography 1. Bonney, M.C., Blunsdon, CaA., Case, K and Porter, J.M.: Man Machine 
Interaction in Work Systems. International Journal of Production Research, Vol. 17, No. 6, 619-629, 1979. 
 2. Case, K and Porter, J.H: SAMMIE, A Computer Aided Ergonomics Design System. Engineering, January 
1980.  3. Case, K., Porter, J.M., and Bonney, M.C.: Design of Mirror Systems for Commercial Vehicles. 
Applied Ergonomics, Vol. II, No. 4, 199-206,  1980.  4. Porter, J.M., Case, K., and Bonney, M.C.: A 
Computer-Generated Three-Dimensional Visibility Chart. in: 'Human Factors in Transport Research' edited 
by D.J. Oborne and J.A, Levis, Vol |, 365-374, 1980.  5. Bonney, M.C., Kase K., Dooner, M., and Porter, 
3.M.: Using SAM}dIE Computer Aided Design System for Workplace Design. Paper presented at the I.M.S. 
Summer School, St. Johns, Cambridge, July ]980.    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806803</article_id>
		<sort_key>171</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Variational geometry in computer-aided design]]></title>
		<page_from>171</page_from>
		<page_to>177</page_to>
		<doi_number>10.1145/800224.806803</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806803</url>
		<abstract>
			<par><![CDATA[<p>A system has been developed which utilizes variational geometry in the design and modification of mechanical parts. Three-dimensional constraints between characteristic points are used to define an object's geometry. Modification of geometry is accomplished by alteration of one or more constraints. A matrix method is used to determine the shape of the part by simultaneous solution of constraint equations. A method for increasing the speed and efficiency of the solution procedure is described. The method uses the relationships between the geometry and constraints to minimize the number of equations and variables to be solved.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer-aided design]]></kw>
			<kw><![CDATA[Constraints]]></kw>
			<kw><![CDATA[Dimensions]]></kw>
			<kw><![CDATA[Geometric modeling]]></kw>
			<kw><![CDATA[Segmentation]]></kw>
			<kw><![CDATA[User interface]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor>Computer-aided design (CAD)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010432.10010439.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010472.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Architecture (buildings)->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P334165</person_id>
				<author_profile_id><![CDATA[81545183956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[V.]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P329595</person_id>
				<author_profile_id><![CDATA[81100084160]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[D.]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Gossard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P333062</person_id>
				<author_profile_id><![CDATA[81545398156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[R.]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Light]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hillyard, R.C. Dimensions and Tolerances in Shape Design, Technical Report No. 8, University of Cambridge Computer Laboratory, Cambridge, England, 1978.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807396</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hillyard, R. C. and Braid, I.C. Characterizing Non-ideal Shapes in Terms of Dimensions and Tolerances, Computer Graphics, ACM-Siggraph, Vol 12, No. 3, pp 234-238, August 1978.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Hillyard, R.C. and Braid, I.C. Analysis of Dimensions and Tolerances in Computer-Aided Mechanical Design, Computer-Aided Design, Vol 10, pp 161-166, June 1978.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Light, R.A. Symbolic Dimensioning in Computer-Aided Design, M.S. Thesis, Massachusetts Institute of Technology, Cambridge, MA, February 1980.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Lin, V.C. Two Segmentation Methods, Technical Document No. 21, MIT Computer-Aided Design Laboratory, Cambridge, MA, November 1980.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I.E. Sketchpad: A Man-machine Graphical Communication System, MIT Lincoln Laboratory Technical Report No. 296, Lexington, MA, January 1963.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 VARIATIONAL GEOMETRY IN COMPUTER-AIDED DESIGN BY V.C. Lin D.C. Gossard R.A. Light Abstract: A system 
has been developed wi~ich utilizes variational geometry in the design and ~podiIfication of mechanical 
parts. Three-dimensional constraints between characteristic points are used to define an object's geometry. 
Modification of geometry is accomplished by alteration of one or more constraints. A matrix method is 
used to determine the shape of the part by simultaneous solution of constraint equations. A method for 
increasing the speed and efficiency of the solution procedure is described. The method uses the relationships 
between the geometry and constraints to minimize the number of equations and variables to be solved. 
KEYWORDS AND PHRASES: dimensions, geometric modeling, computer-aided design, constraints, user interface, 
segmentation INTRODUCTION In most geometric modelling systems, the geometry of a part is used to derive 
dimensions. Creation and combination of geometric entities require exact specification of their positions 
in three-dimensional space. Once a three- dimensional model has been created, the changing of any dimensions 
on the model requires manual repositioning of the geometric entities in three-dimensional space by the 
designer. A system has been developed which utilizes variational geometry to facilitate the design end 
modification of mechanical parts. Once the topology of a part has been defined, three-dimensional constraints 
reflecting a given dimensioning scheme are used to position characteristic points of the part. Geometry 
is therefore derived from the dimei.sions of the part. The process of geometry modification is reduced 
to two steps: selecting the dimension(s) to be changed and entering the new dimensional value(s). Following 
a change in any dimension, a matrix method is used to reconcile the set of constraint equations and coordinate 
points. To increase the speed and efficiency of this method, the relationships between the constraining 
equations and coordinate points are used to segment the total number of equations and coordinates into 
a smaller and more manageable subset. Permission to copy without ~e all or part of this martial is granted 
provided that the copies are not made or distributed ~r direct commercial advantage, the ACM copyright 
noti~ and the title of the &#38;#169;1981 ACM O-8@71-045-1/81-0800-0171 $00.75 Work in this area has 
been done by Ivan Sutherland [6] . Sutherland used constraints between the coordinates of a part to constrain 
the error introduced into the system by a particular constraint. Sutherland made use of constraints as 
a design aid in the creation of a part but did not use geometric constraints for definition of the complete 
geometry nor in modification of part geometry after it was created. Robin Hillyard [1,2,3] proposed a 
general theory which uses geometric constraints between coordinates of a part to constrain variations 
in the dimensions as specified by the tolerances. Hillyard, however, did not address the issues of user 
interface or computational efficiency. The present work also extends the concepts to the constraint of 
curved surfaces. CONSTRAINING PLANAR POLYHEDRA In three dimensional Euclidean space, a point can have 
three degrees of freedom, the x, y, and z coordinate directions. An object, which is defined with respect 
to N characteristic points, will have 3N degrees of freedom. To fix the position of all the points and, 
therefore, the geometry of the object, 3N independent items of information must be supplied from the 
con-straints. A constraint can provide one or more items of constraining information. Each of the items 
of information can be written as a mathematical relationship between the characteristic points of the 
object. After a drawing input stage, in which the object has been checked to be topoldgically correct, 
a constraining stage fixes all the points of the object in three-dimensional space. For an original geometry, 
the constraint equations derived from the dimensioning scheme, are all satisfied. When a dimension is 
altered, the constraint equation which contains the dimension altered is no longer satisfied. To arrive 
at a new geometry, the coordinates of the N characteristics points may be determined by simultaneous 
solution of 3N equations for 3N variables. A modified Newton-Raphson method is used to derive the new 
geometry. publication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 
171 Computer Graphics There are two types of constraints, implicit and explicit. Implicit constraints 
are usually not represented in engineering drawings but are implied. Examples include right angles and 
parallel planes. Explicit constraints are dimensional constraints. They specify either a distance or 
an angle. A list of three dimensional constraints is given in Table I. This list represents constraints 
which the designer can apply interactively to the top- ological reprsentation of a three-dimensional 
object. Alternately, a semi-automatic con- straining scheme can be used whereby the implicit constraints 
are automatically generated by the computer and the explicit constraints are assigned by the designer. 
Although there are twenty-six constraints between entities which can be specified in three-dimensions, 
the set of constraints which are used for most common engineering parts consists of a smaller subset. 
Many of the mathematical relationships representing the constraints can be derived from analytical geometry. 
For example, a common implicit constraint of four points coplanar may be derived as follows. Three non-colinear 
points,P 1 (Xl,Yl,Zl) and P3 (x2,Y2,Z2), and P3(x3,Y3,Z3), define a plane Ax + By + Cz + D = O. If P 
(x,y,z) is a fourth point, the four points P, PI' P2' P3 are coplanar when PI P " (PI P2 x PIP3) = x 
-x I Y-Yl z-z] x2_x I Y2-Yl z2-z 1 x3-x I Y3-Yl z3-z I Expanding we obtain: A = Y2Z3 -Y2Zl -YlZ3 -Y3 
z2+Y3Zl + YlZ2 B =-x2z 3 + x2z I + XlZ 3 -x 3 z2-x3zl -XlZ 2 C = x2Y 3- x 2 Yl -x y -x 3 Y2+X3Yl + XlY 
2 13 D = -XlA yl B - ZlC For a point, P4(x4,Y4,Z4), to be on the plane, the equation Ax 4 + By 4 + Cz 
4 + D satisfied, = 0 must be CONSTRAINING CURVED SURFACES Presently, the types of curved surfaces re-presented 
are cylindrical, spherical, and conical surfaces. The defining points and parameters of each surface 
are constrained. For example, to define the cylindrical round shown in Figure I, it is necessary to constrain 
its six characteristic points. To define the cone in Figure 2, it is necessary to constrain its two characteristic 
points and its base circle radius are con-strained. To constrain the sphere in Figure 3, Volume 15, 
Number 3 August 1981 it is necessary to fix its center and radius. For surface intersection and mass 
property calculations, the implicit or parametric forms of the surface equations can be easily derived 
given the defining points and parameters of the surfaces. To constrain cylindrical, spherical and conical 
surfaces, no new types of constraints need to be added to the existing set of three-dimensional constraints. 
For example, a radical dimension of a cylindrical round can be re- presented by a constraint of the linear 
distance between two points (Figure 4). Tangency between a cylindrical round and a plane can be described 
by a constraint of perpendicular planes (Figure 5). Figure 6 shows an example of a three-dim- ensional 
object with cylindrical surfaces which has been constrained using the constraints described. Figure 7 
through II show the process of changing geometry of an object. To change the diameters of the two cylindrical 
holes, the designer selects the diameter dimension with the cursor, enters the new value, and enters 
the step number. The computer then updates the geometry automatically and displays the change in all 
views. NUMERICAL METHODS AND COMPUTATIONAL EFFICIENCY An iterative method is used in reconciling the 
shape model with the constraint equations. A LU decomposition process is used to determine the validity 
of a constraining scheme. In the case of an invalid constraining scheme, a set of redundant constraint 
equations and unconstrained variables are flagged and displayed so that the designer may correct the 
constraining scheme. Given any dimension change, to make the reconciliation of the shape model with the 
constraint equations more efficient, a method has been developed to reduce the number of equations and 
coordinates which need to be solved for in the iterative method. Given a dimension change, often only 
a subset of the total coordinate points will be changed. Only a subset of the total constraining equations 
will be required to solve for the changed coordinate points. The size of these subsets de- pends directly 
on the constraining scheme chosen. The method uses the Jacobian matrix to determine the sensitivity parameters, 
a backward propagation of coordinates to create a one-to-one bound coordinate-constraint equation relationship, 
and a forward propagation of coordinates to de-rive the sensitive equations and coordinates of a given 
dimension change. The method is as follows: I. Backward propagation to create list of coordinate-equation 
pairs. A. Retrieve the equation, f., which one un- bound coordinate, x., wh~re @fi " 0 172 B. Pair 
unbound coordinate, xj, with dimension change, has been developed. To further retrieved equation, fi" 
increase the speed of execution and to take advantage of the sparseness of the Jacobian, sparse matrix 
C. Remove paired coordinate, xj, from list methods can be implemented. of unbound coordinates. D. If 
the list of unbound coordinates is empty, stop. Otherwise go to A. REFERENCES II. Forward propagation 
to determine sen-sitive coordinates and equations of a I. given dimension change. A. For all changed 
constraint equations, fi' retrieve coordinate pairs, xj. 2. B. For al] x. retrieve all constraint equati°nsJ' 
~i' where @fi T~3 ~ 0. C. If no new constraint equations are found, 3 stop. Otherwise go to A. The process 
of backward propagation is essentially a one-step deduction procedure for matching constraint equations 
with coordinates. It can happen that at some time during the prop-agation, no unique one-to-one match 
can be found between coordinates and equations. This simultaneity property of the constraint equations 
on the coordinates can be approached in two ways. The first approach involves finding the minimum simultaneity 
between coordinates and equations. These groups of equations and coordinates can be linked together in 
the list of coordinate-equation pairs. For example, if only two unbound coordinates appear in only two 
equations, these two coordinates would be linked with the two equations and can be removed from the list 
of unbound coordinates An iterative procedure is used to find the minimum simultan- eity between coordinates 
and equations once the one-step propagation process is blocked. The second approach involves hypothesizing 
a pairing for an unbound coordinate and an equation. This will cause the propagation process to be continued. 
Systematic back- tracking is used whenever a contradiction in en-countered. CONCLUSIONS A system, built 
on the concepts of variational geometry, has been developed. Three-dimensional shape models consisting 
of planes, cylinders, spheres, and cones are built up with th~ use of three-dimensional constraints. 
Mod-ification of part geometry involves a simple two-seep process, selecting the dimension(s) to be changed 
and entering the new value(s). In terms of user interface, this method is preferable for the designer 
because of its simplicity. The use of variational geometry to create and modify a part greatly facilitates 
the process of design. Since design is inherently iterative, the techniques described lend themselves 
naturally to the design process. 4. 5. 6.  Hillyard, R.C. Dimensions and Tolerances in Shape Design, 
Technical Report No. 8, University of Cambridge Computer Laboratory, Cambridge, England, 1978. Hillyard, 
R. C. and Braid, l.C. Characterizin 9 Non- ideal Shapes in Terms of Dimensions and Tolerances, Computer 
Graphics, ACM-Siggraph, Vol i2, No. 3, pp234-238, August 1978. Hillyard, R.C. and Braid, I.C. Analysis 
of Dimensions and Tolerances in Computer-Aided Mechanical Design, Computer-Aided Design, Vo] lO, pp161-166, 
June'1978. Light, R.A. Symbolic Dimensionin 9 in Computer-Aided Design, M.S. Thesis, Massachusetts Institute 
of T~h~-ology, Cambridge, MA, February 1980. Lin, V.C. Two Segmentation Methods, Technical Document No. 
21, MIT Computer-Aided Design Laboratory, Cambridge, MA, November 1980. Sutherland, I.E. Sketchpad: A 
Man-machine Graphical Communication System, MIT Lincoln Laboratory Technical Report No. 296, Lexington, 
MA, January 1963. IMPLICIT CONSTRAINTS I. rigid body translation 2. rigid body rotation 3. four points 
coplanar 4. m points coplanar (m>4) 5. vertical plane 6. horizontal plane 7. perpendicular planes 
 8. parallel planes 9. line parallel to plane I0. parallel lines  II. equal x distance between point 
pairs 12. equal y distance between point pairs 13. equal z distance between point pairs 14. equal linear 
distance between point pairs EXPLICIT CONSTRAINTS 15. x distance between two points 16. y distance 
between two points 17. z distance between two points 18. linear distance between two points 19. distance 
from a point to a plane 20. distance from a point to a line 21. distance between parallel lines 22. 
distance from a line to a plane 23. distance between two planes 24. angle between two lines 25. angle 
between a line and a plane 26. angle between planes  To increase computational efficiency, a method 
of reducing the number of equations and coordinates which need to be solved, given a Table I. Three 
dimensional Constraints 173 Computer Graphics Volume 15, Number 3 August 1981 P6 1"1" P2 I  'PI P4 
I / I t 2 Figure 2. ConeFigure I. Cylindrical Round R ~ I ,> I Figure 3. Sphere I / Figure 5. Tangency 
Constraint Pl Figure 4. Radius Constraint 174 4- ~E][] E] ROTATE VIEW5 STOP PROP Figure 6. Object with 
Cylindrical Surfaces |.o~5 ~. GBDZA ~TT "k' m~ m,eu @ Figure 7. Selecting Dimension to be Changed ,-oi 
~ ....... 4 5 5 7 B 5 IE"T ENTER NEW VALUE~, 1,. e I I G~ ~. me @ Figure 8. Enter New Dimension Value 
7 ~ l ENTER 5TEF'5* IOH~IoLRI I @ Figure 9. Enter Step Number 176 [3[][][] ROTATE VIEWS STOP PROP Figure 
I0. New Geometry 2. ~DrA 1. GODIA L ~e. a. ee @ Figure II. Views of New Geometry 177  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806804</article_id>
		<sort_key>179</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[Presidents' Forum]]></title>
		<page_from>179</page_from>
		<doi_number>10.1145/800224.806804</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806804</url>
		<abstract>
			<par><![CDATA[<p>This session was originally organized in response to manufacturers' requests that the ACM sponsor a forum for discussion of issues relating to business aspects (i.e. management, marketing, finance, corporate planning, product development, etc.) of the computer graphics industry. The presidents' Forum was a first attempt at meeting this need and was designed to gather under one roof the movers and shakers from upper management ranks of the industry and allow them to interact with each other and with the audience while addressing a provocative set of questions.</p> <p>The objectives of the Forum are to involve industry managers in the SIGGRAPH Conference, to offer conference participants the opportunity to meet and interact with the chief executive officers of key companies in the computer graphics industry, and to communicate corporate thinking and varied approaches to timely issues.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 PRESIDENTS' FORUM MOTIVATION AND PURPOSE This session 
was originally organized in response to manufacturers' requests that the ACM sponsor a forum for discussion 
of issues relating to business aspects (i.e. management, marketing, finance, corporate planning, product 
development, etc.) of the computer graphics industry. The presidents' Forum was a first attempt at meeting 
this need and was designed to gather under one roof the movers and shakers from upper management ranks 
of the industry and allow them to interact with each other and with the audience while addressing a provocative 
set of questions. The objectives of the Forum are to involve industry managers in the SIGGRAPH Conference, 
to offer conference participants the opportunity to meet and interact with the chief executive officers 
of key companies in the computer graphics industry, and to communicate corporate thinking and varied 
approaches to timely issues. HISTORY The first Presidents' Forum was held at SIGGRAPH '80 in Seattle. 
Five CEO's/General Managers participated: Mr. Howard "Gene" Brewer, Vice- President and General Manager, 
Graphics Products Division, Calcomp, Inc.; Mr. Doyle Cavin, General Manager of the CPI Business Unit, 
Information Display Division, Tektronix, Inc.; Mr. Charles E. Mc Ewan, President, Ramtek, Inc.; Mr. John 
R. McPherson, President and Chief Executive Officer, Vector General, Inc.; and Mr. Jim Meadlock, President, 
M&#38;S Computing, Inc. The business units represented by these executives had combined revenues in excess 
of $400,000,000 and product offerings in the CAD/CAM, pen plotter, digitizer, direct-view storage tube, 
raster-scan display, calligraphic display, copier, and alphanumeric terminal markets. Topics of discussion 
included the potential effect of the economic downturn on the computer graphics industry, features and 
services requested by the user community which were not then being supplied by the industry, and the 
question, "Will raster graphics replace calligraphic displays?" Discussion was frank, view points were 
varied, and a good interchange with the audience ensued. Highly favorable comments from participants 
and the audience signalled the SIGGRAPH '80 Presidents' Forum as a milestone event. PRESIDENTS' FORUM 
'81 This year's Forum promises to be outstanding. Distinguished panelists include: Mr. David C. Evans, 
President and Chairman of the Board Evans &#38; Sutherland Computer Corporation Mr. Wayne R. Huelskoetter, 
President and Chief Executive Officer Dicomed Corporation Mr. Ren Zaphiropoulos, President, Versatec, 
Inc., and Vice-president, Advanced Product DeveloPment , Information Products Group Xerox Corporation 
 (Confirmation pending on a fourth panelist) The moderator, as in '80, is: Mr. L. Nell Johnson, Manager, 
Advanced Graphics Systems Vector General, In~. The business units represented by the presidents of these 
four companies manufacture a variety of products: calligraphic displays, frame buffers, visual simulators, 
3- dimensional modeling systems, computer output microfilm equipment, slide making systems, electrostatic 
printer-plotters, and turnkey CAD/CAM systems. Each of the companies is well recognized as a leader in 
its target markets. The presidents of these key companies will open a dialogue among themselves and 
with the user community on three contemporary business and technical issues, including a discussion of 
the biggest growth markets for computer graphics in the 1980's. (Due to the informal nature of the Forum 
and spontaneity of discussion, an accurate summary could not be prepared in advance of the event). The 
format for the Forum allows position statements by the panelists, questions or challenges from other 
panelists, and questions or comments from the audience. All the elements are present this year for another 
pace-setting event. ]79 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806805</article_id>
		<sort_key>181</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[An integrated system for creating and presenting complex computer-based documents]]></title>
		<page_from>181</page_from>
		<page_to>189</page_to>
		<doi_number>10.1145/800224.806805</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806805</url>
		<abstract>
			<par><![CDATA[<p>An experimental system is described for the design, development, and presentation of computer-based documents that combine pictures and text on a high-resolution raster color display. Such documents can be used, for example, for maintenance and repair tasks or computer-aided instruction.</p> <p><italic>Documents</italic> are directed graphs whose nodes we refer to as <italic>pages</italic>, in analogy to the pages of a paper book. A page includes a set of simultaneously displayed pictures, actions (procedures and processes) triggered when the page is accessed or when pickable picture elements on it are selected, and indexing information. Pages may be nested arbitrarily deeply in <italic>chapters</italic> that serve much the same organizing function as those of conventional books.</p> <p>The system is comprised of separate programs for laying out and drawing pictures, for graphically specifying the contents of pages, chapters, and their interconnections, and for displaying the document for user interaction.</p> <p>Examples are given from a prototype document for the maintenance and repair of computerized numerical control equipment. Emphasis was placed on designing actions for simple realtime animation (both by color table techniques and by transforming named primitives and manipulating their attributes), and for finding one's way around the document (displays include: a &#8220;timeline&#8221; of recently visited pages, immediate predecessor and successor pages, sibling pages and their interconnections, and those pages satisfying key-word retrieval requests).</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer aided instruction]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Interactive graphics]]></kw>
			<kw><![CDATA[Maintenance and repair]]></kw>
			<kw><![CDATA[Raster graphics]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.2.8</cat_node>
				<descriptor>Spatial databases and GIS</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.7.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003236</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Spatial-temporal systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010500.10010501</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document management->Text editing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003145.10003147.10010887</concept_id>
				<concept_desc>CCS->Human-centered computing->Visualization->Visualization application domains->Geographic visualization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Documentation</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39042315</person_id>
				<author_profile_id><![CDATA[81100427474]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Feiner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Group, Department of Computer Science, Box 1910, Brown University, Providence, Rhode Island]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P333707</person_id>
				<author_profile_id><![CDATA[81100381874]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sandor]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nagy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer and Automation Institute, Hungarian Academy of Sciences, Budapest, Hungary.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P18593</person_id>
				<author_profile_id><![CDATA[81452592989]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andries]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Van Dam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Group, Department of Computer Science, Box 1910, Brown University, Providence, Rhode Island]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bitzer, D. and Johnson, R. PLATO: A Computer-based system used in the engineering of education. Proc. IEEE, Vol. 59, No. 6 (June 1971), 960-968.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bolt, R. Spatial Data-Management. Architecture Machine Group, Massachusetts Institute of Technology, Cambridge, MA (1979).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Catano, J. Poetry and computers: experimenting with the communal text. Computers and the Humanities, Vol. 13 (1979), 269-275.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Engelbart, D; Watson, R.; and Norton, J. The augmented knowledge workshop. Proc. AFIPS Nat. Comput. Conf., Vol. 42 (1973), 9-21.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Feiner, S. A Diagrammatic Animation Language. Brown University Computer Graphics Group, Providence, RI (1981).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988498</ref_obj_id>
				<ref_obj_pid>988497</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[GSPC. Status report of the Graphic Standards Planning Committee. Computer Graphics, Vol. 13, No. 3 (August 1979).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Gurwitz, R.; Feiner, S.; Fleming, R.; and van Dam, A. Future Technical Documentation Delivery Systems for Naval Maintenance and Repair. ONR Report, Brown University Computer Graphics Group, Providence, RI (1979).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[The Learning Research Group. Personal Dynamic Media. Xerox Palo Alto Research Center (March 1976).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Ossanna, J. NROFF/TROFF User's Manual. Bell Laboratories Computing Science Technical Report 54 (1976).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Pato, J. LMS User's Manual. Brown University Computer Graphics Group, Providence, RI (1981).]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Reiss, S. Eris Reference Manual. Brown University Computer Science Dept., Providence, RI (1981).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Robertson, G.; McCracken, D.; and Newell, A. The ZOG Approach to Man-Machine Communication. Dept. of Computer Science Technical Report CMU-CS-97-148, Carnegie Mellon University, Pittsburgh, PA (October 1979).]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Smith, A. Paint. Computer Graphics Lab Technical Memo No. 7, New York Institute of Technology, Old Westbury, NY (July 1978).]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Strandberg, J.; Chomsky, C.; Scholes, R; and van Dam, A. An Experiment in Computer-Based Education Using Hypertext. Brown University Division of Applied Mathematics and Department of English, Providence, RI (June 1976).]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807418</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Shoup, R. Color table animation. Computer Graphics, Vol. 13, No. 2 (August 1979), 8-13.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I. SKETCHPAD: A man-machine graphical communication system. SJCC 1963, Spartan Books, Baltimore, MD (1963).]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Trent, B. SGP User's Manual. Brown University Computer Graphics Group, Providence, RI (1980).]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356591</ref_obj_id>
				<ref_obj_pid>356589</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[van Dam, A. and Rice, D. On-Line Text Editing: A Survey. ACM Computing Surveys, Vol. 3, No. 3 (September 1971).]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Williams, R. On the application of relational data structures in computer graphics, in Klinger, A.; Fu, K.; and Kunii, T. (eds.), Data Structures, Computer Graphics, and Pattern Recognition. Academic Press, NY (1977), 153-165.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Integrated System for Creating and Presenting Complex Computer-Based Documents ÷ Steven Feiner Sandor 
Nagy ~ Andries van Dam Computer Graphics Group Department of Computer Science Box 1910 Brown University 
Providence, Rhode Island 02912  Abstract An experimental system is described for the design, development, 
and presentation of computer-based documents that combine pictures and text on a high-resolution raster 
color display. Such documents can be used, for example, for maintenance and repair tasks or computer-aided 
instruction. Documents are directed graphs whose nodes we refer to as pages, in analogy to the pages 
of a paper book. A page includes a set of simultaneously displayed pictures, actions (procedures and 
processes) triggered when the page is accessed or when pickable picture elements on it are selected, 
and indexing information. Pages may be nested arbitrarily deeply in chapters that serve much the same 
organizing function as those of conventional books. The system is comprised of separate programs for 
lay- ing out and drawing pictures, for graphically specifying the contents of pages, chapters, and their 
interconnections, and for displaying the document for user interaction. Examples are given from a prototype 
document for the maintenance and repair of computerized numerical control equipment. Emphasis was placed 
on designing actions for simple realtime animation (both by color table techniques and by transforming 
named primitives and manipulating their attributes), and for finding one's way around the document (displays 
include: a "timeline" of recently visited pages, immediate predecessor and successor pages, sibling pages 
and their interconnections, and those pages satisfying key- word retrieval requests). Keywords: computer 
graphics, interactive graphics, raster graphics, computer aided instruction, maintenance and repair. 
CR Categories: 3.41,4.33, 4.34, 8.2 J~his work was supported in part by the Office of Naval Research 
under Contract No. N00014-78-C-0306 and the National Science Foundation under Grant No. INT-7302268- 
A03. tpresent address: Computer and Automation Institute, Hungarian Academy of Sciences, Budapest, Hungary. 
Permission to copy without fee all or part of this material is granted provided that the copies are not 
made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by permission of the Association for Computing 
Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. &#38;#169;1981 
ACM O-8971-045-1/81-0800-0181 $00.75 1. Introduction The paper documents by means of which much of today's 
information is created, stored, disseminated, and accessed are inadequate. They are bulky, difficult 
to keep up-to-date, expensive to produce and distribute, static, and limited in the types of information 
that they can provide. These limitations and the problems they create are well exemplified in documentation 
for mainte- nance and repair tasks where timeliness, ease of applying updates, portability, ease of use, 
and clarity in presenting complex information are priorities. The same technology that is responsible 
for the information boom, however, can also provide the ability to manage it. We believe that well-crafted 
computer-based documents can retain the useful aspects of paper documents, while providing sophisticated 
information retrieval facilities and presentation capabilities beyond those of paper. We are developing 
a database model and support software for research in computer-based documents. Documents are directed 
graphs whose nodes we refer to as pages, in analogy to the pages of a conventional book. A page includes: 
(1) a set of simultaneously displayed color pictures containing both text and illustrations, (Z) actions 
that are triggered when the page is accessed or when pickable picture elements on it are selected. These 
may, for example, run animations, access new pages, or even run other programs (a similar facility is 
exploited by ZOG ERobertson, 79] and SDMS [Bolt, 79]). The directed-graph structure arises from actions 
associated with a page that cause other pages to be accessed when they are invoked. The set of actions 
invoked from a document is con- ceptually part of the document itself -different documents may have different 
sets of actions. (3) indexing information, such as keywords, that may be used in honoring retrieval 
requests.  Pages may be arbitrarily deeply nested in chapters that serve much the same organizing function 
as those of paper books. The document itself is treated as a chapter containing those highest-level chapters 
and pages. The choice of a directed-graph structure is an out- 3. Graphics Package and Modeling System 
growth of our earlier work on FRESS (~File Retrieval and Editing SyStem), a text-processing system with 
Power- ful information structuring and retrieval capabilities [van Dam, 75.]. FRESS, like NLS FEngelbart, 
73"], has cross- reference facilities that allow any part of a document to reference or actually access 
any other part of It or another document. It has been used in large document preparation [Stranc~)erg, 
76.], including the preparation of a richly interconnected, multi-author "communal text" [Cateno, 79.]. 
Our experience with FRESS, and the suc- cess of network databases in a variety of fields ranging from 
computer-aided instruction [Bitzer, 71 .] to graphical database management [Bolt, 79"], have convinced 
us of the need for such generality. 2. Implementation Our document system is implemented under UNIX~ 
on a timeshared VAX 11/780. The graphics display is a Ramtek 9400 graphics system with a 1280 x 1024 
x 8 frame buffer, affording high image resolution. The belief, however, that more powerful, less expensive 
hardware can eventually be packaged to be about the size of a book [LRG, 76~] has been a fundamental 
motivating force behind the project. The system is comprised of a set of programs, all of which use common 
graphics support software. Each program corresponds to a phase in the nonsequential pro- cess of creating, 
modifying, and presenting a document: Picture Layout is the creation of individual pictures. Document 
Layout consists of page and chapter lay- out. Page layout involves combining pictures Coften rough sketches 
that serve as "place holders"~) on a page end associating actions with both the picture and its components. 
Chapter layout i~; the creation of chapter hierarchy and the imposition of high-level interconnection 
structure. Document Presentation is the interaction of the user with the document. User-selected actions 
are exe- cuted for the current page and the document is traversed as actions access new pages. We presume 
that the creation and modification of the database, ranging from isolated pictures to a com-pleted document, 
is accomplished by skilled profession- als: designers, illustrators, authors. On the other hand, the 
complexity of the pages and actions that are designed will vary from document to document according to 
the interests and ability of a particular document's intended users. The system is described in more 
detail in the follow- ing sections. {UNIX is a trademark of Bell Laboratories. All document programs 
use a modeling system [Pato, 81-I that builds end maintains a plcture as a hierarchical data structure. 
Pictures are displayed through calls to a 2D graphics package [Trent, 80.] based on the proposed CORE 
graphics standard [GSPC, 79"]. Thus, instead of conceptualizing (and storing~) each picture as a monolithic 
bit map as do many raster graphics "paint" systems [Smith, 781, our system employs an underlying logical 
structure in which a picture is broken into components such as lines, text, and filled areas, much like 
typical vector graphics "sketchpad" systems [Sutherland, 63~]. Although this approach does not offer 
the same facility in creating continuous tone pictures as a "paint" system, it provides several important 
advantages: Smoothly scaled instances. Different pictures may make use of smoothly scaled multiple instances 
of common subpictures. This feature is used exten-sively, both to decrease the drawing time needed to 
make complex pictures via instancing and to enable automatic on-the-fly generation of displays that include 
different-sized instances of previously created pictures. Greatly decreased storage requirements. We 
had an immediate need for writable storage of large quantities (hundreds to thousands~) of pictures. 
Though future mass storage technologies such as optical digital recording will make inexpensive fast 
multiple-gigabyte storage of arbitrary data a reality, current videodisk storage was not acceptable because 
of both its read-only nature and its incom- patibility with non-NTSC equipment such as our own. On our 
system a single unencoded full-screen-sized bitmap would require 1.25 mbyte of storage, as compared to 
the several thousand bytes needed for typical pictures with the present approach. One of our prototype 
documents, containing over 300 pic-tures, is smaller then 1.25 mbyte. Improved picking facilities. The 
logical structure allows data tablet selection of individual logical entities as well as arbitrary rectangular 
areas. The object can change location or size without changing identity and can thus remain pickable. 
Ease of manipulating individual elements. The abil- ity to name and access each of a picture's consti- 
tuent elements individually makes it Possible to alter attributes such as color, scale, translation, 
and rota- tion for animation effects. In addition to the storage constraints mentioned above, we are 
governed as well by severe time con-straints on picture refresh. This is an interactive system, designed 
to be used by a presumably impatient "reader". Consequently, no enti-aliasing has been attempted so far, 
a problem mitigated somewhat by the high resolution of the display. 4: Picture Layout This menu-driven 
program allows its user to draw and position line and solid geometric graphic primitives, bitmaps, and 
text (see Fig. 1 ). Primitives and instances may be scaled, translated, and rotated. The top section 
of the screen is the picture space. The bottom section of the screen contains a palette of pickable color 
chips and several rows of color-coded function buttons. A small area is reserved for prompts and messages. 
A data tablet and stylus (or puck) is used to select buttons and points in the picture space to draw 
and position objects. When alphanumeric input is required (e;g., for text strings) a simple screen editor 
is used. The buttons disappear, via color table manipulation, and the user inputs or edits named fields 
of text by driving a cursor over the lines being edited at the bottom of the screen. In our current system, 
we have allocated six out of the available eight planes to the picture itself, allowing 64 user-selected 
colors. Although every picture has its own palette, our original intention to choose the palette separately 
for each picture in the database yielded to our desire to simultaneously display arbitrary pictures together. 
The remaining two planes are used for the menu, system-controlled highlighting, and grids (described 
below). Input Primitives The user can draw straight and freehand lines in a variety of widths and styles, 
arcs, circles, and ellipses. Solid rectangles and an area-fill facility are provided. Existing pictures 
may be instanced with arbitrary scaling and bitmap images may be included, typically scanned in from 
a video camera. Single lines of text may be set in a variety of fonts. In addition, a simple typesetting 
capability allows the user to input TROFF I'Ossanna, 761 files using a screen editor and format them 
in a desig- nated rectangular section of the picture. All objects pos- sess "gravity" that can be used 
to position them pre- cisely in relation to each other. Editing Functions Any object may be deleted, 
moved, copied, centered between other objects, altered in size with or without regard to aspect ratio, 
and recolored. Certain objects may be modified in additional ways. For example, the font or text string 
of a piece of text may be altered. Objects can be groped by specifying a polygonal extent, allowing all 
enclosed objects to be edited with a single operation. An undo stack is maintained for reverting any 
modifications made during the session.  Grids A grid system allows the user to create grids that are 
used both for layout placement and as an aid in per- spective drawing. Grids are rectangles that may 
be divided independently along their length and width in proportions or absolute units (e.g., inches, 
picas, points) specified by the user. For example, a numeric specifica- tion of 2: .5in: 1 may be used 
to divide a grid horizontally into two columns, the left one twice as wide as the right, separated by 
a .5 inch gutter. Repetition factors allow regular grids to be easily specified. Grids possess "gravity" 
to their lines and intersections that may be turned on or off independently of the gravity possessed 
by objects. Grids may be rotated and distorted into trapezoids, a_facility useful for perspective drawing. 
Although grids are not pert of the picture being created, they may be operated on by the same commands 
used for editing pic- ture elements. The set of grids is named, saved, and retrieved separately from 
the picture. It is displayed in its own plane as an overlay whose colors are a function of the colors 
of the objects that it intersects to improve legibil- ity. 5.- Document Layout It was important to us 
that pictures created with the picture layout system be usable (for slide shows, displays for other programs, 
etc.) apart from the rest of the document system. We therefore decided to isolate from the picture layout 
system those parts of the docu- ment system that dealt with the directed graph intercon- nections. Another 
goal of the project was to impose as little design methodology as possible on the system's users so that 
neither a top-down nor a bottom-up docu- ment building strategy was enforced. As a consequence, the actual 
document design is accomplished with a separate program that provides two displays, one of a chapter, 
the other of a page. The user may interconnect empty pages (which are created as they are specified) 
or specify the contents of a page that has yet to be incorporated into the document. Since both displays 
are part of the same program the user can freely alternate between working on a chapter and working on 
an indivi- dual page. Page Layout The page layout display allows the document designer to position on 
a page one or more pictures made with the picture layout system. Picture elements (or rectangular areas) 
may be designated as buttons that will invoke some specified action when touched by the document presentation 
system's user. As a convenient shorthand, the user may indicate that whenever some picture is instanced 
in any other pic- ture it is to be considered a button with a default action. This feature helps reduce 
the tedium otherwise associ- ated with making standardized buttons on a page. If many pages of a document 
are to contain the same set of buttons, a "button board" picture may he created con-taining an instance 
of a special picture for each button desired. This one picture need only be included on the page to obtain 
its buttons. As any object, including a picture, may be made into a button and buttons may be placed 
anywhere on the page, the page design can be determined entirely by the designer. Actions may be attached 
to the page itself and are executed when it is accessed. Keywords may be asso- ciated with the pages 
for keyword retrieval. Overlays (pictures whose colors are derived by interpolating between an overlay 
color and the colors of regular pic- tures) may be specified. Editing facilities allow pictures to be 
redefined, buttons to be associated with new objects, etc. As with the picture layout system, six planes 
are devoted to the pictures. The remaining planes are used for the menu, grids, names, and for graphically 
indicating the extents of buttons and pictures. Chapter Layout This display allows the user to manipulate 
a chapter's structure (see Fig. 2~). At all times the display is-that of a single chapter which may contain 
an arbitrary number of subchapters and pages, both shown as named solid rectangles. The color of each 
rectangle is specified by the document's designer. Each subchapter's rectangle recursively contains the 
rectangles for those pages and chapters contained inside it. Each page's rectangle con- tains miniature 
instances of its pictures. The level of nested detail displayed by default can be set by the user, who 
may also select any unelaborated chapter or page and ask that its internals be displayed. The document 
is itself considered a chapter enclosing its highest-level chapters and pages and is displayed when the 
program is started up. The size and location of chapters and pages in the display is determined by the 
designer who positions rec- tangles in much the same way as with the picture layout system. The designer 
selects a point on the screen with the data tablet and a page or chapter, as appropriate, is created. 
The chapter display is a picture created with the modeling system (augmented with additional seman- tics 
kept in the same database used by the document display system]). Chapter and pages may not overlap, however. 
This restriction was imposed to minimize the confusion of "hidden" pages and chapters. Positioning is 
aided by the same grid facilities used by the picture lay- out system. The user may traverse the document's 
hierarchy by designating a chapter or page on the screen (or the current chapter's parent]) for further 
editing. If a chapter is selected it may be edited like the document itself. If a page is selected, the 
page editing facilities discussed above are invoked. Links Chapters or pages may be connected by "fuzzy 
links" that are created by pointing to two rectangles: a source and a destination. Fuzzy links may be 
made between any combination of chapters and pages. A fuzzy link between two pages indicates that some 
as-yet-t~specified action in the source page (selecting a button in the current implementation]) will 
access the des- tination page. When the source or destination is a chapter, this means that the chapter 
contains a page (perhaps several levels down]) that is the actual source or destination. The link is 
shown as an arrow whose tail emanates from its source and whose head terminates at its desti- nation. 
If a link in the opposite direction already exists the new arrowhead is added. In defining a fuzzy link 
the user does not have to specify what object is to be made a button or exactly what action is to be 
associated with it (hence our use of the term "fuzzy"~). Both source and destination might even be completely 
empty. When the user later edits a chapter or page associated with an unresolved link he or she will 
be prompted to resolve it by specifying an actual button and action. A consistency check may be run to 
point out unreachable nodes or fuzzy links that have yet to be resolved. Actual links made when editing 
a page are reflected in the document display by the appearance of an appropriately placed arrow. Since 
any page may potentially be connected to any other page, a confusion of arrows could result. There-fore 
arrows are only drawn between pages and chapters that share the same immediate parent chapter. Chapters 
inherit the links of their progeny. Consequently, when two linked pages are in different chapters, some 
arrow (possibly at a higher level]) in the document display will reflect the linkage. No self-links are 
shown, however. The user can also request the details of a link or the links associated with a node. 
A split-screen capability, in which each section of the screen is occupied by a chapter, allows the user 
to connect two nodes at different levels in widely separated parts of the document or work independently 
in different chapters. As previously mentioned, the graphical document structuring facilities that we 
developed grew out of our experience with FRESS. In the construction of large, complex FRESS databases, 
the directed-graph structure had to be entered by hand. This work often began with the implementor sketching 
out network diagrams (similar to the diagrams used both formally in the documentation and informally 
by seasoned users to explain the system to newcomers]). Then the implementor translated the sketches 
into the appropriate editor commands, linking together sections using a light pen. It was important to 
us that this natural way of expressing the graph structure -as a graph -he tt~ actual means by which 
the user structured the document. The added feedback of seeing the pages In miniature, not merely their 
names, makes the job of building a new document or modifying an old one much more pleasant and error-free. 
6. Document Presantation System The document's user traverses the database, usually by touching, with 
the data tablet stylus or puck, buttons that cause new pages to be accessed. The display pro- gram traverses 
the directed graph using the following simple algorithm: Access starting page loop forever wait for user 
interaction execute associated action ondloop The actions included In a document are specific to the 
document itself but make use of facilities provided by the display system. An Infinitely pushable, finitely 
poppable stack of accessed pages Is kept and a set of procedures is provided to access and manipulate 
the stack by pop- ping, pushing, and examining its contents. When a page is made current its list of 
pictures Is compared with that of the previous page so that only those pictures not In the new page are 
erased and only those pictures not in the old page are drawn. Actions have access to procedures that 
create and modify buttons, as well as to the facilities of the model- Ing system. Thus, actions may be 
created that interac- tively fashion an entire page. All actions are provided as "black boxes" to docu- 
ment designers, who need have no understanding of their internals. 7. SMART: A Case Study To aid In developing 
our document presentation sys-tem, we have been designing a document to emulate the SMART (Service, Maintenance, 
And Repair Technology) system. SMART was a concept developed by one of the authors (S. Nagy) for delivering 
technical documentation and assistance needed in the maintenance and repair of complicated equipment. 
We envision a portable maintenance aid for the 1990's in the form of a suitcase-sized device containing 
a ~igh-resolution color screen for the display of pictures and text, a touch-sensitive overlay, voice 
input/output, and a mass storage device [Gurwitz, 793. Sophisticated repair and test equipment would 
be built into (or directly interfaced to) the system. Thus, the technician's role would be minimized, 
whenever posalble, to mechanically attaching test leads to equipment and operating automated repair equipment. 
The system would decide whet needed to he done through a combination of pr.edetermined, proceduralized 
troubleshooting sequences and a generalized reasoning component, developed with artificial Intelligence 
techniques, that understood the dev- ice being serviced. In addition, a rich set of browsing facilities 
would allow an advanced user to freely access any part of the document. A prototype document for servicing 
computerized numerical control equipment was developed first, as this was our field of expertise and 
represented well in micro- cosm the large, complex databases that we wanted to handle. Sample pages are 
shown in Figs. 3-4. In this first document, no attempt was made to Implement actions that would embody 
actual realtime decision mak- ing or interface with external equipment. Instead, we concentrated on developing 
a small set of standard actions based on data tablet Interaction alone to provide a simple, easy-to-use 
interface. These actions will he used in other documents, perhaps with some modifica-tions. Basic actions 
New page. The current page Is replaced with the named page. New page is typically attached to elements 
that are to he made buttons that invoke other pages. It is the "glue" most often used to connect the 
database. Back page. A specified number of pages is popped from the stack and the current page is replaced 
with that last popped. Animation. A named program in a simple diagrammatic animation language [Feiner, 
81] is interpretively exe-cuted. The modest speed with which the screen can he refreshed prohibits the 
use of full animation. We there- fore rely on color table manipulation [Shoup, 791, pan- ning and zooming 
about the frame buffer, and the limited animation that can be accomplished by erasing and redrawing individual 
objects. The color table techniques used include cycling selected portions of the color table and smoothly 
Interpolating a color between other selected colors. Object manipulation is accomplished by scaling, 
translating, and rotating named objects in pic- tures. Overlays. A set of pictures is displayed in its 
own separate plane as a tinted overlay. Its colors are derived by interpolating between the" overlay 
color and the colors of the picture "beneath", providing an effect similar to that of a colored transparent 
overlay. Annotation. Each page is associated with a unique annotation picture. This action displays the 
annotation picture (if it exists) and creates a draw button. When draw is touched it creates the annotation 
picture (if It did not exist) and allows the user to annotate by drawing with the stylus. The annotation 
picture is displayed in Its own plane and is, in essence, a wrltable overlay. Because It is a regular 
picture, it may I~ter be altered with the picture layout system.  Actions that display document structure 
Presented with a computer-based document rich in connective tissue and deprived of the physical feedback 
of holding a book in one's hands, the user could easily become lost. We are trying to design a set of 
actions that will help the user maintain Cor regain) a sense Of where she or he is. Folio. A folio provides 
each page with a standard display of the page name, chapter name, document name, and time at which the 
page was accessed (see Fig. 3). Each name appears on a field that is appropriately colored for the page, 
chapter, or document. A miniature of the previously accessed page is drawn and is labeled as a back page 
button that when touched pops the previ- ous page from the stack and makes it current. The folio action 
would typically be associated with a page's access by the document's designer and produces its pic- ture 
automatically from a stored template. Timaline. This action is associated with a special time-line page 
that is empty except for the action. When the page is accessed the action is executed and completely 
constructs a set of pictures and buttons on the fly, comprising a pictorial transcript of the current 
session (see Fig. 5). The most recently viewed pages on the stack are inspected and classified by parent 
chapter. One horizontal band is drawn for each chapter represented, and is colored in the chapter's color. 
Miniatures of the pages are drawn, each nested in its parent chapter's band, starting with the oldest 
on the left, with the date at which each was accessed appearing below it. The minia- ture pages are made 
into buttons that when touched tran- sport the user back to the selected page. The chapter bands will 
(eventually) allow access to a read-only ver- sion of the document layout system's chapter editing display. 
Buttons permit the user to move forward and backward along the timeline to examine miniatures of pages 
viewed earlier. Index. Keywords picked from the list of keywords attached to pages by the document layout 
program are used to generate an alphabetized display of miniatures of those pages they match; each page 
miniature of this pic- torial index is a button that accesses its page (see Fig. 6). Neighbors. This 
action creates a display in which a selected page is shown in miniature at the center (see Fig. 7). Those 
pages that can access the center page are shown in miniature at the left with arrows leading from their 
associated buttons to the center page; those that can be accessed by the center page are shown at the 
right with arrows leading to each of them from the center page's buttons. If there are more connected 
pages than can fit on either side of the display, then small up and down arrow buttons appear at the 
bottom of the display. These may be used to scroll independently through the set of pages on either side. 
 8. Conclusions A system for creating and presenting richly con-nected computer-based documents has been 
implemented. In recognition of the difficulty of creating and maintaining complex databases we have emphasized 
building power- ful graphical authoring tools. By making all actions part of the database and implementing 
a general directed graph data structure, we have made possible a variety of documents suited to widely 
different purposes and audi- ences. Additions being planned Include= Automated page and chapter layout. 
Creating material that takes advantage of the capabilities of computer-based media is much more difficult 
than designing its paper "equivalent". In our current sys-tem, all page and chapter layout is the responsibility 
of the document layout system's user. We are beginning work on the problem of automating the layout and 
linkage of certain regularly-structured chapters and pages, such as those of parts catalo- gues. In addition, 
we would like to extend the con- cept of generating the contents of pages on the fly beyond that of displays 
such as the timeline, to include actual pages of simple procedural information such as those found in 
repair manuals. 3D capability. The graphics package used by the system is being extended to Include 3D 
primitives. Database extensions. The current database system is being replaced by a sophisticated relational 
data- base system rReiss, 81-J. This will make it possible to store pictures as relations [Williams, 
771, allow-ing queries based on the contents of pictures them- selves. For example, a user's description 
of what a page looked like and when it was seen could, through the mediation of a suitable front end, 
aid in its retrieval. Keyworded conditional links. A similar facility Is used extensively in FRESS to 
provide alternate ver- sions of documents. Text and structure can be selectively hidden and revealed, 
even as the reader explores the document. New actions. New actions will be Implemented, including graphical 
simulators for equipment being documented.  Acknowledgements Members of the Brown University Computer 
Graph- Ics Grot~ whose participation is gratefully acknowledged are: Joseph Pato, who wrote the modeling 
system; Randy Pausch, who wrote the picture layout system; Will Poole and Adam Seidman, who wrote the 
document layout system; and Barry Trent, who wrote the graphics package. Special thanks are due Imre 
Kovacs for his creative direction of prototype documents and high-level design of the picture layout 
system's user Interface.  References Bltzer, D. and Johnson, R. PLATO: A Computer-based system used 
In the engineering of education. Proc. IEEE, Vol. 69, No. 6 (June 1971), 960- 968. Bolt, R. Spatial Data-Management. 
Architecture Machine Group, Massachusetts Institute of Tech- nology, Cambridge, MA (1979). Catano, J. 
Poetry and computers: experimenting with the communal text. Computers and the Humanities, Vol. 13 (1979), 
269-276. Engelbart, D; Watson, R.; and Norton, J. The augmented knowledge workshop. Proc. AFIPS Nat. 
Comput. Conf., Vol. 42 (1973), 9-21. Feiner, S. A Diagrammatic Animation Language. Brown University Computer 
Graphics Group, Providence, RI (1981). GSPC. Status report of the Graphic Standards Planning Committee. 
Computer Graphics, Vol. 13, No. 3 (August 1979). Gurwitz, R.; Felner, S.; Fleming, R.; and van Dam, A. 
Future Technical Documentation Delivery Sys- tems for Naval Maintenance and Repair. ONR Report, Brown 
University Computer Graphics Group, Providence, RI (1979). The Learning Research Group. Personal Dynamic 
Media. Xerox Palo Alto Research Canter (March 1976). Ossenna, J. NROFF/TROFF User's Manual. Bell Laboratories 
Computing Sdance Technical Report 64 (1976). Pato, J. LMS User's Manual. Brown University Corn-puter 
Graphics Group, Providence, RI (1981 ). Relse, S. Eris Reference Manual. Brown University Computer Science 
Dept., Providence, RI (1981 ). Robertson, G.; McCracken, D.; and Newell, A. The ZOG Approach to Man-Machine 
Communication. Dept. of Computer Science Technical Report CMU-CS-97-148, Carnegie Mellon University, 
Pittsburgh, PA (October 1979). Smith, A. Paint. Computer Graphics Lab Technical Memo No. 7, New York 
Institute of Technology, Old Westbury, NY (July 1978). Strandberg, J.; Chomsky, C.; Scholes, R; and van 
Dam, A. An Experiment in Computer-Based Education Using Hypertext. Brown University Division of Applied 
Mathematics and Department of English, Providence, RI (June 1976). Shoup, R. Color table animation. Computer 
Graphics, Vol. 13, No. 2 (August 1979), 8-13. Sutherland, I. SKETCHPAD: A man-machine graphical communication 
system. SJCC 1963, Spartan Books, Baltimore, MD (1963). Trent, B. SGP User's Manual. Brown University 
Corn- puter Graphics Group, Providence, RI (1980). van Dam, A, and Rice, D. On-Line Text Editing: A Sur- 
vey. ACM Computing Surveys, Vol. 3, No. 3 (September 1971 ). Williams, R. On the application of relational 
data struc- tures in computer graphics, In Klinger, A.; Fu, K.; and Kunii, T. (eds.), Data Structures, 
Computer Graphics, and Pattern Recognition. Academic Press, NY (1977), 163-166. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806806</article_id>
		<sort_key>191</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>24</seq_no>
		<title><![CDATA[Towards a laboratory instrument for motion analysis]]></title>
		<page_from>191</page_from>
		<page_to>197</page_to>
		<doi_number>10.1145/800224.806806</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806806</url>
		<abstract>
			<par><![CDATA[<p><italic>Motion analysis</italic> is the systematic and usually quantitative study of the movements of humans, animals, organisms, cells, or other entities as recorded on movie film or video tape. Despite the utility of computer-aided motion analysis to many biological, social, and physical sciences, its role has been limited because it is so time-consuming and so expensive. Automated techniques can only be used on real images in very special cases; interactive techniques have involved laborious <italic>frame by frame</italic> operations.</p> <p>In recent years, Futrelle and Potel have revolutionized the process of interactive motion analysis by demonstrating how to <italic>digitize entire motions with a single sketch</italic> rather than with a sequence of operations on each constituent frame. However, their GALA-TEA system is applicable only to film and not to video tape records, and consists of equipment which cannot easily be engineered into a reliable and mass-producible laboratory instrument.</p> <p>The paper describes a prototype laboratory instrument for the interactive motion analysis of video tape records. The prototype includes a host PDP 11/45 computer, an experimental display processor called SPIWRIT, and a microprocessor-controlled video disk. SPIWRIT generates a computer-animated video representation of the phenomenon being analysed, and superimposes it on the actual video record being streamed from the video disk. SPIWRIT's bit-slice microprogrammable display processor produces true animation on a colour raster display by decoding segmented display file images into alternate halves of a double frame buffer.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer animation]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Display processor]]></kw>
			<kw><![CDATA[Motion analysis]]></kw>
			<kw><![CDATA[Raster graphics]]></kw>
			<kw><![CDATA[Video disk]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Digitizing and scanning</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010444</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P248880</person_id>
				<author_profile_id><![CDATA[81100448368]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ronald]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baecker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Computer Systems Research Group, University of Toronto, Toronto, Ontario, Canada and Human Computing Resources Corporation, Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14067613</person_id>
				<author_profile_id><![CDATA[81332516015]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Micom Co., Montreal]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P334457</person_id>
				<author_profile_id><![CDATA[81100228626]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reeves]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sprocket Systems, Lucasfilm, San Anselmo, California.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807490</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ackland, B. and Weste, N., "Real-time Animation Playback on a Frame Store Display System", Computer Graphics, Vol. 14, No. 3, July 1980, pp. 182-188.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Aisen, A.M., Al-Sadir, J., MacKay, S.A., Potel, M.J., Rubin, J.M., and Sayre, R.E., "Quantitative Ventricular Wall Motion Analysis of Biplane Coronary Angiograms", Proceedings of the IEEE Conference on Computers in Cardiology, Williamsburg, Virgina, Oct. 1980.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Badler, N. and Aggarwal, J. (Editors), Proceedings of the IEEE Workshop on Computer Analysis of Time-Varying Imagery, University of Pennsylvania, Philadelphia, April 1979.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>888547</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Baecker, R.M., "Interactive Computer-Mediated Animation," Ph.D. Thesis, Department of Electrical Engineering, M.I.T., April 1969, reprinted as Project MAC TR-61.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Baecker, R.M., "Picture-Driven Animation," Proceedings of the Spring Joint Computer Conference, May 1969, pp. 273-288.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Baecker, R.M. and Horsley, T., "Computer-Animated Simulation Models: A Tool for Transportation Planning," Transportation Research Record 557, Transportation Research Board, National Research Council, Washington D.C., 1975, pp. 33-44.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Baecker, R.M., Hauer, E., and Bunt, P.D., "Computer Animated Simulation of Taxi Dispatching Strategies," Transportation Research Record 657, Transportation Research Board, National Research Council, Washington D.C., 1977, pp. 14-19.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Baecker, R.M., Reeves, W., Covvey, H.D., Miller, D., and Galloway, D., "Interactive Computer-Aided Analysis of Cardiac Motion Sequences," appears in (Badler 1979).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807424</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Baecker, R.M., "Digital Video Display Systems and Dynamic Graphics", Computer Graphics, Vol. 13, No. 2, August 1979, pp. 48-56.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Baecker, R.M. and Duff, T.D.S., "A Programming Environment for Conversational Dynamic Modelling", Proceedings of the 1979 Summer Computer Simulation Conference, July 1979, pp. 32-39.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Bell System, "UNIX Time-Sharing System" (entire issue devoted to UNIX), The Bell System Technical Journal, Vol. 57, No. 6, Part 2, July-August 1978.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Bresenham, J.E., "Algorithm for Computer Control of a Digital Plotter", IBM Systems Journal, Vol. 4, No. 1, January 1965.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>359432</ref_obj_id>
				<ref_obj_pid>359423</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Bresenham, J.E., "A Linear Algorithms for Incremental Digital Display of Circular Arcs", Communications of the ACM, Vol. 20, No. 2, February 1977.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Burtnyk, N. and Wein, M., "Computer Generated Key Frame Animation", Journal of the Society of Motion Picture and Television Engineers, Vol. 80, March 1971, pp. 149-153.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Donkoh, S., "Computer Analysis Helps Train Athletes", CIPS Review, July/August 1980, pp. 14-15.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Futrelle, R.P., "GALATEA, a Proposed System for Computer-aided Analysis of Movie Films and Videotape," Quarterly Report No. 37, Institute of Computer Research, The University of Chicago, May 1, 1973.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Futrelle, R.P., "GALATEA: Interactive Graphics for the Analysis of Moving Images," Information Processing 74, Proceedings of the 1974 IFIP Conference, North-Holland Publishing Company, pp. 712-716.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Futrelle, R.P. and Potel, M.J., "The System Design for GALATEA, An Interactive Real-time Computer Graphics System for Movie and Video Analysis," Computers and Graphics, Vol. 1, pp. 115-121, Pergamon Press, 1975.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Galloway, D.R., "The Modelling of Dynamic Digital Video Display Systems", M.Sc. Thesis, Department of Computer Science, University of Toronto, 1979.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Hartley, M.G., Waterfall, R.C., and Fisher, A.J., "Preliminary Studies into the CAD of Pedestrian Circulation Areas", Universities Transport Study Group, Ninth Annual Conference, City University, London, Jan. 1978, pp. 693-700.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Hartley, M., "Designing Buildings for People to Move in", New Scientist, 10 August 1978.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Miller, D.H., "A Two-Dimensional Dynamic Video Display System", M.A.Sc. Thesis, Dept. of Electrical Engineering, University of Toronto, September 1980.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Miller, D.H., Vranesic, Z.G., and Baecker, R.M., "SPIWRIT&#8212;A Simple Real-Time Digital Video Display System", in preparation, 1981.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563285</ref_obj_id>
				<ref_obj_pid>965143</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Potel, M.J., "Interacting with the Galatea Film Analysis System," Computer Graphics, Vol. 10, No. 2, July 1976, pp. 52-59.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Potel, M.J. and MacKay, S.A., "Preaggregate Cell Motion in Dictostelium", Journal of Cell Science, Volume 36, 1979, pp. 281-309.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Potel, M.J., Sayre, R.E., and Robertson, A., "A System for Interactive Film Analysis," Computers in Biology and Medicine, Volume 9, 1979, pp. 237-256.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Potel, M.J. and MacKay, S.A., "Graphics Input Tools for Interactive Motion Analysis," Computer Graphics and Image Processing, Volume 14, 1980, pp. 1-23.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Potel, M.J., "An Inexpensive Video-based Graphics System for Interactive Motion Analysis", in preparation, 1981.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Reeves, W.T., "A Device-Independent General-Purpose Graphics System in a Minicomputer Time-Sharing Environment", Technical Report CSRG-93, Computer Systems Research Group, University of Toronto, August 1978.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>909883</ref_obj_id>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Reeves, W.T., "Quantitative Representations of Complex Dynamic Shape for Motion Analysis," Ph.D. Thesis, Dept. of Computer Science, University of Toronto, September 1980.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806814</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Reeves, W.T., "Inbetweening for Computer Animation Utilizing Moving Point Constraints", appears in this issue.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Tsotsos, J., Baecker, R.M., Mylopoulos, J., Reeves, W., Covvey, H.D., and Wigle, E.D., "An Interactive Knowledge-Based Approach to Ventricular Image Analysis," Proceedings of the International Conference on Computers in Cardiology, Rotterdam, The Netherlands, Sept. 29 - Oct. 1, 1977.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Tsotsos, J.K., "A Framework for Visual Motion Understanding", Technical Report CSRG-114, Computer Systems Research Group, University of Toronto, June 1980.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Winter, D.A., Greenlaw, R.K., and Hobson, D.A., "Television-Computer Analysis of Kinematics of Human Gait," Computers and Biomedical Research, Volume 5, 1972, pp. 498-504.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Woltring, H.J., "New Possibilities for Human Motion Studies by Real-Time Light Spot Position Measurement", Biotelemetry, Volume 1, 1974, pp. 132-146.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 TOWARDS A LABORATORY INSTRUMENT FOR MOTION ANALYSIS 
Ronald Baecker(*), David Miller(**), and William Reeves(***) Dynamic Graphics Project Department of Computer 
Science and Computer Systems Research Group University of Toronto Toronto, Ontario, Canada Abstract 
Motion analysis is the systematic and usually quantitative study of the movements of humans, animals, 
organisms, cells, or other entities as recorded on movie film or video tape. Despite the utility of computer-aided 
motion analysis to many biological, social, and physical sciences, its role has been limited because 
it is so time-consuming and so expensive. Automated techniques can only be used.on real images in very 
special cases; interactive techniques have involved laborious frame by frame operations. In recent years, 
Futrelle and Potel have revolutionized the process of interactive motion analysis by demonstrating how 
to digitize entire motions with a single sketch rather than with a sequence of operations on each constituent 
frame. However, their GALA- TEA system is applicable only to film and not to video tape records, and 
consists of equipment which cannot easily be engineered into a reliable and mass-producible laboratory 
instrument. The paper describes a prototype laboratory instrument for the interactive motion analysis 
of video tape records. The pro- totype includes a host PDP 11/45 computer, an experimental display processor 
called SPIWRIT, and a microprocessor-controlled video disk. SPIWRIT generates a computer-animated video 
representation of the phenomenon being analysed, and superim-poses it on the actual video record being 
streamed from the video disk. SPIWRIT's bit-slice microprogrammable display processor produces true animation 
on a colour raster display by decoding seg- mented display file images into alternate halves of a double 
frame buffer. Key words and phrases: Motion analysis, computer graphics, com- puter animation, raster 
graphics, display processor, video disk. CR categories: 3.10, 3.30, 3.89, 6.22, 6.35, 8.2. Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by permission of the Association for Computing 
Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. * Also with 
Human Computing Resources Corporation, Toronto. ** Currently with Micom Co., Montreal. *** Currently 
with Sprocket Systems, Lucasfilm, San Anselmo, Cal- ifornia. ....................................................................... 
 I. WHAT IS MOTION ANALYSIS.'? Motion analysis is the systematic and usually quantitative study of the 
movements of humans, animals, organisms, cells, or other entities as recorded on movie film or video 
tape. Motion analysis through real-time or time-lapse photography is a central tool in numerous fields 
including anatomy, behavioral psychology, biomechanics, cardiology, cell biology, developmental psychology, 
fluid mechanics, industrial psychology, kinesiology, meteorology, physical and health education, physiology, 
radiology, the study of group interactions and processes, and transportation planning and traffic engineering. 
For example, specialists in biomechanics, kinesiology, and psychiatry use recorded movements in studying 
the gait of indi- viduals suffering from a variety of physical and mental disabilities (Winter 1972; 
Woltring 1974). At the opposite extreme, motion analysis is also used to assist professional athletes 
in improving their performance (Donkoh 1980). Transportation engineers analyze pedestrian and vehicular 
flow to enhance the design of cor- ridors within buildings and throughout cities (Hartley 1978a,b). Cell 
biologists study the motion of individual and large groups of cells in slime molds as observed through 
photomicroscopy (Potel 1979a). A final example is cardiology, where research and clinical scientists 
analyze the motions of the left ventricle of the heart, as recorded using a variety of cineangiographic 
(imaging with X-rays), echocardiographic (imaging with ultrasound), and scintigraphic (imaging with radioactive 
tracers) methods (Aisen 1980; Baecker 1979a; Tsotsos 1977). Yet the role of motion analysis has been 
limited by the costly and time-consuming procedures required for frame by frame measurement and computation. 
01981 ACM O-8971-045-1/81-0800-0191 $00.75  Computer Graphics Volume 15, Number 3 August 1981 The earliest 
efforts in the motion analysis of film records proceeded in the following way: The film is projected 
onto a large sheet of paper, preferably with a stop-action, variable speed projec- tor. The paper may 
be placed horizontally or taped to a wall. The analyst goes through the film frame by frame and marks 
positions on the paper, thereby "tracing" significant features which are to be measured. Measurements 
are then made using a variety of instru- ments including ruled graph paper, rulers, protractors, stopwatches, 
and hand-held counters. These must then be entered into a calcu- lator or computer for analysis. Analysis 
typically consists of such computations as distances, velocities, directions, areas, and dura- tions, 
and their means, standard deviations, and distributions. More recently, electronic digitizing pens interfaced 
to a computer have been used to speed and improve the accuracy of the process. The best technique is 
to project the film directly onto the digitizing surface or onto a computer display screen. However the 
speed and scope of the process is still limited by the fact that analysis is carried out on a frame by 
frame basis. II. RECENT ADVANCES IN THE TECHNOLOGY OF MOTION ANALYSIS I Dedicated L Mini~ I CRT projecting 
+k(~z-~ real-time computer- / --~¢ ~ animated image ? with source ~" // film for x / / analysis Overlaid, 
synchronized images on screen x, y data from [~------~ tablet cursor held by user  Figure 1 - Schematic 
Diagram of the GALATEA System The technology described thus far only represents a minor advance in the 
methodology of motion analysis. In 1973, R.P. Futrelle, then at the University of Chicago, proposed a 
major advance through an innovative application of the technology of interactive computer graphics (Futrelle 
1973). Futrelle observed that one could obtain both a quantitative and a qualitative increase in the 
efficacy of motion analysis if one could digitize entire motions with a single sketch rather than only 
recording the instantaneous positions of an object carrying out a motion. This was done using Baecker's 
p-curve, a technique for sketching an entire motion into a computer with a single action (Baecker 1969a,b). 
Futrelle then proposed the design of a system which would allow a researcher to digitize a motion sequence 
during the playback of the film under analysis. Futrelle's technique had another somewhat subtler advantage. 
There are many dynamic processes in nature in which the motion is only visible when it is in process; 
it leaves no visible trace in an individual static frame. The analysis of cell motion in slime molds 
is one good example. A single frame shows only a meaningless cloud of points, although the movement of 
individual cells is clearly visible during the movie playback. Thus Futrelle's technique has enabled 
the analysis of entire classes of motion sequences that were never before susceptible to scientific study. 
Futrelle, his student M.J. Potel, and others in their group then constructed a system callled GALATEA 
for the dynamic analysis of motion picture sequences (Futrelle 1974, 1975). A schematic diagram of their 
apparatus appears as Figure 1. A minicomputer is interfaced to a stop-motion projector, a digitiz- ing 
pen, and an interactive graphics display terminal. The researcher tracks items of interest with the pen. 
A symbol representing his current pen position and a computer-synthesized picture representing the results 
of previous sketches is displayed on the graphics screen. Using a projection kinescope and a series of 
mirrors, the computer-synthesized image is superimposed on the motion picture image, and the result is 
projected onto a large front screen or onto the digitizing surface itself. The researcher can extract 
data from the film by sketching while the movie is played forwards or backwards at a convenient speed 
under control of the computer. Data is considered extracted when the computer-synthesized images match 
to the degree desired by the researcher the features of the raw data which appear in the motion picture 
images. It should be noted that the GALATEA approach as well as previous frame by frame methods have 
been semi-automated rather than automated in the sense that a human being views and extracts the motions, 
which may then be later analyzed by machine. Much current work in pattern recognition and artificial 
intelligence is directed towards "motion understanding", totally automatic extraction of motion from 
a sequence of frames fed into a computer (Badler 1979). This is now practical in some very special cases 
under artificial conditions, for example in analyzing the motion of a small number of luminous spots 
placed on a subject's body for gait studies (Winter 1972; Woltring 1974). However, motion under- standing 
is in general only in the research phase (Tsotsos 1980), and is not yet fast enough, cheap enough, and 
accurate enough to be used in practice. As well, there are some motion sequences which are so difficult 
even for a skilled observer to interpret that they are likely never to be susceptible to a totally automated 
analysis. III. OUR APPROACH The GALATEA system, under Potel's direction for the last four years, is under 
considerable demand, being used around the clock by research scientists in a variety of disciplines (Potel 
1979b, 1980). Yet it would be difficult to engineer into a reliable and mass-producible laboratory instrument. 
It would therefore appear preferable to construct a fully electronic system for the analysis of video 
records. Potel himself has also begun such an effort (Potel 1981). Why would one want to construct a 
system for video rather than film analysis? Video has a number of advantages over film. It is cheaper 
in that no laboratory processing is involved and in that one can reuse the same tape over and over again. 
Video is more secure than film in that one can review it immediately in the field and guarantee that 
the data collection process is proceeding without error. Video is also less intrusive than film because 
the camera makes no noise. Of greatest importance, however, is the fact that video is an essentially 
electronic technology, whereas film is a mechanical technology. It is easier to superimpose and register 
a video image Computer Graphics Volume 15, Number 3 August 1981 with a computer graphics image than 
it is to do this with a film image and a computer graphics image. It is possible to assist the person 
doing the analysis by adjusting the brightness, contrast, and colour controls of the video monitor. Most 
important is that video lends itself more easily to the creation of a cost-effective and mass- producible 
laboratory instrument. Another area in which GALATEA can be extended is in the variety of motions which 
can be analyzed efficiently. It is better suited to analyzing cell movements and pedestrian flows than 
to studying the beating heart or evolving cloud formations. One can follow moving points dynamically 
with the digitizing pen, but there is no comparable technique for following a complex dynamic shape such 
as a beating heart. Thus one would be forced to sketch the outline of the heart in numerous key frames 
and apply the in-betweening techniques of key frame animation (for example, Burt- nyk 1971). Although 
leading to faster data collection than tracing each individual frame, these techniques leave a lot to 
be desired in terms of speed, accuracy, and flexibility. PDP 11/45 I ~]Minic°mputer f Hicroprocessor 
 680~ I Video out Video mixer ~ Video out ii SPIWRIT I Display :Processorl Video Disc ~ TV monitor with 
source video for I 00~0 1 displaying real- time computer analysis images super- imposed with source video 
 We shall describe in the remainder of this paper the approach we have taken towards the design of a 
laboratory instru- ment for the motion analysis of video tape records. The central component of our prototype 
system, SPIWRIT (Miller 1980,1981), is a novel digital video display system capable of the real-time 
ani- mation of images of moderate complexity such as heart borders. The motion sequences output by SPIWRIT 
can be superimposed on standard television signals coming from a video tape playback unit or a video 
disk. Our prototype uses an Ampex analogue video disk capable of storing 500 frames. With this instrument, 
we are able to extract heart borders using the methodology pioneered in GALA- TEA: we construct a dynamic 
sketch, or kinegram, representing the heart wall, and display it with synthesized computer graphics on 
SPIWRIT; we superimpose it on the analogue video record coming from the video disk; and, we refine the 
sketch until it accurately matches the recorded dynamics of the heart wall. To facilitate the analysis 
of motions like beating hearts and evolving cloud formations, we must represent complex dynamic shapes 
by descriptions that are more compact and more powerful than enumerations of points on the border of 
each frame of the sequence. Towards this end, we have formulated an approach to representing complex 
dynamic shapes in a way that facilitates the process of motion analysis (Reeves 1980). The representations 
and algorithms which are the heart of this approach are described in a companion paper (Reeves 1981) 
in these proceedings. The next section of this paper is an overview of the pro- totype motion analysis 
system of which SPIWRIT and the video disk are a part. The following section describes the SPIWRIT display 
processor in considerable detail. Finally, we draw some conclusions from our work and propose some tasks 
for future research and development. IV. A PROTOTYPE SYSTEM FOR VIDEO MOTION ANALYSIS The hardware configuration 
for our prototype system is shown in Figure 2. (The configuration and the choice of some of the components 
are somewhat ad hoc, but are the result of financial necessity. We have spent only $2500 U.S. on new 
hardware and electronic components for this development. Ideal hardware com-ponents are described in 
section VI of the paper.) x, y data from~------~ tablet cursor II C~I~ held by user II -~ Figure 2 - 
Schematic Diagram of the Prototype System The central processor is a PDP-11/45 with a full comple- ment 
of memory and several large disk storage modules. Almost all of the motion analysis software is resident 
on this machine. The MC6800 processor is used as a slave to the 11/45. It functions as an intelligent 
controller to the video disk, which stores in analogue form the raw video images of the entity under 
study. Computer synthesized images from the 11/45 are produced by the SPIWRIT g~phics display processor, 
described in more detail in section V. The images from the video disk and SPIWRIT are fed into a simple 
video mixing unit which superimposes the SPIWRIT images on the raw video images. These images are then 
displayed for the user to see on a standard colour television display screen. The digitizing tablet is 
used by the interactive user to give commands to the sys- tem and to make sketches. Basic system software 
has been written for all of the hardware components under the UNIX operating system (Bell 1978). The 
SPIWRIT display is supported by a general-purpose device-independent graphics package called GPAC (Reeves 
1978). GPAC also supports a generalized event-driven input mechanism and sketching software for the digitizing 
tablet. The MC6800 video disk controller sub-system allows the 11/45 to instruct the con-troller to play 
back asynchronously a sequence of frames at a given rate. The controller returns a message to the 11/45 
when it has finished. The trickiest part of the system software is ensuring that the motion analysis 
system can run in real-time without severely impacting other timesharing users of UNIX. The applications 
software of the prototype system is under construction and will be similar conceptually to the motion 
analysis software of GALATEA (Potel 1976, 1980). The interac- tive user can scan through the raw images 
on the video disk. He gradually constructs a kinegram which is superimposed on top of the raw images 
until the two match closely enough, at which point the kinegram is the desired representation of the 
motion in the video record. In constructing a complex dynamic shape description, such as a representation 
of the motion of the wall of the left ventri- cle of the heart, the user will proceed in two distinct 
phases. In phase one, the user stops at particular frames of the sequence and traces the static shapes 
of the entity" being studied at those instants  Computer Graphics Volume 15, Number 3 August 1981 in 
time. He also sets the video disk cycling over a subrange of frames and traces the dynamics of certain 
key points on the shape as they move through time. The set of static shapes and the set of moving points 
that the user inputs are said to specify the dynamic shape. In phase two, the computer takes this specification 
and generates from it the complete dynamic shape by treating it as a set of constraints on the form and 
motion of the dynamic shape and by intei'polating from them the shape of the entity at all missing times. 
The static shapes and moving points of phase one are used to inform phase two of any significant discontinuities 
in either the shape or the motion of the dynamic shape. The computer interpo- lates all intervening motions 
and transformations from the con-straints in a smooth and continuous fashion. The algorithms used to 
interpolate dynamic shape specifications are presented in (Reeves 1981) and described in detail in (Reeves 
1980). Because the motion analysis system superimposes the interpolated result on the original motion 
in the raw video record, it is easy to see if the analysis is "good enough". If not, then the static 
shapes and moving points must be altered or augmented and a new dynamic shape recomputed. To test the 
efficacy of our prototype system and of the dynamic shape interpolation algorithms, we will conduct the 
follow- ing experiment. An operator uses the system to construct dynamic shape representations for a 
number of cardiovascular case studies. We measure the amount of real time he spends in entering static 
shapes and moving points, and in using the system to refine the resulting dynamic shape. As a control, 
the operator also creates dynamic shape representations of those same cases using the old technique of 
sketching a static shape for every frame in the raw data sequence. Preliminary experiments indicate that 
the new approach will be between 5 and 20 times faster than the old approach. V. THE SPIWRIT DISPLAY 
PROCESSOR A system for video motion analysis must incorporate a dynamic graphics device capable of real-time 
video animation and of electronically superimposing its display with that of a video signal recorded 
on a medium such as video disk or video tape. A system based on such a device should be cheaper and more 
reliable than the aforementioned GALATEA. A recent survey (Baecker 1979b) has indicated the paucity of 
these inexpensive, flexible dynamic digital video display systems, although a more recent paper describes 
a device similar in Concept to SPIWRIT developed con-currently and independently (Ackland 1980). To meet 
the above goals, our device has to accentuate motion quality rather than display quality and still retain 
an inherent flexibility. By flexibility we mean that it can be easily altered to display different classes 
of objects. This can be achieved by designing our system with a microprocessor whose microcode may be 
altered to suit the geometrical complexities of the sequence of pictures being shown. The microcode would 
execute the scan conversion algorithms of the various graphical primitives. With good motion quality, 
the display system could con- duct images at the highest video rate possible --30 frames per second. 
Alternately, onemight want to synchronise the display sys- tem to whatever analogue video playback device 
is available. These units have various normal and slow motion speeds. A user would also desire different 
digital graphics display speeds to achieve his own motion effects. Hence, a variable frame rate mechanism 
is required. These considerations resulted in a display processor called SPIWRIT, portrayed in Figures 
3 and 4. SPIWRIT is an intelligent graphics display device, with a slave relationship to a host PDP-11/45 
running the UNIX operating system and the GPAC display system. SPIWR1T employs bit-slice technology as 
part of a microcoded display processor. At the heart of this processor are the widely used 2901 slices; 
four of them are used to define the 16-bit data width of the microprocessor. The processor decodes seg-mented 
display file images from a 16K-word RAM memory resident in SPIWRIT. A segment is a collection of display 
file graphics primitives (e.g., vectors) that can be manipulated as a single, sub-picture unit. The images 
are scan converted into alternate halves of a double 256 (wide) × 240 (high) pixel frame buffer. Each 
frame buffer stores 4 bits per pixel, allowing 16 possible colours. SPIWRIT is used as follows: The host 
loads microcode for scan conversion into the microprogramme memory, and a seg- mented encoded display 
file into the display file memory. The host then commands the microprocessor to start scan converting 
the encoded display file into single pixels. These pixels are written into one of the two frame buffers. 
Except for the microcode loading, the process is repeated the next frame, only now the frame buffers 
have been switched. As one frame buffer is at the service of the microprocessor, the other is refreshing 
the display. Furthermore, the microprocessor is never aware that two frame buffers exist. Thus image 
expansion (decoding) and raster scan refresh processing (from frame buffer to display) are separate and 
asynchronous. The host need not load a new display file every frame, since it may only require slight 
alterations or additions. Because the display file is loaded and modified by the host, and is decoded 
by the microprocessor in every user frame (the length of time the frame buffers remain switched in one 
posi- tion), it is desireable to use a segmented encoded display file. By segmenting the display file, 
the host can easily alter the displayed picture. By encoding the points to be drawn on the display in 
terms of the segments' graphics primitives, the display file can be made quite small in relation to the 
number of pixels drawn. This is very efficient for both the host system and the display processor. The 
host need not transport large amounts of data across to the microprocessor in order to fill the requirements 
for each frame, thus speeding the transfer of new images to the display; and the display processor does 
not require a large memory size to store a display file. Some very efficient graphical plotting routines 
(Bresen- ham 1965, 1977) exist that were originally intended for incremental displays. These algorithms 
do not require any multiplication or division, only addition (subtraction) or shifting. Incremental display 
devices can linearly move from a current point to one of eight possible adjacent points in response to 
an appropriate pulse. SPIWRIT's microprocessor frame buffer addressing is manipulated to appear as if 
the frame buffer is an incrementally addressed dev- ice. Two 16-bit address pointers, the X and Y address 
counters (XAC and YAC in Figure 4), can be loaded, incremented or decremented, allowing the microprocessor 
to randomly write to any location of the frame buffer. Once these pointers are loaded with the address 
of a screen coordinate, it is the four-bit VECNTL (VEctor CoNTroL) register that dictates to the X and 
Y address counters whether to increment, decrement, or not to step at all every microprocessor cycle. 
Thus we step along the two-dimensional grid of the screen in one of eight directions. The VECNTL can 
be controlled directly from microcode or indirectly from the display file each microcycle. With incremental 
addressing at its disposal, the microprocessor can employ highly efficient graphics algorithms. Computer 
Graphics Volume 15, Number 3 August 1981 11 0 S T J HOST- B U S iINTERFACE~ I " .. I I--- I I FTLE I 
Figure 3 -SPIWRIT System Configuration ,.I"~,,..._~'~~'~, ,,~"~ ~o~ ~s,~ora CONTROL -STORE MEMORY (CS~E~I) 
I .1. i ! ! o ! I ! ii ! i E A ,! ~ ;>iI i 'lO /i d I0 ,[/ir /6t , /~0 *{FIUX ! ......... , . . . . . 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ~ i .i -i ii .i 4:1 i !G SH~,I FT ' "lO [ 
l~I I I to flags &#38; regs -LL_ m.. -= " ~ ~oI I i I i ~ LIP ~ MUX qI ~8.,~ ~',I"~ i i ! i F,L , I O, 
EG i I Ii OPERAND ~ MUX #.~6 ! #i i ~6 I I XAC l I 6 .~ t--3 ~ ~ ~ t)RIVE I ' : ,16 ! i i ,Ii i i i : 
t ' left op right op ALU !. ] rb,,~ , 8 """~ , ~ " ' SPD ~ BKRD i TRANi / ~ /'~ /t, i' i i MAC , ! i 
Apass ,i ALUpass ! Y OUT MUX i 2 9 ~ 1 L p............ w ............................... d i~ .L6/., 
I , h i -~COL i . . * Figure 4 - SPIWRIT Display Microprocessor Architecture Computer Graphics Volume 
15, Number 3 August 1981 Efficiency is gained both in speed and in storage space. Since most of the loading 
of external registers must utilise the ALU in passing data to these registers, it is advantageous not 
to have to load the X and/or Y address for each pixel "plotted". This frees the microprocessor's ALU 
for critical updating and testing of other important parameters associated with the drawing routines. 
Specify-ing graphics types --descriptions residing in display file memory --now takes less space, since 
the ease of decoding allows a wider range of graphics primitives. For example, drawing lines, rectan- 
gles, filled rectangles, and circles can efficiently be left to the microprocessor. A detailed discussion 
of the performance of SPIWRIT and other similar devices appears in (Galloway 1979; Miller 1980, 1981). 
For example, with a particular microcoded display file for- mat in which SPIWRIT uses two 16 bit words 
per vector, it is capa- ble of decoding almost 2000 vectors that are 16 pixels long in a 30th of a second. 
It is the use of the double frame buffer with the fast bit-slice processor that produces the real-time 
animation. The mechanism that controls frame buffer switching can be directed "manually" from the host 
or "semi-automatically" from the microprocessor. The host controls frame buffer switching directly by 
first freezing the buffers with one frame buffer refreshing the display and the other at the disposal 
of the display processor. Then, by pulsing a frame advance bit, the frame buffers will change over. Such 
asynchronous control from the host is useful when running in a "slide projector" mode. Semi-automatic 
sequencing is useful when the host and the microprocessor require only an indirect influence on the swap- 
ping action. An eight bit speed count register is loaded by the microprocessor" at the start of its display 
file decoding cycle. Every vertical retrace time (i.e., every television raster field), this counter 
is automatically decremented as long as it is non-zero. After the count reaches zero, the buffers are 
swapped. In this way, user frame speeds of from 60 frames per second to one every 4 and 1/4 seconds can 
be set in fine gradations of [o second (1 raster field). The display could, at this juncture, remain 
frozen (speed counter equal to zero) until the host gives the microprocessor the signal to start decoding. 
If, in semi-automatic frame stepping, the microprocessor is unable to complete its scan conversion of 
the display file before the frame buffers switch over, then two possible degradation detec- tion means 
are at our service. First, an interrupt to the host may be sent directly from the degradation sense logic. 
Second, the microprocessor itself is capable of polling for this degradation, and then sending its own 
interrupt to the host. The host can then react by either decreasing the complexity of the display file's 
image, or by increasing the microprocessor's decoding time by loading a higher value into the speed counter 
at the start of each user frame. The 4-bit pixel data read out of one of the frame bulTers to refresh 
the video display is compared to the 4-bit pixel value of a transparency colour register. If they are 
equal, then the analogue video switch gates in an external video signal. If the pixel's colour does not 
coincide with the transparency colour, then the generated NTSC video signal of the pixel colour is displayed 
instead. This is the superimposition mechanism that allows us to mix graphics and video tape or video 
disk pictures on the display. VI. SUMMARY AND CONCLUSIONS We have described in this and the companion 
paper (Reeves 1981) advances both in the technology and in the experi- mental methodology of motion analysis. 
Our work is a step towards our ultimate goal of developing an inexpensive reliable laboratory instrument 
for motion analysis which could enhance significantly the research capabilities of investigators in the 
biological, social, and physical sciences. This will necessitate a re-engineering of all the hardware 
and software components of the system in the light of experience gained with the prototype. There are 
two major items that will need to be changed in order to produce a cost-effective laboratory instrument. 
The PDP 11/45 must be replaced with a modern microprocessor such as an 11/23. The expensive analogue 
video disk must be replaced with a modern video disk such as the new laser-based video disk, which already 
sells for only a few thousand dollars. Such a device will have the further advantages of storing two 
orders of magnitude more frames, and of being random access. Its only disadvantage is the current difficulty 
and cost of writing the disk. Nonetheless, for most applications, even if one must ship off some tapes 
in the mail and wait to receive a disk with the data recorded on it, 50,000 frames will provide enough 
data for months of analysis. There are also still a number of questions that must be answered, among 
them: Is the resolution of our device adequate for motion analysis? Is the contrast of typical video 
sequences ade- quate? Is the design of SPIWRIT adequate to the task? What are the human factors issues 
in the design of an appropriate user inter- face? In terms of our application to complex dynamic shape, 
we need to complete the experiment described above. We also need to investigate how to handle sequences 
that are so fuzzy or noisy that there is considerable "missing data" in the static shapes and moving 
points we wish to select from the original video record. Can we find methods by which we can infer or 
extrapolate from the existing data this missing data? Some preliminary results are presented in (Reeves 
1980). There are also a number of software subsystems still required for the system to be truly useful 
as a motion analysis tool: 1) A data base system for motion analysis data. Research is needed here to 
develop techniques whereby the structure of the data base can vary from application to application. A 
user should be able to change the logical organization of the data as he gathers more and more of it. 
2) Basic statistical and analysis software. Various plotting and data display utilities should be developed 
so researchers can visualize the results of their analyses. 3) A computerized modelling system for a 
class of dynamic objects with which to verify empirically hypotheses formulated from an analysis. An 
appropriate computer-based data extraction and analysis environment should facilitate rapid hypothesis 
formation and testing. Some past work in computer-based dynamic simulation modelling is decribed in (Baecker 
1975, 1977, and 1979c). VII. ACKNOWLEDGEMENTS We are indebted to Dominic Covvey, Guy Fedorkow, Dave Galloway, 
Irene Leszkowicz, Richard Sniderman, and Zvonko Vranesic for their significant contributions to this 
work, and to IBM Computer Graphics Canada for their generous donation of the video disk. SPIWRIT was 
designed and built at the University of Toronto's Electrical Engineering Computer Group laboratory and 
is now operational at the University of Toronto's Computer Systems Research Group laboratory. Financial 
support was provided by the Natural Sciences and Engineering Research Council of Canada and by IBM Canada 
under a grant to the University of Toronto Update Fund. VIII. REFERENCES Ackland, B. and Weste, N., "Real-time 
Animation Playback on a Frame Store Display System", Computer Graphics, Vol. 14, No. 3, July 1980, pp. 
182-188. Aisen, A.M., AI-Sadir, J., MacKay, S.A., Potel, M.J., Rubin, J.M., and Sayre, R.E., "Quantitative 
Ventricular Wall Motion Analysis of Biplane Coronary Angiograms", Proceedings of the IEEE Conference 
on Computers in Cardiology, Williamsburg, Virgina, Oct. 1980. Badler, N. and Aggarwal, J. (Editors), 
Proceedings of the IEEE Workshop on Computer Analysis of Time- Varying Imagery, University of Pennsylvania, 
Philadelphia, April 1979. Baecker, R.M., "Interactive Computer-Mediated Animation," Ph.D. Thesis, Department 
of Electrical Engineering, M.1.T., April 1969, reprinted as Project MAC TR-61. Baecker, R.M., "Picture-Driven 
Animation," Proceedings of the Spring Joint Computer Conference, May 1969, pp. 273-288. Baecker, R.M. 
and Horsley, T., "Computer-Animated Simulation Models: A Tool for Transportation Planning," Transportation 
Research Record 557, Transportation Research Board, National Research Council, Washington D.C., 1975, 
pp. 33-44. Baecker, R.M., Hauer, E., and Bunt, P.D., "Computer Animated Simulation of Taxi Dispatching 
Strategies," Transportation Research Record 657, Transportation Research Board, National Research Council, 
Washington D.C., 1977, pp. 14-19. Baecker, R.M., Reeves, W., Covvey, H.D., Miller, D., and Gallo- way, 
D., "Interactive Computer-Aided Analysis of Cardiac Motion Sequences," appears in (Badler 1979). Baecker, 
R.M., "Digital Video Display Systems and Dynamic Graphics", Computer Graphics, Vol. 13, No. 2, August 
1979, pp. 48-56. Baecker, R.M. and Duff, T.D.S., "A Programming Environment for Conversational Dynamic 
Modelling", Proceedings of the 1979 sum- mer Computer Simulation Conference, July 1979, pp. 32-39. Bell 
System, "UNIX Time-Sharing System" (entire issue devoted to UNIX), The Bell System Technical Journal, 
Vol. 57, No. 6, Part 2, July-August 1978. Bresenham, J.E., "Algorithm for Computer Control of a Digital 
Plotter", 1BMSystems Journal, Vol. 4, No. 1, January 1965. Bresenham, J.E., "A Linear Algorithms for 
Incremental Digital Display of Circular Arcs", Communications of the ACM, Vol. 20, No. 2, February 1977. 
Burtnyk, N. and Wein, M., "Computer Generated Key Frame Ani- mation", Journal of the Society of Motion 
Picture and Television Engineers, Vol. 80, March 1971, pp. 149-153. Donkoh, S., "Computer Analysis Helps 
Train Athletes", C1PS Review, July/August 1980, pp. 14-15. Futrelle, R.P., "GALATEA, a Proposed System 
for Computer-aided Analysis of Movie Films and Videotape," Quarterly Report No. 37, Institute of Computer 
Research, The University of Chicago, May 1, 1973. Futrelle, R.P., "GALATEA: Interactive Graphics for 
the Analysis of Moving Images," IroCormation Processing 74, Proceedings of the  Volume 15, Number 3 
August 1981 1974 1FIP Conference, North-Holland Publishing Company, pp. 712-716. Futrelle, R.P. and Potel, 
M.J., "The System Design for GALATEA, An Interactive Real-time Computer Graphics System for Movie and 
Video Analysis," Computers and Graphics, Vol. 1, pp. 115-121, Per- gamon Press, 1975. Galloway, D.R., 
"The Modelling of Dynamic Digital Video Display Systems", M.Sc. Thesis, Department of Computer Science, 
Univer- sity of Toronto, 1979~ Hartley, M.G., Waterfall, R.C., and Fisher, A.J., "Preliminary Stu- dies 
into the CAD of Pedestrian Circulation Areas", Universities Transport Study Group, Ninth Annual Conference, 
City University, London, Jan. 1978, pp. 693-700. Hartley, M., "Designing Buildings for People to Move 
in", New Scientist, 10 August 1978. Miller, D.H., "A Two-Dimensional Dynamic Video Display Sys- tem", 
M.A.Sc. Thesis, Dept. of Electrical Engineering, University of Toronto, September 1980. Miller, D.H., 
Vranesic, Z.G., and Baecker, R.M., "SPIWRIT --A Simple Real-Time Digital Video Display System", in preparation, 
1981. Potel, M.J., "Interacting with the Galatea Film Analysis System," Computer Graphics, Vol. 10, No. 
2, July 1976, pp. 52-59. Potel, M.J. and MacKay, S.A., "Preaggregate Cell Motion in Dictos- telium", 
Journal of Cell Science, Volume 36, 1979, pp. 281-309. Potel, M.J., Sayre, R.E., and Robertson, A., "A 
System for Interac- tive Film Analysis," Computers in Biology and Medicine, Volume 9~ 1979, pp. 237-256. 
Potel, M.J. and MacKay, S.A., "Graphics Input Tools for Interac- tive Motion Analysis," Computer Graphics 
and Image Processing, Volume 14, 1980, pp. 1-23. Potel, M.J., "An Inexpensive Video-based Graphics System 
for Interactive Motion Analysis", in preparation, 1981. Reeves, W.T., "A Device-Independent General-Purpose 
Graphics System in a Minicomputer Time-Sharing Environment", Technical Report CSRG-93, Computer Systems 
Research Group, University of Toronto, August 1978. Reeves, W.T., "Quantitative Representations of Complex 
Dynamic Shape for Motion Analysis," Ph.D. Thesis, Dept. of Computer Sci- ence, University of Toronto, 
September 1980. Reeves, W.T., "lnbetweening for Computer Animation Utilizing Moving Point Constraints", 
appears in this issue. Tsotsos, J., Baecker, R.M., Mylopoulos, J., Reeves, W., Covvey, H.D., and Wigle, 
E.D., "An Interactive Knowledge-Based Approach to Ventricular Image Analysis," Proceedings of the International 
Conference on Computers in Cardiology, Rotterdam, The Nether-lands, Sept. 29 - Oct. 1, 1977. Tsotsos, 
J.K., "A Framework for Visual Motion Understanding", Technical Report CSRG-I14, Computer Systems Research 
Group, University of Toronto, June 1980. Winter, D.A., Greenlaw, R.K., and Hobson, D.A,, "Television-Computer 
Analysis of Kinematics of Human Gait," Computers and Biomedical Research, Volume 5, 1972, pp. 498-504. 
Woltring, H.J., "New Possibilities for Human Motion Studies by Real-Time Light Spot Position Measurement", 
Biotelemetry, Volume 1, 1974, pp. 132-146. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806807</article_id>
		<sort_key>199</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>25</seq_no>
		<title><![CDATA[DATAPLOT&#8212;an interactive high-level language for graphics, non-linear fitting, data analysis, and mathematics]]></title>
		<page_from>199</page_from>
		<page_to>213</page_to>
		<doi_number>10.1145/800224.806807</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806807</url>
		<abstract>
			<par><![CDATA[<p> <italic>This paper describes the design philosphy and features of DATAPLOT&#8212;a high-level (free-format English-like syntax) language for:</italic> </p> <p> <italic>1) graphics (continuous or discrete);</italic> </p> <p> <italic>2) fitting (linear or non-linear);</italic> </p> <p> <italic>3) general data analysis;</italic> </p> <p> <italic>4) mathematics.</italic> </p> <p> <italic>DATAPLOT was developed originally in 1977 in response to data analysis problems encountered at the National Bureau of Standards. It has subsequently been the most heavily-used interactive graphics and non-linear fitting language at NBS. It is a valuable tool not only for &#8220;raw&#8221; graphics, but also for manuscript preparation, modeling, data analysis, data summarization, and mathematical analysis. DATAPLOT may be run either in batch mode or interactively, although it was primarily designed for (and is most effectively used in) an interactive environmnet. DATAPLOT graphics may appear on many different types of output devices. Due to its modular design and underlying ANSI FORTRAN (PFORT) code, DATAPLOT is portable to a wide variety of computers.</italic> </p> <p> <italic>The paper is divided into three general parts: part 1 deals with background motivation and design philosophy; part 2 deals with capability and implementation features; part 3 deals with a comparison of DATAPLOT to other systems/languages.</italic> </p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Data analysis]]></kw>
			<kw><![CDATA[Diagrams]]></kw>
			<kw><![CDATA[Fitting]]></kw>
			<kw><![CDATA[Graphics languages]]></kw>
			<kw><![CDATA[High-level languages]]></kw>
			<kw><![CDATA[Interactive computing]]></kw>
			<kw><![CDATA[Mathematical modeling]]></kw>
			<kw><![CDATA[Mathematics]]></kw>
			<kw><![CDATA[Modeling]]></kw>
			<kw><![CDATA[Portability]]></kw>
			<kw><![CDATA[Scehematics]]></kw>
			<kw><![CDATA[Software]]></kw>
			<kw><![CDATA[Statistics]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>D.3.2</cat_node>
				<descriptor>Very high-level languages</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011009.10011022</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language types->Very high level languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Languages</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P132204</person_id>
				<author_profile_id><![CDATA[81100084098]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Filliben]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Caporal, P. M. and Hahn, G. J. (1981). An Overview of Tools for Automated Statistical Graphics. General Electric Corporate Research and Development Technical Information Series Report No. 81CRD024, Schenectady, New York.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>540110</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Coswell, W. (1977). Portability of Numerical Software. Lecture Notes in Computer Science No. 57 Springer-Verlag, New York.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Filliben, James J. (1978). DATAPLOT&#8212;An Interactive System for Graphics, Fortran Function Evaluation, and Linear/Non-linear Fitting. Proceedings of the Statistical Computing Section of the American Statistical Association 1977.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Filliben, James J. (1979). Factors Affecting the Use of Statistical Graphical Software. Proceedings of the Twelfth Interface Symposium on Computer Science and Statistics. Toronto.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Filliben, James J. (1979). New Features in DATAPLOT&#8212;A Language for Graphics, Non-linear Fitting, Data Analysis, and Mathematics. Proceedings of the Statistical Computing Section of the American Statistical Association: 1977.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Filliben, James J. (1980). A Review of DATAPLOT&#8212;An Interactive High-Level Language for Graphics, Non-linear Fitting, Data Analysis, and Mathematics. Proceedings of the Statistical Computing Section of the American Statistical Association: 1980.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2921</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Francis, I. (1981). Comparative Review of Statistical Software (Edition 2). International Association for Statistical Computing. 358 Ives Hall, Cornell University, Ithaca, New York.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Knott, Gary D. (1979). MLAB&#8212;A Mathematical Modeling Tool. Computer Programs in Biomedicine, pages 271-280. Elsevier/North Holland Biomedical Press.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 DATAPLOT--AN INTERACTIVE HIGH-LEVEL LANGUAGE FOR GRAPHICS~ 
WON-LINEAR FITTINGj DATA ANALYSISy AND R~THE~TICS James J. Filliben National Bureau of Standards  ABSTRACT 
 This paper describes the design philosphy and features of DATAPLOT--a high-level (free-format English-like 
syntax) language for: i) graphics (continuous or discrete); 2) fitting {linear or non-linear); 3) general 
data analysis; 4) mathematics. DATAPLOT was developed originally in 1977 in response to data analysis 
problems encountered at the National Bureau of Standards. It has subsequently been the most heavily-used 
interactive graphics and non-linear fitting language at NBS. It is a valuable tool not only for "raw" 
graphics, but also for manuscript preparation, modeling, data analysis, data summarization, and mathematical 
analysis. DATAPLOT may be run either in batch mode or interactively, although it was primarily designed 
for (and is most effectively used in) an interactive environmnet. DATAPLOT graphics may appear on many 
different types of output devices. Due to its modular design and underlying ANSI FORTRAN (PFORT) code, 
DATAPLOT is portable to a wide variety of computers. The paper is divided into three general parts: 
part 1 deals with background motivation and design philosophy; Part 2 deals with capability and implementation 
features; part 3 deals with a comparison of DATAPLOT to other systems~languages. Keywords: Computer 
Graphics, Graphics Languages, Fitting, Modeling, Statistics, Data Analysis, Mathematics, Mathematical 
Modeling, Scehematics, Diagrams, Software, Portability, High-Level Languages, Interactive Computing. 
 CRCategories: 8.2, 3.1, 3.2, 3.3, 4.0, 4.1, 4.13, 4.2, 4.20, 4.22, 4.6, 5.1, 5.12, 5.15, 5.16, 5.5 
 1. IWTROIYdCTIOW The laboratory scientist in a research environment has become increasingly dependent 
on the computer for carrying out the various facets of his~her investigations. Indeed, over the last 
10 years, there has been an evolution (a revolution!) of computer capabilities with the net result that 
no component in the research process is left untouched by the computer--it is being used to design the 
experiment, to run the experiment, to log the data, to verify, process, analyze, and summarize the data, 
and to prepare the final research manuscript. My personal vantage point in this laboratory computerization" 
has been that of a statistical consultant at the National Bureau of Standards. In my interactions with 
laboratory scientists over the last 12 years, I have been able to observe first hand how the individual 
laboratory scientists are coping with the advent of computers in the laboratory process. Many such scientists 
approach the computer with mixed reactions--being spurred on by the knowledge that there exists the potential 
for enormous time saving, and yet being somehwat hesitant by the reality that the computer demands an 
up-front commitment of time and effort in terms of familiarization with hardware and software. By and 
large, however, the trend is strikingly positive--there being fewer cases of active resistance against 
the intrusion of the computer, and many more cases of active acceptance of the merits of the computer. 
 The majority of "bench scientists, are viewing the computer as a tool (even a "friendly" tool) which 
can be profitably applied at every step of their research effort. It is no longer being applied as a 
last resort; rather, it is being considered as the prime option. In this light, we see laboratory scientists 
asking (even demanding) more and more in the way of capabilities from their local computer operation, 
which in turn is resulting in upgraded requirements for hardware and software. These demands have been 
anticipated by the hardware manufacturers and have been met by the development of faster, more powerful 
maxi/mini computers with larger mass storage capabilities.  Computer Graphics Volume 15, Number 3 August 
1981 More importantly, however, the feature of computing which has had greatest impact on the average 
laboratory scientist has been the widespread introduction of interactive (as opposed to batch) oriented 
systems. This has (via terminals) literally bought the power of the computer into the scientist's laboratory, 
and has scaled down the time frame for computer analyses from days and hours to minutes and seconds. 
This compression of time has had the important practical effect of allowing the analyst the luxury of 
time-continuity in a research analysis--a luxury that so frequently was lost in the overnight turnarounds 
accompaning the batch-oriented systems of a decade ago. Along with interactive-oriented operations, 
a second innovation which is impacting heavily on the scientist is the widespread availability of moderately-priced, 
high-resolution display terminals. These terminals with their enhanced resolution have given the scientist 
the ability to generate graphics output with resolution orders of magnitude better than discrete terminals 
and batch-hardcopy output. High-resolution terminals have allowed the laboratory scientist to "think 
continuous" (as opposed to discrete), and has provided a day-in~day-out graphics tool for research which 
has never before been practically available. The scope of potential graphics activities is limitless--as 
the variety of applica- tions discussed in the computer graphics literature attests to. The graphics 
tool allows the research scientist to visualize process mechanisms, to gain insight into underlying structure, 
to thoroughly test modeling assumptions, to substantiate research conclusions, and to communicate such 
conclusions to the larger scientific community. The remainder of this paper discusses a set of graphics 
activities which has high practival importance because of its universal p~esence in general scientific 
investigations. Further, the paper describes the accompaning development of software aimed at allowing 
the research scientist to take full advantage of the above-described advent of interactively-oriented 
operating systems and high-resolution display terminals. Such complemetary software is directed at permitting 
the laboratory scientist to carry out necessary graphics activities in as easy and effective a fashion 
as possible--thus enhancing the graphics tool in the true sense of the word "tool". From the laboratry 
scientist's point of view, such ease-of-use is so critically important, because it not only allows for 
more insightful and thorough analyses , but also preserves the researcher's time-continuity which is 
so important in effectivly carrying out a scientific investigation. Section 2 develops criteria for 
designing and evaluating graphics software. Section 3 describes a set of activities (graphics and otherwise) 
which are common to most scientific investigations and which in turn define the target set of activities 
for which a software system (DATAPLOT) has been developed. Section 4 decribes DATAPLOT capability features. 
Section 5 gives code and outout examples for a variety of specific graphics activities. Section 6 describes 
DATAPLOT implementation features. Section 7 summarizes DATAPLOT's general features. Section 8 presents 
a comparative review of DATAPLOT. 2. DESIGHING AND EWALUATING GRAPHICAL SOFTNRRE The purpose of this 
section is to discuss various criteria by which graphical software (or any software) may be evaluated. 
Such criteria are important in the formulation of design features for newly-developed software systems9 
and also in the comparison of existing software systems. This section provides background for the command 
considerations and conventions that have been implemented into DATAPLOT. 2.1 ;L, alyst-Sase4 Criterion 
 The term "analyst" is used many times throughout this paper; it typically means a laboratory scientist 
or engineer who does not do full-time programming, but more commonly finds him~herself cycling back to 
the computer every so often in order to carry out a computer-related component of a research project. 
Alternatively, it also means a person who is engaged full-time in carrying out computer-related analyses 
(e.g., a mathematical or statistical consultant). The basic criterion that shall be utilized in the 
evaluation of graphical software is one which is completely analyst-oriented: save the analyst's time. 
 It is important to note that software is basically a tool, and this tool is most properly measured in 
 terms of its human reference point. Other starting points for software evaluation do of course exist 
(as the size of the software metrology literature confirms), but our preference is to work from the human 
"tool" concept directly. In particular, the ability of software to save human time in the completion 
of an assigned task (a common property of all tools) is of prime importance.  2.2 Analgsis-Based Criteri 
nn The term "analysis" as used herein is very general: it refers to any computer-related activity which 
a research scientist~engineer may do in order to carry out the objectives of his~her research project 
(e.g., graphics, data processing, data summarization, model-fitting, equation~diagram~manuscript preparation, 
etc.). Just as the prior subsection dealt with an important "time" characteristic of analysts (namely, 
that they prefer to have the expenditure of their time kept to a minimum), this subsection carries on 
that "time" theme and notes an important property of almost all analyses in a scientific~research environment 
9 namely that: analyses are iterative. It is almost always the rule (especially in a data analysis 
context)p that analyses are iterative/ sequential in the sense that the next step of the analysis is 
primarily dictated by what has been uncovered from the data (or by how the process has  Computer Graphics 
Volume 15, Number 3 August 1981 been updated) in the current step. An obvious example of this is in mathematical 
modeling in which the assessment of goodness of fit of a given model (and the choice of what updated 
model to use next) is dictated entirely by the information latent in the graphical residual analysis 
following the formal fitting of the given model. A second example of the iterative nature of analyses 
is in the construction of diagrams~schematics; for example, the positioning of various components in 
LSI device diagrams is typically done iteratively {interactively) until the individual components and 
the diagram as a whole are aesthetically balanced and "look right" to the eye.  2.3 Basic Questions/Criteria 
 Ass~vne that an analyst has a specific activity A to perform, and assume that he~she is using a specific 
software system~language L to carry it out. In view Of the above points regarding minimizing analyst 
time and iterative nature of analyses, the following questions~criteria naturally follow: i) Power--Can 
the activity be achieved at all (amount of code notwithstanding) within the context of this system~language? 
Analyst time increases in the absence of power in a system~language. If a "hole" exists in the system~language 
so that the desired activity cannot be carried out, then the analyst either foregoes the activity (thereby 
compromising the thoroughness of the analysis), or wastes time programming around" the hole or dumping 
out the necessary intermediate information for input to another system~language. Low-level languages 
(e.g., FORTRAN, BASIC, etc.) generally fare better than high-level languages in regard to power. For 
example, in a graphics context, with a low-enough level language plus the capability to move or draw, 
the analyst has the raw ability to construct any complicated diagram imaginable (and unimaginable). On 
the other hand, high-level languages have the potential problem that if the appropriate command does 
not exist within the context of the language, the analyst may not be able under any circumstances to 
carry out the desired analysis. For high-level languages, this shortcoming can be partially circumvented 
by judicious command selection (e.g., in DATAPLOT, the existence of not only global graphics commands 
such as PLOT and 3D-PLOT, but also elementary graphics commands such as MOVE and DRAW).  2) Lenqth--How 
many lines of code are required to carry out the activity? Analyst time increases as the number of lines 
of code required increases. Low-level langauges fare poorly by this criterion; for example, there exist 
complicated graphical data analytic techniques that could easily take several hundred lines of low-level 
code to achieve. Subroutine libraries and high-level languages fare better. 3) Local Complexity--How 
complicated are the individual lines of code? Analyst time increases as local complexity increases. Complexity 
is difficult to quantify; one guideline which may be used, however, is to adjudge how much of a transformation 
is needed in order to map the in-mind representation of the activity to the computer-coded representation 
of the activity? Subroutine libraries (with their structured argument lists) probably fare poorest in 
this regard. 4) Global Comp!exit~--How easy is it to remember the various commands and conventions required 
by the system~language in order to carry out the activity? Analyst time increases as global compplexity 
increases. Global complexity is also difficult to quantify; however, it is clear that command~procedure~subroutine 
nomenclature which is "English-like" has advantages {for English-speaking countries, of course) in this 
regard over systems whose nomenclature is heavily codified. In any event, consistency of nomenclature 
(and syntax) goes far in terms of reducing global complexity. 5) Interactiveness--Is the iterative nature 
of most analyses easily achieved by this system~language? Analyst time increases drastically over the 
course of an analysis in the absence of an interactive capability. For general analysis activities, batch-oriented 
systems fare poorest in this regard; interactive systems fare best. For general graphics activities, 
systems which have no provisions for interactive graphics feedback (e.g., light-pen, cross-hair cursor, 
etc.) fare poorly relative to systems with such interactive feedback.  6) Completeness-How complete 
is the system~language? Can it perform not only a given specific activity, but also a set of associated 
activities which frequently occur along with this activity? Analyst time increases in proportion to the 
incompleteness of a system~language. Further, the thoroughness of the analysis {and hence the validity 
of the conclusions) may suffer when it is difficult to carry out various concomitant activities typically 
associated with an analysis (e.g., graphical residual analysis after a fit). The completeness of a system/ 
language is defined only in relation to a set S of activities that are expected to be carried out in 
the defined target environment {e.g., in a research environment, in a scientific environment, in an educational 
environment, etc.). It is the composition of the set S of activities for the analyst (or more generally 
for the group of analysts) in the target environ- ment which dictates whether or not a system/ language 
is complete. The flexibility of low- level languages gives them greater chance of completeness; higher-level 
languages can approach this completeness only by judicious choice of commands and capabilities.  Computer 
Graphics Volume 15, Number 3 August 1981 2.4 Practical Power For an activity A and a system~language 
L, the "practical power" P(A,L) is defined as follows: practical power = power x probability of use 
 The first term (power) has been discussed (though Rot rigorously quantified) above; The second term 
 (probability of use) is related to many human engineering considerations; it certainly relates to how 
closely the software system~language matches the in-mind concept for the desired activity. The greater 
the congruence, the higher the probability of use. The probability of use tends to increase with how 
easy the system~language is to use, which in turn is related to the various time-related criteria e~umerated 
above (length, local complexity, global complexity, interactiveness, and completeness). Power in itself 
is an incomplete measure of software utility; a more meaningful measure is practical power. Practical 
power P(A,L) is a function of the activity  A. Interest in an isolated activity is rare however; more 
generally, for day-in~day-out analyses, one is interested in the practical power for not just one activity, 
but across the set S of activities encompassed by these analyses. For example, an ideal (ideal for the 
analyst) graphics system~language is one that has high practical power for the full scope (not just one) 
of commonly-utilized graphical activities. This implies that all of the activities needed by the analyst 
for an analysis could be carried out with a minimal expenditure of time. In this regard, it certainly 
makes sense to consider global measures (such as average practical power ~(S~L) of a system~language 
L over a set S of activities). We formalize this no further here but rather refer the interested reader 
to Filliben (1979, Interface) for a more extended development of these and related issues. We conclude 
this section by noting that an obvious first step in applying global measures (such as average practical 
power) of systems~languages is that the individual activity elements in the set S must be specified 
and enumerated. In the following section, we discuss the set S of activities which served as the target 
set for the development of the DATAPLOT language.  3. DATAPLOTDESIGN PHILOSOPHY: THE DATA ANALFSIS/MATH~4ATICS 
IFfERSBCTION The design philosphy of DATAPLOT is based on the importance and relatedness of the two 
general areas: i) data analysis; and 2) mathematics. In a scientific environment--whether at a university, 
in private industry, or in a government laboratory--it is near-impossible to carry out a research project 
without some facet of the analysis being drawn from both areas. Data analysis and mathematics are complementary 
areas, and a research project which is restricted (by software) to only one without the benefits of the 
other is necessarily limited in terms of the thoroughness of the analysis and in terms of the efficient 
expenditure of the analyst's time. The x-diagram below enumerates several important sub-areas of data 
analysis and mathematics--specific activities that an analyst might typically engage in to carry out 
a research project. The upper-left to lower-right diagonal is the data analysis diagonal--it includes 
the familiar tools which the statistician routinely applies. The lower-left to upper-right diagonal includes 
the tools that a mathematician might commonly employ.  Computer Graphics Volume 15, Number 3 August 
1981 Several comments are appropriate with respect to the X-diagram. First of all (as mentioned above)~ 
it is rare that an analysis (whether statistical or mathematical) does not include components from the 
complementary diagonal. Secondly~ the individual components of the diagonals--especially the data analysis 
diagonal--have a heavy representation of techniques which are graphical in nature (control chartsp probability 
plots~ etc.). Thirdly 9 of all the individual elements on the two diagonals~ there is a select subset 
of three activities which may be properly classified as both statistical and mathematical: i) plotting; 
2) fitting; and 3) function evaluation &#38; variable transformation. These 3 kernel activities are 
common to both diagonals and are of prime importance to both. In terms of factors which affect the use 
of statistical software~ we thus see that the ability~inability to plot general data sets/ functions~ 
to fit general models~ and to evaluate~ transform general functions are key factors in the power and 
ease-of-use of the system. With the above underlying interlocking nature of data analysis and mathematics 
in mind~ the DATAPLOT design philosophy thus becomes straightforward: i) Firstlyp concentrate on developing 
3 powerful and flexible primary commands so as to allow the analyst to easily carry out the above 3 kernal 
activities; these 3 kernal commands are: PLOT (for plotting); FIT (for fitting); and LET (for function 
evaluation &#38; variable transformation): 2) Secondly~ develop the necessary set of secondary commands 
to handle the associated activities on the arms of the diagonals (and for other activities not illustrated). 
 3) Thirdly~ develop the necessary set of tertiary commands to handle the various subsidiary activities 
(e.g., defining the line colors on a plot) that the above primary and secondary commands might require. 
 Thus in short~ the unique design philosophy for DATAPLOT is seen to be a direct by-product of the underlying 
interrelationship between data analysis and mathematics with its common kernal of 3 activities; it is 
these 3 kernal activities which play a central role in the overall development of the language. 4. DATAPLOT 
CAPABILITY FEATURES  The purpose of this section is to enumerate various DATAPLOT capability features. 
As discussed in the previous section~ DATAPLOT has capabilities in 4 areas: i) graphics; 2) fitting; 
3) data analysis; 4) mathematics.  For sake of discussion~ the above 4 areas will have 2 additional 
partitions: the graphics area will be partitioned into 2 parts--one dealing with general graphics and 
the other dealing with diagrammatic graphics; the data analysis area will also be partitioned into 2 
parts--one dealing with graphical data analysis and the other dealing with non-graphical data analysis. 
Thus the framework for the discussion of capabilities will be the following 6 areas~sub-areas: i) general 
graphics; 2) diagrammatic graphics; 3) fitting; 4) graphical data analysis; 5) non-graphical data analysis; 
6) mathematics.  4.1 General Graphics Capabilities Graphics capabilities include continuous display 
 terminal plots (e.g., Tektronix)~ discrete (narrow-width or wide-carriage) terminal plots (e.g., TI 
700), high-speed printer plots, high-quality secondary output plots [e.g., Calcomp); on-line interactive 
definition and plotting of functions; data plots; multi-trace plots; linear or log scale plots; plots 
with or without labels~ titles~ frames, tic marks~ grid lines, legends~ legend boxes~ arrows, etc.; 
 automatic hardcopying of plots; 3-d plots of functions and~or data; multi-colored graphics; all of 
above for full data sets or subsets of data. Examples of use of such graphics capabilities are as 
follows: PLOT EXP(-O.5*X**2) PLOT EXP(-O.5*X**2) FOR X = -3 .i 3 PLOT Y1 Y2 Y3 VERSUS X PLOT Y1 Y2 
Y3 VERSUS X SUBSET LAB 1 TO 4 PLOT Y PRED VERSUS X 3-D PLOT Y X1 X2 3-D PLOT EXP(-O.5*(X**2+Y**2)) 
FOR X = -3 .i 3 FOR Y = -3 .i 3  4.2 DiagraHm~atic Graphics Capabilities Graphical diagrammatic capabilities 
relating to manuscript and slide preparation include sub-vocabularies for interactive diagramming such 
as chemical diagramming, electrical diagramming, IC and LSI device diagramming, printed circuit diagramming, 
flow charting~ logo construction~ and general text~equation writing (with choice of Hershey character 
fonts~ character sizes~ upper and lower Computer Graphics Volume 15, Number 3 August 198 i case, mixture 
of English~Greek~math symbols~ superscripting and subscripting, etc.). Capabilities exist for the saving, 
editing, combining~ superimposing, and redisplaying of any diagram. Examples of use of such diagrammatic 
graphics are as follows: TEXT Y = INTEGRAL() SIN(ALPHA() X) DX DRAW 20 20 40 20 40 40 20 40 20 20 
RESISTOR 30 30 50 30 HEXAGON 60 60 70 60 NAND 50 50 60 50 ARROW 20 60 80 i0   4.3 Fitting Capabilities 
 Fitting capabilities include interactive on-line model specification; fitting of linear, polynomial, 
multi-linear, and non-linear models; fitting may be linear~non-linear, weighted/unweighted, constrained~unconstrained; 
non-linear fitting without need of derivatives; pre-fit analyses for determination of non-linear fit 
starting values; exact rational function fitting; spline fitting; least squares smoothing; robust smoothing; 
automatic storage of predicted values~residuals from all fitting and smoothing operations; superimposed 
raw and predicted value plots; residual plots; fitting and smoothing over full data sets or subset of 
data. Examples of use of such fitting capabilities include: CUBIC FIT Y X FIT Y = (A+B*EXP(-C**X))/(ALPHA+BETA*X) 
FIT Y = AO+AI*EXP(A2*XI)+A3*EXP(A4*X2) EXACT 3/3 RATIONAL FIT Y X CUBIC SPLINE FIT Y X MEDIAN SMOOTH 
Y  4.5 tlon-graphical Data Anal~sis Capabilities Non-graphical data analysis capabilities include 
elementary statistics (25 statistics); analysis of variance; median polish; tabulation of summary statistics; 
on-line definition and execution of functional transformations; cum. dist. functions {24 dist.); prob. 
density functions (24 dist.); percent point functions (24 dist.); random number generation (24 dist.); 
all operations may be over full data sets or subsets of data. Examples of use of such non-graphical 
data analysis capabilities include: LET A = i0~ TRIMMED MEAN Y LET 8 = RANK CORRELATION X Y LET B 
= RANK CORRELATION X Y SUBSET LAB 4 TO 6 ANALYSIS OF VARIANCE Y X1 X2 X3 MEDIAN POLISH Y X1 X2 X3 
MEDIAN POLISH Y X1 X2 X3 SUBSET DAY 1 TO 3 LET Y = (X**LAMBDA)/(LAMBDA-I) LET C = NORPPF(.95) LET 
X = CHI-SQUARED RANDOM NUMBERS FOR N = 1 1 100  4.6 Mathematical Capabilities Mathematical capabilities 
include interactive on-line definition &#38; concatenation/ composition of functions; functional analyses; 
 exact analytic symbolic differentiation; root extraction; definite integration; convolution. Examples 
of use of such mathematical capabilities are as follows: LET FUNCTION F = EXP(-ALPEA*X/Y) LET FUNCTION 
G = SIN(EXP(X)) LET FUNCTION H = LOG(I+F)*G LET FUNCTION F2 = DERIVATIVE F1 WRT X LET A = INTEGRAL 
F WRT X FOR X = 0 TO 4 LET B = ROOTS F WRT X FOR X = 0 TO 100  4.4 Graphical Data Analysis Capabilities 
 Graphical data analysis capabilities include box plots; complex demodulation plots; control charts; 
correlation plots; distributional frequency plots; histograms; lag plots; percent point plots; auto and 
cross periodograms; probability plots (24 distributions); probability plot corr. coef. dist. analysis 
plots {3 families); auto- and cross- spectral plots; scatter plots; pie charts; Youden plots; graphical 
ANOVA/ANOCOV; runs plots; 3-d dist. frequency plots; 3-d histograms; 4-plot per page univariate analysis; 
all of above for full data sets or for subsets of data. Examples of use of such data analysis graphics 
include the following: HISTOGRAM Y WEIBULL PROBABILITY PLOT Y BOX PLOT Y X CROSS-SPECTRAL PLOT Y1 Y2 
MEAN CONTROL CHART Y LAG 3 PLOT Y  Computer Graphics Volume 15, Number 3 August 1981 5. EXANPLES OF 
DATAPLOT CODE AND OO'TPDT The purpose of this section is to present a sampling of DATAPLOT code and 
the resulting output. The examples will focus on commonly-occurring graphical activities. Usually there 
will be several different ways within DATAPLOT to generate the same output; more than one of the different 
ways will at times be presented. Space considerations limit the number of examples presented; the examples 
are based on simple elementary plot activities. Most are drawn from the general graphics category; a 
few simple examples have been drawn from the diagrammatic graphics category~ and a few from the graphical 
data analysis categrory.  5.1 Plot Data Data exists on a file as a series of (x~y) pairs. Two numbers--an 
x and a y--are on each line image. There is an indeterminate (and unimportant) number of line images 
in the file. Read in the (x,y) data points from the file in a format-free fashion. Plot them. See figure 
1 for the output. READ file name X Y PLOT Y X  5.2 Plot a Subset of the Data Read in (xgy) points 
from a file. Plot them but restrict the plot to only those points between x = 400 to x = 500. This demonstrates 
the subsetting feature and will here have the effect of "blowing up" the plot. In general~ subsetting 
may be done on any variable (not just those involved in the plot) and may be done for combinations of 
variables. See fig. 2. READ file name X Y PLOT Y X SUBSET X 400 TO 600  5.3 Plot a Function Plot 
the Normal N(O,I) density function. See figure 3. PLOT (I/SQRT(2.0*PI))*EXP(-O.5*X**2) FOR X = -3 .i 
3 or LET FUNCTION F1 = I/SQRT(2*PI) LET FUNCTION F2 = EXP(-O.5*X**2) LET FUNCTION G = FI*F2 PLOT 
G FOR X = -3 .i 3  5.4 Plot NultipleData Traces (Type i) Data exists on a file for several variables 
(data vectors) with corresponding points on the same line image~ and with some indeterminate (and 
unimportant) number of line images. Read in the data points for the several different variables. Plot 
them (6 traces will result for this example). Have all traces solid except for the first trace which 
is to have a dotted line. See figure 4. READ file name X Y1 Y2 Y3 Y4 Y5 Y6 LINES DOTTED SOLID SOLID 
SOLID SOLID SOLID PLOT Y1 Y2 Y3 Y4 Y5 Y6 VERSUS X -0. 370 4.3?8 -0.3?4 -0.3?6 -0.328 -0.3~) I I I I 
m Figure i. -l,3?O  -4,.3211 -0.3?4 j~I Figure 2. I,.4 0.3 8.a 0.1 0.0 Figure 3. |.| 8.0 1.| '~|~ 
 t.O 0.6 0.0 lw Figure 4.  Computer Graphics Volume 15, Number 3 August 1981 5.5 Plot HultipleData 
Traces (Type 2) Data exists on a file as (y,x) pairs with a third variable--a tag variable--which identifies 
the trace that a given (y,x) pair belongs to. Read in the (y,x,tag) data points. Plot them (4 traces 
will result for this example). Have traces 1 and 2 with plot character A and traces 3 and 4 with plot 
character B. Have traces 1 and 3 as solid and traces 2 and 4 as dotted. See figure 5. :3m ,8/"/-///",//" 
////" /// ........ 41 READ file name Y X TAG CHARACTERS A A B B LINES SOLID DOTTED SOLID DOTTED PLOT 
Y X TAG IN I -2 .... I .... -1 I .... e I 1 5.6 Plot Nuttiple Function Traces Figure 5. Plot the Normal 
N(O,l) density function, along with its first and second derivatives. Have the line types solid~ dotted 
and dashed, respectively. Note line 4 of the code below (a period} is a DATAPLOT null "command" which 
is useful as a visual separator between sections of code. More importantly, note how the PLOT commands 
may be "strung together" via the AND keyword. See figure 6. LET FUNCTION F = (I/SQRT(2*PI))*EXP(-O.5*X**2) 
LET FUNCTION D1 = DERIVATIVE F WRT X LET FUNCTION D2 = DERIVATIVE D1 WRT X 0.4 '" I.I +.e """ '~ " " 
,,, ...... ,,, I ' ;,,...,/" "'" LINES SOLID DOT DASH PLOT F FOR X = -3 .i 3 AND PLOT D1 FOR X = -3 .1 
3 AND PLOT i)2 FOR X = -3 .i 3 -9.4 -3 I -2 ..... "-.k-~ ..... -t Figure 6. l I a .... 5.7 Plot a Mixture 
of Data and Functions Read in (x~y) pairs. Plot these points discrete X's. Superimpose a continuous trace 
with a solid line. See figure 7. as functional IS N READ file name LET AO = -181 LET A1 = 142 LET B1 
=2.8 LET FUNCTION F X Y = (AO+AI*X)/(I+BI*X) $S / CHARACTERS X BLANK LINES BLANK SOLID PLOT Y X AND PLOT 
F FOR X = 1.65 .1 3.3 Figure 7. 5.8 ~ate a Nap Map information exists on a file as (x~y) coor-dinate 
pairs along with a third variable--a tag variable--which identifies the trace (= connected segment) that 
a given (x,y) pair belongs to. The (x,y) pairs are ordered in the sense that they exist in the file in 
the same sequence that they are to be drawn. Read in the (Xgy~tag) data points. Plot out the map. See 
figure 8. READ file name X Y ID PRE-SORT OFF CORNER COORDINATES 20 20 80 90 PLOT Y X ID Figure 8. Computer 
Graphics Volume 15, Number 3 August 1981 5.9 Generate a 3-Dimensional Data Plot (XgygZ) coordinates 
exist on a data file. These data points may define any general 3-dimensional entity--a line in space~ 
a plane9 a surface, a figure 9 etc. Read them in and generate a 3-dimensional plot. Position the eye 
at (x = 29 y = 5, z = 3). See figure 9. READ file name X Y Z EYE COORDINATES 2 5 3 3D-PLOT Z X Y  5.10 
Generate a 3-dimensional Function Plot Figure 9. Generate the bivariate normal N(0,0919190) density function. 
Position the eye at (x = 109 y = 109 z = See figure 10. 11). LET FUNCTION E = -0.5*((X**2)+CY**2)) LET 
FUNCTION F = (I/(2*PI))*EXP(E) EYE COORDINATES i0 10 ii 3D-PLOT F FOR X = -3 .I 3 FOR Y = -3 .i 3 5.11 
Generate 3-Dimensional Multiple Data Traces Data exists on a file as (xgy~z) triples along with a fourth 
variable--a tag variable--which identifies the trace that this (Xgy9 z) triple belongs to. A trace may 
define any general 3-dimensional entity. Read in the (x,y,zgtag) data points. Plot them all as individual 
traces9 have the first trace solid and the second trace dotted. Position the eye at (x = 29 y = 59 z 
= 3). See figure ii. Figure 10. READ file name X Y Z ID LINES SOLID DOTTED EYE COORDINATES 2 5 3 3-D 
PLOT Z X Y ID 5.12 Generate a Boxed Title Go to the middle of the screen and generate the centered string: 
graphics in upper case triplex italic letters. Surround it with a box with lower left corner at (30,45) 
and upper right corner at (70~59). See figure 12. Figure ii. FONT TRIPLEX ITALIC CASE UPPER JUSTIFICATION 
CENTER HEIGHT 5 WIDTH 4 ERASE MOVE 50 50 TEXT GRAPHICS BOX 30 45 70 59 GI~AI=HIC~ I Figure 12.  Computer 
Graphics Volume 15, Number 3 August 1981 5.13Generate a Nathematical Equation Go to a point 20 percent 
of the way across the screen and 50 percent of the way up the screen and write the following equation 
in lower case triplex: y = integral sin(alpha x)dx + exp(bx) Note that the last line of DATAPLOT code 
below contains several key words--INTEGRAL~ ALPHA, and SUP; these key words are identified to DATAPLOT 
as such by the convention of an empty set of parentheses as in INTEGRAL(), ALPHA(), and SUP(). See figure 
13. FONT TRIPLEX CASE LOWER JUSTIFICATION LEFT HEIGHT 5 WIDTH 4 ERASE MOVE 20 50 TEXT Y = INTEGRAL()SIN(ALPHA()X)DX 
+ ESUP()(BX)   5.14 ~ate an Electrical DiagraB Generate a simple electrical diagram. Have the wrinkles 
in the resistor with a height of 1 unit and a width of 1 unit. See figure 14. HEIGHT 1 WIDTH 1 ERASE 
DRAW 20 20 50 20 GROUND 50 15 MOVE 50 20 DRAW 50 20 80 20 80 45 RESISTOR 80 55 DRAW 80 60 80 80 52 80 
CIRCLE 48 80 DRAW 48 80 20 80 20 50 CAPACITOR 20 49 DRAW 20 20  5.15 Generate a Lag Plot  Data exists 
on a file as a series of x values. Read them in and generate a lag plot with lag = i. Use the symbol 
X as the plot character. The lag plot is a graphical data analysis technique for assessing autocorrelation 
structure in time series. See fig. 15. READ file name X CHARACTERS X LINES BLANK LAG 1 PLOT X 5.16 
Generate a Box Plot Data exists on a file as a series of line images with 2 values per line image. The 
first value is a response variable value; the second value is the treatment identifier for a given response. 
Read in the data and generate a box plot. The box plot is a graphical data analysis technique for assessing 
whether significant differences exist between various treatments; it is akin to the statistical procedure 
known as 1-way analysis of variance. See figure 16. READ file name Y X CHARACTERS BOX PLOT LINES BOX 
PLOT BOX PLOT Y X y : /sin(ax)d* + 0'° Figure 13. 1 0 [ ii Figure 14. 401 O -m t e 1 xil % "~..-r" 
.~, , -(too -650 -400 -~qm O ~kl Figure 15. go " I 7 I 'i" I '7" I *7" l *7 l ' "o |O--80 ?0--?O 60- 
50-50 11III011 I ' I ' I : I 40 4ee I s!e I 8ee lee8 leeo 14e$ 1100 q@ Figure 16.  Computer Graphics 
Volume 15, Number 3 August 1981 5.17 Generate a Superimposed Plot of Raw Data and Fitted Values Data 
exists on a file as a series of line images with 2 values per line image (response variable and independent 
variable). Theory suggests an exponential~linear non-linear relationship. Read in the data and carry 
out a least squares fit. Generate a plot of the raw data (as discrete X's) and the superimposed least 
squares fitted curve (as a solid line) (both vertically) versus values of the independent variable (horizontally). 
Such a superimposed plot is a first step in assessing the goodness of fit of a general model. See figure 
17. READ file name Y X LET ALPHA = .i LET A = .001 LET B = .01 FIT Y = EXP(-ALPHA*X)/(A+B*X) CHARACTERS 
X BLANK LINES BLANK SOLID PLOT Y PRED VERSUS X  5.18 Generate a Residual Plot Data exists on a file 
as a series of line images with 2 values per line image (response variable and independent variable). 
Theory suggests a linear relationship. Read in the data and carry out a least squares fit. Generate 
a plot of the residuals (as discrete X's) from the fit (vertically) versus values of the independent 
variable (horizontally). Such a plot is drawn from a battery of graphical procedures known as residual 
analysis--they are essential for assessing goodness of fit of a general model. See figure 18. READ file 
name Y X FIT Y = A+B*X CHARACTERS X LINES BLANK PLOT RES X 6. DATAPLOT IMPLERENTATION FEATURES 
 The DATAPLOT system was initially (1977) written for implementation at the National Bureau of Standards. 
The underlying code consists of about 1000 subroutines and 250,000 lines of code (including abundant 
internal documentation in the form of comment statements). On NBS's Univac 11089 the entire system (with 
overlaying and segmentation) takes 74K words. For continuous graphics, the DATAPLOT system as implemented 
at NBS makes use primarily of Tektronix 4XXX series display terminals. It has run on all of the Tektronix 
terminals from the 4006 on up to the 4054. The 4027 is used for color graphics. It is also compatible 
with the 4112 and 4114 Tektronix terminals. When only discrete graphics is needed 9 DATAPLOT can be 
used with a wide variety of terminals (Texas IN m N n i 2 3 4 5 G Figure 17. 4u . / x x -44m 
0 I ~ I 4 I G I 8 I I0 I 12 14 Figure 18. Instrument 90mron 9 Hazeltine 9 etc.). It can, of course, 
be run in batch mode with output directed to a high-speed printer. The system may also be run with off-line 
continuous graphics devices such as the Tektronix penplotter, Calcomp plotter, Versatec plotter, and 
Zeta ~lotter. For construction of printed circuit diagrams, the Tektronix penplotter on paper or acetate 
medium is of use. Output may be directed to several devices simultaneously. The system as implemented 
at NBS does make use of Tektronix PLOT-10 software, but the linkage to such software is through a single 
subroutine and so substitution of graphics software subsystems other than PLOT-10 is feasible. The 74K 
words size as quoted above is inclusive of this low-level PLOT-10 software. The above information pertaining 
to DATAPLOT at NBS is for illustrative purposes only; the system and the underlying code is in no sense 
lodal or peculiar to the NBS computer; DATAPLOT has been designed and coded for generality:  To enhance 
transportability, machine constants for a wide variety of computer types (IBM, CDC, DEC, Honeywell, Univac, 
Cray, Burroughs 9 Vax 9 Interdata 9 Data General 9 NP, etc.) are automatically included within the DATAPLOT 
code;  To enhance.computer~compiler generality, a safe, conservative discipline of restrict- ing underlying 
source code to subsets of  FORTRAN Standard 66 and 77 has been imposed.  To enhance portability, the 
system is coded (&#38; verified) in PFORT (subset of ANSI FORTRAN);   Computer Graphics Volume 15, 
Number 3 August 1981 To enhance core-size generality, the dimensions of the main internal data storage 
array are hard-coded only within 1 subroutine and symbolically referred to in all of the remaining DATAPLOT 
code;  To enhance paging-size generality (for those computers with paging memory), the system has been 
partitioned into functionally-independent segments;  e To enhance implementation, any system- dependent 
local operations (e.g., opening a mass storage file) are isolated to 1 subroutine, and referred to in 
a system- independent symbolic fashion in all calling subroutines;  To enhance debugging, a dynamic, 
interactive debugging~tracing capability is included as a standard command in the DATAPLOT language; 
  To enhance maintainability, a modularized, structured, uniform style for the underlying code has been 
rigorously adhered to;  To enhance expandability, the system has been partitioned into blocks of stand- 
alone subroutines;  To enhance graphics output generality, primary output may be on continuous display 
terminals (e.g., Tektronix 4014), on discrete narrow-width display terminals  (e.g., TI 700), on discrete 
wide-carriage terminals (e.g., AJ 832), or on the discrete wide-carriage high-speed batch printer; secondary 
output may, of course, be directed to any locally-available secondary output device (e.g., Calcomp, Versatec, 
Fr-80, etc.); To enhance graphics-hardware independence, all device-dependent local operations (e.g., 
generating a local hardcopy of the screen contents) are isolated to 1 subroutine, and referred to in 
a system- independent symbolic fashion in all calling subroutines; To enhance graphics-software independence, 
 all calls to low-level graphics software (e.g.~ accessing the Tektronix PLOT-10 software) are isolated 
to 1 subroutine, and referred to in a system-independent symbolic fashion in all calling subroutines; 
 Extensive external documentation describes not only the language, but more importantly the application 
of the language to a wide variety of real world" problems. The documentation for the important FIT command 
alone consists of over 250 pages, starting with a few pages of FIT specifications, followed by many pages 
of data analysis considerations for model fitting, and finishing with a detailed presentation of 20 commonly-encountered 
fitting examples. 7. A SI~4MARY OF GENERAL DATAPLOT FEATURES  Among the more important features of 
the DATAPLOT language are the following: I) DATAPLOT is a high-level, English-based language; thus the 
researcher~scientist~ engineer with little or no programming experience is able to apply and make use 
of advanced computerized techniques/ tools in the course of an analysis. 2) The DATAPLOT command structure 
is consistent and forgiving; thus the analyst need not waste time transforming the in-mind desired operation 
to atcoded computer instruction; in DATAPLOT, the two are almost always identical. 3) As enumerated 
above, each of the four capabilities areas (graphics, fitting, data analysis, and mathematics) are individually 
wide-ranging and inclusive; the analyst thus has available a full assortment of advanced and powerful 
techniques that he~she may invoke at any time. 4) DATAPLOT is advanced in its scope (graphics, fitting, 
data analysis, and mathematics); thus the analyst may fluidly link operations and techniques across the 
4 areas as the analysis may dictate. The analyst may carry out the range of operations from the initial 
exploratory data analysis, to the intermediate graphics~fits~data analysis~and mathematical operations, 
and then to the final manuscript~slide graphics and equations--all without ever having to leave the language, 
and all without having to resort to the complexities of lower-level languages. 5) DATAPLOT is unique 
in its consistent in-line mechanism for the routine handling (inclusion or exclusion) of special cases 
and subsets; focusing a plot or analysis to a desired subset is done by a simple statement added to the 
end of the command line--no additional lines of code are needed: thus it is as easy for the analyst to 
carry out an operation on any desired subset as it is to carry out the operation over the full set. 
6) The interactive nature of DATAPLOT facilitates the continuity needed for following the "leads" as 
suggested during the course of an analysis--this in turn provides increased insight into underlying structure 
and a significant saving in the analyst's time. 7) The underlying FORTRAN code is modularized, structured, 
top-down, and consistent. It is internally documented and readable. In addition, a dynamic, interactive 
debugging capability is provided as part of the language.  Computer Graphics Volume 15, Number 3 August 
1981 8. COMPARISOH TO OTHER S¥STM~K~/~AGES  The dominant characteristic of DATAPLOT is that it has capabilities 
in: I) graphics; 2) fitting; 3) data analysis; 4) mathematics.  DATAPLOT is a "robust" system; that 
is to say, its capabilities are not restricted to a single area, rather it has good power across several 
relevant areas (e.g., graphics, fitting, etc.), though it may or may not necessarily have optimal power 
for any one of the particular areas. As discussed in section 3, scientific analyses are, by and large, 
multi-faceted; an appreciation of this fact provides more than ample support for "robust" systems/ languages. 
DATAPLOT is therefore most reasonably compared only to other "robust" systems~languages. On the other 
hand, because of the breadth of capabilities spanned by DATAPLOT, there are potentially many local neighbors 
in the various individual areas (e.g., there are many graphical languages existent which may be compared 
to DATAPLOT, though such languages may have no capabilities whatsoever in fitting, data analysis9 and 
mathematics). Such comparisons are valid and informative, though they must be interpreted in light of 
the comments of section 3--that graphics, per se 9 is most typically done in concert with other activities 
(e.g., fitting, data analysis, and~or mathematics) and so one must, from a practical point of view, take 
into account the loss of power, time, and efficiency on the part of the analyst in shifting from a graphics 
system, to a fitting system, to a data analysis system, back to a graphics systems, and so forth during 
the course of an analysis. In addition to the above capability characteristics, DATAPLOT has the following 
major implementation characteristic: 5) it is portable to all major computers.  In terms of the above 
5 characteristics holding simultaneously, DATAPLOT is (to the author's knowledge) unique among currently 
available software systems~languages. The comparison to other systems~languages will use an abbreviated 
version of the DATAPLOT features as enumerated in earlier sections of this paper as a reference point. 
Though such a reference point is clearly biased, it is nevertheless useful because of the underlying 
premise of this manuscript that graphics, per se, is never done in a vacuum, and so realistically one 
must take into account capabilities in other concomitant areas (e.g., fitting). Although DATAPLOT 
has neighbors in all 4 areas (graphics, fitting, data analysis, and mathematics), we shall (due to the 
central role of graphics) restrict our comparison to those systems~languages which at least have capabilities 
in the graphics area. Unfortunately this has the unpleasant side-effect of eliminating many locally powerful 
systems~languages which specialize in particular topics (e.g., partial differential equations 9 eigensystems, 
finite element analysis, etc.). On the other hand, such systems would typically be even more locally 
powerful if they had built-in graphics with which to augment the analysis. Space limitations necessarily 
restrict the thoroughness of the comparison with the net result that the presented evaluation is admittedly 
preliminary and skeletal in nature. This is further true due to limited knowledge (on the part the author) 
about various subtleties in the capabilities of the other software. For any inaccuracies which may have 
found their way into this evaluation, the author asumes full blame and would appreciate correction. There 
is certainly room in the graphics literature for a broader comparative study of available graphics software. 
Such a study would be extremely useful to software devlopers and users alike; the author encourages such 
a study. It should be noted that two excellent comparisons already exist in the statistical software 
literature: Francis (1981) and Caporal/Hahn 61981). The Francis study is a lengthy compendium of over 
100 available statistical systems. The core of this study is a structured 4-page summary of each of the 
systems. The study is by no means limited to graphical software, but rather is concerned with statistical 
software in general 9 with some entries dealing with graphical software. The Caporal/Hahn study also 
has a statistical~data analysis bent to it, but does in fact focus only on those systems which are graphical 
in nature--over 15 such systems are considered. This study provides a useful tabulated comparison of 
the various systems based on capability criteria commonly encountered in a graphical data analysis context. 
 The closest neigbors to DATAPLOT are probably ~LAB, SAS/GRAPH, and PLOT-50. All of these systems are 
"robust" in the sense that in addition to graphics capabilities, there exist capabilities in fitting, 
and~or data analysis, and~or mathematics. We briefly sketch each of the above systems for comparison 
purposes, using DATAPLOT capabilitities as a reference point. MLAB is an interactive interpretive language 
(with APL-like constructs) whose design focus is mathematical modeling (that is, fitting). It was developed 
in the biomedical modeling environment at the National Institutes of Health, and has proved to be a valuable 
modeling tool--both biomedical and otherwise. See Knott (1979) for an excellent summary description of 
MLAB. Computer Graphics Volume 15, Number 3 August 1981 SAS/GRAPH is a recent add-on to the long-standing 
statistical system SAS. SAS/GRAPH has no non-graphics capabilities in itself--such capabilities are drawn 
from SAS. A succinct summary of SAS appears in FRANCIS (1981). More detailed information on SAS/GRAPH 
is available from SAS Institute Inc. (Cary, North Carolina). PLOT-50 is a stand-alone software system 
provided for the Tektronix 405X Intelligent Terminals. Information on PLOT-50 may be obtained directly 
from Tektronix, Inc. (Beaverton, Oregon). 8.1 General Graphics Capabilities MLAB, SAS/GRAPH, and PLOT-50 
all have the ability to plot 2-dimensional and 3-dimensional data vectors. MLAB and PLOT-50 have no color 
capabilities; SAS/GRAPH does. SAS/GRAPH and PLOT-50 lack capabilities for direct l-line plotting of functions, 
l-line plotting of multiple traces, and mixing of data and functions within a plot statement. 8.2 Diagrammatic 
Graphics Capabilities This area includes features such as interactive diagramming (with light-pen or 
cross-hair for positioning), schematic construction (electrical,  LSI device, chemical, etc.), and 
text~equation writing, etc. MLAB and SAS/GRAPH have font specification (e.g., triplex) whereas PLOT-50 
does not. Only Plot-50 of the three has special diagrammatic symbols {e.g.', resistors). MLAB and SAS/GRAPH 
have no interactive feedback of screen position capabilities whereas PLOT-10 does. 8.3 Fitting Capabilities 
 Of the three, MLAB has the most powerful fitting capabilities. All have capabilities in non-linear 
 fitting. All, however, omit such fitting conveniences as automatic checking for replication, automatic 
lack of fit tests, generalized ,re-fit analysis, specialized rational function fitting, and robust 
fitting. 8.4 Graphical Data Analysis Capabilities MLAB is weakest of the three in this area--it has 
 histograms but otherwise omits many of the standard exploratory data analysis techniques such as box 
plots, lag plots, control charts, probability plots, spectral plots, etc. SAS/GRAPH has moderate capabilities 
in this area (e.g., 2 and 3-dimensional histograms, and spectral analyses), but also lacks Dox plots, 
lag plots, control charts, and a full range of probability plots). PLOT-50 also has moderate capabilities 
in this area (e.g., 2- and 3-dimensional histograms, spectral analyses, box plots, uniform probability 
plots, and normal probability plots, but also omits lag plots, control charts, and a full range of 
probability plots). 8.5 Won-Graphical Data Analysis Capabilities MLAB, SAS/GRAPH, and PLOT-50 all 
have the ability to compute elementary statistics; however, they all lack median polish, a full contingent 
of cumulative distribution functions, percent point functions, and random number generators. Further, 
MLAB lacks analysis of variance capabilities. On the other hand, SAS/GRAPH is stronger than DATAPLOT 
in such areas of cluster analysis, discriminant analysis, and factor analysis. 8.6 Kathematical Capabilities 
 MLAB is strongest of the three here--it has capabilities in interactive function definitions, exact 
analytic differentiation, root extraction, and concatenation~composition of functions. PLOT-50 is next 
strongest with some indirect functional capabilities and root extraction. SAS/GRAPH is generally weak 
in this area. MLAB, SAS/GRAPH, and PLOT-50 all have matrix capabilities superior to those of DATAPLOT. 
 8.7 Portability Neither MLAB, nor SAS/GRAPH, nor PLOT-50 is generally portable. MLAB (which is written 
in SAIL) is portable only to DEC 10 and DEC 20 computers. SAS/GRAPH is portable to IBM computers only. 
PLOT-50 is portable to Tektronix 405X Intelligent Terminals only. 8.8 Comparison to Graphics S~stems/Languages 
Only The above comparison dealt with "robust" systems. We now focus our attention briefly on graphics 
systems, that is, systems which specialize in graphics only and have no capabilities in fitting, data 
analysis, and mathematics. A formal comparison will not be carried out; rather, we note in passing that 
DATAPLOT has many strong features in the graphics area itself which suggest that it be near-optimal in 
many respects. To be specific, in DATAPLOT, each of the following graphical operations may be carried 
out by the entry of a single command: 2-dimensional plotting of vectors; 3-dimensional plotting of vectors; 
2-dimensional plotting of functions; 3-dimensional plotting of functions; 2-dimensional mixtures of data 
and functions; 3-dimensional mixtures of data and functions; 2-dimensional plotting over subsets; 3-dimensional 
plotting over subsets; multi-trace plots; colored plots; interactive diagramming interactive equation~text 
writing  Computer Graphics Volume 15, Number 3 August 1981 To the author's knowledge, there are few 
(if any) other graphical systems~languages which can carry out all of these elemental plot operations 
in so simple a fashion. Further 9 if one restricts oneself to systems~languages which are generally available 
and portable, then the author is aware of no other system~language which can carry out all of the above 
operations so simply. Again, a good, general graphical software comparative study would go a long way 
to assessing the validity of the above statements and to assess the general merits of available software 
systems~languages. (The Caporal/Hahn (1981) statistical graphics survey would be an excellent starting 
point in this regard.) More importantly~ such a broad study would provide direction and impetus for further 
development in this ever-more-important area of graphical software. 9. AVAILABILITY IWFORMATIOW  As 
of July I, 19819 DATAPLOT is available for general distribution. It is being distributed through the 
National Technical Information Service. The distribution medium is magnetic tape. The acquisition fee 
is approximately $900. For further information 9 contact the author at the following mailing address: 
 James J. Filliben Statistical Engineering Division Center for Applied Mathematics Administration Building, 
A-337 National Bureau of Standards Washington, D.C. 20234 301-921-3651 or contact NTIS directly at: 
 National Technical Information Service United States Department of Commerce Springfield, Virginia 22161 
 I0. ACK2a.WLEDGEKENTS The author is pleased to acknowledge the excellent comments of his NBS colleague 
Stefan Leigh along with the comments of the Siggraph referees--they all served to significantly improve 
earlier versions of this manuscript. II. REFERENCES Caporal~ P. M. and Hahn, G. J. 61981). An Overview 
of Tools for Automated Statistical Graphics. General Electric Corporate Research and Development Technical 
Information Series Report No. 81CRD024~ Schenectady, New York. Coswell, w. (1977). Portability of Numerical 
Software. Lecture Notes in Computer Science No. 579 Springer-Verlag, New York. 213 Filliben, James 
J. (1978). DATAPLOT--An Interactive System for Graphics, Fortran Function Evaluation, and Linear~Non-linear 
Fitting. Proceedings of the Statistical Computing Section of the American Statistical Association: 1977. 
 Filliben, James J. (1979). Factors Affecting the Use of Statistical Graphical Software. Proceedings 
of the Twelfth Interface Symposium on Computer Science and Statistics. Toronto. Filliben, James J. {1979). 
New Features in DATAPLOT--A Language for Graphics, Non-linear Fitting, Data Analysis9 and Mathematics. 
Proceedings of the Statistical Computing Section of the American Statistical Association: 1977. Filliben, 
James J. (1980). A Review of DATAPLOT-- An Interactive High-Level Language for Graphics, Non-linear Fitting, 
Data Analysis, and Mathematics. Proceedings of the Statistical Computing Section of the American Statistical 
Association: 1980. Francis, I. (1981). Comparative Review of Statistical Software (Edition 2). International 
Association for Statistical Computing. 358 Ives Hall~ Cornell University, Ithaca, New York. Knott, Gary 
D. (1979). MLAB--A Matheatical Modeling Tool. Computer Programs in Biomedicine, pages 271-280. Elsevier~North 
Holland Biomedical Press.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806808</article_id>
		<sort_key>215</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>26</seq_no>
		<title><![CDATA[Creating repeating hyperbolic patterns]]></title>
		<page_from>215</page_from>
		<page_to>223</page_to>
		<doi_number>10.1145/800224.806808</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806808</url>
		<abstract>
			<par><![CDATA[<p>A process for creating repeating patterns of the hyperbolic plane is described. Unlike the Euclidean plane, the hyperbolic plane has infinitely many different kinds of repeating patterns. The Poincare circle model of hyperbolic geometry has been used by the artist M. C. Escher to display interlocking, repeating, hyperbolic patterns. A program has been designed which will do this automatically. The user enters a motif, or basic subpattern, which could theoretically be replicated to fill the hyperbolic plane. In practice, the replication process can be iterated sufficiently often to appear to fill the circle model. There is an interactive &#8220;boundary procedure&#8221; which allows the user to design a motif Which will be replicated into a completely interlocking pattern. Duplication of two of Escher's patterns and some entirely new patterns are included in the paper.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer art]]></kw>
			<kw><![CDATA[Hyperbolic geometry]]></kw>
			<kw><![CDATA[Interactive graphics]]></kw>
			<kw><![CDATA[M. C. Escher]]></kw>
			<kw><![CDATA[Motif]]></kw>
			<kw><![CDATA[Poincare circle model]]></kw>
			<kw><![CDATA[Repeating pattern]]></kw>
			<kw><![CDATA[Symmetry groups]]></kw>
			<kw><![CDATA[Tessellations]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Hierarchy and geometric transformations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.8</cat_node>
				<descriptor>Hyperbolic equations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003727.10003729</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Differential equations->Partial differential equations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010244</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Hierarchical representations</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P69678</person_id>
				<author_profile_id><![CDATA[81430657695]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dunham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Mathematical Sciences, University of Minnesota, Duluth, Duluth, MN]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P331512</person_id>
				<author_profile_id><![CDATA[81332512824]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lindgren]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Mathematical Sciences, University of Minnesota, Duluth, Duluth, MN]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P329829</person_id>
				<author_profile_id><![CDATA[81100315313]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Witte]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Mathematics, University of Chicago, Chicago, IL]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Alexander, H. Periodic Designs in the Euclidean and Hyperbolic Planes, Realized by Means of Computer plus Plotter, 1978 (unpublished).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Coxeter, H.S.M. Crystal symmetry and its generalizations. Trans. Royal Soc. Canada (3), 51(1957), 1-13.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Coxeter, H. S.M. Introduction to Geometry, Wiley, New York, 1961, (2nd ed. 1969)]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Coxeter, H. S.M. and Moser, W.O.J. Generators and Relations for Discrete Groups, Springer-Verlag, New York, 1957 (4th ed. 1980)]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Fricke, R. and Klein, F. Vorlesungen uber die Theorie der elliptischen Modulfunktionen, (Publisher unknown), Leipzig, 1890.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Locher, J. L. (Editor) The World of M. C. Escher, Abrams, New York, 1971.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Creating Repeating Hyperbolic Patterns Douglas Dunham and John Lindgren Department of Mathematical 
Sciences University of Minnesota, Duluth Duluth, MN 55812 David Witte Department of Mathematics University 
of Chicago Chicago, IL 60637 ABSTRACT A process for creating repeating patterns of the hyperbolic plane 
is described. Unlike the Euclidean plane, the hyperbolic plane has infinitely many different kinds of 
repeating patterns. The Poincare circle model of hyperbolic geometry has been used by the artist M. 
C. Escher to display interlocking, repeating, hyperbolic patterns. A program has been designed which 
will do this automatically. The user enters a motif, or basic subpattern, which could theoretically be 
replicated to fill the hyperbolic plane. In practice, the replication process can be iterated sufficiently 
often to appear to fill the circle model. There is an interactive "boundary procedure" which allows the 
user to design a motif Which will be replicated into a completely interlocking pattern. Duplication of 
two of Escher's patterns and some entirely new patterns are included in the paper. KEY WORDS AND PHRASES: 
hyperbolic geometry, Poincare circle model, tessellations, symmetry groups, interactive graphics, motif, 
repeating pattern, M. C. Escher, computer art. CR CATEGORIES: 3.15, 3.41, 8.2 I. INTRODUCTION This 
paper describes a process by which repeating patterns Of the hyperbolic plane may be generated. A repeating 
pattern is defined to be a pattern which remains invariant under certain transformations of the hyperbolic 
plane. The Poincare circle model of hyperbolic geometry gives a concrete realization of the hyperbolic 
plane [Coxeter, 1961]. The points of this model are the interior points of a circle, called the bounding 
circle; the hyperbolic lines are represented by the diameters of the bounding circle and circular arcs 
orthogonal to the bounding circle. Permission to copy without fee all or part of this material is granted 
provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright 
notice and the title of the publication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or 
specific permission. 01981 ACM O-8971-045-1/81-0800-0215 $00.75 The hyperbolic transformations of most 
interest to us are reflections across hyperbolic lines and rotations about points. Hyperbolic reflections 
consist of ordinary Euclidean reflections across diameters and inversions with respect to the circular 
arcs. A rotation about a point can be produced by successive reflections across two lines intersecting 
at that point and at an angle equal to half the angle of rotation. Given a repeating pattern, the symmetry 
group of the pattern consists of all the transformations which preserve that pattern. Conversely, given 
a group of transformations and a basic subpattern or motif, a repeating pattern can be constructed by 
parqueting the hyperbolic plane with copies of the motif obtained by applying transformations from the 
group to the original motif. This is the process that will be described in this paper. The user first 
selects one of the four kinds of groups of transformations. These groups will be described is Section 
2. Then a motif is entered interactively with the aid of a "boundary procedure", to be described in Section 
4. The motifs can be designed to form an interlocking pattern, if desired. Finally, the copies of the 
motif are replicated about the circle model of the hyperbolic plane, using transformations from the selected 
group. 2. THE SYMMETRY GROUPS There are infinitely many types of repeating patterns of the hyperbolic 
plane, giving rise to infinitely many symmetry groups. This paper will concentrate on four families of 
symmetry groups whose patterns are highly symmetric. A regular tessellation, {p,q}, of the hyperbolic 
plane is a covering of the hyperbolic plane by regular p-sided polygons, or simply p-gons, meeting only 
edge-to-edge and vertex-to- vertex, q at a vertex [see Coxeter and Moser, 1957 for notation]. It is necessary 
that (p-2)*(q-2) > 4 in order that the q vertex angles add to 360 degrees. The heavy lines of Figure 
1 show a {6,4}. The symmetry group of the tessellation {p,q} consists of reflections across three kinds 
of lines: the edges of the p-gons, the perpendicular bisectors of the edges, and the radii of the p-gons 
which pass through the vertices. This symmetry group is denoted [p,q] [Coxeter, Moser, 1957]. The light 
and heavy lines of Figure 1 show the lines of reflective symmetry of the {6,4}. These lines of reflective 
symmetry divide the hyperbolic plane  into congruent hyperbolic right triangles with acute angles of 
~/p and ~/q. Any one of these triangles is a fundamental re~ion for the group [p,q] since the images 
of one such triangle by all the transformations in [p,q] will exactly cover the hyperbolic plane. Thus 
to create a repeating pattern with this symmetry group, it is necessary only to create a motif within 
one of the triangles and then successively reflect that motif over the hyperbolic plane. Note that the 
motif need not fill the fundamental region. Also the reflecting edges form a natural boundary beyond 
which the motif need not be extended, since the pattern will automatically be extended there by the reflection 
process. Figure 2 shows a motif within a fundamental region for the {6,4} and Figure 3 shows the complete 
pattern generated by the motif. Figure 3.  The repeating pattern generated by the motif of Figure 2 
and the symmetry group [6,4]. The tessellation {p,q} also has rotational symmetries of orders p, q, 
and 2 about the centers, vertices, and centers of the edges, respectively, of the p-gons. We denote this 
(orientation preserving) group of rotations by [p,q]+. A fundamental region in this case can be taken 
to be an isosceles triangle with angles 2if/p, ~'/q, and ~/q formed from two of the triangles in the 
previous case (Figure 4). Figure i. i "".., ", The tessellation {6,4} (outlined in dark "'-..  lines), 
showing all lines of reflective symmetry "-.. (both dark and light lines). 4, s~ I ",% ! -..j.. Figure 
4. Figure 2. The dashed lines outline a fundamental region A triangular fundamental region for the group 
for the group [6,4]+. The dotted lines outline [6,4] containing a bent arrow motif. adjacent copies of 
the fundamental region. In this case, however, there are no natural boundaries. It is up to the user 
to supply the motif boundary. The motif boundary can extend over the edge of the triangular fundamental 
region, provided that it is correspondingly indented elsewhere along the edge of the fundamental region 
(Figure 4). An interactive "boundary procedure", described in Section 4, aids the user in this process. 
If the motif and any corresponding indentations fill the triangular fundamental region, then the motif 
itself can be taken to be the new fundamental region. In this case, the pattern formed by the transformed 
images of the motif will be completely interlocking. If p-fold rotational symmetry about the centers 
of the p-gons of a {p,q} tessellation is combined with reflective symmetry across the edges, then a new 
group of symmetries, [p+,q], i~ obtained. Here, q must be even so that reflective symmetry occurs only 
across edges. The fundamental region can be taken to be the same isosceles triangle used for the previous 
group. The base of the isosceles triangle, being a line of reflection, forms a natural boundary, but 
the interactive motif "boundary procedure" is needed for the other two sides (Figure 5a). On the other 
hand, the symmetry group consisting of q-fold rotational symmetries about the vertices of a tessellation 
{p,q} together with reflective symmetries across the perpendicular bisectors of the edges is denoted 
by [p~q+]. In this case p must be even. The fundamental region can be taken to be a kite-shaped area 
formed by joining two of the right triangles of the [p,q] case along a hypotenuse (Figure 5b). 11" 
I i I I i j The two edges of the kite corresponding to reflection lines form a natural boundary, 
but again, the interactive "boundary procedure" is needed for the other two sides. (From an abstract 
point of view, this group is the same as the previous one. It is distinguished from that case by having 
reflective rather than rotational symmetry about the center of the Poincare circle model.)  3. HISTORY 
 The first well-known repeating patterns of the hyperbolic plane were the tessellations {p,q} which appeared 
in mathematical expositions [Fricke and Klein, 1890]. Often, for clarity, one'half of each of the isosceles 
triangular fundamental regions for the group [p,q]+ were shaded, the other half being left blank. Figure 
6 shows one such pattern with symmetry group [6,4]+.  Figure 5a. Fundamental region for the group [p+,q] 
 Figure 5b. Fundamental region for the group [p,q+]. Solid lines indicate lines of reflective symmetry. 
Dashed lines complete the outline of the fundamental region. Dotted lines complete the outlines of adjacent 
copies of the fundamental region. Figure 6. A pattern with symmetry group [6,4]+. This pattern appeared 
in an article by H. S. M. Coxeter (Coxeter, 1957) which inspired the Dutch artist M.C. Escher to create 
more complicated repeating patterns of interlocking motifs. Two of the four hyperbolic patterns which 
he created are shown in Figures 7 and 8, with symmetry groups [8,3+] (if differences in shading are ignored) 
and [6,4+] respectively. In 1978, Alexander (Alexander, 1978) developed a computer program to generate 
repeating patterns with symmetry group [p,q] once the coordinates of the motif had been entered.  Figure 
7. M. C. Escher's print Circle Limit II, taken from "The World of M. C. Escher" [Locher 1971]. If shading 
is ignored, this pattern has symmetry group  [8,3+] 4. THE PATTERN CREATION PROCESS The process begins 
with the choice of one of the four groups [p,q], [p,q]+, [p+,q], or [p,q+] which will be the symmetry 
group of the final pattern. Once the group has been chosen, the corresponding fundamental region is displayed 
on the graphics screen. The natural boundaries (i. e. lines of reflective symmetry) of the fundamental 
region are drawn as solid lines; the other edges (where the interactive "boundary procedure" applies) 
are drawn as dashed lines. Copies of the fundamental regions which are adjacent to the original fundamental 
region across non-reflecting edges are outlined with solid lines (corresponding to other reflection lines) 
and dotted lines (Figures 2, 4, and 5). There are zero, three, two, and two of these adjacent copies 
of the fundamental region corresponding to the groups [p,q], [p,q]+, [p+,q], and [p,q+] respectively. 
 The second step is the creation of the motif within the fundamental region. In the case of the group 
[p,q], this is a straightforward process of moving and drawing, using a cursor (since the fundamental 
region has natural boundaries). The second step for the other groups is more interesting, since it is 
possible to draw line segments across the non-reflecting edges of the fundamental region. The interactive 
motif boundary procedure is required to do this. It works as follows: First, that part of the segment 
between the present position and the edge is drawn (Figure 9a). The boundary procedure then draws the 
transformed image of that partial segment in each of the adjacent copies of the fundamental region (Figure 
9b). Finally it is necessary to move the F~ure 9. a Part of a segment . b Transformed ~ma$os of : WhiCh 
 thepartial  Will cross seF~eat~J~ ~! .... flectlng edge of ~ !dJ ..... ~anda.ent.1 :i~i ~ ~' s ~ : 
region. :.,i reglo~./s the fundaaental~ i / s- : Completion of the ~ d Trans£or~ed Figure 8.  i <! 
 M. C. Escher's print Circle Limit IV, taken from "The World of M. C. Escher" [Locher 1971]. The symmetry 
group of this pattern is [6,4+]. Computer Graphics Volume 15, Number 3 August 1981. cursor to the endpoint 
of the transformed image (which will also be on a non-reflecting edge of the fundamental region) and 
then draw the remainder of the original line segment (Figures 9c and 9d). In fact, the interactive boundary 
procedure draws the transformed images of all segments, since it is up to the user to decide which line 
segments will eventually form the motif boundary. As mentioned before, if the motif and its transformed 
images fill the fundamental region, then a completely interlocking pattern, like those of M. C. Escher, 
will be created. During the second step, the moves, draws, and color changes are stored in three arrays: 
Action, which records the action taken, and X and Y which record the (terminal) location of the action 
(terminal location = initial location for a color change). The final step, replication, will be described 
 in the next section. 5. THE REPLICATION ALGORITHM The replication process occurs in two stages. The 
first stage is described as follows: For simplicity, the left-most vertex of the fundamental region is 
taken to be the center of the bounding circle. The fundamental region may be successively reflected and/or 
rotated about this vertex to form a complete p-gon, the central ~, centered within the bounding circle. 
The corresponding collection of images of the motif produced by these reflections and rotations is called 
the p-gon pattern (Figure i0). I I i I  Figure 10a. A motif within the fundamental region for the 
group [6+,4]. Figure 10b. Replication of the motif of Figure 10a to fill the central p-gon, giving the 
p-gon pattern. The arrays X, Y, and Action are extended to record the entire p-gon pattern. This is 
done, for each of the refelections or rotations, as follows: if n actions were required to create the 
motif, then Action[i+n] is the same as Action[i], and X[i+n] and Y[i+n] are computed by applying the 
reflection or rotation to the point (X[i], Y[i]). (Note that these reflections and rotations are ordinary 
Euclidean reflections and rotations since they are performed about the center of the bounding circle.) 
 In order to describe the second stage of the replication process, we define p-gon layers inductively 
as follow: the 0-th layer contains only the central p-gon; the (k+l)-st layer contains all those p-gons 
not in any previous layer but which share an edge or vertex with a p-gon from the k-th layer (Figure 
ii).  1 Figure ll. The 0-th, first, and second layers of septagons of the tessellation {7,3}. The 
pattern is extended from the k-th layer to the (k+l)-st layer by reflecting or rotating (depending on 
the group) the p-gon pattern across those edges and vertices common to both layers (Figure 12). In theory, 
this stage of the replication process could be continued indefinitely, so that the hyperbolic plane would 
be filled with a repeating pattern. In practice, five layers are usually sufficient to give the appearance 
of filling a bounding circle less than a meter in diameter. This process will be described in some detail 
for the groups [p,q] and [p,q]+ when p > 3 and q > 3. The other cases use similar, but slightly more 
complicated algorithms. First, draw the p-gon pattern within the central p-gon. Assuming the existence 
of a procedure DrawPgonPattern which draws a transformed p-gon pattern given a transformation, this can 
be done by: DrawPgonPattern(Identity)  2]9 Then if nLayers represents the number of layers to be drawn, 
the following algorithm will draw the remaining layers. RotateCenter, RotateVertx, and T are variable 
transformations (representing rotation about the centers and vertices of p-gons and a cumulative transformation, 
respectively); RotateP, RotateQ, and RotateEdge are constant rotations by angles of 2~/p, 2W/q, and ~" 
about the centers, vertices, and centers of the edges of the p-gons respectively. Multiplication of transformations 
is performed left to right. RotateCenter := Identity; FOR i := 1 TO p DO BEGIN T := RotateEdge * RotateCenter; 
 ReplicatePattern(T, nLayers -i, Edge); RotateCenter := RotateP * RotateCenter; RotateVertex := RotateQ 
* RotateP; FOR j := 1 TO q -3 DO BEGIN ReplicatePattern(RotateVertex * T, nLayers -I, Vertex); RotateVertex 
:= RotateQ * RotateVertex END END If nLayers is I, ReplicatePattern merely calls DrawPgonPattern once--in 
this case, the above algorithm would draw the 0-th and first layers of p-gons. Figure 12 shows such a 
pattern. Figure 13 shows the patern extended to the second layer. Noting the similarity of the above 
algorithm to ReplicatePattern (below), it is easy to see that ReplicatePattern could be modified so that 
a single call to it would generate the entire pattern.   < 0 / Figure 12. This figure shows the p-gon 
pattern of Figure 10b replicated to fill the first layer of hexagons according to the symmetry group 
[6+,4]. t+:,3b. '/^ Figure 13. The pattern of Figure 12 extended one more layer. PROCEDURE ReplicatePattern 
 (InitialTransform: trnasformation; Layerdiff: integer; Adjacency: connectivity); VAR RotateCenter, 
RotateVertex, T: transformation; i, ExposedEdges, j, PgonsPerVertex: integer; BEGIN DrawPgonPattern(InitialTransform); 
IF Layerdiff > 0 THEN BEGIN CASE Adjacency OF Edge: ExposedEdges := p -3; Vertex: ExposedEdges := p 
-2 END; RotateCenter := RotateP * RotateP * InitialTransform; FOR i := 1 TO ExposedEdges DO BEGIN 
T := RotateEdge * RotateCenter; ReplicatePattern(T, Layerdiff -i, Edge); RotateCenter := RotateP * RotateCenter; 
IF i < ExposedEdges THEN PgonsPerVertex := q -3 ELSE IF i = ExposedEdges THEN PgonsPerVertex := q -4; 
RotateVertex := RotateQ * RotateP; FOR j := 1 TO PgonsPerVertex DO BEGIN ReplicatePattern(RotateVertex 
* T, Layerdiff -I, Vertex); RotateVertex := RotateQ * RotateVertex END END END; See the Appendix for 
a description of DrawPgonPat tern and the transformations RotateP, RotateQ, and RotateEdge.  Figure 
15. A pattern with symmetry group [5,4]+. 7. EXT~qSlONS First, the program could be extended to include 
more complicated symmetry groups, such as the symmetry groups of M. C. Escher's other hyperbolic patterns, 
"Circle Limit I" and "Circle Limit III". Next, the program could allow for color symmetry, i. e. the 
colors would be permuted by successive transformations of the motif. Escher's "Circle Limit II" and 
"Circle Limit III" are examples of such color symmetry. Another natural extension would be to allow 
for the construction of motifs out of shaded polygons--the final pattern being dispalyed on an area-oriented 
output device such as a raster CRT. The motif boundary procedure could easily be modified to handle polygons. 
 Many of the techniques used in creating hyperbolic patterns could also be used to create repeating patterns 
of the Euclidean plane or the sphere. Figure 16. A duplication of the pattern of M. C. Escher's Circle 
Limit II (Figure 7), having symmetry group [8,3+]. 8. SUMMARY This program can create pleasing repeating 
patterns of the hyperbolic plane in a few minutes, a process that would require months to complete to 
the same precision without computer aid. Only the Dutch artist M. C.Escher had the patience to create 
such patterns by hand. The process of creating these patterns brings together the disciplines of computer 
science, art, and mathematics. This is a useful educational tool for the illustration of concepts in 
transformation group theory and hyperbolic geometry. The computer program which generated these figures 
is written in FORTRAN, using Tektronics and Zeta supporting software. Figure 17. A duplication of the 
pattern of M. C. Escher's Circle Limit IV (Figure 8), having symmetry group [6,4+]. APPENDIX The transformations 
used in our program are represented by 3-by-3 real matrices. For instance, reflections across the sides 
of the triangular fundamental region for the group [p,q] can be represented by: ReflectEdgeBisector 
:= -I 0 Ref lectPgonEdge := sinh(2b) 0 cosh(2b~ :os(2 /p) sin(2 Ip) Ref lectHypo t enuse := in(2 /p) 
-cos(2 /p) 0 0 where cosh(b) = cos(W/q) / sin(~'/p) cosh(2b) = 2 * cosh(b)**2 -1 sinh(2b) = sqrt(cosh(2b)**2 
-i) 222 Computer Graphics Volume 15, Number 3 August 1981 The transformations RotateP, RotateQ, and 
RotateEdge are given by: RotateP := ReflectEdgeBisector * Ref lectHypo t enuse RotateQ := ReflectHypotenuse 
* Ref lectPgonEdge RotateEdge := ReflectPgonEdge * Reflect EdgeBisector The DraWPgonPattern algorithm 
can be described as fo i lows : PROCEDURE DrawPgonPat tern (T : transformat ion) ; VAR SumSquare, Tx, 
Ty: real; Z: ARRAY[1..3] OF real; BEGIN FOR i := 1 TO nPgonActions DO BEGIN SumSquare := X[i]*X[i] 
+ Y[i]*Y[i]; Z[l] := 2 * X[i] / (I -SumSquare); Z[2] := 2 * Y[i] / (I -SumSquare); Z[3] := (i + SumSquare)/(l 
-SumSquare); Z :=T'Z; Tx := Z[I]/(I + Z[3]); Ty := Z[2]/(I + Z[3]); CASE Action[i] OF Move: MoveTo(Tx, 
Ty); Draw: LineTo(Tx, Ty); Black: Color := "Black'; Blue: Color := "Blue'; Red: Color := "Red'; Yellow: 
Color := "Yellow" END END END ; BIBLIOGRAPHY [Alexander 1978]. Alexander, H. Periodic Designs in the 
Euclidean and Hyperbolic Planes~ Realized by Means of Computer plus Plotter, 1978 (unpublished). [Coxeter 
1957]. Coxeter, H.S.M. Crystal symmetry and its generalizations. Trans. Royal Soc. Canada (3), 51(1957), 
1-13. [Coxeter 1961]. Coxeter, H. S.M. Introduction to Geometry, Wiley, New York, 1961, (2nd ed. 1969) 
 [Coxeter and Moser 1957]. Coxeter, H. S.M. and Moser, W.O.J. Generators and Relations for Discrete Groups, 
Springer-Verlag, New York, 1957 (4th ed. 1980) [Fricke and Klein 1890]. Fricke, R. and Klein, F. Vorlesungen 
uber die Theorie ~der elliptischen Modulfunktionen, (Publisher unknown), Leipzig, 1890. [Locher 1971]. 
Locher, J. L. (Editor) The World of M. C. Escher, Abrams, New York, 1971.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806809</article_id>
		<sort_key>225</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>27</seq_no>
		<title><![CDATA[Austere C<supscrpt>3</supscrpt> graphics]]></title>
		<page_from>225</page_from>
		<page_to>232</page_to>
		<doi_number>10.1145/800224.806809</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806809</url>
		<abstract>
			<par><![CDATA[<p>The MITRE Corporation, a not for profit Federal Contract Research Center (FCRC), has been involved in Command, Control and Communications (C <supscrpt>3</supscrpt>) Systems Engineering for the past twenty years. MITRE's program in support of the Defense Communications Agency (DCA)/Command and Control Technical Center (CCTC) involves planning and systems engineering for the Worldwide Military Command and Control System (WWMCCS). Included as components of the WWMCCS are survivable command centers such as the National Emergency Airborne Command Post (NEACP). MITRE has been involved in the system engineering functions of the NEACP Automatic Data Processing (ADP) system and other austere command posts. A portion of MITRE's overall system engineering activities included the design of an austere graphics capability for the NEACP ADP system. This paper explains the constraints, problems, needs and solutions to this engineering project.</p> <p>The goal of this paper focuses on selecting a suite of graphics capabilities that can be implemented in an austere computer environment and which can support generic C<supscrpt>3</supscrpt> ADP graphics requirements. This paper also focuses on highlighting graphics capabilities that satisfy C<supscrpt>3</supscrpt> display needs but are realizable in an austere computer system.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>D.4.9</cat_node>
				<descriptor>Command and control languages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.7</cat_node>
				<descriptor>Military</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010476.10010478</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Military</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011054</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Command and control languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14182053</person_id>
				<author_profile_id><![CDATA[81405592810]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Todd]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The MITRE Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332797</person_id>
				<author_profile_id><![CDATA[81100342641]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[DeShazo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The MITRE Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Newman, W. H.; Sproull, R. F., "Principles of Interactive Graphics," New York, 1979.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Thomas, Paul D., "Conformal Projections in Geodesy and Cartography," Washington, 1964.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Austere C 3 Graphics Mr. Robert Todd Mr. Patrick DeShazo The MITRE Corporation SECTION i. INTRODUCTION 
 The MITRE Corporation, a not for profit Fed- eral Contract Research Center (FCRC), has been involved 
in Command, Control and Communications (C 3) Systems Engineering for the past twenty years. MITRE's program 
in support of the Defense Communi- cations Agency (DCA)/Command and Control Technical Center (CCTC) involves 
planning and systems engi- neering for the Worldwide Military Command and Control System (WWMCCS). Included 
as components of the WWMCCS are survivable command centers such as the National Emergency Airborne Command 
Post (NEACP). MITRE has been involved in the system engineering functions of the NEACP Automatic Data 
 Processing (ADP) system and other austere command posts. A portion of MITRE's overall system engi- 
neering activities included the design of an  austere graphics capability for the NEACP ADP sys- tem. 
This paper explains the constraints, prob-  lems, needs and solutions to this engineering pro- ject. 
 The goal of this paper focuses on selecting a suite of graphics capabilities that can be im- plemented 
in an austere computer environment and which can support generic C 3 ADP graphics require- ments. This 
paper also focuses on highlighting graphics capabilities that satisfy C3 display needs but are realizable 
in an austere computer system. SECTION 2. C 3 GRAPHICS IN AN AUSTERE ENVIRONMENT C 3 systems provide 
the U.S. Armed Forces, the Joint Chiefs of Staff (JCS), and the National Command Authorities (NCA) the 
communications, and information support capabilities to direct U.S. military forces and to audit both 
friendly and hostile assets throughout the spectrum of con- flict.  Recently, computer graphics equipment 
has been integrated into computer systems that support command center operations. C 3 graphics provide 
timely summary information on the strategic and tactical posture worldwide. Consequently a signi- ficant 
portion of the graphics displays have been tailored towards cartographic representation of C3 information. 
 U.S. as well as Soviet strategic planning may address the distribution of critical C 3 functions to 
deployable command centers that are capable of enduring through nuclear as well as conventional warfare. 
A graphics capability for information dissemenation in a mobile environment poses both technical and 
operational problems to the C 3 planner. Equipment environmental standards as well as restricted space 
and power capacity in a mobile center limit the computer systemresources that can be executed. 2.1 
Minimum Essential Characteristics Also from a review of command center activi- ties, the following essential 
graphics capabili- ties have been derived: a. Generation, display and maintenance of background cartographic 
information (static data).  b. Generation, display and maintenance of scenario dependent information 
(dynamic data).  c. Construction and placement of arbitrary surfaces in support of scenario informa- 
tion display.  d. Display and processing to provide area magnification.  e. Coordination of user inputs 
to support interactive graphics.  f. Production of hardcopy outputs of the graphics image.   In addition 
the C3 analysis has identified the following highly desirable graphics features: a. Coordinate transformation 
(e.g., rota- tion). b. Color display support. c. Gray scale reproduction.  2.2 Technical Constraints 
 Deployable command centers may be subjected to environmental extremes of temperature, pressure, humidity, 
and nuclear radiation. Consequently equipment employed in these command centers must be engineered to 
survive in such an hostile environ-ment. State-of-the-art graphics technology has Computer Graphics 
Volume 15, Number 3 August 1981 progressed satisfactorily for benign environment, but painfully slow 
advancement has been evident for militarized computer support equipment. General purpose information 
support pro- cessors suited for a hostile environment are pro- duced by only a few venders (e.g., ROLM, 
NORDEN, IBM, CDC). Of these, the ROLM 1666 has been one of the widely implemented in the C3 systems. 
OUTLAW SHARK, Strategic Air Command (SAC) EC-135 Computer Support Program, NEACP ADP and Ground Launched 
Cruise Missile (GLCM) are examples of C 3 programs that employ the ROLM processor. This computer is relatively 
slow (i,000 nanosecond cycle time) compared to current minicomputer state-of- the-art. Also the processor 
is limited in the do- main of directly addressable memory. Unlike the Digital Equipment Corporation (DEC) 
VAX 11/780 and Data General Corporation (DGC) MV8000 with several mil&#38;io~ words of directly addressable 
memory, the ROLM 1666 only supports 128 thousand (k) words (64k per partition). The limited address 
domain imposes development restrictions on the size and flexibility of the program. The Interstate 
Electronics Corporation (IEC) Plasma Display (PD) 3500 alphanumeric/graphics terminal is currently employed 
in a variety of airborne C 3 applications. It is one of a few graphics displays that will operate in 
a hostile environment. The display provides adequate reso- lution (512 by 512 pixel quantization), but 
pro- vides no graphics recall. Consequently informa- tion on the graphics image must be maintained in 
the computer's mass storage device which is dis- similar with many graphics devices which possess internal 
storage. Austere graphics development must address not only technical constraints but also the unique 
operational considerations of C 3 applications. The users of C 3 graphics are not computer program- mers; 
indeed many users are not familiar with ADP operations. Consequently the graphics subsystem must be tailored 
to the layman. Complicated procedures for graphics execu- tion must be eliminated in favor of menu selected 
predefined displays. Display time must be engi- neered to be consistent with human factors criteria. 
Surveys conducted with C3 users reveal- ed that a time to display of over ten seconds is distractingto 
the user. Militarized (airborne qualified) graphics hardcopy devices are provided by Datametrics and 
Miltope Corporation. These dot matrix printer plotters provide adaquate resolution hardcopy that is display 
compatible with the PD 3500. However software to merge the characteristics of the PD 3500 with the plotters 
is not available and would have to be integrated into the graphics subsystem. SECTION 3. TECHNICAL APPROACH 
 Graphics experimentation was conducted using the generalized graphics architecture illustrated in Figure 
3-1. Analysis was performed within the laboratory equipment depicted in Figure 3-2, using MITRE's Graphics 
Experimentation Facility (GEF). Candidate algorithms and displays were exercised and evaluated on the 
GEF. Evaluation criteria included computer system resource loading, ease of implementation and readability. 
Included here is an investigation of techniques for optimizing the graphics functions within the austere 
environment. Techniques which were overly complex, or resource intensive, were discarded, attractive 
techniques were submitted to experimentation and further evaluation. Typical cartographic images for 
the technical assessment experiments were built from then Cen- tral Intelligence Agency (CIA) World Data 
Bank One (WDBO). This data base which specifies the coor- dinates of physical and political boundaries 
are stored in the WDBO file as radians of longitude and latitude. Presentation of geographical regions 
on a cartesian surface can be accomplished by mathema- tical transformation, such as Mercator, Lambert, 
or Gnomonic projections. A special case of the Gnomonic projection is the polar stereographic projection. 
This perspective projection is used widely in the C 3 environment because it provides a minimal distortion 
(conformal), in the Northern Hemisphere, while representing great circle paths as straight lines (path 
length proportional to distance). Figure 3-3 is a conceptial illustra- tion of the polar stereographic 
projection. The polar stereographic projections is repre- sented mathematically by: I t.sin (@). cos 
(a) I + 256 IX = INT 1 + cos (@) IY = INT I r.sin (0)" sin (~) ] + 256 I + cos (~) ~ -~Static Data 
/  / Base Reader/ Generator, ~ Display~ Update I ~, Devices~ ._~Pr°grammerJ ~RDS l ~ Interface ------ 
Off Line Operations Figure 3-1  Where, IX = horizontal axis of display screen IY = vertical axis of 
display screen r = radius of screen (Raster units) = CO-latitude in radians = longitude in radians 
 INT = integer function Using the GEF the above formula was imple-mented.using the WDBO data base as 
input. This formula creates a data base compatible with any 512 x 512 display media (30,000 points) To 
com-press the overall data base size the colocated data points generated by the above formula were removed. 
The resultant data base (Figure 3-4) is approximately 13,000 points IBM 370 Minicomputer I SimulatorPD3000 
I Terminal ~-'~ Graphics cal~;ica~ a s e ScenarioCALCOMP~ r Generator I Plotter Scenario Data Base Emulates 
Raster Scan Plotter Figure 3-2 \ Gnomonic projection \ /--. ~ projected from IcJ ~ ~ center ofsphere, 
~ A: stereographic, / Figure 3-3 Because the CIA WDBO data base used is in Harvard's POLYVERT format 
(polygonal chains) the cartographic images produced by the above formula consisted of a series of contour 
(polygon) seg- ments within which each point of the segment has a high degree of correlation when compared 
to the range of the screen coordinate system Close observation of Figure 3-4 shows the great detail of 
data base. In order to reduce the graphics re- source requirements further coastline contours could be 
smoothed by removing points along them and replacing it with a straight line vector. The  method designed 
for this was a modified least squares fit algorithm called the Contour Approxima- tion Method (CAH). 
The following formula repre- sents the CAM_ used in the experiments: N-I = __ (M i _ Ei ) 2 N -2 X 
 i=2 Ei = M1 + (i -i)~M AM = M N - M1 N -1 Where, M i = ith point in map contour segment Ei = ith 
point of linear estimation of contour (E) N = data points of contour E = estimation error P r, 
Figure 3-4 Figure 3-5 shows line M, a map contour, is replaced by E assuming E, the error, is suffi- 
ciently small compared to a previously defined threshold. Again referring to Figure 3-4 it is evident 
that there are particular geographic regions which are cluttered with small islands Since these islands 
have little or no significance in the Com- mand and Control environment, a method was needed to remove 
them from the data base. Figure 3-6 illustrates the Adaptive Polygon Rejection (APR) principle in use 
showing the island scribed inside the rectangle defined by the maxima and minima of the polygon. For 
most situations this provides a first order estimate of polygon area. This island removal algorithm is 
APR and the formula for identifying candidate polygon's to reject is:  2 Xmax -X , ) +(Y + Ymin )2 
mln max Where, X = Maximum of X in max X . = Minimum of X in mln Y = Maximum of Y in max Y . = Minimum 
of Y in mln M N / M M 1 Figure 3-5 After estimating the polygon size it can be com- pared to a predefined 
threshold to determine if it is to be removed from the data base.  Xmin, Ymax Xmax, Ymax --Small Island 
 e.g., Saipan Polygon Size ///  Xmin, Yrnin Xmax, Ymin Figure 3-6  Cartographic images represented 
as a series of contour segments within which each point has a high degree of correlation when compared 
to the screen size, such as the one represented in Figure 3-4, can reduce their storage requirements 
by a simple contour encoding algorithm. This encoding of map contour points is termed Relative Address 
 Plotting (RAP) and an implementation of this approach is: M. =M. -M. i i i -1 ~here, i = ith encoded 
point M. = ith contour point i These data points can be reconstituted for display by simply following 
the formula: A M. =M. +M. 1 1 1 In the course of normal command and control operations it is necessary 
to monitor United States and foreign activities worldwide. To use computer graphics within the subsystems 
which monitor these forces, different cartographic images were needed. Within the scope of the re- search 
performed by the MITRE Corporation it was decided that a single cartographic projection could be used 
for all the strategic monitoring systems. Figure 3-7 shows an example of how this single cartographic 
data base is used. C 3 graph- ics concentrating on Continental United States (CONUS) could use the partition 
of the polar pro- jection that describes North America (System A). In a similar fashion, operations with 
emphasis on the Union of Soviet Socialist Republics (USSR) could extract the cartographic information 
for that region (System B). For global coverage of the Northern Hemisphere, the full cartographic data 
base would be used (System C). Data Base System A System C g /~i~> System B  l~ '4! Figure 3-7 Some 
ground based C 2 graphics operations have the ability to zoom to an arbitrary (ad-hoc) geo- graphical 
area. Geographical zoom doesn't pose a serious processing problem since it only involves a 2-D clipping 
algorithm. However closer examina- tion of the zooming problem revealed that in order to maintain the 
contour detail after a zoom has taken place thai the originating data base would need to be of greater 
precision than previously described. This added need for data base preci- sion consequently increased 
the size of the data base. Consequently the preprocessing algorithms  discussed above would have little 
application. In order to ~rovide a zoom capability to the users of austere C ° systems and avoid the 
data base prob- lems, a predefined zoom capability was developed. The ad hoc zoom capability was rejected 
because of the increased data base size and subsequent in- crease in display time. The predefined zoom 
capa- bility allows any area to be expanded as long as it has been previously defined in an offline pro- 
cess and has been processed by the CAM, APR and RAP programs to minimize its effects on the over- all 
system performance. Figures 3-8 and 3-9 de- pict the zoom concept as a display would look both before 
and after zoom respectively. i . Figure 3-8 ,   < '>_ ,F _ ~\> ,-4 " ~"~~ "X\\ National ~'\ Al:ort~ 
Figure 3-9  Figure 3-10 entitled SOVIET HF, which illus- trates a typical C 3 display, depicts the coverage 
of Soviet High Frequency radio transmissions. The aggregate coverage of these transmissions is shown 
by the polygon originating in the Soviet Union. That polygon is the resultant combination of two separate 
coverage triangles (FANS) with the in- terior lines removed. This process of removing the interior lines 
of the FANS is called profiling. Profiling is highly desirable when many coverage triangles are presented 
on a small screen since the interior lines only serve to clutter the image. Profiling is performed by 
first calculating all the intersections of the polygons as seen in figure 3-11 and generating subvectors. 
The next step as also seen in figure 3-11 is to determine whether or not the subvector is interior to 
the polygon. This determination is made by taking a point off the screen or display surface and draw- 
ing an invisible line from there to the center of each subvector; each invisible line is checked to see 
how often it intersects the original polygon. If the invisible line intersects the polygon an odd number 
of times the candidate subvector is interior to the polygon and can be discarded. The formula for the 
calculation of the line intersec- tions is as follows: ab. ~a.b n 1 1 n Yint = a - a. n 1 Xin t Yint 
- bn a n a. = &#38;Yi 1 ~X i b. =Y. - ax. 1 1 i Where, Y. = Y intersection int Xin t = X intersection 
 a. I = Slope of line . x b. = Y intercept of line . i . i ~X i = Change in X i ~Y. 1 = Change in 
Y. 1 SECTION 4. PERFORMANCE MEASUREMENT A primary focus of the MITRE experimentation was the reduction 
of data points needed to produce an acceptable cartographic image. The algorithms described in section 
3 (i.e., CAM, APR and RAP) were implemented by the MITRE Corporation using the resources of the GEF. 
 The CAM algorithm described was implemented at the GEF and evaluated using various rejection thresholds. 
Figure 4-1 shows the display time and data base improvements as a result of the increas- ing CAM threshold 
(Tc). Figures 4-2 and 4-3 de- picts the Northern Hemisphere using CAM thresholds of 15 and I00 respectively. 
 229 HIE 87ot/1926.37Z  SOVIET HF CITY F~Q PIeR TIHF I~"~#" " B r . 117"~ 2~ 14.Ni I~ O1 OF BI Figure 
3-10  Generation of Sub-Vectors  12~1 19 ~717 .,K Xo 4 Determine if sub-vector is interior to polygon 
Odd number of ~ intersections with polygon Figure 3-ii  Although CAM reduces the number of points needed 
to define the cartographic image it subse- quently produces a distortion which may be notic~ able to 
the user of the display. Figure 4-3 illustrates the distortion, however, the subsequent display time 
has been reduced from 50 seconds (Figure 3-4) to 12 seconds (Figure 4-3) (at 9,600 baud display rate). 
As seen from the graph in Figure 4-1 CAM has diminishing returns i.e., the higher the threshold the 
smaller the change in data base size and displaying speed. Figure 4-4 illustrates a CAM threshold Tc 
= 1,000 which re- sults in a data base size of 2,421 points with significant geographical distortion. 
 The APR formula shown in Section 3 is capable of removing small islands from the data base. This exact 
formula was developed and implemented on MITRE's GEF. Similar in nature to the CAM algor- ithm APR uses 
a threshold (Tp) to base its decision to reject or retain a candidate polygon in the data base. Figure 
4-5 shows an implementation of APR with a threshold Tp = 4. Comparison of this image to the one illustrated 
in Figure 4-3 illus- trates that little display degredation is notic- able. However, close observation 
of the display reveals that some of the islands in the Baffin Bay, Philippines, and Micronesian areas 
are missing. The utmost significance of APR is evident when the relative data bases of figures 4-3 and 
4-5 are com- pared. The APR processed data base is 1,3000 points wheras the unprocessed data base is 
3,000 points. Most important is the resultant display time savings of over 5 seconds. Figure 4-5 dis- 
plays in under 5 seconds at 9,600 bits per second (bps), whereas Figure 4-3 will display in 12 seconds 
at 9,600 bps. 60 '~ TC Austere t 50K 50 i M Data 40K 3o ~ 20K ~ 20 ~ ,o CAM ,oK/ ~ CAM+APR 10 20 30 
40 50 60 70 80 90 Smoothing Coefficient 11 (Tc) Figure 4-1  The data base size can be reduced even 
further by using the RAP algorithm described in section 3. RAP reduces the need for large on-line disc 
storage without significantly increasing the display time of a cartographic image. The relative savings 
of a data base processed with RAP can be estimated with the following formula (assuming that M occupies 
one byt e/coordinat e) l+n  ~= 1 - -- 2n -1 Where, = storage reduction coefficeint n = average number 
of points per map segment . Figure 4-2  Computer Graphics Volume 15, Number 3 August 1981 ti'.. ! 
Figure 4-3  Using the above formula the relative data base savings for the image depicted in figure 
4-5 is as follows: 1 + 13 14 ~ = 1 -( 3 ~ 13 -1 ) = 1 -(-~-~) = .44  Consequently, in this case storage 
requirements can be reduced by 44% with RAP. " '4 p Figure 4-4 / Figure 4-5  The off-line processing 
outlined in this and the previous section has proven to be invaluable in i'?plementing a graphics capability 
in an aus- tere environment. The resultant display time im- provements seen using CAM and APR was 90% 
and with the inclusion of RAP the subsequent data base re- duction was 96% over the unprocessed data. 
 The implementation of an ad hoc zooming capability is not indiginous to this preprocessed data base 
concept. However, after further dis- cussions with C3 personnel MITRE realized that an ad hoc zoom capability 
is not necessary for many C3 graphics applications. It was found that the predefined zoom area concept 
is satisfactory. The predefined zoom concept allows the designer during the off-line data base build 
process to generate area maps to a detail comencerate with the scenario data to be displayed. The storage 
requirements for this concept are modest (500 - 1,000 vectors per zoom area) without an additional processing 
burden. Further experiments were performed using the University of Arizona's Interactive Graphics Engineering 
Laboratory (IGEL) facility which is comprised of a Eclipse S/230 computer with a RAMTEK 9351 display 
unit. These experiments fur- ther validated that the MITRE approach was austere in both program size 
and Central Processing Unit (CPU) resource usage. During the MITRE experiments IGEL's Eclipse was running 
with 23 users processors executing concurrently and the MITRE demonstration package caused no noticable 
degradation to the users of the Eclipse furthermore the Eclipse user processes caused no noticable degradation 
to the MITRE demonstration. Analysis showed that the entire HITRE demonstration ran in 15 thousand (K) 
words of Eclipse memory.  Computer Graphics Volume 15, Number .3 Auoust 1981 SECTION 5. CONCLUSIONS 
 The MITRE C3 graphics feasibility assessment identified, examined and exercised graphics capa- bilities 
that could be supported by an austere computer system. Implementing the concepts ex- plored in this assessment 
will provide satisfac- tory C 3 graphics while absorbing a minimum of com- puter resources. The human 
factors experiments, conducted through the GEF, have validated the basic graphics capabilities. Lessons 
learned from the use of the GEF have revealed that the imple- mentation of the graphics subsystem should 
be a modest effort, as long as the concept remains austere. Table 5-1 shows the cost of graphics as 
re- lated to a typical austere C 3 development program. Hardware -Display Available off the shelf -Printer 
Available off the shelf Processing -CPU 10% of CPU Resources (ROLM 1666) -Disc 1% of Total Storage (60 
MByte Disc) Display Time -Terminal ~ 6 Seconds (9600 Baud) -Hardcopy ~ 10 Seconds (Dot Matrix Printer/Plotter) 
S/W Development -Terminal Graphics ~, 9-12 Man Months -Hardcopy Part of Vendor Support Graphics Table 
5-1 ADP APR BPS CAM CCTC CIA CONUS CPU C 3 DCA DEC DGC FANS FCRC GEF GLCM HF IEC IGEL JCS K 
NCA NEACP RAP TC TP USSR WDBO WWMCCS GLOSSARY Automatic Data Processing Adaptive Polygon Rejection 
Bits Per Second Contour Approximation Method Command and Control Technical Center Central Intelligence 
Agency Continental United States Central Processing Unit Command, Control and Communications Defense 
Communications Agency Digital Equipment Corporation Data General Corporation Coverage Triangles Federal 
Contract Research Center Graphics Experimentation Facility Ground Launched Cruise Missile High Frequency 
Interstate Electronics Corporation Interactive Graphics Engineering Laboratory Joint Chiefs of Staff 
Thousand National Command Authorities National Emergency Airborne Command Post Relative Address Plotting 
CAMThreshold APR Threshold Union of Soviet Socialist Republic World Data Bank One Worldwide Military 
Command and Control System BIBLIOGRAPHY  Newman, W. H.; Sproull, R. F., "Principles of Interactive 
Graphics," New York, 1979. Thomas, Paul D., "Conformal Projections in Geodesy and Cartography," Washington, 
1964.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806810</article_id>
		<sort_key>233</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>28</seq_no>
		<title><![CDATA[Two aspects of domain designing]]></title>
		<subtitle><![CDATA[C<supscrpt>@@@@</supscrpt> curve rendering and blended map projections]]></subtitle>
		<page_from>233</page_from>
		<page_to>242</page_to>
		<doi_number>10.1145/800224.806810</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806810</url>
		<abstract>
			<par><![CDATA[<p>In 1965, Shepard introduced a general interpolation formula for arbitrarily spaced data over any finite dimensional Euclidean space. The deficiencies in his initial results have been corrected and later derivations have been widely applied to surface fitting problems in aviation design and geology.</p> <p>This paper describes a new research direction in Shepard's method and a new application to computer graphics. In the first, the convex weighting functions are generalized by the creation of a user-defined mathematical domain which is independent of the surface. In the second, the original formula is applied to blend mapping projections for aviation visual simulation and an algorithm for rapid evaluation is presented.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>G.1.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>G.1.1</cat_node>
				<descriptor>Spline and piecewise polynomial interpolation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003649.10003657.10003659</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic representations->Nonparametric representations->Spline models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P15779</person_id>
				<author_profile_id><![CDATA[81100334998]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alyn]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Rockwood]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Evans & Sutherland Computer Corporation, Salt Lake City, Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P334047</person_id>
				<author_profile_id><![CDATA[81430642408]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Jensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Evans & Sutherland Computer Corporation, Salt Lake City, Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[R. K. Adler and P. Richardus, Map Projections, North-Holland Publishing, 1972.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[R. E. Barnhill (1977), Representation and Approximation of Surfaces, Mathematical Software III, edited by J.R. Rice, 69-118, Academic Press, Inc., New York.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[C. Caratheodory (1911), &#220;ber den Variabilitatsbereich det Fourierschen Konstanten von Positiven harmonischen Furktionen. Ren, Circ. Mat. Palermo, No. 32, 193-217.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[N. Draper and H. Smith (1966), Applied Regression Analysis, John Wiley and Sons, New York.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[G.M. Neilson (1974), Some Piecewise Polynomial Alternatives to Splines under Tension. Computer Aided Geometric Design edited by R. Barnhill and R. Riesenfeld, 209-235, Academic Pass, Inc., New York.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[D.T. Pilcher (1974), Smooth Parametric Surfaces, Ibid, 237-253.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>810616</ref_obj_id>
				<ref_obj_pid>800186</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[D. Shepard (1965), A Two Dimensional Interpolation Function for Irregularly Spaced Data, Proc. 23rd Nat. Conf. ACM, 517-523.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 Two Aspects of Domain Designing: C ~ Curve Rendering 
and Blended Map Projections Alyn P. Rockwood Thomas W. Jensen Evans &#38; Sutherland Computer Corporation 
Salt Lake City, Utah 84108 Abstract In 1965, Shepard introduced a general interpolation formula for 
~rbitrarily spaced data over any finite dimensional Euclidean space. The deficiencies in his initial 
results have been corrected and later derivations have been widely applied to surface fitting pro- bler~ 
in aviation design and geology. This paper describes a new research direction in Shepard's method and 
a new applica- tion to computer graphics. In the first, the convex weighting functions are generalized 
by the creation of a user-defined mathematical domain which is independent of the surface. In the second, 
the original formula is applied to blend mapping projections for aviation visual simulation and an algorithm 
for rapid evaluation is presented. Introduction The problem of constructing a smooth interpo- lant 
to arbitrarily scattered data is encountered frequently in many scientific applications. Sur- face and 
hypersurface fitting from meteorological information, stress values from finite element analysis, and 
geographical contour data are current examples. There are many solutions: from trian- gulation and piecewise 
defined surface patches to piecewise and global, rational Shepard schemes. The latter has proven to be 
one of the most general, adaptable and intuitive global methods in the literature. In its elementary 
form, Shepard's formula is capable of producing an interDolant in any given continuity class, including 
C ~. It can also be adapted to interpolate derivative information. Later work showed that it could be 
made visually smooth, that is the interpolant could be made to possess only those features which were 
implied by the original data. rbst recently, general poly- nomial precision was added by combining the 
method with least squares approximation via Boolean sum techniques. We begin with a definition of the 
Shepard interpolant and a precise description of its con- tinuity properties characterized in a theorem. 
Definition: Let {xi}, inl,P be a subset of R m, {Fi},i=l, p a subset of R . F~ P _I ,x~xi, i=l,p f 
SF=~i 1 di r i=l di r (1) L F i x = x i for some i  Where r > 0 and d i = the m-space Euclidean distance 
from x to x i. SF, the Shepard formula, is an in- verse distance formula. As x approaches Xk, i/~ + ~, 
while I/d r < B < ~ for i ~ k and P Fi __ + ~, thus, as x ÷ Xk, F k receives infi~ i=l di r nite weighting. 
Multiplication of the numerator and denomina- P r ter of SF by i~ 1 d i yields the following, more 
useful form. P P (2) SF = ~ wiF i / ~ w i  i=l i=l P  (3) where w i =j~l d~.  j/i Theorem: The continuity 
class of SF depends on r, the exponent of the distance functions, in the following manner. co (i) If 
r is an even integer, then SF ~ C  (ii) If r is an odd integer, then SF ~ cr-~  (iii) If r is not 
an integer, then SF e C [r] For a proof see [2]. The following describes a new generalization of Shepard's 
formula (I), based on mathematical composition which will be useful as an approxima- tion scheme. It 
has the advantage of intuitive, geometric controls which are natural, interactive design handles. The 
primary effect of the~ con- trols is the design of the domain of the Shepard formula weights (3). Just 
as the domain of Shepard's formula may be designed, the formula itself is useful for blending other kinds 
of functions together and designing their domains of influence. A natural application is the creation 
of geographic map pro- jections for computer-generated aviation visual simulation (II).  publication 
and its date appear, and notice is given that copying is by Permission to copy without fee all or part 
of this material is granted permission of the Association for Computing Machinery. To copy provided that 
the copies arc not made or distributed for direct otherwise, or to republish, requires a fee and/or specific 
permission. commercial advantage, the ACM copyright notice and the title of the &#38;#169;1981 ACM O-8971-045-1/81-0800-0233 
$00.75 Computer Graphics Volume 15, Number 3 August 1981 I. C ~ Approximation of Irregularly Spaced 
Points by Variation of Parameterized Moments I.l Parameterized Weighted Moments Scheme in R:  It will 
be useful to look at (2) as a physicist might. If the F i represented the co-ordinates of point masses 
with weights wi, then the wiF i would be the moments of the point masses about the origin. Also, SF(x) 
would be the center of mass of all the points. This would be a unique point if the w i were constant, 
but they are not. They are defined by a parameter x and as it varies, a line is gener- ated by (2) which 
interpolates the x i. One must imagine the different masses becoming the dominant mass in turn and abserving 
the center of mass as it traces out the curve. To understand the theory behind the generali- ~zation 
to (2), consider a line segment along the .u-axis in the two-dimensional "control space" (Figure i). 
Call it CL for control line. We de- sire to map CL onto some approximant of a given interpolation at 
the given data points {(xi,Fi)}. The simplest of these interpolations, that we will use without loss 
of generality, is LF i, which in- terpolates the data pair-wise linearly, i.e., a polygonal path. The 
mapping, ¢, will be one-to- one, and bicontinous (homeomorphic). ¥   I ¢ CONTROL SPACE , b,U olu, 
DATA SPACE Figure 1 The purpose of ¢ is to associate a weighting function, ¢, defined on CL, to LF (see 
Figure I). Thus, LF may be considered a mass that is weighted by both ¢ and ¢, and whose center is computed 
from its moments according to: fwLF i (4) (x'Y-) = fw Where: LF i = ¢(CL), w = ¢(¢-I(LFi)) Finally, 
parameterize ¢ by t and Choose as our approximating curve the set M = { (x(t), ~(t))} given by (4) as 
~ varies with the parameter t. If ¢,¢ and the parameterization are chosen judiciously then M is the desired 
approximation. Criteria for picking ¢ and ¢ will be considered later, but first an example of the above, 
is presented. 1.2 Implementation First, ¢, and CL will be defined by defining the inverse ¢-z on LF 
i by ¢-Z(x,y)= 5)xj~Z ~l (xj,Fj)-(xj_1,Fj_1) I[ + II (x,y)-(xj,Fj)[[ if j~0 and [I (x,Y)-(xj,Fj)ll 
if j=0. and letting If" ]1 denote the Euclidean distance. That is, ¢-I straightens out the piece-wise 
linear path between data points and maps them to the u-axis of the control space (see Figure 2). If LF 
i is non-crossing then ¢-i is one-to-one, which makes ¢ well-defined, and so we defined ¢ and CL.  CONTROL 
SPACE PL (ut,vt) U DATA SPACE Figure 2  For this implementation, the function ¢ will be given as 
an inverse square distance relation be- tween CL and a user-defined curve, PL (for para- meter line) 
(see Figure 2). (4) can conveniently be treated as discrete by thinking of LF i as a piece- wise linear 
dotted line of point masses L~ i in- stead of a solid line. This enables one to replace the integrals 
in (4) with finite sums for computa- tion. If we let {ilk} = ¢-I(EFi), we can now give explicitly for 
discrete points (ut,vt) on PL. (6) Ct(ik) = ~-~, where d k = (ilk - ut)2 + v~ The final form for our 
implementation looks like Z ~t (ilk) ¢ (ilk) (7) (xt,Yt) k = *t(%)  Several matters are important 
to note about the above formula. The parameter t dictates how many points are sampled in PL and how many 
points will be used to display the approximation M. Ex- perience ~hows that the number of fik'S need 
only be Computer Graphics Volume 15, Number 3 August 1981 about the magnitude of the number of t's. 
Any more are lost in the r¢solution of the display of ~. Formula (7) should be reminiscent of (2) Shepard's 
Formula in two dimensions. Indeed, if LF i is chosen to be the original data points, CL=PL and ~ properly 
adjusted, (7) would be a discrete form (23. _~ The choice of ~ as a "carpenter-rule" map-ping was based 
on intuition and its ease of compu- tation. From an interactive point of view it is better to keep it 
straightforward and let ~ drive the difference in the approximation, since # should have better user 
viewability. Figure 3 shows the effect of different parameter lines. The behavior of M at the limits 
of PL are interesting. DATA SPACE ~hT~LSPA~ DATA SPACE .......... .............. ¢ g o f CONTROL ~PACE 
 f ___o I Figure 3  rop: Data with strong local weighting Bottom: Same with weak local weighting 
As PL is removed from CL, the weights. ~(fik)  tend to become more nearly equal for dlfferent (ut,vt) 
and so the (~t,Yt) tend to be closer. They ultimately approach what would be the center of mass of 
L~ i with equal weights. In the other di- rection as PL is nearly CL, M becomes nearly LF. This becomes 
more evident if the sample points of ~F i are increased. In this regard, LF i is a limit~ Ing case 
of (7). There is no constraining reason why d k wa~  chosen as given in (6), except that if d k is 
C , then M can be guaranteed to be C ~ by a smoothly varying t, and CL and PL being disjoint. This 
will follow from M being a rational combination of a C ~ mapping, where all poles are avoided since 
CL and PL are disjoint. M can be given local support by disregarding  those calculations involved 
with all dkwhich generate values for dk less than a certain desired value. This is done for those dkwhich 
would not be significant in the calculations. It can have a positive input on the economy of the scheme 
as well. Local support is also a factor in the char- acteristics of M..This is demonstrated by Figure 
4 (see below). Using local support in this manner  jeopardizes the C ~ status of M as one value of 
Uk pops in or out of the calculations, but this may not be important or even noticeable to a user in 
 many applications. i 0 a t i Figure 4 From bottom to top: Increasingly broader support on the 
same data We consider these and other characteristics in more detail in the context of the following 
applications. 1.3 Non-Linear Regression The data points could represent a Teal observ- able relation. 
Such things as arrival patterns, laboratory results, population samplinF, and exo- genous factors such 
as economic levels and the weather are inherently stochastic, that is, the data "probably" represent 
the actual relationship, but are not necessarily exact. Indeed, even in 'precise" observations the data 
are often given an error margin, which is to say, there is only a tendency for the data to reflect the 
relationship, however strong the tendency. Reducing this data to a curve in a statistical framework with 
the hope that the curve might smooth out the larger discrep- ancies in the data or at least give a more 
palat- able view of the data is called regression.  Computer Graphics Volume 15, Number 3 August 1981 
Least-squares techniques are examples of line-  ar r~gressions. There are many applications where least-squares 
are unacceptable (since the under- pinnings might be dramatically non-linear). For these cases there 
is a rich theory of non-linear regression, for which [3] is a classic. These al- gorithms are ponderous 
and expensive, though, as should be expected when solving systems of non- linear equations. By comparison, 
(7) is an appeal- ling alternative to the traditional approach. Consider the data points in Figure 
3. Assume  they closely represent some empirical results. Our curve should then approach the data closely, 
that is, each datum must, in some region of our curve- rendering procedure exert a bit of weight, although 
 neighboring data modify this by the lesser weights they have in the region. The neighbors will each 
have their turn at being the dominant force as the region of greatest influence is moved along the 
parameter line, PL with varying parameter t. When the parameterization is complete, each point is strongly 
reflected in the curve. Such is the case in the top diagram in Figure 3. Notice that PL is close to CL, 
the control line. This means that in the inverse square distance weight- ing formula at some point, the 
moments of (7) will be heavily weighted in favor of a given data point. By contrast, the bottom diagram 
in Figure 3 reveals what happens when PL has some distance to CL. The weights among neighbors will be 
more near- ly equal since the distances are more nearly equal. Hence, the curve indicates a lower confidence 
level in the accuracy of the data as seen individually, but still valid collectively as embodying a cluster 
effect. There exists a well-posed question with re- spect to the weigjating scheme. If the problem to 
be solved is related to gravity or electrical charge then using the inverse square relation is natural, 
but there are better weighting schemes for solving different problems. In many cases in the real world, 
data clusters according to the Gaussian cu~mnulative distribution function. This is the "dart throwing" 
distribution. If the data points resulted from darts thrown at a given curve, then the weighting function 
would be better based on this distribution. (l'$e note that there are other, perhaps more useful, applications 
of this function). Figure 1 suggests such a scheme. We imagine ~ mov- ing from left to right as the parameterization 
pro- ceeds. All the other elements of distributions could be involved, e.g., variance, skewness, kur- 
tosis. Arrival and departure problems suggest the use of a Poisson distribution. We might even want a 
pseudo-random distribution to produce a con- strained random walk that could be manipulated by the data 
points. 1.4 Approximation A topic closely related to regression is approximation, and as a matter of 
fact, the former is often viewed as a subset of the latter. Approx- imation is distinguished here to 
emphasize the uses of parameterized weighted moments even where the data is assumed to be exact. One 
of the most obvi- ous of these uses is the ability to do what we call r~ising the "apparent" Ck-ness 
of an interpolation. C ~ denotes that the interpolation has derivatives up to order k that are continuous. 
LFi, the piece- wise linear interpolation of the (xi,Fi) is C °. It does not have continuous derivatives 
at the data points as is evident from the angularity of its graph (Figure i). To raise its C~--ness, 
its corners need to be smoothed off. Its status as an interpolation is changed to that of an approxi- 
mation to accomplish this. The local support for the weighting is restricted so that for most of CL, 
M = LF i (see the bottom diagram of Figure 4). That is not true where M is close to a data point. Here 
the influence of the next data point attenu- ates M, rounding off LFi's corner. By broadening the support, 
the rounded portion of the curve is increased (see the top and middle diagram of Figure 4). The term 
"apparent" is used in the sense of minimizing the effects of non-differentiability of order k+l. In 
Figure 4, for example, the differ- ence between left and right-sided derivatives at the still existing 
points of non-differentiability can be made arbitrarily small with adjustments to the weighting function. 
The given weighting func~ tion is already sufficient to give a visual impres- sion of C1-ness. It is 
more a graphics tool than a theoretic one. ~fnen LF i was first chosen as the underlying in-  terpolation, 
it was noted that this should be done without losing generality. If the discrete set £F i had descended 
from some other interpolation, say cubic splines, the question of raising the apparent Ck-ness would 
be applicable for k = 2 or 3. Convexity is a quality of any approximation of the parameterized weighted 
moments variety which can be guaranteed. According to a classical theorem of Caratheodory (1911) any 
linear combina- tion of vertices where the sum of positive scalars is less than or equal to one produces 
a point with- in the convex hull of those vertices. By distribu- ting the denominator of (7).into the 
sum in the numerator and treating the LF i as vertices the theorem of Cartheodory is applicable to (7). 
Every (~, 7) is within the convex hull of the LF i. If the LF i are within the convex hull of the (xi,Fi) 
then so is M. We strengthen the notion of convexity by con- sidering the union of convex hulls as continguous 
triples of data points. Proper choice of the local support clearly places M within this union. That is 
because the theorem need only apply to the data points in triples. Even without this local support M 
is very often contained in this union as with Figure 4. 1.5 Computer Aided Geometric Design The (7) 
generalization greatly expands the applicability of Shepherd's formula to computer aided design. Not 
only is it a more flexible tool for interpolation and approximation in situations where Shepherd's formula 
has traditionally been applied, i.e., over finite sets of arbitrarily spaced data in n dimensions, it 
also possesses pro- perties that make it a reasonable alternative for interactive curve and, consequently, 
surface design via Boolean sum and tensor product techniques. PL is an innate geometric handle for controlling 
the tension of the curve (Figure I). As noted be- fore, the closer PL passes to the x-axis in the neighborhood 
of a parameter point the closer M will pass to the corresponding control point. This contrasts to the 
earlier tensile curve techniques of Nielson and Pilcher [5] where arc length only Computer Graphics 
Volume 15, Number 3 August 1981 was under user control and interpolation was con- strained. Here, PL 
determines the interpolatory tension of each control point as well as the ten- sion of the curve. Thus, 
it represents interpola- tion and approximation as a smooth continuum, over which:the user has complete 
and unambiguous con- trol. DATA SPACE  ........................ -~-~ .................... d( ,', 
CONTROL SPACE ) Figure 5  Figure 5 illustrates this quite clearly. The first three control points are 
coincident with the last three, but the distance of PL to the x-axis increases linearly, thus decreasing 
the interpola- tory tension. The result is a spiral, which when extended, converges to the origin (the 
center of mass of the control points). Formula (7) remains computationally formidable in higher dimensions. 
However, as a curve design- ing scheme, it is computationally equivalent to Pilcher's polynomial alternatives 
to splines under tension. This, coupled with its local support visible smoothness and control points 
(the effect of which can be user definable) imply that it will be a flexible and powerful mathematical 
method for computer aided design. Figure 6 illustrates the technique as applied to design and alter 
a bell-shaped curve. Seven points are required to define the curve completely. As the points are moved 
(interactive- ly) by a user, the curve reshapes itself until the desired shape is achieved. ! !  ..... 
,./... E g I. 6 Sua~nary The purpose of this section was to introduce the concept of "domain designing" 
the graph of a curve (the control space mapping), and to survey some of its characteristics. These were 
demon- strated for a given implementation (7). There are, however, areas of pertinence that are yet to 
be addressed, such as the generalization to higher di- mensions, the application of domain designing 
con- cept to schemes other than Shepard's Formula and the nested use of those concepts. The variational 
principle is another aspect for consideration. For example, does the curve minimize arc length while 
still rendering the desired results. These are topics for future research. II. The Aviation Projection 
Problem and a General Shepard Solution. There exists no mapping from the sphere to the  plane that 
preserves all distances and angles. Visually, the world appears planar, but navigation~ ally its spherical 
properties are more than appar- ent. We design buildings as if the world were flat; we also would like 
to design the data bases for computer-generated visual simulation, which repre~ sents objects such as 
buildings, as if the world were flat. If we do so, and then travel signifi- cant distances, the problem 
is to account for the navigational discrepancies so that places appear where they should. The problem 
is complicated fur- ther by the primary constraint of real time image generation. However, the difference 
between navi- gating a plane and navigating a sphere is to be accounted for, it must be done quickly. 
 II.l Background The obvious solution of approximating the earth through a tessellation is unappealing 
for three reasons. First, it is at least as difficult computationally as a spherical representation and 
second, the loss of smoothness across polygon boundaries could.cause visible distortion. Finally, the 
tessellation might have to be extremely irregu- lar to conform to the requirements of the data base, 
i.e., so that the most "densely" modeled por- tions of the data base would reside.on polygonal- based 
sections. Computer graphics have been used for visual aviation simulation for some time. Generally, 
training has been conducted over data bases repre- senting small areas, such as a single city. For this 
application, the following planar projection, (based on arc length in perpendicular direction) is easy 
to evaluate in real time, typical, and suffi- cient. Y = R~ cos(~) X : R~¢ where: R = Radius of the 
earth. X = Distance north of data base reference. Y = Distance east of data base reference. ¢,k = 
Latitude, longitude of observer. Figure 6  Computer Graphics Volume 15, Number 3 August 1981 6~,6X 
= Dif;Serence latitude, longitude vector of observer and projection reference, Figure 7  Heading is 
measured.from the y-axis and it can be seen from Fi~are 7 that any significant change i= longitude induces 
error in that variable. More- over, meridians of longitude and other great cir- cles, displaced fron 
the mapping reference are dis- torted to the extent that movement along them cre- ates a visually perceptable 
curve. And, because the mapping is not conformal it is often impossible to travel accurate rectangular 
or trangular routes. These distortions are most pronounced in the high latitudes, and are significant 
if the naviga- tional area is greater than four hundred square miles I . II.2 Other Projections Thus, 
a different method of projecting the sphere is necessary if larger areas are to be accurately simulated. 
~qe Transverse Mercator pro- jection, figure g, is conformal, and though meri- dians are deformed, the 
distortion is not visible in navigational areas up to 600 miles east, and west of the reference. Y = 
R in (TAN ($ + ~)) X = R (~-~c) where: = TAN(%)/cos (61) 6 = TAN- 1 (sin(~l)sin(~) T~'J (¢) ¢c,Xc = 
lat., ion. of projection reference The scale distension is only apparent as an in- crease in velocity, 
since all objects are scaled equally, and is not significant. If heading is corrected differentially, 
i.e. hp = h s + TAN -lr~x/ay~ "ax-ax" Where: hp = Heading on projection. hs = Heading on sphere. X 
= First component of t. Mercator projection. Y = Second component of t. Mercator projection. then the 
transverse b~rcator is a dramatic improve- ment in every sense, except the computational one. Even the 
bivariate Taylor approximation requires five.times the calculation of the previous projec- tion. The 
transverse cylindrical projection, figure 9, from which the transverse Mercator projection was developed 
historically, is somewhat simpler to calculate. Y = R (a-~c) X = R TAN (6) Figure 9  Though it is not 
conformal, the overlay of it and the Transverse Marcator, Figure i0, show that the two nearly coincide 
over a large area, which of course, recommends it as an approximation for the conformal mapping. It should 
be noted here, that the technique of overlapping equally scaled pro- jections proved to be an important 
tool for com- parison. Similarities, as well as differences, in geometry and scale were clearly delineated. 
 Figure 8  i. A field of view of 48 degrees and altitudes less than 40,000 feet were pre-supposed. Figure 
10 238 Computer Graphics Volume 15, Number 3 August 1981 II.3 Evaluation In spite of the promise of 
the latter projec- tion, real time evaluation remains an obstacle in implementation. This problem motivated 
development of the following algorithm. A regular, rectangular tesselation T A of the space A = {(~,k):-~/2 
~ ~ ! ~/2, 0 ! ~ ~ 2 ~} is presumed, T A = {Sii} i = l,m  j 1,2m #here: ,kj < X < ~j+1}. Si 3 = {(~'~): 
@i < ~ < @I+~  Sii thus represents a four (which may degenerate to three) curvilinearly sided section 
of the sphere. M is chosen to be the smallest positive integer such that the difference between the approximation 
ahd the projection will not be visually detectable. Initialization consists of finding i,j such that 
the first viewing positive ~a Sij, and evaluating the actual projection P(¢,~) = (x(@,k),y(¢,k) at 
(@i,~j), (~i,kj+~), (~+1,~j), (@1+~,kj+~)  In real time, when the system is navigating the model, evaluation 
of the approximation PA con- sists of two parts: i. Determination of i, j such that VsS.. 2. Calculation 
of actual position, by bilinear interpolation. F PA =  [ P(*i,lj) P(*i,%j+~) ] Where M = P(~i+1 ,%j) 
P(~i+1 ,%j+~)J A third task is performed as well; but notat the refresh rate. Sincemovement in aviation, 
simulation is highly continuous, it may be assumed that V will reside in Sij over many frames. A minimum 
number is calculated based on maximum speed and evaluation of the actual projection ~(~,%) at (~k,%l) 
k = i-i, i+z 1 = j-l, j+l  is distributed over those frames. Hence, when the viewpoint crosses from 
one element of the tessella- tion into another, the data necessary for interpo- lation in available. 
 Note, the approximation method is independent of the choice of projection. Its applicability is a function 
of velocity, time available for computa- tion, and mathematical complexity of the projec- tion. As such 
it significantly enlarges the set of projections appropriate for real time navigation in CIG s~stems. 
 II.4 Blended Projections A conformal, cylindrical projection may be created for any great circle. However, 
it may be necessary to navigate areas other than those sub- tended by a tangental strip or circle. The 
follow- ing projective method was developed from Shepard's interpolation formula for arbitrarily spaced 
data. It 'is particularly useful for CIG simulation be- cause it allows for several arbitrarily spaced 
points of tangency about which, the composite pro- jection is highly accurate. Hence, it is possible 
to create projective accuracy in exactly those places where close correspondence between the visual simulation 
and the navigational simulation is critical. Conceptually, it resembles a tessala- tion of the sphere, 
except that there are no sharp boundaries. Let {(¢i,ki)}ni=l be a set of points on the  sphere. Let 
R i denote a radius of support about (~i,li) and Pi(~,l ) the projection trangent at (~i,li). Let di= 
distance from (~,~) to (~i,li) R.-d. if R.-d.> 0 (Ri-di)% (Ri_di)+ = I i I I-and m i  R.d. 0 if 
R.-d. < 0 11 1 1 The composite projection is then: ~(,,x)--~ n wi~ i(,,~) / n w. z i=i i= : 1 n where 
w. = K m. 1 j=l 1 j¢i possesses the following properties:  i. Q(~i,li) = Pi(~i,l) for all i 2. ~a~-~ 
(~i,Ei) = ~-~-(~i'~i-i )" for all i and ~ ~Pi ~k (~i,Xi) = --~ (@i,~i) for all i which implies Q approximates 
Pi in a neighborhood of (~i,li) 3. Q(~,I) is not affected bY Pi(~,l) if (~,~) is outside the circle 
of radius R i about (~i,li), Hence, it is possible to choose [(~i,li)] and [Ri] so that Q(~,I) = Pi(~,l) 
over a significant area Figure II is an illustration of such a covering.  Figure 11 Fibre 12 is a r~resentation 
of a blended projecti~ composed of five sinusoidal projections. ~r ea~, ~ was ~osen large enou~ so that 
t~ circle of support wo~d incl~e the re~rence points of the ot~r projection. ] Figure 12  II.5 Data 
base Development for Flat World The projective techniques surveyed above are sufficient for most, if 
not all, of the current applications of computer graphics to aviation visual simulation. However, they 
solve only part of the problems associated with navigating exten- sive regions. Since any C.G.I. system 
is capable of representing only a finite number of polygonal or curvilinear objects, navigation over 
large areas implies either loss of detail or a method by which parts of the data base that are not visible 
may be smoothly replaced by parts that soon will be. Modularization of the data base has proven to be 
an effective, flexible method for implementation of the latter. A tessellation of the gaming area is 
included in the data base and associated with each element of the tessellation is a list of modules. 
Each module contMns only a fraction of the topographical information to be displayed at any one time. 
Continuity between elements of the tessellation is achieved by redundancy: adjacent elements contain 
several modules common to both. When developing models for CIG simulation, it is usually most convenient 
to work around a local origin that may come from a map reference or even architectural diagrams. Hodules 
developed in this way, must then be translated and rotated to their proper position in the gaming area. 
It is often inefficient to perform those translationsand rota- tions in real time unless, of course, 
the object is being instanced. And, since the position of a module may be Imown most accurately in terms 
of its spherical coordinates, it is appropriate that the software used for data base development have 
access to the mapping projection that is used in real time. If the system includes a blended projection, 
then the points of tangency, if not the primitive projections themselves, should reside in the data base, 
not in the software, and thus be under model control. Figures 13 through 18 are photographs of an implementation 
based on modularization. The first  four show the data base as it would appear in training; the second 
two show the same data base from an unusually high altitude to demonstrate the change in the model 
precipitated by view position management. Conclusion In this paper, we have presented two rather diverse 
subjects; the one, a mathematical concept with strong ramifications in interactive graphics, the other, 
an application in "simulation" graphics. Both have a common thread in that they descended from considerations 
involving Shepard' s formula. They may be considered separately since each can stand on its own merits, 
or collectively as an example of the diversity of one given subject, namely, Shepard's Formula. Acknowledgments 
 We would like to express our appreciation to the Evans and Sutherland Computer Corporation, in particular, 
R. Moon and C. Francis, and our col- leagues in the NOVOVIEW Research and Development group. We would 
also like to thank M. Mantle for his encouragement. 240 Computer Graphics References i. R.K. Adler 
and P. Richardus, Hap Projections, North-Holland Publishing, 1972. 2. R.E. Barnhill (1977), Representation 
and Approx- imation of Surfaces, ~thematical Software III, edited by J.R. Rice, 69-118, Academic Press, 
Inc., New York.  3. C. Caratheodory (1911), Uber den Variabilitats- bereich det Fourierschen Konstanten 
yon Positi- yen harmonischen Furktionen. Ren, Circ. ~t. Palermo, No. 32, 193-217.  4. N. Draper and 
H. Smith (1966), Applied Regres- sionAnalysis, Jolm Wiley and Sons, New York.  5. G.M. Neilson (1974), 
Some Piecewise Polynomial Alternatives to Splines under Tension. Com~uter Aided Geometric Design edited 
by R. Barrahill and  R. Riesenfeld, 209-235, Academic Pass, Inc., New York.  6. D.T. Pilcher (1974), 
Smooth Parametric Surfaces, Ibid, 237-253.  7. D. Shepard (1965), A Two Dimensional Interpola- tion 
Function for Irregularly Spaced Data, Proc. 23rd Nat. Conf. ACH, 517-523.  Volume 15, Number 3 August 
1981   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806811</article_id>
		<sort_key>243</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>29</seq_no>
		<title><![CDATA[ANSI X3H3 Technical Committee on Computer Graphics (Panel Discussion)]]></title>
		<page_from>243</page_from>
		<doi_number>10.1145/800224.806811</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806811</url>
		<abstract>
			<par><![CDATA[<p>Several members of the American National Standards Institute (ANSI) Technical Committee X3H3 on Computer Graphics will present current issues of the standardization effort. The focus of the discussion will be the technical content of these issues.</p> <p>The purpose of the panel discussion is to encourage technical dialogue between X3H3 participants and interested parties outside of X3H3. To that end, the presentations will describe major areas of concern, avenues of attack, and proposed solutions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Standards</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011017</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Domain specific languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010919.10010177</concept_id>
				<concept_desc>CCS->Computing methodologies->Distributed computing methodologies->Distributed programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944.10011122.10002945</concept_id>
				<concept_desc>CCS->General and reference->Document types->Surveys and overviews</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
			<gt>Standardization</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P223671</person_id>
				<author_profile_id><![CDATA[81100626262]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bono]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Naval Underwater Systems Center]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P333306</person_id>
				<author_profile_id><![CDATA[81100117553]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Puk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Megatek]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P333989</person_id>
				<author_profile_id><![CDATA[81332522908]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ted]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reed]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Los Alamos Scientific Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39079178</person_id>
				<author_profile_id><![CDATA[81332528147]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Skall]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Bureau of Standards]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31100798</person_id>
				<author_profile_id><![CDATA[81450594467]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wright]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ISSCO]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 Panel Discussion on Standards ANSI X3H3 TECHNICAL 
COMMITTEE ON COMPUTER GRAPHICS Several members of the American National Standards Institute (ANSI) Technical 
Committee X3H3 on Computer Graphics will present current issues of the standardization effort. The focus 
of the discussion will be the technical content of these issues. The purpose of the panel discussion 
is to encourage technical dialogue between X3H3 participants and interested parties outside of X3H3. 
To that end, the presentations will describe major areas of concern, avenues of attack, and proposed 
solutions. Questions to be addressed include: Graphics Functionality (Puk) * What graphics data structures 
should the standard support to meet the needs of a wide variety of applications and display hardware? 
 * How should attributes be bundled to best serve the various application areas and yet retain device 
independence and program portability?  Programmers Minimal Interface (Wright) * What features comprise 
a "minimal" set?  * What are the criteria for inclusion and exclusion of features in a minimal set? 
 * How can a minimal standard be upwardly compatible with a full (or rich) standard without either being 
trivial or containing too many complex concepts and functions?  Virtual Device Interface and Metafile 
(Reed) * What is the architecture of the virtual device interface (VDI)?  * How is inquiry handled? 
 * What kinds of required functions are under consideration?  * Should "non-required," but standardized 
functions be specified?  * What are the purposes of a graphics metafile?  * How does the metafile 
fit into the VDI architecture? Conformance and Language Binding (Skall) * What does it mean to be a 
"conforming program"? a "conforming processor"?  * What are the prospects for "certification"?  * Are 
there any differences in conformance requirements for language binding  as a subroutine package and 
as an extension to an existing programming language? Chair: Peter Bono, Naval Underwater Systems Center 
 Panelists: Richard Puk, Megatek Ted Reed, Los Alamos Scientific Laboratory Mark Skall, National Bureau 
of Standards Tom Wright, ISSCO 243 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806812</article_id>
		<sort_key>245</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>30</seq_no>
		<title><![CDATA[Animating facial expressions]]></title>
		<page_from>245</page_from>
		<page_to>252</page_to>
		<doi_number>10.1145/800224.806812</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806812</url>
		<abstract>
			<par><![CDATA[<p>Recognition and simulation of actions performable on rigidly-jointed actors such as human bodies have been the subject of our research for some time. One part of an ongoing effort towards a total human movement simulator is to develop a system to perform the actions of American Sign Language (ASL). However, one of the &#8220;channels&#8221; of ASL communication, the face, presents problems which are not well handled by a rigid model.</p> <p>An integrated system for an internal representation and simulation of the face is presented, along with a proposed image analysis model. Results from an implementation of the internal model and simulation modules are presented, as well as comments on the future of computer controlled recognition of facial actions.</p> <p>We conclude with a discussion on extensions of the system, covering relations between flexible masses and rigid (jointed) ones. Applications of this theory into constrained actions, such as across rigid nonmoving sheets of bone (forehead, eyes) are also discussed.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P333817</person_id>
				<author_profile_id><![CDATA[81339522586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Platt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer, and Information Science, University of Pennsylvania, Philadelphia, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP95031390</person_id>
				<author_profile_id><![CDATA[81452608047]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Norman]]></first_name>
				<middle_name><![CDATA[I.]]></middle_name>
				<last_name><![CDATA[Badler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer, and Information Science, University of Pennsylvania, Philadelphia, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Badler, Norman I., J. O'Rourke, S. Platt, and M. A. Morris. Human Movement Understanding: A Variety of Perspectives. Proceedings of the First Annual National Conference of Artificial Intelligence, 1980, pp. 53-55.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Badler, Norman I., Joseph O'Rourke, Stephen Smoliar, and Lynne Webber. The Simulation of Human Movement by Computer, Movement Project Report No. 14, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA. September 1978.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356760</ref_obj_id>
				<ref_obj_pid>356757</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Badler, Norman I., and Stephen Smoliar. Digital Representation of Human Movements. ACM Computing Surveys 11, March 1979, pp. 19-38.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Baker, Charolette. Regulators and Turn-Taking in American Sign Language Discourse, from New Perspectives on American Sign Language, Academic Press, 1977.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Baker, Charolette, and Carol A. Padden. Focusing on the Nonmanual Components of American Sign Language, from Understanding Language through Sign Language Research, Academic Press, 1978.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Birdwhistell, Ray L. Kinesics and Context, University of Pennsylvania Press, Philadelphia, PA. 1970]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807492</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Calvert, T. W, J. Chapman, and A. Patla. The Integration of Subjective and Objective Data in the Animation of Human Movement. Computer Graphics, vol. 14, no. 3, July 1980.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Ekman, P. and W. Friesen. Facial Action Coding System. Consulting Psychologists Press, Palo Alto, CA. 1978.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563870</ref_obj_id>
				<ref_obj_pid>965141</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Hackathorn, Ronald J. ANIMA III&#8212;A 3-D Color Animation System. Computer Graphics, vol. 11 no. 2, Summer 1977]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Hutchinson, Ann. Labanotation. New York: Theatre Arts Books. 1970.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[O'Rourke, Joseph P. and N. I. Badler. Model-Based Image Analysis of Human Motion Using Constraint Propagation. IEEE PAMI, 2(6), Nov. 1980, pp 522-536.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907312</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Parke, Frederic I., A Parametric Model for Human Faces. Ph. D. dissertation, University of Utah, Department of Computer Science, 1974.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Platt, Stephen M, A Computer Model for the Simulation of Human Faces. MSE thesis, Department of Computer and Information Science, University of Pennsylvania, 1980]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Sutton, Valerie. Sutton Movement Shorthand: Writing Tool for Research, The Movement Shorthand Society Press, Newport Beach, CA. 1978.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Animating Facial Expressions Stephen M. Plait Norman I. Eadler Department of Computer and Information 
Science University of Pennsylvania Philadelphia, PA Abstract: Recognition and simulation of actions 
performable on rigidly-Jointed actors such as human bodies have been the subject of our research for 
some time. One part of an ongoing effort towards a total human movement simulator is to develop a system 
to perform the actions of American Sign Language (ASL). However, one of the "channels" of ASL communication, 
the face, presents problems which are not well handled by a rigid model. An integrated system for an 
internal representation and simulation of the face is presented, along with a proposed image analysis 
model. Results from an implementation of the internal model and simulation modules are presented, as 
well as comments on the future of computer controlled recognition of facial actions. We conclude with 
a discussion on extensions of the system, covering relations between flexlble masses and rigid (Jointed) 
ones. Applications of this theory into constrained actions, such as across rigid nonmovlng sheets of 
bone (forehead, eyes) are also discussed. Introduction: Representation and simulation of gross motions 
of the human body have been investigated [1],[2],[3], modelling the body as rigid segments, and controlling 
it with simulated processors placed at each joint. This concept has been extended in a human movement 
recognizer described by O'Rourke [11], which uses constraints placed on the figure by the joint actions 
of the interconnecting rigid limbs. These techniques, however, cannot be easily extended to modelling 
non-rlgid masses. The work pEeseuted here deals with the problems involved in the manipulation of such 
deformable objects. The solution proposed incorporates the actual causes (motivators) of the actions, 
rather than Just simulation the resulting actions directly. The basic goal of this research is to devise 
an efficient and accurate model of the human face, and to develop or adapt a notational system to This 
research was supported in part by NSF grant MCS78-07466. Permission to copy without fee all or part 
of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. encode actions performable on the face. This 
notation system should lend itself not only to ease of reproduction, but should also be usable as a 
human entry system, and he sufficiently rich and well-deflned to make computer recognition (via computer 
controlled camera) possible. This research forms a part of a larger project involving the recognition 
and simulation of American Sign Language (Ameslan or ASL). As described in Baker [4] and Baker and Padden 
[5], ASL communication can be broken down into five channels: (I) the hands and arms, (2) the head, 
(3) the face, (4) the eyes. and (5) the total body posture. Our approaches to simulating the above item~, 
with the exception of (4), are described in Badler et. el. [I]. We have discovered [I], [3] that it 
is usually mOre convenient to maintain two separate notations for body and action representation, and 
maintain a ~ranslatlon scheme between them. Actlon-based notations are the lower level notation --they 
are more oriented towards the physical image of the object, as in a camera image. Structure-based notations 
are of a higher level --they are based around the logical structure of the object, and are more suited 
to object manipulations. As such, the facial action recognition to simulation system was divided into 
three logical sections, a camera processor, an internal model manipnlator, and a face simulator. &#38;#169;1981 
ACM O-8971-045-1/81-0800-0245 $00.75 Computer Graphics Volume 15, Number 3 August 1981 Input Image I 
I Camera Processor I [High Level Representation] I V Internal Model Manipulator I [Low Level Representation] 
 I Simulator I T Output Image Fig. I: The Basic System (general outline). Initially, all that is 
available is an input from a computer controlled camera. The Camera Processor would manipulate this data, 
determining which facial actions are being performed and interpreting them in terms of the motion representation 
notation. This output (the motion representation notation) is passed to the internal Model Manipulator. 
This module takes the motion representation and prepares it for simulation by converting it into the 
structures needed by the simulator. Finally, the last module, the Simulator, uses this data to manipulate 
its model of the face, producing suitable output. Partitioning the processing in this fashion results 
in two tle-polnts where the data has been partially processed into a compact form --both before and after 
the Internal Model Manipulator. Processing may be halted at either point, allowing storage of the image 
for future use, addltions/modiflcatlons, or transmissions to distant sites for reconstruction.  a communicatlon-notation 
system. This system was designed to record the detailed motions and actions o£ body language and movement 
in interpersonal communlcatlon. It has a large vocabulary of (pictorial) symbols for many actions and 
positions of the body and face, but lacks a scheme for generalization of interactions between facial 
gestures. The last system considered was the Facial Action Coding System (FACS) [8]. Unlike the previously 
described systems, FACS is not graphically oriented. Rather, it describes the set of all possible basic 
actions performable on a human face. Some sample actions are Inner Brow Raiser, Outer Brow Raiser, and 
Lid TiKhtener. Each basic action (called an Action Unit, or AU) is a minimal action in the sense that 
it cannot be broken up into smaller actions. AUs interact in several different fashions to build up a 
full facial expression. The AUs are designed to be closely connected with the anatomy of the face. Each 
AU is caused by a minimal number of muscle contractions or relaxations. This further strengthens its 
claim to minimal actions since each AU is controlled by either a single muscle or a small set of closely 
related muscles. The description of a facial expression using FACS is vastly different from descriptions 
in any of the previously considered systems. The notations of those systems are symbolic or pictorial, 
and while this aids the human user by providing a highly mnemonic symbology, it tends to make computer 
storage of the notation difficult. FACS, on the other hand, describes for each frame (facial expression 
to be analyzed) a llst of the names of AUs being performed. This format is much more suitable to computer 
usage, although it increases the learning difficulty for human notators. FACS itself is not easily extensible 
to other domains --the definitions of the AUs are tied intimately to the human face. However, the theory 
hehlnd FACS, notating based on minimal performance units (in turn based on anatomical structure), ca__nn 
be applied to other domains. Action-based Notations: We examined several action-based notation systems 
for applicability to the problem. Important features for such a system include ease of (computer) adaptation, 
completeness of the notation, and the system's extenslbillty to more general problems. Labanotatlon 
[I0] was examined first as it has already been used in several human motion studies [3], [2], and [7]. 
Labanotation is basically a dance notation scheme, used to record basic human motions. However, there 
is little in the notation for the recording of facial actions. Another notation system is Sutton Notation 
 [14]. Sutton Notation is more pictorially based, but still records actions as changes in body position 
on a staff. However, unlike Labanotatlon, Sutton Notation also has the capability to notate facial expressions. 
Sutton also mentions a project concerning the computer implementation of this notation. Birdwhistell 
[6] describes another scheme for  Structure-based Representations: As previously mentioned, there is 
a second level of internal representation: that of the actual face. It is at this second level that the 
action representations are interpreted and the simulation is performed. The internal structure of the 
face's representation must be well chosen, since the quality of any images produced will rest on this 
representation. For ease and accuracy of translation, the representation must also be easily accessed 
from the facial action coding. The three techniques considered below can all be thought of as variations 
of a single representation --simulating the face by a patchwork model. They only vary in the complexity 
of the techniques used to perform the facial actions. The first of the models is the simple 2D surface 
patch technique. In this model, the head is broken into small patches of "skin', as suggested by [9]. 
A facial actionunder this system would consist of simple warping of a subset of the set of skin patches: 
 246 Computer Graphics Volume 15, Number 3 August 1981 Facial Action == ((Skin-Patch-Number Traus Rot) 
(Skin-Patch-Number Trans Rot) where Skin-Patch-Number is an individual patch identifier, Trans is the 
triple (dx dy dz) --a 3D translation, and Rot is the triple (rho theta phi) --the rotation the patch 
undergoes. Parke [12] took a parametric approach to defining the face and representing facial actions. 
This was an early but impressive approach to computer simulation of the human face, classifying the face 
by a set of parameters defining size attributes of the facial subsections. Expresaiona were coded as 
variationS of these parameters, and animations were performed by interpolating along the change in expression. 
Using this technique, to encode subtle interactions of the face or more complex actions such as bulge 
creation would require increasingly larger and larger sets of parameters. The system does handle Jaw 
motions remarkably well; this is due to the data being "bard--wlred" in --any effects of an action such 
as this must be previously known (such as the effect of a widely open Jaw on certain cheek and llp actions), 
so in this respect the system loses some generality. (It should be noted that we do not currently handle 
jaw actions --to process these "naturally" is a very difficult problem.) The last representation considered 
involves the complete low-level simulation of the face. It is possible to simulate points on the skin, 
muscles, and bone by a set of interconnected 3-dimenslonal networks of points, using arcs between selected 
points to signify relations. The outermost level or surface is the skin The skin can be viewed as a 
continuous 2-dimensional surface, warped and distorted around an ovoid. To simulate the skin, points 
with 3-dlmenslonal coordinates are selected for the surface. As in Farke's model, points are more concentrated 
around the most detailed sections -- the eyes and the mouth (most expressions use mainly these portions 
of the face). Arcs connect a point to all of its "close" neighbors --that is, any neighbors such that 
motion of the skin point would affect the position of the neighboring point. At the innermost level 
there is the bone structure. This is fairly simple to implement -- it is an inflexible surface at some 
distance below the skin (ignoring the Jaw and its related motions). Between the bone and skin layers, 
and spanning the space between them, are the muscles. A muscle is a group of points (muscle fibers) with 
an arc stretching from each fiber-polnt to a point on the underlying bone, and another stretching from 
the fiber-point to one or more skin points. It is necessary to keep various pieces of relevant information 
on the arcs. As the different sections vary in their elasticity (resistance to change in position or 
arclength), information such as length parameters and elasticity information must be stored on the arc. 
 The basic action performable on the neLwork is  the application of a force, or tension, to a select 
portion of the net. From here, the force propagates outward, affecting more and more distant sections 
of the model. Since the tensions are integral to the manipulation of the face, theae networks are referred 
to as "Tension Nets". Th___eeDesign of the System The Selected Structures: When we decided to select 
a pair of representations, we first chose the representation of the face, and then chose the main representation 
which would work best with the facial representation. Since all of the motion schemes could be used in 
some fashion for the image analysis (with appropriate extensions where necessary), it seemed to make 
sense to choose the motion scheme which worked best with the most accurate and usable facial representation. 
 Of the three examined facial structures, the one which can yield the most usable representation of the 
face is the method of tension nets. This is based primarily on the fact that it is a "naturally" based 
system --the "handles" on the represented facial structures are exactly the same as the motivators of 
facial actions --the muscles. Any nuances of the face should naturally fall out as a result of the simulation. 
This, as we later demonstrated, is indeed the case. When comparing the various notatlon schemes to the 
tension net representation for compatibility, one notation stands out as the clear and obvious choice 
--FACS. The output of FACS is a llst of currently performed Action Units. Each AU corresponds directly 
to one or a few muscle contractions or relaxatlons~ precisely the input the tension net scheme requires. 
Thus, FACS-tenslon nets offer the following features: * Any performable action can be simulated. If 
an action can be notated, the notation yields a simulation The notation was designed to be able to handle 
any combination of actions. Naturally based system Analysis and simulation of the face are baaed on the 
actual structures of the face. Therefore, any constraints or peculiarities of the real face should appear 
within the system. Close relation between the notations. This insures a simple translation between notation 
schemes. Close relation between the causes of the actions and their simulation. If a muscle causes 
an action, then in the simulation, the simulated muscles will cause a similar action. Independence of 
FACS from any particular face. The notational scheme was defined to key on changes in features when compared 
to the subject's base (neutral) face, a well-defined item. Uniqueness of decomposition in FACS. Since 
each facial expression has a unique representation.Ln FACS, this insures repraducability of processing, 
as well as cross .checking of processing with human 247 Computer Graphics Volume 15, Number 3 August 
1981 notators. Efficiency of representation. Two very  compact notations signifying the action are 
 available --the llst of AUs being  performed, and the status of the muscles of the face.  Extensibility 
of the theory. The general theory of simulating an object by the causes of its changes can be extended 
to cover other nonrigid objects under different circumstances. (See the section Unsolved Problems.) 
The System's Subprocesses: The processing performed by each segment can now be further described. The 
Camera Processor (not yet implemented) would scan the input image for  certain facial features. From 
this list of features, the list of AUs being performed can be determined. This is a nontrlvial but well 
defined process, as AUs are not exclusionary in their interactions. AUs can combine to produce secondary 
features, they may mask one another, or the performance of one may exclude performance of another. Examples 
of these processes are (respectively) certain sets of llp actions that modify the cheek shape when 
performed together (when the cheek would not be changed by the separate actions); eyebrow raising in 
a fashion that makes detection of certain eyelid actions impossible; and "contradictory" actions such 
as eyelid droop and eyelid open.  The AU Parser takes the list of AUs and their interactions and looks 
up the muscle changes for each one. In certain cases, muscle actions have to be combined to give an 
overall picture of the muscle status of the face.  This llst of muscle changes is then passed to the 
facial Simulator, which performs any necessary tensing or relax/ng of muscles. Since the status of the 
face is saved between camera frames, only actual changes in muscle actions are used as input to this 
stage. This also insures that parallel actions will be performed in parallel (at the same camera frame), 
while sequential actions are performed sequentially. This is important, since in many cases the effect 
of an AU depends on what has been previously performed (the previous structure) and on what is simultmneously 
being performed (allowing for offsetting actions over sections of the face). The Data Structures: We 
constructed the  representat/on of the face in a bottom-up fashion, by starting at the lowest level 
~a single location on the skin), and building successively higher and higher levels of interaction 
until sufficiently high level constructs (much as the AU, the basic manipulative unit of FACS) were 
obtained. These  data structures and interactions are described in more detail in Platt[13].  The 
simplest structure is the point, a 3D location. It represents a single coordinate either on the skin, 
or as part of the underlying structure (muscle or bone) --it is a tissue location. Arcs are used to connect 
points --an arc exists between two points pl and p2 iff pl ad~ p2. Arcs contain information on the nature 
of the matter between the two points --basically, pertaining to the elasticity (spring constant) of 
the area between pl and p2. The basic action of force application to a point treats the face as a network 
of springs, joined at the points. When a force df is applied to a point p, the change in location is 
computed by d ! = d~ / k where k is the sum of the spring constants at the point. The simplest logical 
structure for organizing forceappllcations is the muscle fiber. It consists of a muscle point, to which 
the force will be applied, a bone point, forming a solid base for the contraction, and one or more skin 
points, the head(s) of the fiber. Each fiber consists of the muscle point and the magnitude of the force 
to apply at that point. A simple muscle fiber is shown in figure 2. From here, fibers are collected 
into muscles, the ~asic unit of which contractions are performed. So when a contraction is being specified, 
each individual fiber has its distinct force applied along it, all performed in parallel. (This is further 
described in the section Algorithms.) The highest structure needed in the simulation is that of the 
AU. Since FACS defines AUs in terms of muscles, each AU consists of of a list of muscles and relative 
magnitudes. (For example, an AU pulling a muscle at half its maximal force would have a magnitude of 
0.5.) It should be noted here that it is at this level of notation that translation from FACS notation 
to lower-level notation takes place --a FACS action specification is translated into a list of muscles 
and forces, and from there to individual polnt-force combinations. 9   B M~~. '''  ~Y" "o Fig. 2: 
Muscle Fiber ~.-:.--~. ..... : .... ,". T I::', I A;: :% ,,', ,., Ir . J~ , o , # o "k z ~, o : . 
 t .V-"~'" "~ .... 4 Fig. 3: Muscle  Algorithms: In this section we shall describe the basic techniques 
used to apply a simple muscl~ pull to the curren~ facial_structure. The algorithm for multiple simultaneous 
pulls is a simple extension of this, and is also described. The contraction algorithm is an iteratlve 
simulation of what is really a continuous event. When sufficient resolution is used for each pull, this 
proves to be a sufficient simulation. When a simple fiber contracts, a force is applie d to a muscle 
point in the direction of its tall (bone point). This force causes a displacement of the muscle point; 
the amount of the displacement varies with the elasticity .of the flesh at that point. The force is then 
reflected along all arcs adjacent to the point; these reflected forces are then applied to their corresponding 
adjacent points, In this fashion, a force is propagated out from the initiating point across the face. 
 In actuality, we are dealing with sets of points and forces at all times. To slmulate ~ single muscle 
or a set of muscles contracting,, we only consider the sets of all the fibers (and their applied forces) 
from ALL the muscles currently contracting. As previously mentioned, a muscle consists of a set of fibers 
and their appropriate force-magnltudes. To contract a muscle we apply a force of the prescribed magnitude, 
along a unit vector stretching from bone to skin of the fiber. body, to the muscle (fiber) point itself. 
In a similar fashion, an AUcan be decomposed into a llst of muscles, each with an appropriately calculated 
portion of the magnitude (partial. contraction), to be further decomposed from muscle-magnltudes into 
polnt-forces. Animation: Applying a set of actions to a face is not done with one single application 
of the contraction algorithm, Rather, to help. distribute effects of simultaneous pulls over intermediate 
areas of the face, the contractions are.dlvlded into n contractlons each of force I/n, This is analogous 
to Euler's method for solution of. dlfferentlal equations --smaller and smaller step sizes yield results 
more and more interaction, at a computational cost. Animating a facial action is quite simple once this 
iteration is considered. The intermedlate results are all valid expressio~complexe&#38;, and, as such, 
the~intermedlate results are dlsplayedas separate frames. Since partial contractlona..of muscles are 
performable, temporally offset animations are easily.speclflable by splltting.up the AU's into the overlapping 
and non-ovarlappiu~ contractlon.times. This also. means that expressions can be performed at different 
rates even if occurring simultaneously. Results: A system was implemented using the above structures 
to perform (animate) facial actions on either a muscle or AU level. The output of this program was fed 
into a displ&#38;y program, resulting in an image of the arc-llnes on the Vector General 3404 (a.vector-orlented 
graphlc display device). .This image can be rotated about the X-, Y- and Z-axes for further examination. 
A series of AU contractions were performed to demonstrate the effects of various comblnatlons of actions 
on the farehead r~gian of. the f~ce. , The  contract/nns shown here are restricted to those of the upper 
face. Figures 4a, 4b, and 4e are different views of the neutral face. Muscles are displayed .in 4~ and 
.4b; ~e .(and most of the rest of the figures) have the m~scles omltted for reasons of clarity. The AUs 
defined and demonstrated below are: AU I|~ Inner brow raiser. Raises the inner brow~ by contraetlngthe 
inner fronrmlls muscle. The notatlon "RI" (figures 4c, 4d) is interpreted as the,performance of AU 1 
only on the right ~de. AU 2: Outer brow raiser. Raises the outer brow by contraction of the,outer frontalls 
muscle (figure .4g% 41). Without training in .the performar~e of AU 2, many people .will also contract,AU.4, 
as in figure 4j. AU ~: Brow Lewerer, Imwers the inner brow by the contract.lon of-the corrlgator and 
pyramidus nasl muscles, .It also may create wrinkles at the root of the nose and/or bulges running from 
mid- to inner- eyebrow. These instances of feature creation are demonstrated in figures 4m (neutral face) 
and 4n (AU 4 contracted) in the ~iclnlty of the glabella. AU.6: Cheek Raiser and Lid Compressor. Raises 
the cheek and narrows the eye opening by contracting the outer portion of the orbicularis ocularis. _Figure 
4k shows a one-aidedcontractlon (AU R6), performed on the model's right side, Figure 41 demonstrates 
the combination 4+6, raising the cheeks, lowering the brow, and causing a buckling (creation of a fold) 
around the inner portion of .the upper eye opening. AU 7:. Lid Tightener. Pulls upper and lower eyelids 
towaxds, the inner corner by contracting the inner orhlcularls ocularls. AS our model does not currently 
have eyellds, the e~fects of this AU are minimal and have not been demonstrated.  Camera Recognition: 
Currently, research is being initiated to complete this system by cm~stru£tingan, lmage analysis module. 
We believe that although diffieul~, thlsmodule is realizable, as there are several constraints upon the 
Cgeneral) image analysis problemwhich we can apply. First, the fa¢e has a defined str~cture. In lti~lly, 
we plan to deal wlthallgned images (actor.performlng actions faee.~forward into camera;, predefined images~ 
etc,)., thus the general features of the face willappear in constrained locations,..Interframe alignment 
and locatlon will be simplified by this constraint. In addition, this further constrains the location 
of static (eyes, etc.) and dynamic (wrinkles, mass motions) features. There exists a "base" or neutral 
image to start with. Th/s is the expressionless actor, and provides abase against which_later images 
can be compared for featur~-dlfferences. Finally, the FACS features are predesignated: we know where 
to look and what to look for. Once features are f~und, the. tranalatlon to FACS is a 249 (a) no expression, 
(c) AU RI, with muscles (d) AU RI  (b) no expression, with muscles, with muscles profile (g) AU R2 
(h) AU 4 (e) no expression ~r) AU I (i) AU 2 (J) AU 2+4 (k) AU R6 (i) AU 4+6 Fig. 4: Expression Examples. 
  4P 4P (m) no expression, chin view (n) AU 4, chin view well-defined, although somewhat complex, 
process. Spme Assorted Problems: Initially, several problems appeared which we thought would have to 
be handled by special case processing. This was later shown not to he the case, as all desired results 
fell out naturally as a result of the simulation, This provided further evidence that our system was 
similar enough to FACS and the (real) face tm be confidently used for general animation and simulation. 
 In the design and learning of FACS, some time is spent on the different forms of AU interactions. The 
processing of alternating (either of 2 AUs can be alternately scored), parallel (addltlonal side effects 
due to several closely related AUs), and nonlnteracting seemed relatively easy, however, the problem 
of AU masking was still left. An AU will mask anothen AU when previous performance of the first AU makes 
the performance of a second AU uudetecteble. Essentially, the signs characterizing the second AU are 
either present, or the skin conditions present after the first AU is performed (bulges, wrinkles, skin 
stretching, etc.) are such that any additions of the second AU are undetectahle, An example of this is 
first smillng, then raising one's cheeks, as opposed to just performlng~he cheek raiser. The cheek r~iser 
naturally pulls up the corner of the mouth. If a smile has already been performed, the mouth corners 
are already raised --the the cheek raiser does not cause an additional raising. This problem does u~t 
surface in our model as a result of the way we process skin elastlclties. If one AU is sufficiently strong, 
the skin will be stretched to the point where any additional force will result in no change -- ~/k will 
approach zero as K becnmes very large. A second problem involved the creation of bulges, furrows, and 
wrinkles in the skin, These three phenomena are basically caused by the same state --two forces press 
the points of the skin towards each other, causing a buckling action. Again, our model handles this well 
--they will be created if a point is pushed over another, paint., since the reflected force will apprnach.ze=o 
as the angle of reflection (being Increased as the point approaches) approaches a 90 degree angle.~ 
Unsolved Problems: At the current time, there  are several topics involvlng the areas ,this research 
involves that are suggested_by,r~me~_wor~k, but have ,not been approached. First, the system is not 
ideal with respect t~  muscles following the flow of a bone sheet, such as the area ~t the junctlan 
of an.eye socket and the brow above it. The sudden change of direction will constrain muscle actions 
in a manner not handled well by the model as currently implemented --the flesh should flow over the 
bone, bu~ not through it. At present, the bone is Just represented as tle-polnts for the muscle fibers; 
 there is no concept of a solid mass to be dealt with. The simplest way to. code i~theae constraints 
would be to imcreaae.the complexity of the muscle flher, encoding additional mon0cle action constraints, 
changing the muscle fiber from as represented in figure 3 to that of figure ~, i J J I Fig. 5: Improved 
Muscle The additional Muscle points are used to define the.flow of action of the muscle, while the 
intermediate Bone points can constrain the vector actions of thelr related muscle points, thereby forcing 
the muscle over a bone sheet. Also, Jaw actions are not currently handled by this system. It is believed 
that they are a clear extension of the structures involve~ by treating the bone as a "special case" of 
action, allowing the muscle actions to pivot the bone around a set of axes. This action would, of course, 
be propagated through the bone to other connected muscles, causing a. stretching action along the skin, 
and rendering other actions unperformable. These two problems are combined when dealing with cartilaginous 
areas, such as the nose. Here, muscles flow over and around the cartilage segments; in addition, they 
can force the cartilage to move, resulting in a seml-rlgld actlon. Actions of the cheeks, such as puffing 
and +sucklng~ were also not handled. To process these actions would require a more complex model of the 
face and~ head~ involving fluid (air) filled chambera, the actions .of. the pressures within, and the 
effects of the bone/stretched akln upon the shapea of .the ~hamhers Also not investigated were totally 
nonrigid objects (the feelal actions rely on the underlying bone structure for a baae for the aetlons)~. 
An example of Ch~s within the face,is the tongue and its rela~ed actions and. interactions with the llps 
and .cheeks ,(.the contact problem)~ as well as pure tongue moz~ons~. Another interesting example would 
be a te~al heart slmula-tion, enebllng the testing of varlous sceneries of defects/changes of the s~ucture,~ 
and their effects om the performance. This. would also involve actions ~f pressure chambers as the. 
b~od retailed the various sections of the heart. Conc.lusloD~: .We_have introduced a model of the human 
face, ~-~ilizlng a theory which can be extended to s~mulate and animate any nonrigid object~ whose pexform~le 
a~tions are classifiable by primitive, interacting unII.s of ~tion. A slmula~, fnr thls. has bee~ implemented~ 
and over~la/d..with .I not~t~ fnr aetlons of the face. Sem~ .examples of the outpu~.fxom~ the system 
were pzesentedv and possible extensions_ and other appllcaEiene were suggested. [11 [2] [3] [4] [5] 
 [6] [7] [81 [9] [10] [il] [12]  [13] [14] References Badler, Norman I., J. O'Rourke, S. Platt, 
 and M.A. Morris. Human Movement Understanding: A Variety of Perspectives+ Proceedings of the First 
Annual National Conference of Artificial Intelli~ence+ 1980, pp. 53-55. Badler, Norman I., Joseph 
O'Rourke, Stephen Smollar, and Lynne Webber. The Simulation of Human Movement by Computer, Movement Project 
Report No. 14, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, 
PA. SepHember 1978. Badler, Rot man I., and Stephen Sm~liar. Digital RepxesentatlonofHumanM~vements. 
AC___MM C~mpu¢ing ~urveys il, March 1979, pp. 19-38. Baker, Charolette. Regulators and Turn-Taklng 
in AmericanSign Language Discourse, from New Perspectives on American Sign Language, Academic Press, 
1977. Baker, Charolette, and Carol A. Padden. Focusing on ~he Nonmanual Componente of American SignLanguage, 
from Unde/~tandin~ Language through Sign Language Raaearch, Academic Press, 1978. Birdwhistell, Ray 
L. Kinesics and Context, University of PennsylvaniaPress, Philadelphia, PA. 1970 Calvert, T. W, J. 
Chapman, and A, Patla. The Integration of Subjective and Objective Data in the Animation of Human Movement. 
 Compu~.er G=aphics, vol. 14, no. 3, July 1980. Ekman, P. and W. Friesen. Facial Action Coding System, 
Consulting F~ycholzgists Press, Polo Alto, CA. 1978. Hackathorn, Ronald J. ANIMAIII -- A 3-D Color 
Animation System. Compute= Graphics, vol. II no. 2, Sum~er 1977 Hutchinson, Ann. Labanotation. NewYork: 
 Theatre Arts B~oks. 1970. O'Rourke, Joseph P. and N. I. Badler. Model~Based Imag~Analysis of Human 
Motion Using Conatraint Propagation. -IE~EP~[I, 2(6), Nov. 1980, pp- 522-536. Parke, Frederic I,A 
Parametric Model for Human Faces. Ph. D. dissertation, University of Utah, Department of Computer 
Science, 1974. Platt, Stephen M, A Computer Model for the Simulation of HumanFaces. MSE thesis, Department 
of Computer and Information Science, llDlv~raiHy of ~emn~ylvania, 1980 Sutton, Valerie. Sutton MovemenL 
Shorthand: Writing .T~ol :fG~ Research, The M~vement  Shorthand Society Press, Newport Beach, CA. 1978. 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806813</article_id>
		<sort_key>253</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>31</seq_no>
		<title><![CDATA[Merging and transformation of raster images for cartoon animation]]></title>
		<page_from>253</page_from>
		<page_to>262</page_to>
		<doi_number>10.1145/800224.806813</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806813</url>
		<abstract>
			<par><![CDATA[<p>The task of assembling drawings and backgrounds together for each frame of an animated sequence has always been a tedious undertaking using conventional animation camera stands and has contributed to the high cost of animation production. In addition, the physical limitations that these camera stands place on the manipulation of the individual artwork levels restricts the total image-making possibilities afforded by traditional cartoon animation. Documents containing all frame assembly information must also be maintained.</p> <p>This paper presents several computer methods for assisting in the production of cartoon animation, both to reduce expense and to improve the overall quality.</p> <p>Merging is the process of combining levels of artwork into a final composite frame using digital computer graphics. The term &#8220;level&#8221; refers to a single painted drawing (cel) or background. A method for the simulation of any hypothetical animation camera set-up is introduced. A technique is presented for reducing the total number of merges by retaining merged groups consisting of individual levels which do not change over successive frames. Lastly, a sequence-editing system which controls precise definition of an animated sequence, is described. Also discussed is the actual method for merging any two adjacent levels and several computational and storage optimizations to speed the process.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer animation]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Merging]]></kw>
			<kw><![CDATA[Transformation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Hierarchy and geometric transformations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010244</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Hierarchical representations</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P329264</person_id>
				<author_profile_id><![CDATA[81538667156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bruce]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Wallace]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807360</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Catmull, Edwin, "A Hidden-Surface Algorithm with Anti-Aliasing", Computer Graphics, volume 12, number 3, August 1978.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807505</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Catmull, Edwin, and Smith, Alvy Ray, "3-D Transformation of Images in Scanline Order", Computer Graphics, volume 14, number 3, August 1980.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Levoy, Marc, Computer-Assisted Cartoon Animation, M.S. Thesis Cornell University, August, 1978.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807507</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Levoy, Marc, and Feibush, Eliot, and Cook, Robert, "Synthetic Texturing Using Digital Filters", Computer Graphics, volume 14, number 3, August 1980.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Madsen, Roy, Animated Film: Concepts, Methods, Uses, New York, Interland Publishing Inc., 1969.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807456</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Smith, Alvy Ray, "Tint Fill", Computer Graphics, volume 13, number 2, August 1979.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807457</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Stern, Garland, "SoftCel - An Application of Raster Scan Graphics to Conventional Cel Animation", Computer Graphics, volume 13, number 2, August 1979.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 MERGING AND TRANSFORMATION OF RASTER IMAGES FOR CARTOON ANIMATION Bruce A. Wallace Program of Computer 
Graphics Cornell University Abstract The task of assembling drawings and backgrounds together for each 
frame of an animated sequence has always been a tedious undertaking using conventional animation camera 
stands and has contributed to the high cost of animation production. In addition, the physical limitations 
that these camera stands place on the manipulation of the individual artwork levels restricts the total 
image-making possibilities afforded by traditional cartoon animation. Documents containing all frame 
assembly information must also be maintained. This paper presents several computer methods for assisting 
in the production of cartoon animation, both to reduce expense and to improve the overall quality. Merging 
is the process of combining levels of artwork into a final composite frame using digital computer graphics. 
The term "level" refers to a single painted drawing (cel) or background. A method for the simulation 
of any hypothetical animation camera set-up is introduced. A technique is presented for reducing the 
total number of merges by retaining merged groups consisting of individual levels which do not change 
over successive frames. Lastly, a sequence-editing system which controls precise definition of an animated 
sequence, is described. Also discussed is the actual method for merging any two adjacent levels and several 
computational and storage optimizations to speed the process.  KEYWORDS: computer animation, computer 
graphics, merging, transformation COMPUTER REVIEWS CLASSIFICATION: 3.41, 8.2 Introduction The production 
of cartoon animatio~ has always been a large-scale undertaking involving many man-hours of drawing, inking 
 (or xeroxing), cel painting, background painting, and frame-by-frame film recording using an animation 
camera stand. The traditional cartoon animation process takes the following steps from the original storyboard 
to the finished product: (i) Cartoon characters are drawn in pencil on separate sheets of white paper. 
A new and different drawing is made each time a change is to occur in the appearance of that character. 
 Permission to copy without fee all or part of this material is granted provided that the copies are 
not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the 
publication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 
&#38;#169;1981 ACM O-8971-045-1/81-0800-0253 $00.75 (2) Backgrounds which illustrate the settings into 
which a character is placed are painted on cardboard sheets. ~3) A tabular exposure shee t is written 
to irect the final assembly of cels and backgrounds and to specify how the camera stand should be configured 
for each frame. (4) Each pencil drawing is either traced in ink or xeroxed onto the face of a clear sheet 
of acetate known as a cel. At this point, only a black line drawing appears on an otherwise transparent 
sheet of acetate.  (5) The various areas of each transparent cel which are to appear as color are painted 
with an opaque paint, similar to coloring inside the lines of a child's coloring book. The painting is 
done on the reverse side of.the cel so that none of the black lines on the face are obliterated.  (6) 
Using the exposure sheet as a guide, the specified background and overlaying cel(s) are placed on the 
base of the camera stand. The lateral position of the base is   a~justed depending upon the desired 
position of the characters and the background. Generally, the cels and background can be moved independently. 
The camera is raised or lowered on its vertical support until the desired scaling of the combined image 
is achieved. It is not possible to enlarge levels of artwork independently without the use of an expensive 
multi-plane animation camera stand such as the one introduced by Walt Disney in 1936. The aperture on 
the camera lens can be varied depending upon the desired brightness of the frame. When these physical 
processes are complete, a single frame of animation is recorded onto film with the open and close of 
the camera shutter. A review of Madsen's book "Animated Fil~:Concepts,Methods,Uses" [5] is beneficial 
in understanding in greater detail how the traditional animation process operates. In recent years several 
computer-assisted cartoon animation systems have been developed, primarily for the purpose of quickening 
and improving various stages of the animation process. Two such systems, one at Cornell University and 
the other at New York Institute of Technology, along with several other installations, have shown cost 
savings and greater flexibility over traditional cartoon animation methods. In each of these systems, 
drawings are maintained either in vector format as streams of connected vertices, or in raster format 
as areas of pixel data. Advantages and disadvantages of the two approaches are discussed by Levoy [3]. 
This paper will assume the raster format in all further discussion. Methods for computer assistance in 
steps 4 -6 of the animation process have been developed for each system. Brief descriptions of computer-assisted 
cel creation and coloring processes (steps 4 &#38; 5) are followed by computer methods for assisting 
in the final camera work (step 6). A. Computer-Assistance in the Cel Coloring Process (Step 5) The 
time required for cel coloring has been reduced tenfold through the use of area flooding algorithms such 
as those introduced by Smith [6] and Levoy [3]. Such algorithms process the pixels of a raster image 
directly. An enclosed area of the image is quickly flooded with color simply by indicating one "seed" 
pixel within that area. Each color has an opacity value associated with it which refers to the extent 
with which that color obscures whatever is behind it. This value is used in the process of overlaying 
images, discussed later in this paper. Area flooding algorithms lay at the  heart of all cel opaquing 
programs. Because of the great time savings such a program offers to the animation process, it has been 
the major inspiration for incorporating computers for production assistance. B. Computer-Assistance 
in the Drawing-to-Cel Process (Step 4)  An opaquing program operates on raster data. This means the 
animators' drawings must be entered into the computer in pixel format to correctly interface with such 
a program. Typically, in a raster-based, computer-assisted animation system, a drawing is input as pixel 
data by using a video scanning camera connected to a digital frame buffer. One such implementation is 
discussed by Stern [7]. The contents of the frame buffer can be manipulated by an opaquing program, such 
as the one mentioned above. Much like the drawings, conventionally painted backgrounds can be input 
by scanning with the same type of video camera. Three separate passes are made using optical color filters 
(red, green, blue). Each pixel of the background image is stored as an RGB triplet. All pixels within 
a background are assumed to be fully opaque. C. Computer-Assistance in the Frame Assembly Process (Step 
6) All of the computer-assisted processes discussed above deal with the alteration or creation of individual 
pieces of artwork. Traditionally, these levels have been assembled into a final composite frame on a 
standard animation camera stand. However, since cels and backgrounds are maintained in the computer in 
raster format, a method is necessary for performing the task of building the final frames of animation 
from the pixel data.  Merging is the process of combining pixel-based artwork into a final display frame 
using digital computer graphics. Building a frame of animation on a traditional animation camera stand 
can be simulated on a computer. The pixel-based artwork is assembled and transformed into a final raster 
image. Methods for this simulation process will be the discussed in detail later in this paper. Once 
computed, the final image can either be written to a frame buffer and output onto videotape or recorded 
directly onto film using a precision film recorder. There are several advantages for using a computer 
to assist in this step of the animation process in this manner: (i) The allowable number of cel levels 
is no longer limited by the physical density of the acetate on which it is normally painted. The bottom 
level of artwork in a computer merged frame is as bright as all the levels above it. (2) The physical 
limitations of the camera stand, which have restricted the independent manipulation of each artwork level, 
are no longer present. Levels may be transformed to simulate any hypothetical camera stand, including 
the multi-plane camera. Merging Two Artwork Levels A. Image Data (Artwork) The majority of artwork 
processed by computer consists of "cels" colored by an opaquing program. Backgrounds, input by the three-pass 
scanning method, previously discussed, comprise the remainder of raster data used in producing an animated 
sequence. Each pixel of an artwork level is comprised of three color components (Red, Green, Blue) and 
an opacity value. The opacity refers to the percentage contribution of the pixel's own color versus whatever 
artwork is on the next lower level. While all pixels in a background are fully opaque, pixels in cels 
assume one of three states: i) Fully Opaque: OPACITY(PIXEL)=I This occurs in all colored areas. 2) 
Fully Transparent: OPACITY(PIXEL)=0 This occurs in all areas void of color. 3) Partially Opaque: 0 
< OPACITY(PIXEL) < 1 This occurs at the edges between opaque and adjacent transparent areas, producing 
soft, anti-aliased edges. B. The Merging Process Merging multiple levels of pixel-based artwork for 
cartoon animation can be thought of in terms of a Z-buffer algorithm. Typically, a Z-buffer algorithm 
is used for hidden surface removal of polygonal data. It determines what portions of each polygon are 
ultimately visible in the final raster display. A merging algorithm determines what portions of each 
level of artwork are ultimately visible in the final frame. Because a Z-buffer algorithm only considers 
each level once, an increase in the number of levels produces a linear increase in the time required 
to build a final display image. However, a Z-buffer algorithm can not offer one necessary feature desirable 
in a merging algorithm: acceptable anti-aliasing along edges. There are two cases where adverse effects 
can occur: (i) Catmull [i] has pointed out that Z-buffer algorithms do not produce correctly anti-aliased 
edges in a raster image. Only pixel percentage clipping, using precise edge information, generates the 
correct anti-aliasing of edges in all cases. Without the edge information, slightly incorrect results 
will occur when overlap occurs between two or more pixels through which edges pass. This occurs when 
combining levels of scanned data, because the scanning process does not yield precise edge information. 
Incorrect results are more noticeable in applications where a Z-buffer algorithm is typically used than 
in cartoon animation produced by a merging  algorithm. In the case of polygonal data, many edges may 
overlap. Unfortunately, as the number of overlapping edges increases, so does the error. Since many of 
these images are animated slowly, such errors are easily detected. This is not true in cartoon animation. 
The number of edges that actually overlap are minimal since the number of artwork levels is much less 
than the number of overlapping polygons found in a typical image produced by a Z-buffer algorithm. More 
importantly, frame to frame motion in cartoon animation is typically quick and tends to hide slightly 
incorrect color at intersecting anti-aliased edges. For these reasons, the results, while not perfect, 
are more than adequate for cartoon animation. (2) It has also been pointed out that a Z-buffer algorithm 
will produce inconsistent results at the edges if the order in which levels are pairwise combined is 
not bottom-up. This is due to the fact that the calculations involved in the Z-buffer algorithm are not 
associative. The option of merging levels of artwork, independent of the order in which the merges occur, 
is desirable in a computer-assisted animation system. Frequently, groups of adjacent levels of artwork 
will not change from one frame to another. It would be advantageous to be able to merge these levels 
together and keep the resultant for use in subsequent frames. This reduces the total number of merges 
which have to be performed for an animated sequence. Due to the nature of the animation, the order by 
which groupings of adjacent levels are merged and retained may not necessarily be bottom-up. Thus, the 
need to be able to assemble the final image in any order becomes important. An order-independent merging 
method has been developed which does not require the strict bottom-up merging order by which levels are 
paired and merged into intermediary images. It eliminates this restriction, producing no adverse effects 
in the anti-aliased edges caused by variations in the order by which merges between levels occur. Before 
presenting the method for order-independent merging, the basic logic for a simple merging process must 
be presented. This closely parallels the logic used in a Z-buffer algorithm. Only two levels are merged 
at any one time,  forming a resultant level with its own color and opacity components, which can in 
turn be merged with any other existing level. The calculations for resultant pixels along the edges have 
been included. The formulations in the simple procesS rely upon order dependence for correct results. 
Levels are merged in order from the background upwards. Since the background is known to be fully opaque, 
the opacity of the pixel resulting from each merge operation is equal to I. Thus, a merge is always between 
a fully opaque bottom level and a top level assuming any of the three opacity states outlined previously. 
To produce the merged pixel, RESULT, from the two adjacent level pixels, TOP and BOTTOM, the following 
algorithm is performed: Case 1 if OPACITY(TOP) = 1 then COLOR(RESULT) = COLOR(TOP) If top is fully 
opaque result assumes top color  else Case 2 if OPACITY(TOP) = 0 then COLOR(RESULT) = COLOR(BOTTOM) 
 If top is transparent result assumes bottom color  Case 3 else COLOR(RESULT) = {OPACITY(TOP) * COLOR 
(TOP)} + {{I -OPACITY(TOP)} * COLOR(BOTTOM)} If top is partially opaque result assumes interpolant 
between top and bottom colors regulated by the top opacity fi  The term "COLOR" refers to one of the 
three color components found at the pixel, either red, green, or blue. The algorithm is executed once 
for each component. The bottom-up method always involves building an image which contains all artwork 
from the background up to and including some overlaying cel. In the case of order independence, the process 
of assembling a final raster image is mathematically associative. This is true, because the act of cummulatively 
combining levels of given color and opacity is an additive process. A physical model which has these 
same properties has been constructed to clarify the formulas necessary for calculating a resultant pixel's 
color and opacity values from the known color and opacity values of two cel pixels which are being merged 
together: A pixel can be thought of in terms of the physical analogy of a homogeneous screen of a given 
density. A given percentage of the area contributes the  Volume 15, Number 3 August 1981 color of the 
screen material to the overall appearance of the screen. This percentage will be referred to as the "reflectance". 
The remaining percentage of the area is contributed from levels below. This will be referred to as the 
"tranmittance". t REFLECTANCE (% of plxel area oeaupled by the screen) xl"~" --:~-?~-'--"--~---;;~" 
 ~ ~ ~NsM~cE(~ofplx.l ..... t occupied by the--screen) REFLECTANCE = 1 -TRANSMITTANCE for 0 <= REFLECTANCE 
<= 1 Figure 1 -Screen Model of Pixel  The use of the terms "reflectance" and "transmittance" should 
not be mistaken for those found in discussions of optics and ray tracing. The two terms assist in conceptualizing 
the derivation of the final formulas. Overlaying any two pixels, each with its own opacity (or "reflectance") 
values, can be thought of as overlaying two screens of given density and color with a spatial integrator 
(diffuser) of 100% transmittance between them. A new interpolated color and resultant "reflectance" of 
this combination can be obtained from the original known densities and colors of the two screens. Using 
the following symbols, Figure 2 aids in deriving formulas for calculating the resultant components: 
CT = Color value for top pixel CB = Color value for bottom pixel CR = Color value for resultant pixel 
 RT = Reflectance of top pixel RB = Reflectance of bottom pixel RR = Reflectance of resultant pixel 
 TT = Transmittance of top pixel TB = Transmittance of bottom pixel TR = Transmittance of resultant 
pixel t I Spatlal Integrator (100% T ..... It. )i t l ~=~*~ Figure 2 -Cross-Section of Two Level Merge 
and Resultant Level The following calculations arrive at the solution for CR and RR in terms of the 
known values CT,RT,TT,CB,RB,TB. Note that  Computer Graphics Volume 15, Number 3 August 1981 the term 
"BELOW" refers to whatever is below the bottom pixel. Color Contribution of Resultant = (RR * CR) + 
(TR * BELOW), which is = (RT * CT) + (TT * (RB * CB)) + (TT * TB * BELOW) where (RT * CT) represents 
Color Contribution of Top, (TT * (RB * CB)) represents Color Contribution of Bottom, (TT * TB * BELOW) 
represents Color Contribution of Below, (RR * CR) represents Color Contribution of Resultant, (TR * BELOW) 
represents Color Contribution of Below. Knowing that TR = TT * TB, the color contribution from below 
can be subtracted from both sides of the equation. Substitutions for RR and TR are performed since RR 
= 1 -TR and TR = TT * TB. Thus, solving for CR and RR produces: (CT * RT) + (TT * CB * RB) CR -- 1 -(TT 
* TB) RR = 1 -(TT * TB)  The merging algorithm for the order-independent method is subject to the same 
three case conditionals as the bottom-up method, each governed by the opacity value of the top pixel. 
The first two cases remain the same. Case 3 (partial opacity) assumes the above formulas for CR and RR 
for the resultant color and opacity values. Figure 8 offers a visualization of the opacity mask for 
merging the cels from Figures 4 and 5. Pure white areas represent fully transparent pixels, while areas 
of pure black represent fully opaque pixels. The penumbre surrounding the opaqued areas represents pixels 
of partial opacity, producing soft, anti-aliased edges. The final image created by using this opacity 
mask to merge these cels with the background is also pictured in Figure 8. Transforming One Artwork 
Level  In addition to the task of merging levels of artwork into a final image, it is necessary to be 
able to manipulate each level relative to the "camera's eye". Zooming and panning are examples of commonly 
used operations. Variations in the intensity with which a level contributes to the final image are also 
necessary to simulate the effect of the camera's aperture. Thus, there are two general classes of transformations 
which affect each level of either original or merged artwork. The first class consists of geometric 
transformations and the second class consists of intensity transformations. Geometric transformation 
can be accomplished in several manners: (i) A standard 4 X 4 transformation matrix is constructed from 
scaling, translation, rotation, and perspective information For each pixel in the transformed image, 
an inverse transformation is performed using the 4 X 4 matrix. This determines the location on the untransformed 
image from which pixel information can be used for sampling purposes. Methods of bilinear interpolation 
and filtering are used to calculate the final color of the transformed pixel. Refer to a detailed discussion 
by Levoy, Feibush, and Cook [4].  (2) Alternatively, the transformation can be expressed as an x-pass 
and a y-pass. Filtering is performed in only one direction at a time. Such a two-pass stream processor, 
introduced by Catmull and Smith [2], has the same effect as the 4 X 4 transform above, but reduces the 
total compute time required to transform a full raster image.  Intensity transformation can be accomplished 
by altering the opacity component of each pixel by a given factor. An intensity transform performed on 
one level of artwork will either increase or decrease the extent to which that level contributes to the 
final raster image. Assembling Composite Images  The basic method for merging any two adjacent cels 
and transforming the resultant image have been presented. It is desirable to be able to specify to the 
computer many cels and accompanying transformations for automatic assembly of a final composite image. 
For this reason, a data structure which models any animation camera configuration is necessary. There 
are two components to consider when devising a structural model for a physical animation camera stand: 
 (i) Artwork -One level of artwork, whether it be cel, background, or merged resultant is the basic material 
unit upon which the camera stand operates.  (2) Operations -For the purposes of cartoon animation, a 
subset of all po~ible geometric transforms will be used. A linear intensity transform will also be included 
in the model. Refer to Figure  (below). ZOOM (X &#38; Y Scaling) Zoom in ~Enla~ement) ! /,/  Zoom 
ou~ (Reduetlon) 257 PAN (X &#38; Y Translation) $ INTENSITY /k Figure 3 -Three types of operations 
affecting artwork A modified tree structure, such as the one shown in Figure 10, is used to model all 
camera set-ups. The three types of nodes used in this structure are: (i) Artwork (Original Scanned Image 
Data)  (2) Operation (Zoom, Pan, Intensity)  (3) Merge (Adjacent levels merged into resultant level) 
  All original, scanned artwork resides at the leaves of the tree. By traversing toward the root, performing 
either a merge or operation at each intermediate node, the final frame is constructed. The traversal 
can be seen as an expression evaluator which uses three classes of nodes: operands, unary operators, 
and binary operators. The three types of configuration nodes listed above fall into these same three 
classes. The artwork is the basic operand upon which all operations are performed. A transform operation 
is a unary operator, only performed on one level of original or merged artwork. A merge is a binary operator, 
requiring two levels as input. An intermediary image is kept at any operation or merge node whose subtree 
does not change over a predefined number of frames. This means it is necessary to build the subtree only 
once even if it is used for subsequent frames. As it was earlier stated, order-independent merging allows 
such intermediary images to be built. The configuration tree structure presented here allows such frame-to-frame 
coherencies to be recognized. In addition to this cost saving feature, new and elaborate camera configurations 
can be modeled by constructing the proper tree structure to simulate the desired camera stand. Configuration 
i, as illustrated in Figure I0 produces a final merged image as pictured in Figure ii. By slightly varying 
the tree structure in Configuration 1 to incorporate independent panning of the cels of Example B (Figures 
6 and 7), Configuration 2 is created as shown in Figure 12, and a final merged image is produced, as 
pictured in Figure 13. Improved Merging Methods In typical cartoon animation, and particularly in the 
case of limited animation, three types of coherency can be identified in the final frame assembly process: 
coherency between frames, between areas within each frame, and between pixels within each area. Several 
algorithmic approaches have been developed for taking advantage of each type of coherency, thus reducing 
the overall time, space, and logic required to produce an animated sequence. A. Frame-to-frame Coherence 
 In typical cartoon animation, some or all artwork levels may change from one frame to another. Those 
portions of the configuration tree structure which do not change may be kept for subsequent frames. This 
greatly reduces the number of merges and transform operations which must be performed on each frame. 
In fact, it is possible for entire frames to be kept and used again for subsequent frames if no animation 
occurs at these frames. One example of this occurs when the animation is drawn on "two's". This means 
new drawings are only animated on every other frame of the final sequence. Drawing on two's is prevalent 
throughout traditional cartoon animation as a method of reducing the number of drawings by 50%. All 
assembly information about the frame-to-frame layout of an animated sequence is contained within a tabular 
exposure sheet. Each row on a sheet contains information pertaining to the construction of one frame, 
while each column refers to one level of artwork. An entry within a column on one row can assume one 
of two states: (i) Non-blank : Name of artwork.  (2) Blank : Assumes the default from the first non-blank 
entry above in the same column.  In order to achieve totally automatic production on the computer, 
a sequence database has been developed which makes this information available to the computer. A screen 
editor is used to enter and edit all information concerning configuration structures, names of artwork, 
and operation parameters. A video terminal, such as a DEC VTI00, simplifies this job by disp]aying a 
facsimile of the physical exposure sheet. A sample display from such an editor is pictured in Figure 
16. Once all assembly information has been entered into the sequence database using the interactive 
exposure sheet editor, the process of deciding in which order levels should be merged and which merged 
levels should be retained begins. A frame spread spanning over a predefined number of frames  starting 
at the frame currently being built is examined for groupings of cels on adjacent levels which remain 
unchanged. These intermediary groupings are built while assembling the current frame. A pool of these 
intermediary images is maintained by the computer, and before starting a new frame, the work requirements 
are first determined and any intermediary frames available in the pool are used. B. Area Coherence 
 In many cases, a character drawn for cartoon animation is found to reside within a small portion of 
the full screen area. Generally, cartoon characters are drawn in the center of the viewing area. For 
limited animation, small details, such as eyes or mouths, are drawn as a separate level of artwork. Consequently, 
the majority of the corresponding cel is transparent. Only a small central portion contains the artwork. 
Rather than work with a full screen image for every level of artwork, it is desirable to determine the 
smallest containment area which surrounds the artwork on each cel and to consider only that area when 
merging or transforming that cel. This further reduces the time required to build a final frame. A min-max 
boundary box which surrounds the non-transparent artwork of a cel can be determined from the original 
scanned artwork. This can be achieved by manually locating the four min-max boundaries on the original 
full-screen scanned image. This boundary search can also be accomplished automatically in software by 
examining each pixel of the original scanned data. A third and faster method uses a hardware image analyzer 
capable of sampling intensity levels of pixels in a designated rectangular area of the display. Figures 
4, 5, 6, and 7 are examples of cels which each have surrounding boundary boxes shown against a 12 field 
animation grid for reference. The min-max boundaries of two cels to be merged are used to construct 
a table which describes how the two cels overlap and specifies how they are to contribute to the reshlting 
merged image. This table will be referred to as the "result table" of the two cels being merged together. 
The min-max boundary box of the merged image is assumed to be the minimum rectangular area containing 
the union of the two cel boundary boxes. By extending the boundaries of both cels to the edges of the 
 boundary box of the merged image, nine regions are defined. Each of these regions corresponds to an 
entry in the result table. Two graphical representations of result tables are pictured in Figure 14. 
 Example A in this figure represents the result table of the cel from Figure 5 prior to merging on 
top of the cel from Figure 4. Similarly, Example B represents the result table of the cel from Figure 
7 before merging on top of cel from Figure 6. Each entry in a result table refers to a particular area 
in the merged image. An entry contains the following information: (i) Type of contribution toward the 
merged image  a. BOTH cels  b. TOP cel only  c. BOTTOM cel only  d. NEITHER of the two cels   (2) 
X dimension of the area (Number of pixels wide)  (3) Y dimension of the area (Number of scanlines high) 
  The actual result table dimensioning and entry values for Example B are pictured in Figure 15. The 
result table is used as a guide in assembling the resultant merged image. The merging algorithm outlined 
on page 10 need only be executed in an area of the merged image in which a contribution exists from both 
cels. No other areas require actual calculations. C. Pixel Coherence Since cels are comprised primarily 
of large homogeneous areas, it is desirable to maintain all cels in a run-length encoded (RLE) format. 
On the average, a cel consisting of anti-aliased lines, contains approximately eight pixels to every 
encoded run along a scanline. In order to take advantage of this coherency, an algorithm has been developed 
which allows cels to be merged and transformed in RLE format, reducing the time required to assemble 
a final frame by as much as a factor of eight. The merging algorithm on page 10 can be changed slightly 
to accommodate an encoded run rather than a single pixel. Figure 9 shows an enlarged area of the cel 
from Figure 4, outlined by a white box on the final merged image in Figure 8. The top portion of the 
close-up figure is shown as alternating black and white lines of various lengths. This visual format 
is used to "expose" the underlying RLE scheme. A switch between white and black on any one of these scanlines 
indicates the start of a new encoded run. A run is stored as a pixel counter, three color components 
 (RGB), and an opacity value. The algorithm has been expressly designed such that the merged image produced 
by merging two RLE artwork levels together is also in RLE format. All original, opaqued cels are stored 
in RLE format and all resultant levels are kept in this format during merging and during transform operations 
. The lateral position of a level of artwork is described by the X and Y    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806814</article_id>
		<sort_key>263</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>32</seq_no>
		<title><![CDATA[Inbetweening for computer animation utilizing moving point constraints]]></title>
		<page_from>263</page_from>
		<page_to>269</page_to>
		<doi_number>10.1145/800224.806814</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806814</url>
		<abstract>
			<par><![CDATA[<p>This paper presents an approach to computerized inbetweening which allows the animator more control over an interpolation sequence than existing keyframe techniques. In our approach, the animator specifies in addition to a set of keyframe constraints, a set of new constraints called <italic>moving points</italic>. Moving points are curves in space and time which constrain both the trajectory and dynamics of certain points on the keyframes. The sets of keyframes and moving points form a constraint or <italic>patch network</italic> specification of the desired dynamics. Several algorithms are presented for inbetweening or completing such a patch network. By measuring these algorithms with respect to a set of evaluation criteria, the algorithm which best meets our interpolation needs is selected.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Splines</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003649.10003657.10003659</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic representations->Nonparametric representations->Spline models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P299831</person_id>
				<author_profile_id><![CDATA[81100228626]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Reeves]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Research and Development, Lucasfilm, Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baecker, R. M., "Picture-driven animation," Spring Joint Computer Conference, (1969).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Burtnyk, N., and M. Wein, "Computer generated key frame animation," Journal of the Society of Motion Picture and Television Engineers, Vol. 80, (March 1971): 149-153.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360357</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Burtnyk, N., and M. Wein, "Interactive skeleton techniques for enhancing motion dynamics in key frame animation," CACM, Vol. 19, No. 10, (October 1976).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807414</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Catmull, E., "The problems of computer-assisted animation," Proc. ACM Siggraph, Vol. 12, No. 3, (August 1978).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Catmull, E., "New frontiers in computer animation," American Cinematrographer (Oct. 1979).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Coons, S. A., "Surface patches and B-spline curves," in Computer Aided Geometric Design, Barnhill and Riesenfeld (eds.), Academic Press, (1974).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Csuri, C., "Realtime film animation," Annual Report to NSF, Computer Graphics Group, Ohio State Univ., (1973).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Kahn, K,, "A computational theory of animation," Massachusetts Institute of Technology, AI Working Paper #145, (April 1977).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563871</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Levoy, Marc, "A colour animation system based on the multiplane technique," Proc. ACM Siggraph, (1977).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Mezei, L., and A. Zivian, "ARTA, an interactive animation system," Proc. IFIP 1971, (1971): 429-434.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Miura, T., J. Iwata, and J. Tsuda, "An application of hybrid curve generation&#8212;cartoon animation by electronic computers," Spring Joint Computer Conference, (1967).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>909883</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Reeves, W. T., "Quantitative representations of complex dynamic shape for motion analysis," Ph.D. Thesis, Dept. of Computer Science, Univ. of Toronto, (Sept. 1980).]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Reeves, W. T., and P. Sermer, "Representation of hand-drawn curves," submitted to Journal of the ACM, (May 1981).]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>908714</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Stern, Garland, "GAS&#8212;A system for computer aided keyframe animation," Ph.D. Thesis, Univ. of Utah, (1978).]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Tuori, M. I., "Tools and techniques for computer-aided animation," M.Sc. Thesis, Department of Computer Science, University of Toronto, (1977).]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Inbetweening for Computer Animation Utilizing Moving Point Constraints William T. Reeves Computer Research 
and Development, Lucasfilm, Ltd. ABSTRACT This paper presents an approach to computerized inbetweening 
which allows the animator more control over an interpolation sequence than existing keyframe techniques. 
In our approach, the animator specifies in addition to a set of keyframe constraints, a set of new constraints 
called moving points, Moving points are curves in space and time which constrain both the tra- jectory 
and dynamics of certain points on the key- frames. The sets of keyframes and moving points form a constraint 
or patch network specification of the desired dynamics. Several algorithms are presented for inbetweening 
or completing such a patch network. By measuring these algorithms with respect to a set of evaluation 
criteria, the algorithm which best meets our interpolation needs is selected.  1. INTRODUCTION This 
paper discusses a problem encountered in computer-aided keyframe animation --the problem of automatic 
inbetweening. In a simplified view, inbetweening is the process of generating all the frames of a motion 
sequence given its first and last frames. For example, the animator draws the object to be animated as 
it should appear in frame m and another form of the object as it should appear in frame n. The inbetweening 
problem is to determine what the object should look like in frames m+l to n-1. The essence of the inbetweening 
problem is determining the correspondences between the keyframes. That is, which parts or points of the 
first keyframe should be mapped or transformed into which parts or points of the second keyframe. In 
addition, once these correspondences have been established, their trajectories and transformations through 
time must be determined. The difficulty in determining correspondences automatically is that the keyframe 
drawings are really two-dimensional projections of three-dimensional objects and hence information is 
lost. The inbetween- ing system, in general, has no understanding of what the object really is (an exception 
here is the artificial intelligence approach of [Kahn 77]), and hence cannot determine correspondences 
as accu- rately or easily as humans can. Past inbetweening techniques can be categorized by their method 
for determining correspondences and their method for assigning trajectories and transformations. The 
first successful computer animation system --GENESYS [Baecker 69] --employed p-curves and selection graphs 
to allow the animator to specify rigid body motions and the selection of keyframes. Correspondences were 
specified by the animator on an object-to-object basis. The majority of past systems, [Mezei 71] [Burt-nyk 
71] [Csuri 73] [Tuori 77] [Levoy 771 and [Catmull 791, required the animator to specify correspondences 
in terms of the lines or curves of the two keyframes. Each keyframe had to have the same number of curves 
and they had to be specially ordered by Permission to copy without fee all or part of this material is 
granted provided that the copies are not made or distributed for direct commercial advantage, the ACM 
copyright notice and, the title of the 01981 ACM O-8971-045-1/81-0800-0263 $00.75 263 the animator. By 
default, the corresponding lines were inbetweened using simple linear interpolation. Most systems extended 
this simple technique by allowing the animator to specify the path and speed of interpolation of all 
curves in the keyframe. This was often called path or movealonginterpolation. The skeleton approach of 
[Burtnyk 76] utilized a method of establishing a correspondence between a fully-drawn keyframe and a 
skeletal out-line. To generate a motion sequence, the animator had only to animate the skeleton. Finally, 
[Stern 78] describes a system which attempts to automatically determine correspondences. This system 
does at times generate incorrect matchups which must be manually corrected. The inbetweening approach 
and associated algorithms presented in this paper improve these existing methodologies by: employing 
an intuitive and simple mechanism for specify- ing correspondences  allowing the specification of multiple 
paths and speeds of interpolation between keyframes  allowing control of motion discontinuities which 
some-times occur in an inbetweened sequence across keyframes.  As a result, the animator is given control 
over the motion sequence without having to resort to additional keyframes or breakdowns, and thus is 
able to produce equal quality animation at a faster rate. We should point out that our approach does 
not solve all inbetweening problems automatically --its goal is to make it easier for the animator to 
control the complexities of an animation sequence. In section 2, we will present the animator's view 
of our approach to inbetweening. Section 3 discusses a set of criteria by which to evaluate the inbetweening 
algorithms that we will present in section 4. In section 5, we evaluate these algorithms, and in section 
6 we briefly discuss some algorithms for completing incom- plete animator specifications. Finally, section 
7 summarizes our work. 2, THE ANIMATOR'S VIEW The animator's view of our inbetweening process is an extension 
of the keyframing view. In keyframing, to animate an object the animator specifies an ordered set of 
keyframes {KF1,KF2,... ,KFk} which define the form of the object at the animator-specified times {tl,t2 
..... tk}. Keyframes are usually sketched by the animator on a digitizing tablet. Each keyframe can be 
thought of as a static shape positioned at a fixed point in time which acts as a constraint on the motion 
sequence. To this basic keyframing view of the animator's task, our approach allows the animator to specify 
an additional set of con- straints to control the inbetweening process. These constraints, {MP1,MP2 ..... 
MPq}, are called moving points. A moving point is a curve in space and time which constrains both the 
trajectory and dynamics (i.e., path and speed) of a point on the animated publication and its date appear, 
and notice is given that copying is by permission of the Association for Computing Machinery. To copy 
otherwise, or to republish, requires a fee and] or specific permission.   Computer Graphics Volume 
15, Number 3 August 1981 object. Moving points are normally sketched by the animator and can have any 
shape and dynamics desired. At any time in the motion sequence, a moving point is located at a particular 
x-y position. Points on the keyframes not directly constrained by a moving point are constrained by a 
"smooth blending" of their neighbouring moving points (sections 3, 4, and 5 are dedicated to algorithms 
to perform this blending). As with keyframes, the ani- mator can specify as many moving points as are 
necessary to con- trol the dynamics. The set of keyframes and the set of moving points specify what we 
call a constraint or patch network of the motion sequence. It should be noted that it is the responsibility 
of the system to establish the inter-relationships between the key-frames and moving points --the animator 
just has to specify them. Moving points allow the specification of correspondences, tra- jectories, and 
dynamics, For example, in the typical user specification shown in Figure 1, moving point MP2 indicates 
that point A in keyframe KF2 corresponds (i.e., is constrained to) point B on keyframe KF3. That same 
moving point also specifies that the trajectory of point A as it transforms itself into point B does 
not follow a linear path. Instead, it follows the path of MP2. The dynamics of movement along this free-form 
trajectory are also controlled by the moving point, t Figure 1. Typical User Specification As shown in 
Figure 1, moving points can extend across and hence constrain many keyframes (e.g., MP2). There can also 
be as many moving points constraining a keyframe as necessary, and even multiple moving points constraining 
a single curve. The user interface of our inbetweening system consists of three parts: an editor for 
sketching and composing static shapes, an edi- tor for sketching and composing moving points, and a playback 
subsystem for previewing motion sequences once they have been inbetweened? The static shape editor allows 
the animator to place keyframes at selected times in the motion sequence. The static shapes are usually 
sketched by the animatar, but they can also be scanned in or algorithmically generated externally to 
the editor. Mechanisms for naming and grouping curves into higher level objects so they can be referred 
to and edited as a unit are also included. 1. The dynamics of a moving point are indicated by its hash 
marks. Each hash mark represents an equal length of time. These are equivalent to the animator's standard 
ladder mechanism. 2. There are many other parts to a complete computer animation system [Catmull 78], 
but here we are concentrating on only the inbetweening components.  In the moving point editor, the 
animator normally specifies moving points by sketching on the tablet in equal-time mode. 3 However, it 
is almost always very important to synchronize the dynamics of one moving point with another, so editing 
subsystems exist to allow the animator to specify the trajectory and dynamics of a moving point separately. 
For example, one can edit the dynamics of one moving point while viewing the displayed dynam- ics of 
several other moving points. As well, one can tie into the playback subsystem and sketch a new moving 
point while the current inbetweened motion sequence is played back on the display screen. New moving 
points based on the trajectories or dynamics of existing moving points can be generated. The moving point 
editor also contains mechanisms for very quickly specifying linear moving points and simple eases, and 
for automatically establishing default constraints. The playback subsystem is very important because 
it enables the animator to pencil test the current motion sequence. An important point to note here is 
that the animator works by succes- sive refinement --adding new constraints, seeing how the sequence 
now looks, and going back and adding more constraints (either keyframes or moving points) until the sequence 
is just right. The moving point approach is an improvement on keyframing systems because it provides 
a simple and intuitive mechanism for specifying correspondences. The animator does not have to number 
and count curves in the keyframes. The intersection of two curves in different keyframes with the-same 
moving point enables the system to automatically establish a correspondence. Moving points also allow 
the animator much more control over the inbetweened shapes --the interpolation is not just linear or 
movealong. Multiple moving points can control an interpolation. This very often decreases the number 
of keyframes or breakdowns that the animator must specify. In an animated sequence, keyframing interpolation 
can some-times generate motion discontinuities that we call clicks. Clicks often appear as the motion 
passes through intermediate keyframes because the eye perceives a change in trajectory and dynamics, 
even though the interpolation between keyfrarne pairs is smooth. Clicks occur in conventional keyframing 
because there is no means for specifying and controlling interpolation over a sequence of key- frames. 
In our algorithms, the animator can control clicks with moving points. Our approach is more intuitive 
to the animator than skeleton inbetweening. Moving points are part of the object being animated, whereas 
skeletons are a separate entity that must be created by the animator. 3. CRITERIA FOR THE EVALUATION 
OF INBETWEENING ALGORITHMS Given a set of keyframes and a set of moving points describing a motion sequence, 
how do we algorithmically generate the inbetweens? Three algorithms will be presented. The first is the 
Miura algorithm which exists in the literature [Miura 67]. The second and third algorithms were developed 
within the context of this research. A detailed discussion of these algorithms is presented in [Reeves 
80]. The criteria by which we propose to compare and evaluate these inbetweening algorithms are: 1. The 
generality of the algorithm. 2. The smoothness of the algorithm. 3. The efficiency of the algorithm. 
 4. The economy of specification of the algorithm  The generality criterion is concerned with the flexibility 
of the algorithm --its ability to inbetween accurately the infinitely many 3. The tablet is sampled at 
equal intervals in time. The points so gathered represent the dynamics of drawing as well as the trajectory. 
 264 possible motion sequences. The smoothness criterion states that during an interpolation the inbetweening 
algorithm must not remove any features of static shape that exist in the keyframe constraints nor must 
it introduce any new features not present in them. If the motion sequence is intended to be not smooth, 
then the animator must specify con- straints which identify these discontinuities --the system cannot 
infer them on its own. If an inbetweening algorithm violates the smoothness criteria, we say that the 
algorithm has contorted on the specification. The efficiency of an inbetweening algorithm is a measure 
of its execution speed and memory requirements. The faster the algo- rithm, the better. The economy of 
specification criterion measures a perhaps more important form of efficiency --the efficiency of the 
anima- tor. Can the animator with a few well placed and powerful con- straints control an entire sequence? 
4. THE INBETWEENING ALGORITHMS The inbetweening algorithms of this section are designed to operate on 
complete patch networks; those for which the following conditions apply: 1) a keyframe must be defined 
at the start time of the motion sequence, Ts; 2) a keyframe must be defined at the end time of the motion 
sequence, Te; 3) all moving points must be defined over the entire interval: Ts to Te; 4) moving points 
must be defined to constrain the motions of the ends of all curves in the keyframes; and 5) no inconsistencies 
can exist in the specification. Although these restrictions may appear strong, sec- tion 6 addresses 
the issue of how to automatically complete specifications for the animator, and how to detect inconsistencies 
in the patch network. As an example, the patch network in Figure 1 was incomplete. Figure 2 shows a completed 
version of this net- work. MP, Figure 2. Completed Patch Network A completed patch network is subdivided 
into patches. Each patch is defined by four boundary curves --two are static boundaries derived from 
the two bounding keyframes, and two are dynamic boundaries derived from the two bounding moving points. 
Our inbetweening problem on a patch network can thus be reduced to many inbetweening problems on its 
patches. We shall notate a patch as per [Coons 74]. A generic point (x,y) on the patch is defined by 
the equations: x=X(u, t), and y= Y(u, t). Thus, the patch is the locus of the points (x,y) moving in 
space and time with two degrees of freedom u and t (u is the parametric space variable and t is the parametric 
time vari- able). Generally, the parametric variables are restricted to values between zero and one to 
make the arithmetic more tractable. The patch can thus be written as: (x,y)=P(u,t)=[X(u,t), Y(u,t)],O~< 
u~< 1,0~< t~< 1. The vector quantities P(0, t), P(1, t), P(u,O) and P(u,1) are point loci with a single 
degree of freedom (i.e., curves). The patch is defined over rectangles in the u-t plane. The curves are 
in fact the boundaries (i.e., the constraints) of the patch. Following Coons, we make the following simplification 
of notation: ut= P(u,t), Ot= P(O,t), It= P(1, t), uO= P(u,O), ul = P!u, 1). A patch specification using 
this notation is shown in Figure 3. The curves u0 and ul define the static shape of the object at two 
different times, to and tv The curves 0t and 1 t are moving points specifying the motion of the end points 
of the static shapes.  It 11 19 lid Figure 3. Patch Specification Each patch in the patch network is 
defined in this fashion. The boundary curves can be represented in any form. In related work by the author 
[Reeves 80] [Reeves 811, the representation of hand-drawn curves is discussed in detail. A representation 
based on parametric pi.ecewise hermite cubic polynomials was found to be very good. The inbetweening 
problem is to find an algorithm to define the two-parameter function ut for all patches in the network. 
To gen- erate the inbetween frame at a particular time to, we evaluate all patches overlapping to, holding 
the time parametric variable con- to-ts stant at (ts is the start time of a patch, te the end time) te-ts 
and letting the space parameter vary from zero to one. That is, to generate the inbetween frames, we 
rule the surface.defined by the patch specification at a number of equally spaced intervals in time. 
 4.1 Miura Inbetweening The Miura inbetweening algorithm originates from research at the Hitachi Research 
Laboratory in the mid-60's [Miura 67]. It is the approach in the literature that most closely parallels 
the approach of this paper. Miura inbetweening over a patch is defined in vector form as: ut = (l-t) 
A°t+ t Air where A ° = W0t (u0-10) + 1 t A2r= Wit (ul-ll) + It Wo, = 1 I w°' w°] (00x-10x)%(00y-10) 
2 [ -w °' w0.1   , I wq Wit= (01 _llx)2+(Oly_lly)2[_wJt w~ l w °' = (00x-10x)(0 tx-1 lx)+(OOy-lOy) 
(0ty-1 ty) w Ot = (00y--10y)(0 tx--1 tx)--(OOx--lO x) (0ty--1 ly) and w~ t = (Ol x--'l l x) (Otx--1 tx)+(Ol 
y--l l s) (Oty--1 ty) w~t = (Ol y--l l y) (Otx--l tx)--(Ol x--l l x) (Oty-l ty) Note that the subscript 
x or y is used to represent the x and y components of the patch's boundaries. Despite the dense mathematical 
formulae, the Miura algorithm is quite simple. As shown in Figure 4, the static curve interpo-lated for 
time t is defined as a time-weighted average of two terms --one deriving from the initial static shape, 
KFo (i.e., u0), at time t=0, and the other from the final static curve, KF1 (i.e., ul), at time t=l. 
The first term, A °, is a transformation of KFo by Wot and 1 t. This transformation specifies a rotation, 
scale, and translation of space about KFo such that the straight line segment Lo is transformed into 
the straight line segment L, 4 A ° will therefore rotate, scale, and translate KFo to a new orientation, 
size, and position, but with the same shape. The second term of the transformation, A2t, is a similar 
mapping of KFI by Wit and 1 t which transforms L] into L t. A~ MPt \ Lt L t i t.~-Mp a Mvu~ ~-~'~-~,~ 
I Figure 4. Explanation of the Miura Representation The interpolation algorithm is applied independently 
to each patch in the patch network. 4.2 Coons Patch lnbetweening Our Coons patch inbetweening algorithm 
is derived from the Coons patch representation for three-dimensional objects [Coons 74] which is commonly 
used in computer-aided geometric design. The Coons algorithm has the advantage of being able to control 
normal derivatives across patch boundaries. This is a sufficient mechanism to prevent first derivative 
discontinuities along the boundary between two adjacent patches which, as we will see in section 5, is 
a major weakness of the Miura algorithm. The Coons patch is an interpolation scheme defined over rec- 
tangles in the u-t plane. In its general form, each patch is defined by specifying its four boundaries, 
the normal vectors "across" these boundaries, and the cross-derivative "twists" of the surface at its 
four corners. The resulting surface "contains" or passes through its boundary curves, with tangent vectors 
containing the cross-boundary derivatives and with second derivatives on the boundaries which are weighted 
combinations of the second deriva- tives at the ends of the boundary. The consequence is that adjoin- 
ing patches will be curvature continuous (i.e., C) across their mutually shared boundary. A mosaic of 
such patches prox, ides a compound surface that is everywhere at least curvature continu- ous, provided 
only that the boundary curves are curvature continu- ous. The specification required by the general Coons 
patch algorithm is more general than provided by a patch network. A simplified form useful for inbetweening 
is: (ut)= [u0 ul][F°l + [gougiu][Oll -[Foug, q[OlO0 01l[F°lllllF,~ with the two univariate blending functions, 
Fou = Fo(u) and Flu = Fl(u), being defined by the requirements that F0(0) = 1; F0(1) = 0; El(0) = 0; 
El(l) = 1; F0(0) =0; F0(1) =0; FI(0) =0; FI(1) =0. These functions are called blending functions because 
they "mix" or blend the shapes of the boundary curves to produce the internal curves of the patch. It 
can be shown that for two patches, A and B, as shown in Figure 5, if A(10)u = B(OO)u and A(ll) u = B(01), 
(i.e., the keyframe boundary curves are continuous in slope in the u direction at the ends of the moving 
point boundary between the patches), then we are guaranteed to have A(lt)u= B(Ot)u everywhere along the 
boundary (i.e., first derivative continuity). \ Am u A Figure 5. Adjacent Patches Each blending function 
can be represented as a cubic polynomial. We employ the following definitions of F0 and Ft: Fo(u) = (l-u) 
2 (2u + 1) Fl(u) = u 2 (--2u + 3).  4.3 Cubic Metric Space Inbetweening 4. The Wot term specifies 
the rotation and scaling. The 1 t term specifies the The cubic metric space inbetweening algorithm was 
motivated by translation. the Miura algorithm, but designed to correct its weakness in deal- ing with 
cross-boundary derivatives.  Computer Graphics In the cubic metric space algorithm, interpolations are 
based not on linear cartesian coordinate spaces as in the Miura algorithm, but on non-linear spaces defined 
by cubic equations. Consider Fig- ure 6 which illustrates the algorithm over a patch. The metric space 
about KFo is defined by the parametric cubic curve which matches both the position and first derivative 
of KFo at its two end points. This is shown as Co in the figure, and is called KF0's basis curve. A similar 
curve Cl is defined about KFI. At time t, another similar curve, Ct, is defined. Its end points are the 
posi- tions of the moving points at time t, and its endpoint slopes are the time-weighted average of 
the slopes at the corresponding end- points of KFo and KFI. The Co, Cl, and Ct curves are easily represented 
as parametric hermite cubic polynomials. C o ]~'o ~ ~"~ / Ct i a Figure 6. Cubic Metric Space The cubic 
metric space transformation maps points defined in the spaces about Co and Cl to the space about Cv Not 
only do we wish the transformed shape to match Ct's position at its end points, but also its slope at 
its end points. By controlling slopes along matching boundaries of adjacent patches, we control cross- 
boundary normals as we did in the Coons patch algorithm. In a cubic metric space, each point on a keyframe 
has a corresponding point on its basis curve (they have equal parametric distances). The coordinates 
of a point are based on the distance between it and its corresponding point and the angle between the 
tangent to the basis curve at the corresponding point and the line joining the two points. The details 
of the actual cubic metric space transformation are presented in [Reeves 80]. 5. EVALUATION In this 
section, we shall compare and evaluate the inbetweening algorithms of the last section with respect to 
the criteria of section 3 However, before beginning let us show some example gen- erated inbetweens. 
5 Figure 7 is an animated beating human heart generated by the Coons patch algorithm using three keyframes 
and three moving points. Notice how the dynamics of the cardiac cycle have been controlled by the moving 
points and how smooth the interpolation is. Figure 8 is a rotating arm generated with the cubic metric 
space algorithm using two keyframes and nine moving points (those fingers are tricky!). The arm does 
not foreshorten or twist as it would with keyframe inbetweening. Figure 9 is a turn- ing head generated 
by the Coons patch algorithm using two key- frames and one moving point. 5. All of our examples will 
be shown as overlaid sequences of frames or as cartoon strips. In the latter, frames are ordered from 
left to right and from top to bottom, and keyframes are marked with a "*" Volume 15, Number 3 August 
1981 Figure 7. Beating Heart using Coons Patch Algorithm J Fr~'~e 9 * % L Figure 8. Rotating Arm using 
Cubic Metric Space Algorithm 5.1 The Generality Criterion We have found that all of the algorithms apart 
from some smooth- ness and economy of specification concerns are quite general, and satisfy the boundary 
constraints of a patch network. Motions such as translation, scaling, rotation, bending, warping, shrinking, 
and expanding can all be specified. 5.2 The Smoothness Criterion In terms of smoothness, each algorithm 
has problems. The Miura algorithm generates two forms of contortion. Figure 10 shows the first form. 
Notice the cusps that appear in the inbetweened frames along the border between the bottom two patches. 
These discon- tinuities do not appear in the patch specification --they were created by the algorithm 
because it has no control of derivative continuity across patch boundaries. Each patch was computed independently 
of all the others. ~g Fml Pw9 Figure 9. Turning Head using Coons Patch Algorithm Figure 10. Contortions 
in the Miura Algorithm The Miura algorithm also tends to contort when the boundaries of a patch are 
close to forming a closed curve. The Coons patch algorithm does not contort on patch networks that cause 
the Miura algorithm trouble. Discontinuities do not appear along patch boundaries because it can control 
cross-patch derivatives. Closed curves do not cause it problems because it is a parametric algorithm. 
The Coons algorithm rarely contorts on any specification. When it does, the problems usually occur because 
the algorithm tries so hard to obey the cross-boundary constraints that it contorts interior to the patch. 
For example, in the network shown in Figure 11, the generated curves should not fold over in the interior 
of the middle patch. Note that this contortion can be easily controlled by specifying one additional 
keyframe constraint. The cubic metric space algorithm does not introduce discon- tinuities along patch 
boundaries like the Miura algorithm nor does it contort nearly as much as the Coons patch algorithm interior 
to patches. Unfortunately, the cubic metric space algorithm does contort when there are regions of high 
curvature in some of its cubic basis curves. Figure 11. Contortions in the Coons Patch Algorithm In 
summary, all algorithms experience forms of contortion con- trary to the smoothness criteria. We have 
found that the fre-quency of contortion in the Coons patch algorithm is much less than in the other algorithms. 
It is much easier for the animator to control contortions when the Coons patch or cubic metric space 
algorithms are used. 5.3 The Efficiency Criterion The average execution time of our algorithms and of 
a conven- tional keyframing algorithm over a set of test patch networks is shown in Table 1. The keyframing 
algorithm is the fastest, the Miura and Coons patch algorithms are average, and the cubic metric space 
algorithm the slowest. Algorithm Average Execution Time (see.) conventional keyframing 17.388 Miura 22.601 
Coons patch 26.210 cubic metric space 41.335 TABLE 1. Average Execution Times of Inbetweening Algorithms 
 5.4 The Economy of Specification Criterion We have been experimenting with our three inbetweening algo- 
rithms and a conventional keyframing algorithm for over a year now. We have often found it necessary 
to specify many more static keyframes than we wanted to when using the keyframing algorithm. Much less 
frequently, we have found it necessary to provide additional constraints to control contortions in the 
Miura and cubic metric space algorithms. Finally, we have only had to add extra constraints to the Coons 
patch algorithm in the extreme case examples of this paper. 5.5 Summary The Miura algorithm has average 
smoothness, efficiency, and economy of specification. It has good generality. The cubic metric space 
algorithm has poor efficiency, and average economy of specification. It has good generality and smoothness. 
Finally, the Coons patch algorithm best meets our selected criteria with aver- age efficiency and good 
generality, smoothness, and economy of specification. 6. COMPLETION OF PATCH NETWORKS The inbetweening 
algorithms of section 4 take as input a com-pleted patch network. The user interface of section 2 allows 
the animator to specify an animated sequence with a set of keyframe constraints and a set of moving point 
constraints. Our remaining problem is to define techniques and algorithms which bridge the gap between 
the unstructured, possibly incomplete, and possibly 268 Computer Graphics Volume 15, Number 3 August 
1981 inconsistent animator specification and its corresponding structured and complete patch network. 
The approach taken in our work is discussed in detail in [Reeves 80]. It is based on several simple algorithms, 
some heuristics, and a well chosen set of defaults. 7. CONCLUSIONS The classic keyframing interpolation 
example of automatically transforming a walking man into a speeding racing car actually illustrates what 
we feel has been one of the real bottlenecks of existing computer animation systems --there is no control 
over the transformations, trajectories, and dynamics between the key- frames. Our approach to computerized 
inbetweening, using patch networks composed of moving point and keyframe constraints, allows the animator 
more control over the animated sequence than existing keyframing techniques. Moving points allow the 
animator to specify correspondences between keyframes in an intuitive manner. The animator can con- trol 
the interpolation between two keyframes with as many moving points as necessary. To get the control we 
have in our approach, an animator using standard keyframing has to break down a key-frame into many keyframes 
--one for each moving point --and position them on different levels. But this introduces a serious problem 
on its own which is not solved in keyframing systems --there is now nothing to tie the parts of the animated 
object (i.e., the original keyframe) together. The animator is left on his own trying to coordinate and 
control the many pieces as they move through time. This problem does not arise in our approach. By employing 
a few moving points, an animator can quite often greatly reduce the number of keyframes that have to 
be specified. Finally, because the animator can specify moving points which constrain a sequence of keyframes, 
clicks can be controlled in the animated sequence. We have outlined three inbetweening algorithms for 
interpolat- ing patch networks and upon evaluating them with respect to their generality, smoothness, 
efficiency, and economy of specification have selected the Coons patch inbetweening algorithm. 8. Acknowledgements 
The author would like to thank the following individuals for contributing ideas to this work: Ron Baecker, 
Domminic Covvey, Pavol Sermer, Bob Sproull, and David Miller. The research for this paper was undertaken 
as part of the author's Ph.D. thesis at the University of Toronto. The sup- port of the Department of 
Computer Science, the Computer Systems Research Group, and the National Sciences and Engineering Research 
Council of Canada is gratefully acknowledged. 9. Bibliography Baecker, R. M., "Picture-driven animation," 
Spring Joint Computer Conference, (1969). Burtnyk, N., and M. Wein, "Computer generated key frame animation," 
Journal of the Society of Motion Picture and Television Engineers, Vol. 80, (March 1971): 149-153. Burtnyk, 
N., and M. Wein, "Interactive skeleton techniques for enhanc- ing motion dynamics in key frame animation," 
CACM, Vol. 19, No. 10, (October 1976). Catmull, E., "The problems of computer-assisted animation," Proc. 
ACM Siggraph, Vol. 12, No. 3, (August 1978). Catmull, E., "New frontiers in computer animation," American 
Cinema- trographer (Oct. 1979). Coons, S. A., "Surface patches and B-spline curves," in Computer Aided 
 Geometric Design, Barnhill and Riesenfeld (eds.), Academic Press, (1974). Csuri, C., "Realtime film 
animation," Annual Report to NSF, Computer Graphics Group, Ohio State Univ., (1973). Kahn, K,, "A computational 
theory of animation," Massachusetts Insti- tute of Technology, AI Working Paper #145, (April 1977). 
 Levoy, Marc, "A colour animation system based on the multiplane tech- nique," Proc. ACM Siggraph, (1977). 
Mezei, L., and A. Zivian, "ARTA, an interactive animation system," Proc. IFIP 1971, (1971): 429-434. 
Miura, T., J. lwata, and J. Tsuda, "An application of hybrid curve gen- eration --cartoon animation by 
electronic computers," Spring Joint Com- puter Conference, (1967). Reeves, W. T., "Quantitative representations 
of complex dynamic shape for motion analysis," Ph.D. Thesis, Dept. of Computer Science, Univ. of Toronto, 
(Sept. 1980). Reeves, W. T., and P. Sermer, "Representation of hand-drawn curves," submitted to Journal 
of the ACM, (May 1981). Stern, Garland, "GAS --A system for computer aided keyframe anima- tion," Ph.D. 
Thesis, Univ. of Utah, (1978). Tuori, M. I., "Tools and techniques for computer-aided animation," M.Sc. 
Thesis, Department of Computer Science, University of Toronto, (1977).   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806815</article_id>
		<sort_key>271</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>33</seq_no>
		<title><![CDATA[A software test-bed for the development of 3-D raster graphics systems]]></title>
		<page_from>271</page_from>
		<page_to>277</page_to>
		<doi_number>10.1145/800224.806815</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806815</url>
		<abstract>
			<par><![CDATA[<p>We describe a set of utility routines for 3-D shaded display which allow us to create raster scan display systems for various experimental and production applications. The principal feature of this system is a flexible scan conversion processor that can simultaneously manage several different object types.</p> <p>Communications between the scan conversion routine and processes which follow it in the display pipeline can be routed through a structure called a &#8220;span buffer&#8221; which retains some of the high resolution, three dimensional data of the object description and at the same time has the characteristics of a run length encoded image.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Graphics systems]]></kw>
			<kw><![CDATA[Shaded display]]></kw>
			<kw><![CDATA[Visible surface algorithms]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Verification</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P285628</person_id>
				<author_profile_id><![CDATA[81100586999]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Turner]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Whitted]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Laboratories, Holmdel, New Jersey]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31050684</person_id>
				<author_profile_id><![CDATA[81100631648]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Weimer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Laboratories, Holmdel, New Jersey]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807424</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baecker, R., Digital video display systems and dynamic graphics, Computer Graphics, 13, 2 (August 1979), 48.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>507101</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Blinn, J.F., Simulation of wrinkled surfaces, Computer Graphics, 12, 2 (August 1978), 286.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Carpenter, L.C., and Fournier, A., and Fussell, D., Display of fractal curves and surfaces, to appear, Comm. ACM.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907242</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Catmull, E., A subdivision algorithm for computer display of curved surfaces, PhD thesis, University of Utah, 1974.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807360</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Catmull, E., A hidden surface algorithm with anti-aliasing, Computer Graphics, 12, 3 (August 1978), 6.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807440</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Clark, J.H., A. fast algorithm for rendering parametric surfaces, supplement to Computer Graphics (distributed to SIGGRAPH '79 attendees), August 1979.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Crow, F.C., Computer graphics in the entertainment industry, Computer, 11, 3 (September 1977), 11.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807458</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Csuri, C., Hackathorn, R., Parent, R., Carlson W., and Howard, M., Towards an interactive high visual complexity animation system, Computer Graphics, 13, 2 (August 1979), 289.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Gouraud, H., Continuous shading of curved surfaces, IEEE Trans. Cmptrs, C-20 (June 1971), 623.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807486</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Jackson, J.H., Dynamic scan-converted images with a frame buffer display device, Computer Graphics, 14, 3 (July 1980), 163.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Lane, J.M., and Carpenter, L.C., A generalized scan line algorithm for the computer display of parametrically defined surfaces, Computer Graphics and Image Processing, vol. 11 (1979), 290.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358815</ref_obj_id>
				<ref_obj_pid>358808</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Lane, J.M., Carpenter, L.C., Blinn, J.F., and Whitted, T., Scan line methods for displaying parametrically defined surfaces, Comm. ACM, 23, 1, (January 1980), 23.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Myers, A.J., An efficient visible surface program, Report to National Science Foundation, Grant No. DCR74-00768 A01, Computer Graphics Research Group, Ohio State Univ., July 1975.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>569954</ref_obj_id>
				<ref_obj_pid>800193</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Newell, M.E., Newell, G.S., and Sancha, T.L., A Solution to the hidden surface problem, Proc. of the ACM annual conference, 1973, 443.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907365</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Newell, M.E., The utilization of procedure models in computer synthesized images, PhD thesis, University of Utah, 1975.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Newman, W.M., and Sproull, R., Principles of Interactive Computer Graphics, McGraw-Hill, 1973.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360839</ref_obj_id>
				<ref_obj_pid>360825</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Bui-Tuong Phong, Illumination for computer generated pictures, Comm. ACM, 13, 6 (June 1975), 311.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Rubin, S.M., The representation and display of scenes with a wide range of detail, submitted for publication.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807418</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Shoup, R., Color table animation, Computer Graphics, 13, 2 (August 1979), 8.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>905548</ref_obj_id>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Watkins, G.S., A real-time hidden surface algorithm, PhD thesis, Univ. of Utah, 1970.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>909086</ref_obj_id>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Whitted, J.T., A processor for display of computer generated images, dissertation, North Carolina State University, August 1978.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Whitted, T., Hardware enhanced 3-D raster display systems, Proceedings of Canadian Man-Computer Communication Conference, June 1981.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Williams, L., Casting curved shadows on curved surfaces, Computer Graphics, 12, 2 (August 1978), 270.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 A Software Test-Bed for the Development of 3-D Raster 
Graphics Systems Turner Whitted David M. Weimer Bell Laboratories Holmdel, New Jersey 07733 Abstract 
We describe a set of utility routines for 3-D shaded display which allow us to create raster scan display 
sys- tems for various experimental and production applica- tions. The principal feature of this system 
is a flexible scan conversion processor that can simultaneously manage several different.object types. 
Communications between the scan conversion rou- tine and processes which follow it in the display pipeline 
can be routed through a structure called a "span buffer" which retains some of the high resolution, three 
dimen- sional data of the object description and at the same time has the characteristics of a run length 
encoded image. KEY WORDS AND PHRASES: computer graphics, shaded display, visible surface algorithms, 
graphics sys- tems CR CATEGORIES: 8.2 Design Philosophy The most straightforward approach to shaded 
display of three dimensional objects has been to store all surface elements of a scene in main memory 
and pro- duce images one scan line at a time. The advantage of this method is that all elements which 
are potentially visible at any point on the image are immediately acces- sible to the display process. 
In practice this means that anti-aliasing can be readily accomplished [5], and real- time performance 
is feasible [20,10]. Although main memory size appears to limit the complexity of the images that this 
method can produce, Crow [7] has demonstrated that appropriate tricks will enable the display of complex 
scenes. Permission to copy without fee all or part of this material is granted provided that the copies 
are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of 
the publication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 
&#38;#169;1981 ACM O-8971-045-1/81-0800-0271 $00.75 Alternatively, one may process small parts of an 
object description one at a time, accumulating the final image in a frame buffer memory. If extra bits 
are pro- vided in the frame buffer for depth values, then the object parts may be processed in any order 
using the z-buffer algorithm [4,13]. Otherwise they must be sorted in back-to-front order before being 
scan converted and written into a frame buffer [14]. The two advantages of this approach are linear growth 
of the processing time with respect to the complexity of the object description and the ability to manage 
object descriptions of unlim- ited complexity. This technique of accumulating an image one part at a 
time has facilitated the development of "hybrid" display systems which can process more than one object 
type. Newell's use of procedure models [15] is a good example. The Computer Graphics Research Group at 
Ohio State developed a display system that incorporates lines and points as well as polygons as primitives 
[8] and includes procedure models for producing extremely complex object descriptions. The hybrid display 
system designed by Jim Blinn at JPL is capable of displaying higher order surfaces as well as polyhedra. 
For a broad range of applications, no single approach to display system design will adequately meet the 
requirements of each application. Baecker [1] describes several display system configurations and points 
out their application specific strengths and weaknesses. Recognizing the drawbacks of a rigid approach, 
we have constructed a flexible tool for the development of 3-D raster display systems. It is designed 
to be a "test-bed" that can easily incorporate a variety of user defined techniques while remaining rea- 
sonably efficient. The test-bed can be assembled into the configurations previously mentioned, and is 
further capable of a configuration that allows images to be accu- mulated one part at a time, in any 
order, while retaining the information needed to perform anti-aliasing. In addition it can be arranged 
as a hybrid system that simultaneously processes a variety of object types in a single pass.  System 
Elements The whole idea behind the test-bed is to minimize the amount of special programming required 
for new Computer Graphics Volume 15, Number 3 August 1981 applications. Consequently, a variety of common 
rou-tines are offered to the user to be assembled in a manner to suit the application. Certain properties 
of the system are fixed -it gen- erates an image in scan line order, and it requires that surfaces be 
ultimately reduced to planar convex polygons (although not necessarily prior to scan conver- sion). Its 
basic elements are also quite ordinary-a transformation and clipping front end, a scan conversion processor, 
and a shader. Data passed from the scan converter to the shader may be routed through a "span buffer". 
The advantages of this option are explained later in the section on span buffers and anti-aliasing. The 
format for object descriptions is shown in figure 1. Each object is described in terms of polygons which 
in turn are described by edges which contain ver- tices. Any hierarchical organization at a level higher 
than single objects may be imposed by the user but isn't recognized by the display routines. In the default 
mode, the test-bed routines allocate only enough memory to store the object description being processed. 
An option exists for allocating additional memory to store polygons and edges which are created by procedure 
models during the course of scan conversion. Each of the vertex elements except depth (z) is a 16-bit 
number. Fifteen bits of this resolution are used to define screen coordinates with -16384 < x,y < 16384 
and (x----0,y =0) at the center. Z is represented by a 32-bit number with the binary point in the middle. 
Since perspective transformations compress large, posi- tive z values, this extra precision is required 
for deter- mining the relative visibility of distant, closely spaced objects. These coordinates are completely 
independent of the display resolution. In fact, the display routines are written in such a manner that 
a user can specify any desired resolution between 2x2 and 16384x16384 by altering a single header file. 
Transformation and clipping of conventional, pas-sive polygonal object descriptions proceed in the follow- 
ing order: All rotation, translation and scaling are done first, followed by a clip against the near 
clipping plane. Then comes the perspective divide followed by clipping against the remaining clipping 
planes. The reason for this ordering is to avoid having the perspective divide wrap objects behind the 
viewer around to the front before clipping. This was the easiest scheme to imple- ment, but it is currently 
being replaced by a routine to perform all clipping before the perspective divide. Parameters associated 
with the perspective and viewing transformations are kept as global variables throughout the display 
process so that they may be "undone" by those shaders which need to find the original object space coordinates 
of a point. (For an explanation of transformations associated with the display process, see the text 
by Newman and Sproull [16].) The flexibility of the scan conversion routine stems from its facility for 
on-the-fly procedural expansion which is discussed in the next section. Otherwise it is conventional 
in the way that it processes polygon edges. For each scan line, any edge which is intersected first by 
that scan line is added to a list of active edges. The Object description header part identifier no. 
of polygons no. of edges no. of vertices head of poly list polygons poly identifier ptr to first edge 
storage mask edges forward list ptr backward list ptr interpolation mask ptr to polygon(l) ptr to polygon(2) 
ptr to vertex(l) ptr to vertex(2) vertices x (position) y (position) z (position) nx (normal) ny (normal) 
nz (normal) u (texture index) v (texture index) Figure 1. Object description format endpoints of each 
active edge are represented by at least an x-position, a z-position, and a shade value. Up to nine additional 
variables may be associated with each edge endpoint. The scan converter interpolates these variables 
along each edge using the familiar difference equation, Xn+ 1 ~ x n "~ where (X2 -- X1) 32768Z~X= (Y 
2 --Y 1 ) resolution ' and (Xl,Yl,Zl) and (x2,Y2,Z2) are, respectively, the upper and lower endpoints 
of the edge. Maintaining high resolution for the interpolated variables is neces-sary for anti-aliasing 
and makes variable resolution a lit- tle easier. As an edge is activated, the 16-bit quantities 'stored 
in the object description are placed in the upper half of a 32 bit number in the active edge descriptor 
(z is already a 32-bit number). By assigning the fractional part of each variable to the lower 16 bits 
of the number, the effects of round-off error in the interpolation are avoided. Since it would be wasteful 
to either store or interpolate meaningless data, storage and interpolation masks are included in the 
object description. The test-bed routines allocate memory in the active edge list according to bits set 
in the storage mask and interpolate active edge variables indicated by the interpolation mask. The scan 
converter can then be totally ignorant of the meaning of the variables that it interpolates, Computer 
Graphics Volume 15, Number 3 August 1981 allowing the user to assign variables to such data as sur- face 
normal components, texture map indices, or reflected intensity values in a completely arbitrary manner. 
One could, for instance, simultaneously per-form Gouraud [9] shading on specified polygons and Phong 
[17] shading on others. The shader need only check the interpolation mask to determine which tech-nique 
applies for each polygon. The list of active edges contains forward and back-ward pointers to allow the 
user to maintain the list in x-order. This ordering is required for visibility calcula- tions using a 
Watkins [20] type algorithm, but isn't always necessary. We typically traverse the list of active polygons 
for each scan line to create spans which are passed to the shader. The shader then relies on either a 
z-buffer or a scan line depth sort to determine visibility. Because of our interest in realism, we have 
had occasion to write several different shaders for various purposes. Two of them -a simple Gouraud shader 
and a Phong shader without bells and whistles - are provided as building blocks for users. We recognize 
that many users are not interested in building display systems. In most cases they only need a mechanism 
for creating images from their own data bases. For such users, a "standard" system has been constructed. 
It stores all polygons prior to scan conver-sion, and it produces a 512 by 512 pixel Gouraud shaded monochrome 
image file. It is left to the user to convert his data to the proper object description format. For anything 
more complicated, we require the user to spend some time learning details of the test-bed. A Generalized 
Scan Conversion Processor A typical scan conversion routine that operates on a passive object description, 
i.e. a list of polygons and their edges, will begin its processing by performing a "bucket sort" on all 
of the edges in the object description based on the maximum y value of each edge. Each bucket corresponds 
to an individual scan line and con-tains a list of pointers to all edges that become active on that scan 
line. For the raster test-bed, the scan converter is a gen- eralization of one described by Lane and 
Carpenter [11] in which every scan line bucket is a stack. Elements that can be pushed onto the stack 
include not only polygon edges, but control points for bi-parametric sur-faces, bounding boxes for procedurally 
defined surfaces, or whatever else the user specifies. At every scan line, elements are popped off the 
stack until it is empty. As each element is popped, a subroutine corresponding to the element's type 
is called. With one exception, each of these routines results in new elements being pushed onto the current 
stack or the stack for a lower numbered scan line (i.e. a scan line whose y value is less than or equal 
to that of the current scan line). The exception is the routine that places polygon edges in the active 
edge list. This means, of course, that in order for all stacks to become empty, each type of element 
must ultimately be reduced to polygons. The process is outlined in figure 2. The par- procedure scan_convert 
begin for each scan line do begin for each active edge do begin bump edge variable values if(edge no 
longer active) begin delete edge if(all edges of poly inactive) begin delete poly end end end while 
stack not empty do begin pop(current scan line) if(type = edge) then begin add edge to active list if(poly 
not yet active) begin add poly to active list end end else if(type = patch) then begin if(patch nearly 
planar) then begin add approximating polygon to object description end else begin split into subpatches 
push(ymaxl) subpatch 1 push(ymax2) subpatch 2 push(ymax3) subpatch 3 push(ymax4) subpatch 4 end end else 
if(type = ...) then begin end end end end Figure 2. Generalized scan conversion routine. ticular patch 
splitter that we use is the Lane-Carpenter algorithm [12], although others [4,6] will work perfectly 
well. This on-the-fly expansion can be extended to any procedure that automatically produces a maximum 
y-value as part of the expansion. To take full advantage of the technique, the expansion procedures must 
also provide built-in garbage collection to prevent the accu-mulation of intermediate surface elements. 
For instance, once a procedurally generated edge is added to the active edge list it must be removed 
from the object description. When all edges of a procedurally generated polygon have been added to the 
active list, then the Computer Graphics Volume 15, Number 3 August 1981 polygon description must be 
returned to free storage. In addition to patch subdivision, we are currently incorporating a fractal 
surface subdivision procedure [3]. The test-bed has also been used to develop procedures for on the fly 
expansion of hierarchical object descrip- tions [18]. The prominent feature of this general purpose scan 
converter is its ability to process mixed object descrip- tions without either first sorting all objects 
by depth or maintaining a z-buffer for the entire image. Like other schemes that allow on-the-fly reduction 
to polygons, it can display complicated scenes without having to store all elements in the scene at once. 
 Span Buffers The span buffer is a linked list of scan line seg- ments formed when the edges of a surface 
element are intersected by a scan line. Use of this type of list to accumulate images which are being 
generated one part at a time is not new. The implementation of Newell, Newell, and Sancha's list priority 
algorithm [14] stored spans, called "beads", as a way of conserving memory. Hackathorn [8] describes 
a technique for accumulating run lengths in main memory to avoid frequent accesses to a bit map version 
of an image stored on disk. The spans referred to in this paper are similar to run length codes, but 
they contain considerably more three dimen- sional information. For each span (figure 3) there are left 
and right edge intersection points such that each span is defined by its left x position, left z position, 
length in x, and Az/Ax. If a Gouraud shader is used in the final display stage, then an intensity value, 
i, and Ai/Ax are part of the span definition. Otherwise the three surface normal components and their 
rates of change are stored. Other quantities which can be interpolated across the span are texture map 
indices, direction to viewer, and direction Span buffer header scan line number no. of spans no. of endpoints 
ptr to first span spans object id ptr to next span ptr to left endpoint ptr to right endpoint endpoint 
x (position) z (position) nx (normal) ny (normal) nz (normal) u (texture index) v (texture index) .... 
etc .... Figure 3. Span buffer format. to light source (for nearby light sources). As many or as few 
quantities as necessary may be included in each span definition. There are a variety of operations that 
can be per- formed on span buffers to produce useful effects. One of the simplest is to maintain the 
list in depth order while building up the image. Then all the properties of list-priority algorithms, 
including the ability to simulate transparency, are available without the need for an a-priori depth 
sort on polygons in the object description. A second operation is garbage collection -i.e. the elimination 
of spans that are completely obscured by others. In its simplest form, this operation is merely a scheme 
for increasing the efficiency of the display pro- cess by minimizing the number of unnecessary opera- 
tions performed by the shader. At its most complex it actually performs the visibility calculation by 
removing all but the visible spans. A summary of the other operations follows: Selective display -spans 
can be tagged to make them selectively invisible. This allows such opera- tions as peeling the skin from 
a complex assembly to reveal the innards. Horizontal panning -a crude way of moving the background portions 
of an image without recreating them. Movement involves only translating the x position of the span. Obviously, 
different parts can be moved at different rates. For 3-D animation this is not a very useful feature 
since better looking results can be obtained by applying all movement in the scene to the object description 
before generat- ing the image. However, the span buffer can be applied to multi-plane (2-1/2 D) animation 
as well as the 3-D variety. Panning is a valuable multi- plane technique. Cyclic animation -a special 
case of selective display in which different spans are made visible in different frames to produce an 
effect like that of color map animation [19] but one that is more powerful. Fast previewing - if several 
frames of animation are stored as span buffers, then the sequence may be rapidly previewed by using a 
fast, low quality shad- ing technique. If the sequence is satisfactory, then it can be run through a 
high quality shader for filming or recording. Since the span buffer is the same in both cases, their 
is no need to redo any of the processing that preceded the shader. Clearly, some care must be exercised 
when performing more than one span buffer operation at a time. For instance, indiscriminate garbage collection 
may interfere with the selective display feature. As one might expect, span buffers require huge amounts 
of memory. Unless their advantages are important for a particular application their use is best avoided 
since they not only consume tens of megabytes of disk space, but require a corresponding amount of time 
for reading, updating, and writing. Computer Graphics Anti-aliasing A small modification to the system 
elements pro- vides an effective means of performing anti-aliasing with span buffer data. Spans can be 
thought of as the top and bottom edges of trapezoids that are created by clip- ping polygons against 
successive scan lines. The left and right edges of these trapezoids are implicitly defined by the span 
endpoints. Some polygons do not reduce to trapezoids when clipped against the scan lines (figure 4). 
If the notion of a span is generalized to include these edges which "fall between the cracks", then the 
span buffer will contain enough information to allow the use of an anti-aliasing tiler such as CatmulI's 
[5]. The necessary addition to the span buffer is a number associated with each endpoint to denote the 
fractional y offset of the endpoint from the bottom scan line. For spans representing the tops and bottoms 
of trapezoids the number is zero. A change is also required in the edge activation procedure. When anti- 
aliasing is not performed, edges that fall between scan lines are discarded. For proper anti-aliasing, 
a subrou-tine is included that converts the edge to a properly for- mated span and adds it to the span 
buffer. The most important feature of the span buffer is that it allows the image to be accumulated one 
piece at a time without losing the information needed for anti-aliasing. Capabilities and Performance 
We originally planned this system to be a develop- ment aid for several of our own projects involving 
a 3-D graphics editor, an animation language, further research on shaders, an the substitution of LSI 
circuits for parts of the display process. In addition we have made it available to people who needed 
a convenient mechanism for making pictures. The entire test-bed is coded in the C language and has been 
tested on two machines (a PDP-11/45 and a VAX-11/780) running the UNIX operating system. 1 Because C 
allows the programmer to declare variables as either 16 or 32 bit integers independently of machine word 
length, identical code will run on either machine. We do find, however, that the larger addressing space 
of the 32 bit machine is necessary for conducting any really useful work. So far our most intense area 
of activity has been shader development. Figure 6 is the product of a shader that incorporates shadows, 
transparency, and tex- ture mapping. The shader uses Williams' shadow algo- rithm [23] and Blinn's bump 
mapping technique [2] for the texture mapped lettering. Three routines were assembled from test-bed elements 
to generate the tex-ture map, to generate the shadow map, and to create the final image. For the case 
of the shadow map and the final image, both the curved and polygonal objects were displayed in a single 
pass through the scan conver-  Volume 15, Number 3 August 1981 Bold Rnes indicate spans l x / Figure 
4. Strip between scan lines sion processor. The ability to simulate transparency is gained by maintaining 
a depth ordered span buffer, Since the object description for figure 7 was too complex to process at 
one time, it was produced by mul- tiple passes through the scan conversion processor and accumulated 
in a span buffer. Figure 8 is a combination of separate images of the chessmen, their reflections, and 
the chess board. The object in figure 9 is a fractal terrain model. Figure 11 is representative of a 
simpler object description being used to measure the performance of hardware augmented z-buffer programs 
[22]. In this project the raster test-bed has actually been cannibalized to yield a bare bones display 
routine. The inner loop of the program is currently being transferred to a set of LSI chips. Figure 5 
summarizes some of the performance measurements taken from the test-bed while producing figures 6,7,9,10 
and ll. Execution times given for the shader include z-buffer initialization and comparisons. Overall, 
we have found that the test-bed permits us to quickly try new ideas without writing much new code. While 
anything but the most naive use of the system requires some effort on the part of a potential user, we 
feel that this is small price when compared to the flexi- bility offered by the test-bed. Acknowledgements 
 This system incorporates features that were sug-gested to us in conversations with a number of helpful 
people including Frank Crow of Ohio State University, Jim Blinn of JPL, Ed Catmull and Loren Carpenter 
of LucasFilm Ltd., Jeff Lane of Boeing, and Lance Willi- ams of NYIT. We also wish to thank Steve Rubin 
for acting as our first user and main guinea pig. Experi-ments with span buffers are an extension of 
work [19] that was performed with support from the National Sci- ence Foundation, Grant MCS-75-06599. 
1 UNIX is a trademark of Bell Laboratories; PDP, and VAX are trademarks of Digital Equipment Corp. Performance 
summary percent time by routine picture polys edges cpu time l scan shade misc (seconds) convert Figure 
6 23554 70661 341 36 61 Figure 7 32000 44000 520 60 40 Figure 9 12492 19450 320 20 80 Figure 10 18188 
28421 250 70 30 Figure 11 1800 2760 16 41 58 all times are for 512x512 images without anti-aliasing. 
Figure 5. Statistics for five pictures. References [1] Baecker, R., Digital video display systems and 
dynamic graphics, Computer Graphics, 13,2 (August 1979), 48. [2] Blinn, J.F., Simulation of wrinkled 
surfaces, Com-puter Graphics, 12,2 (August 1978),286. [3] Carpenter, L.C., and Fournier, A., and Fussell, 
D., Display of fractal curves and surfaces, to appear, Comm. ACM. [4] Catmull, E., A subdivision algorithm 
for computer display of curved surfaces, PhD thesis, University of Utah, 1974. [5] Catmull, E., A hidden 
surface algorithm with anti- aliasing, Computer Graphics, 12,3 (August 1978), 6. [6] Clark, J.H., A . 
fast algorithm for rendering parametric surfaces, supplement to Computer Graph- ics (distributed to SIGGRAPH 
'79 attendees), August 1979. [7] Crow, F.C., Computer graphics in the entertain-ment industry, Computer, 
11,3 (September 1977), 11. [8] Csuri, C., Hackathorn, R., Parent, R., Carlson W., and Howard, M., Towards 
an interactive high visual complexity animation system, Computer Graphics, 13,2 (August 1979), 289. [9] 
Gouraud, H., Continuous shading of curved sur-faces, IEEE Trans. Cmptrs, C-20 (June 1971), 623. [10] 
Jackson, J.H., Dynamic scan-converted images with a frame buffer display device, Computer Graphics, 14,3 
(July 1980), 163. [ll]Lane, J.M., and Carpenter, L.C., A generalized scan line algorithm for the computer 
display of parametrically defined surfaces, Computer Graphics andlmage Processing, vol. 11 (1979), 290. 
[12] Lane, J.M., Carpenter, L.C., Blinn, J.F., and Whit- ted, T., Scan line methods for displaying parametri- 
cally defined surfaces, Comm. ACM, 23,1, (January 1980), 23. [13] Myers, A.J., An efficient visible surface 
program, Report to National Science Foundation, Grant No. DCR74-00768 A01, Computer Graphics Research 
Group, Ohio State Univ., July 1975. [14] Newell, M.E., Newell, G.S., and Sancha, T.L., A Solution to 
the hidden surface problem, Proc. of the ACM annual conference, 1973, 443. [15] Newell, M.E., The utilization 
of procedure models in computer synthesized images, PhD thesis, University of Utah, 1975. [16]Newman, 
W.M., and Sproull, R., Principles of Interactive Computer Graphics, McGraw-Hill, 1973. [17] Bui-Tuong 
Phong, Illumination for computer gen- erated pictures, Comm. ACM, 13,6 (June 1975), 311. [18] Rubin, 
S.M., The representation and display of scenes with a wide range of detail, submitted for publication. 
[19] Shoup, R., Color table animation, Computer Graph- ics, 13,2 (August 1979), 8. [20] Watkins, G.S., 
A real-time hidden surface algo-rithm, PhD thesis, Univ. of Utah, 1970. [21] Whitted, J.T., A processor 
for display of computer generated images, dissertation, North Carolina State University, August 1978. 
[22] Whitted, T., Hardware enhanced 3-D raster display systems, Proceedings of Canadian Man-Computer 
Communication Conference, June 1981. [23] Williams, L., Casting curved shadows on curved surfaces, Computer 
Graphics, 12,2 (August 1978), 270. 276   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806816</article_id>
		<sort_key>279</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>34</seq_no>
		<title><![CDATA[A method of interactive visualization of CAD surface models on a color video display]]></title>
		<page_from>279</page_from>
		<page_to>287</page_to>
		<doi_number>10.1145/800224.806816</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806816</url>
		<abstract>
			<par><![CDATA[<p>To introduce rendered surface display technology into the production design environment, many CAD operations envision a single color video display device for <italic>download</italic> processing of selected model pictures. Creation of a single image from a typical industrial CAD model involving a large number of higher order curved surfaces will normally require a minimum of several minutes' delay for data acquisition and visible surface computation. This paper describes a method that extends the visible surface operation to give the designer an interactive tool with which he can more effectively comprehend model composition. Instead of defining data for a single image, an intermediate three-dimensional structured file is created. A <italic>tailored</italic> display program manipulates the structured data file allowing a user to interactively create surface rendered images. Program capabilities include:</p> <p>&#8226; Section and cutaway viewing</p> <p>&#8226; Selected surface color, transparency, or translucency</p> <p>&#8226; Shadows projected from desired light-source locations</p> <p>Changing or combining any of these facilities requires only seconds for a new image to be created.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Color video display]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Computer-aided design]]></kw>
			<kw><![CDATA[Depth buffer]]></kw>
			<kw><![CDATA[Geometric modeling]]></kw>
			<kw><![CDATA[Hidden surface removal]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor>Computer-aided design (CAD)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Hidden line/surface removal</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010472.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Architecture (buildings)->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P223558</person_id>
				<author_profile_id><![CDATA[81100484386]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Atherton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Corporate Research and Development, Schenectady, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>806007</ref_obj_id>
				<ref_obj_pid>800196</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Appel, A., "The Notion of Quantitative Invisibility and the Machine Rendering of Solids," Proceedings ACM, 1967 National Conference.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807461</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Artzy, E., Frieder, G., and Herman, G.T., "The Theory, Design, Implementation and Evaluation of a Three-Dimensional Surface Detection Algorithm," SIGGRAPH 1980 Proceedings.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bouknight, W.J., and Kelley, K.C., "An Algorithm for Producing Half-Tone Computer Graphics Presentations with Shadows and Movable Light Sources," SJCC, AFIPS, Vol. 36, 1970.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807458</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Csuri, C., Hackathorn, R., Parent, R., Carlson, W., and Howard, M., "Towards an Interactive High Visual Complexity Animation System," SIGGRAPH 1979 Proceedings.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358838</ref_obj_id>
				<ref_obj_pid>358826</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Dyer, C.R., Rosenfeld, A., and Samet, H., "Region Representation: Quadtrees from Binary Codes," CACM, June 1980.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807468</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Fuchs, H., Clark, J., Cohen, D., Newell, M., Parke, F., and Sproull, B., "Trends in High Performance Graphics Systems," SIGGRAPH 1980 Conference Panel.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Goldstein, R. and Nagel, R., "3-D Visual Simulation," Simulation, January 1971.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Hinds, J.K., Kuan, L.P., "Sculptured Surface Technology as a Unified Approach to Geometric Definition," Computer and Automation Systems Association of the Society of Manufacturing Engineers, 1979.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Kay, D.S., "Transparency, Refraction and Ray Tracing for Computer Synthesized Images," M.S. Thesis, Cornell University, January 1979.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Nagel, R.N., Braithwaite, W.W., and Kennicott, P.R., "Initial Graphics Exchange Specification IGES Version 1.0," National Bureau of Standards, January 1980.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Newell, M., Newell, R., and Sancha, T., "A New Approach to the Shaded Picture Problem," Proceedings ACM, 1972 National Conference.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807435</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Rhodes, M.L., "An Algorithmic Approach to Controlling Search in Three-Dimensional Image Data," SIGGRAPH 1979 Proceedings.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Romney, G., Watkins, G., and Evans, D., "A Real Time Display of Computer Generated Half-Tone Perspective Pictures," IFIP 1968, Amsterdam, North-Holland, p. 973.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358836</ref_obj_id>
				<ref_obj_pid>358826</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Samet, H., "Region Representation: Quadtrees from Binary Codes," CACM, March 1980.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Watkins, G., "A Real-Time Visible Surface Algorithm," Computer Science Department, University of Utah, UTECH-CSC-70-101, 1970.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Whitted, J.T., "An Improved Illumination Model for Shaded Display," CACM, June 1980.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 A METHOD OF INTERACTIVE VISUALIZATION OF CAD SURFACE 
MODELS ON A COLOR VIDEO DISPLAY Peter R. Atherton General Electric Company Corporate Research and Development 
Schenectady, New York 12345 Abstract To introduce rendered surface display technology into the production 
design environment, many CAD operations envision a single color video display device for download processing 
of selected model pictures. Creation of a single image from a typical industrial CAD model involving 
a large number of higher order curved surfaces will normally require a minimum of several minutes' delay 
for data acquisition and visible surface computation. This paper describes a method that extends the 
visible surface operation to give the designer an interactive tool with which he can more effectively 
comprehend model composition. Instead of defining data for a single image, an intermediate three- dimensional 
structured file is created. A tailored display program manipulates the structured data file allowing 
a user to interactively create surface rendered images. Program capabilities include: Section and cutaway 
viewing  Selected surface color, transparency, or translucency  Shadows projected from desired light-source 
locations  Changing or combining any of these facilities requires only seconds for a new image to be 
created. Key Words: computer graphics, computer-aided design, color video display, hidden surface removal, 
depth buffer, geometric modeling Computer Review Categories: 8.2 Permission to copy without fee all 
or part of this material is granted provided that the copies are not made or distributed for direct com- 
mercial advantage, the ACM copyright notice and the title of the publication and its date appear, and 
notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. &#38;#169;1981 ACM O-8971-045-1/81-0800-0279 
$00.75 1. Introduction For the purpose of three-dimensional model visuali- zation, many hidden surface 
removal algorithms employ a scan-line or image space depth buffer [3,7,11,13,15]. This approach is conceptually 
based upon a three-dimensional array whose nodes directly correspond to the picture elements of a display 
screen. Visibility of each surface is derived in terms of the X-Y elements by performing depth comparisons 
to each relevant node in the array. In concept, the three-dimensional display buffer to be explored in 
this paper is a direct extension of the two-dimensional visible surface structure (Fig-ure 1). The data 
describing hidden surface elements, instead of being discarded, are maintained in a depth priority order 
linked to each X-Y element location. In recent years, some interesting computer graphics work has been 
accomplished using a three-dimensional display file. In 1979, Kay described a method of creat- ing sophisticated 
synthesized images [9] that was similar to the method published by Whitted that same year [16]. Both 
authors employed a methodology of firing a visual ray from the viewpoint into the model en- vironment 
and subsequently reflecting or refracting it into sub-rays. The major difference between the two al- 
gorithms is that Whitted used the original mathematical surface model definitions throughout, while Kay 
prepro- cessed the surface model, constructing a three-dimensional frame buffer. Although Kay's method 
is limited to picture element accuracy, it is general enough to handle any surface type. In 1979, Csuri 
et al. described an animation system that is centered around the use of a three-dimensional frame buffer 
[4]. Utilizing the implicit generality offered by this data structure, they created images of highly 
complex three-dimensional models comprising hundreds of thousands of triangular surfaces. Csuri made 
one particularly astute application by noting the simplicity of combining individual models. For exam-ple, 
a three-dimensional landscape setting could have a bird fly through it for several frames without having 
to regenerate the three-dimensional pixel landscape data. Following the two-dimensional techniques for 
image scene reconstruction, such as those described by Samet, Dyer, and Rosenfeld [5,14], many research 
groups are developing sophisticated algorithms for model compac- tion and reconstruction from three-dimensional 
frame buffer data. In 1979, Rhodes published a method 279 Computer Graphics Volume 15, Number 3 August 
1981 ! I I i I I I I I I I I I I I A. Viewpoint projection B. Visible surface depth buffer C. Three-dimensional 
display buffer Figure 1. The three-dimensional display buffer is an extension of the visible surface 
depth buffer. minimizing storage and computing requirements neces-sary to locate the contours of randomly 
shaped objects described by cross section image planes [12]. In 1980, Artzy, Frieder, and Herman developed 
a method by which surfaces might be derived from three-dimensional volume element or "voxel" data [2]. 
In past efforts employing a three-dimensional frame buffer, the technology of image generation has been 
oriented towards an animation ideology. Consequently, a great deal of time is still required to create 
each image for a mathematically complex curved surface model. The kernel of the speed problem is that 
rotation of a three-dimensional surface model does not lend itself to the digital grid representation 
of video display. When the rotation capability is left at a traditional processing speed level, the characteristics 
can be effectively ex-plored which make the three-dimensional frame buffer such a congenial environment 
to interactively study CAD surface and solid models. The remainder of this paper is devoted to the methodology 
behind this ap-proach and the various applications and implementations that might be considered. 2. Potential 
Forms of the Three-Dimensional Display Structure At this point, no particular structure of the three-dimensional 
display buffer has been specified. Some readers may have already considered a number of different structures 
that might be employed. For an ap- plication like fluid flow, accurate floating point depths could be 
maintained at each X-Y picture element corresponding to surface point locations (Figure 2-A). For solid 
modeling, a run-length format might be used to designate a homogeneous material continuum (Figure 2-B). 
A mathematical function could replace material depth values when describing continuous stress or heat 
variations through a solid. A rather simplistic yet effective three-dimensional frame buffer structure 
will be considered to make the following image generation procedures easier to explain. The basic organization 
can be thought of as a three-dimensional grid matrix whose fundamental elements are essentially three-dimensional 
volume elements or "voxels." These voxels are derived from equally spaced increments along the orthogonal 
X, Y, and jx A. Fluid flow model B. Solid Model Figure 2. Various forms of the three-dimensional display 
buffer 280 Computer Graphics Volume 15, Number 3 August 1981 Z-depth axes on an integer basis. Creation 
of a data model to be contained in this three-dimensional grid matrix involves a process similar to the 
rasterization technique used in many image space, hidden surface re- moval programs. Essentially, the 
process of "voxeliza- tion" delineates each surface of a model by the three- dimensional grid elements 
that contain it in space (Figure 2-C). To conserve space, the three-dimensional grid would employ a compacted 
sparse matrix format by including only voxels that were a part of a surface description and by combining 
any homogeneous con-nected elements. Henceforth, this three-dimensional, reduced sparse matrix will be 
referred to as an object buffer matrix. Figure 2-C. Three-dimensional grid Some negative factors involved 
in the use of the ob- ject buffer matrix should be acknowledged. Although creation of the object matrix 
demands only a little more computing time than a single static image, it does re-quire significantly 
more data space depending upon im- age complexity and display resolution. This factor does affect implementation 
and is directly addressed in a later section of this paper. 3. Image Generation The speed of image generation 
was the foremost concern in the development of the algorithms to be described in this section. Consequently, 
the inherent intricacies of reflected and refracted light modeling, which would cost a considerable increase 
in processing time, were put aside. For the image generation descrip- tions that follow, the terms "translucency" 
and "tran-sparency" refer only to an approximation of the amount of light passing through a surface or 
mass. This deficiency of physical-world realism might initially be perceived as a liability, but it has 
evolved as an asset for the application of interactive CAD model visualization. A complex CAD assembly 
can be visually dismantled to reveal various components by seeing through selected parts without the 
physical problems of glare and distor- tion. Initially, it is assumed that the object matrix voxels will 
contain only one piece of information that describes the display color of the related surface point. 
As with most surface rendering algorithms, the display color is determined from a combination of a preassigned 
hue with an intensity that relates the lightsource and surface orientation by Lambert's cosine law. In 
terms of the object matrix organization, voxels along the Z-depth axis are ordered from nearest to farthest 
from the viewpoint. Modifications of this basic structure will be noted in the algorithm descriptions 
that follow. Hidden Surface Removal Once the model surface data have been delineated into the three-dimensional 
object buffer, hidden surface removal is a trivial operation. For each X-Y picture display element, the 
nearest voxel to the viewpoint is rendered (Figures 3-A and 4-A). In terms of an algo-rithm, the object 
matrix is traversed once, selecting the first element along each Z-depth vector to be displayed. Thus, 
implementation of a hidden surface image re-quires only branching operations for the matrix traversal 
procedure. Cutaway and Section Views Using various interactive devices, a designer can easily define 
a slicing plane that will pass through a selected area of the object matrix. A cutaway view can then 
be generated by ignoring the voxels that lie in front of the cutting plane and applying the hidden sur-face 
removal process to the remainder of the object ma- trix. Algorithmically, each Z-depth vector is searched 
to find the first element that falls behind the point where the cutting plane intersects that vector 
(Figure 3-B). Figure 3-A. Hidden surface removal Figure 3-B. Cutaway Computer Graphics Section views 
can be generated by displaying only those voxels that are actually intersected by the cutting plane. 
For an improved visualization, cutaway views can be enhanced by highlighting the section cut edges and 
thus improving visual comprehension (Figure 4-C). Cutaway and section views do not have to be limited 
to infinite planer cutting surfaces. Higher order curved surfaces can also be used to slice through an 
environ-ment under user control to produce certain desired effects. Bounded surfaces can be used to selectively 
separate or cut away portions of the model. By combin- ing bounded surfaces to define a volume, a user 
could visually isolate the region of interest he wants to focus on. At this point, the important concept 
to stress is that the image regeneration process is very fast. This speed is inherent in the algorithmic 
simplicity and the natural relationship between the object matrix format, the video display, and the 
digital computer. A program to create a section-cut or cutaway image just needs to make a single pass 
through the object buffer and execute only branch and compare statements for matrix traversal. It is 
this simplicity that leads to fast program execution, allowing the designer to study a model in an interactive 
manner. Therefore, the cutaway and sectioning algorithms could become viable tools for computer-aided 
production design. Transparency The notion of transparency directly implies an attri-bute that must be 
attached to every surface definition. Accordingly, each three-dimensional matrix element must either 
contain this information or accommodate an association to its surface source. When a user specifies a 
surface to be transparent, the hidden surface removal program simply ignores the appropriate object matrix 
elements (Figures 4-B and 4-D). On an X-Y display pixel basis, the program will search for and display 
the first element along the Z-depth vector that is not tran- sparent (Figure 3-C). Again, implementation 
requires only a single traversal of the three-dimensional object matrix executing branch and compare 
operations. Thus, image creation involving selective transparency is still very fast and can be executed 
comfortably in an interac- tive environment. 11 /  Volume 15, Number 3 August 1981 through an associated 
surface voxel, the translucency al- gorithm will remember what translucent surface ele-ments it passed 
through and utilize that information in determining the resulting color to be displayed. The amount of 
influence that the translucent surface has on the final color is directly related to the amount of light 
that can pass through it. Consider for example a Z-depth vector whose closest element (to the viewpoint) 
has a translucency association and has been designated blue. If the next element behind it has an opaque 
asso- ciation, then the color rendered is that color assigned to the opaque element tinted with an appropriate 
amount of blue (Figures 3-D and 4-F). Algorithmic execution speed is hindered little by the incorporation 
of translu- cency because only a single traversal of the object ma- trix is still required. When a translucent 
element is found along a Z-depth vector, an addition operation is sufficient to perform the color modification. 
b- Figure 3-D. Translucency Shadows With the three-dimensional object buffer, shadow generation becomes 
a very general operation since any number of light sources can be located inside or outside the data 
model environment. In a manner similar to the method described by Appel [1] and Bouknight and Kelley 
[3], a ray is projected from every visible surface element towards each light source (Figure 4-E). If 
the light vector intersects an opaque voxel, the surface ele- ment is in shadow (Figure 3-E). If the 
light vector passes through a translucent voxel, the surface element is in a tinted shadow. Figure 3-C. 
Transparency Translucency Translucency is a natural extension of transparency. Where both operations 
allow the visual ray to pass Figure 3-E. Shadow generation Computer Graphics Volume 15, Number 3 August 
1981 Shadow generation is the one image capability that cannot be accomplished in a single pass through 
the ob- ject matrix buffer. For every light vector that is con- sidered, various portions of the object 
buffer matrix must be directly accessed to determine the shadow pro- perties. Implications of this characteristic 
will be dis-cussed further in the implementation section of this pa- per. Combinations of Effects Utilizing 
the flexibility of the object buffer matrix and the image generation algorithms previously described, 
a designer can create a vast number of very interesting and informative images. For example, a model 
with some surfaces designated as translucent can also be viewed with the application of a cutaway opera- 
tion (Figure 4-H). It may be desirable for particular surfaces to be designated as opaque to light-source 
rays yet transparent to the viewer. In this way, an architect could visually get inside of his model 
to better analyze shadowing implications. Cutaway views could be gen- erated selectively so that the 
user could look into one component while the remainder of the model would be unaffected. The cutting 
surface used in a cutaway view can be delineated by section cutting external model sur- faces, and performing 
a cutaway operation on the remainder of the model (Figure 4-G). To reiterate, the object matrix structure 
is very amenable to rapid and systematic display of advanced surface and solid model imagery. Therefore, 
this ap- proach can be taken to provide an effective tool for im- proved visualization of sophisticated 
three-dimensional CAD surface models. 4. Interactive Response of Various Implementations In terms of 
CAD interactive response times, there are three general ways of implementing a three- dimensional object 
buffer display program in software. Owing to the speed and simplicity of the image genera- tion algorithms, 
response effectiveness is primarily dependent upon access to the three-dimensional object buffer matrix 
and to the color video display memory. For operations limited to smaller minicomputers, the three-dimensional 
object matrix can be obtained by the host computer from some peripheral storage device like a magnetic 
tape or disk drive. In this way, each Z-depth matrix vector could be sequentially accessed and subse- 
quently output to a color video display system in scan- line fashion. The speed of this type of implementation 
is obviously dependent upon the amount of time re- quired to read the object data from the peripheral 
storage device. Utilizing this approach, sophisticated color video images involving hidden surface removal, 
transparency, translucency, cutaway, and sectioning can be generated from the object matrix in a matter 
of seconds or minutes. However, generation of images with shadows is generally unreasonable because of 
the random nature of the data access requirements. Even without shadowing, it is very significant that 
even small- er computer systems with relatively low-cost color video display systems can produce sophisticated, 
rendered sur- face imagery in a reasonably interactive manner. Image generation execution times can be 
greatly re- duced when the three-dimensional object matrix buffer can be loaded into either the host 
computer memory or a fast, directly addressable peripheral memory. For any CAD model of reasonable size, 
the host computer would be required to have some sort of virtual or ex-tended memory capability. One 
very practical source of a fast peripheral device would be the color video display frame buffer memory 
which is usually accessible via a DMA type of interface. Using the hardware zoom and scroll capability 
that is standard on most commercial, high-resolution color video display systems, one portion of display 
memory can be viewed while the remainder is used for model data storage. With the object descrip- tion 
matrix directly accessible, images that include sha- dows are quite feasible to produce. In general, 
locating display buffer data on the peripheral display memory will require a less sophisticated host 
computer, but im- age generation times will be shorter when the data re-side in the host computer. In 
either case, communica- tion with the color video display system will usually be the slowest link for 
both implementations. Nonsha-dowed images will typically require only a few seconds to generate, while 
shadowed images will demand a minute or more. The ideal implementation would involve a powerful processor 
which has very fast direct access to both the three-dimensional object description matrix and the color 
video display memory. There are commercially available color video frame buffer display systems that 
offer such capabilities and are readily adaptable to the image generation schemes previously described. 
With such an implementation, near real-time execution speeds can be achieved for nonshadowed images, 
and shadowed images can be generated in a very few seconds. With this kind of implementation, a designer 
could simply turn a dial and visually cut through a so- phisticated color video rendered surface display 
of his creation. Each progressive implementation involves a magni-tude of speed improvement and a similar 
hardware/software cost increase. Yet, each one is a very significant improvement over the "traditional" 
static image display presently being proposed to computer-aided design people. 5. Applications Since 
the primary motivation for this work is to pro- vide a more viable design aid for CAD engineers, the 
major theme of this paper has been the color video display of three-dimensional surface models. Exploiting 
the visualization capabilities described herein, a mechan- ical engineer could interactively inspect 
complicated as- sembly models and study various intercomponent link- ages (Figure 5). An architect could 
exhibit various ele- ments of a building design, add or delete selected modules, and demonstrate lighting 
and shadow charac- teristics. With its extended methods of data extraction, the three-dimensional display 
buffer can provide a natural vehicle for the color video display of more amorphous geometries. For example, 
data generated from a series  Computer Graphics Volume 15, Number 3 August 1981 COVPUTERVI S I ON DESIGN 
STATION I IGES ] SURFACE DEFINITIONS CONVERT TO RATIONAL BICUBIC ,f CON~RT SS COLOR VIDEO DISPLAY TO 
DISPLAy FORM ,I,I DISPLAY I NFOI~VIATION I~E~I~ I S'NIttAVISI(ll ' DISPLAYpRoGRAM k (MAGI INC,) DISPLAY 
CC~S INTERACTIVE USER HOST COMPUTER (PDP]]Y70 OR VAX) Figure 6. System organization computer-aided tomography 
scans can be interpolated into a three-dimensional format. Using the image gen- eration techniques previously 
described, tissues could be systematically dissected, looked into, shadowed, and peeled off to provide 
a superior visualization of the hu- man anatomy. In a similar manner, various geological strata could 
be visually sectioned and peeled to reveal oil reservoirs or potential earthquake formations. 6. Trial 
Implementations The color video images in Figure 4 were generated from geometric models originally created 
on the Com- putervision turnkey design system (Figure 6). Surface definitions were transmitted to a PDPll/70 
minicom-puter and converted into a rational bicubic format suit- able for use in the Computer-aided Sculptured 
Surface pre-APT Processor commonly referred to as CAS-PA [8]. In the future, an interface to the Initial 
Graph- ics Exchange Specification (IGES) neutral data base will be developed so that CAD surface models 
can be ac-cessed from any turnkey design system [10]. Using an extended bicubic hidden surface removal 
program, the sculptured surfaces were transformed to a desired view and delineated into the object matrix 
format previously described. The three-dimensional display buffer used to create the images for Figure 
5 was built with data extracted from the SYNTHAVISION program developed by MAGI Inc. Instead of taking 
the form of an object ma- trix, this display buffer was implemented in the form of a solid object description. 
In concept, each Z-depth vec- tor contains the entry and exit points of all solid objects that lie in 
its path (Figure 2-B). The region between these points is understood to represent an area of ma- terial 
continuum which must be accounted for during cross section, shadow, and translucency determinations. 
Early images were created by sequentially reading the object matrix from a disk file and displayed at 
256x256 resolution on a DeAnza 2000 series color video display. Initially, the program was developed 
to interactively generate section and cutaway views of sculptured sur-face data. As the advanced imaging 
potential became obvious, the program was moved to a VAX 11/780 computer where the object matrix could 
be directly ac- cessed from virtual memory. Subsequently, a shadow- ing capability was added to the existing 
transparency, translucency, and sectioning facilities. The images in Figure 5 were taken from an Ikonas 
color video display system and displayed at 256x256 resolution. A fast im- plementation will be developed 
after delivery is taken on the Ikonas microprocessor which is integrated within the system architecture. 
7. Conclusion A technique of interactively analyzing complex three-dimensional CAD surface models on 
a color video display has been presented. The method involves a unique and effective utilization of a 
three-dimensional display buffer structure. By putting aside real-time rota- tional capability [6], the 
user can employ a variety of imaging techniques that can be selectively applied to visualize a CAD model 
in an interactive mode. The three-dimensional display buffer has also been shown to be a valuable resource 
for visually analyzing other models, such as CAT scan and geological forms. Even with data compaction 
techniques, the three-dimensional display buffer does require a relatively large 286 Computer Graphics 
Volume 15, Number 3 August 1981 data space. However, the extra memory or peripheral storage may be an 
insignificant trade-off for the speed and sophistication of image generation that is gained. For example, 
previous algorithms involving a sizable number of translucent and opaque bicubic surfaces could often 
require hours and even days for a shadowed image to be generated when simply moving a light source. With 
a three-dimensional display buffer, similar imagery may be generated in minutes or even seconds. Since 
the cost of memory is continually declining, future benefits will be even more prolific. 8. References 
 1. Appel, A., "The Notion of Quantitative Invisibility and the Machine Rendering of Solids," Proceedings 
ACM, 1967 National Conference. 2. Artzy, E., Frieder, G., and Herman, G.T., "The Theory, Design, Implementation 
and Evaluation of a Th/'ee-Dimensional Surface Detection Algo-rithm," SIGGRAPH 1980 Proceedings. 3. 
Bouknight, W.J., and Kelley, K.C., "An Algorithm for Producing Half-Tone Computer Graphics Presentations 
with Shadows and Movable Light Sources," SJCC, AFIPS, Vol. 36, 1970. 4. Csuri, C., Hackathorn, R., Parent, 
R., Carlson, W., and Howard, M., "Towards an Interactive High Visual Complexity Animation System," SIG-GRAPH 
1979 Proceedings. 5. Dyer, C.R., Rosenfeld, A., and Samet, H., "Region Representation: Quadtrees from 
Binary Codes," CACM, June 1980. 6. Fuchs, H., Clark, J., Cohen, D., Newell, M., Parke, F., and Sproull, 
B., "Trends in High Performance Graphics Systems," SIGGRAPH 1980 Conference Panel.  7. Goldstein, R. 
and Nagel, R., "3-D Visual Simula- tion," Simulation, January 1971. 8. Hinds, J.K., Kuan, L.P., "Sculptured 
Surface Tech- nology as a Unified Approach to Geometric Definition," Computer and Automation Systems 
Asso- ciation of the Society of Manufacturing Engineers, 1979. 9. Kay, D.S., "Transparency, Refraction 
and Ray Tracing for Computer Synthesized Images," M.S. Thesis, Cornell University, January 1979. 10. 
Nagel, R.N., Braithwaite, W.W., and Kennicott, P.R., "Initial Graphics Exchange Specification IGES Version 
1.0," National Bureau of Standards, January 1980. 11. Newell, M., Newell, R., and Sancha, T., "A New 
Approach to the Shaded Picture Problem," Proceed-ings ACM, 1972 National Conference, 12. Rhodes, M.L., 
"An Algorithmic Approach to Con- trolling Search in Three-Dimensional Image Data,"  SIGGRAPH 1979 Proceedings. 
13. Romney, G., Watkins, G., and Evans, D., "A Real Time Display of Computer Generated Half-Tone Perspective 
Pictures," IFIP 1968, Amsterdam, North-Holland, p. 973. 14. Samet, H., "Region Representation: Quadtrees 
from Binary Codes," CACM, March 1980. 15. Watkins, G., "A Real-Time Visible Surface Algo- rithm," Computer 
Science Department, University of Utah, UTECH-CSC-70-101, 1970. 16. Whitted, J.T., "An Improved Illumination 
Model for Shaded Display," CACM, June 1980.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806817</article_id>
		<sort_key>289</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>35</seq_no>
		<title><![CDATA[3D graphics and the wave theory]]></title>
		<page_from>289</page_from>
		<page_to>296</page_to>
		<doi_number>10.1145/800224.806817</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806817</url>
		<abstract>
			<par><![CDATA[<p>A continuing trend in computer representation of three dimensional synthetic scenes is the ever more accurate modelling of complex illumination effects. Such effects provide cues necessary for a convincing illusion of reality. The best current methods simulate multiple specular reflections and refractions, but handle at most one scattering bounce per light ray. They cannot accurately simulate diffuse light sources, nor indirect lighting via scattering media, without prohibitive increases in the already very large computing costs.</p> <p>Conventional methods depend implicitly on a <italic>particle</italic> model; light propagates in straight and conceptually infinitely thin rays. This paper argues that a <italic>wave</italic> model has important computational advantages for the complex situations. In this approach, light is represented by wave fronts which are stored as two dimensional arrays of complex numbers.</p> <p>The propagation of such a front can be simulated by a linear transformation. Several advantages accrue. Propagations in a direction orthogonal to the plane of a front are convolutions which can be done by FFT in O(<italic>n</italic> log <italic>n</italic>) time rather than the <italic>n <supscrpt>2</supscrpt></italic> time for a similar operation using rays. A typical speedup is about 10,000. The wavelength of the illumination sets a resolution limit which prevents unnecessary computation of elements smaller than will be visible. The generated wavefronts contain multiplicities of views of the scene, which can be individually extracted by passing them through different simulated lenses. Lastly the wavefront calculations are ideally suited for implementation on available array processors, which provide more cost effective calculation for this task than general purpose computers.</p> <p>The wave method eliminates the aliasing problem; the wavefronts are inherently spatially filtered, but substitutes diffraction effects and depth of focus limitations in its stead.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39080740</person_id>
				<author_profile_id><![CDATA[81100092634]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hans]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Moravec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Robotics Institute, Carnegie-Mellon University, Pittsburgh, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Feynman, R. P., R. B. Leighton and M. Sands. The Feynman Lectures on Physics. Addison-Wesley Publishing Company, Reading, Massachusetts, 1963.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Gilchrist, A. L. "The Perception of Surface Blacks and Whites." Scientific American 240, 3 (March 1979), 112-124.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Goodman, J. W., Introduction to Fourier Optics. McGraw-Hill Book Company, San Francisco, 1968.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Kay, D. S. Transparency, Refraction and Ray Tracing for Computer Synthesized Images. Master Th., Cornell University, January 1979.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>364111</ref_obj_id>
				<ref_obj_pid>364096</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Lesem, L. B., P. M. Hirsch and J. A. Jordan, Jr. "Computer Synthesis of Holograms for 3-D Display." Comm. ACM 11, 10 (October 1968), 661-674.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Reddy, D. R. and S. Rubin. Representation of Three- Dimensional Objects. Computer Science Department CMU-CS-78-113, Carnegie-Mellon University, April, 1978.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Whitted, T. "An Improved Illumination Model for Shaded Display." Comm. ACM 23, 6 (June 1980), 343-349.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Ceml}uter Grapltic$ Volume 15, Number 3 August 1981 3D Graphics and the Wave Theory Hans P. Moravec 
Robotics Institute Carnegie-Mellon University Pittsburgh, PA 15213 A bst ract A continuing trend in 
computer representation of three dimensional synthetic scenes is the ever more accurate modelling of 
complex illumination effects. Such effects provide cues necessary for a convincing illusion of reality. 
The best current methods simulate multiple specular reflections and refractions, but handle at most one 
scattering bounce per light ray. They cannot accurately simulate diffuse light sources, nor indirect 
lighting via scattering media, witllout prohibitive increases in the already very large computing costs. 
Conventional methods depend implicitly on a particle model; light propagates in straight and conceptually 
infinitely thin rays. This paper argues that a wave model has important computational advantages for 
the complex situations. In this approach, light is represented by wave fronts which are stored as two 
dimensional arrays of complex numbers. The propagation of such a front can be simulated by a linear transformation. 
Several advantages accrue. Propagations in a direction orth0gonal to the plane of a front are convolutions 
which can be done by FFT in O(n Iogn) time rather than the n 2 time for a similar operation using rays. 
A typical speedup is about 10,000. The wavelength of the illumination sets a resolution limit which prevents 
unnecessary computation of elements smaller than will be visible. The generated wavefronts contain multiplicities 
of views of the scene, which can be individually extracted by passing them through different simulated 
lenses. Lastly the wavefront calculations are ideally suited for implementation on available array processors, 
which provide more cost effective calculation for this task than general purpose computers. The wave 
method eliminates the aliasing problem; the wavefronts are inherently spatially filtered, but substitutes 
diffraction effects and depth of focus limitations in its stead. Permission to copy without fee all 
or part of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. 01981 ACM O-8971-045-1/81-0800-0289 $00.75 
 1. Introduction The realism in computer synthesized images of 3D scenes has gradually increased from 
engineering drawings of simple shapes to representations of complex situations containing textured specular 
reflectors and shadows. Until recently the light models have been to first order; the effect of only 
one bounce of light from the light sources to the eye via a surface was simulated. The stakes have recently 
gone up again with the introduction of ray tracing methods that simulate multipl~ non-dispersive refractions 
and specular reflections [4] [7]. With each increment in the faithfulness of the process new properties 
of real light had to be added. Thus far the evolution has been in the direction of the particle theory 
of light, probably because straight rays preserve most of the properties of 3D computer graphics' engineering 
drawing roots. An obvious and desirable next step in the process is the simulation of scattering, as 
well as specular, reflection of light from surface to surface. That such effects are important is shown 
graphically by the subjective impressions of increasing realism as the models become more complex (see 
references in [7]) and objectively in visual psychology experiments such as those by Gilchrist [2], in 
which subjects were able to intuitively separate the effects of surface albedo and light source brightness 
by noting the relative intensity of scattered illumination. While a specular/refractive bounce can convert 
a ray from a simulated camera or lightsource into two rays, a scattering can make hundreds or thousands, 
heading off in a multitude of directions. Since pictures modelling only specular bounces themselves take 
about an hour of computer time to generate, simulation of multiple scattering (and incidentally, extended 
light sources) by extension of conventional methods is out of reach for the immediate future.  2. A 
Way Out One solution may be a different approach in simulating the physical reality. The ray representation 
implicitly assumes an effectively infinite precision in the position of objects and the heading of rays. 
This leads to some unnecessary calculations, as when several rays bouncing from nearly the same surface 
point in nearly the same direction (perhaps originating from different light sources) are propagated 
by separate calculations. We can evade these and other inefficiencies by using the particle theory's 
major competitor, the wave theory of light. Rather than multitudes of rays bouncing from light sources 
and among objects, the illumination is done by complex wavefronts 289 Computer 6raphics Volume 15, Number 
3 August 1981 described over large areas. Such wavefronts can be represented by arrays of complex numbers, 
giving the amplitude and relative phase of portions of the front, embedded in the simulated space. Lesem.et. 
al. used this approach [5] to make physical holograms (though of disappointingly low quality). Note that 
a plane array of complex wave coefficients is very like a window into a scene. The wavefront, fully described 
by the coefficients, can represent multiple point and extended sources of light at various distances 
behind (or even in front of) the window. Figure 1 is a simple example, a slice through a window generating 
light that seems to come from a point source very far away in the lower left direction. A picture of 
the scene behind the window can be formed by passing the wavefront through a simulated lens onto a simulated 
screen. Moving the lens center changes the apparent point of view. The size of the lens and its distance 
from the scene determine the depth of focus of the image. The maximum number of resolvable pixels in 
the image can be no greater than the number of wavefront coefficients - no more information is present 
and any attempt to get more resolution in the image than in the wavefront will result in blurring. Also, 
no object feature smaller than a wavelength of the simulated light can be represented. Thus the wavelength 
must not be too large. It should also not be smaller than necessary, since the computational cost goes 
up dramatically as the wavelength drops. ;k/2 I Array I Coefficients Figu re 1 : How a wavefront description 
in one  3. Simplifications Here's one way to use the light wave idea. It deviates from a true physical 
simulation in the interests of simplicity and computational efficiency. Goodman [3] presents the fundamental 
mathematics. Feynman [1] is a good source for aspects of real light I've maligned. We will use light 
of a single wavelength, and we assume that all optical effects are linear. The tight is propagated according 
to scalar diffraction theory, which behaves realistically except at very short distances. The wavelength 
must be no larger than the smallest object feature we wish to see, but should not be too small because 
the amount of computation increases dramatically as the wavelength drops. To faithfully represent a wavefront, 
there must be a coefficient every half wavelength across the width of a description, so the number of 
complex numbers needed in a description increases as the square of the inverse wavelength. We also assume 
that only the steady state of the illumination is important. This allows us to play cavalierly with time. 
For instance, a wavefront may be propagated from one plane description to a parallel one some distance 
away in a single step, even though different portions of the effect have different transit times. ave 
 plane can represent a wave travelling in an oblique direction. This is a one-dimensional analog of the 
2D wavefronts discussed in the text. The complex wavefront coefficients are shown here as small arrows. 
The length and orientation of each arrow represents the magnitude and phase of one coefficient. Our waves 
are are linearly superposable, and a complex wavefront can be generated simply by adding the coefficients 
from many simple descriptions. ;k is the wavelength. Computer Graphics Volume 15, Number 3 August 1981 
The scene is represented as a box of half wavelength on a side cells, roughly 1000 cells cubed. The content 
of each cell is characterized by three complex coefficients, one giving the absorption and phase shift 
(simulating refractive index) for transmitted light, the other giving the reflection attenuation and 
phase shift (simulating surface scattering). The third term is unconditionally added during a wavefront 
calculation, which is how we make light sources. This 3D array need not be explicitly stored, but 10002 
cell slices perpendicular to at least one of the major axes should be efficiently extractable at will. 
One suitable storage scheme is the recursively subdivided "oct-tree" representation [6], which permits 
large uniform volumes to be kept as single nodes of a tree. 4. Some Details By contriving scenes like 
Figure 4 for computational cheapness, we can try out some of the methods with modest processing power. 
Such "cheap" scenes consist of a number of very thin objects confined to one of a small number of parallel 
planes. The wavefront calculations are done in a direction parallel to these object planes. The lenses 
and in the scenes are actually thin regions of slowly varying transmissive phase shift. The light sources 
in the the first object plane (plane a) generate a wavefront which we will call WF a. WFa(i,j ) where 
i a.nd j are integers between 0 and 511, represents one complex coefficient in a half wavelength grid 
across the area of plane a. The effect of WF a on b, the second plane, (call it WFab ) is the sum of 
the effects of every WFa(i,j) on every coefficient WFab(k,I). Each such contribution involves noting 
tl~e distance between the WFa(i,j ) and the WFab(k,I) cells and adjusting the phase (because of the time 
delay introduced by travelling over such a distance) and the amplitude (because of the inverse square 
and angle of incidence attenuation or equivalently because of the amount of solid angle cell WFab(k,I) 
occupies when seen from WFa(i,j) ). This operation is equivalent to a multiplication of the complex valued 
wavefront coefficient by a complex valued propagation coefficient to obtain a complex valued contribution 
to the new wavefront. (In my program the propagation coefficients were determined by integrating the 
path from the center of the originating cell over the area of the receiving cell, in a 10 by 10 subdivision 
using Simpson's rule. In fact each coefficient is the sum of four such integrations, representing the 
effect of two repetitions of each source cell in X and Y; the first orders of a periodic repetition of 
the matter planes --this is an implementation detail). Doing this process straightforwardly means multiplying, 
each of our 10242 wavefront coefficients by 10242 propagation coefficients and summinq the products appropriately 
into 10242 resultant waveform coefficients. This is over a trillion complex multiplications and additions, 
and too expensive for present day conventional hardware. Fortunately there is a cheaper way tO achieve 
the same result. Note that for a given k and I the distance from WFa(i,j) to WFab(i+k,j+l), and thus 
the propagation coefficient, is the same for all values of i and j. Call this coefficient Pa,b(k,I). 
We can now express the resultant wavefront by WFab(m,n) = Sumi, j Pab(m-i,n-J) WFa(i,j) This is the convolution 
of WF a with Pab' and will be denoted by WFa*Pab. Such a convolution can be accomplished by taking the 
Fourier transforms, multiplying them term by term then taking the inverse Fourier transform of the result: 
 WFab = WFa ~' Pab = F-I(F(WFa) " F(Pab)) where F denotes Fourier transform and. denotes the termwise 
 product of two arrays of like dimension to obtain a third. Since Pab iS an invariant of the scene, F(Pab 
) can be calculated once and for all when the scene is created. The symmetries Pab(i,j) = Pab(J,i) = 
Pab(-i,j) = Pab(J,-i ) can help cut clown that cost a little. The Fast Fourier Transform (FFT) is a 
method of arranging the calculations so that only about n Iog2n multiplications and a similar number 
of additions are required. A two dimensional transform like we require can be accomplished by taking 
a one dimensional FFT of each row, replacing each row by its transform, then doing one dimensional FFT's 
of the columns of the result. In our case the one dimensional transforms have 1024 points, and require 
about 10,000 complex multiplies each. There are 2x1024 of them to be done per 2D transform, for a total 
of about twenty million multiplies. The inverse transform is computationally equivalent to the forward 
one, so the FFT approach requires 60 million multiplies, compared to the million million of the direct 
method. If the F(P) is assumed to be precomputed the cost drops to 40 million. The FFT approach is thus 
about 20,000 times faster than the direct convolution method. If the naive ray tracing schemes were applied 
to the same scene they would work essentially by the direct method, though probably in a less orderly 
fashion. No organization analogous to the FFT method for wavefronts has been offered for the ray schemes, 
so this 20,000 times speedup is a major advantage for the wave approach. Getting back to our scene, we 
have now calculated WFab, the wavefront impinging on plane b. The transmission coefficients of plane 
b (Tb) which tell of the attenuation and refraction caused by the substances at b are multiplied term 
by term with the incoming wave front (a mere million complex multiplies) to generate a modified front, 
to which is added the effect of any light sources at b (Lb) to give us WF b, the wavefront leaving plane 
b. WF b = WFab.T b + L b = (WFa*Pab).T b + L b The reflection coefficients of b (Rb) are also multiplied 
by WFab to obtain the wave reflected back towards a from b. We call it a waveback, and designate it WB 
a. WB a is stored for future reference. We are now ready to carry the wavefront from WF b to and through 
the next plane c in exactly the same manner in which we moved it from a to b: WF c = (WFb* Pbc)-Tc + 
L c we also save for future reference WB b = (WFb*Pbc).R c (doing the convolution just once for the two 
results, of course). Each such move costs about 50 million complex multiplies (= 200 million real multiplications) 
and can be done in about five compute minutes on a typical medium sized machine" like our VAX 11/780. 
With a 15 megaflop array processor like the CSPI MAP- 300 the time is less than 15 seconds. After a complete 
sweep across the scene (Figure 2) we have all the first order lighting effects if the light sources were 
all in plane a, and some higher order ones, as when light is scattered by b then c to illuminate d. We 
now sweep backwards, from the far end of the scene towards a. Each step is similar to the forward sweep 
except that at each plane the waveback left behind in the first pass is added to the outgoing wavefront, 
like the light sources at that plane, simulating the contribution of reflected light from the last pass. 
For example, the return sweep modifies the wavefront  Computer Graphics leaving plane c into the one 
leaving b by the transformation WF b = (WFc*Pab).Tb + L b + WB a in the same step WB c, the effect of 
light from the return sweep reflected from b to c is stored WB c = (WFc*Pab),R b After several back 
and forth passes, each involving WB's from the previous pass, the wavefront contains the effects of highly 
indirect light The iteration can simply be stopped after a fixed number of sweeps, or the wavefront can 
be checked for significant changes between forward/back cycles. If WF a is nearly the same before and 
after two sweeps (by a sum of squares of coefficient differences, for instance), the process has settled 
down. Ten passes is usually enough, though pathological cases like scenes containing two facing mirrors 
may require more. Images are generated from the ultimate wavefront in a final step by passing the wavefront 
through a new plane which contains a simulated lens that projects the image onto a virtual surface where 
magnitude (absolute value) of the complex wavefront coefficients are extracted, scaled and transcribed 
into a pixel array. Rb I Rc Volume 15, Number 3 August 1981 Color pictures can be made of three such 
images, one each for red, green and blue, with different reflection, transmission and emission properties 
of the things in the object planes. The simulated wavelength is the same in all cases, so the simulated 
color has nothing whatever to do with the physiological color property of real light, which depends on 
wavelength. Figure 4 contains two planes, and was subjected to eight wavefront passes (four each way). 
The first plane contains a coherent light source which projects a beam onto the scattering "lampshade" 
in the second plane. The scattered light reflects back to the first plane and illuminates the "checkerboard" 
there, and so on for eight bounces. A thin lens in the second plane distorts some of the checkerboard 
behind it. The two planes are 300 wavelengths apart, and 512 half-wavelengths on a side. Light from the 
scene was intercepted by a square lens of tl~e same size 10,000 wavelengths away, which formed the displayed 
image yet another 10,000 wavelengths further on. The illumination is monochromatic, but color is used 
in one version of the image to represent the relative phase of the incident light. The red, green and 
blue components of the color are varied SWEEP 1 Rd / ++ I I + --Ld SWEEP 2 WBb WBc WBd Figure 2: The 
order of computation in a wavefront image generation. These are the first two of many passes. Each pass 
leaves behind a trail of reflected waves, which are added into the computation on the next pass. Each 
step within a pass involves multiplying a wavefront description by the transmission coefficients of the 
current matter plane, adding in the light source intensities, and the appropriate reflected wavefront 
from the last pass, and convolving with the propagation coefficients for the gap to the next matter plane, 
to produce the wavefront impinging there. In addition the incoming wavefront is multiplied by the reflection 
coefficients of the current matter plane to generate a new reflected wavefront. Computer Graphics Volume 
15, Number 3 August 1981 with phase like the three components of three phase power; red varies with the 
sine of the phase angle, green with the angle shifted 120 degrees, blue by phase shifted 240 degrees. 
We don't yet have an array processor, so the compute time was about an hour. With an FPS-IO0 it could 
have been generated in 8 minutes. CSPI's MAP-300, with twice as many multipliers, could have done it 
in 4. It should also be noted that the calculations are highly suitable for division among parallel processors 
(most of the cost is in the 1024 point complex FFT's, and 1024 of these, one for each row or column, 
could be carried out in parallel). Since the scenes contain extended light sources and model multiple 
indirect light scattering, all conventional ray methods would have required at least thousands of compute 
hours to produce the same result. There may, of course, be as yet unexplored ray approaches that do scattering 
more efficiently. 5. Defeating Diffraction Since the illumination is by simulated monochromatic light, 
potentially all in phase, there is a tremendous opportunity for probably undesirable effects such as 
interference fringes and "laser speckle" to make themselves manifest. These are made worse by the maximally 
long wavelength of the light chosen to reduce the amount of computation to the bare minimum required 
for the image and object resolution desired. If the scene is lit mainly by light sources extending over 
many half-wavelength cells, most of the problem can be eliminated by randomizing the phase, of the light 
emitted by the individual cells. This effectively simulates a spatially non-coherent source, and blurs 
interference patterns into oblivion. If the light comes mostly from a moderate number of very localized 
regions, the picture can be recomputed a few times with different relative phases between the sources, 
and the results averaged. This simulates phase non-coherence between the different sources. If the scene 
is lit by a very small number of very localized sources simple randomization of light source phases won't 
help. If much of the actual illumination in those scenes arrives indirectly by scattering of the light 
from objects of high albedo, randomizing the reflective and transmissive phase shifts of the scattering 
objects will improve things a little. In general it also helps if objects have "soft" edges which blend 
gradually over the distance of a few cells from the properties of their surrounding to the optical properties 
of their interiors. The above techniques remove low frequency interference fringes but do not significantly 
affect the high frequency phase interferences analogous to the "laser speckle" seen when viewing objects 
in coherent light. This pixel graininess can be smoothed by computing the image to higher resolution 
than needed and averaging, or by recomputing it several times with different random phases in the extended 
light sources or the light scattering surfaces, and averaging the results.  6. Finessing Focus Another 
consequence of the large wavelength is the limited depth of focus of scenes viewed from close up. The 
same limitation is seen in physical light microscope images. Though any plane can brought into focus, 
regions a short distance in frortt of or behind are blurred into oblivion. A simple degree of freedom 
argument makes it clear that a high resolution image with a given number of pixels cannot be generated 
from a wavefront description with fewer coefficients. We cannot get a well defined 1024 by 1024 pixel 
picture from the wavefront description passing through a "pinhole" smaller than 1024 elernents across. 
But since our scenes are only about that many elements in size, it implies the "lens aperture" of our 
simulated camera must subtend a very large angle except when the camera is very far away. There is a 
consequent large parallax shift from one edge of the lens to the other, and thus blurring of all scene 
planes but the one in focus. This problem has been minimized in figure 4 by viewing the scene from a 
great distance. This is not a general solution, since we would like to admire the scenery from other 
points of view. An interesting aspect of imaging at great distances is that the paths from all parts 
of the scene to the image pixels are nearly the same, and thus the computational precision required is 
great, since the image depends on small differences between large numbers. In the physical world this 
translates to light gathering ability, or light source intensity. The precision of the numbers in the 
computation is directly analogous to the number of photons in the real world analog. When a scene is 
viewed from far away with a small aperture, only a tiny fraction of the photons it emits are intercepted 
by the camera. We could solve the problem imperfectly by composing the picture in several steps by combining 
images focussed at different depths. Edges would then be sharp, but object boundaries would exhibit some 
degree of translucency. A hybrid wave/ray approach is possible. The scene could be illuminated in the 
manner we have described, but the image would be generated by subsequently examining the stored reflected 
wavefronts with simulated rays. A ray would be launched from the eye position through each image plane 
pixel and intersected with the first non-empty cell it reaches. Examining the wave coefficient at that 
cell is not enough, unless the matter at that position a perfect diffuse reflector, since the light direction 
is encoded in the phase relationships of nearby elements. A small neighborhood around the ray must be 
examined to determine what fraction of the light is being reflected to the eye position. The more nearly 
the viewed surface approximates a specular reflector, the larger the necessary region, and the less attractive 
this approach. This suggests that a further step in the ray direction, a combination in which waves do 
the diffuse reflection and in which rays like Whitted's [7] do the entire specular part, would be even 
better. This would still leave a region of "slightly diffusing" or "almost specular" surfaces which could 
be handled well neither by the ray nor the wave approach. Another possibility is to image the wavefront 
through a smaller pinhole, but to retain the requisite information by resolving the front at super-resolution, 
with coefficients spaced more closely than a half wavelength. As in the case of viewing the scene from 
a great distance, the precision of the computation (or the "number of photons") must be large. Unfortunately 
I have not figured out how to do the super-resolution computation in any way but by the prohibitively 
expensive direct method. 7. Bigger Games So far we have dealt with very simple scenes in which all 
the matter was confined to a small number of parallel planes. Not co-incidentally these are particularly 
suitable subjects for the wave approach. More general scenes can be co.nstructed by increasing the number 
of matter-containing planes. In the most general case there is such a plane every half wavelength, and 
fully three dimensional objects are described as a stack of parallel slices. 293   Computer Graphics 
Volume 15, Number 3 August 1981 This increases the required computation by a factor of several hundred, 
and also creates a major storage problem. A special purpose Fourier device able to compute a 1024 point 
transform in 100 usec would allow fully general scenes with complete illumination to be calculated in 
about an hour by the straightforward application of the above method. It would be necessary to store 
about one billion complex numbers, representing the approximately 1000 wavebacks generated, temporarily 
during the computation. Storage for a billion complex numbers (the precision need not be great) may not 
be excessive in a machine than can compute at 500 megaflops, but it doesn't make the problem easier. 
It may be possible to avoid explicit storage of the wavebacks by combining them as they are generated, 
dragging them along backwards with the wavefronts in a time-reversed way. The composite waveback would 
be subjected to transformations inverse to the ones it will experience when the sweep reverses. Or maybe 
not. All versions of this idea rve examined have some unavoidable high order light interactions that 
do not have analogs in the physical world. 8. Waves and the Oct-Tree The above schemes, particularly 
the fully general one, have an unpleasant feature. They do as much work on the empty spaces in a matter-containing 
plane as they do on the substance. Propagating a wave through a dense stack of matter planes involves 
hundreds of half-wave baby steps, even if the planes contain only a speck of matter each. We have seen 
that a wavefront can be transmitted across an arbitrarily large gap of empty space by a single FFT convolution. 
In fact, it is just as easy to send it through an arbitrarily large mass of any substance of uniform 
attenuation and phase shift; only the propagation coefficients change. Since typical scenes contain large 
homogeneous volumes, we may be wasting Some effort; computing over volumes when only the surface areas 
require the full power of the incremental method. The fundamental space underlying our 3D models is a 
cubic array of half-wavelength cells. We can organize this into convenient large regions by the following 
process. Suppose our scene is contained in a cube 1024 half wavelengths on a side. If this cube is homogeneous, 
stop; we have our simple description. If it is not, divide it into eight 512 on a side subcubes, and 
represent it by an eight way branching node of a tree pointing to the results of the same procedure applied 
recursively to the subcubes. The division process at every level stops if the subcube it is handed if 
homogeneous, or if it is a primitive half-wavelength cell (in which case it is homogeneous by definition). 
A wavefront can be propagated through such an "oct-tree" structure irregularly, advancing through large 
undivided subcubes quickly, creeping through the hopefully few heavily subdivided regions. There is a 
serious complication. While the wavefronts in the "stack of planes" approach of the last section were 
sent between parallel planes, with light leaving at the sides assumed lost (or wrapped around, depending 
on the details of the convolution), waves leaving at right angles to the principal wavefront direction 
in a subcube of the oct-tree must be accounted for, because it affects adjacent subcubes, Part of the 
problem is a matter of bookkeeping, solved by having two wavefront storage arrays at every boundary surface 
between distinct subcubes, one for each direction of light travel (an alternative way of looking at it 
is that each distinct subcube of the tree has one outgoing wave coefficient array on each of its six 
faces. Incoming waves are stored in the outgoing wave arrays of adjacent subcubes). The other half of 
the problem is calculating the outgoing wavefront on a surface orthogonal to the plane of the incoming 
wavefront description. A wave entering one face of a cube leaves by one parallel face and four at right 
angles, While the parallel face calculation is a simple convolution, the orthogenal ones are not so easy. 
Whereas the propagation coefficients between two co-ordinates in an incoming and outgoing waveform in 
a parallel propagation are a function simply of displacement in X and Y, of the form F(Xout-Xin,Y#ut-Yin 
), the coefficients in an orthogonal transfer.look like F(XoutZ2+ that the rows (say) of the xi n ,Yout_Yin), 
reflecting outgoing array are parallel to the rows the incoming array, but the columns in this 3D situation 
are at right angles. In this example the Y direction can be done by an FFT convolution, but the transformation 
over the X coordinate must be done directly (unless I've missed something). For a face size o{ 1024 by 
1024 we must first do 1024 FFT convolutions for the Y direction (30 million multiplies) giving us 1024 
vectors each with 1024 elements. These vectors must then each be multiplied by 1024 coefficients and 
summed into the output array, for a total of over a thousand million multiplications, a number which 
dominates the FFT step. A cube has four such difficult faces, but the calculations can be shared between 
parallel pairs of outgoing faces and a 1024 on a side cube requires atotal of about two billion multiplications. 
A cube of side n needs about 2n 3 multiplies. Since the amount of work thus goes up approximately linearly 
in the volume, we gain little by subdivision, and the method is of limited interest unless a faster way 
of calculating the sidegoing wavefronts is found.  9. The Grand Synthesis Note that in the stack of 
planes approach, the wavefront propagation consists of a chain of linear transformations; a waveform 
is alternately convolved with a propagation array then multiplied by an array of transmission coefficients. 
Both these operations are special cases of a general linear transform which can be expressed as a huge 
array. If we treat our 1024 by 1024 wavefront arrays as a 10242 (call it a million) element vector (name 
it V), then a general linear transform is a million by million matrix (A) to multiply this vector by, 
plus a million element vector offset (B) to be added (V' = AV + B). A convolution is simply a special 
case of such a matrix A where every row is the same as the previous one except shifted right one place. 
The transmission coefficients can be represented in a matrix zero everywhere except on the main diagonal 
where the coefficients reside. Light sources are simply represented in the constant vector B.  Now a 
chain of linear operations can be condensed into a single one by multiplying the matrices and transforming 
and adding the vectors. This implies that a complex scene consisting of a stack of planes (or otherwise 
described) can in principle be multiplied out into a single matrix and offset. The effect on a wavefront 
of a scene so expressed can be determined by a single matrix multiply. The matrix may contain the effect 
of multiple passes through the scene, and thus provide the effect of high order light indirection in 
a single multiply. Multiplying a complex wavefront (representing external illumination) by such a matrix 
produces an image.bearing front which shows the scene encoded in the matrix freshly illuminated. Although 
manipulation of such matrices, with a trillion elements, is beyond the means of present day machinery, 
they may be practical someday when computers become another factor of one 2~4   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806818</article_id>
		<sort_key>297</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>36</seq_no>
		<title><![CDATA[A lens and aperture camera model for synthetic image generation]]></title>
		<page_from>297</page_from>
		<page_to>305</page_to>
		<doi_number>10.1145/800224.806818</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806818</url>
		<abstract>
			<par><![CDATA[<p>This paper extends the traditional pin-hole camera projection geometry, used in computer graphics, to a more realistic camera model which approximates the effects of a lens and an aperture function of an actual camera. This model allows the generation of synthetic images which have a depth of field, can be focused on an arbitrary plane, and also permits selective modeling of certain optical characteristics of a lens. The model can be expanded to include motion blur and special effect filters. These capabilities provide additional tools for highlighting important areas of a scene and for portraying certain physical characteristics of an object in an image.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Camera model]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Lens and aperture]]></kw>
			<kw><![CDATA[Raster displays]]></kw>
			<kw><![CDATA[Visible surface algorithms]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Camera calibration</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010234</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Camera calibration</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14194891</person_id>
				<author_profile_id><![CDATA[81100561593]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Potmesil]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Image Processing Laboratory, Rensselaer Polytechnic Institute, Troy, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39022602</person_id>
				<author_profile_id><![CDATA[81100000871]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Indranil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chakravarty]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Image Processing Laboratory, Rensselaer Polytechnic Institute, Troy, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Catmull, E., "A Subdivision Algorithm for Computer Display of Curved Surfaces", UTEC CSc-74-133, Computer Science Department, University of Utah, 1974]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358815</ref_obj_id>
				<ref_obj_pid>358808</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Blinn, J.F., Carpenter, L.C., Lane, J.M. and Whitted, T., "Scan Line Methods for Displaying Parametrically Defined Surfaces", CACM, 23, (1), January 1980, 23-34]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Whitted, T., "An Improved Illumination Model for Shaded Display", CACM, 23, (6), June 1980, 343-349]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>507101</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Blinn, J.F., "Simulation of Wrinkled Surfaces", Computer Graphics 12,(3), Atlanta, Ga, 286-292]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356750</ref_obj_id>
				<ref_obj_pid>356744</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Carlbom, I. and Paciorek, J., "Planar Geometric Projections and Viewing Transformations", Computing Surveys, 10, (4), December 1978, 465-502]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Newman, W.M. and Sproull, R.F., Principles in Interactive Computer Graphics, McGraw Hill, Inc., New York, 1973]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Goodman, J.W:, Introduction to Fourier Optics, McGraw Hill, Inc., New York, 1968, Chapters 4,5]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Born, M. and Wolf, E., Principles of Optics, 3rd (revised) edition, Pergamon Press Ltd., London, 1965, Chapter 8]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 A LENS AND APERTURE CAMERA MODEL FOR SYNTHETIC IMAGE 
GENERATION by Michael Potmesil* and Indranil Chakravarty** Image Processing Laboratory Rensselaer 
Polytechnic Institute Troy, New York 12181 Abstract 1.0 Introduction In the past few years several algorithms 
have beenThis paper extends the traditional pin-hole camera developed for realistic rendering of complex 
three-projection geometry, used in computer graphics, to a dimensional scenes on raster displays [1,2,3]. 
These more realistic camera model which approximates the algorithms have been generically called hidden 
surfaceeffects of a lens and an aperture function of an actual algorithms in the sense they display only 
the visible camera. This model allows the generation of synthetic surfaces from a given vantage point. 
All theseimages which have a depth of field, can be focused on algorithms, however, have continued to 
use the pin-hole an arbitrary plane, and also permits selective modeling camera projection geometry 
which was developed forof certain optical characteristics of a lens. The model display of 3D line drawings 
on vector devices. The can be expanded to include motion blur and special purpose of this paper is to 
develop a more complexeffect filters. These capabilities provide additional tools camera model, which 
although computationally morefor highlighting important areas of a scene and for expensive, provides 
the means of generating moreportraying certain physical characteristics of an object in realistic synthetic 
images closely approximating a scenean image. imaged by an actual camera. These synthesized images are 
suitable for display only on raster devices. Key Words and Phrases: computer graphics, visible The purpose 
of generating such synthetic images, surface algorithms, raster displays, camera model, lens which in 
a sense incorporate the constraints of an optical and aperture system and the imaging medium, is twofold: 
1. It gives the ability to capture the viewers' attention CR category: 8.2 to a particular segment of 
the image, that is, it allows selective highlighting either through focusing or some optical effects. 
2. It permits adaptation of many commonly used cinematographic techniques for animated sequences, such 
as fade in, fade out, uniform defocusing of a scene, depth of field, lens distortions and filtering. 
It should be stated, however, that the objective is to model only those features which can be used to 
some advantage for special effects. No attempt will be made *Present address: Bell Laboratories, Holmdel, 
NJ 07733 to model flaws inherent in a lens such as optical**Present address: Schlumberger-Doll Research, 
aberrations or the lens transfer function. We will also Ridgefield, CT 06877 assume that the imaging 
medium is perfect, that is, a point is reproduced with perfect fidelity. The image generation process 
described here consists Permission to copy without fee all or part of this material is granted of two 
stages. In the first stage, a hidden-surfaceprovided that the copies are not made or distributed for 
direct processor generates point samples of intensity in the commercial advantage, the ACM copyright 
notice and the title of the publication and its date appear, and notice is given that copying is by image 
using a geometric pin-hole camera model. Each permission of the Association for Computing Machinery. 
To copy sample consists of image plane coordinates, RGB otherwise, or to republish, requires a fee and/or 
specific permission. intensities, z depth distance and identification of the visible surface. In the 
second stage, a post-processor converts the sampled points into an actual raster image. &#38;#169;1981 
ACM O-8971-045-1/81-0800-0297 $00.75 Each sampled point is converted into a circle of confusion whose 
size and intensity distribution are determined by the z-depth of the visible surface and the characteristics 
of the lens and aperture model. The intensity of a pixel is computed by summing the intensity distributions 
of the overlapping circles of confusion of all sample points. A circle of confusion and its intensity 
distribution may be stretched in the image along the projected path of a moving surface to approximate 
motion blur caused by a finite exposure time. Special effect filters such as star or diffraction filters 
may be convolved with the image at this stage.  2.0 The Camera Model 2.1 Camera Geometry This section 
describes the camera geometry used in projecting a 3D scene onto an image plane. The projection matrix, 
also called the camera transformation matrix, is a function of the camera parameters. These parameters 
are the pan, tilt and swing angles of the camera, the location of the camera lens, the focal length and 
the size of the image frame, as illustrated in Figure 1. z 1 ~ .... 0 2 1 ~Y / ~i 2 \ Z x 3 2 x I \\ 
O 3 image plane "~-~ I " | x2 sample ray viewing pyramid center of projection Fig. 1 Camera Geometry 
The use of homogeneous coordinates permits modelling this transformation as a single 4x3 matrix [5,6]. 
This transformation provides a perspective mapping of a point from the global coordinate system O z, 
in which a scene is defined, to the image plane coordinate system 0 3 . The pan, tilt and swing angles 
specify the direction the camera is looking at, the lens center is the vantage point relative to coordinate 
system 01, and the focal length along with the size of the image frame specifies the viewing pyramid. 
Points in O l which lie outside the viewing pyramid are clipped from the image plane. Volume 15, Number 
3 August 1981 2.2 The Finite Aperture Camera Model The basic law governing image formation through a 
lens can be described by the lens formula used in ray optics: ± + ± _ ! (]) U v F where U is the object 
distance, V is the image distance and F the focal length of the lens all measured along the optical axis 
[Figure 2]. We add to this basic lens model an aperture function which limits the lens diameter and fixes 
the location of the image plane at the focal plane of the lens. The introduction of these two constraints 
allows the notion of focusing the lens (by moving it relative to the fixed image plane), and associated 
with it, a depth of field. It should be stated that the notion of depth of field, that is, some objects 
appearing to be in focus while others are out of focus, is not a function of the optics but rather a 
function of the resolving capability of the human eye and the imaging medium. ~ ge plane optical axis 
 F 1"6" U :~ V :~ Fig. 2 Lens Law We will assume the accepted standard that for a viewing distance U, 
the smallest resolvable patch by the human eye has a diameter of U/1000. This means that anything larger 
than this diameter is viewed as an area (patch) rather than as a point. Alternatively, one could say 
that the angular resolving power of the eye is 1 rail. Based on this criteria, the depth of field can 
be derived as follows. Assume that at distance U, we have a patch diameter U/1000 which is in focus, 
that is, we can resolve it as a point. This is illustrated in Figure 3. Let diameter of the patch be 
MN. Extending the patch in both directions, towards and away from the lens, we obtain triangles Alvin 
and BMN. Note that LD is the effective lens diameter, defined as the focal length divided by the aperture 
number (F/n). The resulting extension of the patch MN, denoted by ~" and tf', on the optical axis, [Figure 
3], will also remain in focus since their respective diameters are less than U/IO00. Since AALD iS similar 
to AAMN and ABLD is similar to ABMN we obtain the following two equations: U+-t] u + U-O-tr- a~d (2) 
MN LD MN LD 298 Computer Graphics Solving for u + and U-we obtain: u + -u/(t o (F/,) 1000) (3) U IF 
-u/(l + (F/.) tO00 ) (4) The following observation can now be made: 1. If U -1000F then 0 ~ -,- ** and 
IF --500F. The tl I1 distance 1000F is a close approximation to the n hyperfocal distance of a lens. 
A lens focused at this distance yields the maximum depth of field. 2. As the effective lens diameter 
is decreased, by increasing the aperture number n, the hypeffocal distance becomes smaller in magnitude 
yielding greater depth of field. Let us denote the hyperfocal distance as H. Then if we are focused at 
some plane U, the limits of the depth of field is given by: U+ UH H-O' H> U (5) - ~ H<~U IF UH H+U (6) 
,__ D + Fig. 3 Depth of Field 2.3 Image Focusing In this section we develop a measure of how well a 
point appears to be in focus on the image plane. Since t:he image plane is fixed at F, the focal length, 
a point that is out of focus will converge on a plane away from F, projecting onto the image plane as 
a circle rather than a point. This circle is called the circle of confusion and is a measure of how defocussed 
the image point is. When we are focused at some image distance U, based on our earlier calculations, 
a U* and a IF exist which also appear to be in focus. This implies that the corresponding image points 
V, Y + and w form circles on the image plane whose diameters are less than or equal to F/1000. Thus, 
observed from a distance F, the  Volume 15, Number 3 August 1981 images of these points are resolved 
by the eye as points and appear to be in focus. The diameter of the circle of confusion for each point 
in the scene, can be expressed as follows. Let V, and Vp be the image of points U and P. ~ forms a point 
on the image plane whereas v, projects into a circle as it converges a distance vp -v, away. Following 
Figure 4 we note that ALDA and ACBA are similar. Thus we have LD CB FU where r,- U-¥ U>F (7) v. v.-v, 
FD Vp -P'"F P> F Since LD =F/n and solving for CB, the diameter of the circle of confusion, we obtain 
F (8) c-I v.-Vpl .v. ~ L mage plane P ~ ~ -Vp--~4 U ~ VU~ Fig. 4 Circle of Confusion Note that as 
U--P, the plane of focus, the circle of confusion approaches zero. Points at infinity approach a limiting 
diameter given by ,Fv, F The diameter n" of the circle of confusion is highly asymmetric as one moves 
away from the plane of focus in the two directions along the optical axis as illustrated in Figure 5. 
V D F Diameter of . C = -----Circle of Confusion nF n f i t I 1 1 U-~U + Distanc~ Object plane U in 
focus Fig. 5 Asymmetricity of Depth of Field 2.4 D~raction Effects It is well known from physical optics, 
that diffraction effects due to a finite aperture size cause the image of point source to be spread over 
a larger area. Our objective in studying these effects is to determine the distribution of light intensity 
in the circle of confusion. It can be physically observed that a defocused image results in a loss of 
contrast. Assuming no energy loss in lens transmittance, the distribution of energy in the image must 
change to account for the loss of contrast. We describe here methods to determine the light intensity 
distribution within the circle of confusion for defocused points. Consider the diffraction of monochromatic 
light by a finite circular aperture of diameter d in an infinite opaque screen [Figure 6]. As shown, 
the screen is planar with rectangular coordinate system (xl,y,) and the image plane is parallel to the 
screen, at a distance a = F (focal length), with coordinate system(xe,yo). Let the field distribution 
of the wave be written as a complex function U(P) -A (P) e -~t(e) (9) where A(P) and ~(P) are the amplitude 
and phase of the wave at position P. '1 Y image plane x I / observation / zI=F (Focal Length) / Fig. 
6 Diffraction by a Circular Aperture The field amplitude at point (xo,yo), using the Huygens-Fresnel 
principle, can be written as U(xo,yo) -f f h~(xo,yo,x~,yx)U(xx,yx)axx ay, (10) ri circ (~2) where ho(xo.yo.xl.y,) 
1 e eel --cos(~,7ol)J~. rex k -2=/X I" 1 circ(-~2)" [1 2rl/d~l [ 0 otherwise el -I z~ + (Xo -xl) ~ + 
(.v0 -yx)~l ~" If the distance rot is large compared to the diameter of the aperture, the Fresnel approximation, 
and assuming an ideal thin lens whose transmittance is simply a phase transformation, then U(xo,y0) reduces 
to the Fourier transformation of the portion of field subtended by the lens aperture. Assuming a unit-amplitude, 
normally incident plane wave and radius coordinates ,0 -,/~ + y0' and ,i -~ + y~ ejkF Jkro2 U(r0) -~ 
e ~r l-I {U(r,)} (11) where H is the Hankel transform (due to the radial rl symmetry) of the aperture 
circ(~-2). The light intensity distribution (power spectrum) is given by [ kd212 I Jl(kdro/F) ]2 l(,o)-I--I 
2 (12) I 8F I kdro/F which is the well-known Airy pattern. A cross-section of this function is shown 
in Figure 7 where Jx is the Bessel function of the first order. The amplitude is dependent upon the lens 
diameter d thus making the intensity a function of the aperture. 1.0 i .~,,, 1 -4 -3 -2 -1 1 2 3 4 Fig. 
7 Light Intensity Distribution Function The derivation of the light distribution, for defocused monochromatic 
images of a point source by a circular aperture, was first investigated by E. Lommel in 1885. Details 
of this derivation, based on the Huygens- Fresnel principle, can be found in [8]. The results due to 
Lommel can be used directly with some minor changes in interpretation. The Huygens-Fresnel integral used 
to determine the light intensity in the circle of confusion cannot be obtained in a closed form and must 
be evaluated in terms of Bessel functions. The light intensity in the circle of confusion is given as 
follows: l(u,v) -[2l',y2(u,v) + Y,(,,,)] , o (13) where kd 2 le-8F 2 and V, and V, are Lommel functions. 
These functions can be evaluated by a series approximation given by V,(u,y) ~(-I)' (_")(.+2,) J(.+2,)(~) 
(14) - y 8-O where J(,+2,) are the Bessel functions of the n + 2s order. The variables u and v are dimensionless 
and specify the position of a point within the circle of confusion. They can be expressed as follows: 
u -k(_-~d_)' z (15)21 + V -- k'~"~ r0 --k -~ + y02 (16) Based on the expression derived by Lommel, the 
following observations are made: 1. At u=0, that is, the distribution of light intensity of a point focused 
on the image plane, is given by which is the previously derived Airy pattern. 2. Consider a plane at 
object distance p which is not in focus. This implies that the image of a point on this plane will converge 
at some image distance Vp. if the plane s is in focus, then Vp-v, is the distance by which the plane 
p is out of focus. To find the intensity distribution from the resulting circle of confusion, note that 
z = v, -V, or  U -k (d)~ (r, -V,) (17) Z/" and d v-k -ff ro (18) gives the position in the circle of 
confusion. 3. Note that for v-0, the light distribution at the center of the circle of confusion, we 
obtain [ sin(u/4)]l #(.,o)-i (u/4) I *'° (19) This shows that the highly peaked light distribution of 
the Airy pattern obtained for points in focus reduces in amplitude until it actually becomes zero at 
u/4 --±~ or z-+-4,rF2/kd ~. Thus, the distribution changes significantly for points out of focus resulting 
in lack of contrast for defocused points. 3.0 Synthetic Image Generation 3.1 Hidden-Surface Processor 
The image formation process consists of two stages. In the first stage, a ray-tracing hidden-surface 
algorithm generates point samples of light intensity in the 3D scene within the field of view of the 
camera model. The program uses Whitted's recursive illumination model [3] to generate images with surface-to-surface 
reflections, refraction in transparent surfaces, illumination by point light sources and shadows in complex 
3D scenes. Planar, quadric and bicubic surfaces can be rendered by the program at the present time. The 
raster image generated by this program is considered to be a grid of square pixels lying in the image 
plane, that is, each pixel is a finite square area of the image. The 3D scene is sampled at points which 
project into the pixel corners by intersecting rays from sample points, through the center of projection 
(focal point) and into the 3D scene [Figure 1]. The intensity of a pixel is computed by averaging the 
four corner samples, which is equivalent to fitting a bilinear function into the four samples and integrating 
this function over the area of the pixel. Antialiasing is performed by subdividing a pixel, which has 
a large difference in the sampled values, into 2x2 squares [Figure 8] and recursively repeating the sampling 
process as described in [3]. This subdivision is done only in pixels which contain sharp intensity changes 
in the image, typically caused by edges, silhouettes or textures. : / i I~ pixel e/, : ~-'-I--~ subdivisiOn 
 "/" " image sample point /  L/_ (at ~_p~xe i corner) . I I I t I    /:: L_;I kedge "-.... Fig. 
8 Image Antialiasing The program can optionally, in addition to generating the actual raster image, save 
the sampled values into a separate file. Each sampled value contains the following information: 1. The 
x, y coordinates in 0 3 , the 2D image plane coordinate system, of the sampled point, 2. the red, green 
and blue intensity values of the 3D scene at the sampled point, 3. the z coordinate (depth) in O ~, 
the 3D camera coordinate system, of the visible surface, and 4. the identification number of the visible 
surface (to be used in motion blur).  The parameters of the geometric camera model are also saved in 
this file. 3.2 Focus Processor The focus processor can generate a raster image, which is focused and 
has a depth of field, from the image point samples and the geometric camera model supplied by the hidden-surface 
processor and from given lens and aperture parameters. The focus processor approximates the integration 
process that takes place on the film plane during an exposure. We consider each intensity sample to 
be an independent source of light represented by a delta function with magnitude equal to the light intensity. 
The response of the optical system (the lens and aperture) to this delta function is an intensity distribution 
function on the image plane. Assuming that the intensity distribution is negligible outside the circle 
of confusion and that there is no energy loss in the system, the integral of the intensity distribution 
function over the circle of confusion is equal to the magnitude of the input delta function. The size 
of the circle of confusion of a point sample is computed from its z depth and the lens and aperture parameters 
by equation (g), the intensity distribution within this circle is computed by equation (13). The integral 
of the intensity distribution over the area of a pixel [Figure 9] is the contribution of the sample point 
to the intensity of the pixel. The sum of such contributions, from all sample points in the image, yields 
the intensity of the pixel. The integral of the intensity distribution is also attenuated by the square 
of the z depth distance of the point sample. Finally, the brightness of the images generated by the focus 
processor should be the same as the brightness of the source image generated by the hidden surface processor. 
Brightness changes in the image caused by film sensitivity, aperture setting and exposure time are not 
modeled here although they would be simple to add. Hence the following expression is used by the focus 
processor to compute pixei intensity of the three primary colors: N f(x~,y,,z,) Q(X,X+AX, Y,Y+A Y) '~ 
~ 'q' (20) " N f(x,,y,,~) where Q = the final intensity at pixel area (X,X + AX)(Y,Y + &#38;Y)  N = 
the number of point samples in the image q, = the intensity of point sample i x, ----- the x coordinate 
of sample i in the image plane y, = the y coordinate of sample i in the image plane za --- the z depth 
of sample i and also X+AX Y+A ¥ f(X,,,i,Z,)" ~X ~ '(Z~, ~/(X--Xi)2+(,--)',)2)z~ dX (21) with the condition 
that "f l(g,'~(x--x,)2+ly-y,)2)dy dx -l -w-m T. xmage plane / / / / / / Yi / Y Y+ ~Y X+ AX inte( 
ral f over pixel area (X, X+~ X) (Y, Y+ AY) Fig. 9 Integration of Intensity Distribution over a Pixel 
Area The focus processor computes a number of two-dimensional lookup tables at equally spaced z depth 
coordinates between the minimum and maximum z depth sample points in the scene. Each table contains the 
integral of the intensity distribution function, evaluated at the given z depth by (13), divided by the 
square of the z depth. This value is computed at each pixel that the corresponding circle of confusion 
overlaps. The table entries outside of the circle of confusion are filled with zeros. The size of each 
table is therefore determined by the size of the circle of confusion, in pixel units, at that z depth. 
The processor maintains a block of four values for each pixel in the image. These values contain the 
numerator of equation (20), one for each primary color, the fourth value contains the denominator of 
equation (20), common to all primary colors. For each input sample point, the processor selects the two 
lookup tables nearest to the z depth of the sample. The coresponding pixel entries in the two tables 
are linearly interpolated into a temporary table of coeffcients for the z depth of the current sample. 
The center of this table is displaced to the image coordinates of the sample point and the four values 
of all pixels that this table overlaps are updated. After all samples have been processed, the three 
primary color intensities of each pixel are computed by dividing the three numerators by the common denominator. 
 4.0 Results The focus processor described in the previous section has been implemented and used to 
generate a number of versions, with different lens and aperture parameters, of a test image. The geometric 
(pin-hole) camera version of the test image, at 512x512 resolution,  imagery. The model has been applied 
to the output of a hidden-surface program to generate images focused on various planes and an associated 
depth of field as a function of the camera parameters. Further extensions of this work should include 
generation of motion blur and modeling of special effect camera filters.  6.0 Acknowledgments The research 
reported here was supported by the National Science Foundation, Automation, Bioengineering, and Sensing 
System Program, under grant ENG 79-04821, Professor Herbert Freeman, Principal Investigator. This support 
is gratefully acknowledged. The authors would also like to thank Schlumberger-Doll Research for the facilities 
made available for the preparation of this paper. 7.0 References 1. Catmull, E., "A Subdivision Algorithm 
for Computer Display of Curved Surfaces', UTEC CSc-74-133, Computer Science Department, University of 
Utah, 1974 2. Blinn, J.F., Carpenter, L.C., Lane, J.M. and Whitted, T., "Scan Line Methods for Displaying 
Parametrically Defined Surfaces', CACM, 23, (1), January 1980, 23-34 3. Whitted, T., "An Improved Illumination 
Model for Shaded Display', CACM, 23, (6), June 1980, 343- 349 4. Blinn, J.F., "Simulation of Wrinkled 
Surfaces', Computer Graphics 12,(3), Atlanta, Ga, 286-292 5. Carlbom, I. and Paciorek, J., "Planar Geometric 
Projections and Viewing Transformations', Computing Surveys, 10, (4), December 1978, 465-502 6. Newman, 
W.M. and Sproull, R.F., Principles in Interactive Computer Graphics, McGraw Hill, Inc., Hew York, 1973 
 7. Goodman, J.W', Introduction to Fourier Optics, McGraw Hill, Inc., New York, 1968, Chapters 4,5 8. 
Born, M. and Wolf, E., Principles of Optics, 3rd (revised) edition, Pergamon Press Ltd., London, 1965, 
Chapter 8   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806819</article_id>
		<sort_key>307</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>37</seq_no>
		<title><![CDATA[A reflectance model for computer graphics]]></title>
		<page_from>307</page_from>
		<page_to>316</page_to>
		<doi_number>10.1145/800224.806819</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806819</url>
		<abstract>
			<par><![CDATA[<p>This paper presents a new reflectance model for rendering computer synthesized images. The model accounts for the relative brightness of different materials and light sources in the same scene. It describes the directional distribution of the reflected light and a color shift that occurs as the reflectance changes with incidence angle. The paper presents a method for obtaining the spectral energy distribution of the light reflected from an object made of a specific real material and discusses a procedure for accurately reproducing the color associated with the spectral energy distribution. The model is applied to the simulation of a metal and a plastic.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Image synthesis]]></kw>
			<kw><![CDATA[Reflectance]]></kw>
			<kw><![CDATA[Shading]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Reflectance</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP35024317</person_id>
				<author_profile_id><![CDATA[81100111623]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Cook]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University, Ithaca, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31097274</person_id>
				<author_profile_id><![CDATA[81332531868]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kenneth]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Torrance]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sibley School of Mechanical and Aerospace Engineering, Cornell University, Ithaca, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Barkman, E. F., "Specular and Diffuse Reflectance Measurements of Aluminum Surfaces," Appearance of Metallic Surfaces, American Society for Testing and Materials Special Technical Publication 478, pp. 46-58, 1970.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Beckmann, Petr and Spizzichino, Andre, The Scattering of Electromagnetic Waves from Rough Surfaces, MacMillan, pp. 1-33, 70-98, 1963.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bennett, H. E. and Porteus, J. O., "Relation Between Surface Roughness and Specular Reflectance at Normal Incidence," Journal of the Optical Society of America, v.51 pp. 123-129, 1961.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360353</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Blinn, James F. and Newell, Martin E., "Texture and Reflection in Computer Generated Images," Communications of the ACM, v.19 pp. 542-547, 1976.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563893</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Blinn, James F., "Models of Light Reflection for Computer Synthesized Pictures," SIGGRAPH 1977 Proceedings, Computer Graphics, v.11 #2 pp. 192-198, 1977.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>908845</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Blinn, James F., "Computer Display of Curved Surfaces," PhD dissertation, University of Utah, Salt Lake City, 1978.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[CIE International Commission on Illumination, "Official Recommendations of the International Commission on Illumination," Publication CIE No. 15, Colorimetry (E-1.3.1), /Bureau Central de la CIE, Paris,* 1970.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Davies, H., "The Reflection of Electromagnetic Waves from a Rough Surface," Proceedings of the Institution of Electrical Engineers, v. 101 pp. 209-214, 1954.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Gubareff, G. G., Janssen, J. E., and Torborg, R. H., Thermal Radiation Properties Survey: A Review of the Literature, Honeywell Research Center, Minneapolis, 1960.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Hunter, Richard S., The Measurement of Appearance, John Wiley &amp; Sons, New York, pp. 26-30, 1975.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Judd, Deane B., and Wyszecki, Guenter, Color in Business, Science, and Industry, John Wiley &amp; Sons, New York, pp. 170-172, 1975.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807502</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Meyer, Gary W., and Greenberg, Donald P., "Perceptual Color Spaces for Computer Graphics," SIGGRAPH 1980 Proceedings, Computer Graphics, v.14 #3 pp. 254-261, 1980.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>906584</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Phong, Bui Tuong, "Illumination for Computer-Generated Images," PhD dissertation, University of Utah, Salt Lake City, 1973.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360839</ref_obj_id>
				<ref_obj_pid>360825</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Phong, Bui Tuong, "Illumination for Computer Generated Pictures," Communications of the ACM, v.18 pp. 311-317, 1975.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Porteus, J. O., "Relation between the Height Distribution of a Rough Surface and the Reflectance at Normal Incidence," Journal of the Optical Society of America, v.53 pp. 1394-1402, 1963.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Purdue University, Thermophysical Properties of Matter, vol. 7: Thermal Radiative Properties of Metals, 1970.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Purdue University, Thermophysical Properties of Matter, vol. 8: Thermal Radiative Properties of Nonmetallic Solids, 1970.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Purdue University, Thermophysical Properties of Matter, vol. 9: Thermal Radiative Properties of Coatings, 1970.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Siegel, Robert and Howell, John R., Thermal Radiation Heat Transfer, McGraw-Hill, New York, pp. 64-73, 1980.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Sparrow, Ephraim M. and Cess, R. D., Radiation Heat Transfer, McGraw-Hill, New York, pp. 64-68, 1978.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Torrance, Kenneth E. and Sparrow, Ephraim M., "Biangular Reflectance of an Electric Nonconductor as a Function of Wavelength and Surface Roughness," Journal of Heat Transfer, v.87 pp. 283-292, 1965.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Torrance, Kenneth E. and Sparrow, Ephraim M., "Theory for Off-Specular Reflection From Roughened Surfaces," Journal of the Optical Society of America, v.57 pp. 1105-1114, September 1967.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Whitted, Turner, "An Improved Illumination Model for Shaded Display," Communications of the ACM, v.23 pp. 343-349, 1980.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Whitted, Turner, private communication.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 A REFLECTANCE MODEL FOR COMPUTER GRAPHICS Robert 
L. Cook Program of Computer Graphics Cornell University Ithaca, New York 14853 Kenneth E. Torrance Sibley 
School of Mechanical and Aerospace Engineering Cornell University Ithaca, New York 14853 Abstract This 
paper presents a new reflectance model for rendering computer synthesized images. The model accounts 
for the relative brightness of different materials and light sources in the same scene. It describes 
the directional distribution of the reflected light and a color shift that occurs as the reflectance 
changes with incidence angle. The paper presents a method for obtaining the spectral energy distribution 
of the light reflected from an object made of a specific real material and discusses a procedure for 
accurately reproducing the color associated with the spectral energy distribution. The model is applied 
to the simulation of a metal and a plastic. Key words: computer graphics, image synthesis, reflectance, 
shading Computing Reviews category: 8.2 Introduction The rendering of realistic images in computer 
graphics requires a model of how objects reflect light. The reflectance model must describe both the 
color and the spatial distribution of the reflected light. The model is independent of the other aspects 
of image synthesis, such as the surface geometry representation and the hidden surface algorithm. Most 
real surfaces are neither ideal specular (mirror-like) reflectors nor ideal diffuse (Lambertian) reflectors. 
Phong [13,14] proposed a Permission to copy without fee all or part of this material is granted provided 
that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice 
and the title of the publication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or 
specific permission. &#38;#169; 1981 ACM O-8971-045-1/81-0800-0307 reflectance model for computer graphics 
that was a linear combination of specular and diffuse reflection. The specular component was spread out 
around the specular direction by using a cosine function raised to a power. Subsequently, Blinn [5,6] 
used similar ideas together with a specular reflection model from [22] which accounts for the off-specular 
peaks that occur when the incident light is at a grazing angle relative to the surface normal. Whitted 
[23] extended these models by adding a term for ideal specular reflection from perfectly smooth surfaces. 
All of these models are based on geometrical optics (ray theory). The foregoing models treat reflection 
as consisting of three components: ambient, diffuse and specular. The ambient component represents light 
that is assumed to be uniformly incident from the environment and that is reflected equally in all directions 
by the surface. The diffuse and specular components are associated with light from specific light sources. 
The diffuse component represents light that is scattered equally in all directions. The specular component 
represents highlights, light that is concentrated around the mirror direction. The specular component 
was assumed to be the color of the light source and the Fresnel equation was used to obtain the angular 
variation of the intensity, but not the color, of the specular component. The ambient and diffuse components 
were assumed to be the color of the material. The resulting models produce images that look realistic 
for certain types of materials. This paper presents a reflectance model for rough surfaces that is more 
general than previous models. It is based on geometrical optics and is applicable to a broad range of 
materials, surface conditions, and lighting situations. The basis of this model is a reflectance definition 
that relates the brightness of an object to the intensity and size of each light source that illuminates 
it. The model predicts the directional distribution and spectral composition of the reflected light. 
A procedure is described for calculating RGB values from the spectral energy distribution. The new reflectance 
model is then applied to the simulation of a metal and a plastic, with an explanation Qf why images rendered 
with previous models often look plastic and how this plastic appearance can be avoided. Computer Graphics 
Volume 15, Number 3 August 1981 The Reflectance Model Given a light source, a surface, and an observer, 
a reflectance model describes the intensity and spectral composition of the reflected light reaching 
the observer. The intensity of the reflected light is determined by the intensity and size of the light 
source and by the reflecting ability and surface properties of the material. The spectral composition 
of the reflected light is determined by the spectral composition of the light source and the wavelength-selective 
reflection of the surface. In this section, the appropriate reflectance definitions are introduced and 
are combined into a general reflectance model. Figure I contains a summary of the symbols used in this 
model. The geometry of reflection is shown in Figure 2. An observer is looking at a point P on a surface. 
V is the unit vector in the direction of the viewer, N is the unit normal to the surface, and L is the 
unit vector in the direction of a specific light source. H is a normalized vector in the direction of 
the angular bisector of V and L, and is defined by V+L H len(V+L) It is the unit normal to a hypothetical 
surface that would specularly reflect light from the light source to the viewer, a is the angle between 
H and N, and 0 is the angle between H and V, so that cos(e) = V'H = L'H. The energy of the incident 
light is expressed as energy per unit time and per unit area of the reflecting surface. The intensity 
of the incident light is similar, but is expressed per unit projected area and, in addition, per unit 
solid angle [19]. The energy in an incoming beam of light is E i = Ii(N.L)dm i Except for mirrors or 
near-mirrors, the incoming beam is reflected over a wide range of angles. For this reason, the reflected 
intensity in any given direction depends on the incident energy, not just on the incident intensity. 
The ratio of the reflected intensity in a given direction to the incident energy from another direction 
(within a small solid angle) is called the bidirectional reflectance. This reflectance is fundamental 
for the study of reflection (for additional discussion, see [19]). For each light source, the bidirectional 
reflectance R is thus I r R = E i The reflected intensity reaching the viewer from each light source 
is then I r = RE i = RI~(N.L)d~ i The bidirectional reflectance may be split into two components, 
specular and diffuse. The specular component represents light that is reflected from the surface of the 
material. The a angle between N and H e angle between L and H or V and H D facet slope distribution 
function d fraction of reflectance that is diffuse d~ i solid angle of a beam of incident light E i energy 
of the incident light F reflectance of a perfectly smooth surface f unblocked fraction of the hemisphere 
G geometrical attenuation factor H unit angular bisector of V and L Ii average intensity of the incident 
light Iia intensity of the incident ambient light I r intensity of the reflected light Ira intensity 
of the reflected ambient light L unit vector in the direction of a light k extinction coefficient m root 
mean square slope of facets N unit surface normal n index of refraction R a ambient reflectance R total 
bidirectional reflectance R d diffuse bidirectional reflectance R s specular bidirectional reflectance 
s fraction of reflectance that is specular V unit vector in direction of the viewer wm relative weight 
of a facet slope Figure I. Summary of symbols. N -p J Figure 2. The geometry of reflection. diffuse 
component originates from internal scattering (in which the incident light penetrates beneath the surface 
of the material) or from multiple surface reflections (which occur if the surface is sufficiently rough). 
The specular and diffuse components can have different colors if the material is not homogeneous. The 
bidirectional reflectance is thus R = sR s + dR d , where s+d=l. In addition to direct illumination 
by individual light sources, an object may be illuminated by background or ambient illumination. All 
light that is not direct illumination from a specific light source is lumped together into ambient illumination. 
The amount of light reflected toward the viewer from any particular direction of ambient illumination 
is small, but the effect is significant when integrated over the entire hemisphere of illuminating angles. 
Consequently, it is convenient to introduce an ambient (or Computer Graphics Volume 15, Number 3 August 
1981 hemispherical-directional) reflectance, Ra. This reflectance is an integral of the bidirectional 
reflectance R and is thus a linear combination of R s and R d. For simplicity, we assume that R a is 
independent of viewing direction. In addition we assume that the ambient illumination is uniformly incident. 
The reflected intensity due to ambient illumination is defined by I = R I. f ra a la  The term f is 
the fraction of the illuminating hemisphere that is not blocked by nearby objects (such as a corner) 
[24]. It is given by f = T (N'L)d~ i , where the integration is done over the unblocked part of the 
illuminating hemisphere. The total intensity of the light reaching the observer is the sum of the reflected 
intensities from all light sources plus the reflected intensity from any ambient illumination. Assuming 
that f=1, the basic reflectance model used in this paper becomes: \ I r =IiaR a + ~ li£(N'Ll)d~il(sRs+dRd 
) £ This formulation accounts for the effect of light sources with different intensities and different 
projected areas which may illuminate a scene. For example, an illuminating beam with the same intensity 
(I i) and angle of illumination (N.L) as another beam, but with twice the solid angle (d~ i) of that 
beam, will make a surface appear twice as bright. An illuminating beam with twice the intensity of another 
beam, but with the same angle of illumination and solid angle, will also make a surface appear twice 
as bright. This paper does not consider the reflection of lig~ from other objects in the environment. 
This reflection can be calculated as in [23] or [6] if the surface is perfectly smooth, but even this 
pure specular reflection should be wavelength dependent. The above reflectance model implicitly depends 
on several variables. For example, the intensities depend on wavelength, s and d depend on the material, 
and the reflectances depend on these variables plus the reflection geometry and the surface roughness. 
The next two sections consider the directional and wavelength dependence of the reflectance model. Directional 
Distribution of the Reflected Lisht The ambient and diffuse components reflect light equally in all 
directions. Thus, Ra and Rd do not depend on the location of the observer. On the other hand, the specular 
component reflects more light in some directions than in others, so that R s does depend on the location 
of the observer. The angular spread of the specular component can be described by assuming that the 
surface consists of microfacets, each of which reflects specularly [22]. Only facets whose normal is 
in the direction H contribute to the specular component of reflection from L to V. The specular component 
is F D G Rs ~ (N'L) (N'V) The Fresnel term F describes how light is reflected from each smooth microfacet. 
It is a function of incidence angle and wavelength and is discussed in the next section. G, the geometrical 
attenuation factor, accounts for the shadowing and masking of one facet by another and is discussed elsewhere 
in detail [5,6,22]. Briefly it is 1 2(N'H)(N'V) 2(N'H)(N'L) 1 G = min I, (V'H) ' (V'H) The facet slope 
distribution function D represents the fraction of the facets that are oriented in the direction H. Various 
facet slope distribution functions have been considered by Blinn [5,6]. One of the formulations he described 
is the Gaussian model [22]: D = ce -(~/m)2 where c is an arbitrary constant. In addition to the ones 
mentioned by Blinn, other facet slope distribution models are possible. In particular, models for the 
scattering of radar and infrared radiation from surfaces are available, and are applicable to visible 
wavelengths. For example, Davies [8] described the spatial distribution of electromagnetic radiation 
reflected from a rough surface made of a perfect electrical conductor. Bennett and Porteus [3] extended 
these results to real metals, and Torrance and Sparrow [21] showed that they apply to nonmetals as well. 
Beckmann [2] provided a comprehensive theory that encompasses all of these materials and is applicable 
to a wide range of surface conditions ranging from smooth to very rough. For rough surfaces, the Beckmann 
distribution function is I tan2 ~ [  1 e-l---~--) D = m2 cos 4 This distribution function is similar 
in shape to the three functions mentioned by Blinn. The advantage of the Beckmann function is that it 
gives the absolute magnitude of the reflectance without introducing arbitrary constants; the disadvantage 
is that it requires more computation. In all of the facet slope distribution functions, the spread of 
the specular component depends on the rms slope m. Small values of m signify gentle facet slopes and 
give a distribution that is highly directional around the specular direction, as shown in Figure 3a for 
the Beokmann distribution model and in Figure 3b for the Gaussian model. Large values of m imply steep 
facet slopes and give a distribution that is spread out, as shown in Figures 3c and 3d for the Beckmann 
and Gaussian models, respectively. Note the similarity between the two models. Computer Graphics Volume 
15, Number 3 August 1981 /..,.,r,,..,, ; ! ' Figure 3a. Beckmann distribution for m=0.2. Figure 3b. 
Gaussian distribution for m=0.2. Figure 3c. Beckmann distribution for m=0.6. The wavelength dependence 
of the reflectance is not affected by the surface roughness except for surfaces that are almost completely 
smooth, which are described by physical optics (wave theory) and which have a distribution function D 
that is wavelength dependent. The Beckmann distribution model accounts for this wavelength dependence 
and for the transition region between physical and geometrical optics, i.e., between very smooth surfaces 
and rough surfaces. For simplicity we will ignore the cases in which D is wavelength dependent. (For 
a further discussion, see [2] and [8].) Some surfaces have two or more scales of roughness, or slope 
m, and can be modeled by using two or more distribution functions [15]. In such cases, D is expressed 
as a weighted sum of the distribution functions, each with a different value of m: \ D = ~_wmjD(mj) 
, J where mj = rms slope of the jth distribution. wmj = weight of the jth distribution. The sum of 
these weights is I. Spectral Composition of the Reflected Light The ambient, diffuse, and specular 
reflectances all depend on wavelength. Ra, Rd, and the F term of R s may be obtained from the appropriate 
reflectance spectra for the material. A nonhomogeneous material may have different reflectance spectra 
for each of the thFee reflectances, though R a is restricted to being a linear combination of Rs and 
Rd. ./'/'/'," / I" I / i I ....... '..~..\ . .. . i i i ! i -t .".".-".-"-.... ~ '~ i ~ ~ ~ ', ~ 
 Figure 3d. Gaussian distribution for m=0.6. Reflectance spectra have been measured for thousands of 
materials and have been collected in [9,16-18]. The reflectance data are usually for illumination at 
normal incidence. These values are normally measured for polished surfaces and must be multiplied by 
I/~ to obtain the bidirectional reflectance for a rough surface [19]. Most materials were measured at 
only a few wavelengths in the visible range (typically around 10 to 15), so that values for intermediate 
wavelengths must be interpolated (a simple linear interpolation seems to be sufficient). The reflectance 
spectrum of a copper mirror for normal incidence is shown for visible wavelengths in Figure 4a. In choosing 
a reflectance spectrum, careful consideration must be given to the conditions under which which the measurements 
were made. For example, some metals develop an oxide layer with time which can drastically alter the 
color [I]. The spectral energy distribution of the reflected light is found by multiplying the spectral 
energy distribution of the incident light by the reflectance spectrum of the surface. An example of this 
is shown in Figure 4b. The spectral energy distributions of the sun and a number of CIE standard illuminants 
are available in [7]. The spectral energy distribution of CIE standard illuminant D6500, which approximates 
sunlight on a cloudy day, is the top curve shown in Figure 4b. The lower curve shows the corresponding 
spectral energy distribution of light reflected from a copper mirror illuminated by CIE standard illuminant 
D6500 at normal incidence. It is obtained by multiplying the top curve by the reflectance spectrum in 
Figure 4a. In general, R d and F will vary with the geometry of reflection. For convenience, we will 
subsequently take R d to be the bidirectional  Computer Graphics Volume 15, Number 3 August 1981 where 
c = cos(e) = V.H g2 = n 2 +c 2_I. (Note that a similar expression in [5] is missing the 1/2 factor.) 
At normal incidence, e=O, so c=I, g=n and I n-ll 2 F o = /~-T/ Solving for n gives the equation n 
1 -~/~. Values of n determined in this way are then substituted into the original Fresnel equation to 
obtain the reflectance F at other angles of incidence. The procedure may repeated at other wavelengths 
to obtain the spectral and directional dependence of the reflectance. The dependence of the reflectance 
on wavelength and the angle of incidence implies that the color of the reflected light changes with 
the incidence angle. Reflectance spectra for copper are shown in Figure 5a. As the incidence angle 
(e) approaches ~/2, the color of the reflected light approaches the color of the light source (since 
 the reflectance F approaches unity). The colors corresponding to the reflection of white light (CIE 
standard illuminant D6500) from copper are shown as a function of e in Figure 5b. It is evident that 
the color shift from the Fresnel equations only becomes important as e approaches ~/2 (i.e., as the 
angle between V and L approaches ~). Calculation of the color shift is computationally expensive. It 
can be simplified in one of two ways: by creating lookup tables or by using the following approximation. 
Values of F are first calculated for a value of n corresponding to the average normal reflectance. These 
values are then used to interpolate between the color of the material at e=0 and the color at 8=~/2, 
which is the color of the light source because F~/2 is 1.0 at every wavelength. For example, let the 
red component of the color at normal incidence be Red o and let the red component of the color of the 
incident light be Red~/2. Then the red component of the color at other angles is max(O ,F O -Fo ) Red 
O = Redo + (Red,t/2-Redo) F~/2-Fo The green and blue components are interpolated similarly. Figure 
5e shows the effect of using the approximate procedure to estimate the color of copper as a function 
of incidence angle. The approximate procedure yields results that are similar to those from the complete 
(but more expensive) procedure (Figure 5b). The foregoing approximation must always be used if the 
spectral energy distribution of the reflected light is not known, in which case all of the RGB values 
are estimates. Determining the RGB Values For a computer synthesized scene to be realistic, the color 
sensation of an observer watching the synthesized scene on a color television monitor must be approximately 
equivalent to the color sensatio n of an observer watching a corresponding scene in the real world. To 
produce this equivalent color sensation, the laws of trichromatic color reproduction are used to convert 
the spectral energy distribution of the reflected light to the appropriate RGB values for the particular 
monitor being used. Every color sensation can be uniquely described by its location in a three dimensional 
color space. One such color space is called the XYZ space. A point in this space is specified with three 
coordinates, the color's XYZ tristimulus values. Each spectral energy distribution is associated with 
a point in the XYZ color space and thus with tristimulus values. If two spectral energy distributions 
are associated with the same tristimulus values, they produce the same color sensation and are called 
metamers. The red, green, and blue phosphors of a monitor can be illuminated in proportions to produce 
a set of spectral energy distributions which define a region of XYZ space called the gamut of the monitor. 
The goal, then, is to find the proportions of phosphor illumination that produce a spectral energy distribution 
that is a metamer of the spectral energy distribution of the reflected light. These proportions are 
determined by calculating the XYZ tristimulus values that are associated with the spectral energy distribution 
of the reflected light and then calculating the RGB values that produce a spectral energy distribution 
with these tristimulus values. To do this, the spectral energy distribution of the reflected light is 
multiplied by the XYZ matching functions (obtained from [7]) at every wavelength. The resulting spectra 
are then integrated to obtain the XYZ tristimulus values. These XYZ values are converted by a matrix 
multiplication to RGB linear luminance values for a particular set of phosphors and monitor white point. 
The linear luminances are then converted to RGB voltages, taking into account the nonlinearities of 
the monitor and the effects of viewing conditions. For a more complete description of this procedure, 
see [12]. The monitor has a maximum luminance at which it can reproduce a given chromaticity. Any XYZ 
values that represent luminances greater than this maximum are outside the gamut of the monitor. To avoid 
this problem, all XYZ values in the scene are scaled equally so that they all lie inside the monitor 
gamut and usually so that the colbr .with the greatest luminance is reproduced on the monitor at the 
maximum luminance possible for its chromaticity. But even with scaling, some spectral energy distributions 
are associated with trimstimulus values that lie outside the gamut of the monitor. Because such a color 
cannot be reproduced on the monitor at any luminance, it must be approximated by a similar color that 
lies inside the monitor gamut. This color may be chosen in many different ways; in this case we   Computer 
Graphics Volume 15, Number 3 August 1981 International Commission on Illumination," Publication CIE No. 
15, Colorimetry (E-1.3.1), Bureau Central de la CIE, Paris, 1970. 8. Davies, H., "The Reflection of 
Electromagnetic Waves from a Rough Surface," Proceedings of the Institution of Electrical Ensineers, 
v.I01 pp.209-214, 1954.  9. Gubareff, G. G., Janssen, J. E., and Torborg, R. H., Thermal Radiation Properties 
Survey: A Review of the Literature, Honeywell Research Center, Minneapolis, 1960.  10. Hunter, Richard 
S., The Measurement of Appearance, John Wiley &#38; Sons, New York, pp.26-30, 1975.  11. Judd, Deane 
B., and Wyszecki, Guenter, Color in Business. Science. a~ Industry, John Wiley &#38; Sons, New York, 
pp.170-172, 1975.  12. Meyer, Gary W., and Greenberg, Donald P., "Perceptual Color Spaces for Computer 
Graphics," SIGGRAPH 1980 Proceedings, Computer Graphics, v.14 #3 pp.254-261, 1980.  13. Phong, Bui Tuong, 
"Illumination for Computer-Generated Images," PhD dissertation, University of Utah, Salt Lake City, 1973. 
 14. Phong, Bui Tuong, "Illumination for Computer Generated Pictures," Communications of the ACM, v.18 
pp.311-317, 1975.  15. Porteus, J. 0., "Relation between the Height Distribution of a Rough Surface 
and the Reflectance at Normal Incidence," Journal of the Optical Society of America, v.53 pp.1394-1402, 
1963.  16. Purdue University, Thermophysical Properties o_ff Matter, vol. 7." Thermal Radiative Properties 
o_ff Metals, 1970.  17. Purdue University, Thermophysical Properties of Matter, vol. 8: Thermal Radiative 
Properties of Nonmetallic Solids, 1970.  18. Purdue University, ~ Properties  of Matter, vol. 9: Therma~ 
Radiative Properties o_ff Coatings, 1970.  19. Siegel, Robert and Howell, John R., Thermal Radiation 
Heat Transfer, McGraw-Hill, New York, pp.64-73, 1980.  20. Sparrow, Ephraim M. and Cess, R. D., Radiation 
Heat Transfer, McGraw-Hill, New York, pp.64-68, 1978.  21. Torrance, Kenneth E. and Sparrow, Ephraim 
M., "Biangular Reflectance of an Electric Nonconductor as a Function of Wavelength and Surface Roughness," 
Journal of Heat Transfer,  v.87 pp.283-292, 1965.  22. Torrance, Kenneth E. and Sparrow, Ephraim M., 
"Theory for Off-Specular Reflection From Roughened Surfaces," Journal of the Optical Society of America, 
v.57 pp.1105-1114, September 1967.  23. Whitted, Turner, "An Improved Illumination Model for Shaded 
Display," Communications of the ACM, v.23 pp. 343-349, 1980.  24. Whitted, Turner, private communication. 
   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>806820</article_id>
		<sort_key>317</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1981</article_publication_date>
		<seq_no>38</seq_no>
		<title><![CDATA[Vectorized procedural models for natural terrain]]></title>
		<subtitle><![CDATA[Waves and islands in the sunset]]></subtitle>
		<page_from>317</page_from>
		<page_to>324</page_to>
		<doi_number>10.1145/800224.806820</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=806820</url>
		<abstract>
			<par><![CDATA[<p>A ray-tracing procedural model is described, in which ocean waves and islands are rendered by different but related algorithms. The algorithms are based on analytic formulas involving arithmetic operations, trigonometric functions, and square roots, and are organized for a vectorizing compiler on a Cray 1, a &#8220;supercomputer&#8221; with a vector pipeline architecture. Height field methods are used, one vertical scan line at a time, to trace the direct rays to the ocean, where they are reflected. Approximate methods are then applied to find whether the reflected rays meet any other object on their way to the sky. The output, at eight bits per pixel, gives information for shading, e.g. the angle of the surface normal for rays meeting the islands, or the angle of elevation from the horizon for rays continuing unobstructed to the sky.</p> <p>The output is recorded on a magnetic tape for each frame in one cycle of the wave motion, and plotted offline on a Dicomed D-48 color film recorder. The eight bits per pixel are interpreted by a color translation table, which is gradually changed as the wave cycle is repeated to simulate the changing illumination during sunset.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Color table animation]]></kw>
			<kw><![CDATA[Height field]]></kw>
			<kw><![CDATA[Line buffer]]></kw>
			<kw><![CDATA[Natural terrain]]></kw>
			<kw><![CDATA[Piercing]]></kw>
			<kw><![CDATA[Pipeline]]></kw>
			<kw><![CDATA[Procedural model]]></kw>
			<kw><![CDATA[Ray tracing]]></kw>
			<kw><![CDATA[Reflection]]></kw>
			<kw><![CDATA[Vectorized]]></kw>
			<kw><![CDATA[Water waves]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>B.2.1</cat_node>
				<descriptor>Pipeline</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010600.10010615</concept_id>
				<concept_desc>CCS->Hardware->Integrated circuits->Logic circuits</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP15033556</person_id>
				<author_profile_id><![CDATA[81100480335]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nelson]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Max]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lawrence Livermore National Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>907365</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Newell, M. The Utilization of Procedure Models in Digital Image Synthesis. PhD Thesis, University of Utah (1975)]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807479</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Rubin, S., and Whitted, T., A 3-Dimensional Representation for Fast Rendering of Complex Scenes. Computer Graphics Vol. 14, No. 3 (1980) pp. 110-116]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Schachter, B., Long Crested Wave Models. Computer Graphics and Image Processing Vol. 12 (1980) pp. 187-201]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Pyramid Catalogue, (1981) Pyramid, Box 1048 Santa Monica, or poster available from Jannes Art Publishing, Chicago]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Whitted, T., An Improved Illumination Model for Shaded Display. Comm. ACM Vol. 23, No. 6, (1980) pp. 343-349]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Minnaert, M., Light and Color in the Open Air. Dover Publications, Inc. (1954) New York]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807418</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Shoup, R., Color Table Animation. Computer Graphics Vol. 13, No. 2 (1979) pp. 286-292]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Longuet-Higgins, M., The Statistical Analysis of a Random, Moving Surface. Proc. Roy. Soc. of London Vol. 249 (1957) pp. 321-387]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Stokes, G.G., Mathematical and Physical Papers. Vol. 1 (1880) p. 341, Cambridge University Press]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Schwartz, L., Computer Extension and Analytic Continuation of Stokes' Expansion of Gravity Waves. J. Fluid Mech. Vol. 62, part 3 (1974) pp. 553-578]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Fishman, B., and Schachter, B., Computer Display of Height Fields. Computers and Graphics Vol. 5 (1980) pp. 53-60]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563893</ref_obj_id>
				<ref_obj_pid>965141</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Blinn, J., Models of Light Reflection for Computer Synthesized Pictures. Computer Graphics Vol. 11, No. 2 (1977) pp. 192-198]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>507101</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Blinn, J., Simulation of Wrinkled Surfaces. Computer Graphics Vol. 12, No. 3 (1978) pp. 286-292]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806819</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Cook, R., and Torrance, K., Reflectance Models for Computer Graphics. Vol. 15, No. 13 (1981)]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 15, Number 3 August 1981 Vectorized Procedural Models for Natural Terrain: 
Waves and Islands in the Sunset Nelson L. Max Lawrence Livermore National Laboratory Abstract A ray-tracing 
procedural model is described, in which ocean waves and islands are rendered by different but related 
algorithms. The algorithms are based on analytic formulas involving arithme- tic operations, trigonometric 
functions, and square roots, and are organized for a vectorizing compiler on a Cray i, a "supercomputer" 
with a vector pipeline architecture. Height field meth- ods are used, one vertical scan line at a time, 
to trace the direct rays to the ocean, where they are reflected. Approximate methods are then applied 
to find whether the reflected rays meet any other object on their way to the sky. The output, at eight 
bits per pixel, gives information for shading, e.g. the angle of the surface normal for rays meeting 
the islands, or the angle of elevation from the horizon for rays continuing unobstructed to the sky. 
 The output is recorded on a magnetic tape for each frame in one cycle of the wave motion, and plotted 
offline on a Dicomed D-48 color film recorder. The eight bits per pixel are inter- preted by a color 
translation table, which is gradually changed as the wave cycle is repeated to simulate the changing 
illumination during sunset. Key Words Ray tracing, vectorized, pipeline, height field, natural terrain, 
color table animation, piercing, line buffer, reflection, procedural model, water waves. C.R. Catagories 
8.2, 3.14 Permission to copy without fee all or part of this material is granted provided that the copies 
are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of 
the publication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 
&#38;#169;1981 ACM O-8971-045-1/81-0800-0317 $00.75 Introduction A ray tracing algorithm for raster 
computer graphics follows the ray from the observer through each pixel, as it meets and possibly reflects 
from surfaces in a scene. Newell ~l~,and Eubin and Whitted ~2~ have proposed procedural models for ray 
tracing. Each object comes with its own algorithm to detect whether and where a ray meets it, and to 
determine the reflection and/or shading. If algorithms requiring a search in a data base are excluded, 
and combinations of standard analytic functions are used instead, such algorithms can run efficiently 
on a vector pipeline machine. The algorithms reported here were implemented on a Cray i, but the CDC 
Cyber 205 and the TI ASC have similar pipeline architectures. Such machines can perform arithmetic operations 
much more efficiently when the same operation is applied to large vectors of operands, in which case 
one says that the computation is vectorized. This paper presents vectorized procedural al- gorithms 
for natural scenes, including reflections from water surfaces rippled by superimposed traveling sine 
waves. Minnaert E6~ has excellent explanations for the optical phenomena during sun- sets, and produced 
by reflections in water waves. Schlachter E3~ has proposed a model for random wave fields, which involves 
table look-up of pre- computed narrow band waveforms, and is thus not easily vectorized. Information 
International used an illumination model to produce a leader for Pyramid films E4~, showing beautiful 
eycloidal waves in the sunset. More complex effects, such as the reflection of objects in the water, 
require the tracing of reflected rays. Whitted E5~describes an algorithm which exhaustively traces all 
reflected and refracted rays, bu~ which operates very slowly. In the current worM, each ray is reflected 
a maximum of two times from the water, and then continues toward the other objects in the scene, which 
are diffusely reflecting. This allows the tracing to be treated in a uniform vectorized manner, resulting 
in much higher efficiency. Color Table Animation Color table animation has been used by Shoup E7~ on 
a single picture in a frame buffer. In the movie described here, this technique is used instead with 
a periodic cycle of frames to expand it to a longer film. 3]7 Computer Graphics Volume 15, Number 3 
August 1981 The pictures are plotted offline from mag- netic tape, by a Varian V-75 minicomputer control- 
ling a Dicomed D-48 color film recorder. The Dicomed operates at 8 bits per pixel, and can pass the 8 
input bits through a color translation table before recording onto film. For the pictures pre- sented 
here, the same data was input three times, and recorded once through each of the red, green, and blue 
filters, with a different color transla- tion table in each pass. The 256 available colors in this "paint 
by numbers" scheme were divided up into groups; one group was used for green island colors, one group 
for brown island colors, and one group for sky or sun colors. Within each group, the numbers coded for 
illumination information which, together with the current position of the sun, was used to specify the 
color. The ray from the eye through the center of each pixel was traced until it ended up at its final 
destination~ either piercing an island or continuing on to the sky, perhaps after reflecting once or 
twice on the water. A number in one of the groups was then assigned to the pixel, as described below. 
Note that this procedure makes anti-aliasing between islands and sky impossible. To make the longest 
sequence in the film, one cycle of periodic motion was recorded on tape, and repeatedly rendered onto 
film. The sun was gradu- ally moved around a great circle C in the sky, and the color tables were changed 
appropriately, so that the repeated frames were differently illumin- ated each time through the cycle. 
 Color Specification for Diffuse Reflection Figure i Figure i shows a unit sphere centered at the observer 
O, on which the circle C is projected. The vector U points to the setting sun, the vector V points toward 
the sun at its maximum elevation, and the vector W = U x V is normal to the plane of C. The angle e 
between P and U is 2w/~4 times the number of hours before sunset. The sun's position as a function of 
e, is P(e) = U cos e + V sin 8 Suppose a ray traced from the observer through a given pixel ends up, 
perhaps after reflection, at a point on diffusely reflecting object where the unit surface normal is 
N. If N'P < O, the surface faces away from the sun, and will be in shadow. If N-P > 0 the point may or 
may not be illuminated, depending on the presence of other surfaces between it and the su~. The pictures 
here were computed assuming no other surfaces cast shadows. Since N'P = N'U cos 8+ N'V sin e =N'U~ 
+N-V sin e, N'P becomes zero when Q(N) = sin ~, where N.U N~U  n/ + N For a given position P(8) 
of the sun, the arc on the surface where p(N) = sin e divides the sur- face into a region in light and 
a region in shadow. A sequence of these arcs divide the surface into regions which successively darken 
as the sun sets. Thus, a function of the form A + Bp(N) can be used to specify the color number. When 
the sun is at position P(e), those numbers which are greater than A + B sin 8 correspond to shadows, 
while numbers decreasing from this value become brighter, with the number A + B sin (8-w/2) the brightest. 
The two parameter family of normal vectors has been reduced to one parameter, which is not sufficient 
to simulate Lambert's law of diffuse reflection. Nevertheless, this shading scheme does indicate the 
 shape of the surface. In order to break up the boundaries between the "paint by numbers" regions, a 
random number between 0 and i is added to A + B p(N) before the result is truncated to an integer. The 
seed of the random number generator is set to the same value at the beginning of each frame, so the texture 
generated by this process will not jitter from frame to frame. Color Specification for the Sky The 
sky is normally a darker and more saturated  blue near the zenith than near the horizon. During sunset, 
the brilliant sky colors also occur in approximately horizontal bands. Thus the z compon- ent of a 
ray reaching the sky can be used in computing a code for the color. All the ray tracing and hidden 
surface compu- tations are done by the Cray I, and the Varian V-75 controlling the film recorder has 
no know- ledge of the scene which produced a given raster image. However, the V-75 can compute a simple 
function of the color code in the input data and the raster position of the pixel to determine the output 
color on the film. The color translation is such a function, depending only on the input data. Another 
such calculation adds the sun's disk to the appropriate pixels by the rule: "If the pixel is above the 
horizon and entirely inside the sun's disk, and the input data specifies a sky color, then render the 
pixel sun color." An additional rule for anti-aliasing might read: "If the pixel is above the horizon 
and near the bound- ary of the sun's disk, and the input data specifies a sky color, render the pixel 
an appropriately weighted average of sUn color and the sky color." In order to show the glimmer of the 
sun re- flecting in the water, some of the 256 possible num- bers are assigned to identify the highlights 
pro- duced by different possible sun positions. A ray reflected in the direction of the unit vector R 
can meet the sun's disk only if IW'HI < cos 6, Computer Graphics Volume 15, Number 3 August 1981 where 
~ is the angular radius of the sun, about 75 , and W is the unit vector normal to the plane of the sun's 
motion, as in figure i. If this is the case, then for afternoon positions of the sun, between U and V 
in figure i, the z component R of R can be used to identify the time of day when z these reflections 
produce the highlights. For other times of day, the same z component can be used to determine the sky 
color as before. Thus, a second range of z component codes is used to iden- tify reflected rays along 
the track of the sun. As the unit vector P in the direction of the sun approaches a position with a specific 
z value, the corresponding table entry is changed smoothly from its sky value to the sun color, and then 
back again as the vector P passes and a new table entry brightens. At least one entry is sun color at 
any time, and adjacent entires are intermediate be- tween sky and sun color. In addition, a third range 
of z component codes is used for values of IW.RI between cos ~ and cos (3 ~), to give further intermediate 
colors for rays deviating horizontally from the sun's position. These intermediate colors allow the reflections 
to change continuously with the time of day, and serve to increase the area in highlight and to anti-alias 
the edge of the bright highlight areas. An appropriately scaled random number is added to IW.R] and to 
R before they are thresholded to determine the colo~. This again breaks up the boundaries between colored 
regions in the sky, and in its reflection. Figure 4 shows the result when the water is perfectly still. 
The sun's reflection is oblong because ~was chosen larger to brighten the glimmer, and is trapezoidal 
because the sun is following a diagonal course as in figure i, so that W is not horizontal. Water Waves 
 Waves on the surface of water are affected by two forces: gravity and surface tension. However, when 
the wavelength is longer than a few inches, gravity is the dominant force. The motion of the water is 
described by non-linear partial differen- tial equations. In an approximate solution to these equations, 
valid for waves of small amplitude, the velocity c is proportional to the square root of the wavelength 
l: c = g~  Here k = 2w/X is called the wave number, and g is the acceleration of gravity. The height 
of the free surface can be approximated to first order by the wave train z(x,t) = a cos k(x -ct) = 
a cos (kx -~t)  Here a is the amplitude, and ~ = kc is the angular frequency in radians per second, 
On a two dimen- sional surfac% the wave train is f~rthe~ spe$ified by a wave vector (£,m) such that 
Z + m = k-. ~he equation for the surface of a long crested wave train then becomes z(x,y,t ) = -h 
+ a cos (£x + my -mt)  where h is the distance of the mean sea level be- low the eye at z = o. In 
the linear first approximation, these wave trains pass through each other without modification, and 
a wavey surface can be represented as a sum of several wave trai~s: n f(x,y,t) = -h + ~ a. cos (£.x 
+ ...I) j=l J J mjy-~jt)  If a cycle of waves is to be repeated in a film, this function must be periodic 
in time, and the frequencies ~. must all be multiples of some fun- damental freqOaency. Since m = kc 
and c =~ , this mes.ns that the wave numbers must be propor- tional to the square of the frequencies. 
 If the largest wave has a period equal to the repeating cycl~ and wavelength ~, the available wavelengths 
are thus ~, ~/4, ~/9, ~/16, ... . This puts some restrictions on the superimposed wave trains, but it 
is still possible to produce a reasonable looking surface. More random sur- faces have been studied by 
Schachter E3] and Longuet-Higgins E8] . Water waves with large amplitude have wide shallow troughs, 
and narrow more highly curved crests, as shown in figure 2. Thus the cosine wave approximation is inadequate 
for large amp- litudes, and a more precise solution is required. There is an exact solution to the equations 
of motion in which a vertical section of the free sur- face takes the form of a cycloid. If the water 
is excited from rest by wind on its surface, or a group of waves entering from a distance, one can prove 
that the curl of the velocity vector field remains zero, i.e.,the flow remains irrotational. The exact 
solution just men- tioned is not irrotational, and is therefore unreasonable. Stokes [9] gave a method 
for calculating perio- dic irrotational waves of finite amplitude by successive approximation of their 
fourier series. If the waves are symmetrical about their peaks at x = O, the fourier series takes the 
form z (x,t) = ~ a cos n k (x -ct) ...2) n n=l  Let a = ka be the maximum slope of the first term 
 i  of this series. Stokes solved for the other am- plitudes as expansions in powers of a, and found, 
to fourth order, a I = a/k a 2 = (a2/2 + iTa4/24)/k a 3 = (3a3/S)/k a h (a4/3)/k  Figure 2 shows the 
contribution of these first four terms to the shape of the wave. Schwartz ~I0] ..... ---.......-- Figure 
2. Stokes' approximations to water wave forms.  Computer Graphics Volume 15, Number 3 August 1981 has 
recently found many more terms of this series by computer. The resulting waves are called Stokes waves, 
and form a one parameter family of shapes, depending on the value a. Their velocity also depends on a, 
but only in the quadratic and higher terms This effect has been neglected here. A few terms of the series 
2) can easily be added into the sum i) to give the waves of large slope a more realistic appearance. 
In the pic- tures here, only the second order term was added to the largest wave. For more speed, it 
would be possible to expand the first few terms of 2) as a polynomial in cos k (x -ct), so that the cosine 
need only be computed once. Hidden Surfaces for Height Fields Fishman and Sehachter [ii~ have described 
an algorithm for rendering raster images of height fields, i.e.,single valued functions of two var- iables. 
Assume f(x,y) is such a function, and an observer at (0,0,0) looks along the y axis at the surface z 
= f(x,y), projecte d onto a picture plane at y=l. A vertical scan line in this plane at x=s contains 
the projections of points lying in the plane x=sy. y = = f(x,y) Figure 3. Perspective projection along 
a vertical scan line. In figure 3 the point P = (sy, y, f (sy,y)) on the surface projects to the point 
~ on the pic- ture plane at height z = g(y) = f(sy,y)/y. Al- though f is single valued, the function 
g(y) in general is not. The height field algorithm in Ell~ computes g(yi ) for evenly spaced values of 
y., and l assigns to a pixel the color of the surface at the first (sy.,l y')l where g(yi ) exceeds 
the z value for the center of the pixel. Such evenly spaced y. are clearly inefficient for showing 
an un- 1 . . bounded surface in perspectlve, slnce the pro- jected points become increasingly dense 
near the horizon Instead, the expected density o of pro- jected values g(y.) per pixe] should be constant 
 I l This will be the case if y. = h/(io), where the . l surface is assumed to lle near the plane 
z = -h. This choice of y. reproduces the detail of the l small rlpples near the eye, but does not 
waste time near the horizon. For the pictures here, ~= 4. The values g(y.) are computed from equation 
i) i in a vectorlzed loop. In FORTRAN array notation, let y(i) = y. and g(i) = g(y.),and let h(k) be 
the z value in t~e picture plane of the center of plxel i k. The following loop computes the array 
yp, where yp(k) is the approximate y value for the point projecting into pixel k. k=l DO 30 i = 2,n 
 40 IF (g(i).GT. h(k)) THEN yp(k) = y(i-l) + (y(i) -y (i-l))* i (h(k) -g(i-l)) / (g(i) -g(i-l)) k=k+l 
 GO TO 40 ENDIF 30 CONTINUE This loop cannot be vectorized because the indices k change in an unpredictable 
manner, but it constitutes only a small part of the computation. Once the values yb = yp(k) have been 
found, the partial derivatives ~/Sx and ~f/~y of the surface at each point (x,y) = (sy k, yk ) can be 
computed from equation i): n ~f (sYk' Yk' t) /~x = -Z ~ a sin (bjy k + cj) j=l J J n ~f (sYk' Yk' 
t) /3y = -Z m a sin (bjy k + cj) j=l j J where b.3 gjs + mj, and cj = -~3.t The range of k is much larger 
than the range of j, so the loop on k is made the inner one. It can be easily vectorized, taking advantage 
of the common subexpression a. sin (bjy k + cj) The surface _D normal (x, y, z) is then computed by 
 = 1/~(~f/~x)2 + (~f/~y)2 + m x = -z (~f/~x) If a ray is traced from the eye to the point (sy, y, f 
(sy, y)), its reflection can be found using the normal vector (~, ~, ~) and the formulas in Blinn ~2~ 
The shading will be computed by tracing this reflected ray to its final destina- tion. The Islands 
The same height field hidden surface algorithm is used for the islands, which are represented as elliptical 
paraboloids with superimposed cosine  Computer Graphics Volume 15, Number 3 August 1981 terms to give 
rolling hills. However, the shading is determined from the direction of the normal vector, rather than 
from the reflected ray. The elliptical parabo~id for island k is given by Pk(X,y) = h k -ek(X-Xk )2 
-fk(X-Xk)(Y-Yk) -gk(Y-Yk)2 ...3) where (X. Y~, h. ) is the highest point on the K K K parabolold. 
After the cosine terms are added, the surface becomes Qk(X,y) = Pk(X,y) n k +.Z a. cos (~jk x + mjkY) 
...4) j=l jk The maximum perturbation that the cosine terms can make is % =jZ__~ I ajk Similarly, 
the maximum perturbation a water wave from equation i) can make on the sea level -h is n d= Z a . j=l 
J Therefore, any visible points (x,y) where the island protrudes above the level of the ocean lie inside 
the ellipse Pk(X,y) + d k + h + d = 0 The sample values (sYi, yi ) need only be taken inside this ellipse. 
 The height field computation is performed for the ocean surface and also for all the islands~ and 
the surface closest to the viewer is chosen to shade each pixel. This gives a brute force depth buffer 
hidden surface algorithm on each vertical scan line, which gains its efficiency through the vector 
pipeline processing. Each island is actually formed from three para- boloids of different colors. The 
beach is a shallow brown paraboloid, which is rendered tan because of its nearly horizontal surface. 
The cliffs are rendered by a steeper brown paraboloid, and the rolling hills above them are made of 
a third green paraboloid. The simple depth buffer al@orithm could take the union of these three para- 
 boloids and compute the nearest point. However, when comparing the hills above the cliffs, it is actually 
the farther of these two surfaces which is visible, so special logic is required. This is simplified 
by defining the hills by the same equa- tion as the cliffs, and taking the top of the cliffs to lie 
in a single plane. Whenever the equation generates a point above this plane, the height above the cliff 
level is decreased by a constant factor, and the point is colored green. A more realistic cliff line 
would result if a separate equation were used for the hills. Tracin~ Reflected Rays A ray reflected 
from the ocean must be traced to find out if it pierces any other objects before meeting the sky. Since 
these rays do not originate at the eye, an involved computation would be required to reproduce for reflected 
rays the accuracy provided by the height field algorithm for the direct rays. Instead, the scene is 
approxim- ated by simpler quadratic surfaces, which permit direct ray piercing algoritllms. For the 
purposes of ray piercing, the islands are represented by equation 3), without the cosine terms. A position 
S at a distance t along the ray in direction (R , R , R ) from the point (Qx' Qy' Qz ) is given byXtheYvec~or 
function (Sx,Sy,S z) = (Qx+tRx,%+tRy,Qz+tRz) ---5) To find the distances t_ and t_ at which this ± 
w ray plerces the paraboloid, S and S are sub- . . x . stituted into equatlon 3) and the re~ultmng 
sur- face height is set equal to S ; giving z Pk(Qx+tRx , Qy+tRy) - Qz - tRz = 0 Since P~ (x,y) is 
quadratic in x and y, this K yields an equation in t of the form at 2 + bt + c = 0 ...6) whose coefficients 
are expressed in terms of the constants in equations 3) and 5). The well-known quadratic formula gives 
t = -btiT 2a where the discriminant d=b2-4ac. The quadratic formula is vectorized by using the absolute 
value of d in the square root, and the smallest root t I is found. If d is negative the ray actually 
 misses the paraboloid, and if t is negative, the ray meets it in the negative o~the reflected direction. 
These cases are eliminated in a second vectorized loop, which also checks whether t is less than the 
distance to the closest island f~und so far for the ray. If all the conditions are met, the name and 
distance to the closest is- land are updated. Since the islands represent the intersections of cliff 
and hill paraboloids, the ray must pierce both paraboloids to hit the island. If the hill surface has 
the same equation as the cliff, except for an abrupt change in slope at the cliff edge, the formulas 
for the coefficients a, b, and c of equations 6) for the two paraboloids contain many common subexpressions, 
which leads to greater efficiency. When the nearest island (if any) has been found for each ray, the 
surface normal at the piercing point is evaluated from equation 4) with all the cosine terms, and used 
to find the shading as before. This is similar to the approximation used by Blinn [13~ to render wrinkled 
surfaces; the wrinkles show up in the interior shading, but not on the profile. The discrepancy in the 
re- flection of the profile would never be noticed if there were ripples in the water. Figure 4 shows 
a reflection in completely still water. The bright highlight on the cliff results from normal vectors 
which face towards the sun, computed by the wrinkle algoritlnu at piercing points which would   Computer 
Graphics Volume 15, Number 3 August 1981 a more realistic (but less colorful) image, second- ary reflections 
on the water must be taken into account. For this purpose, the first crest of the wave of largest amplitude 
in front of the reflected ray is approximated by a parabolic cylinder which is tangent to the crest and 
has the same curvature as the Stokes wave at the crest. A parabolic cylinder is a degenerate form of 
an elliptical paraholoid, and a simplified version of the piercing algorithm discussed above is used 
to find the piercing point. The normal to the actual wave at the x, y coordin- ates of the piercing point 
is then used to find the second reflected ray. It is still possible that a third reflection could occur, 
but this is unlikely, and is not considered. Instead the z component of the second reflected ray is replaced 
by its absolute value. About 10% to 15% of the rays undergo secondary reflection. When these cases are 
detected from the discriminant of the quadratic equation 6), their data is copied ("compressed" or "gathered") 
into consecutive positions in auxilary data vectors. This allows subsequent calculations for these cases 
to be efficiently vectorized. The resulting second reflected ray is then copied back ("de- compressed" 
or "scattered") into the arrays con- taining the first reflected rays, so that all the rays can be continued 
together to meet the islands or the sky. Figures 6 and 7 show the results of this algorithm. They are 
made at 1020 x 1023 pixel resolution, using the same 8 bit data, with diff- erent color translation tables. 
The computation time was 34 seconds on the Cray l, and the plotting time 5 minutes on the Dicomed, including 
45 seconds to backspace the tape twice between color passes. Directions for the Future There are several 
possible improvements for these pictures. First, no account has been taken of refraction. According to 
Fresnel's law, (see Cook and Torrence [14~) a glancing ray is almost completely reflected by a water 
surface, but for rays closer to the surface normal, a substantial fraction of the energy is refracted 
into the water. Conversely a ray approaching the surface of the water from below will have this same 
fraction re- fracted into the air. Sunlight scattered back up by small particles in the water may refract 
~ward the observer. Thus the front surfaces of waves take on the color of the water rather than the color 
of the sky, and appear dark green. To handle this refraction correctly, one would need many interpolated 
colors between sky color and water color, and initial tests indicated that an 8 bit color table is inadequate 
for this purpose. A larger (software) color table could he used, or the ray tracing program could output 
the actual red, green, and blue values for each pixel instead of one color table index. When sunlight 
reflects from rippled water and then strikes another object, one can see ripples of light on the object. 
It should be possible to trace a family of sun rays meeting a small patch of water, and then trace the 
reflected and re- fracted rays until they pierce a small object extending above and below the water 
line. The rays could be averaged into an array representing the surface of the object, to create the 
ripples of illumination. The illuminated object could also be seen by reflection in the water, and by 
re- fraction through it, modified and attenuated by the scattering. Finally, it should be possible to 
render clouds by the height function techniques. The deviations of the clouds from a mean cloud level 
could be com- puted using formulas involving polynomials, cosines square roots, ...etc. The shading would 
not depend on the normal vector, but rather on the depth be- low mean cloud level. The height field algorithm 
could be modified to handle double valued func- tions whose domain is only a subset of the plane. As 
before, an approximation would replace the height field algorithm for reflected rays. These would be 
traced till they pierced the mean cloud level. If the piercing point lay in the domain of the cloud function, 
the shade would be assigned according to the color of the cloud below the x, y coordinates of the piercing 
point. Clouds between the sun and the water would intercept the rays generating the highlights, and cause 
shadows in the sun's glimmer on the water. The clouds could also glow in the sunset. Acknowledgments 
 This work was inspired by the films of Turner Whitted and Loren Carpenter. I would like to thank Turner 
Whitted for encouraging conversations, John Engle, AI Shannon, and Frank McMahon for helping me vectorize 
the FORTRAN code for the Cray i, John Guckenheimer for pointing out references E93, the reviewers for 
pointing out references E3~ and[ll~, Don Faul and Steven Williams for carefully reading the manuscript, 
Dick Rau and Rick Simms for making last minute color corrected prints, and many friends and colleagues 
for suggesting ways to make the pictures more realistic. This work was performed under the auspices of 
the U.S. Department of Energy by Lawrence Livermore National Laboratory under contract W-7405-Eng-48. 
 Disclaimer This document was prepared as an account of work sponsored by an agency of the United States 
Government. Neither the United States Government nor the University of California nor any of their employees, 
make any warranty, express or implied, or assumes any legal liability or responsibility for the accuracy, 
completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents 
that its use would not infringe privately owned rights. Reference herein to any specific commercial products, 
process, or service by trade name, trademark, manufacturer, or otherwise, does not necessarily constitute 
or imply its endorsement, recommendation, or favoring by the United States Government or the University 
of California. The views and opinions of authors expressed herein do not necessarily state or reflect 
those of the United States Government thereof, and shall not be used for advertising or product endorsement 
purposes. Computer Graphics Volume 15, Number 3 August 1981 References Eli Newell, M. The Utilization 
of Procedttre Models in Digital Image Synthesis. PhD Thesis, University of Utah (1975) E2] Rubin, S., 
and Whitted, T., A 3-Dimensional Representation for Fast Rendering of Complex Scenes. Computer Graphics 
Vol. 14, No. 3 (1980) pp. 110-116 [3] Schachter, B., Long Crested Wave Models. Computer Graphics and 
Image Processing Vol. 12 (1980) pp. 187-201 E4] Pyramid Catalogue, (1981) Pyramid, Box 1048 Santa Monica, 
or poster available from Jannes Art Publishing, Chicago E5] Whitted, T., An Improved lllumination Model 
for Shaded Display. Comm. ACM Vol. 23, No. 6, (1980) pp. 343-349 E6] Minnaert, M., Light and Color in 
the Open Air. Dover Publications, Inc. (1954) New York E73 Shoup, R., Color Table Animation. Computer 
Graphics Vol. 13, No. 2 (1979) pp. 286-292 E83 Longuet-Higgins, M., The Statistical Analysis of a Random, 
Moving Surface. Proc. Roy. Soc. of London Vol. 249 (1957) pp. 321- 387 E9] Stokes, G.G., Mathematical 
and Physical Papers. Vol. i (1880) p. 341, Cambridge University Press ElO] Schwartz, L., Computer Extension 
and Analytic Continuation of Stokes' Expansion of Gravity Waves. J. Fluid Mech. Vol. 62, part 3 (1974) 
pp. 553-578 [li] Fishman, B., and Schaehter, B., Computer Display of Height Fields. Computers and Graphics 
Vol. 5 (1980) pp. 53-60 El2] Blinn, J., Models of Light Reflection for Computer Synthesized Pictures. 
Computer Graphics Vol. ii, No. 2 (1977) pp. 192-198 [13] Blinn, J., Simulation of Wrinkled Surfaces. 
Computer Graphics Vol, 12, No. 3 (1978) pp. 286-292 E14] Cook, R., and Torrance, K., Reflectance Models 
for Computer Graphics. Vol. 15, No. 13 (1981)  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1981</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
</content>
</proceeding>
