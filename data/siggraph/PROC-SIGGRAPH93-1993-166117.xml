<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>08-02-1993</start_date>
		<end_date>08-06-1993</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Anaheim]]></city>
		<state>CA</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>166117</proc_id>
	<acronym>SIGGRAPH '93</acronym>
	<proc_desc>Proceedings of the 20th annual conference</proc_desc>
	<conference_number></conference_number>
	<proc_class>conference</proc_class>
	<proc_title>Computer graphics and interactive techniques</proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn>0-89791-601-8</isbn>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>1993</copyright_year>
	<publication_date>09-01-1993</publication_date>
	<pages>432</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source>ACM member price $40</other_source>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node>I.3.0</cat_node>
			<descriptor/>
			<type/>
		</primary_category>
		<other_category>
			<cat_node>I.3.3</cat_node>
			<descriptor></descriptor>
			<type></type>
		</other_category>
		<other_category>
			<cat_node>I.3.5</cat_node>
			<descriptor></descriptor>
			<type></type>
		</other_category>
		<other_category>
			<cat_node>I.3.7</cat_node>
			<descriptor></descriptor>
			<type></type>
		</other_category>
	</categories>
	<ccs2012>
		<concept>
			<concept_id>0.10010147.10010371.10010396</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010371.10010352</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
			<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010371.10010372</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10003752.10010061.10010063</concept_id>
			<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10003752.10010061</concept_id>
			<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010371.10010382</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010371</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
			<concept_significance>500</concept_significance>
		</concept>
	</ccs2012>
	<general_terms>
		<gt>Algorithms</gt>
		<gt>Design</gt>
	</general_terms>
	<chair_editor>
		<ch_ed>
			<person_id>P193374</person_id>
			<author_profile_id><![CDATA[81100122627]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[Mary]]></first_name>
			<middle_name><![CDATA[C.]]></middle_name>
			<last_name><![CDATA[Whitton]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[]]></affiliation>
			<role><![CDATA[Chairman]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>1993</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>166118</article_id>
		<sort_key>15</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[2-D shape blending]]></title>
		<subtitle><![CDATA[an intrinsic solution to the vertex path problem]]></subtitle>
		<page_from>15</page_from>
		<page_to>18</page_to>
		<doi_number>10.1145/166117.166118</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166118</url>
		<keywords>
			<kw><![CDATA[character animation]]></kw>
			<kw><![CDATA[numerical algorithms]]></kw>
			<kw><![CDATA[shape blending]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Hierarchy and geometric transformations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010244</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Hierarchical representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P281616</person_id>
				<author_profile_id><![CDATA[81100400673]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Sederberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brigham Young Univ., Provo, UT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39028932</person_id>
				<author_profile_id><![CDATA[81339500292]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Peisheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brigham Young Univ., Provo, UT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14083480</person_id>
				<author_profile_id><![CDATA[81406591738]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Guojin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Zhejiang Univ., China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P111631</person_id>
				<author_profile_id><![CDATA[81392614583]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brigham Young Univ., Provo, UT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[J. Alan Adams. The intrinsic method for curve definition. Computer-Aided Design, 7(4):243-249,1975.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Harold J. Bailey, Kathleen M. Brautigam, and Trudy H. Doran. Apple Logo. Brady Communications Company, Inc., Bowie, MD, 1984.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617460</ref_obj_id>
				<ref_obj_pid>616003</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Shenchang Eric Chen and Richard Parent. Shape averaging and its applications to industrial design. IEEE CG&amp;A, 9(1):47-54,1989.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Peisheng Gao. 2-d shape blending: an intrinsic solution to the vertex path problem. Master's thesis, Brigham Young University, Department of Civil Engineering, 1993.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Andrew Glassner. Metamorphosis. preprint, 1991.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134004</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[John F. Hughes. Scheduled fourier volume morphing. Computer Graphics (P1vc. SIGGRAPH), 26(2):43-46,1992.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Anil Kaul and Jarek Rossignac. Solid-interpolating deformations: Construction and animation of PIPs. In F.H. Post and W. Barth, editors, P~vc. Eurographics '91, pages 4931505. Elsevier Science Publishers B.V, 1991.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134007</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[James R. Kent, Wayne E. Carlson, and Richard E. Parent. Shape transformation for polyhedral objects. Computer Graphics (P1vc. SIGGRAPH), 26(2):47-54, 1992.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[S. C. Malik. MathematicaIAnalysis. John Wiley &amp; Sons, Inc., New York, 1984.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Eadweard Muybridge. Animals in Motion. Dover Publications, Inc., New York, 1957.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Thomas W. Sederberg and Eugene Greenwood. Shape blending of 2-d piecewise curves. Submitted.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134001</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Thomas W. Sederberg and Eugene Greenwood. A physically based approach to 2-d shape blending. Computer Graphics (P1vc. SIGGRAPH), 26(2):25-34, 1992.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Yoshihisa Shinagawa and Tosiyasu L. Kunii. The differential model: A model for animating transformation of objects using differntial information. In Tosiyasu L. Kunii, editor, Modeling in Computer Graphics, pages 5-15, Tokyo, 1991. Springer-Verlag.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Geoffrey Slinker. Inbetweening using a physically based model and nonlinear path interpolation. Master's thesis, Brigham Young University, Department of Computer Science, 1992.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 2 D Shape Blending: An Intrinsic Solution to the Vertex Path Problem Thomas W. Sederberg1, Peisheng 
Gao1, Guojin Wang2, and Hong Mu1 Abstract This paper presents an algorithmfor determiningthe paths along 
which corresponding vertices travel in a 2 D shape blending. Rather than considering the vertex paths 
explicitly, the algorithm de.nes the inter­mediate shapes by interpolating the intrinsic de.nitions of 
the initial and .nal shapes. The algorithm produces shape blends which gener­ally are more satisfactory 
than those produced using linear or cubic curve paths. Particularly, the algorithm can avoid the shrinkage 
that normally occurs when rotating rigid bodies are linearly blended, and avoids kinks in the blend when 
there were none in the key polygons. Categories and Subject Descriptors: I.3.3 [ Computer Graphics]: 
Pic­ture/Image Generation; I.3.5 [ Computer Graphics]: Computational Geometry and Object Modeling. General 
Terms: Algorithms Additional Key Words and Phrases: Shape blending, character anima­tion, numerical algorithms. 
 1 Introduction This paper deals with shape blending of 2 D polygons. As illustrated in Figures 1 and 
2, a shape blendalgorithmdetermines the in-betweenpolygonswhich providea smooth transformation between 
two given 2 D polygons, referred to as the key polygons. Shape blending requires the solution of two 
main subproblems: the vertex cor­respondence problem (that is, determining which vertex on one key polygon 
will travel to which vertex on the other key polygon), and the vertex path problem (that is, determining 
along what path each vertex will travel). For 2 D polygonal shapes, a solution to the vertex correspondence 
problem is presented in [12]. Shape blending of 2 D B´ezier curve shapes is addressed in [11]. Various 
solutions to the shape interpolation of 3 D polyhedra are presented in [3, 5, 7, 8]. This paper addresses 
the vertex path problem and is motivated by two .gures from [12]. Figure 1.a provides an example of a 
shape blend in which the middle shapes is derived from its neighboring key polygons. This is basically 
a good shape blend, except that the dancer s arm in the middle frame is only half as long as it is in 
the key frames. The shape blend in Figure 2.a looks .ne except that the chicken s neck gets shorter. 
These shortenings occur because of the linear path followed by vertices during the shape blend, as shown 
by the path travelled by the chicken s beak. These problems seriously weaken the practical use of 2 D 
shape blending using linear vertex motion, for applications such as character animation. The contribution 
of this paper is an improved method for computing in-between frames, once the Permission to copy without 
fee all or part of this material is granted provided that the copies are not made or distributed for 
direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, 
and notice is given that copying is by permission of the Association for Computing Machinery. To copy 
0 otherwise, or to republish, requires a fee and/or specific permission. &#38;#169;1993 ACM-0-89791-601-8/93/008/0015 
$1.50  a. Compressed Neck b. More Natural Neck Figure 2: Chicken correspondence between key polygons 
has been determined as in [12]. Sample results of the new vertex path algorithm are shown in Figures 
1.b and 2.b. A referee of paper [12] remarked: I am unhappy with the phrase, physically based, in this 
context. The physics here has nothing to do with the physics of chickens, . . . , or any of the other 
nominal subjects of interpolation. That observation formulates precisely the problem we confront in trying 
to infer the correct motion between two changing shapes. While [12] demonstrates that an algorithm which 
knows nothing about the physics of a chicken is able to correlate the prominent features of two chicken 
outlines, the accurate computation of motion as a chicken lowers his head really calls for a model of 
the chicken s skeleton, musculature, etc.. What we seek is a tool that might assist a traditional animator 
to create convincing computer-assisted in-betweens, when the only information available is contained 
in the two key frames. The solution presented here is a heuristic whose justi.cation lies in the fact 
that it generally seems to work rather well. 1Engineering Computer Graphics Laboratory 368 Clyde Building 
Brigham Young University Provo, UT 84602 tom@byu.edu 2Zhejiang University, China 1.1 Proposed solution 
A polygon de.nition which lists the Cartesian coordinates of its vertices might be called an explicit 
description. An alternate means of de.ning a polygon is in terms of the lengths of its edges and the 
angles at its vertices. Such a polygon description forms the basis of an approach to geometry, popular 
in elementary education, known as Turtle Graphics [2] wherein a polygon is de.ned by instructions such 
as: walk 10 paces to the east, turn 45to the left and proceed 6 paces, turn 30 to the right and go 5 
more paces, . . .. This paper postulates that the heuristic of blending intrinsic de.nitions (edge lengths 
and vertex angles) of two key polygons will generally produce a more satis­ factory in-between motion 
than will linear vertex paths. Evidence that this is so is provided in the .gures. 1.2 Related work One 
alternative to linear vertex paths is to de.ne vertex paths of higher degree. For 3 D polyhedral shape 
transformations, [8] proposes using an Hermite cubic path with end tangents set equal to the vertex normals. 
While this idea evidently is effective for the transformations between highly dissimilar shapes addressed 
in [8], it would not generallyworktoowell forcharacteranimationsince motiondoes notuniformlyoccur normal 
to a curve outline. [14] develops an approach to character animation using quadratic B´ezier vertex paths. 
By default, vertices travel along a parabolic arc such that the distance from each vertex to the center 
of mass of all vertices changes monotonically. Also, it allows the user to signify a pivot point for 
appendages. The current algorithm works with less user interaction. In other approaches to shape blending, 
such as Minkowski sums [7], the ver­tex path and vertex correspondence problems are coupled and solved 
simultaneously. Minkowski sums, however, blur even gross details such as arms and legs when blend­ing 
non-convex objects, and hence are not suitable for character animation. Shape blends that operate on 
an implicit de.nition of the curve or surface, 0 or f x y z f x y 0, [6] likewise don t currently support 
the detail required for character animation. Of course, the substantial literature on physically based 
modelling and synthetic actors is also highly relevant, though such methods rely on more information 
than is available to us. Ideas for modeling with intrinsically de.ned curves are proposed in [1], and 
[13] looks at curve and surface kinematics based on differential equations. 2 Intrinsic shape interpolation 
Denote the vertices of the two key polygons by 01. . .1. P A t P A m n P B P B n P AA PP BiB i d P A 
t i d P B mi P xi n We assume that both key polygons have the same number of edges, as will be the 
case after vertex correspondence is established [12]. In this discussion, we use the convention that 
counter-clockwise angles are positive. For convenience, we adopt the notation 1 where is the number of 
polygon edges. Our goal is to compute the vertices 12. . .for the polygon which is of the way betweenand 
, 0 1. 0 will be taken as the anchor point, and its position determines the rigid body translation of 
the shape. This, along with the directed anglesandformed by the -axis and the vectors 00 and , is discussed 
further in section 3. 01 01 P2 .2L1 .1 Pn-2 PB1 P1 PA1 LB0 LA0 .n-2 Ln-2 = PAn PA0 P0= Pn Pn-1 PB0.n-1 
Figure 3: Intrinsic variables Begin by obtaining the intrinsic de.nitions of and by computing the polygon 
angles and edge lengths shown in Figure 3: 12... 1 L Ai j P Ai A i P BAi i j t i A L B i t j P mBA 
B i and 2 1 1 The intermediate polygons in the shape blend are then computed by interpolating the respective 
vertex angles and edge lengths: j 01 3 00 112...41012...5 L i i t t L AAii tLt B i i m m  Unfortunately, 
the problem is not completely solved at this point, since the re­sulting polygon will not generally close. 
Figure 4 shows what the chicken and dancer polygons look like at this stage of the algorithm. It is somewhat 
surprising that these Figure 4: Unclosed polygons polygons, each with over 200 vertices, come so close 
to ending where they started (and this happens typically, in our experience). But the problem remains, 
how do we best adjust the lengths and angles so that the polygon does close. There are two solutions 
to this problem. The .rst is to leave the angles unchanged and tweak the lengths (section 2.1). This 
turns out to have a straightforward, closed­form solution. The other approach is to treat the open polygon 
as a piece of wire for which we de.ne the physical rules for stretching and vertex bending. Adjustments 
to angles and/or edges can then be computed iteratively by determining the equilibrium shape when the 
two open polygon vertices are forced to coincide. 2.1 Edge Tweaking To close the polygons by adjusting 
the edge lengths only, re-write equation 5 as 1012...6L Bi j L ABtolL i i n - ty L A L iA ii2 L + Btm 
tL i BBS i L L tol A S ii L B i i e  mm j L A i It seems smart that the magnitudes of should 
roughly be proportional to , since if an edge has the same length on both key polygons, it ought to have 
aboutthatsamelengththroughouttheshapeblend. Onecandreamupsimpleexamples for which this is not desirable, 
but for most reasonable cases, experience has veri.ed this to be wise. Therefore, de.ne max 012...7 where 
00001max is needed to avoid division 0 by zero. S S m X m Our goal is to .nd 01. . ., so that the objective 
function 2 f S S S m i L AB S ii ... 01 L S S S m XX i + L Ai tL Bi S i B i 20 is minimized subject 
to the two equality constraints (which force closure of the poly­gon): 1 01...1cos00 201...1 sin0 L S 
S S m im + t L Ai tL Bi S i B i 0 whereare the directed angles from the -axis to the vectors 1, 112... 
8 The method of Lagrange multipliers [9] can now solve for the desired tweak values as follows. Set S 
ii + + i S i 2 S i S m xi f + Lm + P L i P i F1201...1122 where 1 and 2 are the multipliers. From 
F 21cos 2sin 0 0 1 ... 1 cos 0 0 + 8. P .. + Si m L + ABSii t + n L A FE i + i tL G+F B + i +VS U i B 
i i m 1 sin 0 0 we obtain 12 where cos 10 0 2 FEG X im X i mm L AB L AB ii i cos i i sin 11 0 2 sin2 
12 0 21 cos13 0 UV 2 X i E mm G + + VUF 6 tGFtL A i tL B i B i 21 sin 14 0 Thus under the condition 
2 0 we can get 1 15 2 16 and 1 21cos 2sin 0 1 ... 17 2 Using equations 4, 6, and 8, we can now calculate 
the coordinates of thevertices 1 2 ... : S i x i P x ii 2 L AB i i LS i + i 2 + mEF ii 2 +VU 1 y 2 i 
y ii 2 2 E F 2 F G iL i 2 i m 2 x i y i 9 1 1cos1 1 1sin1 18 2.2 Tweaking Lengths and/or Angles The edge-tweaking-only 
method generally gives good results and is relatively fast. Also, as suggested from Figure 4, often very 
little edge length adjustment is needed. However, some simple examples can be found where the edge lengths 
may change more than is desirable using the edge-tweaking-only method. This can be detected by checking 
the values of . In such a case, the required edge length adjustments can be diminished by also allowing 
the angles to change. A good solution to this problem is to treat the unclosed polygon as a piece of 
wire which can possibly stretch, but which can only bend at polygon vertices. The stretching stiffness 
for each polygon edge is inversely proportional to the change in length experienced by that edge between 
the two key frames. Likewise, the bending stiffness of each angle is inversely proportional to the change 
between key frames of the respective angle. These stiffness values tend to enforce rigidity for identical 
portions of the two key polygons. Theshape oftheclosed intermediatepolygonis thencomputedbyforcingthe 
two unclosed joints to coincide, and determining the unique equilibrium shape of the wire. Further details 
can be found in [4]. 3 Anchor points and angle lines Since an intrinsic de.nition of a polygon is invariant 
to rigid body motion, a shape blend must specify translations and rotations for the intermediate shapes. 
Translation is speci.ed using an anchor point path, and rotation is constrained by designating the rotation 
function of an angle line. The anchor point can be a polygon vertex, or any other point that is well 
de.ned for each step in the shape blend. For example, center of area is a good anchor point for objects 
in free fall. For bodies in free fall, such as a diver, a parabolic anchor path simulates the effects 
of gravity. The angle line can be any line whose association with each shape in the blend can be determined, 
such as a non-degenerate polygon edge, the line between any two points on the polygon, or a principle 
axis of a shape (if the major and minor axes are well de.ned, ie., the product of inertia is non-zero). 
In Figure 3, the anchor point is P L 0 and the angle line is 0. 4 Discussion Figure1showsthatthemainadvantageofusingturtlegraphicsinshapeblendingis 
that it helps solve the withering arm problem. Another bene.t is that it can provide more nearlymonotonicanglechangesthandoeslinear-vertex-pathshapeblending. 
Figure5 shows a shape blend, taken from [12], in which a shape which should undergo a simple rigid body 
motion experiences shrinking and kinking. The kinking occurs in this case becauseof a poorchoice ofvertices, 
a commonoccurrencein linear-vertex-pathshape blending. Clearly, turtle graphics shape blending would 
have no problem in this case.  Figure 5: Shrinking plus kinking A more impelling example is the dancer 
s arm which withers under linear vertex path motion. The magni.cation in Figure 6 illustrates that the 
intrinsic method produces inherently smoother blends than the linear vertex paths. [12] goes to great 
lengths investigating how to minimize this angle non-monotonicity which can occur with linear vertex 
paths. As an added bene.t of the intrinsic algorithm, this detailed search for non-monotonic angle changes 
is rendered unnecessary.  Linear vertex paths Intrinsic blend Figure 6: Closeup of dancer s arm Althoughslower 
than the linear patch method, the intrinsic algorithm can compute a shape blend for the chicken in Figure 
2.b (which has 230 vertices) in 0.02 seconds using the method in Section 2.1 and in 0.05 seconds using 
the method in Section 2.2, on an HP 730 workstation. It is easy to contrive examples for which this algorithm 
performs poorly, although most of the realistic cases we have tried produced good results. In cases where 
some adjustment is called for, additional constraints can be imposed, such as specifying that the distance 
between speci.ed pairs of non-adjacent polygon vertices should change monotonically from one key frame 
to the next. See [4] for more details.  Figure 7: Cantering Horse Experience suggests that this algorithm 
may work well enough for many appli­cations to character animation. The sequence of a cantering horse 
in Figure 7 was taken from the classic photographic study, Animals in Motion [10], .rst published in 
1887. The top four .gures are digitizations of actual photographs from the book. In the bottom row, the 
middle two .gures are shape blends interpolating the .rst and last .gures. The vertex correspondence 
was determined using the algorithm in [12], and the vertex paths were computed using the algorithm in 
this paper. The horse s two left legs were treated as independent shape blends. Acknowledgements Geoffrey 
Slinker, Hank Christiansen, Alan Zundel, and Kris Klimaszewski provided much helpful discussion. This 
work was supported under NSF grant DMC-8657057. Author/Title Index [1] J. Alan Adams. The intrinsic method 
for curve de.nition. Computer-Aided Design, 7(4):243 249, 1975. [2] Harold J. Bailey, Kathleen M. Brautigam, 
and Trudy H. Doran. Apple Logo. Brady Communications Company, Inc., Bowie, MD, 1984. [3] Shenchang Eric 
Chen and Richard Parent. Shape averaging and its applications to industrial design. IEEE CG&#38;A, 9(1):47 
54, 1989. [4] Peisheng Gao. 2 d shape blending: an intrinsic solution to the vertex path problem. Master 
s thesis, Brigham Young University, Department of Civil Engineering, 1993. [5] Andrew Glassner. Metamorphosis. 
preprint, 1991. [6] JohnF.Hughes.Scheduledfouriervolumemorphing. Computer Graphics (Proc. SIGGRAPH), 
26(2):43 46, 1992. [7] Anil Kaul and Jarek Rossignac. Solid-interpolating deformations: Construction 
and animation of PIPs. In F.H. Post and W. Barth, editors, Proc. Eurographics 91, pages 493 505. Elsevier 
Science Publishers B.V, 1991. [8] James R. Kent, Wayne E. Carlson, and Richard E. Parent. Shape transformation 
for polyhedral objects. Computer Graphics (Proc. SIGGRAPH) , 26(2):47 54, 1992. [9] S. C. Malik. Mathematical 
Analysis. John Wiley &#38; Sons, Inc., New York, 1984. [10] Eadweard Muybridge. Animals in Motion. Dover 
Publications, Inc., New York, 1957. [11] Thomas W. Sederberg and Eugene Greenwood. Shape blending of 
2 d piecewise curves. Submitted. [12] Thomas W. Sederberg and Eugene Greenwood. A physically based approach 
to 2 d shape blending. Computer Graphics (Proc. SIGGRAPH) , 26(2):25 34, 1992. [13] YoshihisaShinagawaandTosiyasuL.Kunii.Thedifferentialmodel:Amodelfor 
animatingtransformationof objects usingdifferntialinformation. InTosiyasuL. Kunii, editor, Modeling in 
Computer Graphics , pages 5 15, Tokyo, 1991. Springer-Verlag. [14] Geoffrey Slinker. Inbetweening using 
a physically based model and nonlinear path interpolation. Master s thesis, Brigham Young University, 
Department of Computer Science, 1992.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166119</article_id>
		<sort_key>19</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Mesh optimization]]></title>
		<page_from>19</page_from>
		<page_to>26</page_to>
		<doi_number>10.1145/166117.166119</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166119</url>
		<keywords>
			<kw><![CDATA[geometric modeling]]></kw>
			<kw><![CDATA[model simplification]]></kw>
			<kw><![CDATA[range data analysis]]></kw>
			<kw><![CDATA[surface fitting]]></kw>
			<kw><![CDATA[three-dimensional shape recovery]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.6</cat_node>
				<descriptor>Least squares methods</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003716</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003716</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P112851</person_id>
				<author_profile_id><![CDATA[81100397561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hugues]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoppe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39045514</person_id>
				<author_profile_id><![CDATA[81100493833]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[DeRose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP17009957</person_id>
				<author_profile_id><![CDATA[81100301736]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Duchamp]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39075754</person_id>
				<author_profile_id><![CDATA[81452609404]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McDonald]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P298076</person_id>
				<author_profile_id><![CDATA[81100357122]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Werner]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stuetzle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>105540</ref_obj_id>
				<ref_obj_pid>105539</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ruud M. Bolle and Baba C. Vemuri. On three-dimensional surface reconstruction methods. IEEE PAMI, 13(1): 1-13, January 1991.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[T. DeRose, H. Hoppe, T. Duchamp, J. McDonald, and W. Stuetzle. Fitting of surfaces to scattered data. SPIE, 1830:212-220, 1992.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Gene Golub and Charles Van Loan. Matrix Computations. John Hopkins University Press, 2nd edition, 1989.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Ardeshir Goshtasby. Surface reconstruction from scattered measurements. SPIE, 1830:247-256, 1992.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134011</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle. Surface reconstruction from unorganized points. Computer Graphics (SIGGRAPH '92 Proceedings), 26(2):71-78, July 1992.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle. Mesh optimization. TR 93-01-01, Dept. of Computer Science and Engineering, University of Washington, January 1993.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[J.L. Mallet. Discrete smooth interpolation in geometric modeling. CAD, 24(4):178-191, April 1992.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Samuel Matin and Philip Smith. Parametric approximation of data using ODR splines. GMR 7057, General Motors Research Laboratories, May 1990.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122742</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[J.V. Miller, D.E. Breen, W.E. Lorensen, R.M. O'Bara, and M.J. Wozny. Geometrically deformed models: A method for extracting closed geometric models from volume data. Computer Graphics (SIGGRAPH '91 Proceedings), 25(4):217-226, July 1991.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134010</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[William Schroeder, Jonathan Zarge, and William Lorensen. Decimation of triangle meshes. Computer Graphics (SIGGRAPH '92 Proceedings), 26(2):65-70, July 1992.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[R. B. Schudy and D. H. Ballard. Model detection of cardiac chambers in ultrasound images. Technical Report 12, Computer Science Department, University of Rochester, 1978.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[R.B. Schudy and D. H. Ballard. Towards an anatomical model of heart motion as seen in 4-d cardiac ultrasound data. In Proceedings of the 6th Conference on Computer Applications in Radiology and Computer- Aided Analysis of Radiological Images, 1979.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122745</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Stan Sclaroff and Alex Pentland. Generalized implicit functions for computer graphics. Computer Graphics (SIGGRAPH '91 Proceedings), 25(4):247-250, July 1991.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[E.H. Spanier. Algebraic Topology. McGraw-Hill, New York, 1966.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134008</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Greg Turk. Re-filing polygonal surfaces. Computer Graphics (SIG- GRAPH '92 Proceedings), 26(2):55-64, July 1992.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[G. Wyvill, C. McPheeters, and B. Wyvill. Data structures for soft objects. The Visual Computer, 2(4):227-234, August 1986.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Mesh Optimization Hugues Hoppe3 Tony DeRose3 Tom Duchampt John McDonald+ Werner Stuetzle+ University 
of Washington Seattle, WA 98195 Abstract We present a method for solving the following problem: Given 
a set of data points scattered in three dimensions and an initial triangular mesh Mo, produce a mesh 
M, of the same topological type as Mo, that .ts the data well and has a small number of vertices. Our 
ap­proach is to minimize an energy function that explicitly models the competing desires of conciseness 
of representation and .delity to the data. We show that mesh optimization can be effectively used in 
at least two applications: surface reconstruction from unorga­nized points, and mesh simpli.cation (the 
reduction of the number of vertices in an initially dense mesh of triangles). CR Categories and Subject 
Descriptors: I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling. Additional Keywords: 
Geometric Modeling, Surface Fitting, Three-Dimensional Shape Recovery, Range Data Analysis, Model Simpli.cation. 
1 Introduction The mesh optimization problem considered in this paper can be roughly stated as follows: 
Given a collection of data points Xin R3and an initial triangular mesh Monear the data, .nd a mesh M 
of the same topological type as Mothat .ts the data well and has a small number of vertices. As an example, 
Figure 7b shows a set of 4102 data points sampled from the object shown in Figure 7a. The input to the 
mesh optimiza­tion algorithm consists of the points together with the initial mesh shown in Figure 7c. 
The optimized mesh is shown in Figure 7h. Notice that the sharp edges and corners indicated by the data 
have been faithfully recovered and that the number of vertices has been signi.cantly reduced (from 1572 
to 163). 3Department of Computer Science and Engineering, FR-35 tDepartment of Mathematics, GN-50 1Department 
of Statistics, GN-22 This work was supported in part by Bellcore, the Xerox Corporation, IBM, Hewlett-Packard, 
AT&#38;T Bell Labs, the Digital Equipment Corpora­ tion, the Department of Energy under grant DE-FG06-85-ER25006, 
the Na­ tional Library of Medicine under grant NIH LM-04174, and the National Science Foundation under 
grants CCR-8957323 and DMS-9103002. Permission to copy without fee all or part of this material is granted 
 provided that the copies are not made or distributed for direct provided that the copies are not made 
or distributed for direct commercial advantage, the ACM copyright notice and the title of the commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by publication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinery. permission of the Association for Computing Machinery. To 
copy otherwise, or to republish, requires a fee and/or specific permission. otherwise, or to republish, 
requires a fee and/or specific permission. &#38;#169;1993 -0---8/93/008/0015 $1.50 &#38;#169;1993 ACM 
-0-89791 -601 -8/93/008 $1.50 To solve the mesh optimization problem we minimize an energy function that 
captures the competing desires of tight geometric .t and compact representation. The tradeoff between 
geometric .t and compact representation is controlled via a user-selectable parameter Crep. A large value 
of Crepindicates that a sparse representation is to be strongly preferred over a dense one, usually at 
the expense of degrading the .t. We use the input mesh Moas a starting point for a non-linear optimization 
process. During the optimization we vary the number of vertices, their positions, and their connectivity. 
Although we can give no guarantee of .nding a global minimum, we have run the method on a wide variety 
of data sets; the method has produced good results in all cases (see Figure 1). We see at least two applications 
of mesh optimization: surface reconstruction and mesh simpli.cation. The problem of surface reconstruction 
from sampled data occurs in many scienti.c and engineering applications. In [2], we outlined a two phase 
procedure for reconstructing a surface from a set of un­organized data points. The goal of phase one 
is to determine the topological type of the unknown surface and to obtain a crude es­timate of its geometry. 
An algorithm for phase one was described in [5]. The goal of phase two is to improve the .t and reduce 
the number of faces. Mesh optimization can be used for this purpose. Although we were originally led 
to consider the mesh optimiza­tion problem by our research on surface reconstruction, the algo­rithm 
we have developed can also be applied to the problem of mesh simpli.cation. Mesh simpli.cation, as considered 
by Turk [15] and Schroeder et al. [10], refers to the problem of reducing the number of faces in a dense 
mesh while minimally perturbing the shape. Mesh optimization can be used to solve this problem as follows: 
sample data points Xfrom the initial mesh and use the initial mesh as the starting point Moof the optimization 
procedure. For instance, Fig­ure 7q shows a triangular approximation of a minimal surface with 2032 vertices. 
Application of our mesh optimization algorithm to a sample of 6752 points (Figure 7r) from this mesh 
produces the meshes shown in Figures 7s (487 vertices) and 7t (239 vertices). The mesh of Figure 7s corresponds 
to a relatively small value of Crep, and therefore has more vertices than the mesh of Figure 7t which 
corresponds to a somewhat larger value of Crep. The principal contributions of this paper are: It presents 
an algorithm for .tting a mesh of arbitrary topolog­ical type to a set of data points (as opposed to 
volume data, etc.). During the .tting process, the number and connectivity of the vertices, as well as 
their positions, are allowed to vary.  It casts mesh simpli.cation as an optimization problem with an 
energy function that directly measures deviation of the .­nal mesh from the original. As a consequence, 
the .nal mesh  Figure 1: Examples of mesh optimization. The meshes in the top row are the initial meshes 
M; the meshes in the bottom row are the o corresponding optimized meshes. The .rst 3 columns are reconstructions; 
the last 2 columns are simpli.cations. Simplicial complex K 1, 2, 3 vertices:{} {} {} edges: {12}, {23}, 
{13 ,, ,} faces: {123, ,} () K Topological realization Geometric realization V v3 fV( K ) z e1 v1 pb 
e2 f yv2 e3 Rm R3 x Figure 2: Example of mesh representation: a mesh consisting of a single face. naturally 
adapts to curvature variations in the original mesh. It demonstrates how the algorithm s ability to recover 
sharp edges and corners can be exploited to automatically segment the .nal mesh into smooth connected 
components (see Fig­ure 7i).  2 Mesh Representation Intuitively, a mesh is a piecewise linear surface, 
consisting of trian­gular faces pasted together along their edges. For our purposes it is important to 
maintain the distinction between the connectivity of the vertices and their geometric positions. Formally, 
a mesh Mis a pair (K,V), where: Kis a simplicial complex representing the connectivity of the vertices, 
edges, and faces, thus determining the topological type of the mesh; V={VI,...,Vm}, ViER3is a set of 
vertex positions de.ning the shape of the mesh in R3(its geometric realization). A simplicial complex 
Kconsists of a set of vertices {I,...,m}, together with a set of non-empty subsets of the vertices, called 
the simplices of K, such that any set consisting of exactly one vertex is a simplex in K, and every non-empty 
subset of a simplex in Kis again a simplex in K(cf. Spanier [14]). The 0-simplices {i}EK are called vertices, 
the 1-simplices {i,j}EKare called edges, and the 2-simplices {i,j,k}EKare called faces. A geometric 
realization of a mesh as a surface in R3can be obtained as follows. For a given simplicial complex K, 
form its topological realization IKIin Rmby identifying the vertices {I,...,m}with the standard basis 
vectors {eI,...,em}of Rm. For each simplex sEKlet IsIdenote the convex hull of its vertices in Rm, and 
let IKI=UsEKIsI. Let q:Rm-R3be the linear map that sends the i-th standard basis vector eiERmto ViER3 
(see Figure 2). The geometric realization of Mis the image qv(IKI), where we write the map as qvto emphasize 
that it is fully speci.ed by the set of vertex positions V={VI,...,Vm}. The map qvis called an embedding 
if it is 1-1, that is if qv(IKI)is not self-intersecting. Only a restricted set of vertex positions Vresult 
in qvbeing an embedding. If qvis an embedding, any point pEqv(IKI)can be parame­terized by .nding its 
unique pre-image on IKI. The vector bEIKI with p=qv(b)is called the barycentric coordinate vector of 
p (with respect to the simplicial complex K). Note that barycentric coordinate vectors are convex combinations 
of standard basis vec­tors eiERmcorresponding to the vertices of a face of K. Any barycentric coordinate 
vector has at most three non-zero entries; it has only two non-zero entries if it lies on an edge of 
IKI, and only one if it is a vertex. 3 De.nition of the Energy Function Recall that the goal of mesh 
optimization is to obtain a mesh that provides a good .t to the point set Xand has a small number of 
vertices. We .nd a simplicial complex Kand a set of vertex posi­tions Vde.ning a mesh M=(K,V)that minimizes 
the energy function E(K,V)=Edist(K,V)+Erep(K)+Espring(K,V). The .rst two terms correspond to the two 
stated goals; the third term is motivated below. The distance energy Edistis equal to the sum of squared 
distances from the points X={XI,...,Xn}to the mesh, n Edist(K,V)=d2(Xi,qv(IKI)). i=I The representation 
energy Ereppenalizes meshes with a large number of vertices. It is set to be proportional to the number 
of vertices mof K: Erep(K)=Crepm. The optimization allows vertices to be both added to and removed from 
the mesh. When a vertex is added, the distance energy Edist is likely to be reduced; the term Erepmakes 
this operation incur a penalty so that vertices are not added inde.nitely. Similarly, one wants to remove 
vertices from a dense mesh even if Edistincreases slightly; in this case Erepacts to encourage the vertex 
removal. The user-speci.ed parameter Crepprovides a controllable trade-off be­tween .delity of geometric 
.t and parsimony of representation. We discovered, as others have before us [8], that minimizing Edist+Erepdoes 
not produce the desired results. As an illus­tration of what can go wrong, Figure 7d shows the result 
of min­imizing Edistalone. The estimated surface has several spikes in regions where there is no data. 
These spikes are a manifestation of the fundamental problem that a minimum of Edist+Erepmay not exist. 
To guarantee the existence of a minimum [6], we add the third term, the spring energy Espring. It places 
on each edge of the mesh a spring of rest length zero and spring constant .: Espring(K,V)=.lVJ0Vkl2 
{J,k}EK It is worthwhile emphasizing that the spring energy is not a smoothness penalty. Our intent is 
not to penalize sharp dihedral angles in the mesh, since such features may be present in the un­derlying 
surface and should be recovered. We view Espringas a regularizing term that helps guide the optimization 
to a desirable local minimum. As the optimization converges to the solution, the magnitude of Espringcan 
be gradually reduced. We return to this issue in Section 4.4. For some applications we want the procedure 
to be scale­invariant, which is equivalent to de.ning a unitless energy function E. To achieve invariance 
under Euclidean motion and uniform scal­ing, the points Xand the initial mesh Moare pre-scaled uniformly 
to .t in a unit cube. After optimization, a post-processing step can undo this initial transformation. 
 4 Minimization of the Energy Function Our goal is to minimize the energy function E(K,V)=Edist(K,V)+Erep(K)+Espring(K,V) 
over the set Jof simplicial complexes Khomeomorphic to the ini­tial simplicial complex Ko, and the vertex 
positions V de.ning the embedding. We now present an outline of our optimization algo­rithm, a pseudo-code 
version of which appears in Figure 3. The details are deferred to the next two subsections. To minimize 
E(K,V)over both Kand V, we partition the prob­lem into two nested subproblems: an inner minimization 
over Vfor .xed simplicial complex K, and a outer minimization over K. In Section 4.1 we describe an algorithm 
that solves the inner min­imization problem. It .nds E(K)=minvE(K,V), the energy OptimizeMesh(Ko,Vo) 
{ K:= Ko V:= OptimizeVertexPositions(Ko,Vo) Solve the outer minimization problem. repeat { (K',V') := 
GenerateLegalMove(K,V) V'= OptimizeVertexPositions(K',V') if E(K',V')<E(K,V)then (K,V) := (K',V') endif 
}until convergence return (K,V) } Solve the inner optimization problem E(K)=minvE(K,V) for .xed simplicial 
complex K. OptimizeVertexPositions(K,V) {repeat { Compute barycentric coordinates by projection. B:= 
ProjectPoints(K,V) Minimize E(K,V,B)over Vusing conjugate gradients. V:= ImproveVertexPositions(K,B) 
}until convergence return V  } GenerateLegalMove(K,V) {Select a legal move K'K'. Locally modify Vto 
obtain V'appropriate for K'. return (K',V') } Figure 3: An idealized pseudo-code version of the minimization 
algorithm. of the best possible embedding of the .xed simplicial complex K, and the corresponding vertex 
positions V, given an initial guess for V. This corresponds to the procedure OptimizeVertexPositions 
in Figure 3. Whereas the inner minimization is a continuous optimization problem, the outer minimization 
of E(K)over the simplicial com­plexes KEJ(procedure OptimizeMesh) is a discrete optimization problem. 
An algorithm for its solution is presented in Section 4.2. The energy function E(K,V)depends on two parameters 
Crep and .. The parameter Crepcontrols the tradeoff between concise­ness and .delity to the data and 
should be set by the user. The pa­rameter ., on the other hand, is a regularizing parameter that, ide­ally, 
would be chosen automatically. Our method of setting .is described in Section 4.4. 4.1 Optimization 
for Fixed Simplicial Complex (Procedure OptimizeVertexPositions) In this section, we consider the problem 
of .nding a set of vertex positions Vthat minimizes the energy function E(K,V)for a given simplicial 
complex K. As Erep(K)does not depend on V, this amounts to minimizing Edist(K,V)+Espring(K,V). To evaluate 
the distance energy Edist(K,V), it is necessary to compute the distance of each data point Xito M=qv(IKI). 
Each of these distances is itself the solution to the minimization problem d2(Xi,qv(IKI))=minlXi0qv(bi)l2, 
biElKl in which the unknown is the barycentric coordinate vector biE IKIcRmof the projection of Xionto 
M. Thus, minimizing E(K,V)for .xed Kis equivalent to minimizing the new objective function n E(K,V,B) 
= lXi0qv(bi)l 2 +Espring(K,V) i=I n = lXi0qv(bi)l 2 + lVJ0Vkl 2 i=I {J,k}EK R3 over the vertex positions 
V={VI,...,Vm},ViEand the Rm barycentric coordinates B={bI,...,bn},biEIKIc. To solve this optimization 
problem (procedure OptimizeVertex-Positions), our method alternates between two subproblems: 1. For .xed 
vertex positions V, .nd optimal barycentric coordi­nate vectors Bby projection (procedure ProjectPoints). 
 2. For .xed barycentric coordinate vectors B, .nd optimal vertex positions Vby solving a linear least 
squares problem (proce­dure ImproveVertexPositions).  Because we .nd optimal solutions to both of these 
subproblems, E(K,V,B)can never increase, and since it is bounded from below, it must converge. In principle, 
one could iterate until some formal convergence criterion is met. Instead, as is common, we perform a 
.xed number of iterations. As an example, Figure 7e shows the result of optimizing the mesh of Figure 
7c over the vertex positions while holding the simplicial complex .xed. It is conceivable that procedure 
OptimizeVertexPositions returns a set Vof vertices for which the mesh is self-intersecting, i.e. qvis 
not an embedding. While it is possible to check a posteriori whether qvis an embedding, constraining 
the optimization to always pro­duce an embedding appears to be dif.cult. This has not presented a problem 
in the examples we have run. 4.1.1 Projection Subproblem (Procedure ProjectPoints) The problem of optimizing 
E(K,V,B)over the barycentric coor­dinate vectors B={bI,...,bn}, while holding the vertex posi­tions V={VI,...,Vm}and 
the simplicial complex Kconstant, decomposes into nseparate optimization problems: bi=argminlXi0qv(b)l 
bElKl In other words, biis the barycentric coordinate vector correspond­ing to the point pEqv(IKI)closest 
to Xi. A naive approach to computing biis to project Xionto all of the faces of M, and then .nd the projection 
with minimal distance. To speed up the projection, we .rst enter the faces of the mesh into a spatial 
partitioning data structure (similar to the one used in [16]). Then for each point Xionly a nearby subset 
of the faces needs to be considered, and the projection step takes expected time O(n). For additionalspeedupweexploitcoherencebetweeniterations. 
Instead of projecting each point globally onto the mesh, we assume that a point s projection lies in 
a neighborhood of its projection in the pre­vious iteration. Speci.cally, we project the point onto all 
faces that share a vertex with the previous face. Although this is a heuristic that can fail, it has 
performed well in practice.  4.1.2 Linear Least Squares Subproblem (Procedure ImproveVertexPositions) 
Minimizing E(K,V,B)over the vertex positions Vwhile holding Band K.xed is a linear least squares problem. 
It decomposes into three independent subproblems, one for each of the three coordi­nates of the vertex 
positions. We will write down the problem for the .rst coordinate. Let ebe the number of edges (1-simplices) 
in K; note that eis O(m). Let VIbe the m-vector whose i-th element is the .rst coor­dinate of Vi. Let 
dIbe the (n+e)-vector whose .rst nelements are the .rst coordinates of the data points Xi, and whose 
last eelements are zero. With these de.nitions we can express the least squares dI2 problem for the 
.rst coordinate as minimizing lAVI0lover VI. The design matrix Ais an (n+e)2mmatrix of scalars. The .rst 
nrows of Aare the barycentric coordinate vectors bi. Each of  the trailing erows contains 2 non-zero 
entries with valuesand  0in the columns corresponding to the indices of the edge s end­points. The .rst 
nrows of the least squares problem correspond to Edist(K,V), while the last erows correspond to Espring(K,V). 
An important feature of the matrix Ais that it contains at most 3 non-zero entries in each row, for a 
total of O(n+m)non-zero en­tries. To solve the least squares problem, we use the conjugate gradient method 
(cf. [3]). This is an iterative method guaranteed to .nd the exact solution in as many iterations as 
there are distinct singular val­ues of A, i.e. in at most miterations. Usually far fewer iterations are 
required to get a result with acceptable precision. For exam­ple, we .nd that for mas large as I04, as 
few as 200 iterations are suf.cient. The two time-consuming operations in each iteration of the con­jugate 
gradient algorithm are the multiplication of Aby an (n+e)­vector and the multiplication of ATby an m-vector. 
Because Ais sparse, these two operations can be executed in O(n+m)time. We store Ain a sparse form that 
requires only O(n+m)space. Thus, an acceptable solution to the least squares problem is obtained in O(n+m)time. 
In contrast, a typical noniterative method for solv­ing dense least squares problems, such as QR decomposition, 
would require O((n+m)m 2)time to .nd an exact solution.  4.2 Optimization over Simplicial Complexes 
(Procedure OptimizeMesh) To solve the outer minimization problem, minimizing E(K)over K, we de.ne a set 
of three elementary transformations, edge col­lapse, edge split, and edge swap, taking a simplicial complex 
Kto ' another simplicial complex K(see Figure 4). We de.ne a legal move to be the application of one 
of these el­ementary transformations to an edge of Kthat leaves the topolog­ical type of Kunchanged. 
The set of elementary transformations is complete in the sense that any simplicial complex in Jcan be 
obtained from Kothrough a sequence of legal moves I . Our goal then is to .nd such a sequence taking 
us from Koto a minimum of E(K). We do this using a variant of random descent: '' we randomly select a 
legal move, K'K. If E(K)<E(K), we accept the move, otherwise we try again. If a large number of trials 
fails to produce an acceptable move, we terminate the search. More elaborate selection strategies, such 
as steepest descent or simulated annealing, are possible. As we have obtained good re­sults with the 
simple strategy of random descent, we have not yet implemented the other strategies. Identifying Legal 
Moves An edge split transformation is always a legal move, as it can never change the topological type 
of K. The other two transformations, on the other hand, can cause a change of IIn fact, we prove in [6] 
that edge collapse and edge split are suf.­cient; we include edge swap to allow the optimization procedure 
to tunnel through small hills in the energy function. edge collapse edge split edge swap Figure 4: Local 
simplicial complex transformations topological type, so tests must be performed to determine if they 
are legal moves. We de.ne an edge {i,j}EKto bea boundary edge if it is a subset of only one face {i,j,k}EK, 
and a vertex {i}to be a boundary vertex if there exists a boundary edge {i,j}EK. ' An edge collapse 
transformation K'Kthat collapses the edge {i,j}EKis a legal move if and only if the following conditions 
are satis.ed (proof in [6]): For all vertices {k}adjacent to both {i}and {j}({i,k}EK and {j,k}EK), {i,j,k}is 
afaceof K.  If {i}and {j}are both boundary vertices, {i,j}is a boundary edge.  Khas more than 4 vertices 
if neither {i}nor {j}are boundary vertices, or Khas more than 3 vertices if either {i}or {j}are boundary 
vertices.  ' An edge swap transformation K'Kthat replaces the edge ' {i,j}EKwith {k,I}EKis a legal 
move if and only if {k,I} E K. 4.3 Exploiting Locality The idealized algorithm described so far is too 
inef.cient to be of practical use. In this section, we describe some heuristics which dramatically reduce 
the running time. These heuristics capitalize on the fact that a local change in the structure of the 
mesh leaves the optimal positions of distant vertices essentially unchanged. 4.3.1 Heuristics for Evaluating 
the Effect of Legal Moves Our strategy for selecting legal moves requires evaluation of '' ' E(K)=minvE(K,V)for 
a simplicial complex Kobtained from Kthrough a legal move. Ideally, we would use procedure Op­timizeVertexPositions 
of Section 4.1 for this purpose, as indicated in Figure 3. In practice, however, this is too slow. Instead, 
we use fast local heuristics to estimate the effect of a legal move on the energy function. Each of the 
heuristics is based on extracting a submesh in the neighborhood of the transformation, along with the 
subset of the data points projecting onto the submesh. The change in overall en­ergy is estimated by 
only considering the contribution of the sub­mesh and the corresponding point set. This estimate is always 
pes­simistic, as full optimization would only further reduce the energy.  s star{s,K} t star{t,K} Figure 
5: Neighborhood subsets of K. i  j i Figure 6: Two local optimizations to evaluate edge swap Therefore, 
the heuristics never suggest changes that will increase the true energy of the mesh. De.nition of neighborhoods 
in a simplicial complex To refer to neighborhoods in a simplicial complex, we need to introduce some 
'' further notation. We write s:sto denote that simplex sis a non­ ' empty subset of simplex s. For 
simplex sEK, star(s;K)={sE ' K:s:s}(Figure 5). Evaluation of Edge Collapse To evaluate a transformation 
K' ' Kcollapsing an edge {i,j}into a single vertex {h}(Figure 4), we take the submesh to be star ({i};K)Ustar({j};K), 
and optimize over the single vertex position Vhwhile holding all other vertex positions constant. Because 
we perform only a small number of iterations (for rea­sons of ef.ciency), the initial choice of Vhgreatly 
in.uences the accuracy of the result. Therefore, we attempt three optimizations, with Vhstarting at Vi, 
VJ, and I(Vi+VJ), and accept the best one. 2 The edge collapse should be allowed only if the new mesh 
does not intersect itself. Checking for this would be costly; instead we settle for a less expensive 
heuristic check. If, after the local opti­ ' mization, themaximumdihedralangleoftheedgesinstar ({h};K) 
is greater than some threshold, the edge collapse is rejected. Evaluation of Edge Split The procedure 
is the same as for edge collapse, except that the submesh is de.ned to be star ({i,j};K), and the initial 
value of the new vertex Vhis chosen to be I(Vi+VJ). 2 Evaluation of Edge Swap To evaluate an edge swap 
transfor­ ' mation K'Kthat replaces an edge {i,j}EKwith ' {k,I}EK, we consider two local optimizations, 
one with sub­ ' mesh star({k};K), varying vertex Vk, and one with submesh ' star({I};K), varying vertex 
VI(Figure 6). The change in energy is taken to best of these. As is the case in evaluating an edge collapse, 
we reject the transformation if the maximum dihedral angle after the local optimization exceeds a threshold. 
 4.3.2 Legal Move Selection Strategy (Procedure GenerateLegalMove) The simple strategy for selecting 
legal moves described in Sec­tion 4.2 can be improved by exploiting locality. Instead of selecting edgescompletely 
at random, edgesare selected from a candidate set. This candidate set consists of all edges that may 
lead to bene.cial moves, and initially contains all edges. To generate a legal move, we randomly remove 
an edge from the candidate set. We .rst consider collapsing the edge, accepting the move if it is legal 
and reduces the total energy. If the edge col­lapse is not accepted, we then consider edge swap and edge 
split in that order. If one of the transformations is accepted, we update the candidate set by adding 
all neighboring edges. The candidate set becomes very useful toward the end of optimization, when the 
fraction of bene.cial moves diminishes.  4.4 Setting of the Spring Constant We view the spring energy 
Espringas a regularizing term that helps guide the optimization process to a good minimum. The spring 
con­stant determines the contribution of this term to the total energy. We have obtained good results 
by making successive calls to proce­dure OptimizeMesh, each with a different value of , according to 
a schedule that gradually decreases . As an example, to obtain the .nal mesh in Figure 7h starting from 
02 03 04 the mesh in Figure 7c, we successively set to I0,I0,I0, and I008(see Figures 7f 7h). This same 
schedule was used in all the examples.  5 Results 5.1 Surface Reconstruction From the set of points 
shown in Figure 7b, phase one of our re­construction algorithm [5] produces the mesh shown in Figure 
7c; this mesh has the correct topological type, but it is rather dense, is far away from the data, and 
lacks the sharp features of the origi­nal model (Figure 7a). Using this mesh as a starting point, mesh 
optimization produces the mesh in Figure 7h. Figures 7i 7k,7m 7o show two examples of surface reconstruc­tion 
from actual laser range data (courtesy of Technical Arts, Red­mond, WA). Figures 7j and 7n show sets 
of points obtained by sam­pling two physical objects (a distributor cap and a golf club head) with a 
laser range .nder. The outputs of phase one are shown in Fig­ures 7k and 7o. The holes present in the 
surface of Figure 7k are ar­tifacts of the data, as self-shadowing prevented some regions of the surface 
from being scanned. Adaptive selection of scanning paths preventing such shadowing is an interesting 
area of future research. In this case, we manually .lled the holes, leaving a single bound­ary at the 
bottom. Figures 7l and 7p show the optimized meshes obtained with our algorithm.  5.2 Mesh Simpli.cation 
For mesh simpli.cation, we .rst sample a set of points randomly from the original mesh using uniform 
random sampling over area. Next, we add the vertices of the mesh to this point set. Finally, to more 
faithfully preserve the boundaries of the mesh, we sample additional points from boundary edges. As an 
example of mesh simpli.cation, we start with the mesh containing 2032 vertices shown in Figure 7q. From 
it, we obtain a sample of 6752 points shown in Figure 7r (4000 random points, 2032 vertex points, and 
720 boundary points). Mesh optimization, with Crep =I005, reduces the mesh down to 487 vertices (Fig- 
 Fig. #vert. #faces #data Parameters Resulting energies time (min.) m n Crep r Edist E 7c 7e 7f 7g 7h 
15721572508270163 315231521024548334 4102 4102410241024102 -1005100510051005 -100210021003varied 8.5721002 
8.04210046.84210046.08210044.8621004 -4.8421002 3.62210026.94210032.1221003 -1.5 (+3.0) (+2.2) 17.0 7k 
9220 18272 12745 - - 6.4121002 - - 7l 690 1348 12745 1005 varied 4.2321003 1.1821002 47.0 7o 4059 8073 
16864 - - 2.2021002 - - 7p 262 515 16864 1005 varied 2.1921003 4.9521003 44.5 7q 2032 3832 - - - - - 
- 7s 487 916 6752 1005 varied 1.8621003 8.0521003 9.9 7t 239 432 6752 1004 varied 9.1921003 4.3921002 
10.2 Table 1: Performance statistics for meshes shown in Figure 7. ure 7s). By setting Crep =I004, 
we obtain a coarser mesh of 239 vertices (Figure 7t). As these examples illustrate, basing mesh simpli.cation 
on a measure of distance between the simpli.ed mesh and the original has a number of bene.ts: Vertices 
are dense in regions of high Gaussian curvature, whereas a few large faces span the .at regions.  Long 
edges are aligned in directions of low curvature, and the aspect ratios of the triangles adjust to local 
curvature.  Edges and vertices of the simpli.ed mesh are placed near sharp features of the original 
mesh.  5.3 Segmentation Mesh optimization enables us to detect sharp features in the under­lying surface. 
Using a simple thresholding method, the optimized mesh can be segmented into smooth components. To this 
end, we build a graph in which the nodes are the faces of mesh. Two nodes of this graph are connected 
if the two corresponding faces are ad­jacent and their dihedral angle is smaller than a given threshold. 
The connectedcomponents of this graph identify the desired smooth segments. As an example, Figure 7i 
shows the segmentation of the optimized mesh into 11 components. After segmentation, vertex normals can 
be estimated from neighboring faces within each com­ponent, and a smoothly shaded surface can be created 
(Figure 7m). 5.4 Parameter Settings and Performance Statistics Table 1 lists the speci.c parameter values 
of Crepand used to generate the meshes in the examples, along with other performance statistics. In all 
these examples, the table entry varied refers to 02 03 04 08 a spring constant schedule of {I0,I0,I0,I0}. 
In fact, all meshes in Figure 1 are also created using the same parameters (except that Crepwas changed 
in two cases). Execution times were obtained on a DEC uniprocessor Alpha workstation.  6 Related Work 
Surface Fitting There is a large body of literature on .tting em­beddings of a rectangular domain; see 
Bolle and Vemuri [1] for a review. Schudy and Ballard [11, 12] .t embeddings of a sphere to point data. 
Goshtasby [4] works with embeddings of cylinders and tori. Sclaroff and Pentland [13] consider embeddings 
of a deformed superquadric. Miller et al. [9] approximate an isosurface of volume data by .tting a mesh 
homeomorphic to a sphere. While it appears that their method could be extended to .nding isosurfaces 
of arbi­trary topological type, it it less obvious how it could be modi.ed to handle point instead of 
volume data. Mallet [7] discusses interpola­tion of functions over simplicial complexes of arbitrary 
topological type. Our method allows .tting of a parametric surface of arbitrary topological type to 
a set of three-dimensional points. In [2], we sketched an algorithm for .tting a mesh of .xed vertex 
connectivity to the data. The algorithm presented here is an extension of this idea in which we also 
allow the number of vertices and their connectivity to vary. To the best of our knowledge, this has not 
been done before. Mesh Simpli.cation Two notable papers discussing the mesh simpli.cation problem are 
Schroeder et al. [10] and Turk [15]. The motivation of Schroeder et al. is to simplify meshes gener­ated 
by marching cubes that may consist of more than a million triangles. In their iterative approach, the 
basic operation is removal of a vertex and re-triangulation of the hole thus created. The crite­rion 
for vertex removal in the simplest case (interior vertex not on edge or corner) is the distance from 
the vertex to the plane approx­imating its surrounding vertices. It is worthwhile noting that this criterion 
only considers deviation of the new mesh from the mesh created in the previous iteration; deviation from 
the original mesh does not .gure in the strategy. Turk s goal is to reduce the amount of detail in a 
mesh while re­maining faithful to the original topology and geometry. His basic idea is to distribute 
points on the existing mesh that are to become the new vertices. He then creates a triangulation containing 
both old and new vertices, and .nally removes the old vertices. The density of the new vertices is chosen 
to be higher in areas of high curvature. The principal advantage of our mesh simpli.cation method com­pared 
to the techniques mentioned above is that we cast mesh sim­pli.cation as an optimization problem: we 
.nd a new mesh of lower complexity that is as close as possible to the original mesh. This is recognized 
as a desirable property by Turk (Section 8, p. 63): An­other topic is .nding measures of how closely 
matched a given re­tiling is to the original model. Can such a quality measure be used to guide the re-tiling 
process? . Optimization automatically retains more vertices in areas of high curvature, and leads to 
faces that are elongated along directions of low curvature, another property rec­ognized as desirable 
by Turk.  7 Summary and Future Work We have described an energy minimization approach to solving the 
mesh optimization problem. The energy function we use consists of three terms: a distance energy that 
measures the closeness of .t, a representation energy that penalizes meshes with a large number of vertices, 
and a regularizing term that conceptually places springs of rest length zero on the edges of the mesh. 
Our minimization algo­rithm partitions the problem into two nested subproblems: an inner continuous minimization 
and an outer discrete minimization. The search space consists of all meshes homeomorphic to the starting 
mesh. Mesh optimization has proven effective as the second phase of our method for surface reconstruction 
from unorganized points, as discussed in [5]. (Phase two is responsible for improving the geo­metric 
.t and reducing the number of vertices of the mesh produced in phase one.) Our method has also performed 
well for mesh simpli.cation, that is, the reduction of the number of vertices in a dense triangular mesh. 
It produces meshes whose edges align themselves along directions of low curvature, and whose vertices 
concentrate in areas of high Gaussian curvature. Because the energy does not penalize surfaces with sharp 
dihedral angles, the method can recover sharp edges and corners. A number of areas of future research 
still remain, including: Investigate the use of more sophisticated optimization meth­ods, such as simulated 
annealing for discrete optimization and quadratic methods for non-linear least squares optimization, 
in order to avoid undesirable local minima in the energy and to accelerate convergence.  Gain more insight 
into the use of the spring energy as a regu­larizing term, especially in the presence of appreciable 
noise.  Improve the speed of the algorithm and investigate implemen­tations on parallel architectures. 
 Develop methods for .tting higher order splines to more accu­rately and concisely model curved surfaces. 
 Experiment with sparse, non-uniform, and noisy data.  Extend the current algorithm to other distance 
measures such as maximum error (..norm) or average error (.Inorm), in­stead of the current .2norm.  
 References [1] Ruud M. Bolle and Baba C. Vemuri. On three-dimensional surface reconstruction methods. 
IEEE PAMI, 13(1):1 13, January 1991. [2] T. DeRose, H. Hoppe, T. Duchamp, J. McDonald, and W. Stuetzle. 
Fitting of surfaces to scattered data. SPIE, 1830:212 220, 1992. [3] Gene Golub and Charles Van Loan. 
Matrix Computations. John Hop­kins University Press, 2nd edition, 1989. [4] Ardeshir Goshtasby. Surface 
reconstruction from scattered measure­ments. SPIE, 1830:247 256, 1992. [5] H. Hoppe, T. DeRose, T. Duchamp, 
J. McDonald, and W. Stuetzle. Surface reconstruction from unorganized points. Computer Graphics (SIGGRAPH 
92 Proceedings), 26(2):71 78, July 1992. [6] H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle. 
Mesh optimization. TR 93-01-01, Dept. of Computer Science and En­gineering, University of Washington, 
January 1993. [7] J.L. Mallet. Discrete smooth interpolation in geometric modeling. CAD, 24(4):178 191, 
April 1992. [8] Samuel Marin and Philip Smith. Parametric approximation of data us­ing ODR splines. GMR 
7057, General Motors Research Laboratories, May 1990. [9] J.V. Miller, D.E. Breen, W.E. Lorensen, R.M. 
O Bara, and M.J. Wozny. Geometrically deformed models: A method for extracting closed ge­ometric models 
from volume data. Computer Graphics (SIGGRAPH 91 Proceedings), 25(4):217 226, July 1991. [10] William 
Schroeder, Jonathan Zarge, and William Lorensen. Decima­tion of triangle meshes. Computer Graphics (SIGGRAPH 
92 Pro­ceedings), 26(2):65 70, July 1992. [11] R. B. Schudy and D. H. Ballard. Model detection of cardiac 
cham­bers in ultrasound images. Technical Report 12, Computer Science Department, University of Rochester, 
1978. [12] R. B. Schudy and D. H. Ballard. Towards an anatomical model of heart motionasseenin4-dcardiacultrasounddata. 
In Proceedingsof the 6th Conference on Computer Applications in Radiology and Computer-Aided Analysis 
of Radiological Images, 1979. [13] Stan Sclaroff and Alex Pentland. Generalized implicit functions for 
computer graphics. Computer Graphics (SIGGRAPH 91 Proceed­ings), 25(4):247 250, July 1991. [14] E. H. 
Spanier. Algebraic Topology. McGraw-Hill, New York, 1966. [15] Greg Turk. Re-tiling polygonal surfaces. 
Computer Graphics (SIG-GRAPH 92 Proceedings), 26(2):55 64, July 1992. [16] G. Wyvill, C. McPheeters, 
and B. Wyvill. Data structures for soft objects. The Visual Computer, 2(4):227 234, August 1986. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166120</article_id>
		<sort_key>27</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Interactive texture mapping]]></title>
		<page_from>27</page_from>
		<page_to>34</page_to>
		<doi_number>10.1145/166117.166120</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166120</url>
		<keywords>
			<kw><![CDATA[interaction]]></kw>
			<kw><![CDATA[realistic rendering]]></kw>
			<kw><![CDATA[texture map distortion]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P119285</person_id>
				<author_profile_id><![CDATA[81100269040]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J&#233;r&#244;me]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maillot]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P113078</person_id>
				<author_profile_id><![CDATA[81100197908]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hussein]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yahia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31098486</person_id>
				<author_profile_id><![CDATA[81100126125]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Anne]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Verroust]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>122744</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[C. Bennis, J.M. Vezien, G. Iglesias, Piecewise flattening for non-distorted texture mapping. SIGGRAPH 91, Proc. of Computer Graphics, 25(4):237-246. July 1991.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[E.Bier, K.Sloan, Two-part texture mapping. IEEE Computer Graphics and applications, 40-53. September 1986.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360353</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[J.F. Blinn and M. E. Newell, Texture and Reflection in Computer Generated Images. Communications of the ACM, 19(10):542-547. October 1976.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325249</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[J. Bloomenthal, Modeling the mighty maple. SIGGRAPH 85, Proc. of Computer Graphics, 19(3):305-311. July 1985.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907242</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[E. Catmull, A subdivision algorithm for computer display of curved surfaces. Phd dissertation, University of Utah. December 1974.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[P. Ciarlet, Mathematical Elasticity, Vol. I, 3-Dimensionnal Elasticity. North Holland, 1988]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[R.M. Christensen, Mechanics of Composite Materials. McGraw- Hill, 1967]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808600</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[F. Crow, Summed-area tables for texture mapping. SIGGRAPH 84, Proc. of Computer Graphics, 18(3):207-212. July 1984.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[M.P. Do Carmo, Differential Geometry of curves and surfaces. Prentice-Hall, Inc. 1976.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[E. Diday, J.C. Simon, Clustering Analysis. Communication and Cybernetics, 10, Digital pattern recognition, 47-94. 1976.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[J.D. Foley, A. van Dam, S.K. Feiner and J.F. Hughes, Computer Graphics, Principles and Practice, 2nd edition. Addison-Wesley, 1990]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97903</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[P. Hanrahan, P. Haeberly, Direct WYSIWYGpainting and texturing on 3D shapes. SIGGRAPH 90, Proc. of Computer Graphics, 24(4):215-223. August 1990.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>13027</ref_obj_id>
				<ref_obj_pid>13021</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[P. S. Heekbert, Survey of Texture Mapping. IEEE Computer Graphics and Applications 6(11):56-67. November 1986.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[M. Hirseh, Differential topology. Graduate texts in mathematics 33, Springer-Verlag. 1976.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[J. Maillot, Trois approches du plaquage de texture sur un objet tridimensionnel. These de doctorat en sciences, Universite de Paris- Sud, Centre d'Orsay, may 1992.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[S.D. Ma, H. Lin, Optimal texture mapping. EUROGRAPHICS 88, 421-428. September 1988.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[J. E. Marsden, T. J. R. Hughes, Mathematical Fundations of Elasticity. Prentice Hall, 1983]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[W.H. Press, B. P. Flannery, S. A. Teukolsky and W. T. Vetterling, Numerical Recipes. Cambridge University Press, Cambridge, 1986]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[P. Sander, S. Zueker, Inferring surface trace and differential structure from 3-D images. Rapport de recherche No 1117, INRIA. 1989.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Graphic library, programming guide. Silicon Graphics manual, IRIS 4D VGX series.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[M. Spivak, A comprehensive introduction to differential geometry, Vol I. Publish or perish, Inc., Berkeley. 1979.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Texture Mapping ttzzz Jer^ome Maillot, Hussein Yahia, Anne VerroustThomson Digital ImageINRIA-Rocquencourt 
Abstract This paper describes a new approach to texture mapping. A global method to lower the distortion 
of the mapped image is presented; by considering a general optimization function we view the map­ping 
as an energy-minimization process. We have constructed an interactive texture tool, which is fast and 
easy to use, to manip­ulate atlases in texture space. We present the tool s large set of interactive 
operations on mapping functions. We also introduce an algorithm which automatically generates an atlas 
for any type of object. These techniques allow the mapping of different tex­tures onto the same object 
and handle non-continuous mapping functions, needed for complicated mapped objects. CR Categories and 
subject descriptors: I.3.3 [Computer Graphics] Pic­ture/Image Generation. I.3.7 [Computer Graphics] Graphics 
and Realism -Color, Shading and Texture. Additional Keywords: Texture Mapping, Texture Map Distortion, 
Real­istic Rendering, Interaction. 1 Introduction. Texture mapping is a method in Computer Graphics to 
enhance the richness of computer-generated images [3, 13]. A texture is a 2D image to be mapped onto 
a synthetic 3D object. The 2D space of the texture image is often called texture space. Each point on 
the object has to be associated with an element in texture space. One of the first algorithms used the 
parametric representation of patches to find texture addresses [3]. With this method, some problems may 
occur at the junction of two patches [4]. Another method is to project the texture onto the object using 
an intermediate 3D shape like a box or a cylinder [2]. Also, some applications have been developed to 
provide direct drawing onto the object, in which the user interactively modifies the texture via the 
mapping function [12]. m Thomson Digital Image, 20-22, rue Hegesippe Moreau, 75018 Paris, France. z INRIA-Rocquencourt 
B.P. 105, 78153 Le Chesnay, France. Permission to copy without fee all or part of this material is granted 
provided that the copies are not made or distributed for direct provided that the copies are not made 
or distributed for direct commercial advantage, the ACM copyright notice and the title of the commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by publication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinery. To copy permission of the Association for Computing Machinery. 
To copy otherwise, or to republish, requires a fee and/or specific permission. otherwise, or to republish, 
requires a fee and/or specific permission. &#38;#169;1993 ACM-0-89791-601-8/93/008 $1.50 &#38;#169;1993 
ACM-0-89791-601-8/93/008/0015 $1.50 In addition, the rendering of an object mapped with a pre­existing 
image, such as a digitized one, has been improved using dedicated techniques [3, 8, 2, 1]. We are interested 
in the case of 2D textures. In general, there is no natural mapping from the texture to the object: the 
image is necessarily distorted (see [13]). Also, there is a need for interactive tools to help the user 
define how to map a pre-defined image onto a surface, or to improve a mapping function. We present new 
methods for solving these problems. We first propose, in section two, a mathematical formulation for 
the distortion of the mapped image. After deriving a general formula for the deformation energy, we describe 
simplified for­mulae, fast enough to be used inside an interactive loop to improve the mapping function. 
Section three is devoted to the notion of an atlas, derived from a mathematical notion and adapted to 
the special case of textures. In the first subsection curvature is used to build an ultrametric for automatically 
creating atlases on any object. In the second subsection a set of interactive functions is presented 
for manipulating atlases. We address only the case of polygonized surfaces. This sim­plifies the definition 
of the mapping function to one that depends only on the position in the texture plane of the points associated 
to all vertices. Our texture mapping tool is designed for a naive user who is not necessarily a specialist 
in image synthesis. The interface is very intuitive for the basic functions, and also provides some control 
structures which free the user from repetitive tasks. This program could be used, for example, by fashion 
designers to map woven or leather textures onto polygonal surfaces describing shoes, clothes or seats. 
 2 Deformation measures. The first problem one has to solve when mapping textures is to define the quality 
of the final rendered object. We propose to mea­sure the distortion introduced by the mapping as the 
deformation energy . In the first subsection, we derive a general formula for EEE . We then propose, 
in the following subsections, a simplified formula for , whose minimization can be done in real-time, 
and which gives very good visual results. 1 2.1 General formula. Suppose that the surface on which we 
want to map textures is defined by a parametric function: being an open set of IR2 and Euclidean ordinary 
3-space: to  U umv UddUE E d umv each pointof , associates a pointon the surface; thus defines a trivial 
mapping function onto the surface (for example, in [5], the surface is a bicubic patch, and the mapping 
function is exactly given by the parameterization). It should be noticed that any regular surface can 
be defined in this way, at least locally. If the texture is to be mapped onto an elastic surface, one 
can measure the deformation of the texture through by computing the elastic deformation of the planar 
section when one applies to it. We use the first fundamental form [9, chap 2.5, 4.2] d r d um : v 2 UI 
o ddd to measure at each pointthe differences of lengths and angles between the initial plane and the 
tangent plane of the surface. Denoting by the Jacobian Matrix of , we let t (1) I o I o umv r d ar dd 
In particular, is the identity matrix of IR2 if and only if is an (infinitesimal) isometry, which means 
that the mapping function does not distort the image. Let Id be the identity matrix of IR 2, and be any 
norm defined on the set of 22 matrices. We can take as a measure of deformation energy at a point jjjj 
jj I o umv jj umv of parameter space the quantity Id2, known by mechanical engineers as the Green-Lagrange 
deformation tensor. The deformation energy can then be defined over the whole underlying set since 2d2d 
 E UE UZ Z UZ U.. Z du U jj I o t E jj ..duu a v.. dvZ tZ U .. jj dev jj u vuv Id dd(2) This equation 
can be written as (see Appendix A): 2d 12221d(3) If the surface is defined locally by non-overlapping 
regions, then the total energy is obtained by summing all the energies of each region : . U i E P i E 
U i 2.2 Interpretation in the Linear Theory of Elasticity. We may imagine that our surface is made of 
rubber, and that we want to deform it in such a way that it can be equated with the texture image. If 
the material is isotropic, the elastic energy depends only on two parametersand(see [17, 6]). Writing 
trfor the trace of matrix (the sum of its diagonal terms), then: 2  e E t 2 a ZZ e U 2 a a me m t 2e 
vE tr2tr(4) assuming that0 and0, which express that is positive definite. Since only the ratio is significant 
for comparing mapping distortions, we can take1. The coefficientcharacterizes how the material is deformed 
orthogonally to the direction of a tensile stress. All existing materials have a positive , referring 
the fact that any object shrinks in direction when 1 1  5. 5. Fig. 1: Two squares of elastic material 
with different values of were stretched in direction.  u one stretches it along . With0 the deformation 
in and u u v are independent. Figure 1 shows the influence of the sign ofon the shape of a deformed object. 
Equation 3 correspond to the case 0,1. For texture mapping, except in very special cases, the best results 
are obtained with by setting0. It is important to note that two symmetrical surfaces have the same deformation 
energy. This implies that it is not possible to determine whether the mapping function inverts the image 
or not. We will show in section 2.4 that this may lead to problems. 2.3 Triangulated surfaces. Fig. 2: 
Locally, the mapping function is an affine application  mmmA which associates triangle 123 with triangle 
123. It is the inverse function of . The deformation of the mapped image can be computed by evaluating 
the deformation of each triangle. As a result, the parametric function is affine (see figure 2), and 
its gradient is the associated linear map. Write and , 123, for dm i dm i M i m i i 2f mm g the vertices 
in IR3 and their associated positions in texture plane. Thenis defined by nine numbers. If the previous 
formula is expanded, the total energy can be expressed as the sum of rational fractions of , with numerator 
of degree 8 and denominator of degree 6. To find the best mapping function, one has to find the coordinates 
of all s that minimize this energy. The solution can only be computed numerically, using an optimization 
method. The efficient algorithms need to know the gradient of the energy. Even in the simplest case of 
triangulated surfaces, the expression is complex and long to process. One can remark that most of the 
existing finite element programs are built to treat linear cc elasticity. They are highly optimized, 
but cannot be used to solve Although in linear elasticity theory, the displacement of any point is supposed 
to remain small compared to the object, this is not the case in a rotation, for example. Such an approximation 
gives very bad results when applied to texture mapping.  texture mapping problems. Thus, to be fast 
enough for interactive applications, the optimization process requires a simplified form of the energy 
equation that we present below. In [1] a flattening algorithm is proposed for parametric patches. It 
is based on a relaxation procedure and runs incrementally. We want to find a global minimum for any type 
of polyhedral surface, using energy-minimization techniques. The method to be presented here will tackle 
the problem from a different point of view. 2.4 A simple, distance based energy. If the surface is triangulated, 
its first fundamental form is com­pletely characterized by the length of its edges. Furthermore, the 
lengths measured in IR3 and in the texture plane are all the same if and only if the mapping function 
is an isometry. This is a consequence of the fact that two triangles are isometric as soon as their three 
edges have same lengths. Let s introduce the length energy . The simplest form of energy that preserves 
length is the following: 222 EE ll e iej X e 2 a jj m i m jj j M jj i jj MM j jj i M j jj m 2 (5) Edges 
Using the squared norm gives us a simple form for the gradient. E l represents the energy of a spring 
net initially lying on the surface, and for which each spring induces a force proportional to the square 
of the distance (instead of the distance, as for classical springs). This give a higher energy for the 
most elongated springs than is given by the classical spring response, and thus increases the mean elongation, 
but lowers the maximum elongation. The final state is not very different from what would be obtained 
with standard springs, but using this formula we obtain a faster optimization algorithm. Normalization 
(that is, dividing by the term 2) is chosen so that the energy does not change jj M i M j jj when the 
surface is subdivided by splitting each triangle into four similar parts. Without such a normalization, 
an object with very different face sizes would not be processed correctly. This would be the case, for 
example, if the surface is constructed from hierarchical splines. Taking the symmetry of formula 5 into 
account, the part of the energy depending on point is: 222 E lm k X m a i jj im i m jj k M jj i jj MM 
k jj i M k jj m 2 2 (6) adjacent to The energy gradient is a degree three polynomial: 22 8 2() 22 E 
xy iill X kk aa jj m i m jjjj MM k jj ii MM jj M kk jjjj i M k jj mm yx ii yx kk 8 2() This form of 
energy is easy to compute and gives good results as long as the surface is simple. But when the surface 
is difficult to map (when the total curvature is too large), some triangles reverse their orientations. 
One can notice this effect in figure 3. In figure 3a we show two objects: object 1 on the left and object 
2 on the right. Object 2 is just object 1 with a little band added near the equator. In figure 3b we 
see the results of minimization, Fig. 3.a: Object 1 and object 2 T exture T exture Fig. 3.b: The two 
maps. Fig. 3.c: Rendered objects. Fig. 3: Problems with length based energy. orthogonally mapped into 
texture space: the contraction becomes too high at the center and the best solution to preserve the lengths 
is to fold the map. This is a direct consequence of the fact that the energies of two symmetrical triangles 
are the same. The same problem occurs with linear springs. If one compresses a spring along its axis, 
it bends and its projection onto its axis may overlap. This phenomenon is named buckling (see [7]) in 
elasticity. The final result is chaotic, in the sense that compressing two almost identical springs may 
produce two very different results. The two main problems with energy measure are that, E l firstly, 
the final state of the map becomes unstable when the object is complex. We can see for example in figure 
3 that the symmetry along the and axes is broken in the map. The XY result may depend strongly on small 
numerical errors. Secondly, the rendering is poor when some triangles have reversed their orientations. 
The patterns are multiplied, and grouped by triples, with opposite orientations. Such a final state is 
not acceptable. 2.5 A surface and length based energy. To solve the problem of overlapping regions in 
the texture map, a second term can be added in the energy formula. It is chosen so that wrongly-oriented 
triangles will have a high energy. Energy  E s E l is defined using the difference of signed areas 
for each triangle. It can be computed with cross products in IR 3 and determinants in the texture plane. 
The final energy is a linear combination of and . By default we take the arithmetic mean. 2 E s E Ms 
i M j M X k h m i m j m m  M i m i M kj e  M M i M i M k j  e M i M k  i det triangle  For this 
definition, the surface is implicitly supposed to be ori­entable, and with all triangles described in 
a direct  M i MM k E s sense, according to the normal. Again we normalize . The 1 0 2 0 1 2 Fig. 4: 
Object 2 from figure 3 optimized with 1  E E l t . E s E l dimensions of and are identical, which justifies 
the final form: r EEE s EE l t l E s x i y i 1 whereis a real coefficient to be taken between 0 and 
1. Gradient is a degree three polynomial in and which can be quickly computed. Thus, a conjugate gradient 
method can be used to find the best mapping function [18]. Case1 correspond to and is not satisfactory. 
Figure 4 shows that0 is not good either. The problem is that there are an infinite number of triangles 
with the same surface. In particular, when a vertex is translated in a parallel direction with the opposite 
edge, the surface does not change. This explains why triangles appear so stretched close to the border 
of the object. However, we have measured the surface differences surface, and our experience is that 
the differences between planar and 3-D triangles was less than 01%. A good result is obtained in figure 
4 by combining a both terms of the energy. Depending on the geometry, the optimal visual effect is obtained 
by tuningbetween 0 and 1. In many  cases,1 is satisfactory. 2 There are still some objects for which 
optimization with any value ofwould give bad results. In these cases, the problem results from the fact 
that the object is too complicated to be mapped with a single image. The solution is then to use atlases, 
as we will show in the next section. 3 Use of atlases. As we have noticed in the previous section, a 
global continuous mapping function may excessively distort the image of a complex or a highly curved 
object. The natural way to solve this problem is to split the object into several independent regions. 
The practice has been to do so implicitly using the construction of the object. For example in [16], 
a textured teapot is split into three parts whose shape comes from the patch description. To be as general 
as possible, we disconnect the texturing regions from the 3-D representation of the surface: the user 
may want to represent a surface mapped only partially (for example, an object with a logo stuck on), 
with different textures, such as a patchwork, or with local discontinuities (as on some clothes). Thus 
we introduce a data structure called an atlas which is derived from the notion of atlas used in differential 
geometry [14, 21]. In our case, an atlas is composed of a set of charts 1, where each is an  f UmaaamU 
n g U i f dmaaamd n g d i application from a subset of the surface to the Euclidean plane, such that:1is 
a cover of the surface, each is continuous inside the faces, and discontinuities are allowed along edges 
 p i 6 d i jd i d j for , and do not overlap except on the edges. Each chart is associated to its own 
image. The words atlas and chart have here slightly different meanings than the ones mathematicians give 
them, due to differences in the regularity and boundary conditions, but the main idea of atlases is kept: 
the covering of a surface. Good atlases are closely linked to the geometry of the object and can be difficult 
to build. Hence in the first subsection we present an algorithm to automatically build an atlas from 
scratch. Then we describe in the second subsection an interactive tool which makes easy the manipulation 
of atlases and of texture mappings for a given polyhedral surface. 3.1 A creation tool. To automatically 
define an atlas, it seems natural to use the curvature information: a surface is developable (isometric 
to a plane) if and only if the curvature matrix has a zero determinant [9, pp. 194-197]. In fact, a first 
rough subdivision of the surface in buckets is made using only the normal vectors of the surface and 
then the curvature information is used to control the merging of adjacent buckets and obtain the atlas. 
Then the regions are flattened on the texture plane (a more detailed description of the whole process 
can be found in [15]). The curvature information is essential in our computation. Let us describe first 
how we proceed to get this information. 3.1.1 Computing the curvature. There exist precise but costly 
algorithms to compute the curvature of a polyhedral surface [19]. Since interactive visual feedback is 
SS Fig. 5: Uniform cover of the sphere 2. one of our main goals, we designed a very fast algorithm, whose 
results are accurate enough for our needs. Let be a tangent N u vector to the surface and the Gauss map 
[21] which associates to each point of the surface the unit normal vector at that point. The unit normal 
vector is represented as a point on the unit sphereS u 2 2. Curvature in direction is defined by the 
formula [9, pp. 135-151]:  Cuu a N u We use finite differences to approximate derivatives. Normals 
must be evaluated at three close, non-aligned locations to evaluate the three coefficients of the curvature 
matrix. We use smoothed normals coming from the rendering procedure. We put normal at the center of gravity 
of the face, . We get the point , normal , and a set of vertices associated to normals . With this data 
we seek to evaluate d: we must find a symmetrical linear N L S i N P GGm f N g dc N i GNi map in the 
tangent plane such that, for all , L GS i PL N i N dN i N (7) draws nearer to . Formula 7 shows that 
the matrix of the linear map is also a curvature matrix. To eliminate the case of non-planar facets, 
all differences are projected in plane . Coefficients of curvature are then computed using a least-squares 
method. The reader is referred to Appendix B for the details. 3.1.2 Subdividing the surface. The Gauss 
map defines for each surface a partition of 2 into areas S of various densities. To efficiently define 
an atlas of connected regions related to curvature information, we introduce the buckets induced by a 
homogeneous cover of 2 as shown in figure 5: CS the buckets are the maximal connected regions of the 
surface, composed of faces which normal vectors belong to the same element of . CG B Aconnectivitygraph 
isbuiltfromthesetsofbuckets,adding an edge between two buckets when they have a common boundary (see 
figure 6). 3.1.3 Merging of buckets. To keep control of the distortion when merging two adjacent regions 
of the subdivision, we define a notion of similarity on the set of buckets. To each bucketwe compute 
the following information which will be used in defining the similarity:  p c N N : average of normal 
to faces in bucket. Three coefficients because the curvature matrix is symmetric. Fig. 6: The battered 
surface is represented by the buckets whose connectivity graph is on the right of the figure. One can 
see how this surface is simplified: only 6 average directions are kept from the initial surface. : average 
of directions of maximal curvature. Besides these geometric attributes, we use the connectivity graphdefined 
in the last subsection. Between two neighboring buckets1 and2 we introduce the similarity d12 : 12 22 
 G N p N m b R N G m N Rjj N N j 1bNN a N 2 jjj t n j bN m ma N j n d121 11 2 An ultrametric (see [10]) 
on the set of regions is then defined by: d12 mind12 d N 2R eN 2R d 1 122 Givenathresholdvalue ,webuildasegmentationoftheobjectby 
merging all region whose distances are below . The edges of the connectivity graph are sorted with respect 
to their corresponding similarity values in a preprocessing phase in which we sort the edges of graph 
according to d. The selection of the first edges on the connectivity graph will trigger successive unions 
of the adjacent regions associated with the edges. The user can then choose the number of charts in the 
atlas by selecting a number corresponding to the number of edges in . G N 3.1.4 Flattening the regions. 
Once the segmentation is computed, the last step is to flatten the regions in order to define the charts. 
We use a region growing algorithm in which new faces are added in such a way that the surface s geometry 
is not distorted too much and so that the final energy of flattened pieces not too high. We first pick 
up one face of each region, orthogonally flatten it with respect to its normal, and then depth-first 
traverse the faces graph from the initial facet. For each new vertex adjacent to two already fixed points, 
the location of in texture space is computed taking the average MA j m ki M i M ki M i M i M j M k m 
j m k of the over all the triangles such that 1. and have the corresponding points and in texture plane 
and 2. is such that the following two triangles are similar:  the triangle defined by the projection 
of and  p A jk M i M i M j M k M i M j M k orthogonally w.r.t. the face and the triangle .  p A jk 
M i mm j mm k This atlas creation algorithm is useful if the surface is complex, but it may not be entirely 
satisfying. For this reason we provide the set of interactive functions to manipulate atlases described 
below. 3.2 An interactive tool. 3.2.3 Interactive functions. As emphasized in [12], interactivity is 
important when texturing To modify an existing atlas, the user is provided with several types 3D shapes. 
3.2.1 Drawing an atlas. Left Above Texture Perspective Fig. 7: Sphere portion associated with a two-charts 
atlas. To visualize an atlas on the screen, we use two displays, one showing a wire frame projection 
of the polyhedral surface and the other showing the chart represented by the same network of polygons 
after the mapping transformation (see Figure 7). The chart is put in a special Texture view. The current 
chart and the selected points are highlighted in all the views. 3.2.2 Data structures. To compute the 
texture mapping inside a face, the positions in the texture plane of all of its vertices are needed. 
Then, the rendering is computed using an algorithm similar to color calculation for Gouraud shading [11, 
20]. Since local discontinuities are allowed along edges, a vertex may have as many 2-D positions as 
there are faces adjacent to it. Thus, an atlas depends on the location in the texture plane of the angles 
where is a vertex f j v i mf j v i belonging to face (see Figure 8.a). To avoid redundancies, angles 
are regrouped in sectors: sets of connected angles for which the mapping function is continuous (see 
Figure 8.b). The mapping function is then defined by a position in the texture plane for each sector. 
All sectors belonging to the same vertex are stored in a linked list. Pointers to the list heads are 
stored in an array associated with the 3-D vertices of the surface. Access to a sector and data structure 
modifications can be computed in almost constant time. S1 A2 A3 A1 A4 A5A6 S2 S3 Fig. 8.a: Fig. 8.b: 
sectors Fig. 8: Case of a vertex corresponding to three sectors and six angles. of interactive functions 
which operate in the texture plane. Positioning functions let the user adjust the scale, stretch, angle 
and position of the whole atlas or of each chart using linear trans­formations. Finer-grained operations 
are obtained by selecting only a group of sectors. A function to align a set of sectors in the or direction 
is also provided. This simple function is very xy useful when one wants the edges of the charts to exactly 
match the border of a texture. The constraint function marks sectors for the optimization pro­cedure. 
Sectors can be fixed in , , or along both axes. With xy this function, one can adjust sectors in the 
mapped image so that a specific pattern lies precisely on a given place of the surface, and let all the 
rest of the texture be optimized. The cut function defines discontinuities in the charts. Cuts can be 
seen as the snips of a tailor s scissors inside the piece of material that may stretch or shrink during 
the optimization procedure. The user gives a path along the edges which must not self-intersect. The 
chart is then possibly separated into several charts, or may be only internally cut. The merge function 
reconnects charts along a given path. Faces connected do not necessarily belong to different charts. 
The user can either select the connection path, or pick two charts and let the program find the common 
edges and the best displacement to attach the first chart to the second. The optimization function improves 
the charts, taking into ac­count the user specified constraints. Parameter of the energy can be adjusted 
for special cases. Note the difference between the constraint and the merge func­tion: using the constraint 
function, one may fix sectors together corresponding to the adjacent path of two charts, producing the 
same visual result in a texture space as a merge between these two charts. But a call to the optimization 
function will lead to different results: when the charts have been merged, the optimization is per­formed 
on the resulting charts and globally reduces the distortion. The sectors belonging to the common path 
may have moved during this process. p on the other hand, when the sectors of the common path have been 
fixed together, two optimization processes are performed independently, one for each chart, keeping the 
fixed sector in the same position in the texture space.   4 Applications in the field of animation. 
The tool we have described has been built principally for static objects. Nevertheless, it appears that 
special effects in animation could be obtained very easily. One example consists in moving and distorting 
some charts of the object. The result is a sliding texture onto a fixed shape. For example, one can draw 
a scrolling text onto any shape by simply translating down the chart of the object. Another example involves 
interpolated objects. In this case, it is generally difficult to obtain a deformation that mimics an 
elastic deformation. Optimizing the mapping function with the criterion described previously can replace 
the 3-d elasticity system by a 2-d optimization which is simpler and faster. To obtain a realistic deformation 
of a textured object, one has to optimize the chart of the undeformed object, then apply any geometric 
transformation to it, and optimize again constraining the edge points not to move. This constraint corresponds 
to the fact that the piece of texture used for the object does not change with time. We made a short 
test animation in which the deformation was obtained by interpolating between key-objects, and it appeared 
that the mapping function could also be interpolated, thus requiring only a few optimizations, one for 
each key. 5 Remarks on Figures. Let us add some comments on the figures appearing at the end of the 
paper: p Figure 9 shows the different effects obtained using cylindrical and spherical projections of 
a checkerboard as a texture mapping onto the Utah teapot. We also display the same teapot textured with 
our tool, thus demonstrating automatic creation of an atlas and the use of interactive functions. Here 
the atlas is composed of four charts: the spout, the handle, the cap and the body of the teapot. The 
common boundaries of the cap and the body of the teapot are fixed in the texture plane to ensure visual 
continuity between the two charts. We see in this figure that the distortion is very low. p The last 
two pictures illustrate the capabilities of atlases. Each shows geometrical objects whose atlases have 
been split into several charts (4 charts per object). 6 Conclusion. We have presented a method for measuring 
the deformation energy of the mapping of an image onto a surface. The measure proposed here is an approximation 
of the integral of the Green-Lagrange deformation tensor. It can be minimized in real time and gives 
accurate results. We have also addressed the problem of segmenting a 3D ob­ject in regions on which the 
mapping is not too distorted. We solved the problem by introducing the concept of an atlas together with 
interactive functions to edit and manipulate atlases and data structures which are efficient for these 
operations. We described a method which for any object automatically generates atlases, and we showed 
how to efficiently merge charts on an existing atlas. Efficient merging uses segmentation techniques 
based on curvature and ultrametrics. Specific data structures are proposed to handle atlases efficiently. 
Acknowledgements: We wish to thank Alain Chesnais for revising an early version of this paper, Lars W. 
Ericson for carefully proof-reading the manuscript, Francis Lazarus, Arghyro Paouri, Marie-Luce Viaud 
and Jean-Luc De Antoni for their help during modeling, and the audiovisual department of INRIA for the 
photos and videos. References [1] C. Bennis, J.M. Vezien, G. Iglesias, Piecewise flattening for non-distorted 
texture mapping. SIGGRAPH 91, Proc. of Computer Graphics, 25(4):237-246. July 1991. [2] E.Bier, K.Sloan, 
Two-part texture mapping. IEEE Computer Graphics and applications, 40-53. September 1986. [3] J. F. Blinn 
and M. E. Newell, Texture and Reflection in Computer Generated Images. Communications of the ACM, 19(10):542-547. 
October 1976. [4] J. Bloomenthal, Modeling the mighty maple. SIGGRAPH 85, Proc. of Computer Graphics, 
19(3):305-311. July 1985. [5] E. Catmull, A subdivision algorithm for computer display of curved surfaces. 
Phd dissertation, University of Utah. December 1974. [6] P. Ciarlet, Mathematical Elasticity, Vol. I, 
3-Dimensionnal Elas­ticity. North Holland, 1988 [7] R. M. Christensen, Mechanics of Composite Materials. 
McGraw-Hill, 1967 [8] F. Crow, Summed-area tables for texture mapping. SIGGRAPH 84, Proc. of Computer 
Graphics, 18(3):207-212. July 1984. [9] M.P. Do Carmo, Differential Geometry of curves and surfaces. 
Prentice-Hall, Inc. 1976. [10] E. Diday, J.C. Simon, Clustering Analysis. Communication and Cybernetics, 
10, Digital pattern recognition, 47-94. 1976. [11] J.D. Foley, A. van Dam, S.K. Feiner and J.F. Hughes, 
Computer Graphics, Principles and Practice, 2nd edition. Addison-Wesley, 1990 [12] P. Hanrahan, P. Haeberly, 
Direct WYSIWYG painting and textur­ing on 3D shapes. SIGGRAPH 90, Proc. of Computer Graphics, 24(4):215-223. 
August 1990. [13] P. S. Heckbert, Survey of Texture Mapping. IEEE Computer Graphics and Applications 
6(11):56-67. November 1986. [14] M. Hirsch, Differential topology. Graduate texts in mathematics 33, 
Springer-Verlag. 1976. [15] J. Maillot, Trois approches du plaquage de texture sur un objet tridimensionnel. 
These de doctorat en sciences, Universite de Paris-Sud, Centre d Orsay, may 1992. [16] S.D. Ma, H. Lin, 
Optimal texture mapping. EUROGRAPHICS 88, 421-428. September 1988. [17] J. E. Marsden, T. J. R. Hughes, 
Mathematical Fundations of Elasticity. Prentice Hall, 1983 [18] W. H. Press, B. P. Flannery, S. A. Teukolsky 
and W. T. Vetter­ling, Numerical Recipes. Cambridge University Press, Cambridge, 1986 [19] P. Sander, 
S. Zucker, Inferring surface trace and differential structure from 3-D images. Rapport de recherche No 
1117, INRIA. 1989. [20] Graphic library, programming guide. Silicon Graphics manual, IRIS 4D VGX series. 
[21] M. Spivak, A comprehensive introduction to differential geometry, Vol I. Publish or perish, Inc., 
Berkeley. 1979. A Derivation of equation 5. All norms on the vector space of 2 2 matrices being equivalent, 
we take the Euclidean norm. It is basis independent because an easy calculation 1 t   jj M jj = MM 
e shows that tr2 , tr being the trace i.e. the sum of diagonal coefficients. Now 22 1dd 2 1 E = Z ZZ 
Z UU u I o M  M ou o oM ) ov I o  u M oouv oo M vov u ) v  vu M v uv trIdtIddd 22 122212dd 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166121</article_id>
		<sort_key>35</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Efficient, fair interpolation using Catmull-Clark surfaces]]></title>
		<page_from>35</page_from>
		<page_to>44</page_to>
		<doi_number>10.1145/166117.166121</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166121</url>
		<keywords>
			<kw><![CDATA[B-spline surfaces]]></kw>
			<kw><![CDATA[computer-aided geometric design]]></kw>
			<kw><![CDATA[subdivision surfaces]]></kw>
			<kw><![CDATA[thin-plate splines]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.1</cat_node>
				<descriptor>Computations on matrices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.1</cat_node>
				<descriptor>Spline and piecewise polynomial interpolation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003648.10003649.10003657.10003659</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic representations->Nonparametric representations->Spline models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003719</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on matrices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P190354</person_id>
				<author_profile_id><![CDATA[81100290289]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Halstead]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39078647</person_id>
				<author_profile_id><![CDATA[81100215003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kass]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39045514</person_id>
				<author_profile_id><![CDATA[81100493833]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[DeRose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>42459</ref_obj_id>
				<ref_obj_pid>42458</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[A. A. Ball and J. T. Storry. Conditions for tangent plane continuity over recursively defined B-spline surfaces. A CM Transactions on Graphics, 7(2):83-102, April 1988.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[E. Catmull and J. Clark. Recursively generated B-spline surfaces on arbitrary topological meshes. Computer Aided Design, 10(6):350-355, 1978.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122746</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[George Celniker and Dave Gossard. Deformable curve and surface finite elements for free-form shape design. In Proceedings of SIGGRAPH '91, pages 257-265, July 1991.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[G. Chaikin. An algorithm for high speed curve generation. Computer Graphics and Image Processing, 3:346- 349, 1974.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[R. Clough and J. Tocher. Finite element stiffness matrices for analysis of plate bending. In Matrix Methods in Structural Mechanics (Proceedings of the conference held at Wright-Patterson Air Force Base, Ohio, 26-28 October 1965), pages 515-545, 1966.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[D. Doo and M. Sabin. Behaviour of recursive division surfaces near extraordinary points. Computer Aided Design, 10(6):356-360, 1978.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Gene H. Golub and Charles F. Van Loan. Matrix Computations. The Johns Hopkins University Press, Baltimore, 2nd edition, 1989.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[G. Herron. Techniques for visual continuity. In G. Farin, editor, Geometric Modeling, pages 163-174. SIAM, 1987.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Charles T. Loop. Smooth subdivision surfaces based on triangles. M.S. Thesis, Department of Mathematics, University of Utah, August 1987.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617779</ref_obj_id>
				<ref_obj_pid>616025</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Michael Lounsbery, Stephen Mann, and Tony DeRose. Parametric surface interpolation. IEEE Computer Graphics and Applications, 12(5):45-52, September 1992.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134035</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Henry P. Moreton and Carlo S~quin. Functional optimization for fair surface design. In Proceedings of SIG- GRAPH '92, pages 167-176, July 1992.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>27628</ref_obj_id>
				<ref_obj_pid>27625</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Ahmad H. Nasri. Polyhedral subdivision methods for free-form surfaces. A CM Transactions on Graphics, 6(1):29-73, January 1987.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Malcolm Sabin. Recursive division singular points. Unpublished manuscript, June 1992.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[L. Shirman and C. S~quin. Local surface interpolation with B~zier patches. Computer Aided Geometric Design, 4(4):279-296, 1988.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 E.cient, Fair Interpolation using Catmull-Clark Surfaces Mark Halstead* Michael Kass Tony DeRose Apple 
Computer, Inc. Abstract We describe an e.cient method for constructing a smooth surface that interpolates 
the vertices of a mesh of arbitrary topological type. Normal vectors can also be interpolated at an arbitrary 
subset of the vertices. The method improves on existing interpolation techniques in that it is fast, 
robust and general. Our approach is to compute a control mesh whose Catmull-Clark subdivision surface 
interpolates the given data and minimizes a smoothness or fairness measure of the surface. Following 
Celniker and Gossard, the norm we use is based on a linear combination of thin-plate and mem­brane energies. 
Even though Catmull-Clark surfaces do not possess closed-form parametrizations, we show that the rel­evant 
properties of the surfaces can be computed e.ciently and without approximation. In particular, we show 
that (1) simple, exact interpolation conditions can be derived, and (2) the fairness norm and its derivatives 
can be computed exactly, without resort to numerical integration. CR Categories and Subject Descriptors: 
I.3.5 [Com­puter Graphics]: Computational Geometry and Object Modeling -curve, surface, solid, and object 
representations; J.6 [Computer-Aided Engineering]: Computer-Aided Design (CAD); G.1.2 [Approximation]: 
Spline Approximation. Additional Key Words and Phrases: Computer-aided geometric design, B-spline surfaces, 
subdivision surfaces, thin-plate splines.  1 Introduction The construction of smooth interpolating 
surfaces is becom­ing increasingly important in a number of applications in­cluding statistical data 
modeling, interactive design, and scienti.c visualization. Typical input to an interpolating method is 
a collection of points to be interpolated, and a *Work done while a summer intern from the University 
of Cal­ifornia, Berkeley. Work done while on sabbatical leave from the University of Washington. Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct provided that the copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the commercial advantage, the ACM copyright notice and the 
title of the publication and its date appear, and notice is given that copying is by publication and 
its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy permission of the Association for Computing Machinery. otherwise, or to republish, requires 
a fee and/or specific permission. otherwise, or to republish, requires a fee and/or specific permission. 
&#38;#169;1993 ACM -0 -89791 -601 -8/93/008 $1.50 &#38;#169;1993 ----8/93/008/0015 $1.50 mesh that describes 
the connectivity of the points. Nor­mal vectors are sometimes also speci.ed at some or all of the data 
points. If the shape to be modeled is a deformed plane, tech­niques from function approximation, such 
as Clough-Tocher interpolation [5], can be used. An advantage of the Clough-Tocher interpolant is that 
the construction is local, meaning that modi.cation of a data point a.ects only a local portion of the 
surface. However, a drawback of Clough-Tocher in­terpolation is that there are typically remaining degrees 
of freedom not directly constrained by the data. These extra degrees of freedom are often set using local 
heuristics and typically result in surfaces that are not fair , that is, sur­faces having extraneous 
bumps and wiggles. Another serious drawback to Clough-Tocher interpolation, and indeed to any method 
that requires continuity of parametric derivatives (so-called parametric continuity), is the inability 
to model surfaces of arbitrary topological type (cf. Herron [8]). It is not possible, for instance, to 
model a sphere or a deformed sphere using a Clough-Tocher interpolant. Celniker and Gossard [3] recently 
presented an interpo­lation method that extends Clough-Tocher interpolation by setting the remaining 
degrees of freedom so as to minimize a fairness norm. The fairness norm they use is quadratic, so it 
can be minimized by solving a (sparse) linear system. As a result, their method is fast enough for interactive 
design. However, being based on Clough-Tocher interpolants, their technique is not capable of describing 
surfaces of arbitrary genus. A number of interpolation methods appropriate for sur­faces of arbitrary 
genus have been developed in recent years. A survey of these can be found in Lounsbery et al. [10]. The 
method developed by Shirman and S´equin [14] is a gener­alization of Clough-Tocher interpolation to surfaces 
of arbi­trary topology. The generalization is achieved by replacing parametric continuity with .rst order 
geometric continuity (continuity of tangent planes). Like Clough-Tocher interpo­lation, Shirman-S´equin 
interpolants have degrees of freedom not directly constrained by the data, and local heuristics for setting 
these degrees of freedom have fallen well short of producing fair surfaces (see Figure 4). Last year 
Moreton and S´equin [11] presented a method capable of producing fair interpolating surfaces of arbitrary 
genus. They achieved this in much the same way as Celniker and Gossard by solving a minimization problem 
using .nite elements. However, rather than using Clough-Tocher ele­ments and a quadratic fairness norm, 
Moreton and S´equin used biquintic B´ezier patches and a fairness norm based on 0 intrinsic measures 
of curvature variation. The surfaces pro-v duced are the most impressive to date, but improved shape 
and arbitrary genus are obtained at the expense of dramat­ically increased running time. It appears that 
Moreton and S´equin s method is far too expensive for use in an inter­active environment today (computation 
time is on the or­der of hours). Another shortcoming of their method is that it constructs surfaces that 
are only approximately tangent plane smooth since inter-patch continuity is modeled using a penalty function 
added to the fairness norm. Finally, their surfaces are only curvature continuous within each biquintic 
patch. Here we present a scheme that combines the speed of Cel­ niker and Gossard s method with the 
ability to model tan­gent plane continuous surfaces of arbitrary genus. We do this by using a quadratic 
fairness norm similar to the one used by Celniker and Gossard together with Catmull-Clark subdivision 
surfaces. We show that Catmull-Clark surfaces o.er a number of advantages over previous methods based 
on piecewise polynomial elements; these include: They are curvature continuous everywhere except at 
a .nite number of isolated extraordinary points.  The high order of continuity is obtained with very 
few control points, meaning that the dimension of the space over which the optimizer must search is far 
lower for Catmull-Clark surfaces than for the method described by Moreton and S´equin.  They reduce 
to traditional bicubic B-splines when the points to be interpolated form a regular rectangular grid. 
It should therefore be possible to more smoothly incorporate them into existing geometric modeling sys­tems. 
 The use of Catmull-Clark surfaces presents some chal­lenges, however. First, Catmull-Clark surfaces 
do not gen­erally interpolate their control points, so to achieve interpo­lation, a system of interpolation 
constraints must be solved. The constraints relate the data points and normals to be interpolated with 
points and normals on the .nal surface. Formulating the interpolation constraints at .rst appears problematic 
for a Catmull-Clark surface because the surface is de.ned as the limit of an in.nite number of subdivisions. 
We show that it is possible to derive closed form expressions for these constraints. A second challenge 
posed by Catmull-Clark surfaces is that e.cient surface optimization depends on fast and reliable evaluation 
of the fairness norm and its derivatives. We show that it is possible to evaluate the fair­ness integral 
and its derivatives exactly, without resort to numerical integration, even though Catmull-Clark surfaces 
do not possess a closed form polynomial representation. Figure 5 illustrates the basic idea of our approach. 
The original mesh is shown in the upper left. Subdividing it us­ing Catmull-Clark subdivision results 
in the surface shown in the lower left. The surface approximates, but does not interpolate the vertices 
of the original mesh. By solving the system of interpolation constraints, we obtain a new mesh which 
is shown in the upper center. Subdividing the new mesh results in the surface in the lower center which 
does interpolate the vertices of the original mesh. Unfortunately, the direct application of the interpolation 
conditions to the mesh causes undesirable undulations in the surface. To com­bat this di.culty, we subdivide 
the mesh to add new degrees of freedom, and we set these new degrees of freedom to min­imize a fairness 
measure subject to the interpolation con­straints. The resulting mesh is shown in the upper right of 
Figure 1: The situation around a vertex v 0 of order n. Figure 5 and the corresponding subdivision surface 
is shown in the lower right. Note that minimizing the fairness measure removes the spurious undulations 
introduced by the direct application of the interpolation constraints. The remainder of the paper is 
structured as follows. In Section 2 we provide some necessary background on sub­division surfaces in 
general, paying particular attention to Catmull-Clark surfaces. In Section 3, we derive the linear constraints 
on a Catmull-Clark mesh which guarantee that the surface interpolates given points and normals. We also 
show that applying these constraints directly to a mesh re­sults in a surface which solves the interpolation 
conditions, but is unsatisfactory because of spurious wiggles. Then, in Section 4, we show how to reduce 
these artifacts by adding additional degrees of freedom through subdivision, and then setting them by 
optimizing a fairness norm based on the membrane/plate energy. Several implementation de­tails along 
with performance statistics are provided in Sec­tion 5. In Section 6 we present a number of examples, 
and provide some comparisons to previous methods. Finally, in Section 7 we summarize our .ndings and 
describe several avenues of future research.  2 Subdivision Surfaces In 1974 Chaikin [4] introduced 
the idea of generating a curve from a polygon by successively re.ning the polygon with the addition of 
new vertices and edges. In 1978, Catmull and Clark [2] and Doo and Sabin [6] generalized the idea to 
sur­faces. In these schemes, an initial control mesh is re.ned by adding new vertices, faces and edges 
at each subdivision step. In the limit as the number of subdivision steps goes to in.nity, the control 
mesh converges to a surface. With careful choice of the rules by which new vertices, edges and faces 
are introduced, it is possible to show that the limiting surface exists, is continuous, and possesses 
a continuous tan­gent plane. The Doo-Sabin subdivision rules generalize the subdivision rules for biquadratic 
B-splines, and the Catmull-Clark subdivision generalizes bicubic B-splines. An example of a Catmull-Clark 
surface of genus 3 is shown in Figure 3. A more recent method developed by Loop [9] general­izes quartic 
triangular B-splines. We focus on the Catmull-Clark scheme primarily because of the popularity of bicubic 
patches, however, much of the analysis we present is appli­cable to a wide class of subdivision schemes 
including those of Doo-Sabin and Loop. When dealing with spline surfaces it is often helpful to maintain 
the distinction between global and local control meshes. By a local control mesh, we mean a subset of 
the global mesh that in.uences a local region of the surface. To generate interpolating surfaces for 
other subdivision Toward this end we use carets to denote global quantities. schemes we need a method 
of determining the position and normal at a set of points on the limit surface. Because the M0 Mi mesh 
produced after i applications of the Catmull-Clark sub- V V Letdenote the initial mesh, and let denote 
the surface is the result of repeated application of a subdivision step, we can analyze the behavior 
of a small neighborhood of points as they converge to the limit surface in order to division step. To 
describe the i + 1-st subdivision step, con­ i Mi n edge points e1i , ..., e ni and n faces, as shown 
in Figure 1 for i = 0. Such a vertex is said to be of order n. As indicated in V sider the neighborhood 
of a vertex v ofsurrounded by determine the surface properties at the point of convergence. 3.1 Interpolation 
Conditions Figure 1, a new face point f1 i+1, ..., fni+1 is placed at the cen­ i+1 i+1 troid of each 
face of MVi. Each new edge point e1 , ..., e n is then computed by taking an average of surrounding points. 
Speci.cally, i ij + fi+1 + fi+1 v + e i+1 j-1 j e = , j 4 where subscripts are to be taken modulo n. 
Finally, a new vertex point v i+1 is computed as After one subdivision step there arises an arrangement 
of vertices that persists (i.e. the same topology will be observ­able) for any number of subsequent subdivisions. 
To analyze the limiting behavior of the surface near a vertex it is there­fore convenient to introduce 
a matrix that describes the sub­division process locally, that is, in the neighborhood of the vertex 
[6]. It is not necessary to compute local subdivision matrices in practice; they are simply tools used 
to derive formulas describing the limiting behavior of the surface. V Mi i i i fji+1 . (v ,e 1, ..., 
e n,f 1i, ..., f ni )T be the column vector of vertices in i , let Vni Let v be a vertex of order n 
of the mesh= i+1 n - 2 i 1 1 i v = v + 2ej + 2 nnn the neighborhood of v i, and let Vni+1 be the corresponding 
jj column vector of points in the neighborhood after subdivi-The Catmull-Clark subdivision process is 
such that: sion. Since the points in Vni+1 are computed by linear com­binations of the points in Vni, 
we can use a square matrix Sn The surfaces can be of arbitrary genus since the subdi-to express the subdivision: 
vision rules can be carried out on a mesh of arbitrary i+1 i topological type. Vn = SnVn. After the 
.rst subdivision step all faces are quadrilater- For instance, for Catmull-Clark surfaces the matrix 
S4 is als. .. 33331111 9 22224444 Except at extraordinary vertices (vertices of order n = .......... 
.......... 6 6 1 0 1 1 0 0 1 6 1 6 1 0 1 1 0 0 6 0 1 6 1 0 1 1 0 6 1 0 1 6 0 0 1 1 4 4 4 0 0 4 0 0 0 
4 0 4 4 0 0 4 0 0 4 0 0 4 4 0 0 4 0 4) the limiting surface can be shown to converge to a bicubic B-spline. 
The surface is therefore curvature con- V tinuous except at extraordinary vertices. 1 M1 S4 = * . The 
number of extraordinary vertices is .xed, and is 16 equal to the number of extraordinary vertices in, 
the mesh produced after the .rst subdivision step. Near an extraordinary vertex the surface does not 
pos­sess a closed form parametrization; it consists of an in.­nite number of bicubic patches that converge 
to a limit point. The surface can be shown to have a well de.ned tangent plane at the limit point, but 
the curvature there is generally not well de.ned [1].  3 Interpolation using Subdivi­sion Surfaces 
Given a mesh IVof arbitrary topological type, the idea is 440040004 Repeated subdivision is expressed 
by repeated multiplica­tion and hence powers of Sn, so i+1 i 1 V = SV n nn . The properties of the limit 
surface will be governed by the properties of Vni+1 as i approaches in.nity. Since Vni+1 is the image 
of V 1 under Si , the eigenstructure of Sn naturally nn plays a key role. In Appendix A we analyze the 
behavior of the limit sur- V M 0 surface it de.nes interpolates some or all of the vertices of IV. to 
generate a control meshsuch that the subdivision face in terms of the matrix Sn by building on the analytical 
 techniques of Doo and Sabin [6] and Ball and Storry [1]. Like It is also possible to constrain the surface 
to have a speci.ed normal at each interpolation point. Nasri [12] generates interpolating surfaces using 
the bi­quadratic formulation of Doo and Sabin [6]. Like biquadratic B-splines, Doo-Sabin surfaces interpolate 
the centroid of each face in the control mesh. Thus a linear constraint on the control vertices can be 
generated for each interpolation point and the resultant system solved for the desired control mesh1. 
It appears that Nasri had no simple formulation for the surface normal at the centroid, and so was unable 
to specify normals at these points. 1Although Nasri does not mention it, it is possible for the coe.cient 
matrix in the linear system to be singular. Loop [9], we .nd that the positions and normals of the limit 
surface can be expressed explicitly in terms of the vertices of the control mesh. However, whereas Loop 
s analysis was peculiar to his subdivision surfaces, our analysis applies to any subdivision scheme whose 
local matrix Sn satis.es the conditions listed in Appendix A. In particular, our analy­sis exposes the 
following simple dependence between the left eigenvectors of Sn and limit points and normals. Let .1 
= .2 = .3 be the three largest eigenvalues of Sn and let l1,l2,l3 be the corresponding left eigenvectors. 
In Appendix A we show that a point v 1 having a neighborhood V 1 n converges to the point v 8 = l1 · 
Vn 1 (1) and the normal vector to the surface at v 8 is given by N8 = c2 × c3 (2) where c2 = l2 · Vn 
1 and c3 = l3 · Vn 1, and where × denotes vector cross product. Explicit formulas for l1,l2 and l3 for 
Catmull-Clark surfaces can be found in Appendix A. Equation 1 provides an interpolation condition that 
is lin­ear in the control points of Vn 1 , but Equation 2 at .rst appears to impose a quadratic constraint 
on Vn 1 s control points. Fortunately, we can require a surface to have a given normal vector N, using 
the following two linear constraints: N · c2 = 0 and N · c3 = 0 (3) In addition to providing interpolation 
constraints, the limit point and normal vector formulas can also be used to compute exact points and 
normal vectors on the surface for use during rendering [9]. The color images (Figures 3 through 7) have 
all been computed this way. 3.2 Solving the Interpolation Problem Ignoring the interpolation of normals 
for the time being, we can use the interpolation condition in Equation 1 to compute a control mesh MV0 
with the property that the subdivision surface it de.nes interpolates the vertices of a given mesh IV. 
It is natural to do this by selecting MV0 to have the same mesh topology as IV, that is, the same number 
and connectivity of vertices, faces, and edges. This approach leads to a square linear system of the 
form Ax = b (4) where x is the column vector of the unknown vertex coor­dinates in MV0, and b is the 
corresponding column vector of vertex coordinates of IV. The rows of the square matrix A are determined 
by the interpolation conditions and mesh topology. In some cases, the matrix A is singular, so we use 
a least-squares solution to Equation 4. An example is shown in Figure 5. The original mesh is shown in 
the upper left. Subdividing it according to the usual Catmull-Clark rules re­sults in the lower-left 
surface which approximates, but does not interpolate the vertices of the original mesh. By solving Equation 
4, we obtain a new mesh which is shown in the upper center. Subdividing the new mesh according to the 
usual Catmull-Clark rules gives the surface in the lower cen­ter which does interpolate the vertices 
of the original mesh.  4 Fairing The surface in the lower center of Figure 5 is curvature con­tinuous 
almost everywhere and interpolates the vertices of the original mesh. Nonetheless, for many purposes 
it is an unsatisfactory interpolating surface because of its excessive undulations. These undulations 
appear to be artifacts of the interpolation process since they are not indicated by the shape of the 
original mesh. For example, the surface has a number of concavities where the original mesh is convex. 
Note that some of the undulations are present in the ordi­nary approximating Catmull-Clark surface, but 
they have become more severe and objectionable in the interpolating surface. This di.erence is typical 
of interpolating and ap­proximating surfaces. Nothing in our formulation of the interpolation conditions 
in Section 3 prohibits or discourages undulations in the sur­face, so this type of behaviour should not 
be surprising. In order to improve the quality of the interpolant, we introduce additional degrees of 
freedom into the surface by subdivision, and then set the degrees of freedom by optimizing a fairness 
norm on the surface subject to a set of linear constraints given by the interpolation conditions. 4.1 
Evaluating the Fairness Norm Celniker and Gossard [3] were able to improve the quality of interpolating 
surfaces using a fairness norm based on a linear combination of the energy of a membrane and a thin plate. 
Without any fundamental changes, the norm can be given directional preferences and nonuniform weighting 
over the surface, but for clarity of presentation, we consider the isotropic uniform case: E(W )= aEm(W 
)+ ßEp(W ) (5) where Em(W ) and Ep(W ) denote the membrane and thin­plate energies respectively: .. Em(W 
)= .Wu.2 + .Wv.2 du dv .. Ep(W )= .Wuu.2 +2.Wuv.2 + .Wvv.2 du dv, and where W (u, v)=(x(u, v),y(u, v),z(u, 
v)) is a paramet­ric representation of the surface, where subscripts on W rep­resent parametric derivatives, 
and where a and ß are freely selectable weights. Since the membrane/plate norm is de.ned in terms of 
a parametric representation of the surface, it cannot be di­rectly applied to Catmull-Clark surfaces 
since in general they have no natural parametrization near extraordinary points. The remainder of this 
section describes how we ex­tend the de.nition of the norm in a way that can be used with Catmull-Clark 
surfaces. As we show below, the ex­tended norm will be constructed to be quadratic in the con­trol points 
of the mesh. The optimization can consequently be performed quickly without iteration by solving a linear 
system. Moreover, there is a unique minimum since the Hes­sian of the norm is symmetric and positive 
de.nite. The membrane/plate norm can be evaluated without modi.cation on a bicubic patch W as follows. 
First, we note that the norm can be written as E = Ex + Ey + Ez, where Ex depends only on the x component 
of W , Ey only on the y component and Ez only on the z component of W . Let Px be a 16-element column 
vector of positions of the x coordinates of the control points W . Figure 2(a) schemati­cally depicts 
a 16 element control net and the bicubic patch it de.nes. The x component of the fairness norm for the 
patch can be expressed as Ex = PxT · K · Px (6) where the entries of the 16 × 16 matrix K can be computed 
exactly from the integrals in Equation 5 for bicubic B-spline basis functions. Similar formulas hold 
for the y and z com­ponents. Figure 2(b) depicts a mesh that includes an extraordinary point. The region 
of the limit surface corresponding to the central face in the mesh is shown at the center bottom, but 
the limit surface is not in general a parametric polynomial, so we cannot directly apply the membrane/plate 
norm used above for a bicubic mesh. However, we can subdivide the mesh in Figure 2(b) to obtain the mesh 
in (c). After sub­division, the limit surface is divided into four subpatches. Three of these subpatches 
(shown shaded in (c)) are bicubic B-splines, so on these patches we can in principle evaluate the fairness 
norm exactly. By repeating this procedure we can write an in.nite series for the fairness norm of the 
origi­nal extraordinary patch of Figure 2(b). In order to fully de­.ne the series, we must choose a parametrization 
for each of the B-spline subpatches during subdivision. Unfortunately, the most straightforward way to 
assign the parametrizations causes the in.nite series for the thin plate energy to diverge (see Appendix 
B). There are several methods that could be applied to over­come the problem of the divergent series. 
For instance, we might try to .nd an alternate method of parametrizing the subpatches that leads to convergent 
sequences. We are cur­rently investigating this possibility, but we have found that the following method 
gives good results. Intuitively, we in­tend to modify the thin plate energy so that it integrates to 
zero for surface patches de.ned by planar and regular control meshes. For a bicubic mesh it is relatively 
clear that a regular mesh is one that is an a.ne image of Fig­ure 2(a) since such a mesh has vanishing 
second derivatives. As shown in Appendix B, it is possible to generalize the notion of regularity for 
meshes containing an extraordinary vertex. It is also possible to measure the deviation of an arbitrary 
mesh of control points P from it s regular compo­nent P .. We therefore de.ne the modi.ed thin plate 
energy of P to be the thin plate energy of P - P .. In symbols, the norm we use can be written as E(P 
)= aEm(P )+ ßEp(P - P .). (7) We have written this norm as a function of the control mesh P rather than 
the limit surface that P de.nes. This is to emphasize that the norm is not, strictly speaking, a prop­erty 
of the limit surface. It is more appropriate to think of Equation 7 as a norm on meshes, because it is 
not gen- E(P i+1 erally the case that E(P i) = ) where P i and P i+1 denote the mesh after i and i+1 
subdivisions. Although this might be considered a theoretical de.ciency, it has posed no di.culties in 
practice. Using the modi.ed norm, the in.nite series is a conver­gent geometric series, so we can express 
its limiting value analytically. Appendix B contains the relevant details, but the result is that we 
can exactly compute the entries of a new quadratic form Kn that can be applied around an ex­traordinary 
vertex of order n. Now that we have de.ned the local fairness norm for patches surrounding extraordinary 
patches, we de.ne the global fairness norm as the sum of the fairness norms over each of the patches 
using the standard membrane/plate norm for bicubic patches and the modi.ed norm of Equa­tion 7 for extraordinary 
patches. We can write the global fairness norm as PVT VP where V K VK is a sparse matrix obtained from 
the various Kn by iterating over the individual vertices and collecting the entries into a global system, 
and where PVis a column vector containing the x, y and z coordinates of the control vertices in the global 
mesh . MV0 4.2 Minimizing the Fairness Norm Since we have a global expression for the fairness norm, 
we are now in a position to express and solve the minimization problem. Given a mesh IVwith t vertices, 
r of which are (a) (b) (c) Figure 2: (a) A regular control mesh (above) which gener­ates a bicubic B-spline 
patch on the limit surface (below). (b) A control mesh with an extraordinary point (above), and the extraordinary 
surface patch it de.nes (below). (c) The control mesh after one subdivision (above), and the four subpatches 
after subdivision (below). The three bicubic sub­patches are shaded gray, and the remaining extraordinary 
subpatch is shaded white. constrained to have a speci.ed limit point and s of which are constrained to 
have speci.ed normals, we seek the vec­tor of 3t vertex coordinates P such that the limit surface Vsatis.es 
the 3r +2s linear interpolation conditions and the fairness norm PVT VP is minimized over all possible 
PV. 2 Be- K V cause the constraints are linear and the norm is quadratic in the unknowns, this problem 
can be solved directly without iteration. If we have only positional constraints, the x, y and z com­ponents 
of the mesh are independent, so the whole problem decouples into three completely independent optimizations, 
one for each component of the mesh. If normal vectors are to be interpolated, the x, y and z components 
of the mesh are no longer independent, so the problem must be solved as a single optimization. Even so, 
the x, y and z components of the mesh remain nearly decoupled (in the sense that the linear system is 
block diagonal except for a few o.-diagonal terms) and sparse matrix methods exist that can exploit this 
fact[7]. The r position constraints and s normal constraints on the t mesh points can be represented 
by the equation BPV= D where B is a (3r +2s)×3t matrix and D is a vector of length 3r +2s. Let C be the 
3t by l matrix whose columns span the null space of B and let PV0 be any vector satisfying BPV0 = D. 
Then all PVwhich satisfy the interpolation constraints can be written in the form PV0 + CR for some l-vector 
R. Therefore we wish to .nd the vector R that minimizes: T V (PV0 + CR)K (PV0 + CR)= T TT T VT VV RCKCR 
+2RCKPV0 + PV0 KPV0. V K is symmetric and positive de.nite, so R is found by setting the gradient of 
this function to zero: CT V KCR + CT KVPV0 =0. (8) 2Each of the t vertices has three coordinates, so 
the total num­ber of unknowns is 3t. Each position interpolation constraint imposes three conditions, 
one per coordinate, and each normal vector constraint imposes the two conditions in Equation 3.  5 Implementation 
For simplicity and speed, our current implementation of the fairing process uses only positional constraints 
and exploits the fact that the linear systems for x, y and z decouple in this case. As a result, the 
implementation is able to compute the minimum energy mesh by solving three linear systems, each involving 
one third as many variables as Equation 8. To further speed the computation, each of these systems is 
solved using sparse-matrix methods. Given a mesh IVwhose vertices are to be interpolated, we must .rst 
choose the structure of the mesh MV0 whose ver­tices we compute. Our current implementation chooses MV0 
to have the structure that would result from subdividing IVtwice. This choice has two bene.ts. First, 
it adds enough extra degrees of freedom for the fairing to be e.ective. Sec­ond, it places enough new 
vertices between the interpola­tion points to ensure that the interpolation conditions for all vertices 
of IVare independent, making the construction of a sparse representation of the required null space easy. 
Since we are considering a single component x, y or z at a time and not allowing normal constraints, 
we can still write the interpolation conditions as BPV= D but now B is an r×t matrix and D is a vector 
of length r. We compute a sparse set of null-space vectors for B as follows. Suppose the ith row of B 
has k non-zero entries in columns (a1,a2,...,ak). Because of the way the positional constraints decouple 
after two subdivisions, all other entries of B in those k columns are zero. As a result, it is an easy 
matter to .nd k - 1 independent null-space column vectors which are zero ex­cept in rows (a1,a2,...,ak). 
Collecting these for each row of B yields a collection of sparse vectors that completely span the null 
space of B unless B contains zero columns. If (b1,b2,...,bm) are the zero columns of B, we complete the 
null space by adding the m vectors Qs,1 = s = m where Qs is one in the bsth entry and zero elsewhere. 
In addition to the null space, we need a feasible mesh V P0 which satis.es the constraints. We construct 
this mesh as follows. For each row i in B, with non-zero entries in columns (a1,a2,...,ak), set the entries 
of PV0 at indices (a1,a2,...,ak) to Di and set any remaining entries of PV0 to zero. Then since all the 
rows of B sum to one, the resulting PV0 will solve the equation BPV0 = D. Finally, given the null space 
basis C and the feasible mesh PV0, we compute the minimum energy mesh by solving Equa­tion 8 three times 
using sparse LU decomposition, once for each component of the mesh. If the mesh is a regular square v 
grid, the bandwidth of the linear system will be O( n), and the linear system will take O(n 2) time to 
solve. The running time is more di.cult to analyze for general meshes, but the times we have observed 
to date are consistent with O(n 2) performance. 6 Results Figure 5 shows the complete process of interpolation 
and fairing. The original mesh is shown in the top left. The interpolating mesh is shown at top center. 
The faired, in­terpolating mesh is shown at top right. Below each mesh is the corresponding Catmull-Clark 
limit surface. Note that the spurious undulations in the interpolating limit surface are greatly reduced 
in the faired interpolating surface. The additional subdivisions in the faired interpolating mesh pro­vide 
the degrees of freedom necessary to do this. For the examples presented in this paper, we set a = 0 and 
ß = 1. Often it is desirable to fair only a local region of the sur­face, either to have more control 
over the fairing or because the number of vertices in the control mesh is large. In this case we select 
a subset of control vertices that are free to move and compute the solution to the constrained minimiza­tion 
over the surface patches a.ected by this set. Figure 6 illustrates this process. The user has selected 
a subset of 52 vertices that are allowed to vary during the minimization process. These vertices are 
highlighted in red. Other nearby vertices which in.uence the minimization, but are not al­lowed to change, 
are shown in magenta. After fairing, the undulations in the faired region have been reduced, but they 
persist in the unfaired regions. In this case, the fairing took .18 seconds on an SGI Crimson workstation. 
Lounsbery et al. [10] have done a survey of the previously published interpolation methods and found 
that existing lo­cal interpolation schemes do an unsatisfactory job of con­structing fair surfaces, even 
for the simple cases such a data sampled from a torus. To facilitate comparison with these methods, we 
have run our algorithm and a representative local interpolant, that of Shirman and S´equin [14], on the 
same coarsely sampled toroidal data set. The results are shown in Figure 4. The upper left shows the 
original mesh used as input for the interpolants. The upper right shows the surface produced by the Shirman-S´equin 
algorithm. The odd looking specular highlights in the Shirman-S´equin in­terpolant point out some interpolation 
artifacts which are typical of local methods. Global methods tend to have a dif­ferent appearance. The 
surface in the lower left of Figure 4 is a Catmull-Clark surface that interpolates the original mesh 
using the methods of Section 3. This surface has di.erent (lower frequency) artifacts than the Shirman-S´equin 
inter­polant, but they are nonetheless objectionable. The surface in the lower right is an interpolating 
faired surface computed using our method. The surface has no visible artifacts, an observation con.rmed 
by examining the surface from other viewpoints. The implementation took 36.5 seconds to fair the entire 
600 point mesh at once on an SGI Crimson work­station. The result of applying the interpolation algorithm 
to a more complicated model is shown in Figure 7. The origi­nal mesh is shown at the far left. The left 
center shows the ordinary approximating Catmull-Clark surface. Note the ar­tifacts throughout the stem 
and where the stem meets the base. These artifacts are accentuated in the interpolating Catmull-Clark 
surface shown in the right center. In ad­dition, the interpolating surface shows severe overshoot at 
the bottom of the stem. This type of overshoot is typical of interpolation without fairing. The far right 
shows the faired interpolating Catmull-Clark surface computed using our method. The artifacts along the 
stem and where the stem joins the base have been removed. Fairing the 1273 point mesh took 127.8 seconds 
on an SGI Crimson worksta­tion. 7 Conclusions We have described an e.cient method for constructing fair 
surfaces that interpolate the vertices of a mesh of arbitrary topological type; normal vectors can also 
be interpolated at an arbitrary subset of the vertices. Our approach is to compute a control mesh describing 
a Catmull-Clark surface that interpolates the given data and minimizes a quadratic norm that combines 
thin plate and membrane energies. Our method improves on previous techniques by com­bining many of the 
strengths of the methods described by Celniker and Gossard and by Moreton and S´equin. Like Celniker 
and Gossard, we use a quadratic norm to achieve practical fairing at interactive rates. Like Moreton 
and S´equin, we use a representation capable of modeling arbi­trary topological surfaces. In addition, 
the Catmull-Clark representation we use provides improved surface continuity with remarkably few degrees 
of freedom. More speci.cally, Celniker-Gossard surfaces meet with only tangent plane con­tinuity along 
patch boundaries, and those of Moreton-S´equin meet with only approximate tangent plane continuity. Our 
surfaces, in contrast, are curvature continuous everywhere except at a .nite number of isolated points. 
Our work also provides two new analytical tools for an­alyzing and manipulating subdivision surfaces: 
limit point and normal vector analysis based on left eigenvectors of the local subdivision matrix, and 
a method for developing exact formulas for evaluating quadratic membrane/plate function­als and their 
derivatives. As a topic for future research, we plan to investigate using the surfaces produced by our 
method as a starting point for minimizing the intrinsic MVS norm developed by Moreton and S´equin. We 
are also interested in developing subdivision schemes that are curvature continous everywhere. References 
[1] A. A. Ball and J. T. Storry. Conditions for tangent plane continuity over recursively de.ned B-spline 
sur­faces. ACM Transactions on Graphics, 7(2):83 102, April 1988. [2] E. Catmull and J. Clark. Recursively 
generated B-spline surfaces on arbitrary topological meshes. Computer Aided Design, 10(6):350 355, 1978. 
[3] George Celniker and Dave Gossard. Deformable curve and surface .nite elements for free-form shape 
design. In Proceedings of SIGGRAPH 91, pages 257 265, July 1991. [4] G. Chaikin. An algorithm for high 
speed curve genera­tion. Computer Graphics and Image Processing, 3:346 349, 1974. [5] R. Clough and J. 
Tocher. Finite element sti.ness ma­trices for analysis of plate bending. In Matrix Methods in Structural 
Mechanics (Proceedings of the conference held at Wright-Patterson Air Force Base, Ohio, 26-28 October 
1965), pages 515 545, 1966. [6] D. Doo and M. Sabin. Behaviour of recursive division surfaces near extraordinary 
points. Computer Aided Design, 10(6):356 360, 1978. [7] Gene H. Golub and Charles F. Van Loan. Matrix 
Com­putations. The Johns Hopkins University Press, Balti­more, 2nd edition, 1989. [8] G. Herron. Techniques 
for visual continuity. In G. Farin, editor, Geometric Modeling, pages 163 174. SIAM, 1987. [9] Charles 
T. Loop. Smooth subdivision surfaces based on triangles. M.S. Thesis, Department of Mathematics, University 
of Utah, August 1987. [10] Michael Lounsbery, Stephen Mann, and Tony DeRose. Parametric surface interpolation. 
IEEE Computer Graphics and Applications, 12(5):45 52, September 1992. [11] Henry P. Moreton and Carlo 
S´equin. Functional opti­mization for fair surface design. In Proceedings of SIG-GRAPH 92, pages 167 
176, July 1992. [12] Ahmad H. Nasri. Polyhedral subdivision methods for free-form surfaces. ACM Transactions 
on Graphics, 6(1):29 73, January 1987. [13] Malcolm Sabin. Recursive division singular points. Un­published 
manuscript, June 1992. [14] L. Shirman and C. S´equin. Local surface interpolation with B´ezier patches. 
Computer Aided Geometric De­sign, 4(4):279 296, 1988. Appendix A Properties of the Limit Sur­face To 
develop formulas for limit points and normals on sub­division surfaces, we examine the eigenstructure 
of the lo­cal subdivision matrix Sn associated with the subdivision scheme. (Some of the following analysis 
appears to have been developed independently by Sabin [13].) Let m =2n + 1 denote the size of Sn, and 
let .1 = .2 = ... = .m denote the eigenvalues of Sn with corresponding right eigenvectors r1, ..., rm 
and left eigenvectors l1, ..., lm. If Sn is not defective, the right eigenvectors form a basis, and the 
left eigenvectors can be chosen so that (cf. Golub and Van Loan [7]) lk · rj = dkj . (9) Thus, assuming 
that Sn is not defective, the neighborhood V 1 n can be expanded uniquely as Vn 1 = c1r1 + ··· + cmrm 
(10) where the c s are geometric position vectors and where the r s are column vectors of scalars. The 
ck, k =1, ..., m can be determined by dotting both sides of Equation 10 with lk and using Equation 9: 
lk · Vn 1 = c1lk · r1 +··· +cklk · rk +··· +cmlk · rm = ck. (11) Using this expansion of Vn 1 , V i = 
Si V 1 = .i 1c1r1 + ··· + .i cmrm. nnn m For a non-trivial limit to exist as i .8, it is necessary for 
the magnitude of the largest eigenvalue .1 to be 1. In this case, 8 i 1 Vn := lim Vn = c1r1 =l1 · Vnr1 
i.8 For a subdvision scheme to be a.ne invariant (that is, inde­pendent of the coordinate system in which 
the calculation is Mi+1 performed), the points of Vmust be a.ne combinations of the points in MVi, meaning 
that each of the rows of Sn must sum to one. In matrix form: Sn(1, ..., 1)T = (1, ..., 1)T . In other 
words, the column vector of 1 s is the eigenvector r1 associated with eigenvalue 1. Since r1 is a column 
vector of 1 s, every point in the neighborhood converges to the point c1 = l1 · Vn 1 (12) on the limit 
surface. Stated more formally, we have proven that: Proposition 1: A point v 1 of MV1 with neighborhood 
Vn 1 and local subdivision matrix Sn, converges to the point v 8 = l1 · Vn 1 on the limit surface where 
l1 is the left eigenvector of Sn associated with eigenvalue 1, assuming that Sn satis.es the following 
conditions: i) Sn is not defective. ii) Sn describes an a.ne invariant process. iii) The magnitude of 
the largest eigenvalue is 1 and it has multiplicity 1. Using a discrete Fourier analysis similar to 
the one de­scribed by Ball and Storry [1], one can show that for Catmull-Clark surfaces the above conditions 
on Sn hold and that 1 l1 =(n 2 , 4, ..., 4, 1, ..., 1), n(n + 5) meaning that n 2 v 1 +4 e 1 j + fj 1 
8 jj v = . (13) n(n + 5) Equation 13 can be used as an interpolation condition on the points of MV1 by 
setting v 8 to a point to be interpolated. Note that the interpolation conditions are on the vertices 
of MV1, not on the vertices of the initial control mesh MV0 , since the analysis above requires that 
each face has exactly four edges. This apparent restriction poses no problem in practice since fairing 
requires the extra degrees of freedom present in MV1 . To develop an interpolation condition on normal 
vectors, we must determine the normal vector (if it exists) to the limit surface at v 8. This normal 
vector can be simply computed from the eigenstructure of Sn, as indicated by the following proposition. 
Proposition 2: The normal vector to a subdivision sur­face at a limit point v 8 corresponding to a vertex 
v 1 whose neighborhood is MVn 1 is the vector N8 = c2 × c3 where c2 = l2 · MVn 1 and c3 = l3 · MVn1, 
assuming that the local subdivision matrix Sn satis.es the conditions of Proposition 1 in addition to: 
iv) The eigenvalues .1 =1 = .2... are such that .2 = .3 > .4. Proof sketch: The general idea behind the 
proof is to show that there is a common plane to which all points in the neighborhood are converging. 
The vector N8 will then be chosen to be perpendicular to this plane. Let uji denote the vector from v 
8 to the j-th point pji of the neighborhood MVi . Roughly speaking, if a common plane exists, then it 
should be possible to .nd an expression for a vector N8 that is perpendicular to each of the uji s in 
the limit i .8. Stated as an equation, we might seek a vector N8 such that N8 · uji . 0 for j =2, ..., 
m as i .8. This does not quite work, how­ever, because each u ij is approaching the zero vector, im­plying 
that the above condition would trivially hold for any vector N8 . This problem is overcome by considering 
the unit vectors uji . Thus, we seek a vector N8 such that N8 · u ji . 0 for j =2, ..., m as i .8. If 
rjk denotes the entry in the j-th row of rk, then pji - v 8 u j = i - v8. i .pj .i(c2rj2 + c3rj3)+ .i 
4c4rj4 + ··· = ..i(c2rj2 + c3rj3)+ .i 4c4rj4 + ···. .i (c2rj2 + c3rj3)+ .4 i c4rj4 + ··· = .i .(c2rj2 
+ c3rj3)+ .4 i c4rj4 + ···. In the limit as i .8, 8 i c2rj2 + c3rj3 u j = lim u j = . (14) i.8 .c2rj2 
+ c3rj3. Equation 14 implies that each of the limiting unit vectors 8 u j , j =2, ..., m is a linear 
combination of the vectors c2 8 and c3. All the vectors uj must therefore lie in the plane spanned by 
c2 and c3. The normal vector N8 we seek is therefore c2 × c3. . Again using a discrete Fourier transform 
technique, one can show that for Catmull-Clark surfaces, 4+ An. := .2 = .3 = 16 2pj 2pj 2p(j + 1) c2 
= An cos( )ej 1 +(cos( )+cos( ))fj 1 nnn j where 2pp 2p An =1+cos( )+cos( ) 2(9+cos( )). nn n The vector 
c3 is obtained from c2 by replacing ej 1 with e 1 j+1 and fj 1 with fj1+1.  B Integrating the fairness 
func­tional In this appendix, we consider the problem of evaluating the fairness norm of Equation 7 for 
a patch whose local control mesh P contains an extraordinary point, such as the one shown in Figure 2(b). 
As motivated in Section 4, we will ultimately evaluate only the non-divergent part of the thin plate 
energy corresponding to the deviation of P from its regular component P . . As we show below, it is not 
neces­sary to compute P . explicity, so we will for the time being evaluate the energy of P . The quadratic 
form K referred to in Equation 6 can be written as a weighted sum of two quadratic forms Km and Kp, representing 
the membrane and plate energies, respec­tively for a bicubic patch: K = aKm + ßKp. Let E(n, P, j) denote 
the fairness norm of Equation 7 in­tegrated over a patch containing at most one extraordinary point of 
order n whose local mesh is described by the column vector of control points P , and whose level of subdivision 
is j. As outlined in Section 4, when n = 4, we evaluate E(n, P, j) by splitting the patch into four subpatches, 
three of which are ordinary (shown in gray in Figure 2), and one of the same form as the original. the 
This leads to the following recurrence relation for E(n, P, j): E(4, P, j)= P T (aKm +4j ßKp) P 3 E(n, 
P, j)= E(4, OkP, j + 1) + E(n, O4P, j + 1) k=1 where O1, O2, O3 are matrices that carry P into the local 
meshes for the ordinary (shaded) subpatches, and where O4 is the matrix that carries P into the local 
mesh for the re­maining (unshaded) extraordinary subpatch. The factor of 4j in front of Kp re.ects the 
change of in­tegration variables when a patch is subdivided j times. The choice of powers of 4 is somewhat 
arbitrary. It corresponds to the parametrization assigned to the bicubic subpatches created when the 
extraordinary patch is subdivided. We have chosen powers of 4 since it is the correct factor for bicubic 
patches. We are, however, currently experimenting with methods to select this factor based on n. The 
above recurrence can be unrolled to produce an in.­nite series for E(n, P, 0): 8 3 j-1 E(n, P, 0) = E(4, 
OkO4 P, j) j=1 k=1 which can be written as E(n, P, 0) = P T KnP where 8 j-1 Tj ¯j-1 Kn := (O4 )(¯Kp)O4 
, Km +4 j=1 and where 3 T ¯ Km := aOk KmOk, k=1 3 ¯ßOT Kp := k KpOk. k=1 The limiting value of the series 
can be found by expanding O4 in its basis of eigenvectors: O4 = X.X-1 where . is a diagonal matrix containing 
the eigenvalues of O4, and where the columns of X are the corresponding right eigenvectors. Without loss 
of generality we can assume that the eigenvalues appear in decreasing order down the diago­nal. Kn can 
now be written as 8 -Tj-1T ¯j-1-1 Kn = X.XKmX.X+ j=1 Km 8 -Tjj-1T ¯j-1-1 X4.XKpX.X. j=1 Kp Since . 
is diagonal, the ab-th entry of is Km 8 ( T ¯j-1j-1 Km)ab =(XKmX)ab (.aa)(.bb). j=1 The above series 
is geometric, so if .aa.bb < 1, it converges to (XT ¯ KmX)ab (K m)ab = . 1 - .aa.bb Using arguments as 
in appendix A, it can be shown that the largest eigenvalue of O4 is one, meaning that the product .aa.bb 
is at most one, and this occurs only when a = b = 1. The membrane energy is invariant under translation, 
which ¯ is re.ected in the fact that (XT KmX)11 is zero; hence ( )11 0. Km= A similar analysis for shows 
that Kp 8 ( T ¯j-1j-1j-1 Kp)ab = 4(XKpX)ab 4(.aa)(.bb). j=1 Thus, ( )ab .bb < 1. The factor Kpis .nite 
whenever 4.aa4.aa.bb can be shown to be one or larger when 1 = a, b = 3. Just as for the membrane energy, 
the 11 entry poses no ¯ di.culty since (XT KpX)11 = 0, indicating that the thin plate energy is invariant 
under translation. The remaining 8 entries of K p are unbounded for n> 4. When n = 4 (i.e., the ordinary 
case), 4.aa.bb = 1, yet we know that the entries of are .nite since bicubic patches Kp have .nite thin 
plate energy. We therefore conclude that for n = Kp= 0 for 1 = a, b = 3. This re.ects the 4,( )ab fact 
that regular control meshes have zero thin plate energy. To generalize this idea to arbitrary n, we simply 
set the remaining 8 divergent terms to zero, which is equivalent to evaluating the norm on P - P . . 
To summarize, the quadratic form related to the thin plate energy is taken to be ¯ 4(XT KpX)ab ( if4.aa.bb 
< 1 Kp)ab =1 - 4.aa.bb 0 otherwise Figure 3: Upper Left: Tetrahedral mesh with holes. Up­per Right: The 
mesh after one Catmull-Clark subdivision. Lower Left: The mesh after two subdivisions. Lower Right: The 
limit surface. Figure 4: Interpolating a coarsely polygonized torus. Up­per left: original mesh. Upper 
right: Shirman-S´equin interpolation[14]. Lower left: Interpolating Catmull-Clark surface. Lower right: 
Faired interpolating Catmull-Clark surface. Figure 5: Top row: Original mesh, Interpolating mesh, Faired 
interpolating mesh. Bottom row: Corresponding Catmull-Clark surfaces. Interpolation introduces wiggles 
which are removed by fairing. Figure 6: Lower left: unfaired interpolating surface. Up­per center: Interactive 
fairing. Red vertices are allowed to move. Magenta vertices in.uence the minimization, but re­main .xed. 
Lower right: Result after fairing. Figure 7: From left to right: Original goblet mesh containing 190 
vertices. Ordinary Catmull-Clark surface (approximating). Interpolating Catmull-Clark surface. Faired 
interpolating Catmull-Clark surface. The far right surface interpolates the original mesh without the 
artifacts present in the middle two surfaces. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166122</article_id>
		<sort_key>45</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Implementing rotation matrix constraints in Analog VLSI]]></title>
		<page_from>45</page_from>
		<page_to>52</page_to>
		<doi_number>10.1145/166117.166122</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166122</url>
		<keywords>
			<kw><![CDATA[CMOS]]></kw>
			<kw><![CDATA[VLSI]]></kw>
			<kw><![CDATA[adaptive]]></kw>
			<kw><![CDATA[analog]]></kw>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[constraint solution]]></kw>
			<kw><![CDATA[interaction]]></kw>
			<kw><![CDATA[robotics]]></kw>
			<kw><![CDATA[rotation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>B.7.1</cat_node>
				<descriptor>VLSI (very large scale integration)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.1.2</cat_node>
				<descriptor>Parallel processors**</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>B.7.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010600</concept_id>
				<concept_desc>CCS->Hardware->Integrated circuits</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010528</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Parallel architectures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010633</concept_id>
				<concept_desc>CCS->Hardware->Very large scale integration design</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14068579</person_id>
				<author_profile_id><![CDATA[81100166914]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[B.]]></middle_name>
				<last_name><![CDATA[Kirk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14034821</person_id>
				<author_profile_id><![CDATA[81100070192]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Barr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>97914</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Barkans, Anthony C., "High Speed High Quality Antialiased Vector Generation," Computer Graphics, Vol. 24, No. 4, August, 1990, pp. 319-326.]]></ref_text>
				<ref_id>Barkans 90</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Barzel, Ronen, "Structured Modeling for Computer Graphics," Academic Press, Cambridge, MA, 1992.]]></ref_text>
				<ref_id>Barzel 92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801272</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Clark, James, "The Geometry Engine: A VLSI Geometry System for Graphics," Computer Graphics, Vol. 16, No. 3, July, 1982, pp. 127-133.]]></ref_text>
				<ref_id>Clark 82</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Denyer, Peter B., John Mavor, "MOST Transconductance Multipliers for Array Applications," IEEE Proceedings, Volume 128, Pt. I, Number 3, pp. 81-86, June 1981.]]></ref_text>
				<ref_id>Denyer 81</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74341</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Fuchs, Henry, J. Poulton, J. Eyles, T. Greer, J. Goldfeather, D. Ellsworth, S. Molnar, G. Turk, B. Tebbs, and L. Israel, "Pixel-Planes 5: A Heterogeneous Multiprocessor Graphics System Using Processor-Enhanced Memories," Computer Graphics, Vol. 23, No. 3, July, 1989, pp. 79-88.]]></ref_text>
				<ref_id>Fuchs 89</ref_id>
			</ref>
			<ref>
				<ref_obj_id>164697</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Kirk, David B., "Accurate and Precise Computation using Analog VLSI, with Applications to Computer Graphics and Neural Networks," Ph.D. Thesis, California Institute of Technology, Caltech-CS-TR-93-08, June, 1993.]]></ref_text>
				<ref_id>Kirk 93</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Kirk, David, Kurt Fleischer, and Alan Barr, "Constrained Optimization Applied to the Parameter Setting Problem for Analog Circuits," IEEE Neural Information Processing Systems 1991 (NIPS 91), Morgan Kaufman, San Diego, 1991.]]></ref_text>
				<ref_id>Kirk 91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>64998</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Mead, Carver, "Analog VLSI and Neural Systems," Addison-Wesley, 1989.]]></ref_text>
				<ref_id>Mead 89</ref_id>
			</ref>
			<ref>
				<ref_obj_id>76524</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Platt, John, "Constrained Optimization for Neural Networks and Computer Graphics," Ph.D. Thesis, California Institute of Technology, Caltech-CS-TR-89-07, June, 1989.]]></ref_text>
				<ref_id>Platt 89</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74339</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Rhoden, Desi, and Chris Wilcox, "Hardware Acceleration for Window Systems" Computer Graphics, Vol. 23, No. 3, July, 1989, pp. 61-67.]]></ref_text>
				<ref_id>Rhoden 89</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378517</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Voorhies, Douglas, D. Kirk, and O. Lathrop, "Virtual Graphics," Computer Graphics, Vol. 22, No. 4, August, 1988, pp. 247-253.]]></ref_text>
				<ref_id>Voorhies 88</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Implementing Rotation Matrix Constraints in Analog VLSI David B. Kirk Alan H. Barr California Institute 
of Technology Computer Graphics 350-74 Pasadena, CA 91125 email: dk@egg.gg.caltech.edu Abstract We describe 
an algorithm for continuously producing a 3x3 ro­tation matrix from 9 changing input values that form 
an approx­imate rotation matrix, and we describe the implementation of that constraint in analog VLSI 
circuits. This constraint is useful when some source (e.g., sensors, a modeling system, other ana­log 
VLSI circuits), produces a potentially imperfect matrix, to beusedasarotation. The9valuesarecontinuouslyadjustedover 
time to .nd the nearest true rotation matrix, based on a least­squares metric. The constraint solution 
is implemented in analog VLSI circuitry; with appropriate design methodology [Kirk 93], adaptive analog 
VLSI is a fast, accurate, and low-power compu­tational medium. The implementation is potentially interesting 
to the graphics community because there is an opportunity to apply adaptive analog VLSI to many other 
graphics problems. CR Categories and Subject Descriptors: C.1.2 [Processor Architectures]: Multiprocessors 
-parallel processors; C.1.3 [Processor Architectures]: Other Architecture Styles; I.3.1 [Computer Graphics]: 
Hardware Architecture -raster display devices; I.3.3 [Computer Graphics]: Picture/Image Genera­tion; 
I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling; I.3.7 [Computer Graphics]: Three-Dimensional 
Graphics and Realism General Terms: Algorithms, Graphics, Hardware Additional Key Words and Phrases: 
Animation, rotation, robotics, simulation, constraint solution, interaction, adaptive, analog, CMOS, 
VLSI. 1 Introduction This paper has two main purposes. First, we demonstrate the implementation of a 
nontrivial constraint technique in analog VLSI. Second, since some of the computer graphics community 
may not be familiar with recent developments in analog VLSI technology, we describe some of the potential 
bene.ts. We believe that analog VLSI has great potential as a computation medium for implementing rendering, 
modeling, and interactive operations. Permission to copy without fee all or part of this material is 
granted provided that the copies are not made or distributed for direct provided that the copies are 
not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the 
commercial advantage, the ACM copyright notice and the title of the publication and its date appear, 
and notice is given that copying is by publication and its date appear, and notice is given that copying 
is by permission of the Association for Computing Machinery. permission of the Association for Computing 
Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. otherwise, 
or to republish, requires a fee and/or specific permission. &#38;#169;1993 8/93/008/0015 $1.50 &#38;#169;1993 
ACM - 0 - 89791 - 601 - 8/93/008 $1.50 1.1 Computation in Computer Graphics There is a history of digital 
VLSI acceleration in computer graphics: geometry engines [Clark 82], hardware frame buffer assists [Rhoden 
89], vector generators [Barkans 90], systems [Voorhies 88] [Fuchs 89], etc. Most high-performance graphics 
workstations have a substantial amount of special purpose digital chips to provide the kind of interactive 
performance that we have come to expect. Most of this silicon is dedicated to rendering tasks, although 
it can be argued that the geometric transforma­tions performed in hardware constitute modeling hardware. 
Any computational medium used for graphics needs to be able to perform mathematical operations accurately 
and pre­cisely. The bulk of simulation and modeling calculations for computer graphics are performed 
in software. For instance, for physically-based modeling, the shapes and motions of graphi­cal objects 
are computed according to the physics underlying the simulation. This process requires the solution of 
differential equations (for converting the relation F = ma into position and velocity). As we consider 
collisions between objects, we may also be required to solve for roots of nonlinear equations. For modeling, 
we require the ability to accurately and precisely solve a variety of mathematical equations. Applying 
real physical constraints to computer graphics models requires great computational resources. Even relatively 
simple simulations, involving only a few primitives, may con­sume many seconds of CPU time on a fast 
computer. The tradi­tional arguments are that CPU price-performance doubles every year, and that massive 
parallelism will save us. We claim that current digital computation approaches are approximately a fac­tor 
of 10,000 times too slow for real-time simulation of complex scenes. In order to be effective in addressing 
this problem, a compu­tational medium must be fast and accurate. If we can produce a technology which 
can accurately and precisely compute the so­lutions of equations, we can then use the technology to construct 
computer graphics hardware. We hope that adaptive analog VLSI can be used to realize the goal of performing 
graphics calculations thousands of times faster than is possible today. 1.2 Adaptive Analog VLSI There 
has been increasing interest recently in using analog VLSI [Mead 89] for a variety of computational tasks. 
Mead and others have pursued the paradigm of using analog transistors to model components of neural systems. 
Related research has focused on increasing the accuracy and precision of computation with analog VLSI 
[Kirk 93], and on developing a design methodology for creating analog VLSI circuits which can be adjusted 
to perform tothedesiredaccuracy[Kirk 91]. Thesetechniquesmakeanalog VLSI more tractable for quantitative 
computation. This is not the .rst appearance of analog computation in computer graphics. Certainly, there 
is some amount of analog hardware in every graphics system, at least in the form of a D/A (digital-to-analog) 
converter in the path to the video monitor. There have also been more extensive uses of analog, however. 
For instance, Vector General implemented matrix multiplication for the purpose of performing coordinate 
transformations in ana­log circuitry, although not in analog VLSI. It is important to note that in these 
discussions, we have cho­sen a particular constraint to demonstrate the general technique of implementing 
a constraint in analog VLSI. There are many other examples of useful constraint computation that could 
be formulated in a similar fashion [Platt 89] [Barzel 92], and also could be implemented in analog hardware. 
The particular con­straint that we have chosen to implement is meant to be rep­resentative of a large 
set of possibilities. Our example raises the exciting prospect of implementation of extensive hardware 
modeling assists in analog VLSI. There are also many rendering tasks which are appropriate for analog 
VLSI hardware imple­mentation, but we won t discuss them in this paper. We have chosen to describe a 
constraint technique that is appropriate for interactive input devices, and has application to modeling 
as well. 1.3 The Rotation Matrix Constraint The constraint technique that we have chosen is the ortho­normalization 
of a rotation matrix. We chose the 3x3 matrix formulation because it is easier to perform coordinate 
transfor­mations with the same underlying computational modules that are used to implement the constraints. 
In Sec. 4, we describe several computational blocks that we can also use to construct coordinate transformation 
hardware. The matrix formulation is also complex enough to be interesting as an example problem for hardware. 
real world input  in "imperfect" rotation matrix M (t) Rotation Matrix Constraint out improved rotation 
matrix M (t) output to application Figure 1: A system-level view of the rotation matrix constraint enforcement, 
and how the result of applying the constraint might be used. The rotation matrix constraint is particularly 
useful as part of an interactive system, as shown in Fig. 1. For virtual reality applications, a sensor 
may be used to produce a 3D orientation, in the form of a 3x3 rotation matrix. Sensors are often .awed, 
noisy, or otherwise inaccurate and do not provide suf.cient and reliable information for producing an 
accurate rotation matrix. In such cases, we then wish to continuously produce a best es­timate rotation 
matrix, based on the sensor measurements. One example of such a system involves producing rotation matrices 
from approximate inputs from sensors or interactive devices. The system produces approximate rotation 
matrices over time from angular velocity (t), according to the following relation:   m 0 m r M(t)=M(t) 
(1) Such a system would produce an approximate rotation matrix at each time step, and may accumulate 
errors over time. The errors could be corrected by the constraint technique described in this paper. 
A similar task exists in robotics applications. We might have a sensor which can detect the position 
of an end effector of a robot arm, and also a measure of the control inputs. In practice, a robot arm 
is often controlled by providing joint angle control inputs. However, the control may be inaccurate, 
and there may be slop in the joints. We may want to then compute an estimate of the actual joint angles, 
which, if the arm segments are rigid, must be pure rotations. There are also many applications to physically-based 
mod­eling. When solving constraint equations for motion of rigid bodies, we may produce values that are 
inaccurate due to ac­cumulating arithmetic roundoff errors, integration step size, or approximations 
in our model. When combined to form a ro­tation matrix to describe the orientation of a body, the errors 
may cause the introduction of scaling or skewing into the matrix. The constraint technique described 
in this paper will allow us to automatically adjust for these errors. In Sec. 2, we describe the constraint 
algorithm that we use to produce the rotation matrix. In Sec. 3, we introduce in more detail the technology 
used for the implementation (analog VLSI), and explain why we believe that it has great potential to 
be useful for computer graphics. In Sec. 4, we present a block diagram description of the constraint 
chip. 2 The Constraint Algorithm Our goal is to produce a 3x3 rotation matrix containing no scale or 
skew components, given 9 numbers which are already nearly rr a rotation matrix. 6 For a mathematically 
perfect rotation matrix M, MMT = I where I is the identity matrix. We de.ne the function f (): f (M) 
= (MMT I) : (MMT I) where the double-dot operator (:) denotes the sum of products of terms of the two 
matrices, producing a scalar result, analogous to the dot product of two vectors. When M is a rotation 
matrix (or re.ection), f (M) in Eqn. 3 is equal to zero, and when M is not purely a rotation matrix, 
f (M) = 0. Since f (M) is greater than or equal to zero, M is a rotation matrix when f (M) is minimized. 
We perform continuous gradient descent to minimfunction f (), as follows: M0 (t) =r : r f (M(t)) (2) 
(3) always ize the (4) where epsilon is a parameter which determines the speed of the descent. Appendix 
1 describes the derivation of our gradient calculation method in detail. The analog VLSI implementation 
does not suffer from many of the problems of digital implementations, since analog circuits can operate 
in continuous time. For instance, in a digital im­plementation, Euler s method might be used to solve 
Eqn. 4. With large step sizes, Euler s method frequently becomes un­stable. With small step sizes, Euler 
s method may converge slowly or not at all. Other techniques, such as the conjugate gradient method, 
may improve the performance in digital imple­mentations. The continuous nature of an analog implementation, 
however, avoids this type of problem entirely. As the computation proceeds, two kinds of changes are 
oc­curring. First, the imperfect input matrix may be changing over time. Second, based on our optimization 
process, the output matrix will be changing to ful.ll our rotation matrix constraint. Since the analog 
VLSI circuit operates very quickly, and in con­tinuous time, the optimization can occur at a much .ner 
time scale than the changing of the input matrix. 3 Adaptive Analog VLSI There has been increasing interest 
recently in using analog VLSI [Mead 89] for a variety of computational tasks. One of Mead s insights 
is that rather than developing an entirely new manu­facturing technology for producing analog VLSI chips, 
we can produce analog CMOS VLSI chips using standard digital CMOS VLSI processes. The key element in 
this strategy is to produce designs that are tolerant to the device variations that are present in a 
digital production process. Another component of this de­sign philosophy is the exploration of architectures 
and circuits that are tolerant of device variations. 3.5e-05 3e-05 2.5e-05 2e-05 1.5e-05 1e-05 
5e-06 0  2.5e-05 2e-05 1.5e-05 1e-05 5e-06 0  Figure 2: The upper graph shows the drain current 
of a single transistor, as the gate voltage is varied from 0 to 5 volts. The family of curves represents 
varying the difference between the source and drain voltage. The lower graph shows the drain current 
as the source-to-drain voltage difference is varied from 0 to 5 volts. The family of curves represents 
varying the gate voltage. The analog VLSI multipliers discussed in Sec. 3 operate in the nearly linear 
region to the right of the upper graph and to the left of the lower graph. Other research has focused 
on increasing the accuracy and Current (amps) Current (amps) precision of computation with analog VLSI 
[Kirk 93], and on developing a design methodology for creating analog VLSI cir­cuits which can be adjusted 
to perform to the desired accuracy [Kirk 91]. This work can be characterized as using adaptation and 
optimization to harness analog VLSI for more conventional computing applications. This approach is attractive 
because analog transistors provide a rich computational gamut. Fig. 2 (upper) shows the current .owing 
through an analog transistor as its gate voltage is varied. Fig. 2 (lower) shows the current as the source-to-drain 
voltage is varied, while holding the gate voltage constant. These .gures are meant as a qualitative demonstration 
of the variety of current-voltage responses available from a sin­gle transistor. Note the regions of 
roughly linear, exponential, and quadratic I-V relation. It is possible make analog circuits more quantitatively 
useful, by designing compensatable circuit building blocks that can be adjusted to perform more closely 
to some performance metric. For example, let us assume that our goal is to build a perfect analog multiplier. 
In analog VLSI, we can easily build a circuit which computes an imperfect multiply-like operation, but 
the perfect multiply is more elusive. We can design a multiplier that is monotonic within some input 
range, and operates in four quadrants (the sign of the output is correct for all combinations of inputs 
sign). Without extreme care in the design, however, the multiplier would have a number of drawbacks. 
The cir­cuit s responsemight deviate signi.cantly from the desired linear function of its inputs f (xy)= 
xy (5) e w The multiplier would also, very likely, have nonzero input offsets1. A compensated multiplier 
has adjustable parameters which allow for the improvement of the linear range of behavior, as well as 
the cancellation of input offsets. A description of how to design, build, and optimize compensatable 
components is presented in detail in [Kirk 93]. Sec. 5 presents some measure­ments from chips implemented 
and compensated using these techniques. 4 Applying Analog VLSI to the Constraint Problem Now that we 
have described the desired constraints (in Sec. 2) and the substrate technology of adaptive analog VLSI 
(Sec. 3), we will explain, at a block diagram level, how we use analog VLSI to solve the constraint problem. 
These block diagrams represent a hierarchical decomposition of the chip that we built. As one might guess 
from the form of the equations of the derivation in Sec. 2, the circuit architecture is a nested, structured 
hierarchy of dot products, with some additional computation. We can think of the nine input values,the 
imperfect rotation matrix, as the three 3D basis vectors, X, Y, and Z, (the three columns of the matrix). 
We can see by examining Eqn. 13 that the computation of the various components of the gradient, pq, 33 
requires dot products of the matrix basis vectors. Appendix 1 describes the calculation of pq. Fig. 3 
shows a functional block which computes two of the six basis vector dot products that are required. Fig. 
4 shows a set of three functional blocks (from Fig. 3) which together compute the six 3D basis vector 
dot products that are required to form the gradient, pq, as shown in Eqn. 13. The details of the circuit, 
the device layout, and the compensation e 6 3 e tg e 6 1Input offsets are present for an analog multiplier 
f (xy) xy when f (x0) = 0 or f (0y) = 0. X X Y X Y  Figure 3: A functional block containing two dot 
products. The inputs are the matrix column vectors X and Y, and the outputs are the scalars XX and XY. 
procedure for the multiplier and dot product blocks are presented in [Kirk 93]. Z Z  Y Y X X Y X 
 X 0 Z X Y Figure 4: A collection of three dot product blocks, from Fig. 3. With the 3D basis vector 
inputs X, Y, and Z, they compute the six dot products required to enforce the constraints. Fig. 5 shows 
the use of the basis vector inputs and three of the dot product results to produce the gradient components 
for one of the basis vectors, in this case, X. Fig. 6 shows a set of three constraint blocks, from Fig. 
5, which together compute all of the components of the gradient for the correction of the imperfect matrix. 
The combination of these three constraint blocks and the three dot product blocks 0 Z 0 Z 0 from Fig. 
4 forms the gradient calculation hardware. X1, X2, X3, Y1, Y2, Y3, Z1, Z2, and Z3 are the nine derivative 
components. To- X X-1 X Y X Z  Figure 5: A basis vector constraint block, using the outputs from Fig. 
4. This computational element implements the rotation matrix constraint for one of the three matrix column 
vectors. the gradient will produce a matrix which ful.lls our constraints. We use the derivative terms 
from Fig. 6 to add or subtract from the original input values of the matrix, M. Since the cir­cuits are 
analog and operate in continuous time, we can integrate these corrections on capacitors, and use the 
gradient components to set the level of current to add/subtract. Thus, this circuit struc­ture can be 
use to continuously track and correct a (potentially .awed) matrix that changes over time. Fig. 7 shows 
the con­nections required to provide the feedback from the calculated gradient components to modify the 
input matrix components. The gradient calculation occurs in continuous time, using the analog VLSI hardware. 
The input can change continuously, or discretely (using the reset input in Fig. 7), and the constraint 
solution will track the input. Fig. 8 shows a schematic view of the the rotation matrix constraint solution 
box connected as part of a system. Given a source of approximate rotation matrices Min(t), the constraint 
enforcement produces rotation matrices Mout (t), which can be used for modeling, rendering, or control 
applications.   5 Results We have designed, implemented, fabricated, and tested chips which contain 
compensated multipliers, dot products, and con­straint blocks, as described in Fig. 3 through Fig. 6. 
The design gether, they form the gradient, which we will use to optimize the is modular (similar to 
the structure of the .gures), so that we components of the matrix M. Descending along the direction of 
are con.dent that the system will work, given the partial test X Y X Z Y X  Y Z Z XZ Y  real world 
input "imperfect" Rotation improved rotation Matrix rotationin matrix M (t) Constraint matrix Mout(t) 
 X1 Y1 Z1  output to application Figure 8: A system-level view of the rotation matrix constraint enforcement, 
and how the result of applying the constraint might be used. X2 Y2 Z2 slope line formed by the square 
symbols. That line represents the results of multiplying zero by a set of other quantities, so should 
be horizontal, at zero. 1.5e-06  X3 Y3 Z3 1e-06 5e-07 -5e-07 -1e-06  Figure 6: A collection of three 
constraint blocks, from Fig. 5. The combination of these three constraint blocks and the three dot product 
blocks from Fig. 3 forms the gradient calculation hardware. in  M (t) Output (amps) 0   Calculation 
-1.5e-06 -2.6 -2.4 -2.2 -2 -1.8 -1.6 -1.4 G1(X1) (volts) Figure 9: The output from an uncompensated 
multiplier circuit (actual measured chip data). The analog multiplier circuit has not been adapted to 
compensatefor input offsets and other device variations. Notethenonzerooffsets,asevidencedbythenonzero 
slope line formed by the square symbols. That line represents the results of multiplying zero by a set 
of other quantities, so should be horizontal, at zero. Fig. 10 shows the output of a compensated multiplier 
cir­cuit. Note that the zero line (again delineated by the square symbols), is much closer to horizontal 
at zero, due to the effects of the compensation. It is appropriate to discuss accuracy and precision 
at this time. As a multiplier, the circuit is highly ac­curate: it computes a function that is very close 
to the desired e f (xy)= kxy. The precision is more dif.cult to quantify than the accuracy, however. 
The relative error quantity (0.1%) seems to indicate 10 bits of precision, although noise may reduce 
the repeatable precision to somewhat less than that. Although in this case we have only compensated for 
.rst-order effects of device variations, it is possible to design circuits which compensate for higher 
order nonlinearities as well. In order to use compensated components to produce an accurate and precise 
computational system, care must be taken to consider the quality and magnitude of errors that can be 
tolerated at each stage of the computation. Fig. 11 shows the compensated voltage-in, voltage-out mul­tiplier 
performance. The signal presented in this .gure is an 1.5e-06 1.5e-06 1e-06 1e-06 5e-07 5e-07 Output 
(amps) Output (amps) 0 -5e-07 -1e-06 -1.5e-06 -2.6 -2.4 -2.2 -2 -1.8 -1.6 -1.4 G1(X1) (volts) Figure 
10: The output from a compensated multiplier circuit (actual measured chip data). The relative error 
(ourput error / input range) is less than 0.1% over most of the operating range. At extreme (large) inputs, 
the relative error may be as large as 2%. For this application, the precision is most important for small 
values. intermediate value in the hierarchical constraint computation. Its nonlinearity and nonzero offset 
characteristics re.ect the fact that this output contains biases to compensate for variations in the 
next stage of computation. These curves represent the sum of the multiplier output and the compensation 
input for a subse­quent computational element. [Kirk 93] contains more detailed descriptions of hierarchical 
compensation techniques. 0.02 0.01 0 -0.01 -5e-07 -1e-06 -1.5e-06 -2.6 -2.4 -2.2 -2 -1.8 -1.6 -1.4 
G1(X1), G1(X2), G1(X3) (volts) Figure 12: The 3 components of a dot product (actual measured chip data). 
The characteristics of the three multiply operations are similar, with respect to the input offset magnitudes 
and shape of nonlinearities. Fig. 13 shows the results of a simulation of our constraint technique in 
action. Output (volts) -0.02 -0.03 -0.04 -0.05 -0.06 -0.07 Figure 13: The results of a software simulation 
of our constraint -2.6 -2.4 -2.2 -2 -1.8 -1.6 -1.4  G1(X1) (volts) technique in action. The outer curved 
octant represents the manifold of a set of points transformed using the input imperfect Figure 11: The 
output of a multiplier, after nearly linear current­ to-voltage conversion (actual measured chip data). 
matrix. The inner, more spherical shape represents the same points (and more) transformed through the 
constrained rotation matrix. The lines drawn between the two shapes represent the Fig. 12 shows the 
three multiply components of a compen-constraint optimization path taken by our algorithm. sated dot 
product. Note that the offset correction is very accurate, but that the linearity is somewhat less accurate. 
5.1 Expected Performance We compare the expected performance of the continuous-time analog VLSI rotation 
constraint chip to a software implementa­tion on a fast digital computer. Using the constraint algorithm 
described in this paper, and Euler s method to perform the op­timization, we expect that the orthonormalized 
matrix can be produced in about 75 microseconds on a roughly 100 M.op workstation. The multiplier core 
used in the analog VLSI rotation con­straint chip can easily be run at product rates in excess of 2 Mhz 
[Denyer 81]. Since the multipliers and constraint circuitry oper­ate in continuous time, we expect convergence 
at a much greater rate than in the discrete digital case. The analog VLSI rota­tion constraint chip should 
produce an orthonormalized matrix in roughly 2-3 microseconds. So, the current implementation should 
outperform a general-purpose digital solution by about a factor of 25, and we believe that this is a 
conservative esti­mate. Furthermore, the analog VLSI solution is extremely low cost, and low power, and 
leaves the workstation processor free to pursue other tasks. The analog VLSI chips were fabricated in 
2.0 micron CMOS using the MOSIS fabrication service, and dissipate power on the order of microwatts. 
The entire constraint solution circuit consumes roughly 2 square millimeters of chip area. Finally, faster 
multiplier circuits can be used to further increase the analog VLSI performance.  6 Conclusions We describe 
a constraint technique for producing orthogonal, unit scale rotation matrices from imperfect inputs. 
The tech­nique is potentially useful in a system which produces a sequence of approximate rotation matrices 
over time. Additional potential applications are covered brie.y in Sec. 1. We also describe the emerging 
and evolving technology of adaptive analog VLSI and speculate on its possible value to the .eld of computer 
graphics. In the example of the rotation system above, an analog VLSI rotation matrix constraint solver 
could enforce the rotation constraint continuously as the matrix is updated. Interpreting this result 
with a broader view, we have demon­strated the implementation of a nontrivial constraint in analog VLSI. 
This is signi.cant because it implies a future of imple­menting hardware for modeling in the form of 
hardware con­straint solution. Current digital implementations of constraint systems cannot compute real 
time constraint solutions for mod­els containing more than a few bodies. Many of the tasks in computer 
graphics simulation and mod­eling involve the solution of various types of mathematical equa­tions. The 
development of analog VLSI technology for accurate and precise computation [Kirk 93], makes it possible 
to build analog hardware to solve these equations. The use of CMOS VLSI fabrication makes analog implementations 
scalable and mass producible. Therefore, adaptive analog VLSI presents an exciting opportunity to consider 
building hardware to accelerate modeling to a level of performance commensurate with that of digital 
rendering hardware. We believe that Analog VLSI has the potential to be a signi.cant tool for computer 
graphics. 7 Acknowledgements This work was supported in part by an AT&#38;T Bell Laboratories Ph.D. 
Fellowship, and by grants from Apple, DEC, Hewlett Packard, and IBM. Additional support was provided 
by NSF (ASC-89-20219), as part of the NSF/DARPA STC for Computer Graphics and Scienti.c Visualization. 
All opinions, .ndings, conclusions, or recommendations expressedin this document are those of the authors 
and do not necessarily re.ect the views of the sponsoring agencies. Thanks also to the anonymous reviewers 
for their many helpful comments. References [Barkans 90] Barkans, Anthony C., High Speed High Quality 
Antialiased Vector Generation, Computer Graphics, Vol. 24, No. 4, August, 1990, pp. 319-326. [Barzel 
92] Barzel, Ronen, Structured Modeling for Computer Graphics, Academic Press, Cambridge, MA, 1992. [Clark 
82] Clark, James, The Geometry Engine: A VLSI Ge­ometry System for Graphics, Computer Graphics, Vol. 
16, No. 3, July, 1982, pp. 127-133. [Denyer 81] Denyer, Peter B., John Mavor, MOST Transcon­ductance 
Multipliers for Array Applications, IEEE Proceed­ings, Volume 128, Pt. I, Number 3, pp. 81-86, June 1981. 
[Fuchs 89] Fuchs, Henry, J. Poulton, J. Eyles, T. Greer, J. Gold­feather, D. Ellsworth, S. Molnar, G. 
Turk, B. Tebbs, and L. Israel, Pixel-Planes 5: A Heterogeneous Multiproces­sor Graphics System Using 
Processor-Enhanced Memories, Computer Graphics, Vol. 23, No. 3, July, 1989, pp. 79-88. [Kirk 93] Kirk, 
David B., Accurate and Precise Computation using Analog VLSI, with Applications to Computer Graphics 
and Neural Networks, Ph.D. Thesis, California Institute of Technology, Caltech-CS-TR-93-08, June, 1993. 
[Kirk 91] Kirk, David, Kurt Fleischer, and Alan Barr, Con­strained Optimization Applied to the Parameter 
Setting Prob­lem for Analog Circuits, IEEE Neural Information Process­ing Systems 1991 (NIPS 91), Morgan 
Kaufman, San Diego, 1991. [Mead 89] Mead, Carver, Analog VLSI and Neural Systems, Addison-Wesley, 1989. 
[Platt 89] Platt, John, Constrained Optimization for Neural Networks and Computer Graphics, Ph.D. Thesis, 
California Institute of Technology, Caltech-CS-TR-89-07, June, 1989. [Rhoden 89] Rhoden, Desi, and Chris 
Wilcox, Hardware Ac­celeration for Window Systems, Computer Graphics, Vol. 23, No. 3, July, 1989, pp. 
61-67. [Voorhies 88] Voorhies, Douglas, D. Kirk, and O. Lathrop, Vir­tual Graphics, Computer Graphics, 
Vol. 22, No. 4, August, 1988, pp. 247-253.  Appendix 1: Derivation of Constraint Equations The expression 
A : A can be written: X A : A = AjkAjk (6) jk So, we can rewrite Eqn. 3 as: XX r X  r . f(M) = (( 
MijMik) jk)(( MjMk) jk) (7) jk i whereij indicates the identity matrix (ij = 1 when i = j and 0 and, 
we can now write in terms of X, Y, and Z: 3 otherwise). In order to use Eqn. 7 to enforce a constraint, 
we would 11 = (D11 1)X1 D12Y1 D13Z1 (28) like to pose it in a form which allows us to do some sort of 
12 = D21X1(D22 1)Y1 (29) D23Z1 optimization. More speci.cally, in order to perform a gradient = D32Y1 
 descent operation, we require a gradient. So, we compute the 13 D31X1(D33 1)Z1 (30) gradient, using 
Einstein Summation Notation (ESN):21 = (D11 1)X2 D12Y2 D13Z2 (31) 22 = D21X2(D22 1)Y2 D23Z2 (32) f = 
(8) 23 = D31X2 D32Y2(D33 1)Z2 (33)Mpq 31 = (34) = 2(4(MMijiqMMikik jkqk)()Mppk jqMk + Mjpqk) (10)(9) 
32 = (D11 1)(XD322 D1)12YY33 DD1323ZZ33 (35)D21X3 =  r ss 0 r r : r   3 r rr r rr r 33 = D31X3 D32Y3(D33 
1)Z3 (36) We wish to use Eqn. 8 to perform gradient descent to mini­mize the function f (), as follows: 
M(t)= f (M(t)) (11) where epsilon is a parameter which determines the speed of the descent. We de.ne 
as the gradient of f ():  33 r pq = 4(MiqMik qk )Mpk (12) We can also simplify MiqMik by introducing 
B1, B2, and B3 as basis vectors of the matrix M, and Dij as the dot product of Bi and Bj: 3 f ir r pq 
= 4(BqBk qk)Mpk (13) = 4(Dqk qk)Mpk (14) Since the dot products are symmetric, there are only 6 unique 
Dqk terms: the 3 diagonal terms, D11, D22, and D33, and the three unique cross terms, D12 (or D21), D23 
(or D32), and D13 (or D31). So, the following set of equations describe a form of the gradient descent 
process: Mnew = Mold r :3:: pqpq pq (15) andwecanabsorbthe4fromEqn.13into ,since isanarbitrary constant. 
We have the following set of 9 equations for the components of the gradient: 11 = (D11 1)M11 + D12M21 
+ D13M31 (16) 12 = D21M11 +(D22 1)M21 + D23M31 (17) r r 13 = D31M11 + D32M21 +(D33 1)M31 (18) 21 = (D11 
1)M12 + D12M22 + D13M32 (19) 22 = D21M12 +(D22 1)M22 + D23M32 (20) 23 = D31M12 + D32M22 +(D33 1)M32 (21) 
31 = (D11 1)M13 + D12M23 + D13M33 (22) 32 = D21M13 +(D22 1)M23 + D23M33 (23) 3 rr r 33 = D31M13 + D32M23 
+(D33 1)M33 (24) We can de.ne B1= X, B2= Y, and B3= Z, so we can now write the discrete time step gradient 
descent optimization as: Xnew Xold = p1 (25) Ynew Yold = p2 (26) Znew Zold   r : 33 = p3 (27) 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166123</article_id>
		<sort_key>53</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Correcting for short-range spatial non-linearities of CRT-based output devices]]></title>
		<page_from>53</page_from>
		<page_to>56</page_to>
		<doi_number>10.1145/166117.166123</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166123</url>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>B.4.2</cat_node>
				<descriptor>Image display</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Grayscale manipulation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39074349</person_id>
				<author_profile_id><![CDATA[81332508787]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[R.]]></first_name>
				<middle_name><![CDATA[Victor]]></middle_name>
				<last_name><![CDATA[Klassen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xerox Webster Research Center, Webster, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39048930</person_id>
				<author_profile_id><![CDATA[81100567677]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Krishna]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bharat]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology, Atlanta]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Allebach, J. Binary display of images when spot size exceeds step size. Applied Optics 19, 15 (August 1980), 2513-2519.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807417</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Catmull, E. A tutorial on compensation tables. Computer Graphics 13, 2 (1979), 1-7.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>90980</ref_obj_id>
				<ref_obj_pid>90967</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Cole, A. Naive halftoning. In CG International '90 (1991), Springer-Verlag, pp. 203-222.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801163</ref_obj_id>
				<ref_obj_pid>964967</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Cowan, W. An inexpensive scheme for calibration of a colour monitor in terms of CIE standard coordinates. Computer Graphics 17, 3 (1983), 315-321.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Dong, C.-K. Perceptual printing of gray scale images. Master's thesis, MIT, 1992.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Fawcett, G., and Schrack, G. Halftoning techniques using error correction. In Proceedings of the SID (1986), vol. 27, no. 4, pp. 305-308.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Floyd, R., and Steinberg, L. An adaptive algorithm for spatial gray scale. In Society for Information Display 1975 Digest of Technical Papers (1975), pp. 36-37.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>34716</ref_obj_id>
				<ref_obj_pid>34713</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Griffiths, J., and Yang, C. Algorithms for generating improved images of curved surfaces by distributing errors along Hilbert's curve. Computer-Aided Design 19, 6 (July 1987), 299-304.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>916250</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Klassen, R. Device Dependent Image Construction for Computer Graphics. PhD thesis, University of Waterloo, 1989. Available as technical report #CS-91-19.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Lyons, N., and Farrell, J. Linear systems analysis of crt displays. In Society for Information Display 89 Digest (1989), vol. 20, pp. 220-223.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Naiman, A., and Makous, W. Spatial non-linearities of grayscale crt pixels. In Proc. SPIE vol 1666, Human Vision, Visual Processing and Digital Display III (1992), pp. 41-56.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Pappas, T., and Neuhoff, D. Model-based halftoning. In Proc. SPIE/IS&amp;T Symposium on Electronic Imaging Science and Technology, Human Vision, Visual Processing, and Digital Display H (1991).]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Roetling, P., and Holladay, T. Tone reproduction and screen design for pictorial electrographic printing. Journal of Applied Phot. Eng. 15, 4 (1979), 179-182.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122727</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Velho, L., and Gomes, J. Digital halftoning with space filling curves. In Proceedings SIGGRAPH '91 (1991), pp. 81-90.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Whitten, I., and Neal, R. Using Peano curves for bilievel display of continuous tone images. IEEE Computer Graphics and Applications 202 (May 1982), 47-52.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>139894</ref_obj_id>
				<ref_obj_pid>139834</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Wyvill, G., and McNaughton, C. Three plus five makes eight: a simplified approach to halftoning. In CG International '91 (1991), Springer-Verlag, pp. 397-392.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Correcting for Short-Range Spatial Non-Linearities of CRT-based Output Devices R. Victor Klassen Xerox 
Webster Research Center ABSTRACT Most graphical output devices exhibit what has been termed spatial non-linearity: 
the effect of setting two adjacent pixels to a given value is not the same as the sum of the effects 
of setting those two pixels to the same value in isolation: checkerboards of different frequencies do 
not have the same apparent luminance. We present a method applicable to bit-mapped devices for compensating 
for short-range spatial non-linearity in error-diffused images. The modi.cation to error diffusion is 
such that it can be used with any error diffusion technique. In essence, it consists of .nding the in.uence 
of the neighbouring (output) pixels when making the decision of whether to turn on a given pixel, and 
passing errors computed accordingly. CR Descriptors: B.4.2 [Input/Output and Data Com­munications]: Input/Output 
Devices Image display; I.3.1 [Computer Graphics]: Hardware Architecture Raster display devices I.3.3 
[Computer Graphics]: Picture/image generation Display algorithms; I.3.6 [Computer Graphics]: Methodology 
and Techniques; I.4.3 [Image Processing]: enhancement. 1 Introduction While the full-colour display is 
becoming more and more common, bit-mapped CRTs remain commonplace as well. These have advantages in terms 
of speed, resolution, and cost that cannot be matched by colour displays. Occasionally it is necessary 
to display an image on such a device. Moreover, certain colour­table animation techniques rely on the 
use of single bit-planes of a full-colour display. Here the full colour display is being used to simulate 
a bit-mapped display with a very fast frame update rate. A common method of converting from full-colour 
continuous tone to black and white binary is to error diffuse the luminance component. Various forms 
of error diffusion have been suggested[7, 15, 6, 8, 3, 16, 14]; the particular choice of error diffusion 
technique has relatively little effect on the appearance of an image when it is displayed on a suf.ciently 
high-resolution monitor. Xerox Corporation, Webster Research Center, Building 128 27E, 800 Phillips Road, 
Webster, NY 14580. College of Computing, Georgia Institute of Technology, 801 Atlantic Drive, Atlanta, 
GA 30332 0280 Permission to copy without fee all or part of this material is granted provided that the 
copies are not made or distributed for direct provided that the copies are not made or distributed for 
direct commercial advantage, the ACM copyright notice and the title of the commercial advantage, the 
ACM copyright notice and the title of the publication and its date appear, and notice is given that copying 
is by publication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Machinery. To copy permission of the Association for Computing Machinery. otherwise, or 
to republish, requires a fee and/or specific permission. otherwise, or to republish, requires a fee and/or 
specific permission. &#38;#169;1993 ACM -0 -89791 -601 -8/93/008 $1.50 &#38;#169;1993 ----8/93/008/0015 
$1.50   Krishna Bharat Georgia Institute of Technology The value of gamma-correction of colour displays 
(or better still instrumented compensation)[2, 4], is well known. On a bit­mapped display the concept 
of gamma-correction is meaningless. As Naiman has noted, CRTs exhibit spatial non-linearities [11], , 
as can be easily seen by displaying a checkerboard of period two pixels adjacent to a checkerboard of 
twice that period. When viewed from a suf.cient distance to cause the coarser checkerboard to appear 
smooth, these two images should ideally appear the same intensity. On most output devices they do not. 
(An LCD display may be an exception). Much has been said about correcting for neighbourhood effects in 
prints. Commonly, it is based on a simple model of circular pixels with greater than unit area [13, 1, 
12, 5]. For the SIGGRAPH audience, two more important display devices are the CRT and the .lm recorder. 
We begin with the simplest example: the bit-mapped CRT. Bit-mapped CRTs are so common that most readers 
of this paper are likely to have one. The improvement can be quite striking, as shown by .gures 1 and 
2. A linearity assumption (ie. that the phosphors are not saturated) allows the extension to greyscale 
and colour monitors, and to .lm recorders. 2 SIMPLE CRT CORRECTION The general idea behind neighbourhood-based 
compensation is that the intensity generated at a pixel depends not only on the setting of that pixel 
but also on the intensity of the neighbouring pixels. The CRT is a special case. Here the non-linearities 
are primarily in the ampli.ers driving the electron gun(s), so it is suf.cient to consider only the left 
and right neighbours (whichever have been visited). An isolated pixel does not contribute as much intensity 
as it would with its neighbour on. . Neighbours in adjacent scanlines have no effect under this assumption 
(valid for CRTs [10]). To test the assumption of independent scanlines, display four images: with a) 
alternate scanlines b) alternate columns c) alternate pairs of scanlines, and d) alternate pairs of columns 
intensi.ed. If scanlines are independent, a) and c) should have the same intensity. In the unlikely event 
that b) and d) appear the same, the monitor has excellent high frequency response, and no correction 
is necessary. If .icker causes a problem with interlaced displays when displaying single scanlines it 
can be alleviated by using a checkerboard and changing only the vertical frequency. The second assumption 
is one of single neighbours contributing. This can be tested using a pattern of decreasing frequency 
vertical lines. For the (SONY) monitors we tested, the difference between single and double pixel lines 
was much greater than that between double and triple pixel width lines, so the assumption appears safe. 
 Figure 1 A radiosity-like scene, error diffused without correction. Note the dark band in the shadow. 
(The image was enlarged to 150% actual size to improve reproduction. It is best viewed from 2m/6 7 feet.) 
To correct for the presence or absence of a neighbouring pixel, the algorithm in the CRT case is as follows: 
for each pixel if no neighbouring output pixel is on (white) if value (including errors passed in) > 
threshold 0 . set the pixel quantization error . value 0 (1 0 . ) else value = threshold 0 . quantization 
error . value else a neighbouring output pixel is on if value (including errors passed in) > threshold 
set the pixel quantization error . value 0 1 else value = threshold quantization error . value Diffuse 
quantization error in the normal way If there is no neighbouring pixel on, the effect of turning the 
current pixel on is reduced. This is re.ected both in the turn-on decision, and in the calculation of 
the quantization error. The speci.cation deliberately leaves open the choice of error diffusion algorithm, 
including the order in which pixels are visited. Left and right neighbours are treated equally, although 
in reality pixels are only affected by the state of their left neighbours. The result of processing some 
pixels in right to left order, rather than left to right, results in the same average intensity overall, 
with a slight phase shift. The value of . must be determined experimentally: to do so, display a checkerboard 
containing 222 squares adjacent to a region of mid-grey that has been error diffused using the modi.ed 
error diffusion algorithm. Vary . across the error diffused region (Figure 3), and .nd the point where 
the two regions have the same luminance. We have found values in the 5 30% range apply to the monitors 
we tried. Figure 4 is a photograph of a screen with the pattern of Figure 3 displayed on the screen. 
The crossover point on the screen photographed is about midway across the Figure 2 After correction the 
shadow fades smoothly through its penumbral region.   .gure (the process of photographing and printing 
the image may have changed the crossover point in the picture).  3 GREY SCALE MONITOR OR FILM RECORDER 
The difference between a bit-mapped monitor and a greyscale one is the frame buffer behind it. Both employ 
an electron beam directed at phosphors; the spatial non-linearity effects are identical. As long as images 
displayed on greyscale monitors do not have high frequency information in them, their spatial non­linearities 
will be hidden. Where high contrast edges appear, the non-linearities can affect image quality. Fortunately, 
spatial non­linearities due to gun ampli.er non-linearity are close enough to intensity invariant that 
the methods above can be safely generalized. Before proceeding to correct for spatial non-linearities, 
it should be ascertained that the monitor is corrected for gun non­linearities. Given an otherwise corrected 
monitor, the value of . can be determined as above, using patterns of full-on, full-off. It is not normal 
to error diffuse images unless the display is operating from a low depth frame buffer (eg. 8 bits for 
all  a Figure 4 The result of displaying a pattern similar to that shown in Figure 3. three components). 
If it is, the error diffusion algorithm can be adjusted in the same way as described above. In the typical 
case of a 24 (or higher) bit frame buffer, error diffusion can still be applied, without the quantization 
step. Normally there would be no error generated, but the alteration to the input values can still be 
applied, possibly generating out-of-gamut values. For example, a white pixel immediately followed by 
a black pixel would lead to a request for a negative pixel value for the second one. A remapping of the 
input (reducing the contrast) can prevent such negative pixel values entirely. A partial contrast reduction 
can make such negative pixel values infrequent. This is similar to eliminating phosphor trails in temporally 
varying displays, as described in [9]  4 SUMMARY &#38; CAVEAT We have described a simple technique for 
improving the tonal reproduction accuracy of CRTs. For bit-mapped displays, it serves the usual function 
of gamma correction. For regular CRTs it performs in image regions of high spatial frequency what gamma 
correction or instrumented compensation does in image regions of low spatial frequency. The method involves 
very little extra computation over that required for conventional error diffusion, and is simple to implement 
and calibrate. It should be noted that the generalization to print is complicated by the larger neighbourhoods 
affecting pixels, two (spatial) dimensional interactions, and non-linear colour mixing in the case of 
coloured printing. REFERENCES [1] Allebach, J. Binary display of images when spot size exceeds step 
size. Applied Optics 19, 15 (August 1980), 2513 2519. [2] Catmull, E. A tutorial on compensation tables. 
Computer Graphics 13, 2 (1979), 1 7. [3] Cole, A. Naive halftoning. In CG International 90 (1991), Springer-Verlag, 
pp. 203 222. [4] Cowan, W. An inexpensive scheme for calibration of a colour monitor in terms of CIE 
standard coordinates. Computer Graphics 17, 3 (1983), 315 321. [5] Dong, C.-K. Perceptual printing of 
gray scale images. Master s thesis, MIT, 1992. [6] Fawcett, G., and Schrack, G. Halftoning techniques 
using error correction. In Proceedings of the SID (1986), vol. 27, no. 4, pp. 305 308. [7] Floyd, R., 
and Steinberg, L. An adaptive algorithm for spatial gray scale. In Society for Information Display 1975 
Digest of Technical Papers (1975), pp. 36 37. [8] Grif.ths, J., and Yang, C. Algorithms for generating 
improved images of curved surfaces by distributing errors along Hilbert s curve. Computer-Aided Design 
19, 6 (July 1987), 299 304. [9] Klassen, R. Device Dependent Image Construction for Computer Graphics. 
PhD thesis, University of Waterloo, 1989. Available as technical report #CS-91-19. [10] Lyons, N., and 
Farrell, J. Linear systems analysis of crt displays. In Society for Information Display 89 Digest (1989), 
vol. 20, pp. 220 223. [11] Naiman, A., and Makous, W. Spatial non-linearities of grayscale crt pixels. 
In Proc. SPIE vol 1666, Human Vision, Visual Processing and Digital Display III (1992), pp. 41 56. [12] 
Pappas, T., and Neuhoff, D. Model-based halftoning. In Proc. SPIE/IS&#38;T Symposium on Electronic Imaging 
Science and Technology, Human Vision, Visual Processing, and Digital Display II (1991). [13] Roetling, 
P., and Holladay, T. Tone reproduction and screen design for pictorial electrographic printing. Journal 
of Applied Phot. Eng. 15, 4 (1979), 179 182. [14] Velho, L., and Gomes, J. Digital halftoning with space 
.lling curves. In Proceedings SIGGRAPH 91 (1991), pp. 81 90. [15] Whitten, I., and Neal, R. Using Peano 
curves for bilievel display of continuous tone images. IEEE Computer Graphics and Applications 202 (May 
1982), 47 52. [16] Wyvill, G., and McNaughton, C. Three plus .ve makes eight: a simpli.ed approach to 
halftoning. In CG International 91 (1991), Springer-Verlag, pp. 397 392.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166125</article_id>
		<sort_key>57</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Pad]]></title>
		<subtitle><![CDATA[an alternative approach to the computer interface]]></subtitle>
		<page_from>57</page_from>
		<page_to>64</page_to>
		<doi_number>10.1145/166117.166125</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166125</url>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Windowing systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.2.2</cat_node>
				<descriptor>X-Window</descriptor>
				<type>P</type>
			</other_category>
			<other_category>
				<cat_node>I.7.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011074.10011075.10011079.10011080</concept_id>
				<concept_desc>CCS->Software and its engineering->Software creation and management->Designing software->Software implementation planning->Software design techniques</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010500.10010501</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document management->Text editing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011074.10011081</concept_id>
				<concept_desc>CCS->Software and its engineering->Software creation and management->Software development process management</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011053</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Window managers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39077218</person_id>
				<author_profile_id><![CDATA[81100250413]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Perlin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31078218</person_id>
				<author_profile_id><![CDATA[81539164556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fox]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>151235</ref_obj_id>
				<ref_obj_pid>151233</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Sara Bly et. al.,Media Spaces: Bringing People Together in a Video, Audio, and Computing Environment, CACM, Vol. 36, 1993, No. 1., pp. 28-47.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>247</ref_obj_id>
				<ref_obj_pid>245</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Peter Burt, A multiresolution spline with applications to image mosaics, ACM Transactions on Graphics, Vol. 2, No. 4, Oct. 1983, pp. 217-236.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360354</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[James H. Clark. Hierarichical geometric models for visible sulface algorithms. ACM Communications, Vol. 19, No. 10, Oct. 1976, pages 547-554.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807391</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[William C. Donelson, Spatial Management of Information, ACM SIGGRAPH 1978 Conference Proceedings.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[H. Edelsbrunner, A new approach to rectangle intersections, Part H, Int'l Journal of Computational Mathematics, No. 13, pp. 221-229, 1983.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97933</ref_obj_id>
				<ref_obj_pid>97924</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[S. Feiner and C. Beshers, Worlds within worlds: Metaphors for exploring n-dimensional virtual worlds. Proc. UIST '90 (ACM Syrup. on User Interface Software and Technology), Snowbird, UT, Oct. 3-5, 1990, pp. 76-83.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Matthew Fuchs, unpublished Ph.D. dissertation in progress.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>22342</ref_obj_id>
				<ref_obj_pid>22627</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[George Furnas, Generalized Fisheye Views, Human Factors &amp; Computer Systems, CHI 89 Conference proceedings, pp. 16- 23.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>151238</ref_obj_id>
				<ref_obj_pid>151233</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Ralph Hill, et. al., The Rendezvous Language and Architecture, CACM Vol. 36, 1993, No. 1., pp. 62-67.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Hypertext on Hypertext, Macintosh Version: Disk #1 and #2. ACM Press, New York, 1988.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[I. Lu et. al., Idea management in a shared drawing tool. Proceedings of the Second European Conference on Computer-Supported Cooperative Work-ECSCW '91, Amsterdam, Holland, 1991.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97898</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[J. Mackinlay et. al.,Rapid Controlled Movement Through a Virtual 3D Workspace. ACM SIGGRAPH 1990 Conference Proceedings.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Nelson Max, ACM SIGGRAPH 1975 Film show.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>108893</ref_obj_id>
				<ref_obj_pid>108844</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Minneman, S. and Bly, S.A.Managing a trois: A study of a multi-user drawing tool in distributed design work, Proceedings of the CHI'91 Conference on Human Factors in Computer Systems., New Orleans, La., 1991.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Ted Nelson, Literary Machines. Swarthmore, PA, 1981.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>153582</ref_obj_id>
				<ref_obj_pid>255950</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Jakob Nielsen, Non-command User Interfaces, CACM, Vol. 36 No. 4, (April 1993), pp. 83-99.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Ken Perlin and Luis Velho, A Wavelet Representation for Unbounded Resolution Painting, NYU Technical Report.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>4333</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Franco P. Preparata, Michael Ian Shamos, Computational Geometw: An Introduction, Springer Verlag, New York, 1989.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[David Small, Masters Thesis, MIT Media Laboratory, 1989.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Randall B. Smith, Tim O'Shea, Claire O'Malley, Eileen Scanlon, and Josie Taylor. Prelimina~7 Experiments with a distributed, multi-media, problem solving environment. In Proceedings of the First European Conference on Computer Supported Cooperative Work (Gatwick, UK) 1989, pages 19- 34.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97302</ref_obj_id>
				<ref_obj_pid>97243</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[J.C. Tang and S.L. Minneman, Videodraw: A video intelface for collaborative drawing. Proceedings of the CHI '90 Conference on Human Factors in Computing Systems, Seattle, Wash., 1990.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[S. L. Tanimoto, and T. Pavlidis, A hierarchical data structure for picture processing. Computer Graphics and Image Processing, Vol. 4, 1975, pp. 104-119.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>33404</ref_obj_id>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Edward Tufte, The Visual Display of Quantitative Information, Graphics Press, 1983.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[M. Weiser, The Computer for the 21st Centu~7, Sci. Am. 265,3 (September 1991), pp. 94-104.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801126</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Lance Williams, Pyramidal Parametrics. ACM SIGGRAPH 1982 Conference Proceedings.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Lance Williams, personal communication.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Pad An Alternative Approach to the Computer Interface Ken Perlin David Fox Courant Institute of Mathematical 
Sciences New York University 719 Broadway 12th Floor New York, NY 10003 Abstract We believe that navigation 
in information spaces is best supported by tapping into our natural spatial and geographic ways of thinking. 
To this end, we are developing a new computer interface model called Pad. The ongoing Pad project uses 
a spatial metaphor for computer interface design. It provides an intuitive base for the support of such 
applications as electronic marketplaces, information services, and on-line collaboration. Pad is an infinite 
two dimensional information plane that is shared among users, much as a network file system is shared. 
Objects are organized geographically; every object occupies a well defined region on the Pad surface. 
For navigation, Pad uses portals - magnifying glasses that can peer into and roam over different parts 
of this single infinite shared desktop; links to specific items are established and broken continually 
as the portal s view changes. Portals can recursively look onto other portals. This paradigm enables 
the sort of peripheral activity generally found in real physical working environments. The apparent size 
of an object to any user determines the amount of detail it presents. Different users can share and view 
multiple applications while assigning each a desired degree of interaction. Documents can be visually 
nested and zoomed as they move back and forth between primary and secondary working attention. Things 
can be peripherally accessible. In this paper we describe the Pad interface. We discuss how to efficiently 
implement its graphical aspects, and we illustrate some of our initial applications.  Introduction Imagine 
that the computer screen is a section of wall about the size of a typical bulletin board or whiteboard. 
Any area of this surface can then be accessed comfortably without leaving one s chair. Imagine further 
that by applying extraordinarily good eyesight and eye-hand coordination, a user can both read and write 
as comfortably on any micron wide section of this surface as on any larger section. This would allow 
the full use of a surface which is several million pixels long and high, on which one can comfortably 
create, move, read and compare information at many different scales. Permission to copy without fee all 
or part of this material is granted provided that the copies are not made or distributed for direct 
provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright 
notice and the title of the commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. permission of the 
Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific 
permission. otherwise, or to republish, requires a fee and/or specific permission. &#38;#169;1993 8/93/008/0015 
$1.50 &#38;#169;1993 ACM - 0 - 89791 - 601 - 8/93/008 $1.50 The above scenario would, if feasible, put 
vast quantities of information directly at the user s fingertips. For example, several million pages 
of text could be fit on the surface by reducing it sufficiently in scale, making any number of on-line 
information services, encyclopedias, etc., directly available. In practice one would arrange such a work 
surface hierarchically, to make things easier to find. In a collaborative environment, one could then 
see the layout (in miniature) of many other collaborators surfaces at a glance. The above scenario is 
impossible because we can t read or write at microscopic scale. Yet the concept is very natural since 
it mimics the way we continually manage to find things by giving everything a physical place. A good 
approximation to the ideal depicted would be to provide ourselves with some sort of system of magic magnifying 
glasses through which we can read, write, or create cross-references on an indefinitely enlargeable ( 
zoomable ) surface. This paper describes the Pad interface, which is designed using these principles. 
1.1 Overview of the Paper We begin section one with a brief summary of the basic ideas and components 
of the Pad Model. We then finish section one with a comparison of Pad to the window/icon paradigm and 
a summary of prior work. Section two is a description of a typical Pad application, and section three 
covers the principles of the Pad system. Section four covers several issues in our implementation of 
Pad, and section five lists some ongoing and future projects. Finally, section six presents our conclusions 
and acknowledgments. 1.2 Basic Pad Model The Pad Surface is an infinite two dimensional information 
plane that is shared among users, much as a network file system is shared. It is populated by Pad Objects, 
where we define a Pad Object to be any entity that the user can interact with (examples are: a text file 
that can be viewed or edited, a clock program, a personal calendar). Pad Objects are organized geographically; 
every object occupies a well defined region on the Pad surface. To make themselves visible, Pad Objects 
can create two types of ink, graphics and portals, and place them on the Pad Surface. A graphic is simply 
any sort of mark such as a bitmap or a vector. Portals are used for navigation, they are like magnifying 
glasses that can peer into and roam over different parts of the Pad Surface. A portal may have a highly 
magnified view or a very broad, panoramic view, and this view can be easily changed. The screen itself 
is just a special root portal. A portal is not like a window, which represents a dedicated link between 
a section of screen and a specific thing (e.g.: a Unix shell in X-Windows or a directory in the Macintosh 
Finder). A portal is, rather, a view into the single infinite shared desktop; links to specific items 
are established and broken continually as the portal s view changes. Also, unlike windows, portals can 
recursively look onto (and into) other portals. Figure 1 shows a very large financial document on the 
Pad surface. The small portal at the top of the figure shows an overview of the entire report. The two 
other portals show successive closeups of portions of the report. 1.3 Object/Portal Interaction A Pad 
object may look quite different when seen through different portals. There are two techniques that allow 
objects vary their appearance: semantic zooming and portal filters. Every object visible on the screen 
has a magnification that depends upon the sequence of portals it is being seen through. As the magnification 
of an object changes, the user generally finds it useful to see different types of information about 
that object. For example, when a text document is small on the screen the user may only want to see its 
title. As the object is magnified, this may be augmented by a short summary or outline. At some point 
the entire text is revealed. We call this semantic zooming. Semantic zooming works using the expose event, 
which says that a particular portion of the Pad Surface will be rendered at a particular magnification. 
When an object receives this event it generates the display items needed to give an appropriate appearance 
at that magnification. Objects can also manage portal filters - portals that show non­literal views of 
cooperating objects. For example, a portal may show all objects that contain tabular data as a bar chart, 
but display other objects as would any other portal. This would enable an application to embed a bar 
chart within a document by placing in it a portal filter that looks onto an object that contains tabular 
data. Another application can then allow text or spreadsheet style editing of the tabular data itself 
by some user. These edits will be seen as changes in the bar chart by any user who is looking at the 
document. The effect is that the bar chart filter portal will see any tabular data as a bar chart, but 
will see other objects in the usual way. Portal filters work by intercepting the expose event for objects 
which it knows how to render. It then asks the object or objects for any information it needs to create 
the display items to render them. Another interesting portal filter would be a control modifier. Imagine 
for example that a paint program has several types of brush. Normally one would click on an image of 
a particular brush to select it. When seen through a control modifier portal filter, each brush image 
would appear as a panel of parameter controls with which the user can change that brush s internal state 
(width, spattering law, etc). The same portal filter could be used to modify the controls of any application 
on Pad that recognizes its message conventions. 1.4 Pad vs. the Window/Icon Paradigm An important distinction 
between the Pad universe and the universe of other window systems is that in Pad every interaction object 
possesses a definite physical location. In this sense Pad is a two dimensional virtual reality. Yet a 
user s changing view can allow objects to appear larger or smaller. This paradigm allows for the sort 
of peripheral activity found in real physical working environments. Each object on a user s screen commands 
a degree of attention commensurate with how big the object appears to that user. This allows each object 
to vary the amount of detail it presents to each user. Different users can share and view multiple applications 
while assigning to each one a desired degree of interaction. Documents can be visually nested and zoomed 
as they move back and forth between primary and secondary working attention. Things can be peripherally 
accessible. For example, on the Macintosh desktop a user double clicks on a folder icon to see the contents 
of a directory in a window. But to see the contents of any folder within that folder, the user must double 
click to create a separate window. In comparison, a user of Pad generally views a directory through a 
portal. The contents of any subdirectories are visible, in miniature, through sub-portals. This allows 
the user a peripheral awareness of a subdirectory s contents, without the user having to perform any 
explicit action. In this sense, Pad is better suited to non-command user interfaces [16]. 1.5 Prior 
and related work A number of researchers developed ways to visually structure interactive information 
that offer an alternative to windows/icons. One of the first such systems was the Spatial Data Management 
System [4] at MIT, which presented an information landscape on two screens: one screen for a panoramic 
overview and another (application) screen providing a closer view. The user could either pan locally 
around on the application screen or else could go directly to an area by pointing on the panoramic view. 
On the other hand, Hypertext systems [15][10] allow the user to jump from one place to another in a conceptual 
information space. A notable problem with the current state of hypertext systems is the difficulty of 
knowing one s location in this space; unless the application is designed very carefully the user can 
easily get lost. In other related work, many desktop publishing systems provide tiny thumbnail sketches 
of images that are stored on disk. To open an image file the user simply points to these miniature images 
instead of specifying a file name. A unique approach to providing peripheral information has been developed 
by George Furnas at Bellcore Applied Research. His Fisheye user interface [8] shows information of current 
interest in great detail, while showing a progressively less detailed view of surrounding information. 
Also, some of the components of fast image zooming have existed for a while. Williams [25] has used a 
pyramid of images for texture filtering, and Burt [2] for image processing, both based on the prior work 
of Tanimoto [22]. The Bad Windows interface [19] allows drawings to be accessed at multiple levels of 
detail. Three dimensional interactive virtual offices that allow a user to change viewpoint are being 
developed by Mackinlay et. al. as well as Feiner [12][6]. Changes of scale have long been used in computer 
graphics for both entertainment and for scientific visualization [3]. One notable early example was the 
molecular simulation work of Nelson Max [13]. At Xerox PARC there has been a large body of interesting 
work on enabling groups to remotely share a common drawing surface for collaborative work [11][14][21]. 
This is part of their larger ongoing research effort in shared Media Spaces [1]. Similarly, the Rendezvous 
system at Bellcore is a general meta­system for building shared conversational interfaces for teleconferencing 
situations [9], as is the work of Smith et. al. [20]  2 An Example Application The multiscale daily/monthly 
calendar is a study of semantic zooming. Figures 2 through 4 show what the calendar looks like at various 
successive magnifications. At any level, the user can type or draw on the calendar. As the user zooms 
away from the scale at  Figure 1: Quarterly report. Portals are views onto other parts of the Pad surface. 
 Figure 2: As you approach the calendar object the large scale display items fade out and disappear. 
 which the annotations were drawn they become first translucent, then invisible. In this way, a user 
can overlay many levels of annotation on a calendar without confusion. The major problem with an application 
of this type is that it can involve a large number of display items, since the spatial density of display 
items on the Pad grows geometrically as the user zooms into the calendar. Yet at any one time only a 
fairly small number of display items is visible, since as the user zooms in the screen occupies an ever 
smaller absolute area on the Pad. We address this problem by designing the calendar object as an expandable 
semantic tree, and identifying display items with different nodes of this tree. Each time the calendar 
is displayed this semantic tree is traversed. As each node is reached, display items are generated as 
needed. Individual display items are ephemeral - if an item is off the screen for a while it is quietly 
removed by the calendar object. In this way the total number of display items always remains manageably 
small. This general notion of a geographic database that will expand and self-prune as the user roams 
around the Pad has now been encapsulated in a Scheme library called an ephemeral database manager. We 
plan to apply this library to other Pad applications that have an inherently tree structured semantics. 
 3 System Structure In this section we introduce the abstract data types needed to implement Pad. First 
we will describe the concepts necessary for display, then those needed to support interaction. 3.1 Addresses 
and Regions A Pad address A = (x, y, z) has both a location and a scale, and defines the linear transformation 
TA:(u,v) . (x + u2z ,y + v2 z ). Here z represents the log2 of scale. A Pad region R = [A, w, h] is a 
rectangle defined by an address together with a raster width and height (w, h). A region covers the portion 
of the Pad surface from TA (0,0) to TA (w ,h), or from (x, y) to (x +u2z ,y + v2z ).  3.2 Display Items 
The lowest level entities in the Pad universe are the display items, which come in two basic types: graphic 
and portal. Display items are the only entities actually visible on the user s screen. A graphic consists 
of a raster image I and an address A. Every display item is said to have a region [A, Iw , Ih ], which 
is the portion of the pad surface which it occupies. A portal is a graphic that has an additional address, 
called its look-on L. Using its raster image I as a mask, a portal have as its look-on the region [L,Iw 
,Ih ] on the Pad surface. The portion of the Pad surface which the look-on covers and which is not masked 
by the portal s graphic is visible at the location of the portal s region. This raster masking enables 
a portal to give a shaped view onto the Pad surface. Thus, a portal can be square, round, or even shaped 
like some well known corporate logo. We refer to a display item s Az as its scale. In general, a display 
item becomes visible on the screen only after being viewed through a succession of portals, each of which 
may transform it. We refer to a display item s apparent z, as it is seen on the screen, as its magnification. 
The image on the user s screen is created from a set of display items. There is one portal associated 
with the user s screen called the root portal ; the display process consists of rendering the root portal. 
This means rendering the region of the Pad surface which the root portal looks onto. Those display items 
that overlap the root portal s look-on are rendered. This procedure is then applied recursively to render 
any display item which is itself a portal. As the display process recurses through each portal, the transformation 
T (A)T -1(L) is applied, where A is that portal s address and L is that portal s look-on. This recursion 
can be expanded to compute the location of any display item on the screen. Suppose item i is viewed through 
successively nested portals p1 pn . Then to determine where (and at what magnification) to display i 
on the screen, we apply the transformations: T -1(Lroot )T (Ap1)T -1( Lp1) T( Apn )T -1(Lpn )T (Ai ) 
Incrementing the z component of a display item s address will increase its magnification. Incrementing 
the z component of a portal s look-on will double the size of its looked-on region - and will therefore 
decrease the magnification of every item seen through it. (Think of it as increasing the viewer s altitude.) 
There are several other properties of primitive display items which are important to note: Visibility 
Range: Each graphic object can have a range of magnification outside of which it is invisible. This is 
important since most display items are only useful within a certain range of magnification. Transparency 
Range: Similarly, each graphic can have a range of magnification outside of which the graphic is transparent. 
This allows objects to fade away gracefully as they are magnified up or down. Transparency is achieved 
by masking with a patterned pixel mask at screen resolution. Private Display Items: Display items may 
be attached to a portal, in which case they are only visible when viewed through that portal and their 
addresses are relative to that of the portal. This creates a hierarchy of display items and is used to 
implement the filters described below. 3.3 Pad Objects Graphics and Portals suffice to make an interesting 
multi-scale drawing program. However to use Pad as a system for building general user interfaces requires 
a higher level structure called a Pad Object to interpret events and control these display items so they 
behave as a single application. In Pad an object consists of a region together with a package of code 
and data which respond to event messages. An object s behavior is specified by the application developer. 
In order to make itself seen, each object manages a collection of display items, creating, modifying, 
and deleting them. Pad Objects receive events from the user s mouse and keyboard, plus timer events, 
channel events (events representing other types of input, e.g. the output of a process), and expose events 
which inform the object that some portion of itself will become visible on someone s screen. Events which 
would normally have an x-y location have instead an address, and this address is transformed if the event 
passes through a portal before being received by an object which is interested in it. Similarly, an expose 
event covers a region rather than just a rectangle, and this region is also transformed by portals so 
that each object can be informed which portion of its region will be rendered and at what magnification. 
Objects are maintained in an order, just as display items have a drawing order, so that if two or more 
objects are at the mouse address the mouse events are sent to the one in front. The object may use this 
event for its own purposes, or it may pass the event on to the objects behind it, or it may transform 
the event s address and pass it on to some other part of the Pad. Events thus passed may go unused by 
the objects below, in which case the original object may then use the event for its own purposes. 3.4 
Display Display is complicated by the fact that objects may be continually creating and destroying display 
items. Before we can create the display we first need to give each object an opportunity to know at what 
magnification it will be called upon to appear, since this will probably influence what display items 
it chooses to show. Therefore display is a two phase process. In the first phase, each object gathers 
all the necessary information about what portions of it will appear on the screen and at what magnifications. 
During this first phase display items may be spawned. In the second phase the screen image is actually 
drawn. During phase one each portal is displayed by having the Pad object that controls it communicate 
with all objects that in- tersect the portal s look-on region. This process begins with a special root 
object, which controls the user s root portal. For a portal controlled by an object O1 the procedure 
is as follows: O1 sends an expose event for the portal s look-on region. This event will be received 
by all objects whose regions intersect the portal s look-on region.  for each object O2 that responds: 
 -O1 tells O2 to produce display items for itself with the proper magnification and clip. If O2 controls 
any portals, the procedure is invoked for them recursively. -any display items that O1 receives back, 
it attaches to the portal. This process continues recursively until all items large enough to see on 
the screen are accounted for. In the second phase, each portal is painted from its accumulated list of 
display items. This process starts with the root portal, and continues on through all portals seen by 
the root portal, and then recursively through those portals. Note that if two portals on the screen have 
overlapping look-on regions, their lists may have display items in common. 3.5 Interacting Objects and 
Portals Semantic zooming is implemented by having the object s display method depend upon its magnification. 
The object is always told its magnification during display phase one. Portal filters are implemented 
as follows. Consider the case of the bar chart filter portal described earlier. Suppose this portal filter 
is managed by object O1. During phase one of the portal display procedure, O1 sends an expose event for 
this portal, and receives a number of acknowledgments. Suppose O1 has just received such an acknowledgment 
from object O2. O1 queries O2 to find out whether O2 is a tabular object. If yes, then O1 gets the tabular 
data from O2 , builds its own display items for the bar chart, and attaches these to the portal. If no, 
then O1 asks O2 to produce a list of display items as usual. The effect is that the filter portal will 
see any tabular data as a bar chart, but will see other objects in the usual way.  4 Implementation 
Details The Pad system is written in three layers, a real-time display layer written in C++, a Scheme 
interpreter providing an interface to the C++ layer, and a collection of Scheme code implementing the 
Pad application interface. It currently runs under X Windows and MS-DOS. The X Windows version has been 
compiled and run on SunOS, AIX and Linux. The source code of the most recent released version is available 
via anonymous FTP from cs.nyu.edu in the directory pub/local/perlin. 4.1 Rendering Display Items It is 
absolutely essential to our system that arbitrarily scaled bitmaps can be displayed in real time. Without 
an algorithm to achieve this, our desktop model would either require special purpose hardware, or else 
would lose real-time response. Either scenario would limit the model s general usefulness on typical 
currently available graphical workstations. The method we use to render the raster image of a graphic 
item depends upon the item s magnification. The following decisions are based on our trial and error 
experiences; they reflect our best results in tuning this process. We use four different techniques for 
drawing the raster image of a graphic, depending on the range of magnification m. m > 16. At the largest 
magnifications it is quickest to simply draw individual filled squares for each pixel.  1 > m = 16. 
At moderate magnifications we use look up tables indexed by the byte pattern, amount of magnifica­tion, 
and bits of shift to properly position the result within the destination word. Different tables are used 
depending on the depth of the image.  m = 1. With no magnification we only need to worry about the amount 
of shift necessary to position the result.  = m < 1. To demagnify images we index into a  1 1024 precomputed 
pyramid of images [25]. This precomputation is done at the time a graphic is created; it creates about 
a 3/2 speed penalty to that process. Since graphic items are generally reused over many screen refreshes, 
this penalty is not usually a problem in practice. 1 m < . Beyond some amount of demagnification the 
1024 bitmap is not visible and need not be drawn at all. These techniques yield a display time for each 
object approximately proportional to the size of the entire screen image. In practice this tends to keep 
refresh time dependent only upon screen resolution, not upon image complexity. 4.2 Address Space Limits 
Addresses are implemented using floating point arithmetic, so we cannot claim an infinite address space 
for our current system. A true unbounded address space could be achieved by using extended integer arithmetic. 
Even in its current form, the space provided is astronomical. Suppose our numbers have a 48 bit mantissa 
and we have a 212 by 212 screen. To position an object on the screen uses 12 of those 48 bits, leaving 
a minimum of 36 bits of precision to position our look-on anywhere within the square -1 =x, y = 1. This 
means, for example, that you could lay out 236 by 236 pages of text in that area.  5 Ongoing and Future 
Work 5.1 Shared Object Space Perhaps our most important goal is to create a truely distributed Pad system, 
where Pad objects can exist on remote machines and can migrate from machine to machine. When Pad objects 
are distributed over many computers the problem of updating the display of a region on one s screen becomes 
a combined distributed database and computational geometry problem. This is the subject of ongoing research 
[7], and is beyond the scope of this paper. For in-depth discussions of the implementation problems we 
refer the readers to Preparata &#38; Shamos [18] for an overview of computational geometry and to Edelsbrunner 
[5] for an optimal data structure for rendering. 5.2 Continuous Zoom Early prototypes have used discreet 
zoom levels to achieve high performance. We have also implemented a continuous zoom algorithm (based 
on Bresenham s midpoint line drawing algorithm) that allows continuous scaling of raster images at approximately 
half the speed of discreet zooming on unenhanced bitmapped workstations. The algorithm uses table lookups 
to greatly speed up the calculation. 5.3 Hierarchical Text Editor A number of generalizations of familiar 
applications to the hierarchical domain suggest themselves. A multiscale text editor is a generalization 
of a traditional text editor, with the added capabilities that text can appear at many different sizes, 
with recursively inserted text. Therefore the screen structure is no longer a two dimensional array - 
it is more like a set of nested boxes. This allows a more direct look-and-feel for hypertext - footnotes 
and references can be embedded in their entirety at the point of reference. Successive zooming by the 
user gradually expands the contents seen of the work referenced. Text is structured as hypertext -a text 
string may contain embedded links to other text strings. The structure of the document can be an arbitrary 
directed graph. Visually, text that is linked to appears to be at the location of the link, only smaller. 
Contents of a hyperlink can be accessed without a disruptive sudden change in the view of the text that 
references it. Text can also be made semantically zoomable: when text is visibly small it appears only 
as a title. As the user zooms in, this expands to include an abstract. Further zooming reveals first 
an outline with short text descriptions, then finally the full text. There are several options for where 
exactly to visually place linked-to text. The text can appear in miniature either beneath the lines of 
parent text or, alternatively, superimposed on the parent text. The latter option requires zoom-dependent 
translucency. As the user zooms in, text seen through hyperlinks fades up and the visually larger text 
that references it simultaneously fades out. Text can be visible simultaneously in any number of portals. 
Each view must maintain a certain amount of state information. For example, there needs to be a cursor 
for each view. This means that if the mouse is over a particular portal, and the user types, the insertion 
point is at the cursor of that view. Since portals can contain ownership attributes, they can be used 
to restrict access to parts of a document. Text visibility through any particular portal depends upon 
the text s ownership - public (shared by many users) or private (seen by only one user). Public text 
can contain links to private text. In general, the visibility attributes of text can vary, depending 
upon whether the text is being viewed by its owner or by someone else.  5.4 An Infinitely Scalable Painting 
Program We have, together with Luis Velho, begun applying multiscale principles to an infinitely detailable 
painting program [17]. Organizing an infinite multiscale canvas is straightforward, requiring only a 
Quad-tree. Unfortunately, simulating the application of a paint brush requires a compositing operation 
- an alpha blending of the underlying image with the brush image. Since this operation is non-commutative, 
it is easy to run into problems. For example, let s say the user zooms way in to paint a Figure 5: Overview 
of branching tree story. The story begins with a single sentence. The branches of the tree represent 
story paths - as the reader zooms into different branches, different stories unfold.  scene at a fine 
scale, then pulls out to paint an atmospheric wash at a coarse scale, and finally zooms back in to touch 
up fine scale details. How should the system implement this? A straightforward approach, used by Williams 
[26], is to immediately apply the coarse scale operations to the finer level pixels. But this is computationally 
prohibitive for highly scaled scenes, since the number of fine scale pixels affected grows exponentially 
with the difference between coarse and fine scale. Clearly a pyramid of some kind is called for. But 
because of non-commutativity, successive operations at different levels cannot be separated into a traditional 
Laplacian or similar multilevel pyramid (as they could be in, say, a strictly additive system). Our solution 
is to use B-spline wavelets. We break the brush image into its component wavelet basis, and apply independently 
at each level of a wavelet basis pyramid. Then the B-spline wavelet reconstruction will produce the correct 
result. We have implemented this to a one-dimensional canvas, and are now working on a two or more dimensional 
version.  5.5 Multiple Narrative Paths Pad is a good way to store documents with hierarchy and multiple 
narrative pathways. Side discussions in a textbook can be embedded in situ. This allows for some interesting 
possibilities. For example, a novel may be written with bifurcations, allowing its reader to explore 
many interleaving stories - a sort of visual Alexandria Quartet. For example, we have been creating a 
user browseable novel literally shaped into a tree, as seen in figures 5, 6, 7. 5.6 Cooperative Pad 
Applications With the onset of high bandwidth consumer information services, Pad provides a viable look-and-feel 
for information browsing. As the customer zooms in to an information service, the semantic zoom level 
(and hence the information content) increases. Zoomed-down browsing can be made freely available, and 
the customer can be billed at successively higher rates for more specific data. Figure 6: One level of 
zoom into branching tree story. At this scale the narrative contains one or two paragraphs of detail. 
For example, the title and a brief synopsis of a video may be accessible at low zoom levels. Higher zoom 
levels actually play the movie. At the browsing level, the customer might see geographically arranged 
clusters of films that may be of related interest (e.g. films by a particular director). Similarly, our 
Pad Map project will provide a substantial user community with access to a shared map of Manhattan, annotated 
with information about cultural events. The users will be able to add their own annotations, such as 
restaurant or movie reviews, or just graffiti. As part of the Pad system, annotations could be at any 
scale, and contain links to other annotations: though it is desirable to keep all the reviews of a given 
film together, portals could make them visible at each theatre which is showing that film. The project 
will explore the mechanisms necessary manage user contributions without any one user monopolizing or 
degrading the system for others. Our Shared Spreadsheet project re-casts the spreadsheet application 
in a more hierarchical and sharable form. For example, hierarchy can be imposed by placing spreadsheet 
A in a cell of spreadsheet B, and designating a particular cell of A to be the value that appears in 
B s cell when the magnification of A is low. The value of sharing such a spreadsheet among users comes 
from immediate access to the latest data, and the elimination of the need to merge copies of the spreadsheet 
which have been updated independently, etc. Eventually, as display and communication technology improves, 
pieces of display surface scattered around a work environment will become more common - on walls, desks, 
electronic PostIt notes[24]. Pad is well suited to such a distributed environment, since it places the 
user at a floating location in an information geography. The Windows/Icon/Menu/ Pointer model is less 
well suited to this, since it is motivated by the desire to create a desktop metaphor on a single display 
screen.  6 Conclusions We have described a new kind of graphical space that has a number of advantages 
over traditional window systems. Its key advantage is that it allows a user or a group of users to share 
and view multiple applications in a manner that assigns them various levels of importance, with easy 
visual nesting and zooming of documents as they move from peripheral to primary working attention. As 
compared to standard current window models, this system makes it easier for the user to exploit visual 
memory of places to organize informationally large workspaces. We believe that this approach enriches 
the workstation/ window paradigm in a fundamental way. 6.1 Acknowledgments This research was funded by 
a grant from the NYNEX Corporation and by NSF grant number IRI-9015445. We would like to thank Nathan 
Felde at NYNEX for the initial discussions leading to this work, and Jack Schwartz, Lorie Loeb, Raj Raichoudhury, 
Allison Druin, and Gene Miller, all of whom contributed valuable ideas and time, as well as the Apple 
corporation for their generous equipment donation. Particular credit goes to Matthew Fuchs, who is developing 
the Distributed Pad/Scheme system DREME.  References [1] Sara Bly et. al., Media Spaces: Bringing People 
Together in a Video, Audio, and Computing Environment, CACM, Vol. 36, 1993, No. 1., pp. 28-47. [2] Peter 
Burt, A multiresolution spline with applications to image mosaics, ACM Transactions on Graphics, Vol. 
2, No. 4, Oct. 1983, pp. 217-236. [3] James H. Clark. Hierarichical geometric models for visible surface 
algorithms. ACM Communications, Vol. 19, No. 10, Oct. 1976, pages 547-554. [4] William C. Donelson, Spatial 
Management of Information, ACM SIGGRAPH 1978 Conference Proceedings. [5] H. Edelsbrunner, A new approach 
to rectangle intersections, Part II, Int l Journal of Computational Mathematics, No. 13, pp. 221-229, 
1983. [6] S. Feiner and C. Beshers, Worlds within worlds: Metaphors for exploring n-dimensional virtual 
worlds. Proc. UIST 90 (ACM Symp. on User Interface Software and Technology), Snowbird, UT, Oct. 3-5, 
1990, pp. 76-83. [7] Matthew Fuchs, unpublished Ph.D. dissertation in progress. [8] George Furnas, Generalized 
Fisheye Views, Human Factors &#38; Computer Systems, CHI 89 Conference proceedings, pp. 16­ 23. [9] Ralph 
Hill, et. al., The Rendezvous Language and Architecture, CACM Vol. 36, 1993, No. 1., pp. 62-67. [10] 
Hypertext on Hypertext, Macintosh Version: Disk #1 and #2. ACM Press, New York, 1988. [11] I. Lu et. 
al., Idea management in a shared drawing tool. Proceedings of the Second European Conference on Computer-Supported 
Cooperative Work-ECSCW 91, Amsterdam, Holland, 1991. [12] J. Mackinlay et. al., Rapid Controlled Movement 
Through a Virtual 3D Workspace. ACM SIGGRAPH 1990 Conference Proceedings. [13] Nelson Max, ACM SIGGRAPH 
1975 Film show. [14] Minneman, S. and Bly, S.A. Managing a trois: A study of a multi-user drawing tool 
in distributed design work, Proceedings of the CHI 91 Conference on Human Factors in Computer Systems., 
New Orleans, La., 1991. [15] Ted Nelson, Literary Machines. Swarthmore, PA, 1981. [16] Jakob Nielsen, 
Non-command User Interfaces, CACM, Vol. 36 No. 4, (April 1993), pp. 83-99. [17] Ken Perlin and Luis Velho, 
A Wavelet Representation for Unbounded Resolution Painting, NYU Technical Report. [18] Franco P. Preparata, 
Michael Ian Shamos, Computational Geometry: An Introduction, Springer Verlag, New York, 1989. [19] David 
Small, Masters Thesis, MIT Media Laboratory, 1989. [20] Randall B. Smith, Tim O Shea, Claire O Malley, 
Eileen Scanlon, and Josie Taylor. Preliminary Experiments with a distributed, multi-media, problem solving 
environment. In Proceedings of the First European Conference on Computer Supported Cooperative Work (Gatwick, 
UK) 1989, pages 19­ 34. [21] J.C. Tang and S.L. Minneman, Videodraw: A video interface for collaborative 
drawing. Proceedings of the CHI 90 Conference on Human Factors in Computing Systems, Seattle, Wash., 
1990. [22] S. L. Tanimoto, and T. Pavlidis, A hierarchical data structure for picture processing. Computer 
Graphics and Image Processing, Vol. 4, 1975, pp. 104-119. [23] Edward Tufte, The Visual Display of Quantitative 
Information, Graphics Press, 1983. [24] M. Weiser, The Computer for the 21st Century, Sci. Am. 265,3 
(September 1991), pp. 94-104. [25] Lance Williams, Pyramidal Parametrics. ACM SIGGRAPH 1982 Conference 
Proceedings. [26] Lance Williams, personal communication.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166124</article_id>
		<sort_key>65</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Autocalibration for virtual environments tracking hardware]]></title>
		<page_from>65</page_from>
		<page_to>72</page_to>
		<doi_number>10.1145/166117.166124</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166124</url>
		<keywords>
			<kw><![CDATA[autocalibration]]></kw>
			<kw><![CDATA[tracking]]></kw>
			<kw><![CDATA[virtual environments]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Input devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Photometry</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010391</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P267285</person_id>
				<author_profile_id><![CDATA[81100288313]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stefan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gottschalk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40035638</person_id>
				<author_profile_id><![CDATA[81100166298]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Hughes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>897994</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ronald Azuma and Mark Ward. Space Resection by Collinearity: Mathematics behind the Optical Ceiling Head-Tracker. Technical Report TR91- 048, UNC-Chapel Hill Department of Coputer Science, November 1991.]]></ref_text>
				<ref_id>AW91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378509</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ronen Barzel and Alan H. Barr. A Modeling System Based on Dynamic Constraints. Computer Graphics, 22(4):179-188, August 1988.]]></ref_text>
				<ref_id>BB88</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[F.H. Raab, E. B. Blood, T. O. Steiner, and H. R. Jones. Magnetic Position and Orientation Tracking System. IEEE Transactions on Aerospace and Electronic Systems, AES15(5):709-718, September 1979.]]></ref_text>
				<ref_id>RBSJ79</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[C.C. Slama, editor. Manual of PhotogrammenT. American Society of Photogrammetry, Falls Church, Va, fourth edition, 1980.]]></ref_text>
				<ref_id>Sla80</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[J.F. Wang, R. Azuma, G. Bishop, V. Chi, J. Eyles, and H. Fuchs. Tracking a Head-Mounted Display in a Room-sized Environment with Head-Mounted Cameras. Proc SPIE 1990 Technical Symposium on Optical Engineering and Photonics in Aerospace Sensing, 1290, 1990.]]></ref_text>
				<ref_id>WAB+90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147162</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[M. Ward. R. Azuma, R. Bennett, S. Gottschalk, and H. Fuchs. A Demonstrated Optical Tracker with Scalable Work Area for Head-Mounted Display Systems. In Proceedings of 1992 Symposium on Interactive 3D Graphics, Cambridge, Mass., pages 43-52, March 1992.]]></ref_text>
				<ref_id>WAB+92</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Jib-Fang Wang. A Real-time Optical 6D Tracker for Head-mounted Display Systems. Technical Report TR90-011, UNC-Chapel Hill Department of Computer Science, March 1990.]]></ref_text>
				<ref_id>Wan90</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Autocalibration forVirtual Environments Tracking Hardware* Stefan Gottschalk John F. Hughes Computer 
Science Department Computer Science Department University of North Carolina Brown University Chapel Hill, 
NC Providence, RI gottscha@cs.unc.edu jfh@cs.brown.edu Abstract We describe two instances in which precise 
mechanical calibration of virtual environments equipment has been replaced by automated algorithmic calibration 
through software that encapsulates the hardware design and uses a goal-based approach to adjust calibration 
parameters. We describe a back-projection system for adjusting the assumed locations of beacons in a 
head-mounted display tracking system; the calculated errors in the navigation system are used to compute 
adjustments to the beacon positions to reduce such errors. In a second application, a piggyback head­tracking/hand-tracking 
system is calibrated by a similar reduction of computed errors. CR Categories: I.3.m [Computer Graphics]: 
Miscellaneous; I.3.7 [Computer Graphics]: 3-dimensional Graphics and Realism Virtual Reality; I.4.8 
[Image Processing] Scene Analysis Photometry Additional Keywords: Virtual environments, tracking, autocalibration. 
1 Introduction A number of calibration issues for virtual environments (VE) hardware are approached with 
standard engineering techniques in which the accuracy of the calibration is directly dependent on the 
accuracy of the assemblies in the VE machinery. This approach is successful to a degree but has several 
drawbacks. First, it makes the machinery very sensitive to rough handling. Second, frequent realignment 
may be required, which may be time-consuming and may be necessary so frequently that extended use of 
the equipment becomes impossible. Third, modifications of the machinery become very difficult. We therefore 
take a goal-based approach to these problems, applying methods learned in computer graphics to solve 
engineering problems. Instead of requiring precise calibration of parts, we ask the systems to autocalibrate, 
a notion that was inspired in part by the auto-assembling systems of Barzel and Barr [BB88] * This work 
was sponsored in part by grants from NSF, DARPA, IBM, NCR, Sun Microsystems, DEC, and HP. Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct provided that the copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the commercial advantage, the ACM copyright notice and the 
title of the publication and its date appear, and notice is given that copying is by publication and 
its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. otherwise, or to republish, requires a fee and/or specific permission. 
&#38;#169;1993 8/93/008/0015 $1.50 &#38;#169;1993 ACM - 0 - 89791 - 601 - 8/93/008 $1.50 but which first 
appeared in Wang s dissertation [Wan90]. This allows us to write a program encoding the design of the 
system that uses the system s observations to adjust itself. Since realignment can sometimes actually 
be done while the machinery is in use, rather than in a separate calibration phase, the first and second 
problems above are reduced. And because the software that implements the autocalibration encodes the 
intent of the design, the mechanical design can be modified in parallel with software modification, helping 
to reduce the third problem. In this paper, we discuss two sample applications: calibration of a head-tracking 
system and of a piggyback hand tracker attached to the head­tracking unit. We stress that the techniques 
here serve the general goal of head-tracking. The current interest in Virtual Reality, evidenced by the 
attention it has attracted in both the technical literature and the media, may well have led to unjustified 
expectations. There is a belief that any day now the technology will become available. But there are 
three substantial obstacles: (1) for comfort, the units need small, high-resolution displays; (2) graphics 
hardware must be capable of real-time, low-latency image generation; (3) a low­latency, high-accuracy 
system for head tracking in unprepared, possibly-noisy environments is necessary. We are addressing the 
third of these issues. There is as yet no tracking system that is lightweight and works in unprepared 
environments and in large spaces. As far as we know, no one has demonstrated a working head-tracking 
system for a room-sized environment (about 15' x 15'). The ceiling tracker described here is a start: 
the environment is large and expandable and the equipment, although heavy, is bearable. We envision an 
eventual system in which methods similar to those described here are used to calibrate the system s view 
of its environment. The algorithms may differ, but the principle having the system model its sources 
of error and calibrate itself against them will remain. We wish to make one more point: the two examples 
presented in this paper give details of a general principle, and this general principle is applicable 
to cases other than the ones we describe. In short, as one designs a tracker (or other electro-mechanical 
assembly), one has the opportunity to leave some physical parameters fixed but unknown, and to then determine 
their exact values after construction. Doing this kind of post-construction calibration does, however, 
require that some aspects of the system be overdetermined. In the head-tracker example below, we could 
not have performed autocalibration if the tracker computed its position from just three LED beacons, 
since there would be no error measure as we computed the position the equations would be exactly determined 
rather than overdetermined. Similarly, without multiple samples in the hand-tracking application, we 
could not determine the orientation matrix. So the principle is this: if one wishes to use autocalibration, 
the system must have a surplus of information and a way to measure whether this information is internally 
consistent. The cost of obtaining this surplus of information is a design tradeoff, and should be considered 
during the design rather than after. 2 Operation of the Ceiling Tracker Most current head trackers achieve 
a large working volume at the expense of accuracy and precision. However, some virtual environments applications 
require a large working volume and some minimum tracking precision. A team at UNC-CH has developed an 
optoelectronic tracking system capable of tracking head motion with precision of approximately 0.2 degrees 
orientation and 1 mm translation. (A description of this system and its design can be found in the references 
[WAB+92][WAB+90]). System accuracy has not been measured precisely, but has been found to be very adequate 
for the purposes of head-mounted display (HMD) applications. At present, the working volume is a 10' 
by 12' area, but the tracking area can in principle be expanded arbitrarily by adding LED­studded ceiling 
panels. The method used by the optoelectronic tracker is conceptually similar to celestial navigation. 
A mariner observes the angles between some number of stars and the horizon, and then, knowing the stars 
locations in the heavens, determines the vessel s position. Similarly, we observe a number of ceiling-mounted 
infrared LEDs, and knowing their positions, we compute the location (and orientation) of the head-tracking 
unit. To be more precise, we have a helmet with cameras mounted on it. Some of the ceiling LEDs are rapidly 
flashed in a known sequence, and each one is possibly sighted by a camera. The choice of subset and sequence 
is not preset, but is determined on the fly as it is learned which LEDs are visible to which cameras. 
The cameras are lateral-effect photodiodes with lenses, and each can report the centroid of a spot of 
light that strikes its surface. The centroid s location is reported in image plane coordinates, x and 
y. We call these photocoordinates (see Figure 1). The placements of the cameras on the helmet are known, 
as are the locations of the principal points of the lens systems and the placements of the photodiodes 
image planes within the camera casings. Thus, when the camera reports the photocoordinates of LED image 
on its image plane, we can compute the line, in head space, along which the LED must lie. We call this 
line a back­projection, because it is the result of projecting the ray from the photodiode back through 
the lens system and outward. Now, given several back-projections in head space, and given the true locations 
of the LEDs in world space, where must the head be in world space so as to cause the back-projections 
to pass through their respective LEDs? With three (sufficiently general) back-projections, an unique 
solution can be found. With more than three, we have an overdetermined system and we compute a best fit 
according to a least-squares criterion, using a method called space­resection by collinearity (abbreviated 
CA for collinearity algorithm ). We briefly describe CA in Section 3.1; full details can be found elsewhere 
[AW91]. Several questions about this tracker design that are often raised are discussed in an Appendix. 
 3 Explaining the Problem The current design uses an adjustable superstructure to support the ceiling 
panels. The adjustments are needed because any conceivable support structure would bend under the loading 
of the panels, giving an undesirable curvature to the ceiling s surface. With the current design of 10' 
by 12' (30 2' by 2' panels), the leveling process requires about 90 minutes of operator time, with specialized 
equipment. We plan to build another, larger ceiling without this superstructure. The panels will be 
of the same size, but will drop directly into the standard ceiling grid, replacing the acoustic tiles 
found in many buildings. This ceiling will be 18' by 30'; the expense of a comparable-size superstructure 
is prohibitive, and leveling time would be several hours. Standard ceiling grids are by no means flat, 
and we have therefore developed the autocalibration technique described here to determine the location 
of the LEDs after the panels are installed. Before describing that technique, however, we give more details 
of the collinearity algorithm. 3.1 The Collinearity Algorithm The collinearity algorithm (CA) works by 
observing many (typically 10 to 20) LEDs and then computing a best estimate of headmount position and 
orientation. When an LED shines onto a photodiode, the photodiode reports the centroid of the LED s image 
on its face. Since the algorithm knows the headmount geometry, it is able to compute, in head space, 
where the back-projection emerges and in what direction it is pointed. Somewhere along this back-projection 
lies the LED (see Figure 2). Thus R( p +.d) +h = t , .> 0 (1) where t the is location of the LED in world 
space, R is the matrix that takes vectors in head coordinates to world coordinates (i.e., R defines the 
orientation of the head-mount), h is the world-space coordinates of the origin of the head-mount coordinate 
system; and p and d are the basepoint and direction (unit vector) of the back­projection ray in head-coordinates; 
. is the distance from the camera to the LED. Equation 1 actually consists of three scalar equations, 
one for each of the x-, y-, and z-components. We can solve the z component for . and substitute this 
into the x and y components. This eliminates . and leaves us with two scalar equations in the unknowns 
R and h. Many LEDs are seen at the same moment. Each of these generates two scalar equations. So each 
observation, which sights 12 to 20 LEDs, constructs a system of 24 to 40 equations in the unknowns R 
and h. CA seeks those values of R andh that minimize the residuals of these equations in the least-squares 
sense. These values are found by applying a multidimensional Newton s method. The method is most successful 
when given an initial guess very close to the optimal solution. In practice, this is easy to supply. 
The optical tracker typically provides updates every 12 to 20 milliseconds, and a person does not move 
far in that interval. Thus, for the initial guess, the algorithm merely uses the value of the previous 
update, which is guaranteed to be close.  4 Autocalibration: Rationale and Description We have pointed 
out that it is very desirable to be able to construct the ceiling with loose tolerances, and be able 
to determine the locations of the LEDs afterward. CA does not depend upon any particular configuration 
of LED beacons all places are alike to it. It does, however, require an exact knowledge of the locations 
of the LEDs, wherever they may be. An engineering approach to achieving agreement between the physical 
geometry of the beacons and their software representation is prohibitively expensive. Therefore, we sought 
a way to determine the locations of the LEDs using existing hardware and some numerical processing. The 
collinearity algorithm was derived from photgrammetric methods. Our LED calibration method, which makes 
use of CA as one step, was based primarily on influences from mathematics and computer graphics rather 
than the photgrammetry literature. We have since learned, however, that our approach has parallels in 
that literature, although we have found no exact analog. Nonethless, we strongly recommend that others 
working on optical tracking systems consult the photogrammetry literature [Sla80] for many ideas which, 
with slight modifications, may prove valuable in tracking. We begin with an estimate of the beacon locations. 
We then take several thousand headmount observations (collecting 25,000 observations takes about 45 minutes) 
from a variety of positions, and use CA to fit the position of each observation to its beacon data. Of 
course, we know only approximately where the LEDs are, but fitting the headmount position to the beacon 
data allows some of the error in the beacon location estimates to cancel. The CA solution for the location 
of the headmount at each of these thousands of observations is likely to be rather bad: the sum of squares 
value will be large. To return to the marine analogy, it is as though the several circles of positions 
on the earth, each determined by a single star, failed to intersect at a single point, and instead intersected 
pairwise at several different points that surrounded a large region. The mariner estimates the vessel 
s position as somewhere at the center of the region, and begins to doubt the accuracy of the almanac 
s star locations. After this initial set of observations, we derive the back­projections from each of 
these computed headmount locations, to yield sightings of the LEDs from roughly known positions. An LED 
sighted from several positions should be located at the intersection of the back-projections extending 
from those positions, but in general, the back-projections do not come together at a point, but tend 
instead to cluster in a particular region. We therefore adjust our estimate of each LED to be closer 
to this back-projection cluster. (The mariner, after several sets of inconsistent observations, decides 
to correct the almanac). This is the second step of our autocalibration. After we adjust all the LEDs, 
the old observation positions are no longer optimal solutions in CA. So, we apply CA again to the observation 
positions, using the same data as before, but with the new beacon location estimates. (The mariner re-computes 
the vessel s position on each of the previous days, and now has circles of position that come closer 
to intersecting at single points). Thus we repeat the first step. We now continue, alternating between 
the two steps in this fashion, adjusting first one set of parameters and then the other, until we have 
settled to some configuration. It seems surprising at first that this process converges at all; it is 
even more surprising to see how fast and how accurately it converges. We tested this by perturbing three 
of the ceiling panels as shown in Figure 4, and then running the algorithm. The average error-vector 
magnitudes for the first five full iterations were 13.1 mm, 4.7 mm, 3.2 mm, 2.5 mm, 2.2 mm, and 1.9 mm. 
After 20 iterations, which takes about two hours for 25,000 observations, the average error vector is 
down to 1.1 mm. Figure 5 is a computer­generated picture of the tracker ceiling. The beacons on the tilted 
panels are clearly visible. The adjustment made to an LED s location depends on its relationship to the 
back-projections associated with it. A back­projection, in general, passes nearby the LED s estimated 
location. The vector drawn from the LED s estimated position to the back­projection s closest approach 
to that position is the error vector for that back-projection. A given LED has many back-projections, 
for each of which there is an associated error vector. We average these error vectors, and use this average 
as the adjustment to the LED s estimated position. In a sense, each observation of an LED votes in the 
adjustment. An observation typically sees many LEDs, and cannot find a position from which to spear all 
its LEDs with its back­projections. The smallest adjustment possible for each LED that would completely 
satisfy an observation s collinearity conditions, would be an adjustment along the error vector. However, 
such an adjustment might conflict with the adjustment required by another observation. The averaging 
is thus done as a compromise among the needs of the various observations that sight a given LED. It is 
possible to determine a new position for the LED that actually minimizes the sum of the squared lengths 
of the error vectors, but it is computationally expensive, and the averaging method works well and fast 
in practice. 4.1 Concerns About Noise: the Method in Practice The autocalibration method was originally 
tried with simulated data so that it could be evaluated in the absence of noise and other complicating 
factors. It was found to be quite effective, providing rapid convergence. Performance on real data was 
not nearly as good -for reasons we now discuss. First, the photodiode readings are noisy. The photocoordinates 
have as much as 12 microns of uncertainty. If the LED is a meter away from the camera, which has a 50-mm 
lens, the back­projection will miss by more than .25 mm even if headmount s position and orientation 
are exactly correct. Second, the system of equations produced by each observation assumes that the LEDs 
are sighted simultaneously, and this is not true in practice. The LEDs are sampled in sequence, and each 
sample may take as much as a millisecond. If the user s head is turning at the (reasonable) rate of 180 
degrees per second, the LED is 1 m. away from the axis of rotation, and 20 LEDS are sampled for the observation, 
then in the 20 milliseconds of sampling, the back­projection to the first LED may have traveled 6 cm. 
This causes the system of equations given by the observation data to be inconsistent, so that it cannot 
be satisfied by any position and orientation. The fact that the equations cannot be satisfied implies 
that the back-projections are simply wrong, and hence will pull on the LEDs wherever the observation 
settles. Third, acquiring the right spatial distribution of observations is surprisingly difficult. The 
LEDs in the corner of the ceiling are typically seen in many fewer observations than the ones in the 
center. And when the LEDs in the corner are seen, it tends to be from one direction. Naturally, an LED 
in the corner can be seen only from one octant: below ceiling height and beneath the ceiling. But diversity 
in the angles from which the LED is seen is helpful. If an LED is seen from within a narrow cone of positions, 
then the location of the back-projection cluster is more sensitive to the errors mentioned earlier: a 
slight distortion in the back-projections placement tends to disperse the cluster, denying the LED a 
strong centering influence. Three observations can be made about the first source of error. First, in 
addition to using superior photodiodes and electronics, the error can be reduced by using lenses of longer 
focal length. With longer focal lengths, the 12-micron error in the LED image location would translate 
to an even narrower error cone for the corresponding back-projection. The primary disadvantage of the 
resulting small fields of view is that they can slip between the LEDs and fail to see any at all. Second, 
one can allow the headmount to sit still, accumulating photocoordinates, and average them over time to 
distill a more accurate reading. Unfortunately, with thousands of observations required, data acquisition 
for calibration would be very time-consuming. Third and most important, however, sensor noise error is 
insignificant in comparison to the other two sources of error. The second source of error comes from 
the motion of the headmount. Again, for calibration purposes, we could take data points only when the 
headmount is still. But this again would make data acquisition intolerably slow. In practice, we have 
found that moving the headmount slowly helps substantially in reducing this error. A better solution 
is to change the system of equations to take into account the headmount velocity, both linear and rotational. 
This would require a minimum of six LEDs per observation to obtain a fully-determined system, but typical 
counts are already 12 to 20 LEDs per observation. This is future work. The third problem is being addressed 
by an graphics application that assists in data acquisition. A top view (map) of the ceiling is displayed 
on a nearby workstation, on which LEDs presently observed are marked. (This is needed because the LEDs 
emit infrared light, invisible to the naked eye.) The least-sampled LEDs are marked in a different color, 
allowing the operator to direct his efforts to sighting those LEDs. During the calibration process, in 
addition, certain LEDs are identified as having unusually large error vectors, meaning that their associated 
back-projections do not cluster tightly enough. A second run of data collection can be made, and special 
attention paid to these trouble spots. In addition to the precautions and program assistance mentioned 
above, the calibration algorithm tests for high error vectors and culls out observations for which CA 
cannot find a satisfactory solution. (This is similar to computing robust statistics by eliminating outliers.) 
In this way, the algorithm is made somewhat more tolerant of operator mistakes or wild readings from 
the sensors (which are very rare). Two features of the automated calibration method have not yet mentioned. 
First, the ceiling tracker is in frequent use. We can simply collect the observations during use and 
use these in an off­line calibration computation, so that we can keep the tracking system aligned without 
downtime. At present the system does not need frequent recalibration, and we do separate calibration 
runs, allowing us to collect only good data (i.e., data taken with slow head motion). Second, the entire 
algorithm is subject to a kind of systematic error: if we apply a rigid motion to our estimates of the 
beacon locations, CA converges exactly as well as before. This means that if one wishes to calibrate 
the system in absolute coordinates (relative to some frame of reference for the room in which the ceiling 
tracker sits), one may have to apply a rigid motion to the computed beacon positions so taht the estimated 
locations of a few key beacons are their actual positions as determined, for example, by measurements 
from the walls of the room.  5 Using a Headmounted Magnetic Tracker for Handtracking Although the optical 
tracker gives satisfactory accuracy over a large working volume, its design does not lend itself to hand 
tracking for several reasons: the bulkiness of the cameras, the geometry of the situation (the user s 
body may obscure the hand s view of the ceiling, and the hand may not be held upright), and the dynamic 
range requirements on photodiode sensitivity (because of changing distances from the ceiling). We have 
found, however, that magnetic trackers [RBJ79] usually provide satisfactory performance within a small 
tracking volume, although in our environment they report significantly distorted position and orientation 
outside of a range of about five feet. Since one s hands never get farther than a few feet from one s 
head, we decided to place a magnetic source on the headmount and track hand motion from there. Ultimately, 
however, we want to know the hand s location in the ceiling coordinate system. The optical tracker reports 
the head location in ceiling space, the magnetic source lies at some fixed location in head space, and 
the Polhemus tracking system reports the hand s location in source space. We compose the change-of­coordinate 
transformations among these three systems to get the hand s location in ceiling space. Of course, the 
fixed location of the magnetic source within head space must be known before we can compose the transforms. 
As before, we have two choices: engineering, i.e., careful placement of the source on a precise rigid 
mount attached to the headframe, and autocalibration, in which we place the source approximately and 
then infer its position precisely using autocalibration. We chose the latter approach. 5.1 The Calibration 
Problem and Solution We attach the magnetic source to the headmount with a rigid Plexiglas framework 
whose position is known within a few inches, and whose orientation is easy to measure within about 10 
degrees. These are clearly not adequate measurements: if the hand is held 3' from the source, a 1 degree 
error in the measurement of the source s orientation would cause a 15 mm error in the computation of 
the hand s placement. Our calibration approach is simple. We take simultaneous optical tracker and magnetic 
tracker readings, and use them to recover the placement of the source within head space. The algorithm 
starts with a very approximate estimate of the source s placement, such as might be obtained by inspection. 
We start by fixing the Polhemus sensor at some location in ceiling space. The exact location is not important 
 it need only stay still. Now consider what should happen (if the system were calibrated properly) as 
the headmount moves about in the proximity of the sensor. We receive readings from the optical and magnetic 
trackers. The optical tracker produces the Ceiling-from-Head transform, and the magnetic tracker provides 
the Source-from-Sensor transform. If the Head-from-Source is correct, then the composition of these transforms, 
Ceiling-from-Head × Head-from-Source × Source-from-Sensor, should remain constant and should be the Ceiling-from-Sensor 
transform, which is constant (because the sensor is not moving) (see Figure 3). If, for observation i, 
Ri is the reported Ceiling-from-Head transform, Ti is the reported Source-from-Sensor transform, S is 
the unknown but fixed Head-from-Source transform, and M is the unknown Ceiling-from-Sensor transform, 
then for any pair of reports from the trackers, RiSTi = M, provided the trackers are accurate. But if 
S is wrong, then as we walk around the room, the sensor s position and orientation (i.e., M), as computed 
by the transform composition, will drift, appearing to be in different places, depending on where we 
are standing. After n readings from n different places, we have a system of n equations, RiSTi = Mi , 
i = 0 n- 1, where S is our (incorrect) estimate of Head-from-Source, and each Mi is computed as Ri STi 
. We seek the value of S that will make the Mi s equal (i.e., the value of S that keeps our reports of 
the sensor positions and orientation constant). Our estimate of S and the readings Ri and Ti give rise 
to many estimates of the sensor location Mi . We might get closer to the true value of M by taking some 
compromise among the Mi s, say, by estimating that it is the average of the Mi s. We actually bias this 
average slightly by averaging the matrix entries, and then performing the Gram-Schmidt process on the 
rotational part of the matrix. This averaging and orthonormalization step is likely to prompt objections, 
which we address below. For now, we continue with our description of the algorithm. Let s call this resulting 
average transform Q. If we imagine that this is the correct value for the sensor location, then we can 
write the system Ri STi = Q, i = 0 n - 1, If Q really were the correct location, then we could take any 
one of the equations Ri STi = Q and solve for S to recover that value, since the remaining transforms 
would be known. However, when we actually do this we find that we get different values for S. Why? Because 
Q is not correct but it might be close. Solving for S gets us -1 Si = Ri -1 QTi , i = 0 n -1, each of 
which suggests a different value for S. We average these in exactly the way we did the Mi s to arrive 
at a new estimate for S. This completes one iteration of the algorithm. With our new estimate of S we 
go back and acquire new Mi s, which we average to get Q, which we substitute back into the system so 
we can solve for the S_i s, which we average to get our new S. We iterate until the value of S stabilizes. 
 5.2 Justification for averaging matrices Averaging makes sense for points in a linear space like a plane, 
but we are trying to use it in a nonlinear space (the set of 3 by 3 rotation matrices). But in general, 
the average of a set of points on a non­linear space like a sphere is almost always a point that is not 
on the sphere. Even so, if all the points are very close together on the sphere, this averaging yields 
a point that is near to a point on the sphere that one might call the average. The reason is that the 
local geometry of the sphere is well approximated by any of the tangent planes within the local region, 
and so the sphere-based averaging is a close approximation to the tangent-plane averaging. Since the 
average of the sphere points does not lie on the sphere, however,to get a meaningful average we must 
project back onto the sphere. The critical properties of the projection map here are (1) it is continuous 
in a neighborhood of the sphere, and (2) for points already on the sphere, the projection is the identity. 
We now explain why the process we used in averaging matrices is analogous. The set Q of 4 by 4 translation-and-rotation 
matrices is a subset of R16 ; it is curved in much the same way that the sphere is a curved subset of 
R3. We can average a collection of points on the object Q (i.e., several matrices), in much the same 
way as we averaged points on the sphere. Before this can make sense, though, we must honor the restriction 
that the points being averaged should be close to one another. And the same caveat applies: the R16 ­average 
of a set of points in Q is not likely to lie in Q, and will need to be projected back to Q, which is 
what the Gram-Schmidt process does. Note, though, that the Gram-Schmidt process has the same properties 
as radial projection: it is a continuous function of the entries of the matrix (at least for matrices 
that are close to rotation matrices), and for a rotation matrix, the Gram-Schmidt process does nothing. 
Still, there remains the question, How small a region must the points be gathered in for averaging to 
make sense? On the sphere, it certainly makes sense when all the points are contained in some hemisphere. 
For matrices, the averaging of the translational part is simply an average in a linear space, and needs 
no justification; for the rotational part, we believe (but have not proved formally) that the averaging 
process makes sense for any collection of (rotation) matrices A_i for which all the inner products zij 
= trace(Ai Atj) are greater than 1/2. In practice, however, our matrices are all quite close to one another, 
and these inner products are large. Furthermore, the algorithm in practice is far more robust than we 
had expected. In a 2D simulation of the problem, for example, it takes some effort to give an initial 
estimate of the matrix S that makes the algorithm diverge.  5.3 Noise in the data, and the algorithm 
in practice The accuracy of this method depends on the accuracy of the trackers providing the data. The 
optical and magnetic trackers, providing the Ri s and Ti s are noisy. In general, no choice of S and 
M satisfies all the equations simultaneously. It is impossible to determine what the correct values are, 
and we can only hope to get an approximation to the correct S. Nonetheless, the error in the estimates 
of S and M, since they are based on multiple samples, should average out the random noise from the trackers. 
The systematic noise (e.g., one tracker always reports a slightly scaled x­coordinate) is not averaged 
out, but is also inherent in the system; if such systematic noise were too large, the system would be 
unusable in practice. Our experience is that the values of S and M converge quite rapidly to values that 
provide quite good hand-tracking. There is one important observation about this instance of autocalibration: 
the sensor readings from which the calibration is done must be in fairly general position. In some cases, 
for example if the orientation of the headmount remains constant throughout the sampling process and 
the headmount is translated only along a single axis, then a little linear algebra shows that the estimates 
of S and M can all be identical but nonetheless be incorrect. But if the headmount is tilted and translated 
about all three axes during data gathering, and if multiple tilts and translations about each axis are 
included, then the equations will be sufficiently general to guarantee convergence (given a good enough 
initial estimate of S). 5.4 Remarks on the Method One nice aspect of this method is that no exact measurements 
are needed. The location of the sensor somewhere in lab space may remain unknown. The position of the 
source in head space need only be estimated and that is the only measurement necessary: the rest of 
the information is taken directly from the tracker sensors themselves. The calibration procedure takes 
about 20 minutes in all: 5 minutes to put the sensor in place and gather data, and about 15 minutes (including 
graphical display of progress at each step) to settle on a value for S. The number of equations and the 
tightness of the cluster of estimates can give a feel for the accuracy of the estimate. In averaging 
the Si s, we can compute a residual for each, that is the magnitude of the deviation from the average 
Si (deviation, here, being the difference in the translation components of the transforms). The angular 
deviation could be treated in precisely the same manner: the angle of rotation required to get from one 
transform s orientation to the other s. The root mean square of these residuals can be used as a reasonable 
metric for the tightness of the estimates of S. In a typical calibration run of 25 measurements, the 
RMS value of the deviations from the mean Si was about 4.6 millimeters. These residuals are not the same 
as the error in the result, although they are related. The more equations we use, the more likely the 
resulting transform is to be close to the actual one. This is somewhat like averaging a random variable 
 the variance can be very high, but the longer we average, the closer we are likely to get to the expected 
value.  6 Conclusion We have described two applications of a goal-based approach to alignment of mechanical 
systems in VE tracking. In both cases, the automated calibration simplifies the construction of the systems, 
and makes it easier to modify the systems without extensive redesign of hardware or software. Note that 
the autocalibration system is designed to calibrate against a particular source of error, LED position 
error in the first case and Polhemus source location error in the second. Other sources of error in the 
system will confound the autocalibration process, so that if they are persistent enough, the autocalibration 
model should be revised to incorporate them as well. As the number of variables to be calibrated is increased, 
the number of observations must increase as well, of course, but in the head-tracking system, we have 
calibrated about 3000 variables successfully. 7 Acknowledgments We would like to thank Al Barr for his 
initial involvement in the discussion of autocalibration, which helped to lead us away from the engineering 
approach and into the mathematical one. We also thank J.-F. Wang for having the idea of autocalibration 
for headtracking systems in the first place. Henry Fuchs persistent demands for greater accuracy and 
bigger tracking spaces have provided a constant impetus. And we both owe a debt to our colleagues who 
have supported us in this project, particularly Ron Azuma and Russell Taylor.  References [AW91] Ronald 
Azuma and Mark Ward. Space Resection by Collinearity: Mathematics behind the Optical Ceiling Head-Tracker. 
Technical Report TR91­048, UNC-Chapel Hill Department of Coputer Science, November 1991. [BB88] Ronen 
Barzel and Alan H. Barr. A Modeling System Based on Dynamic Constraints. Computer Graphics, 22(4):179-188, 
August 1988. [RBSJ79] F. H. Raab, E. B. Blood, T. O. Steiner, and H. R. Jones. Magnetic Position and 
Orientation Tracking System. IEEE Transactions on Aerospace and Electronic Systems, AES15(5):709-718, 
September 1979. [Sla80] C. C. Slama, editor. Manual of Photogrammetry. American Society of Photogrammetry, 
Falls Church, Va, fourth edition, 1980. [WAB+90] J. F. Wang, R. Azuma, G. Bishop, V. Chi, J. Eyles, and 
H. Fuchs. Tracking a Head-Mounted Display in a Room-sized Environment with Head-Mounted Cameras. Proc 
SPIE 1990 Technical Symposium on Optical Engineering and Photonics in Aerospace Sensing, 1290, 1990. 
[WAB+92] M. Ward. R. Azuma, R. Bennett, S. Gottschalk, and H. Fuchs. A Demonstrated Optical Tracker with 
Scalable Work Area for Head-Mounted Display Systems. In Proceedings of 1992 Symposium on Interactive 
3D Graphics, Cambridge, Mass., pages 43-52, March 1992. [Wan90] Jih-Fang Wang. A Real-time Optical 6D 
Tracker for Head-mounted Display Systems. Technical Report TR90-011, UNC-Chapel Hill Department of Computer 
Science, March 1990.  Appendix: Head Tracker Design Several issues concerning our current tracker design 
are often raised by those unfamiliar with it. First, why use exotic, expensive lateral-effect photodiodes 
instead of the highly developed, inexpensive CCD technologies? The reason is timing. We want updates 
from the tracking system every 12 to 20 milliseconds. With CCDs, both the bandwidth required for data 
transfer and the image processing necessary per frame were prohibitive. We found it more feasible to 
digitize the voltages coming from the lateral-effect photodiodes (the only thing of interest after all 
in the image that a CCD camera would have seen) and transmit this comparatively low bandwidth signal. 
Second, why use multiple cameras with narrow fields of view? Why not use a single camera with a wide-angle 
lens? The problem here is the limited precision of the photocoordinates. We have observed that, in practice, 
the photocoordinates reported by the camera may be off by as much as 12 microns. A narrow field of view 
helps reduce this problem, but since CA requires disparate angles to operate effectively (otherwise the 
matrices involved tend to become ill-conditioned), this field-of­view requirement compels us to use multiple 
cameras. Lastly, why put cameras on the head and LEDs on the ceiling, rather than vice versa, since the 
headmount would be much lighter with LEDs rather than cameras? To explain our strategy, we call the cameras 
on the walls the outside-looking-in approach, and the cameras on the headmount the inside-looking-out 
approach. Inside-looking-out has three advantages over its counterpart: sensitivity to orientation, economical 
scalability, and energetics considerations. Sensitivity to orientation is the ability to detect a head 
rotation. In the current system, a .5 degree turn of the head, for instance, causes a very significant 
change in the LEDs coordinates on the photodiodes, regardless of their distances from the camera. By 
contrast, in the outside-looking-in approach this change in orientation would be almost imperceptible. 
An economically scalable system is one in which the cost of increasing the working volume is low in terms 
of cost per unit tracking space. Because of the narrow field-of-view requirement on the cameras, the 
outside-looking-in approach (on a 30 ft^2 area) would need many cameras mounted on the walls. Covering 
the ceiling with LEDs is less expensive. Energetics refers to how light energy is received from an LED. 
Quadrupling the distance between LED and camera, for instance, decreases the light energy received by 
a factor of 16. Furthermore, LEDs do not emit light uniformly in every direction: most of their power 
is emitted in the direction they face, and drops off with the angle away from their axis (depending on 
the packaging). If the cameras are wall-mounted, and the LEDs are head-mounted, then, as the user walks 
about, many LEDs may be oblique to the cameras, and the distances between user and cameras may vary a 
great deal. These two effects combine to make the range of signal strengths received by the cameras too 
wide.  Figure 4: Tilting three panels in the ceiling to test autocalibration. Figure 5: Computer display 
of calibrated beacon locations. The beacons shown in red were insufficiently sampled and could not be 
calibrated by the algorithm (see Section 4.1). 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166126</article_id>
		<sort_key>73</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Toolglass and magic lenses]]></title>
		<subtitle><![CDATA[the see-through interface]]></subtitle>
		<page_from>73</page_from>
		<page_to>80</page_to>
		<doi_number>10.1145/166117.166126</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166126</url>
		<keywords>
			<kw><![CDATA[button]]></kw>
			<kw><![CDATA[control panel]]></kw>
			<kw><![CDATA[lens]]></kw>
			<kw><![CDATA[macro]]></kw>
			<kw><![CDATA[menu]]></kw>
			<kw><![CDATA[multi-hand]]></kw>
			<kw><![CDATA[transparent]]></kw>
			<kw><![CDATA[viewing filter]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Graphics editors</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39026440</person_id>
				<author_profile_id><![CDATA[81100084142]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Bier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39079647</person_id>
				<author_profile_id><![CDATA[81100388123]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Maureen]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Stone]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31090161</person_id>
				<author_profile_id><![CDATA[81342507475]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14129282</person_id>
				<author_profile_id><![CDATA[81452616426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Buxton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P284281</person_id>
				<author_profile_id><![CDATA[81100493833]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Tony]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[DeRose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>102723</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Adobe Systems Incorporated. PostScript@ Language Reference Manual, second edition. Addison-Wesley, 1990.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bartlett, Joel F. Transparent Controls for Interactive Graphics. WRL Technical Note TN-30, Digital Equipment Corp., Palo Alto, CA. July 1992.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Beck, Kent, Becher, Jon, and Zaide, Liu. Integrating Profiling into Debugging. Proceedings of the 1991 International Conference on Parallel Processing, Vol. H, Software, August 1991, pp. II-284-II-285.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Guiard, Yves. Asymmetric Division of Labor in Human Skilled Bimanual Action: The Kinematic Chain as a Model. The Journal of Motor Behavior, 19, 4, (1987), pp. 486-517.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>120791</ref_obj_id>
				<ref_obj_pid>120782</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Bier, Eric A. and Freeman, Steve. MMM: A User Interface Architecture for Shared Editors on a Single Screen. Proceedings of the ACM SIGGRAPH Symposium on User Intelface Software and Technology (Hilton Head, SC, November 11-13), ACM, New York, (1991), pp. 79-86.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>146547</ref_obj_id>
				<ref_obj_pid>146486</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Bier, Eric A., EmbeddedButtons: Supporting Buttons in Documents. ACM Transactions on Information Systems, 10, 4, (1992), pp. 381-407.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>22390</ref_obj_id>
				<ref_obj_pid>22627</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Buxton, William and Myers, Brad A. A Study in Two- Handed Input. Proceedings of CHI '86 (Boston, MA, April 13-17), ACM, New York, (1986), pp. 321-326.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>58105</ref_obj_id>
				<ref_obj_pid>58076</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Buxton, William. There's More to Interaction Than Meets the Eye: Some Issues in Manual Input. Readings in Human-Computer Interaction: A Multidisciplinary Approach. (Ronald M. Baecker, William A.S. Buxton, editors). Morgan Kaufmann Publishers, Inc., San Mateo, CA. 1987.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806801</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Dill, John. An Application of Color Graphics to the Display of Surface Curvature. Proceedings of SIGGRAPH '81 (Dallas, Texas, August 3-7). Computer Graphics, 15, 3, (1981), pp. 153-161.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Goldberg, Adele and Robson, Dave, A Metaphor for User Interface Design, Proceedings of the University of Hawaii Twelfth Annual Symposium on System Sciences, Honolulu, January 4-6, (1979), pp.148-157.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>61924</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Goodman, Danny. The Complete HyperCard Handbook. Bantam Books, 1987.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Harrington, Steven J. and Buckley, Robert R. Interpress, The Source Book. Simon &amp; Schuster, Inc. New York, NY. 1988.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>169414</ref_obj_id>
				<ref_obj_pid>169059</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Kabbash, Paul, MacKenzie, I. Scott, and Buxton, William. Human Performance Using Computer Input Devices in the Preferred and Non-preferred Hands. Proceedings of InterCHI '93, (Amsterdam, April 24-29), pp. 474-481.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>50759</ref_obj_id>
				<ref_obj_pid>50757</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Krasner, Glenn and Hope, Stephen, A Cookbook for Using the Model-View-Controller User Interface Paradigm in Smalltalk-80, Journal of Object-Oriented Programming, 1, 3, (1988), pp. 26-49.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>317463</ref_obj_id>
				<ref_obj_pid>317456</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Krueger, Myron W., Gionfriddo, Thomas, and Hinrichsen, Katrin. VIDEOPLACE - An Artificial Reality. Proceedings of CHI '85 (San Francisco, April 14-18). ACM, New York, (1985), pp. 35-40.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378495</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Kurlander, David and Bier, Eric A. Graphical Search and Replace. Proceedings of SIGGRAPH '88 (Atlanta, Georgia, August 1-5) Computer Graphics, 22, 4, (1988), pp. 113-120.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[MacDraw Manual. Apple Computer Inc. Cupertino, CA 95014, 1984.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Newman, William. Markup User's Manual. Alto User's Handbook, Xerox PARC technical report, (1979), pp. 85-96.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166125</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Perlin, Ken and Fox, David. Pad: An Alternative Approach to the Computer Interface. this proceedings.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>51309</ref_obj_id>
				<ref_obj_pid>51292</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Pier, Ken, Bier, Eric, and Stone, Maureen. An Introduction to Gargoyle: An Interactive Illustration Tool. Proceedings of the Intl. Conf. on Electronic Publishing, Document Manipulation and Typography (Nice, France, April). Cambridge Univ. Press, (1988), pp. 223-238.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>142763</ref_obj_id>
				<ref_obj_pid>142750</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Sarkar, Manojit and Brown, Marc H. Graphical Fisheye Views of Graphs. Proceedings of CHI '92, (Monterey, CA, May 3-5, 1992) ACM, New York, (1992), pp. 83-91.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Spence, Robert and Apperley, Mark. Data Base Navigation: An Office Environment of the Professional. Behaviour and Invormation Technology, 1, 1, (1982), 43-54.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[ImageVision, Silicon Graphics Inc., Mountain View, CA.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>6466</ref_obj_id>
				<ref_obj_pid>6465</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Swinehart, Daniel C., Zellweger, Polle T., Beach, Richard J., Hagmann, Robert B. A Structural View of the Cedar Programming Environment. ACM Transactions on Programming Languages and Systems, 8, 4, (1986), pp. 419-490.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>3865</ref_obj_id>
				<ref_obj_pid>3864</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Weyer, Stephen A. and Borning, Alan H., A Prototype Electronic Encyclopedia, ACM Transactions on Office Systems, 3, 1, (1985), pp. 63-88.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Toolglass and Magic Lenses: The See-Through Interface Eric A. Bier, Maureen C. Stone, Ken Pier, William 
Buxton , Tony D. DeRose Xerox PARC, 3333 Coyote Hill Road, Palo Alto, CA 94304 University of Toronto, 
University of Washington Abstract Toolglass widgets are new user interface tools that can appear, as 
though on a transparent sheet of glass, between an application and a traditional cursor. They can be 
positioned with one hand while the other positions the cursor. The widgets provide a rich and concise 
vocabulary for operating on application objects. These widgets may incorporate visual filters, called 
Magic Lens filters, that modify the presentation of application objects to reveal hidden information, 
to enhance data of interest, or to suppress distracting information. Together, these tools form a see-through 
interface that offers many advantages over traditional controls. screen space. ly. CR Categories and 
Subject Descriptors: H.5.2 [Information Interfaces and Presentation faces-interaction styles; I.3.3 Picture/Image 
Generation-viewing algorithms; I.3.4 [ Graphics]: Graphics Utilities-graphics editors panel, menu, transparent, 
macro  1. Introduction see-through interface. transparent interactive tools, called Toolglass tion 
and a traditional cursor. viewing filters called Magic Lens render in wireframe, and then points through 
the widgets and lenses. terface systems. Two hands can be used to operate the see-through interface. 
 object in a single two-handed gesture. Permission to copy without fee all or part of this material 
is granted Permission to copy without fee all or part of this material is granted provided that the copies 
are not made or distributed for direct provided that the copies are not made or distributed for direct 
commercial advantage, the ACM copyright notice and the title of the commercial advantage, the ACM copyright 
notice and the title of the publication and its date appear, and notice is given that copying is by publication 
and its date appear, and notice is given that copying is by permission of the Association for Computing 
Machinery. To copy permission of the Association for Computing Machinery. To copy otherwise, or to republish, 
requires a fee and/or specific permission. otherwise, or to republish, requires a fee and/or specific 
permission. &#38;#169;1993 ACM-0-89791-601-8/93/008 $1.50 &#38;#169;1993 ACM-0-89791-601-8/93/008/0015 
$1.50 A set of simple widgets called click-through buttons is shown in figure 1. These buttons can be 
used to change the color of objects below them. The user positions the widget in the vicinity and indicates 
precisely which object to color by clicking through the button with the cursor over that object, as shown 
in figure 1(b). The buttons in figure 1(c) change the outline colors of objects. In addition, these buttons 
include a filter that shows only outlines, suppressing filled areas. This filter both reminds the user 
that these buttons do not affect filled areas and allows the user to change the color of outlines that 
were obscured. programming language and environment,24 running on the SunOS UNIX .compatible operating 
system on Sun Microsystems SPARCstations and other computers. The Gargoyle graphics editor,20 as integrated 
into MMM, serves as a complex application on which to test our interface. We use a standard mouse for 
the dominant hand and a MicroSpeed FastTRAP trackball for the non-dominant hand. The trackball includes 
three buttons and a thumbwheel, which can be used to supply additional parameters to the interface. The 
remainder of this paper is organized as follows. The next section describes related work. Section 3 describes 
some examples of the tools we have developed. Section 4 discusses general techniques for using the see-through 
interface. Section 5 discusses some advantages of this approach. Section 6 describes our implementation. 
Sections 7 and 8 present our conclusions and plans for future work. Except for figures 12 and 16, all 
of the figures in this paper reflect current capabilities of our software. 2. Related Work The components 
of the see-through interface combine work in four areas: simultaneous use of two hands, movable tools, 
transparent tools, and viewing filters. In this section, we describe related work in these four areas. 
 Multi-Handed Interfaces Several authors have studied interfaces that interpret continuous gestures of 
both hands. In Krueger s VIDEOPLACEs system,15 the position and motion of both of a participant s hands, 
as seen by a video camera, determine the behavior of a variety of on-screen objects, including animated 
creatures and B-spline curves. Buxton and Myers discovered that users naturally overlap the use of both 
hands, when this is possible, and that, even when the two hands are used sequentially, there is still 
a performance advantage over single-hand use.7,8 Other work characterizes the situations under which 
people successfully perform two-handed tasks. Guiard presents evidence that people are well-adapted to 
tasks where the non-dominant hand coarsely positions a context and the dominant hand performs detailed 
work in that context.4 Similarly, Kabbash presents evidence that a user s non-dominant hand performs 
as well or better than the dominant hand on coarse positioning tasks.13 Our system takes full advantage 
of a user s two-handed skills; the non-dominant hand sets up a context by coarsely positioning the sheet, 
and the dominant hand acts in that context, pointing precisely at objects through the sheet. Movable 
Tools Menus that pop up at the cursor position are movable tools in the work area. However, such a menu 
s position is determined by the cursor position before it appears, making it difficult to position it 
relative to application objects. Several existing systems provide menus that can be positioned in the 
same work area as application objects. For example, MacDraw tear-off menus allow a pull-down menu to 
be positioned in the work area and repositioned by clicking and dragging its header.17 Unfortunately, 
moving these menus takes the cursor hand away from its task, and they must be moved whenever the user 
needs to see or manipulate objects under them. Toolglass sheets can be positioned relative to application 
objects and moved without tying up the cursor. Transparent Tools Some existing systems that allow menus 
to be positioned over the work area make these menus transparent. For example, the Alto Markup system18 
displays a menu of modes when a mouse button goes down. Each menu item is drawn as an icon, with the 
space between icons transparent. Bartlett s transparent controls for interactive graphics use stipple 
patterns to get the effect of transparency in X Windows.2 While these systems allow the user to continue 
to see the underlying application while a menu is in place, they don t allow the user to interact with 
the application through the menu and they don t use filters to modify the view of the application, as 
does our interface. Viewing Filters Many existing window systems provide a pixel magnifier. Our Magic 
Lens filters generalize the lens metaphor to many representations other than pixels and to many operations 
other than magnification. Because they can access application-specific data structures, our lenses are 
able to perform qualitatively differ­ent viewing operations, including showing hidden information and 
showing information in a completely different format. Even when the operation is magnification, our lenses 
can produce results of superior quality, since they are not limited to processing data at screen resolution. 
The concept of using a filter to change the way information is visualized in a complex system has been 
introduced before.25,10,14 Recent image processing systems support compostition of overlapping filters.23 
However, none of these systems combine the filtered views with the metaphor of a movable viewing lens. 
Other systems provide special-purpose lenses that provide more detailed views of state in complex diagrams. 
For example, a fisheye lens can enhance the presentation of complicated graphs.21 The bifocal display22 
provides similar functionallity for viewing a large space of documents. The MasPar Profiler3 uses a tool 
based on the magnifying lens metaphor to generate more detail (including numerical data) from a graphical 
display of a program. Magic Lens filters combine viewing filters with interaction and composition in 
a much broader way than do previous systems. They are useful both as a component of the see-through interface 
and as a general-purpose visualization paradigm, in which the lenses become an integral part of the model 
being viewed. 3. Examples This section shows several tools that demonstrate features of the see-through 
interface. Because we have implemented primarily in the graphical editing domain, most of these tools 
are tailored to that application. However, the see-through interface can be used in a wide variety of 
other application domains. Shape and Property Palettes Palettes are collections of objects or properties 
that can be added to a scene. Figure 1 showed two widgets that apply color to shapes. Similar tools can 
be designed to apply other graphical properties, such as type and line styles to an illustration, shading 
parameters to a 3D model, or initial values to a simulation. Figure 4 illustrates a widget containing 
graphical shapes that can be pushed through from the tool into the illustration below. In figure 4(a), 
the user has positioned a shape palette widget (shown in cyan) over an illustration (shown in magenta). 
When the user clicks on a shape on the tool, a copy of that shape is added to the illustration. The widget 
attaches the copied shape to the cursor for interactive dragging until the final shape position is achieved 
(figure 4(b)). (a) (b) Figure 4. Shape palette. (a) Choosing a shape. (b) Placing the shape. Figure 5 
shows a design for a property palette for setting the face of text in a document. Each face (regular, 
bold, etc.) has an active region on the right side of the tool. Selecting the text displayed in this 
region changes its face. temporal modes and modes created regular by holding down a keyboard key with 
italic spatial modes. Because these spatial bold modes can be changed directly in the bold italic application 
work area, the cursor and the user s attention can remain on the Figure 5. Font face palette. The word 
 directly is being selected and changed to bold face. Clipboards Clipboard widgets pick up shapes and 
properties from underlying objects, acting as visible instantiations of the copy and paste keys common 
in many applications. Clipboards can pick up entire objects or specific properties such as color, dash 
pattern or font. They can hold single or multiple copies of an object. The objects or properties captured 
on the clipboard can be copied from the clipboard by clicking on them, as in the palette tools. Figure 
6 shows a symmetry clipboard that picks up the shape that the user clicks on (figure 6(a)) and produces 
all of the rotations of that shape by multiples of 90 degrees (figure 6(b)). Moving the clipboard and 
clicking on it again, the user drops a translated copy of the resulting symmetrical shape (figure 6(c)). 
Clicking the small square in the upper left corner of the widget clears the widget so that new shapes 
can be clipped. (a) (b) (c) Figure 6. Symmetry clipboard. (a) Picking up an object. (b) Rotated copies 
appear. (c) The copies are moved and pasted. Figure 7 shows an example of a type of clipboard that we 
call a rubbing. It picks up the fill color of an object when the user clicks on that object through the 
widget (figure 7(a)). The widget also picks up the shape of the object as a reminder of where the color 
came from (figure 7(b)). Many fill-color rubbings can be placed on a single sheet, allowing the user 
to store several colors and remember where they came from. The stored color is applied to new shapes 
when the user clicks on the applicator nib of the rubbing (figure 7(c)). Besides implementing graphical 
cut and paste, clipboards provide a general mechanism for building customized libraries of shapes and 
properties. Previewing Lenses In graphical editing, a lens can be used to modify the visual properties 
of any graphical object, to provide a preview of what changing the property would look like. Properties 
include color, line thickness, dash patterns, typeface, arrowheads and drop shadows. A previewing lens 
can also be used to see what an illustration would look like under different circumstances; for example, 
showing a color illustration as it would be rendered on a black/white display or on a particular printer. 
Figure 8 shows a Celtic knotwork viewed through two lenses, one that adds drop shadows and one that shows 
the picture in black and white. The achromatic lens reveals that the drop shadows may be difficult to 
distinguish from the figure on a black/white display. Figure 8. An achromatic lens over a drop shadow 
lens over a knotwork. (Knotwork by Andrew Glassner) Previewing lenses can be parameterized. For example, 
the drop shadow lens has parameters to control the color and displacement of the shadow. These parameters 
can be included as graphical controls on the sheet near the lens, attached to input devices such as the 
thumbwheel, or set using other widgets. Selection Tools Selection is difficult in graphical editing 
when objects overlap or share a common edge. Our selection widgets address this problem by modifying 
the view and the interpretation of input actions. For example, figure 9 shows a widget that makes it 
easy to select a shape vertex even when it is obscured by other shapes. This tool contains a wire-frame 
lens that reveals all vertices by making shape interiors transparent. Mouse events are modified to snap 
to the nearest vertex. Select Select Vertex Vertex (a) (b) (c) Figure 9. Vertex selection widget. (a) 
Shapes. (b) The widget is placed. (c) A selected vertex.  (a) (b) (c) Figure 7. Fill-color rubbings. 
(a) Lifting a color. (b) Moving the clipboard. (c) Applying the color. Figure 10. The local scaling lens. 
(Tiling by Doug Wyatt) Figure 10 shows a lens that shrinks each object around its own centroid. This 
lens makes it easy to select an edge that is coincident with one or more other edges.  Grids Figure 
11 shows three widgets, each of which displays a different kind of grid. The leftmost two grids are rectangular 
with different spacings. The rightmost grid is hexagonal. Although each grid only appears when the lens 
is in place, the coordinates of the grid are bound to the scene, so that grid points do not move when 
the sheet moves. By clicking on the grid points and moving the widget, the user can draw precise shapes 
larger than the widget. If the sheet is moved by the non-dominant hand, the user can quickly switch between 
the grids during an editing motion. Visualization Figure 12. Gaussian curvature pseudo-color lens with 
overlaid tool to read the numeric value of the curvature. (Original images courtesy of Steve Mann) 4. 
Using the See-Through Interface Widgets and lenses are most effective when supported by appropriate conventions 
specifying how to position, size, organize, and customize them. This section discusses a few of these 
issues. Moving and Sizing the Sheet or the Application A Toolglass sheet can be moved by clicking and 
dragging on its border with a mouse or by rolling the trackball. The sheet and all its widgets can stretch 
and shrink as a unit when the user works a a second controller such as a thumbwheel. With these moving 
and sizing controls, the user can center a widget on any applica­tion object and size the widget to cover 
any screen region. Large widgets can be used to minimize sheet motion when applying a widget to several 
objects. A widget that has been stretched to cover the entire work area effectively creates a command 
mode over the entire application. By clicking a button on the trackball, the user can disconnect the 
trackball from the sheet and enable its use for scrolling and zooming a selected application area. If 
a sheet is over this appli­cation, the user can now move an application object to a widget instead of 
moving a widget to an object. This is a convenient way to use the see-through interface on illustrations 
that are too large to fit on the screen. Managing Sheets A typical application will have a large number 
of widgets in its in­terface. To avoid clutter, we need a way to organize these widgets and sheets. One 
approach is to put all of the widgets on a single sheet that can be navigated by scrolling and zooming. 
Perlin and Fox s paper in these proceedings19 describes tech­niques for creating and navigating unlimited 
structures on a single sheet. A second approach is to have a master sheet that generates other sheets. 
Each of these sheets could generate more sheets, like hierarchical menus. A third technique, used in 
our prototype, is to allow a single sheet to show different sets of widgets at dif­ferent times. The 
set to display can be selected in several ways: the user can click a special widget in the set, like 
the arrows in HyperCard, 11 that jumps to another set. In addition, a master view provides a table of 
contents of the available sets allowing the user to jump to any one. To use different sets simultaneously, 
the user creates additional sheets. Customizing Sheets Because sheets can contain an unlimited number 
of widgets, they provide a valuable new substrate on which users can create their own customized widgets 
and widget sets. In effect, the sheets can provide a user interface editor, allowing users to move and 
copy existing widgets, compose macros by overlapping widgets, and snap widgets together in new configurations. 
Indeed, with the techniques described in this paper, one Toolglass sheet could even be used to edit another. 
 5. Advantages of See-Through Tools In this section, we describe some advantages we see for using the 
see-through interface. Most of these advantages result from placing tools on overlapping layers and from 
the graphical nature of the interface. In most applications, a control panel competes for screen space 
with the work area of the application. Toolglass sheets exist on a layer above the work area. With proper 
management of the sheets, they can provide an unlimited space for tools. The widgets in use can take 
up the entire work area. Then, they can be scrolled entirely off the screen to provide an unobstructed 
view of the application or space for a different set of widgets. The see-through user interface can be 
used on tiny displays, such as notebook computers or personal digital assistants, that have little screen 
real estate for fixed-position control panels. It can also be used on wall-sized displays, where a fixed 
control panel might be physically out of reach from some screen positions. These tools can move with 
the user to stay close at hand. A user interface layer over the desktop provides a natural place to locate 
application-independent tools, such as a clipboard that can copy material from one window to another. 
These widgets can combine multiple task steps into a single step. For example, the vertex selection widget 
of figure 9 allows the user to turn on a viewing mode (wire-frame), turn on a command mode (selection), 
and point to an object in a single two-handed gesture. Most user interfaces have temporal modes that 
can cause the same action to have different effects at different times. With our inter­face, modes are 
defined spatially by placing a widget and the cursor over the object to be operated on. Thus, the user 
can easily see what the current mode is (e.g., by the label on the widget) and how to get out of it (e.g., 
move the cursor out of the widget). In addition, each widget can provide customized feedback for its 
op­eration. For example, a widget that edits text in an illustration can include a lens that filters 
out all the objects except text. When several widgets are visible at once, the feedback in each one serves 
a dual role. It helps the user make proper use of the widget and it helps the user choose the correct 
widget. The visual nature of the see-through interface also allows users to construct personalized collections 
of widgets as described above. 6. Implementation This section provides an overview of our implementation 
of the see-through interface. Toolglass Sheets We describe three Toolglass subsystems: one that handles 
simul­taneous input from two pointing devices and updates the screen after multiple simultaneous changes, 
one that modifies pointing events as they pass through widgets, and one that modifies graph­ical output 
as it passes up through each widget. Multi-Device Input and Screen Refresh Our Toolglass software uses 
the MMM framework.5 The see­through interface relies on the following features of MMM. MMM takes events 
from multiple input devices, such as the mouse and trackball, keeps track of which device produced which 
event, and places all events on a single queue. It dequeues each event in order and determines to which 
application that event should be delivered. MMM applications are arranged in a hierarchy that indicates 
how they are nested on the screen. Each event is passed to the root application, which may pass the event 
on to one of its child applications, which may in turn pass the event on down the tree. Mouse events 
are generally delivered to the most deeply nested application whose screen region contains the mouse 
coordinates. However, when the user is dragging or rubberbanding an object in a particular application, 
all mouse co­ordinates go to that application until the dragging or rubberbanding is completed. Keyboard 
events go to the currently selected application. To support Toolglass sheets, MMM s rules for handling 
trackball input were modified. When a sheet is movable, trackball and thumbwheel events go to the top-level 
application, which interprets them as commands to move or resize the sheet, respectively. When the sheet 
is not movable, the trackball and thumbwheel events are delivered to the selected application, which 
interprets them as commands to scroll or zoom that appli­cation. Filtering Input Through Lenses and Widgets 
 Root Application Text Editor Toolglass Sheet  Graphical Editor (a) (b) Figure 13. A simple hierarchy 
of applications Ordinarily, MMM input events move strictly from the root appli­cation towards the leaf 
applications. However, to support the see­through interface, input events must be passed back up this 
tree. For example, figure 13(b) shows an application hierarchy. The left-to-right order at the lower 
level of this tree indicates the top­to-bottom order of applications on the screen. Input events are 
first delivered to the Toolglass sheet to determine if the user is interacting with a widget or lens. 
If so, the event is modified by the sheet. In any case, the event is returned to the root applica­tion, 
which either accepts the event itself or passes it on to the child applications that appear farther to 
the right in the tree. The data structure that represents an MMM event is modified in three ways to support 
Toolglass sheets. First, an event is annotated with a representation of the parts of the application 
tree it has already visited. In figure 13, this prevents the root applica­tion from delivering the event 
to the sheet more than once. Second, an event is tagged with a command string to be interpreted when 
it reaches its final application. For example, a color palette click-through button annotates each mouse-click 
event with the command name FillColor followed by a color. Finally, if the widget contains a lens, 
the mouse coordinates of an event may be modified so the event will be correctly directed to the object 
that appears under the cursor through that lens.  (a) (b) (c) Figure 14. Composing color-changing widgets. 
Widgets can be composed by overlapping them. When a stack of overlapped widgets receives input (e.g., 
a mouse click), the input event is passed top-to-bottom through the widgets. Each widget in turn modifies 
the command string that has been assembled so far. For example, a widget might concatenate an additional 
com­mand onto the current command string. In figure 14, a widget that changes fill colors (figure 14(a)) 
is composed with a widget that changes line colors (figure 14(b)) to form a widget that changes both 
fill and line colors (figure 14(c)). If the line color widget is on top, then the command string would 
be LineColor blue after passing through this widget, and LineColor blue; FillColor cyan after both 
widgets. Filtering Output Through Lenses and Widgets Ordinarily, MMM output is composed from the leaf 
applications up. To support lenses, the normal screen refresh composition has been extended to allow 
information to flow down and across the tree as well as up. For example, if the widgets in figure 13 
contain one or more lenses, and if any of those lenses is situated over the graphical editor, each lens 
must examine the contents of the graphical editor (which is the lens s sibling in the hierarchy) in order 
to draw itself. In addition, to improve performance, MMM applications compute the rectangular bounding 
box of the regions that have recently changed, and propagate this box to the root application, which 
determines which screen pixels will need to be updated. Generally, this bounding box is passed up the 
tree, transformed along the way by the coordinate transformation between each ap­plication and the next 
one up the tree. However, lenses can modify the set of pixels that an operation affects. A magnifying 
lens, for example, generally increases the number of pixels affected. As a result, the bounding box must 
be passed to all lenses that affect it to determine the final bounding box. Magic Lens Filters A Magic 
Lens filter modifies the image displayed on a region of the screen, called the viewing region, by applying 
a viewing filter to objects in a model. The input region for the lens is defined by the viewing region 
and the viewing filter. It may be the same size as the viewing region, or different, as in the magnification 
lens. For a 3D model, the input region is a cone-shaped volume defined by the eye point and the viewing 
region. Input regions can be used to cull away all model objects except those needed to produce the lens 
image. Our current implementations do not perform this culling; as described below, there are advantages 
to lenses that operate on the entire model. When several lenses are composed, the effect is as though 
the model were passed sequentially through the stack of lenses from bottom to top, with each lens operating 
on the model in turn. In addition, when one lens has other lenses below it, it may modify how the boundaries 
of these other lenses are mapped onto the screen within its own boundary. The input region of a group 
of lenses taken as a whole can be computed by applying the inverses of the viewing filters to the lens 
boundaries themselves. Our lenses depend on the implementation of Toolglass sheets to manage the size, 
shape and motion of their viewing regions. This section describes two strategies we have tried for implementing 
viewing filters: a procedural method that we call recursive ambush, and a declarative method that we 
call model-in model­out. We also describe a third method that promises to be convenient when applicable, 
called reparameterize-and-clip Finally, we discuss issues that arise in the presence of multiple model 
types. Recursive Ambush In the recursive ambush method, the original model is described procedurally 
as a set of calls in a graphics language such as Interpress 12 or PostScript.®1 The lens is a new interpreter 
for the graphics language, with a different implementation for each graphics primitive. In most cases, 
the implementation of a given graphics primitive first performs some actions that carry out the modifying 
effect of the lens and then calls the previous implementation of the primitive. For example, a lens that 
modifies a picture such that all of its lines are drawn in red would modify the DrawLine primitive 
to set the color to red and then call the original DrawLine primitive. When lenses are composed, the 
previous implementation may not be the original graphics language primitive, but another lens primitive 
that performs yet another modification, making composition recursive. Recursive ambush lenses appear 
to have important advantages. Because they work at the graphics language level, they work across many 
applications. Because they work procedurally, they need not allocate storage. However, the other methods 
can also work at the graphics language level. In addition, recursive ambush lenses have three major disadvantages. 
First, making a new lens usually requires modifying many graphics language primitives. Second, debugging 
several composed lenses is difficult because the effects of several cooperating interpreters are hard 
to understand. Finally, performance deteriorates rapidly as lenses are composed because the result of 
each lens is computed many times; the number of computations doubles with the addition of each lens that 
overlaps all of the others. Model-In Model-Out In the model-in model-out (MIMO) method, we make a copy 
of the original model as the first step. This model might be the data structure of an editor, a representation 
of graphics language calls, an array of pixels or some other picture representation. The implementation 
walks through this data structure and modifies it in accordance with the desired behavior of the lens. 
When composed with other lenses, a MIMO lens takes each model that is produced by each lens under it, 
produces a modified version of that model, and associates it with the clipping region formed by intersecting 
its clipping region with that of the lens underneath. The resulting models are passed on to lenses above. 
Although MIMO lenses must allocate storage, this investment pays off in several ways. First, during the 
rendering of a single image, each lens computes its output models only once, and then saves them for 
use by any lenses that are over it. In addition, if the computed model is based on the entire original 
model, then redrawing the picture after a lens moves is just a matter of changing clipping regions; no 
new model filtering is needed. In this case, each lens maintains a table of the models it has produced. 
The table is indexed by the models it has received as input and when they were last modified. The action 
of such a lens often consists of a single table lookup. - Figure 15. The snowflake lens. (a) Two triangles. 
(b) Snowflake lens over part of the scene. An important variation of MIMO is to allow the output model 
to differ in type from the input model. For example, a lens might take a graphics language as input and 
produce pixels as output. In this case, the lens walks the original model, rather than copying it, and 
allocates data structures of the new model type. Reparameterize and Clip If the original image is being 
produced on the screen by a renderer with variable parameters, it is easy to implement lenses that show 
the effects of varying those parameters. To function, the lens modifies a renderer parameter and asks 
the renderer to redraw the model clipped to the boundary shape of the lens. For example, a lens showing 
the wireframe version of a 3D shaded model can be implemented this way. Several reparameterize-and-clip 
lenses can be composed if the parameter changes made by these lenses are compatible. In the region of 
overlap, the renderer re-renders the original model after each of the overlapping lenses has made its 
changes to the renderer parameters. The flow of control and performance of a stack of these lenses is 
like that of MIMO lenses; a new output is computed for each input region received from lenses underneath. 
These lenses differ from MIMO in that each output is computed from the original model, and each output 
is always a rendering. Multiple Model Types In our discussion above, lenses are used to view a single 
type of model, such as a graphical editor data structure or a graphical language. In practice, multiple 
model types are often present, for two reasons. First, a lens can overlap multiple applications at the 
same time, where the applications have different model types, as shown above in figure 13. Second, a 
lens may overlap both an application and a lens, where the lens output and application model are of different 
types. For example, in figure 16, the wireframe lens converts from a 3D model to a 2D line drawing. The 
magnifier lens, which operates on 2D drawings, overlaps both the original image and the output of the 
wireframe lens. Rich illustrations can be produced by permitting lenses to overlap mul­tiple model types 
in this way. Supporting multiple model types requires type conversion and type tolerance. When a lens 
that expects one type of model as input is moved over a model of a different type, the system may automatically 
convert the model to be of the type required; this is type conversion. For example, all of our applications 
produce Interpress graphics language calls as part of drawing themselves on the screen. When a lens that 
takes Interpress as input is positioned over one of these applications, that application converts its 
model to Interpress on demand for that lens. Figure 16. A bridge made of shaded, 3D blocks showing a 
3D wireframe lens and a 2D magnifier. Alternatively, when presented with a model it does not understand, 
a lens can simply pass that model through unchanged; this is type tolerance. For example, a lens that 
operates only on a graphics editor s data structures will only modify the image in the part of that lens 
s boundary that overlaps the graphics editor; other regions are unchanged. Composing Widgets and Lenses 
When a widget and a lens are composed, their functions combine. For example, consider a click-through 
button on top of a magnifying lens. Mouse events pass through the button, are annotated with a command, 
and then pass through the lens, which applies the inverse of its transformation to the mouse coordinates. 
During screen refresh, the widget adds its appearance to the output of the lens. If the lens is on top 
of the widget, input events are first transformed by the lens and then tested to see if they fall within 
the button or not; during refresh, the widget adds its appearance to the model, which is then acted on 
by the lens. A widget and lens can be very tightly coupled. For example, an editing tool could include 
a lens that displayed control points or editing handles implemented as widgets. Performance Our sheets 
and lenses are already fast enough to be useful on current hardware, but need to be faster for smooth 
motion. For example, using our prototype on a SPARCstation 10, we measured the time it takes to redraw 
the screen after moving a wireframe lens of size 70 by 70 pixels over the Penrose tiling of figure 10, 
containing 117 filled and outlined shapes. For the MIMO implementation of the lens, once it has cached 
its output scene, it takes an average of 300 milliseconds to repaint the scene, of which 120 milliseconds 
are spent drawing the lens interior. The same lens implemented using recursive ambush takes %15 longer 
to redraw the lens interior, which we attribute to the procedure call overhead of the recursive approach. 
Computing the filtered scene for the MIMO lens takes an average of 480 milliseconds for this example. 
This computation is performed whenever the illustration under the lens is changed or lens parameters 
are modified. Figure 17. The Magic Lenses logo. 7. Conclusions We have described a new style of user 
interface, the see-through interface, based on Toolglass widgets and Magic Lens filters. The see-through 
interface offers a new design space for user interfaces based on spatial rather than temporal modes and 
provides a natural medium for two-handed interaction. Because the interface is movable and overlays the 
application area, it takes no permanent screen space and can be conveniently adapted to a wide range 
of display sizes. Because the overlaid tools are selected and brought to the work area simply by moving 
the Toolglass sheet, the user s attention can remain focused on the work area. Because the operations 
and views are spatially defined, the user can work without changing the global context. The see-through 
interface provides a new paradigm to support open software architecture. Because Toolglass sheets can 
be moved from one application to another, rather than being tied to a single application window, they 
provide an interface to the common functionality of several applications and may encourage more applications 
to provide common functionality. Similarly, Magic Lens filters that take standard graphics languages 
as input work over many applications. In addition to their role in user interfaces, Magic Lens filters 
pro­vide a new medium for computer graphics artists and a new tool for scientific visualization. When 
integrated into drawing tools, these filters will enable a new set of effects and will speed the production 
of traditional effects. Figure 17 shows a magnifying lens and a wireframe lens used to produce our Magic 
Lenses logo. Integrated into scientific visualization tools, these filters can enhance understanding 
by providing filtered views of local regions of the data while leaving the rest of the view unchanged 
to provide context, as was shown in the visualization example in figure 12. We hope the see-through interface 
will prove to be valuable in a wide variety of applications. While the examples in this paper stress 
applications in graphical editing, these tools can potentially be used in any screen-based application, 
including spreadsheets, text editors, multi-media editors, paint programs, solid modelers, circuit editors, 
scientific visualizers, or meeting support tools. Consider that most applications have some hidden state, 
such as the equations in a spreadsheet, the grouping of objects in a graph­ical editor, or the position 
of water pipes in an architectural model. A collection of widgets and lenses can be provided to view 
and edit this hidden state in a way that takes up no permanent screen space and requires no memorization 
of com­mands. We believe that the see-through interface will increase productivity by reducing task steps 
and learning time, providing good graphical feedback, and allowing users to construct their own control 
panels and spatial modes. 8. Plans for Future Work The see-through interface is a framework that can 
be used to create many new tools in many application domains. Exploring the current space of possibilities 
will take many people many years. Furthermore, this design space will be enlarged by future software 
and hardware. We will carry out some of this exploration ourselves, creating new widgets in different 
applica­tion domains, working out taxonomies for the tools we discover, designing new conventions for 
composing, editing, navigating, organizing and triggering these tools, combining them with existing user 
interface techniques, and testing them on users performing real work. We are building two Toolglass widget 
toolkits. The first is a traditional toolkit in which widgets are created through object­oriented programming. 
The second toolkit is based on our EmbeddedButtons project;6 here, users draw new widgets and collections 
of widgets using a graphical editor and then apply behavior to these graphical forms, where the behavior 
is expressed in a user customization language. We are designing new algorithms to increase the speed 
of these tools. It is clear that Magic Lens filters and, to a lesser extent, Toolglass widgets provide 
a new way to consume the graphics power of modern computers. Finally, we are working to better understand 
how to model and implement general composition of widgets and lenses, especially those that work with 
multiple model and applications types. Acknowledgments We thank Blair MacIntyre for implementing our 
first lenses for 2D graphics and Ken Fishkin for his demonstration of lenses for text editing. We thank 
many of our colleagues at PARC for fruitful discussions and enthusiasm, including Stu Card, Ken Fishkin, 
Andrew Glassner, David Goldberg, Christian Jacobi, Jock Mackinlay, David Marimont, George Robertson, 
Marvin Theimer, Annie Zaenen, and Polle Zellweger, plus our consultants Randy Pausch and John Tukey. 
Finally, we thank Xerox Corporation for supporting this work. Trademarks and Patents: Toolglass, Magic 
Lens and Interpress are trademarks of Xerox Corporation. Postscript is a trademark of Adobe Systems, 
Inc. UNIX is a trademark of AT&#38;T. FastTRAP is a trademark of MicroSpeed Inc. Patents related to the 
concepts discussed in this paper have been applied for by Xerox Corporation. References 1. Adobe Systems 
Incorporated. PostScript® Language Reference Manual, second edition. Addison-Wesley, 1990. 2. Bartlett, 
Joel F. Transparent Controls for Interactive Graphics. WRL Technical Note TN-30, Digital Equipment Corp., 
Palo Alto, CA. July 1992. 3. Beck, Kent, Becher, Jon, and Zaide, Liu. Integrating Profiling into Debugging. 
Proceedings of the 1991 International Conference on Parallel Processing, Vol. II, Software, August 1991, 
pp. II-284-II-285. 4. Guiard, Yves. Asymmetric Division of Labor in Human Skilled Bimanual Action: The 
Kinematic Chain as a Model. The Journal of Motor Behavior, 19, 4, (1987), pp. 486-517. 5. Bier, Eric 
A. and Freeman, Steve. MMM: A User Interface Architecture for Shared Editors on a Single Screen.  Proceedings 
of the ACM SIGGRAPH Symposium on User Interface Software and Technology (Hilton Head, SC, November 11-13), 
ACM, New York, (1991), pp. 79-86. 6. Bier, Eric A., EmbeddedButtons: Supporting Buttons in Documents. 
ACM Transactions on Information Systems, 10, 4, (1992), pp. 381-407. 7. Buxton, William and Myers, Brad 
A.. A Study in Two-Handed Input. Proceedings of CHI 86 (Boston, MA, April 13-17), ACM, New York, (1986), 
pp. 321-326. 8. Buxton, William. There s More to Interaction Than Meets the Eye: Some Issues in Manual 
Input. Readings in Human-Computer Interaction: A Multidisciplinary Approach. (Ronald M. Baecker, William 
A.S. Buxton, editors). Morgan Kaufmann Publishers, Inc., San Mateo, CA. 1987. 9. Dill, John. An Application 
of Color Graphics to the  Display of Surface Curvature. Proceedings of SIGGRAPH 81 (Dallas, Texas, August 
3-7). Computer Graphics, 15, 3, (1981), pp. 153-161. 10. Goldberg, Adele and Robson, Dave, A Metaphor 
for User Interface Design, Proceedings of the University of Hawaii Twelfth Annual Symposium on System 
Sciences, Honolulu, January 4-6, (1979), pp.148-157. 11. Goodman, Danny. The Complete HyperCard Handbook. 
Bantam Books, 1987. 12. Harrington, Steven J. and Buckley, Robert R.. Interpress, The Source Book. Simon 
&#38; Schuster, Inc. New York, NY. 1988. 13. Kabbash, Paul, MacKenzie, I. Scott, and Buxton, William. 
Human Performance Using Computer Input Devices in the Preferred and Non-preferred Hands. Proceedings 
of InterCHI 93, (Amsterdam, April 24-29), pp. 474-481. 14. Krasner, Glenn and Hope, Stephen, A Cookbook 
for Using the Model-View-Controller User Interface Paradigm in Smalltalk-80, Journal of Object-Oriented 
Programming, 1, 3, (1988), pp. 26-49. 15. Krueger, Myron W., Gionfriddo, Thomas, and Hinrichsen, Katrin. 
VIDEOPLACE - An Artificial Reality. Proceedings of CHI 85 (San Francisco, April 14-18). ACM, New York, 
(1985), pp. 35-40. 16. Kurlander, David and Bier, Eric A.. Graphical Search and Replace. Proceedings 
of SIGGRAPH 88 (Atlanta, Georgia, August 1-5) Computer Graphics, 22, 4, (1988), pp. 113-120. 17. MacDraw 
Manual. Apple Computer Inc. Cupertino, CA 95014, 1984. 18. Newman, William. Markup User s Manual. Alto 
User s Handbook, Xerox PARC technical report, (1979), pp. 85-96. 19. Perlin, Ken and Fox, David. Pad: 
An Alternative Approach to the Computer Interface. this proceedings. 20. Pier, Ken, Bier, Eric, and 
Stone, Maureen. An Introduction to Gargoyle: An Interactive Illustration Tool. Proceedings of the Intl. 
Conf. on Electronic Publishing, Document Manipulation and Typography (Nice, France, April). Cambridge 
Univ. Press, (1988), pp. 223-238. 21. Sarkar, Manojit and Brown, Marc H.. Graphical Fisheye Views of 
Graphs. Proceedings of CHI 92, (Monterey, CA, May 3-5, 1992) ACM, New York, (1992), pp. 83-91. 22. Spence, 
Robert and Apperley, Mark. Data Base Navigation: An Office Environment of the Professional. Behaviour 
and Invormation Technology, 1, 1, (1982), 43-54. 23. ImageVision, Silicon Graphics Inc., Mountain View, 
CA. 24. Swinehart, Daniel C., Zellweger, Polle T., Beach, Richard J., Hagmann, Robert B.. A Structural 
View of the Cedar Programming Environment. ACM Transactions on Programming Languages and Systems, 8, 
4, (1986), pp. 419-490. 25. Weyer, Stephen A. and Borning, Alan H., A Prototype Electronic Encyclopedia, 
ACM Transactions on Office Systems, 3, 1, (1985), pp. 63-88.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166127</article_id>
		<sort_key>81</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[An interactive 3D toolkit for constructing 3D widgets]]></title>
		<page_from>81</page_from>
		<page_to>84</page_to>
		<doi_number>10.1145/166117.166127</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166127</url>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.2.2</cat_node>
				<descriptor>User interfaces</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.1.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011066.10011069</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Development frameworks and environments->Integrated and visual development environments</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011058</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Visual languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P246989</person_id>
				<author_profile_id><![CDATA[81100099358]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Zeleznik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P160097</person_id>
				<author_profile_id><![CDATA[81332503677]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kenneth]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Herndon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P59283</person_id>
				<author_profile_id><![CDATA[81100066066]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Robbins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14038297</person_id>
				<author_profile_id><![CDATA[81452602804]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Nate]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31086605</person_id>
				<author_profile_id><![CDATA[81332515403]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Meyer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P209646</person_id>
				<author_profile_id><![CDATA[81100169621]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Noah]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Parker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40024462</person_id>
				<author_profile_id><![CDATA[81100166298]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Hughes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AVS, Inc. AVS Developer'sGuide, v. 3.0, 1991.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808573</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[A.H. Barr. Global and local deformations of solid primitives. Computer Graphics (SIGGRAPH '84 Proceedings), 18(3):21- 30, July 1984.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91446</ref_obj_id>
				<ref_obj_pid>91394</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Eric A. Bier. Snap-dragging in three dimensions. Computer Graphics (1990 Symposium on Interactive 3D Graphics), 24(2):193-204, March 1990.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>108874</ref_obj_id>
				<ref_obj_pid>108844</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Stuart K. Card, George G. Robertson, and Jock D. Mackinlay. The information visualizer, an information workspace. In Proceedings of ACM CHI' 91 Conference on Human Factors in Computing Systems, pages 181-188, 1991.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147199</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[D. Brookshire Conner, Scott S. Snibbe, Kenneth E Herndon, Daniel C. Robbins, Robert C. Zeleznik, and Andries van Dam. Three-dimensional widgets. Computer Graphics (1992 Symposium on Interactive 3D Graphics), 25(2):183-188, March 1992.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[James D. Foley, Andries van Dam, Steven Feiner, and John E Hughes. Computer Graphics: Principles and Practice. Addison-Wesley, 2nd edition, 1990.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134088</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Michael Gleicher and Andrew Witkin. Through-the-lens camera control. Computer Graphics (SIGGRAPH '92 Proceedings), 26(2):331-340, July 1992.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378494</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Paul E. Haeberli. Conman: A visual programming language for interactive graphics. Computer Graphics (SIGGRAPH '88 Proceedings), 22(4): 103-111, August 1988.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>142622</ref_obj_id>
				<ref_obj_pid>142621</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Kenneth P. Herndon, Robert C. Zeleznik, Daniel C. Robbins, D. Brookshire Conner, Scott S. Snibbe, and Andries van Dam. Interactive shadows. 1992 UIST Proceedings, pages 1-6, November 1992.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134087</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Michael Kass. CONDOR: Constraint-based dataflow. Computer Graphics (SIGGRAPH '92 Proceedings), 26(2):321- 330, July 1992.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>93804</ref_obj_id>
				<ref_obj_pid>93791</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Brad A. Myers, Dario A. Guise, Roger B. Dannenberg, Brad Vander Zanden, David S. Kosbie, Edward Pervin, Andrew Mickish, and Philippe Marchal. GARNET comprehensive support for graphical, highly interactive user interfaces. IEEE COMPUTER magazine, pages 71-85, November 1990.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Open Software Foundation. OSF/Motif Reference Guide.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Steve Sistare. Graphical interaction techniques in constraintbased geometric modeling. In Steve MacKay and Evelyn M. Kidd, editors, Graphics Interface '91 Proceedings, pages 161-164. Canadian Man-Computer Communications Society, March 1991.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134091</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Scott S. Snibbe, Kenneth E Herndon, Daniel C. Robbins, D. Brookshire Conner, and Andries van Dam. Using deformations to explore 3d widget design. Computer Graphics (SIGGRAPH '92 Proceedings),26(2):351-352, July 1992.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134089</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Paul S. Strauss and Rikk Carey. An object-oriented 3d graphics toolkit. Computer Graphics (SIGGRAPH '92 Proceedings), 26(2):341-349, July 1992.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122730</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Robert C. Zeleznik, D. Brookshire Conner, Matthias M. Wloka, Daniel G. Aliaga, Nathan T. Huang, Philip M. Hubbard, Brian Knep, Henry Kaufman, John F. Hughes, and Andries van Dam. An object-oriented framework for the integration of interactive animation techniques. Computer Graphics (SIGGRAPH '91 Proceedings), 25(4):105-112, July 1991.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Interactive 3D Toolkit for Constructing 3D Widgets Robert C. Zeleznik, Kenneth P. Herndon, Daniel 
C. Robbins, Nate Huang, Tom Meyer, Noah Parker and John F. Hughes Brown University Department of Computer 
Science Providence, RI 02912 (401) 863-7693; bcz,kph,dcr,nth,twm,nfp,jfh@cs.brown.edu f g CR Categories 
networks, data-and control-.ow graphs). A second paradigm, based on graphically manipulating function 
networks [1; 8; 10], is I.3.6 [Computer Graphics]: Methodology and Techniques; Inter­more accessible 
to the non-programmer, but still suffers because action Techniques D.1.7 [Programming Languages]: Programming 
inherently geometric relationships must be speci.ed by wiring 2D Techniques; Visual Programming D.2.2 
[Software Engineering]: boxes together. Tools and Techniques; User Interfaces Our toolkit uses direct 
manipulation of 3D widgets to model the construction of widgets and application objects whose geometric 
 1 Introduction Today s user interfaces for most 3D graphics applications still de­pend heavily on 2D 
GUIs and keyboard input. There have been several recent attempts both to extend these user interfaces 
into 3D and to describe intermediary 3D widgets1 that control application objects [3; 4; 5; 7; 13; 15]. 
Even though this style of interaction is a straightforward extension of interaction through intermediary 
2D widgets such as dials or sliders, we know of no efforts to develop interactive 3D toolkits akin to 
UIMX or Garnet [11]. The Brown Graphics Group has had considerable experience us­ing its Uni.ed Graphics 
Architecture (UGA) system [16] to script 3D widgets such as deformation racks [14], interactive shadows 
[9], parameterized models, and other constrained 3D geometries. Us­ing this experience, we have developed 
an interactive toolkit to facilitate the visual programming of the geometry and behavior of such interactive 
models. The toolkit provides both a core set of 3D widget primitives for constructing interactive behaviors 
based on constrained af.ne transformations, and an interactive 3D interface for combining these primitives 
into more complex widgets. This video paper describes the fundamental concepts of the toolkit and its 
core set of primitives. In particular, we describe (i) the conceptual structure of the primitives, (ii) 
the criteria used to select a particular primitive widget set that would be expressive enough to let 
us construct a wide range of interactive 3D objects, and (iii) the constraint relationships among the 
primitives. 2 Overview of our 3D Toolkit The traditional approach to designing user interface toolkits 
is to create a library of software objects and customize them through instantiation and specialization 
within standard programming lan­guages [12; 15]. Although this approach is extremely powerful, ex­ploring 
the full potential requires that programmers be able to visual­ize complex relationships among software 
objects ( e.g., constraint Permission to copy without fee all or part of this material is granted provided 
that the copies are not made or distributed for direct provided that the copies are not made or distributed 
for direct commercial advantage, the ACM copyright notice and the title of the commercial advantage, 
the ACM copyright notice and the title of the publication and its date appear, and notice is given that 
copying is by publication and its date appear, and notice is given that copying is by permission of the 
Association for Computing Machinery. To copy permission of the Association for Computing Machinery. To 
copy otherwise, or to republish, requires a fee and/or specific permission. otherwise, or to republish, 
requires a fee and/or specific permission. &#38;#169;1993 ACM-0-89791-601-8/93/008 $1.50 &#38;#169;1993 
ACM-0-89791-601-8/93/008/0015 $1.50 1That is, encapsulations of geometry and behavior. components are 
af.nely constrained. This paradigm is more natural than scripting or data.ow programming because the 
process of constructing such objects is inherently geometric, and also enables non-programmers and designers 
to construct these objects visually. The scope of these constructions includes, for example, all of the 
widgets we have built in the last few years and standard joints such as slider, pin, and ball joints. 
We introduce the notion of primitive 3D widgets that can be combined with other primitive 3D widgets, 
using a process called linking, to establish one or more constraint relations between them. In some cases, 
the resulting composite objects are still considered widgets; in others, they are thought of as the behavioral 
scaffolding to which the geometry of application objects can be attached. The fact that the interface 
and application objects exist in the same underlying system, UGA, allows us to blur the distinction between 
them. We feel that such blurring is natural for 3D applications in general, and especially for virtual 
reality applications. Linking is related to snapping [3], but differs in requiring explicit interactive 
selection of source and destination objects, followed by explicit user con.rmation. This protocol reduces 
clutter by eliminating alignment objects. In the interest of simplifying the user interface, all linking 
operations are unparameterized, although in future work, parameterized linking for more advanced users 
and more complicated widgets will be explored. 3 Conceptual Structure of Widget Primitives A primitive 
widget combines the geometries and behaviors of its ports and other more simple primitives. A port is 
an encapsulation of one or more constraint values and a geometric representation. It can be loosely considered 
a data type with the additional requirement that its visual appearance suggest the meaning of the data. 
Ports are related to one another within a single widget via a network of bi­directional constraints. 
In addition, speci.c interaction techniques are associated with each port. Each interaction technique 
tells how to modify a port while maintaining constraints on other ports. For example, if a user manipulates 
a point that is constrained to be on a line, the constraint could be resolved by moving the line with 
the point, by restricting the user s interaction so that the point never leaves the line, or by a combination 
of the two. We must choose one of these as we implement the toolkit. These interaction techniques can 
be thought of as hints to a constraint solver when the constraint network is underdetermined so it can 
provide real-time, precise interactions. In addition to having an internal constraint network, a primitive 
widget can be related to another primitive widget by linking a port of the former to a port of the latter. 
This establishes a constraint (bi-or uni-directional) between the two ports. Ports are already constrained 
by the internal constraint network of a primitive, and the new constraints must be consistent with the 
existing constraints. Therefore, associated with each port is a function that determines how to attach 
new constraints to that port and how to modify its interaction techniques so as to facilitate constraint 
maintenance. 4 Description of the Toolkit Primitives Having selected this framework to build our toolkit, 
we designed a general set of primitives to allow the interactive construction of not only the various 
3D widgets previously scripted,but also application objects such as parameterized geometric models. These 
primitives are intended to be general enough to allow exploration of a wide set of object designs without 
having to resort to hand-coding. We chose a coordinate system metaphor as a basis for our primitives. 
Each primitive visually represents a 0D, 1D, 2D, or 3D coordinate system and each can be constrained 
by af.ne transforma­tions to the coordinate systems of other primitives. This metaphor can be used to 
express a wide variety of user interactions, includ­ing those of our previous 3D widgets [5; 14; 9]. 
However, the coordinate-system metaphor is only a framework for conceptual­izing the primitives, not 
a strict de.nition of them. That is, the primitives were designed with regard to the sometimes antagonistic 
desires both to represent the coordinate system metaphor faithfully and to provide the semantics most 
useful for geometric and behav­ioral constructions. The toolkit has primitives that correspond to position, 
orienta­tion, measure (linear and angular), 2D and 3D Cartesian coordinate systems, a general extension 
mechanism for importing an arbitrary relationship, and the full set of UGA s geometric models. The two 
most basic primitives, Point and Ray, encapsulate po­sition and orientation respectively. Points and 
Rays represent 0D coordinate system entities; i.e., there isonlyoneelement2 of aPoint or a Ray and therefore 
0 coordinates are required to specify it. (Con­trast this with a line, which has an in.nite number of 
elements, each speci.ed by one coordinate.) The Point primitive, represented by a small sphere, is an 
abstraction of a single 3D point. The Ray primitive, represented by an arrow, corresponds to a based 
vector, although we often treat it as just a vector (its position being a display convenience). Both 
primitives can be freely translated in space, but only the Ray can be rotated. The notion of distance 
(linear measure) is represented through a 1D coordinate system primitive, the Length, represented by 
two Points, a port for the 1D coordinate system (represented by a thin cylinder connecting the Points), 
and a port for the Length s measure (represented by a small marker at the middle of the thin cylinder). 
While the Length appears as a bounded line segment, it actually encapsulates the notion of an in.nite 
1D coordinate system whose origin is at the line s start point (indicated by a small disc) and whose 
unit length is equal to the distance between the two points measured in the world coordinate system. 
We reuse the Point for the endpoints to help de.ne the user interaction with the Length. Each of the 
Length s endpoints can be directly translated while the other remains .xed. Translating the cylinder 
joining the two Points translates both endpoints by the same amount. An alternate formulation of the 
Length would have both endpoints move whenever either was translated. Choosing either formulation is 
dif.cult in the absence of an application, so we chose the technique that seemed most useful. Angular 
measure is represented by a two-handedclock-like prim­itive, the Angle. Each hand of the clock represents 
a vector and the outer ring of the clock represents the angle between the two vectors. 2In the sense 
of sets. 2D coordinate system Projection of Up Vector Resize Handle  Figure 1: The ports of the Plane 
primitive. The most complex primitive, the Plane, represents both a 2D Cartesian plane and a 3D Cartesian 
space. We opted to combine both concepts into a single primitive because users frequently use the two 
concepts in conjunction with one another and because the sets of ports are nearly identical, with a space 
being a superset of a plane. Visualizing an oriented plane requires ports for the plane s normal, center, 
and up-vector (similar to the PHIGS VUP), and for the size of a unit vector in each of the plane s axes. 
In addition, a port is required for the concept of the plane itself (as opposed to parameters that de.ne 
the plane). A rectangle in the plane represents this port; its size determines the magnitude of each 
unit vector in the Plane s coordinate system. We also include a useful port for the projection of the 
plane s up-vector onto the plane, although this is not a required part of a Cartesian plane. To handle 
a 3D Cartesian space, the only additional port required is something to represent the concept of the 
space itself. The Plane reuses Points and Rays and introduces new geometry to represent the concept of 
the plane (a rectangle) and the space (a cube at the top of the up vector). In order that the toolkit 
be extensible enough to handle new problem domains, there are also Black-box primitives, each rep­resenting 
a relationship with some number of ports that lacks a natural geometric representation. Ports on black-boxes 
are geomet­rically represented as labeled buttons. The accompanying video shows two Black-boxes: an interface 
to Barr s nonlinear deforma­tion functions [2] and a PHIGS camera speci.cation [6]. Finally, all the 
geometric objects in 3D modeling environments (cubes, spheres, CSGs, etc.) are considered collectively 
as a single primitive class called Geometries. In terms of the data it represents, each of the Geometries 
is essentially equivalent to a Cartesian space, although it is not annotated with additional geometry 
(as is the Plane primitive). In our system, each geometric object has an internal boundaryrepresentation 
relative to a local object coordinate system. This local coordinate system is used as a default coordinate 
system associatedwith a Geometry primitive to make it functionally equivalent to a Cartesian space. Since 
Geometries are not annotated with the ports of a Plane primitive, linking operations must infer from 
the context of the link operation which port of the implicit Cartesian space is intended. Linking operations 
usually apply to the origin of the Geometry s local coordinate system, though they can apply to the local 
coordinate system s normal and up-vector. When the default linking operation chooses the wrong port, 
the user can override the choice by making the object s local coordinate system explicit and choosing 
ports directly. 5 Linking the Toolkit Primitives We now describe what occurs in the toolkit when a port 
of one primitive is linked to a port of another. Again, our choices for the semantics of inter-primitive 
linking are guided by the desire to stay close to the coordinate-system metaphor and the desire to have 
reasonable behaviors when there is no obvious answer in the underlying metaphor. A linking operation 
generally asserts one of two types of re­lations: it either establishes a bi-directional equality relationship 
between two similarly typed ports or projects one port into the coor­dinate system of the other port, 
using their common 3D embedding as the medium of projection. Consider linking a Point to another Point: 
here, the .rst Point is set to be positionally equivalent to the second Point. However, linking a Ray 
to a Ray is slightly different in that the orientation of the .rst Ray is made equivalent to that of 
the secondRay, but the po­sitions of the two Rays remain distinct. Rotating either Ray causes the other 
to change, but translating either Ray has no effect on the other. This choice of how to link two Rays 
together is ambiguous, because a Ray actually represents two geometric values, a position and an orientation. 
Thus the action is chosen by considering the context of the linking operation. In linking a Ray to a 
Ray, the user typically wants them both to have the same orientation, so only the orientation values 
are linked. If a user wishes to equate the positions of the Rays, then the position port of the Ray must 
be made explicit by linking each Ray to a common Point. A different form of linking occurs when a lower-dimensional 
primitive is linked to a higher-dimensional one. Such a link causes the lower-dimensional primitive to 
be geometrically projected onto the implied span3 of the higher-dimensional primitive. After this projection, 
the lower-dimensional primitive is associated with a coordinate in the higher-dimensional primitive based 
on the loca­tion of the lower-dimensional primitive in the span of the higher­dimensional primitive. 
This association is then enforced during subsequent manipulation. Typically, higher-dimensional primitives 
are composed of a number of lower-dimensional primitives, each of which can still be linked to higher-level 
primitives (e.g., the center point of a Plane primitive is a Point primitive and can be linked to other 
higher-dimensional primitives.) To illustrate, consider linking a Point to a Plane. This link operation 
causes the Point s position to be projected onto the Plane. The Point is then constrained to be at the 
coordinate associated with that projection point, unless it is moved directly. Whenever the Plane is 
manipulated, the Point will remain at the same position relative to the origin and orientation of the 
Plane. Yet, if the Point is manipulated, it will move in the span of the Plane, and thereby change its 
associated coordinate in the Plane s span. Some link operations do not fall directly into either category. 
When this occurs, we chose what we considered the most reason­able solution. For example, we de.ned the 
linking of a Geometry primitive to a Length s measure port as a scale operation on the Geometry primitive 
along the axis of the Length. If the Length s orientation is linked to a principal axis of the Geometry 
primitive (or vice versa), then the Length acts as a standard 1D scale operation along that axis; otherwise 
it is a shear. Figure 2 displays the link behavior that applies to the toolkit primitives when neither 
primitive has been linked to anything else. In cases where one primitive has already been linked, very 
different behavior may result; space prevents us from de.ning all these pos­sibilities. Consider a Point 
linked to a Plane. The Point becomes constrained to move only in the Plane. If the Point is subsequently 
linked to a second Point, a different table takes into account the pre­existing constraints on the .rst 
Point. In this case, the .rst Point is constrained to lie at the position of the projection of the second 
Point onto the Plane. 3In the linear algebra sense; a Length s span is the line de.ned by the endpoints, 
a Plane s span is the plane de.ned by the Plane s center point and normal vector. 6 Implementation details 
The toolkit is implemented in UGA s scripting language, with ge­ometry provided by UGA s interactive 
solids modeler. The linking constraints between primitives are established using UGA s object­dependency 
network. User feedback is provided in the course of a linking operation to aid in link speci.cation. 
When the user picks a primitive to be linked, it is highlighted and the cursor changes to indicate that 
the system is waiting for the user to pick the object to link to. After the user picks the object to 
link to, the system indicates its ready state through a cursor change that prompts for a mouse click 
to con.rm the link. Otherhighlightingmethodsindicateaprimitive s degreesoffree­dom. For example, a Ray, 
like other primitive widgets, is green when it is created, indicating that it is unconstrained. If it 
is linked to another Ray, its orientation is linked but not its translation, and it turns yellow to indicate 
a partial constraint. When it is linked again to a Point, it turns red, indicating that all of its degrees 
of freedom are constrained. Another possibility would have been to change the primitive geometries after 
linking (e.g., a spherical Point primitive could become a thin cylinder when it is linked to a Length, 
and could become a disc when linked to a Plane, although this strategy can result in a overly large collection 
of shapes). 7 Future Work The toolkit as described lacks techniques for specifying range limits on a 
primitive s degrees of freedom. These would be especially useful when modeling the behavior of real-world 
objects, or when creating interface objects such as bounded sliders, joints, and dials. We intend to 
add this functionality (and perhaps other inequality constraints too), and also extend the range of our 
toolkit to deal with other graphics concerns, such as surface and volumetric modeling, scienti.c data 
exploration of scalar and vector .elds, and behavior modeling including dynamic simulations. When two 
primitives are linked together,a single constraint based on Figure 2 is installed. However, it would 
often be useful to have a set of possible link behaviors that the user can select from. Advanced users 
would be able directly select the desired behavior with only a single link operation. Once a complex 
widget has been constructed from primitives, it is useful to interactively encapsulate it, along with 
appropriate parameters, for reuse in a tool library. For example, having con­structed a shadow widget, 
the user should be able to easily apply the same process to any other object. This amounts to interac­tively 
de.ning a function and embodying it in a new, higher-level primitive. Highly complex widgets linked together 
from dozens of primi­tives may present ef.ciency problems, especially for real-time inter­action. It 
may be necessary to optimize the constraint network after the widget has been completed in order to maximize 
the toolkit s evaluation speed. It would also be useful to display graphically the constraint relations 
between primitives to provide feedback on the links established on any widget.  8 Conclusions This toolkit 
provides a methodology for interactively constructing the geometric behavior of a variety of 3D widgets 
and parameter­ized 3D application objects, so that non-technical users can rapidly and interactively 
generate constrained 3D objects. Previously, such widget construction required programming in C or our 
scripting lan­guage. Even for experienced programmers, graphical construction is a more suitable and 
ef.cient environment to conceive, prototype, and implement many types of interactive 3D objects. destination 
source Linking constrains Point Ray Length Body Length Measure Angle Measure Plane Frame Plane Space 
Geometry Point positions are equated Point to lie on Ray Point to lie on Length body Point to lie in 
Plane Point to be in Plane s 3D coordinate system position of Point to position of Geometry Ray Ray to 
position of Point orientations are equated orientation of Ray to orientation of Length orientation of 
Ray to be orientation of length measure orientation and position of Ray to lie in Plane Ray to be in 
Plane s 3D coordinate system orientation of Ray to orientation of Geometry Length Body End Points of 
Length to lie on Ray  End Points of Length to lie in Plane End Points of Length to be in Plane s 3D 
coordinate system Length Measure End Points of Length to lie on Ray length of .rst Length to be length 
of second Length length of Length to map to Angle s measure  Angle Measure  Angle s measure to map 
to length of Length .rst Angle s measure to be second Angle s measure  Geometry position of Geometry 
to position of second Point orientation of Geometry to Ray s orientation Geometry to lie on Length body 
scale of Geometry to length of Length Geometry to lie in Plane Geometry to be in Plane s 3D coordinate 
system positions are equated Figure 2: Linking behaviors for unconstrained primitives. Acknowledgments 
This work was supported in part by the NSF/ARPA Science and Technology Center for Computer Graphics and 
Scienti.c Visualiza­tion and by ONR Contract N00014-91-J-4052, ARPA Order 8225. We also gratefully acknowledge 
the sponsorship of IBM, NCR, Sun Microsystems, Hewlett Packard, Digital Equipment Corporation, and NASA. 
We thank Andries van Dam and the members of the Brown University Graphics Group for their help and support. 
Please contact the authors for a copy of the accompanying videotape. References [1] AVS, Inc. AVS Developer 
s Guide, v. 3.0, 1991. [2] A. H. Barr. Global and local deformations of solid primitives. Computer Graphics 
(SIGGRAPH 84 Proceedings) , 18(3):21 30, July 1984. [3] Eric A. Bier. Snap-dragging in three dimensions. 
Com­puter Graphics (1990 Symposium on Interactive 3D Graph­ics), 24(2):193 204, March 1990. [4] Stuart 
K. Card, George G. Robertson, and Jock D. Mackinlay. The information visualizer, an information workspace. 
In Proceedings of ACM CHI 91 Conference on Human Factors in Computing Systems, pages 181 188, 1991. [5] 
D. Brookshire Conner, Scott S. Snibbe, Kenneth P. Herndon, Daniel C. Robbins, Robert C. Zeleznik, and 
Andries van Dam. Three-dimensional widgets. Computer Graphics (1992 Sym­posium on Interactive 3D Graphics) 
, 25(2):183 188, March 1992. [6] James D. Foley, Andries van Dam, Steven Feiner, and John F. Hughes. 
Computer Graphics: Principles and Practice . Addison-Wesley, 2nd edition, 1990. [7] Michael Gleicher 
and Andrew Witkin. Through-the-lens cam­era control. Computer Graphics (SIGGRAPH 92 Proceed­ings), 26(2):331 
340, July 1992. [8] Paul E. Haeberli. Conman: A visual programming language for interactive graphics. 
Computer Graphics (SIGGRAPH 88 Proceedings), 22(4):103 111, August 1988. [9] Kenneth P. Herndon, Robert 
C. Zeleznik, Daniel C. Robbins, D. Brookshire Conner, Scott S. Snibbe, and Andries van Dam. Interactive 
shadows. 1992 UIST Proceedings , pages 1 6, November 1992. [10] Michael Kass. CONDOR: Constraint-based 
data.ow. Com­puter Graphics (SIGGRAPH 92 Proceedings) , 26(2):321 330, July 1992. [11] Brad A. Myers, 
Dario A. Guise, Roger B. Dannenberg, Brad Vander Zanden, David S. Kosbie, Edward Pervin, An­drew Mickish, 
and Philippe Marchal. GARNET comprehen­sive support for graphical, highly interactive user interfaces. 
IEEE COMPUTER magazine, pages 71 85, November 1990. [12] Open Software Foundation. OSF/Motif Reference 
Guide. [13] Steve Sistare. Graphical interaction techniques in constraint­based geometric modeling. In 
Steve MacKay and Evelyn M. Kidd, editors, Graphics Interface 91 Proceedings , pages 161 164. Canadian 
Man-Computer Communications Society, March 1991. [14] Scott S. Snibbe, Kenneth P. Herndon, Daniel C. 
Robbins, D. Brookshire Conner, and Andries van Dam. Using defor­mations to explore 3d widget design. 
Computer Graphics (SIGGRAPH 92 Proceedings), 26(2):351 352, July 1992. [15] Paul S. Strauss and Rikk 
Carey. An object-oriented 3d graphics toolkit. Computer Graphics (SIGGRAPH 92 Proceedings) , 26(2):341 
349, July 1992. [16] Robert C. Zeleznik, D. Brookshire Conner, Matthias M. Wloka, Daniel G. Aliaga, Nathan 
T. Huang, Philip M. Hub­bard, Brian Knep, Henry Kaufman, John F. Hughes, and An­dries van Dam. An object-oriented 
framework for the integra­tion of interactive animation techniques. Computer Graphics (SIGGRAPH 91 Proceedings), 
25(4):105 112, July 1991.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166128</article_id>
		<sort_key>85</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[EXACT]]></title>
		<subtitle><![CDATA[algorithm and hardware architecture for an improved A-buffer]]></subtitle>
		<page_from>85</page_from>
		<page_to>91</page_to>
		<doi_number>10.1145/166117.166128</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166128</url>
		<keywords>
			<kw><![CDATA[A-buffer]]></kw>
			<kw><![CDATA[anti-aliasing]]></kw>
			<kw><![CDATA[exact area coverage calculation]]></kw>
			<kw><![CDATA[priority-masks]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Antialiasing**</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010386</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Antialiasing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP35046241</person_id>
				<author_profile_id><![CDATA[81100595324]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andreas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schilling]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39084084</person_id>
				<author_profile_id><![CDATA[81100575262]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wolfgang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stra&#223;er]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>808585</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CARPENTER, L. The a-buffer, an antialiased hidden surface method. Computer Graphics 18, 3 (July 1984), 103-108.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[COHEN, D. A vlsi approach to the cig problem. Presentation at SIGGRAPH 1980, 1980.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378468</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[DEERING, M., WINNER, S., SCHEDIWY, B., DUFFY, C., AND HUNT, N. The triangle processor and normal vector shader: A vlsi system for high performance graphics. Computer Graphics 22, 4 (Aug. 1988), 21-30.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>142455</ref_obj_id>
				<ref_obj_pid>142443</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DUNNETT, G. J., WHITE, M., LISTER, P. F., GRIMSDALE, R. L., AND GLEMOT, F. The image chip for high performance 3d rendering. IEEE Computer Graphics &amp; Applications 12, 6 (Nov. 1992), 41-52.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801143</ref_obj_id>
				<ref_obj_pid>964967</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[FIUME, E., FOURNIER, A., AND RUDOLPH, L. A parallel scan conversion algorithm with anti-aliasing for a general-purpose ultracomputer. Computer Graphics 17, 3 (July 1983), 141- 150.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74341</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[FUCHS, H., POULTON, J., EYLES, J., GREER, T., GOLDFEATHER, J., ELLSWoRTH, D., MOLNAR, S., TURK, G., TEBBS, B., AND ISRAEL, L. Pixel-planes 5: A heterogeneous multiprocessor graphics system using processor-enhanced memories. Computer Graphics 23,3 (July 1989), 79-88.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134067</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[MOLNAR, S. Pixelflow: High-speed rendering unsing image composition. Computer Graphics 26,2 (July 1992), 231-240.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122733</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[SCHILLING, A. G. A new simple and efficient antialiasing with subpixel masks. Computer Graphics 25, 4 (July 1991), 133-141.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>58990</ref_obj_id>
				<ref_obj_pid>58985</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[SCHNEIDER, B.-O. A processor for an object-oriented rendering system. Computer Graphics Forum 7 (1988), 301-310.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806789</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[WEINBERG, R. Parallel processing image synthesis and antialiasing. Computer Graphics 15,3 (Aug. 1981), 55-62.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>145453</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[MOLNAR, S. Image-Composition Architectures for Real-Time Image Generation. PhD thesis, University of North Carolina at Chapel Hill, 1991.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 EXACT: Algorithm and Hardware Architecture for an Improved A-Buffer Andreas Schilling, Wolfgang Straßer 
Universit¨ubingen at T¨ . Bundesrepublik Deutschland Abstract 1 The Problem: Exact anti-aliasing The 
EXACT (EXact Area Coverage calculaTion) algo-Rasterizing produces aliasing artifacts. If a box .lter 
is used to rithm presented in this paper solves the Hidden Surface perform anti-aliasing the brightness 
and color of edge pixels are Elimination (HSE) problem on the subpixel level. functions of the pixel 
area covered by the objects as well as of the object colors. The ideal intensity would be described by 
the formula The use of subpixel masks for anti-aliasing causes some 1, where and are the areas and intensities 
of  I f A P i I i A i A i I i A problems with the HSE on the pixel level that are dif­the visible surfaces 
within the pixel and is the total pixel area. .cult to overcome. The approximations of the well Subpixel 
masks can be used to calculate the fraction of the pixelknown A-buffer algorithm are replaced by an exact 
so­area covered by an object. However, if the sample point is outsidelution that avoids erratic pixels 
along intersecting or the polygon, its z-value is more or less useless for a correct HSE. Atouching surfaces. 
 complete hidden surface elimination for the pixel area is required With EXACT the HSE problem on the 
subpixel level is solved with the help of p-masks. P-masks (prior­ity masks) are subpixel masks that 
indicate for each 2 Current Status subpixel which one of two given planes is closer to the viewer. An 
algorithm to produce the p-masks in A traditional algorithm that approximately evaluates the box-.ltered 
an ef.cient way and its hardware implementation are intensity is the A-buffer Algorithm described by 
Carpenter [1]. Thepresented. The p-mask generator is used in a hardware contributions of surfaces that 
cover a pixel partially are arranged in implementation of an A-buffer algorithm in the form of a list 
that is sorted front-to-back. Two z-values are stored for each a rendering pipeline. Of course the algorithm 
can also fragment, and . When all fragments have been added  z min z max be used in software to enhance 
an existing A-buffer to the list, the intensity is calculated in a process called packing. implementation. 
 Beginning with the frontmost object the contribution is determined The paper ends with the description 
of the list process-using subpixel masks. For each fragment the exact covered pixel ing architecture 
for which the EXACT A-buffer has area is stored in addition to the subpixel mask. In certain cases the 
been built1 exact area can be used instead of the subpixel count to calculate the contribution. A subpixel 
already covered by an opaque object is excluded from further processing which results in a z-buffer-CR 
Categories and Subject Descriptors: I.3.1 [Computer Graph-like behavior on the subpixel level. The difference 
to an actual ics]: Hardware Architecture -raster display devices ; I.3.3 [Com-z-buffer on the subpixel 
level is that for each fragment only two z puter Graphics]: Picture/Image generation -display algorithms 
values are stored per pixel. Intersecting surfaces are treated with an approximation. Intersection is 
assumed if the z ranges of two Additional Key Words and Phrases: anti-aliasing, A-buffer, different objects 
overlap. It is further assumed that the two surfacespriority-masks, exact area coverage calculation. 
 are oriented as indicated in Fig. 1. Wilhelm Schickard Institut f¨ur Informatik, Graphisch Interaktive 
Systeme, Auf The visible area of the front fragment is then calculated as: der Morgenstelle 10/C9, 72076 
T¨ubingen, E-mail: andreas@gris.informatik.uni­tuebingen.de, strasser@gris.informatik.uni-tuebingen.de. 
1The experiences described here were gained in a research project partly supported by the Commission 
of the European Communities through the ESPRIT II-Project [5]. Vis front fo Zmax u Z minmax a fnextront 
u T Z o Zminmax fr u ont Zmin a next SPIRIT-workstation, Project No. 2484. . The method will fail very 
often though, because it depends on Permission to copy without fee all or part of this material is granted 
assumptions that are hardly ever ful.lled. For example the surfaces provided that the copies are not 
made or distributed for direct provided that the copies are not made or distributed for direct in Fig. 
2 are rendered exactly like the ones in Fig. 1 although one commercial advantage, the ACM copyright notice 
and the title of the commercial advantage, the ACM copyright notice and the title of the of the objects 
is not visible at all. publication and its date appear, and notice is given that copying is by publication 
and its date appear, and notice is given that copying is by It should also be mentioned that other even 
more troublesome 2 permission of the Association for Computing Machinery. permission of the Association 
for Computing Machinery. To copy problem cases exist that are very dif.cult to handle. If only one otherwise, 
or to republish, requires a fee and/or specific permission. otherwise, or to republish, requires a fee 
and/or specific permission. &#38;#169;1993 -0---8/93/008/0015 $1.50 &#38;#169;1993 ACM -0-89791 -601 
-8/93/008 $1.50 2More troublesome: Intersecting surfaces could be forbidden and don t exist in many implementations 
of rendering systems. But objects touching each other as e.g. in Fig. 3 appear in nearly every picture 
and cannot be avoided. Figure 1: Visible fraction of front fragment ([1]). Figure 2: Front fragment should 
cover the whole pixel. Figure3: Object1disappears(zvaluesampledatpixelcenterseems further away). Figure 
4: Object 2 shines through (z value sampled at pixel center seems closer). z -value is available as it 
is the case in the z-buffer things become especially dif.cult. If the center of the pixel where the -values 
zzz are sampled is outside of the object the -values are nearly useless because they don t tell anything 
about the real location of the object if the slopes in -direction are not known. Some of the very common 
problem cases are shown in Fig. 3 ­ 5. The bold dashed objects are not drawn although they should be 
visible. These problems are not taken into account with most rendering algorithms. Fig. 13 shows some 
of the resulting artifacts; the correct image is produced with the EXACT method, described in the following 
section (Fig. 14).  3 Solution If two objects (or the planes of the two objects resp.) intersect within 
a pixel a subpixel mask is generated which we call priority mask (p-mask). It indicates in which part 
of the pixel object #1 is the front object and in which part of the pixel object #2 is the front object. 
This subpixel mask is used to modify the edge subpixel masks of the two objects in the following way 
(see Fig. 6): &#38;&#38;&#38; (1)  A f AA C B new BB &#38;&#38;&#38;(2) where Figure 5: Object 1 disappears, 
but should be visible, object 2 is visible (situation similar to Fig. 3). Figure 6: Generation of the 
modi.ed edge subpixel masks BA new and from the original edge subpixel masks and using A B new the priority 
mask . Shown is the subdivided pixel area, projected C on the planes of two intersecting objects. A: 
edge subpixel mask for object # 1 B: edge subpixel mask for object # 2 C: p-mask for objects #1 and #2, 
plane #1 in front of plane #2 subpixel = 1 Two tasks remain to be solved: 1. The priority mask has to 
be calculated in an ef.cient way. 2. The decision has to be made, when two object planes intersect within 
a pixel s area.  3.1 The calculation of the priority mask Virtually any rasterization system uses a 
unit that interpolates colors and z-values by repeatedly adding increments to a starting value. The priority 
mask generator uses the increments for the z value in dz dzz xyx f f w zdz zw c nx u xnynyx z .u dz z 
nxny dz y z f wz c w dz xnyynx the x and y directions and . (The values for the two objects are marked 
with indices, e.g. 1). The z-values at the pixel centers are known ( 1 and 2). If we calculate the difference 
of the corresponding values for the two objects we get: 1 2 (3) 12(4) 12(5) These parameters describe 
a plane that indicates, where plane #1 is in front of plane #2 by the sign of its z-value. The intersection 
with the plane z=0 denotes the border between the two areas where plane #1 or plane #2 resp. is in front 
of the other plane. The representation of this plane with the above mentioned pa­rameters resembles very 
much the representation of the polygon edges in some rendering systems, e.g. in the PIXEL PLANES sys­tem 
[6]. The mechanisms that exist to generate subpixel masks representing edges can therefore be used to 
generate the priority mask. A scheme producing subpixel masks that exactly represent the covered fraction 
of the pixel is described in [8]. The generation of the priority mask can be done by software of hardware. 
Our contribution aims for a hardware solution. If a software solution is considered, several criteria 
can be used to reduce signi.cantly the number of cases where the priority mask has to be calculated: 
&#38;0 6 12and217 or a much better criterion instead of (7): z u zz min o o o j dzz nxmax u AdzB nx f 
6j T z j min dz ny o u zdz maxny j a b o oa a 21 212128 The .rst criterion (6) is obvious: if the subpixel 
masks of the two objects don t overlap, none of the objects can hide the other one. The second criterion 
is expressed by relations (7). It eliminates the trivial cases where the -ranges of the two objects don 
t overlap. The priority mask thus consists of only 1s or only 0s, resp. This criterion is not very strong 
however, because objects with overlap­ping -ranges do not necessarily have to intersect each other (see 
e.g. Fig. 2). Also the values of and might not be known, though they could easily be calculated. This 
leads us to the stronger criterion expressed in equation (8). Only if this relation is true, will an 
intersection of the two objects occur within the pixel area. Using this criterion, the case of Fig. 2 
is a trivial case with only 0s or 1s in the priority mask.  zzz min z max Figure 7: Block diagram of 
the p-mask generation on the EXACT-Chip 4 Hardware Implementation of the P-Mask Generation The block 
diagram of the p-mask generation in Fig. 7 shows, how the mask is calculated. The block labelled EXACT 
takes two ­values and the corresponding increments as input and calculates from these values the parameters 
of the intersection line. These parameters are used to lookup the .nal p-mask. The contents of the corresponding 
lookup table can simply represent the order of the planes at the subpixel locations. It should however, 
be consistent with the method used for the generation of the coverage masks. The EXACTchip, liketherenderchipinthe 
SPIRIT workstation[4] uses the EASA concept3, described in detail in [8]. The design of the EXACT block 
(Fig.8) is intended to exploit parallelism as much as possible. Three parallel subtractors calcu­late 
the -difference and the differences of the -increments. The zz absolute values of the results are calculated 
in the next stage. The resulting three values (, and ) are the parameters of the equation (9) for a straight 
line, the line of intersection between the two planes (origin of the coordinate system is the pixel center). 
  LF o xtdzy a x f z T dzx y z p dz x T y p p dz y dz y y x fT Ldz y oa 09 This equation has to be normalized 
so that the parameters can be used to look up the resulting p-mask. The normalization could be performed 
by dividing the equation by 22. However the square root can be avoided if we divide by the 1-norm instead 
of the 2-norm4. This means that we divide by the sum of the absolute values of and . The precision that 
is required so that the error introduced by the parameter calculation is smaller than one subpixel can 
be found if we apply the law of error propagation. For a 4 4 subpixel mask, only four bits are needed 
for each normalized parameter. To keep the dividers simple (Fig. 9), barrel shifters are used to properly 
scale the input parameters. 5 System aspects The EXACT-hardware is part of a new graphics system. The 
main concepts of its architecture are described in the following section. 5.1 Processing of lists the 
concept of the A-buffer A big difference between the A-buffer and a traditional z-buffer lies in the 
fact that in the A-buffer lists of contributions to each pixel are stored whereas in the z-buffer only 
one item per pixel 3The EASA (Exact Area Subpixel Algorithm) is used to determine the subpixel mask. 
In contrast to the conventional approach, we do not sample at the subpixel cen­ters. Instead, the covered 
portion of the pixel area is calculated exactly and converted into the corresponding subpixel count. 
The location of the subpixels is chosen in a way, that preserves the geometry best. For details see [8] 
4The 1normisalsoknownasManhattandistance,because,ratherthantheshortest L Figure 8: The p-mask generation 
on the EXACT-Chip (12000 Gates) Figure 9: The Dividers on the EXACT-Chip has to be stored the one currently 
closest to the viewer. Most rendering hardware today supports the z-buffer for obvious reasons: the list 
handling required by the A-buffer is much more dif.cult to implement in hardware. The question that could 
be asked at this point is: Why should we store more than one object per pixel? There are several answers 
to this question. The .rst one: Anti­aliasing. The second one: Transparency. Anti-aliasing of edges implies 
the blending of the colors of differentobjects. Therearecasesinwhichthecolorscanbeblended using a normal 
z-buffer. For example, if one object appears in front of an other big object the colors can be blended 
with the weight A o u A a A factors and 1 , being the pixel area covered by the second object. But what 
if three or more objects contribute to a pixel? A blending in the described way will lead to errors (see 
Fig. 10). The second reason, transparency handling, is obvious. There may be several transparent objects 
covering a pixel. They have to be depth-sorted before their colors can be blended using the appropriate 
transparency factors and sorting requires that more than one object is stored.  5.2 The List Processing 
Pipeline Which hardware architecture is capable of supporting an A-buffer like rendering scheme? It is 
an architecture that has been known quite a while but normally was only used as a functional replace­ment 
for the z-buffer: the pixel processing pipeline. Cohen and Demetrescu presented such a processor pipeline 
already in 1980 distance, it describes the distance between two points, one would have to walk in a city 
[2]. Systems like the Triangle Processor and Normal Vector Shader with a rectangular grid of streets. 
System [3] or PixelFlow [7] form such a pipeline and use it for Figure 10: If the colors of object 1 
and object 2 are blended (each of them contributing 50% to the .nal color), green will be part of the 
pixel color (25%). If the color of object 3 is then blended to the pixel color, green will erroneously 
still be part of the .nal pixel color. Figure 11: Pipeline of comparators performing n z-buffer opera­tions 
simultaneously without problem of buffer access bottleneck. While e.g. the third comparator works on 
pixel #1, the second comparator works on pixel #2 and the .rst comparator is already working on Pixel 
#3. what Molnar calls image composition. As multiple z-buffer opera­tions take place at the same time 
(see Fig. 11), the traditional frame buffer access bottleneck problem is solved in an elegant way. This 
might be a reason for this type of system to be more widely used in the future. Simply by adding more 
stages to the pipeline the rendering speed of the system can be increased inde.nitely. The only penalty 
is a slightly increased latency time that up to several hundred pipeline stages doesn t exceed the frame 
time. But now this pipeline architecture can not only be used as a z-buffer replacement; it is an outstanding 
architecture to perform the list processing required by the A-buffer algorithm. Schneider proposed in 
1988 the PROOF system [9] that uses a pipeline and transfers not only one object per pixel through the 
pipeline but a list of contributing objects for each pixel, similar to a proposal by Weinberg [10]5. 
The hidden surface elimination was performed in a special post-processing stage. The architecture proposed 
in this paper performs the whole list processing in list processors that contain the EXACT hardware for 
the hidden surface elimination on the subpixel level6. Fig.12showstheblockdiagramofalistprocessorpipeline. 
The polygon descriptions are distributed in a round robin fashion among the rasterizer units (RU), which 
ensures a good load distribution with minimum effort. The rasterizers interpolate the z-and color­values 
(or resp. normals or texture coordinates) and send the sorted pixel contributions down to the list processors. 
Each rasterization 5A good analysis of problems related to A-buffer architectures can be found in Molnar 
s excellent PhD thesis [11]. Accidentally this reference is only contained in the CD-ROM release of the 
proceedings. 6Other features of the list processing pipeline, like image processing capabilities (.ltering 
with arbitrary kernel) are not subject of this paper but also are arguments for using such an architecture. 
Figure 12: List Processor Pipeline Architecture unit is capable of rendering several thousand objects 
per second (about 20 MPixel/sec.) and contains a standard RISC Processor and RAM as well as an ASIC for 
the pixel generation. The list processors, realized as ASICs, contain the described hardware for the 
EXACT algorithm and perform the modi.cation of the subpixel masks coming from the RUs as well as the 
depth­sorting of the pixel contributions. Visible fragments are inserted into the lists at their appropriate 
positions which is important for transparent objects. Mutually intersecting transparent objects can be 
handled by splitting the subpixel mask of one of the objects in two parts: one in front of, the other 
behind the second object. The output of the pipeline consists of a depth sorted list of object contributions 
for each pixel, with nonoverlapping subpixel masks for opaque objects and transparent objects appearing 
in the correct sequence. As each list processor can only handle one additional object per pixel, list 
processors that receive several objects concerning one pixel .ag all but the last of these objects as 
not processed and send them in front of the already processed list to the next stage. If this stage didn 
t receive an object from its RU for this pixel the last of the not processed objects is treated by this 
stage. If any objects remain unprocessed at the end of the pipeline the concerned pixels are cycled through 
the pipeline again to handlethe unresolved objects. In order to keep the sequence of the pixels intact 
a FIFO is used to store the output of the pipeline during the recycling of the incompletely processed 
pixels. By adding several list processors without connected RUs to the end of the pipeline the probability 
for such cases can be signi.cantly reduced. The output of the pipeline can be directed to one of two 
RAM buffers. This allows the rendering of scenes with changing parts. The static parts are rendered once 
into the RAM buffer. Then the RAM serves as input for the pipeline where only the changing parts have 
to be added for each frame. The RAM buffer is also used in other applications like image processing or 
form factor calculations for a radiosity algorithm. In the post-processing stage the trans­parency calculations 
are performed and the subpixel contributions are summed up.   6 Conclusion A principle of rasterization 
is, that it produces aliasing artifacts. The quest for increased realism by developing sophisticated 
illumination models can not be successful without properly dealing with anti­aliasing. This problem can 
be partially solved by increasing the screen resolution of color monitors, but this is very costly and 
limited by physical constraints. On the other hand anti-aliasing by means of the EXACT A-buffer solves 
the problem adequately and offers a better cost/performance ratio for future display systems. Author/Title 
Index [1] CARPENTER, L. The a-buffer, an antialiased hidden surface method. Computer Graphics 18, 3 (July 
1984), 103 108. [2] COHEN, D. A vlsi approach to the cig problem. Presentation at SIGGRAPH 1980, 1980. 
[3] DEERING, M., WINNER, S., SCHEDIWY, B., DUFFY, C., AND HUNT, N. The triangle processor and normal 
vector shader: A vlsi system for high performance graphics. Computer Graph­ics 22, 4 (Aug. 1988), 21 
30. [4] DUNNETT, G. J., WHITE, M., LISTER, P. F., GRIMSDALE, R. L., AND GLEMOT, F. The image chip for 
high performance 3d rendering. IEEE Computer Graphics &#38; Applications 12 ,6 (Nov. 1992), 41 52. [5] 
FIUME, E., FOURNIER, A., AND RUDOLPH, L. A parallel scan conversion algorithm with anti-aliasing for 
a general-purpose ultracomputer. Computer Graphics 17, 3 (July 1983), 141 150. [6] FUCHS, H., POULTON, 
J., EYLES, J., GREER, T., GOLDFEATHER, J., ELLSWORTH, D., MOLNAR, S., TURK, G., TEBBS, B., AND ISRAEL, 
L. Pixel-planes 5: A heterogeneous multiprocessor graphics system using processor-enhanced memories. 
Com­puter Graphics 23, 3 (July 1989), 79 88. Figure 14: Same scene with the EXACT algorithm. [7] MOLNAR, 
S. Pixel.ow: High-speed rendering unsing image composition. Computer Graphics 26, 2 (July 1992), 231 
240. [8] SCHILLING, A. G. A new simple and ef.cient antialiasing with subpixel masks. Computer Graphics 
25, 4 (July 1991), 133 141. [9] SCHNEIDER, B.-O. A processor for an object-oriented render­ing system. 
Computer Graphics Forum 7 (1988), 301 310. [10] WEINBERG, R. Parallel processing image synthesis and 
anti­aliasing. Computer Graphics 15, 3 (Aug. 1981), 55 62. [11] MOLNAR, S. Image-Composition Architectures 
for Real-Time Image Generation. PhD thesis, University of North Carolina at Chapel Hill, 1991.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166129</article_id>
		<sort_key>93</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Graphics rendering architecture for a high performance desktop workstation]]></title>
		<page_from>93</page_from>
		<page_to>100</page_to>
		<doi_number>10.1145/166117.166129</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166129</url>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Parallel processing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.1.2</cat_node>
				<descriptor>Single-instruction-stream, multiple-data-stream processors (SIMD)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010169</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010528.10010534</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Parallel architectures->Single instruction, multiple data</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P42708</person_id>
				<author_profile_id><![CDATA[81100055072]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chandlee]]></first_name>
				<middle_name><![CDATA[B.]]></middle_name>
				<last_name><![CDATA[Harrell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P83621</person_id>
				<author_profile_id><![CDATA[81100336727]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Farhad]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fouladi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>378516</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[K. Akeley, T. Jermoluk, "High-Performance Polygon Rendering", Computer Graphics (Proc. SIG- GRAPH), Vol. 22, No. 4, August 1988, pp. 239-246.]]></ref_text>
				<ref_id>AKEL88</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617502</ref_obj_id>
				<ref_obj_pid>616006</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[K. Akeley, "The Silicon Graphics 4D/240GTX Superworkstation", IEEE Computer Graphics and Applications, Vol. 9, No. 4, July 1989, pp. 71-83.]]></ref_text>
				<ref_id>AKEL89</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378518</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[B. Apgar, B. Bersack, A. Mammen, "A display system for the Stellar Graphics Supercomputer Model GS1000", Computer Graphics (Proc. SIGGRAPH), Vol. 22, No. 4, August 1988, pp. 255-262.]]></ref_text>
				<ref_id>APGAR88</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[J. Foley, A. van Dam, S. Feiner, J. Hughes, "Computer Graphics, Principles and Practice", 2nd edition,, Addison-Wesley Publishing, 1990.]]></ref_text>
				<ref_id>FOLEY90</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA["The Indigo2 Technical Report", Silicon Graphics Computer Systems, 1993.]]></ref_text>
				<ref_id>INDIG93</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA["Iris Partner Catalogue", Silicon Graphics Computer Systems, 1992.]]></ref_text>
				<ref_id>IRIS92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97912</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[D. Kirk, D. Voorhies, "The Rendering Architecture of the DN 10000VS", Computer Graphics (Proc. SIG- GRAPH), Vol. 24, No. 4, August 1990, pp. 299-308.]]></ref_text>
				<ref_id>KIRK90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[W. Newman, R. Sproull, "Principles of Interactive Computer Graphics", McGraw-Hill Book Company, Second Edition, 1979.]]></ref_text>
				<ref_id>NEWM79</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA["The Personnal IrisTM: A Technical Report", Silicon Graphics Computer Systems, 1988.]]></ref_text>
				<ref_id>PERS88</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[M. Segal, K. Akeley, "The OpenGLTM Graphics System: A Specification (version 1.0)", Silicon Graphics Computer Systems, 30 June 1992.]]></ref_text>
				<ref_id>SEGAL92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37426</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[J. Torborg, "A Parrallel Processor Architecture for Graphics Arithmetic Operations", Computer Graphics (Proc. SIGGRAPH), Vol. 21, No. 4, July 1987, pp 197-204.]]></ref_text>
				<ref_id>TORB87</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[A. van Dam, et. al., "PHIGS+ Functional Description Rev. 2", Jointly developed PHIGS+ specification, 1987.]]></ref_text>
				<ref_id>VAND87</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Graphics Rendering Architecture for a High Performance Desktop Workstation Chandlee B. Harrell Farhad 
Fouladi Silicon Graphics Computer Systems 2011 North Shoreline Blvd. Mountain View, CA 94039-7311 Abstract 
Hundreds of commercial applications used in mainstream design activities have demonstrated proven demand 
for 3D graphics ren­dering products. The demand is for faster and more powerful ren­derers, thus creating 
the system design problem of how to achieve maximum rendering performance from the technology available 
to implement the system. This paper describes a graphics rendering ar­chitecture that takes advantage 
of several novel architectural fea­tures: a custom floating point processing core with tailored data 
stores and bussing structures, the arrangement of these cores into a SIMD processor for low overhead 
multiprocessing, and the hyper­pipelining of the fixed point scan conversion units for low over­head, 
high bandwidth pixel generation into an interleaved frame buffer. These features combine to form a solution 
to the system de­sign problem which distinguishes itself by its overall performance and its ability to 
maximize performance while minimizing system size. The resulting architecture is capable of over a half 
million gouraud shaded Z-buffered triangles per second, with a sustained fill rate for gouraud shaded 
and Z-buffered pixels of 80M pixels per second. The architecture fits in a desktop workstation.  Introduction 
A graphics rendering architecture for a high performance desktop workstation is described. 3D graphics 
workstations are used by a broad range of applications [IRIS92]. Many of the applications fall into the 
categories tradition­ally called computer-aided design (CAD), where the designer makes progressive refinements 
on the shape and dimensioning of a product based on feedback from visual modeling, and computer­aided 
engineering (CAE), where the designer also wishes to analyze properties of the design such as thermal 
and stress gradients or structural strength, in addition to shape and appearance. 3D graph­ics workstations 
are used in the following applications, among oth­ers: car and airplane design, tool design, packaging 
design, indus­trial and product design, furniture design, clothing and shoe design, architectural and 
civil engineering, production floor and plant de­sign, geothermal and atmospheric analysis, molecular 
modeling, pharmaceutical design, chemical analysis, and film animation and special effects. Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct provided that the copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the commercial advantage, the ACM copyright notice and the 
title of the publication and its date appear, and notice is given that copying is by publication and 
its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. otherwise, or to republish, requires a fee and/or specific permission. 
&#38;#169;1993 ----8/93/008/0015 $1.50 &#38;#169;1993 ACM -0 -89791 -601 -8/93/008 $1.50 Application 
packages today running on 3D workstations enable de­sign efforts that are compute intensive, limited 
only by today s ren­derers. The complexity of models that renderers can effectively handle is far less 
than the model complexity with which users are attempting to work. This creates tremendous demand for 
faster and more powerful graphics rendering systems. How to achieve the highest performance rendering 
system from the technology avail­able is the system design problem that this demand presents to the system 
designer. Further clarification of the graphics rendering system design prob­lem is necessary. Most graphics 
renderers today perform rapid, ac­celerated rendering of 3-sided polygons and straight line segments. 
The renderer receives these basic graphics primitives, each primi­tive with vertex descriptions defined 
by the application, and per­forms the calculations to render the primitive as pixel values into the frame 
buffer [FOLEY90,SEGAL92,VAND87]. The basic graphics primitives allow close approximation to any arbitrary 
curve or surface by sub-dividing the curve into line segments or the surface into polygons to the point 
where the rendered image is vi­sually acceptable to the user. For the system designer, the primitives 
provide a simple and limited set of processing algorithms that must be accelerated, enabling the focus 
to achieve high performance sys­tems. A top level flow diagram is presented in Figure 1 illustrating 
the process for rendering the basic graphics primitives. The graphics renderer receives polygons or lines 
from the application process and performs the steps shown in the flow diagram to render each polygon 
or line as color and Z pixel values into the frame buffer. Details of each processing step are carefully 
discussed in [FO-LEY90] and [NEWM79]. Implementation bottlenecks in a graphics rendering system typical­ly 
appear: 1) in the floating point compute power available for the world coordinate to screen coordinate 
transformations and for ver­tex color computations; 2) in the floating or fixed point compute power available 
for triangle slope and line slope calculations; 3) in the rate of generation of pixel values from the 
fixed point iterators; 4) and in the achieved pixel bandwidth into the frame buffer. Commercial architectures 
have approached these bottlenecks in a variety of ways. [KIRK90] presents an architecture where the per 
vertex and slope calculations are performed on the host CPU and multiple iteration engines drive an interleaved 
frame buffer. [AP-GAR88] also executes the per vertex calculations on the host, but off-loads most of 
the slope calculations to a fixed point engine, and uses a unique combination of multiple iteration units 
to drive pixel results into an interleaved system memory. [AKEL88,89] describe an approach utilizing 
a serial pipeline of floating point processors for the per vertex calculations, fixed point engines for 
the slope cal­culations, and multiple iteration units to drive an interleaved frame buffer. The architecture 
introduced in [TORB87] also uses multiple floating point processors but arranges them into a MIMD parallel 
processor, uses a fixed point slope engine, and multiple iterators to drive an interleaved frame buffer. 
[PERS88] uses a single floating point processor to perform both per vertex and slope calculations, and 
a single iterator to drive an interleaved frame buffer. Note that the interleaved frame buffer is the 
only feature common to all the approaches, and that most approaches use multiple iteration units. The 
goal of the architecture described here is to provide a powerful graphics rendering system, maximizing 
performance while mini­mizing size. The architecture utilizes several novel approaches to overcoming 
rendering bottlenecks. Floating point performance is accomplished through the custom design of a highly 
efficient float­ing point processing core, and by employing multiple cores con­trolled in a low overhead 
SIMD parallel processor. The floating point core is tailored to accommodate both the per vertex calcula­tions 
and the triangle and line slope calculations. Fixed point itera­tion performance is achieved through 
hyper-pipelining two identi­cal iteration units, allowing each unit to sustain the pixel generation requirements 
of multiple pixel memory busses. Each iteration unit is pipelined until technology limits of integration 
are encountered. The multiple memory busses provide the necessary bandwidth into the frame buffer memory. 
These features result in a graphics rendering system solution distin­guished by overall performance, 
and by compactness of size. The architecture is implemented in a desktop workstation [INDIG93]. It is 
capable of over 1.3 million depth-cued lines per second, over half a million gouraud shaded Z-buffered 
polygons per second, with a sustained fill rate of 80M gouraud shaded Z-buffered pixels per sec­ond. 
 APPLICATION EXECUTING ON CPU PER VERTEX CALCULATIONS, FLOATING POINT FOR RANGE &#38; PRECISION GRAPHICS 
RENDERING SYSTEM SLOPE CALCULATIONS, FLOATING OR FIXED POINT SIMPLE FIXED POINT INTERATION FRAME BUFFER 
PIXEL BANDWIDTH Figure 1. Process for rendering basic graphics primitives TOP LEVEL SYSTEM VIEW This 
section presents a block diagram of the architecture in Figure 2. The key components are briefly introduced, 
followed by a de­scription of the overall control structure and the data flow through the system. The 
subsequent sections discuss each of these key com­ponents in detail, describing the critical decisions 
made to deter­mine their structure, then detailing the internal operation of each component. The final 
section discusses the technology targeted for the architectural implementation and the implementation 
results. The block diagram is shown in Figure 2. The key components are the FIFO interface to the system 
bus, the Command Processor (CP), the SIMD parallel processor, the dual Raster Engines (RE), and the frame 
buffer. The SIMD processor is made up of a sequencer, a mi­crocode store, and multiple Geometry Engines 
(GE). Each GE is a custom floating point processing core. Each Raster Engine is a hy­per-pipelined iteration 
unit. The SIMD parallel processor executes all the per vertex calcula­tions and the slope calculations 
shown in Figure 1, the REs perform the fixed point iteration, and the frame buffer pixel bandwidth is 
de­termined by the multiple busses into the frame buffer. Operation is initiated by the CPU sending polygon 
and line render­ing commands into the FIFO across the system bus. The FIFO al­lows the CPU to generate 
commands at a rate independent of how fast the rendering occurs. If the FIFO fills up, an interrupt is 
gener­ated to the CPU for exception handling. The SIMD parallel processor is fed data from the FIFO by 
the Com­mand Parser. The CP moves data from the FIFO into the ping-pong input buffers of the Geometry 
Engines. The GEs read data from the ping-pong buffers, perform necessary floating point computations, 
and write results to their respective output FIFOs. GE execution is controlled by the common sequencer 
and control store. A bus controller resident in the even Raster Engine reads data from the GE output 
FIFOs and transfers the data into the RE input ping­pong buffers. The REs perform necessary iterations 
to generate col­or and Z values and perform the correct pixel updates into the frame buffer. The odd 
RE generates pixels for the odd numbered scan lines of the frame buffer, and the even RE generates pixels 
for the even numbered scan lines. The sections below first discuss the GE custom floating point core 
solution, followed by a discussion of the control structures required to arrange the GEs into the SIMD 
parallel processor. This is fol­lowed by a description of the hyper-pipelined RE iteration solution. 
 GEOMETRY ENGINE The goal for the Geometry Engine design is to achieve the maxi­mum realized floating 
point performance for graphics algorithms, in a single chip solution. The algorithms used for evaluating 
perfor­mance are the per vertex and slope calculations of Figure 1. The de­cision is made to combine 
the per vertex and slope calculations into a single floating point solution. Slope calculations are comprised 
of relatively complex algorithms, difficult to implement in a hard­wired fashion, and therefore most 
effectively implemented in a mi­crocoded processor. Also, the compute cycles required for per ver­tex 
calculations is almost evenly balanced with the cycles required for slope calculations. Combining the 
per vertex and slope calcula­tions into the GE relieves the need to design a second microcoded fixed 
point processor of similar complexity; and the replication of GEs in the SIMD parallel processor increases 
both the per vertex and slope processing power together. The GE design goal is met with a custom floating 
point processing core. Analysis shows that a custom unit with tailored data stores, CPU SYSTEM SIMD Parallel 
Processor  Figure 2. Block Diagram of the architecture bussing structures, and sequencing control achieves 
higher realized performance and a more compact solution than available commer­cial alternatives. Therefore 
a custom approach is chosen. Analysis of the per vertex calculations and the slope calculations shows 
an even balance between multiplies and adds, therefore one multiplier and one adder are chosen for the 
GE core. The GE design approach follows the fundamental principle of maximizing the uti­lization of the 
most expensive resource: the floating point multiplier (FMPY) and the floating point adder (FALU). The 
following obser­vations for maximizing utilization are taken into account in the GE design: high data 
bandwidth to the correct operands is needed into the FMPY and FALU; multiple threads of the same algorithm 
must be active simultaneously. Enough bandwidth to appropriate data storage and data sources is needed 
to avoid lost cycles waiting on an operand that is slow to retrieve. A single thread of execution may 
have several additions followed by several multiply operations, thus wasting the FMPY or the FALU until 
a result is available from the other unit. Multiple threads of execution is the solution. The Geometry 
Engine block diagram is shown in Figure 3. Six dif­ferent busses and four ports from the register file 
drive the four in­puts to the FMPY and the FALU. Two of the busses provide imme­diate wrap-around of 
FMPY and FALU results back to their inputs. One bus gives access to the ping-pong buffer loaded by the 
Com­mand Parser, while two more busses give access to a pair of special data stores. The sixth bus accesses 
off-chip memory that is used for expansion, and typically holds the global variables for the GE. A multi-port 
register file is included for scratch storage of interme­diate results. The register file is critical 
to allowing multiple simul­taneous threads of calculation. Feedback paths from FMPY and FALU result outputs 
are provided for single-threaded operation, but when two threads conflict by needing the same unit for 
their next computation, then one thread must be stalled by storing the interme­diate result in the register 
file until the appropriate unit becomes free. On the other hand, a multi-port register file is an expensive 
com­modity and its size is limited. Reviewing the per vertex calculations concludes that the ping-pong 
buffer and the register file are suffi­cient to perform the per vertex calculations with maximum FMPY 
and FALU utilization. On reviewing the slope calculations, howev­er, it is noted that frequently data 
from each vertex of a triangle, or both vertices of a line, are needed simultaneously during multi- S 
Per E Vertex Q and U E Slope N Calculations C E R Iterators Frame Buffer Pixel Bandwith threaded computation. 
The register file cannot be made big enough to hold the data structures for each vertex. The GE is designed 
to have three separate data stores, one for each vertex of a triangle or for the two vertices of a line, 
used during the slope calculation pro­cess. The ping-pong buffer is used to hold the data structure for 
one vertex, while the two special data stores hold the data structures for up to two more vertices. This 
extensive memory and bussing structure is wasted without flexible independent addressing and flexible 
control of data move­ment. This is accomplished through a very wide instruction word which allows control 
of the breadth of resources. The result of the described structure is that simultaneous access can be 
made to the ping-pong buffer, the two special data stores, the global variables memory, the result outputs, 
and the register file by any of the four FMPY and FALU inputs. Multiple threads of exe­cution supported 
by this accessible bandwidth into the FMPY and FALU inputs maximizes FMPY and FALU utilization. GE-DATA 
GE-RE BUS Figure 3. Geometry Engine block diagram GE operation occurs as follows. The Command Parser 
loads data into the ping-pong buffer. The ping-pong buffer allows CP loading of data into one side of 
the buffer while the GE is executing and ac­cessing the other side of the buffer. The CP initiates GE 
execution by informing the GE sequencer that data is fully loaded. The se­quencer looks up instructions 
in the GE microcode store, and these instructions control the execution functions of the GE. For lines 
and triangles, the GE performs per vertex calculations, accessing data from the ping-pong buffer, then 
constructs vertex data structures based on screen space coordinates and puts one vertex data struc­ture 
back in the ping-pong buffer and up to two more vertex data structures into each of the special data 
stores. Slope calculations are then performed, drawing operands from the ping-pong buffer and the two 
special data stores. Calculated iteration coefficients and ini­tial values are passed to the Raster Engines 
by storing them to the output FIFO.  SIMD PARALLEL PROCESSOR A single floating point processor cannot 
achieve the desired perfor­mance. Therefore multiple floating point processors are used in the design. 
The following goals for multiprocessing led to the SIMD parallel processor solution: 1) a linear performance 
increase must be achieved with the addition of Geometry Engines; 2) the multi­processing solution must 
have the lowest possible impact over and above a uniprocessor solution. Three approaches are considered 
for the multiprocessing solution. The first is a pipeline of floating point processors [AKEL88, 89]. 
Each pipeline stage performs a subset of the per vertex and slope computations, passing intermediate 
results to the next processor in the pipeline. Each pipeline processor is executing a different set of 
code to implement its separate subset of the algorithm. This ap­proach has several disadvantages. The 
throughput of a pipeline is the speed of the slowest processing step. Overall performance is de­termined 
by the processor with the biggest subset of the algorithm to process. Since the algorithm cannot be divided 
into perfectly equal subsets, a less-than-linear performance gain is achieved. Also note, that to add 
processors, a new subdivision of the algorithm must take place and new code must be written and tuned. 
The final disadvantage of this approach is in the burden of overhead the ap­proach requires. Although 
having the advantage of not requiring the distribution mechanism at the head of the pipe needed by the 
next two approaches considered, each processor does require its own se­quencer, microcode store, globals 
data store, in addition to control logic to interface each of the pipeline stages. The second approach 
considered is a parallel MIMD (Multiple In­struction Multiple Data) array of processors [TORB87]. Each 
pro­cessor performs independent execution of the per vertex and slope calculations for its own polygon 
or line primitive. Linear perfor­mance gains are attained when the same kind of primitive is distrib­uted 
to each processor, thus satisfying the first multiprocessing goal. Processors may be added without requiring 
changes to proces­sor code. The disadvantage of the MIMD parallel processor lies in the overhead required 
to implement such an approach. A parallel processor requires a distribution function that takes primitives 
in the FIFO (received from the CPU) and disburses a primitive to each of the processors present. A MIMD 
parallel processor also requires that each processor has its own sequencer, microcode store, and globals 
data store. The third approach considered is a parallel SIMD (Single Instruc­tion Multiple Data) array 
of processors. Each processor executes the same instruction in lockstep, but is computing results for 
its own polygon or line primitive. Like the MIMD processor already exam­ined, the SIMD parallel processor 
achieves linear performance gains with the addition of processors when the same kind of primi­tive is 
distributed to each processor. The advantage of the SIMD ap­proach is in the low overhead required to 
implement a multiproces­sor. All processors share the same sequencer, the same microcode store, and the 
same globals data memory. The only implementation overhead required over a uniprocessing solution is 
the addition of the distribution function. It is worth noting that this is a simple func­tion and therefore 
a small overhead to tolerate. The SIMD parallel processor is chosen as it optimally achieves the multiprocessing 
goals. Note that a key assumption to accomplishing linear performance gain from a parallel processor 
(SIMD or MIMD) is that the same kind of primitive is distributed to each of the processors (all lines 
or all polygons). This requires that the primitives coming through the FIFO from the CPU arrive in significant 
groupings of lines together and polygons together, rather than a fully random distribution of lines and 
polygons. For a MIMD processor, if the FIFO holds alter­nating lines and polygons, the throughput slows 
down to the rate of the slower primitive - the polygon. For a SIMD processor, alternat­ing lines and 
polygons is a worst case scenario. Performance will reduce to that of a uniprocessor. Extensive analysis 
of model data sets used on 3D workstations shows polygons typically clump in large bunches and lines 
do the same. This is particularly true of CAD/CAE applications. The result is linear performance gain 
for parallel processor arrangements. The unique system features required for SIMD parallel processing 
will now be discussed. Please refer to Figure 2. The features includ­ed for SIMD processing are the distribution 
function performed by the CP, sequencing functions to allow SIMD branching, common bus for the microinstruction, 
common bus for the globals data store, and indirect addressing requirements into GE memories. The GE 
input ping-pong buffer and output FIFO are also crucial to perfor­mance. COMMAND PARSER To describe the 
operation of the Command Parser, we must first ex­plain the needs of the distribution function. The purpose 
of the CP is to analyze the command and data stream coming through the FIFO, distribute data accordingly 
to the GEs, and subsequently ini­tiate GE execution. To perform this function, the CP must detect boundaries 
between primitives, detect whether subsequent primi­tives are of the same or different kind, and maintain 
the correct or­der of primitive disbursement to the GEs. Please refer to Figure 4 for a diagram of the 
Command Parser. The CP is microcoded for flexibility. This allows different routines for primitives comprised 
of vertices with different kinds of attributes, and the exception handling of polygons with greater than 
three sides. CP operation begins with the arrival of a command token in the FIFO. The command token causes 
the CP sequencer to branch to a routine appropriate for the kind of primitive arriving in the FIFO. This 
branch mechanism inherently defines primitive boundaries. The command token is read from the FIFO and 
stored in the Current Command register. A compare function allows branching based on whether the current 
command token just arrived is identical or dif­ferent from the last command token received. If the token 
is identi­cal, then the arriving primitive can be distributed to the next GE in the parallel processor. 
If the token is different, then the GEs that have already been loaded with data must swap their input 
ping-pong buffer and begin executing before the arriving primitive can be dis­tributed to the next GE. 
The token compare mechanism allows the CP to branch to different routines to handle these two cases. 
The CP must determine to which GE the arriving primitive should be written. A round robin scheme of distribution 
is chosen, SYSTEM BUS FIFO GE COUNTER GE POINTER VERTEX COUNTER Figure 4. Command Parser block diagram 
primitives being loaded in a continuous sequence from GE #0 through to GE #7, and back around. Referring 
back to Figure 2, primitive coefficients calculated by the GEs are pulled from the GE output FIFOs in 
the same round robin order. A pointer to the GE that is currently being loaded, and a counter which maintains 
the number of GEs that have been loaded since the last execute com­mand provide the tools to determine 
for which GE the arriving primitive is destined. The incrementing and clearing of these counters is under 
microcode control. After choosing the appropriate GE, the CP pulls vertex data from the FIFO and writes 
it across the CP-GE Bus and into the GE s ping-pong buffer. Once all 8 GEs have been loaded, or when 
the current primitive is different from the previous primitive, the CP must initiate GE exe­cution. The 
CP first tells the GE sequencer which GEs are loaded, passes the GE sequencer the appropriate address 
to begin execution, and then issues the GE sequencer an execute command. An inter­lock mechanism will 
stall the CP if the GE is currently executing at the time of the CP execute command, and will initiate 
GE execution only when the previous execution is complete. Once the interlock mechanism clears, it is 
an indication that the GE ping-pong buffers have been swapped, and the CP resumes distribution of primitives 
from the FIFO. GE SEQUENCER The GE sequencer is shown in Figure 2. The sequencer is based on a standard 
uniprocessor design. Flexible branch functions are sup­ported for jumps and subroutine calls. Branching 
is controlled with­in separate fields of the GE s wide instruction word. This allows concurrent branching 
with the GE datapath control, thus not affect­ing datapath performance thru branches. To this uniprocessor 
design base are added functions which allow control of multiple SIMD processors. The GE sequencer has 
control to stall each of the GEs independently. This control is used in two different ways. The first 
is on receipt of an execute command from the CP once the GEs are idle. The GE sequencer will decode which 
GEs the CP has loaded from information passed by the CP. Those GEs not loaded will be stalled by the 
GE sequencer for the duration of the primitive execution. The second fashion the stall control is used 
is for implementing conditional subroutine calls across SIMD processors. If a subset of the processors 
does not pass the condition, that subset is stalled by the GE sequencer for the duration of the subroutine 
call, while the remaining processors execute the subrou­tine. As an example, conditional subroutine calls 
are used for im­plementing the lighting and clipping branches shown in Figure 1. MICROCODE STORE AND 
GLOBALS MEMORY STORE The GE sequencer accesses the next microinstruction from the GE microcode store 
(Figure 2). The microinstruction word controls all the GE internal functions, as well as the GE sequencer. 
The piece of the microinstruction word controlling the GEs is bussed to all the GEs for simultaneous 
execution. Additional memory (not depicted) can be added external to the GEs as an expansion memory to 
store global variables required in exe­cution. The GE Data Bus (Figure 3) of each GE is bussed together 
and connected to a globals memory store. INDIRECT ADDRESSING As explained in the section above on the 
Geometry Engine (Figure 3), data is read from the ping-pong buffer and the two special data stores to 
perform the slope calculations for a line or triangle. De­pending upon orientation of the primitive on 
the screen, these data stores may need to be accessed differently by different processors. In order to 
do this effectively in a SIMD processing environment, indirect addressing is provided into these data 
stores. This minimiz­es cycles spent out of SIMD lockstep execution and is crucial to SIMD performance. 
 INPUT PING-PONG BUFFER AND OUTPUT FIFO The GE input ping-pong buffer and the GE output FIFO are also 
crucial to SIMD performance. Without a ping-pong buffer at the in­put to the GE, the CP would have to 
load 8 GEs after GE execution of the previous primitive completes, eliminating significant paral­lelism. 
The FIFO at the GE output allows all GEs to write their re­sults in lockstep execution. Without the FIFO, 
a SIMD implemen­tation would not be feasible.  RASTER ENGINE The goal for the Raster Engine is to obtain 
the fastest gouraud shad­ed Z-buffered fill rate in a single chip. It is also desired to be able to use 
multiple copies of the same chip to obtain further increases in rendering performance. There are two 
major bottlenecks in rasterization: pixel generation, and memory bandwidth. Pixel generation, the first 
bottleneck, can be increased in two different ways. Contemporary architectures have traditionally increased 
the rate of pixel generation by replicat­ing in parallel the number of fixed point iterators, utilizing 
enough iterators to achieve the desired pixel rate. Hyper-pipelining a single iteration unit is the approach 
taken in this architecture. Hyper-pipe­lining adds pipeline stages to a single iterator until the desired 
rate of pixel generation is achieved. The pipeline stages added to the it­erator require significantly 
fewer gates than would be required to replicate iterators. Therefore, hyper-pipelining is chosen as the 
min­imum solution for performance. Memory bandwidth, the second bottleneck, is increased by using an 
interleaved frame buffer across multiple memory banks. Determining the total number of pipeline stages 
and the number of memory busses for the RE is a recursive process, and depends on the integration limits 
of technology. To achieve the maximum fill rates, the iteration pipeline must support a pixel generation 
rate of  10 cycles R G BA Z XSY COUNT Span Processor 3 cycles A RED GREEN BLUE X Y Z Blend Blend 
Blend 13 cycles multiplier multiplier multiplier 3 cycles Color buffer banks 0-4 control data buses Figure 
5. Raster Engine block diagram N times the page mode bandwidth of a frame buffer DRAM, where N is the 
number of memory busses used. A sample pipeline depth is analyzed and the die size computed. The conclusion 
of this recur­sive process led to the resultant architecture with a single hyper­pipelined Raster Engine 
driving a five-way interleaved color buffer. Given a five-way interleave on the color buffer, the pipeline 
clock rate is set at five times the DRAM page mode bandwidth, under the assumption a pixel is generated 
every clock. The slowest element of the RE pipeline is the key to ensuring the clock rate can be met, 
and is what was checked during the recursive analysis. This element is the DDA unit of the iterators. 
A DDA unit consists of a two input adder with a 2:1 multiplexer on one of its inputs. The output of the 
adder is fed into a register which is then fed back to the second input of the adder. The resultant clock 
rate for a five-way interleave color buffer drives the number of pipeline stages in the Raster Engine. 
The hyper-pipelined Raster Engine has 26 pipeline stages from the input ping-pong registers which hold 
the line and triangle iteration parameters to the point where pixels are written into the color buff­er. 
For the system architecture implemented, it is decided to incorpo­rate two raster engines to obtain the 
desired performance on the desktop. The RE implementation is now discussed in detail. A diagram of the 
Raster Engine is shown in Figure 5. The RE is capable of drawing rectangle, triangle and line primitives. 
Each primitive requires a set of iteration coefficients which are downloaded from the GE FIFOs into the 
RE ping-pong buffers. Once the ping-pong buffers are load­ed, the RE initiates rendering of the primitive. 
The execution units of the Raster Engine consist of four major sec­tions: > edge processor;  > span 
processor;  > per-pixel operators;  > memory controllers.  The edge processor combines with the span 
processor to perform the task of converting a primitive into pixels. The edge processor decomposes triangles 
into horizontal spans, and decomposes lines into pixels. It has two iterators for computing the beginning 
and end X location of the span, and six iterators to computer R,G,B,A,Z,Y  control Z buffer banks 0-9 
Z buffer banks 0-9 address buses data buses Color buffer banks 0-4 address buses for the first pixel 
on the span. Next some terms must be defined. The major edge of a triangle connects the vertex with maximum 
Y co­ordinate value to the vertex with minimum Y. The edge connecting the vertex with maximum Y to the 
vertex with the middle Y value is called the first minor edge. The edge that runs between the vertex 
with the middle Y and the vertex with the minimum Y value is termed the second minor edge. The edge processor 
begins by iterat­ing down the major edge and the first minor edge. When the proces­sor detects the middle 
Y has been crossed, it swaps the first minor edge with the second minor edge and continues down the triangle 
until the minimum Y coordinate is reached. For each span, the edge processor computes the initial R,G,B,A,X,Y,Z 
values for the first pixel on the span as well as the number of pixels that have to be ren­dered for 
that span. This information is passed to the span proces­sor. When drawing lines, only one of the two 
edge iterators is used to generate the X coordinate. The edge processor has 10 pipe stages and can generate 
a new span every other clock. The span processor has 6 iterators. These iterators walk through the pixels 
on a span and generate the R,G,B,A,X,Z parameters for each pixel on the span. The processor can generate 
one or four pixels per clock. When gouraud shading and/or Z-buffering, the span proces­sor will generate 
one pixel per clock in the X direction. When a span is flat shaded and not Z-buffered, the span processor 
generates 4 pixels per clock. The block write feature of the VRAMs used in the color buffer is utilized 
to write all 4 pixels generated in one memory cycle, thus quadrupling the fill performance for screen 
clears and for rendering flat shaded 2D surfaces. For lines, parameters from the edge processor get passed 
through. The span processor has a pipeline latency of 3 clocks. The Raster Engine supports a rich set 
of pixel operators required by commonly used graphics libraries [SEGAL92, VAND87]. Pixels operators fall 
into two categories. The first category of operators modify the color of the pixel, such as logicop and 
blend. Blend and logicop are operations performed between the generated source col­or and the destination 
color that is already stored in the color buffer. They require readback from the color buffer which is 
described be­low. There are three sets of multipliers to perform the blend func­tion for the R,G,B components. 
These multipliers are followed by an ALU which performs the logic operations. These two sections together 
contain 10 pipeline stages. The second category of pixel operators perform tests on pixel pa­rameters 
to allow conditional updating of color pixel values. Exam­ples in this category are the Z-compare test 
and stencil test. The Z­compare test is used to determine pixel visibility in the third dimen­sion. The 
stencil test is used to provide more general conditional test operations. The Z-comparison is done in 
parallel with blend and logicop in the same number of pipeline stages. There are memory controllers for 
two separate memory ports on the Raster Engine: the color buffer port and the Z-buffer port. The color 
buffer is a five-way interleaved memory port, and the Z-buffer is a 10-way interleaved memory port. The 
Z-buffer operation consists of reading back the old Z value stored in the Z-buffer, comparing that Z 
value with the newly generated Z value and, if the compari­son passes indicating the new pixel is visible, 
the new Z value and color value are written into the Z-buffer and color buffer respective­ly. Since the 
Z-buffer requires two accesses (a read and a write) for every write access to the color buffer, the Z-buffer 
port is designed with twice the interleaving of the color buffer to accommodate Z­buffered fill at the 
color gouraud shaded update rates. As we noted above, a write access to the color buffer takes 5 clocks. 
Similarly, the pipelined read-modify-write access to the Z-buffer takes 10 clocks. Adjacent pixels along 
a span are allocated to adja­cent banks of the Z-buffer interleave. Since it takes 10 clocks to per­form 
a read-modify-write, and we have a 10-way interleave, bank contention does not occur along a span and 
a one pixel per clock comparison rate is achieved. The 10 banks of the Z-buffer interleave share the 
same page address to reduce memory controller complexity. There is a single block of logic for page fault 
detection. Each bank can access a different col­umn address within the page. A score boarding technique 
is used to keep track of the state of each bank. When a pixel is dispatched to a bank, a bit in the score 
board is set to specify that the bank is busy. Thus, any pixel accesses to the same bank will be blocked 
and a bank contention stall generated to stop pixel flow until the bank is again idle. The color buffer 
has a five-way interleave. As explained above, the pipeline depth is chosen such that five pixels are 
generated in a sin­gle VRAM page mode cycle time, allowing contentionless color fills along a span. Read-modify-write 
operations to the color frame buffer (for blend and logicop) are supported at half the fill perfor­mance 
of straight color write operations. Values in the color buffer are first read into a FIFO in the RE to 
await the modify step of the operation. When the FIFO fills, the contents of the FIFO are then merged 
with the newly generated incoming pixel stream and the re­sult is written back into the color buffer. 
This two-pass operation is continued until rendering is complete. The color buffer memory controller 
has a 3 clock latency. The operation of two REs together will be briefly discussed. The two Raster Engines 
work on the same primitive together. The ren­dering task is split based on span number. All even spans 
of a prim­itive (when the Y coordinate is even) are rendered by the even Raster Engine; all odd spans 
are rendered by the odd Raster En­gine. This results in a doubling of fill performance. The edge pro­cessor 
in each RE iterates through all spans, but each RE rejects the spans that do not belong to it, and the 
edge processor continues it­eration to the next span.  TECHNOLOGY This section briefly discusses the 
technology used in the implemen­tation. The technology targeted for the custom logic design is a 1.0 
micron double metal CMOS gate array and standard cell process. The process can achieve the equivalent 
of 100K gates on a single die. The 1M-bit DRAM family is the targeted memory technology. The design consists 
primarily of custom parts and memory compo­nents. The design contains over a million gates of custom 
logic, and is implemented across three 5 x 13 PC boards.  CONCLUSION A graphics rendering architecture 
has been described which is dis­tinguished by its overall performance, and by its ability to maxi­mize 
performance while minimizing system size. The architecture is shipping as a product in the IRIS Indigo 
Extreme. A scaled ver­ sion of the architecture was introduced in IRIS Indigo2 Elan. The architecture 
provides state-of-the-art rendering performance in a desktop 3D workstation. ACKNOWLEDGEMENTS Sincere 
thanks to Marc Hannah and Dave Galbi for their major ef­forts on the architecture. Thanks to Vimal Parikh 
for his advice on the floating point solution. Finally, overwhelming appreciation must go to the whole 
design team, every one of whom made signif­icant contributions, and who made it possible.  REFERENCES 
[AKEL88] K. Akeley, T. Jermoluk, High-Performance Polygon Rendering , Computer Graphics (Proc. SIG-GRAPH), 
Vol. 22, No. 4, August 1988, pp. 239-246. [[AKEL89] K. Akeley, The Silicon Graphics 4D/240GTX Su­perworkstation 
, IEEE Computer Graphics and Ap­plications, Vol. 9, No. 4, July 1989, pp. 71-83. [APGAR88] B. Apgar, 
B. Bersack, A. Mammen, A display sys­tem for the Stellar Graphics Supercomputer Model GS1000 , Computer 
Graphics (Proc. SIGGRAPH), Vol. 22, No. 4, August 1988, pp. 255-262. [FOLEY90] J. Foley, A. van Dam, 
S. Feiner, J. Hughes, Comput­er Graphics, Principles and Practice , 2nd edition,, Addison-Wesley Publishing, 
1990. [INDIG93] The Indigo2 Technical Report , Silicon Graphics Computer Systems, 1993. [IRIS92] Iris 
Partner Catalogue , Silicon Graphics Computer Systems, 1992. [KIRK90] D. Kirk, D. Voorhies, The Rendering 
Architecture of the DN10000VS , Computer Graphics (Proc. SIG-GRAPH), Vol. 24, No. 4, August 1990, pp. 
299-308. [NEWM79] W. Newman, R. Sproull, Principles of Interactive Computer Graphics , McGraw-Hill Book 
Company, Second Edition, 1979. [PERS88] The Personnal Iris : A Technical Report , Silicon Graphics Computer 
Systems, 1988. [SEGAL92] M. Segal, K. Akeley, The OpenGL Graphics Sys­tem: A Specification (version 1.0) 
, Silicon Graphics Computer Systems, 30 June 1992. [TORB87] J. Torborg, A Parrallel Processor Architecture 
for Graphics Arithmetic Operations , Computer Graph­ics (Proc. SIGGRAPH), Vol. 21, No. 4, July 1987, 
pp 197-204. [VAND87] A. van Dam, et. al., PHIGS+ Functional Description Rev. 2 , Jointly developed PHIGS+ 
specification, 1987.  Figure 6. Demonstration of curve and surface approximation using graphics primitives. 
Note effect of increasing tessellation depth on image quality, and on the number of primitives to ren­der. 
Figure 7. Shaded-lighted image (2 directional lights) (Data Courtesy of Cisigraph Corporation) has 31774 
triangles, 827961 pixels and was rendered in 0.13 seconds.  Figure 8. Shaded-lighted image (2 directional 
lights) (data Figure 9. Indigo2 Extreme graphics render board set.. courtesy of Cisigraph Corporation) 
has 77420 triangles, 526235 pixels, and was rendered in 0.29 seconds.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166130</article_id>
		<sort_key>101</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Leo]]></title>
		<subtitle><![CDATA[a system for cost effective 3D shaded graphics]]></subtitle>
		<page_from>101</page_from>
		<page_to>108</page_to>
		<doi_number>10.1145/166117.166130</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166130</url>
		<keywords>
			<kw><![CDATA[3D graphics hardware]]></kw>
			<kw><![CDATA[antialiased lines]]></kw>
			<kw><![CDATA[floating-point microprocessors]]></kw>
			<kw><![CDATA[gouraud shading]]></kw>
			<kw><![CDATA[parallel graphics algorithms]]></kw>
			<kw><![CDATA[rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Antialiasing**</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010386</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Antialiasing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010528.10010531</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Parallel architectures->Multiple instruction, multiple data</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P196763</person_id>
				<author_profile_id><![CDATA[81100240083]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Deering]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14145112</person_id>
				<author_profile_id><![CDATA[81332518220]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Nelson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Abi-Ezzi, Salim, and L. Shirman. Tessellation of Curved Surfaces under Highly Varying Transformations. Proc. Eurographics '91 (Vienna, Austria, September 1991), 385-397.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378516</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Akeley, Kurt and T. Jermoluk. High-Performance Polygon Rendering, Proceedings of SIGGRAPH '88 (Atlanta, GA, Aug 1-5, 1988). In Computer Graphics 22, 4 (July 1988), 239-246.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Anido, M., D. Allerton and E. Zaluska. MIGS - A Multiprocessor Image Generation System using RISC-like Microprocessors. Proceedings of CGI '89 (Leeds, UK, June 1989), Springer Verlag 1990.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378468</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Deering, Michael, S. Winner, B. Schediwy, C. Dully and N. Hunt. The Triangle Processor and Normal Vector Shader: A VLSI system for High Performance Graphics. Proceedings of SIGGRAPH '88 (Atlanta, GA, Aug 1-5, 1988). In Computer Graphics 22, 4 (July 1988), 21-30.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134039</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Deering, Michael. High Resolution Virtual Reality. Proceedings of SIGGRAPH '92 (Chicago, IL, July 26-31, 1992). In Computer Graphics 26, 2 (July 1992), 195-202.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>142455</ref_obj_id>
				<ref_obj_pid>142443</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Dunnett, Graham, M. White, P. Lister and R. Grimsdale. The Image Chip for High Performance 3D Rendering. IEEE Computer Graphics and Applications 12, 6 (November 1992), 41-52.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Foley, James, A. van Dam, S. Feiner and J Hughes. Computer Graphics: Principles and Practice, 2nd ed., Addison- Wesley, 1990.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134069</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Kelley, Michael, S. Winner, K. Gould. A Scalable Hardware Render Accelerator using a Modified Scanline Algorithm. Proceedings of SIGGRAPH '92 (Chicago, IL, July 26-31, 1992). In Computer Graphics 26, 2 (July 1992), 241-248.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97912</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Kirk, David, and D. Voorhies. The Rendering Architecture of the DN10000VS. Proceedings of SIGGRAPH '90 (Dallas, TX, August 6-10, 1990). In Computer Graphics 24, 4 (August 1990), 299-307.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134067</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Molnar, Steven, J. Eyles, J. Poulton. PixelFlow: High-Speed Rendering Using Image Composition. Proceedings of SIG- GRAPH '92 (Chicago, IL, July 26-31, 1992). In Computer Graphics 26, 2 (July 1992), 231-240.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Nelson, Scott. GPC Line Quality Benchmark Test. GPC Test Suite, NCGA GPC committee 1991.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37426</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Torborg, John. A Parallel Processor Architecture for Graphics Arithmetic Operations. Proceedings of SIGGRAPH '87 (Anaheim, CA, July 27-31, 1987). In Computer Graphics 21, 4 (July 1987), 197-204.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Leo: A System for Cost Effective 3D Shaded Graphics Michael F Deering, Scott R Nelson Sun Microsystems 
Computer Corporation ABSTRACT A physically compact, low cost, high performance 3D graphics ac­celerator 
is presented. It supports shaded rendering of triangles and antialiased lines into a double-buffered 
24-bit true color frame buf­fer with a 24-bit Z-buffer. Nearly the only chips used besides stan­dard 
memory parts are 11 ASICs (of four types). Special geometry data reformatting hardware on one ASIC greatly 
speeds and simpli­fies the data input pipeline. Floating-point performance is enhanced by another ASIC: 
a custom graphics microprocessor, with special­ized graphics instructions and features. Screen primitive 
rasteriza­tion is carried out in parallel by five drawing ASICs, employing a new partitioning of the 
back-end rendering task. For typical render­ing cases, the only system performance bottleneck is that 
intrinsi­cally imposed by VRAM. CR Categories and Subject Descriptors: C.1.2 [Processor Archi­tectures]: 
Multiprocessors; I.3.1 [Computer Graphics]: Hardware Architecture; I.3.3 [Computer Graphics]: Picture/Image 
Generation Display algorithms; I.3.7 [Computer Graphics]: Three Dimension­al Graphics and Realism. Additional 
Keywords and Phrases: 3D graphics hardware, ren­dering, parallel graphics algorithms, gouraud shading, 
antialiased lines, floating-point microprocessors. 1 INTRODUCTION To expand the role of 3D graphics in 
the mainstream computer in­dustry, cost effective, physically small, usable performance 3D shaded graphics 
architectures must be developed. For such systems, new features and sheer performance at any price can 
no longer be the driving force behind the architecture; instead, the focus must be on affordable desktop 
systems. The historical approach to achieving low cost in 3D graphics sys­tems has been to compromise 
both performance and image quality. But now, falling memory component prices are bringing nearly ideal 
2550 Garcia Avenue, MTV18-212 Mountain View, CA 94043-1100 michael.deering@Eng.Sun.COM (415)336-3017 
scott.nelson@Eng.Sun.COM (415)336-3106 Permission to copy without fee all or part of this material is 
granted provided that the copies are not made or distributed for directprovided that the copies are 
not made or distributed for direct commercial advantage, the ACM copyright notice and the title of thecommercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is bypublication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinery.permission of the Association for Computing Machinery. To 
copy otherwise, or to republish, requires a fee and/or specific permission.otherwise, or to republish, 
requires a fee and/or specific permission. &#38;#169;1993 8/93/008/0015 $1.50&#38;#169;1993 ACM - 0 - 
89791 - 601 - 8/93/008 $1.50 frame buffers into the price range of the volume market: double buffered 
24-bit color with a 24-bit Z-buffer. The challenge is to drive these memory chips at their maximum rate 
with a minimum of sup­porting rendering chips, keeping the total system cost and physical size to an 
absolute minimum. To achieve this, graphics architectures must be repartitioned to reduce chip count 
and internal bus sizes, while still supporting existing 2D and 3D functionality. This paper describes 
a new 3D graphics system, Leo, designed to these philosophies. For typical cases, Leo s only performance 
limit is that intrinsically imposed by VRAM. This was achieved by a combination of new architectural 
techniques and advances in VLSI technology. The result is a system without performance or image quality 
compromises, at an affordable cost and small physical size. The Leo board set is about the size of one 
and a half paperback nov­els; the complete workstation is slightly larger than two copies of Foley and 
Van Dam [7]. Leo supports both the traditional require­ments of the 2D X window system and the needs 
of 3D rendering: shaded triangles, antialiased vectors, etc.  2 ARCHITECTURAL ALTERNATIVES A generic 
pipeline for 3D shaded graphics is shown in Figure 1. ([7] Chapter 18 is a good overview of 3D graphics 
hardware pipeline is­sues.) This pipeline is truly generic, as at the top level nearly every commercial 
3D graphics accelerator .ts this abstraction. Where in­dividual systems differ is in the partitioning 
of this rendering pipe­line, especially in how they employ parallelism. Two major areas have been subject 
to separate optimization: the .oating-point inten­sive initial stages of processing up to, and many times 
including, primitive set-up; and the drawing-intensive operation of generating pixels within a primitive 
and Z-buffering them into the frame buffer. For low end accelerators, only portions of the pixel drawing 
stages of the pipeline are in hardware; the .oating-point intensive parts of the pipe are processed by 
the host in software. As general purpose processors increase in .oating-point power, such systems are 
start­ing to support interesting rendering rates, while minimizing cost [8]. But, beyond some limit, 
support of higher performance requires dedicated hardware for the entire pipeline. There are several 
choices available for partitioning the .oating­point intensive stages. Historically, older systems performed 
these tasks in a serial fashion [2]. In time though, breaking the pipe into more pieces for more parallelism 
(and thus performance) meant that each section was devoting more and more of its time to I/O overhead 
rather than to real work. Also, computational variance meant that many portions of the pipe would commonly 
be idle while others were overloaded. This led to the data parallel designs of most recent 3D graphics 
architectures [12].  SBus  DrawingIntensive Functions Floating-pointIntensive Functions VRAM Frame 
Buffer Digital to Analog Conversion Figure 1: Generic 3D Graphics Pipeline of further fragmenting the 
tasks and inducing additional overhead. But the most severe performance bottleneck lies in the pixel 
draw­ing back-end. The most fundamental constraint on 3D computer graphics architecture over the last 
ten years has been the memory chips that comprise the frame buffer. Several research systems have attempted 
to avoid this bottleneck by various techniques [10][4][8], but all commercial workstation systems use 
conventional Z-buffer rendering algorithms into standard VRAMs or DRAMs. How this RAM is organized is 
an important de.ning feature of any high per­formance rendering system. 3 LEO OVERVIEW Figure 2 is a 
diagram of the Leo system. This .gure is not just a block diagram; it is also a chip level diagram, as 
every chip in the Figure 2: The Leo Block Diagram. Every chip in the system is represented in this diagram. 
system is shown in this diagram. All input data and window system interactions enter through the LeoCommand 
chip. Geometry data is reformatted in this chip before being distributed to the array of Leo-Float chips 
below. The LeoFloat chips are microcoded specialized DSP-like processors that tackle the .oating-point 
intensive stages of the rendering pipeline. The LeoDraw chips handle all screen space pixel rendering 
and are directly connected to the frame buffer RAM chips. LeoCross handles the back-end color look-up 
tables, double buffering, and video timing, passing the .nal digital pixel values to the RAMDAC. The 
development of the Leo architecture started with the con­straints imposed by contemporary VRAM technology. 
As will be derived in the LeoDraw section below, these constraints led to the partitioning of the VRAM 
controlling LeoDraw chips, and set a maximum back-end rendering rate. This rate in turn set the perfor­mance 
goal for LeoFloat, as well as the data input bandwidth and processing rate for LeoCommand. After the 
initial partitioning of the rendering pipeline into these chips, each chip was subjected to additional 
optimization. Throughput bottlenecks in input geometry format conversion, .oating-point processing, and 
pixel rendering were identi.ed and overcome by adding reinforcing hardware to the appropriate chips. 
Leo s .oating-point intensive section uses data parallel partition­ing. LeoCommand helps minimize load 
balancing problems by breaking down rendering tasks to the smallest isolated primitives: individual triangles, 
vectors, dots, portions of pixel rasters, render­ing attributes, etc., at the cost of precluding optimizations 
for shared data in triangle strips and polylines. This was considered acceptable due to the very low 
average strip length empirically observed in real applications. The overhead of splitting geometric data 
into isolated primitives is minimized by the use of dedicated hardware for this task. Another bene.t 
of converting all rendering operations to isolated primitives is that down-stream processing of primitives 
is considerably simpli.ed by only needing to focus on the isolated case. 4 INPUT PROCESSING: LEOCOMMAND 
  Feeding the pipe Leo supports input of geometry data both as programmed I/O and through DMA. The host 
CPU can directly store up to 32 data words in an internal LeoCommand buffer without expensive read back 
testing of input status every few words. This is useful on hosts that do not support DMA, or when the 
host must perform format con­versions beyond those supported in hardware. In DMA mode, Leo-Command employs 
ef.cient block transfer protocols on the system bus to transfer data from system memory to its input 
buffer, allow­ing much higher bandwidth than simple programmed I/O. Virtual memory pointers to application 
s geometry arrays are passed direct­ly to LeoCommand, which converts them to physical memory addresses 
without operating system intervention (except when a page is marked as currently non-resident). This 
frees the host CPU to perform other computations during the data transfer. Thus the DMA can be ef.cient 
even for pure immediate-mode applications, where the geometry is being created on the .y. Problem: Tower 
of Babel of input formats One of the problems modern display systems face is the explosion of different 
input formats for similar drawing functions that need to be supported. Providing optimized microcode 
for each format rapidly becomes unwieldy. The host CPU could be used to pretrans­late the primitive formats, 
but at high speeds this conversion oper­ation can itself become a system bottleneck. Because DMA com­pletely 
bypasses the host CPU, LeoCommand includes a program­mable format conversion unit in the geometry data 
pipeline. This reformatter is considerably less complex than a general purpose CPU, but can handle the 
most commonly used input formats, and at very high speeds. The geometry reformatting subsystem allows 
several orthogonal operations to be applied to input data. This geometric input data is abstracted as 
a stream of vertex packets. Each vertex packet may contain any combination of vertex position, vertex 
normal, vertex color, facet normal, facet color, texture map coordinates, pick IDs, headers, and other 
information. One conversion supports arbitrary re-ordering of data within a vertex, allowing a standardized 
element order after reformatting. Another operation supports the conversion of multiple numeric formats 
to 32-bit IEEE .oating-point. The source data can be 8-bit or 16-bit .xed-point, or 32-bit or 64-bit 
IEEE .oating-point. Additional miscellaneous reformatting allows the stripping of headers and other .elds, 
the addition of an internal­ly generated sequential pick ID, and insertion of constants. The .nal reformatting 
stage re-packages vertex packets into complete isolated geometry primitives (points, lines, triangles). 
Chaining bits in vertex headers delineate which vertices form primitives. Like some other systems, Leo 
supports a generalized form of trian­gle strip (see Figure 3), where vertex header bits within a strip 
spec­ify how the incoming vertex should be combined with previous ver­tices to form the next triangle. 
A stack of the last three vertices used to form a triangle is kept. The three vertices are labeled oldest, 
mid­dle, and newest. An incoming vertex of type replace_oldest causes the oldest vertex to be replaced 
by the middle, the middle to be re­placed by the newest, and the incoming vertex becomes the newest. 
This corresponds to a PHIGS PLUS triangle strip (sometimes called a zig-zag strip). The replacement type 
replace_middle leaves the oldest vertex unchanged, replaces the middle vertex by the newest, and the 
incoming vertex becomes the newest. This corresponds to a triangle star. The replacement type restart 
marks the oldest and mid­dle vertices as invalid, and the incoming vertex becomes the newest. Generalized 
triangle strips must always start with this code. A trian­gle will be output only when a replacement 
operation results in three valid vertices. Restart corresponds to a move operation in polylines, and 
allows multiple unconnected variable-length triangle strips to be described by a single data structure 
passed in by the user, Vertex Codes 246 1 Restart 2 RO 3 RO 4 RO 135 5 ROTriangle Strip 6 RO 7 Restart 
10 8 RO 9 RO 10 RM 8 9 11 14 11 RM 12 RM Triangle Star 13 RM 13 12 14 RM 15 Restart 16 16 RO  Independent17 
RO Triangle 18 Restart 19 RO 15 17 20 RO 21 RO 19 21 22 Restart 23 RO  Independent 24 RO Quad 25 RO 
18 20 26 RO 27 RO 23 25 27 28 RO 28 29 RM 30 RM 31 RM 22 24 29 32 RM 32 33 RO 30 31 RO = Replace 
Oldest 33 RM = Replace Middle Mixed Strip Figure 3: A Generalized Triangle Strip reducing the overhead. 
The generalized triangle strip s ability to ef­fectively change from strip to star mode in the middle 
of a strip allows more complex geometry to be represented compactly, and re­quires less input data bandwidth. 
The restart capability allows sev­eral pieces of disconnected geometry to be passed in one DMA op­eration. 
Figure 3 shows a single generalized triangle strip, and the associated replacement codes. LeoCommand 
also supports header­less strips of triangle vertices either as pure strips, pure stars, or pure independent 
triangles. LeoCommand hardware automatically converts generalized trian­gle strips into isolated triangles. 
Triangles are normalized such that the front face is always de.ned by a clockwise vertex order after 
transformation. To support this, a header bit in each restart de.nes the initial face order of each sub-strip, 
and the vertex order is re­versed after every replace_oldest. LeoCommand passes each com­pleted triangle 
to the next available LeoFloat chip, as indicated by the input FIFO status that each LeoFloat sends back 
to Leo-Command. The order in which triangles have been sent to each LeoFloat is scoreboarded by LeoCommand, 
so that processed trian­gles are let out of the LeoFloat array in the same order as they en­tered. Non-sequential 
rendering order is also supported, but the automatic rendering task distribution hardware works so well 
that the performance difference is less than 3%. A similar, but less com­plex vertex repackaging is supported 
for polylines and multi­polylines via a move/draw bit in the vertex packet header. To save IC pins and 
PC board complexity, the internal Leo data bus­ses connecting LeoCommand, LeoFloat, and LeoDraw are 16 
bits in size. When colors, normals, and texture map coef.cients are being transmitted on the CF-bus between 
LeoCommand and the Leo-Floats, these components are (optionally) compressed from 32-bit IEEE .oating-point 
into 16-bit .xed point fractions by Leo-Command, and then automatically reconverted back to 32-bit IEEE 
.oating-point values by LeoFloat. This quantization does not effect quality. Color components will eventually 
end up as 8-bit values in the frame buffer. For normals, 16-bit (signed) accuracy represents a resolution 
of approximately plus or minus an inch at one mile. This optimization reduces the required data transfer 
bandwidth by 25%. Input from off-chip   5 FLOATING-POINT PROCESSING: LEOFLOAT After canonical format 
conversion, the next stages of processing tri­angles in a display pipeline are: transformation, clip 
test, face deter­mination, lighting, clipping (if required), screen space conversion, and set-up. These 
operations are complex enough to require the use of a general purpose processor. Use of commercially 
available DSP (Digital Signal Processing) chips for this work has two major drawbacks. First, most such 
pro­cessors require a considerable number of surrounding glue chips, especially when they are deployed 
as multi-processors. These glue chips can easily quadruple the board area dedicated to the DSP chip, 
as well as adversely affecting power, heat, cost, and reliability. Second, few of these chips have been 
optimized for 3D graphics. A better solution might be to augment the DSP with a special ASIC that would 
replace all of these glue chips. Given the expense of de­veloping an ASIC, we decided to merge that ASIC 
with a custom DSP core optimized for graphics. The resulting chip was LeoFloat. LeoFloat combines a 32-bit 
mi­crocodable .oating-point core with concurrent input and output packet communication subsystems (see 
Figure 4.), similar to the ap­proach of [3]. The only support chips required are four SRAM chips for 
external microcode store. A number of specialized graphics in­structions and features make LeoFloat different 
from existing DSP processors. Each individual feature only makes a modest incremen­tal contribution to 
performance, and indeed many have appeared in other designs. What is novel about LeoFloat is the combination 
of features, whose cumulative effect leads to impressive overall sys­tem performance. The following sections 
describe some of the more important special graphics instructions and features. Double buffered asynchronous 
I/O register .les. All input and output commands are packaged up by separate I/O packet hardware. Variable 
length packets of up to 32 32-bit words are automatically written into (or out of) on-chip double-buffered 
register .les (the I and O registers). These are mapped directly into microcode register space. Special 
instructions allow complete packets to be requested, relinquished, or queued for transmission in one 
instruction cycle. Enough internal registers. Most commercial DSP chips support a very small number of 
internal fast registers, certainly much smaller than the data needed by the inner loops of most 3D pipeline 
algo­rithms. They attempt to make up for this with on-chip SRAM or data caches, but typically SRAMs are 
not multi-ported and the caches not user-schedulable. We cheated with LeoFloat. We .rst wrote the code 
for the largest important inner loop (triangles), counted how many registers were needed (288), and built 
that many into the chip. Parallel internal function units. The .oating-point core functions (32-bit IEEE 
format) include multiply, ALU, reciprocal, and inte­ger operations, all of which can often be executed 
in parallel. It is particularly important that the .oating-point reciprocal operation not tie up the 
multiply and add units, so that perspective or slope calculations can proceed in parallel with the rest 
of geometric pro­cessing. Less frequently used reciprocal square root hardware is shared with the integer 
function unit. Put all non-critical algorithms on the host. We avoided the neces­sity of building a high 
level language compiler (and support instruc­tions) for LeoFloat by moving any code not worth hand coding 
in microcode to the host processor. The result is a small, clean kernel of graphics routines in microcode. 
(A fairly powerful macro-assem­bler with a C -like syntax was built to support the hand coding.) Software 
pipeline scheduling. One of the most complex parts of modern CPUs to design and debug is their scoreboard 
section, which schedules the execution of instructions across multiple steps in time and function units, 
presenting the programmer with the illusion that individual instructions are executed in one shot. Leo-Float 
avoided all this hardware by using more direct control .elds, like horizontal microprogrammable machines, 
and leaving it to the assembler (and occasionally the programmer) to skew one logical instruction across 
several physical instructions. Special clip condition codes &#38; clip branch. For clip testing we employ 
a modi.ed Sutherland-Hodgman algorithm, which .rst computes a vector of clip condition bits. LeoFloat 
has a clip test in­struction that computes these bits two at a time, shifting them into a special clip-bits 
register. After the bits have been computed, spe­cial branch instructions decode these bits into the 
appropriate case: clip rejected, clip accepted, single edge clip (six cases), or needs general clipping. 
There are separate branch instructions for trian­gles and vectors. (A similar approach was taken in [9].) 
The branch instructions allow multiple other conditions to be checked at the same time, including backfacing 
and model clipping. Register Y sort instruction. The .rst step of the algorithm we used for setting up 
triangles for scan conversion sorts the three triangle vertices in ascending Y order. On a conventional 
processor this re­quires either moving a lot of data, always referring to vertex data through indirect 
pointers, or replicating the set-up code for all six possible permutations of triangle vertex order. 
LeoFloat has a special instruction that takes the results of the last three comparisons and re­orders 
part of the R register .le to place vertices in sorted order. Miscellaneous. LeoFloat contains many performance 
features tra­ditionally found on DSP chips, including an internal subroutine stack, block load/store 
SRAM, and integer functions. Also there is a kitchen sink instruction that initiates multiple housekeeping 
functions in one instruction, such as transmit current output packet (if not clip pending), request new 
input packet, extract op-code and dispatch to next task. Code results: equivalent to 150 mega.op DSP. 
Each 25 MHz LeoFloat processes the benchmark isolated triangle (including clip­test and set-up) in 379 
clocks. (With a few exceptions, microcode instructions issue at a rate of one per clock tick.) The same 
graphics algorithm was tightly coded on several RISC processors and DSP chips (SPARC, i860, C30, etc.), 
and typically took on the order of 1100 clocks. Thus the 379 LeoFloat instruction at 25 MHz do the equivalent 
work of a traditional DSP chip running at 75 MHz (even though there are only 54 mega.ops of hardware). 
Of course these numbers only hold for triangles and vectors, but that's most of what LeoFloat does. Four 
LeoFloats assure that .oating-point processing is not the bottleneck for 100-pixel isolated, lighted 
triangles. 6 SCREEN SPACE RENDERING: LEODRAW VRAM limits Commercial VRAM chips represent a fundamental 
constraint on the possible pixel rendering performance of Leo s class of graphics accelerator. The goal 
of the Leo architecture was to ensure to the greatest extent possible that this was the only performance 
limit for typical rendering operations. The fundamental memory transaction for Z-buffered rendering algorithms 
is a conditional read-modify-write cycle. Given an XY address and a computed RGBZ value, the old Z value 
at the XY ad­dress is .rst read, and then if the computed Z is in front of the old Z, the computed RGBZ 
value is written into the memory. Such transactions can be mapped to allowable VRAM control signals in 
many different ways: reads and writes may be batched, Z may be read out through the video port, etc. 
VRAM chips constrain system rendering performance in two ways. First, they impose a minimum cycle time 
per RAM bank for the Z­buffered read-modify-write cycle. Figure 5 is a plot of this cycle Figure 5: VRAM 
cycle time and theoretical maximum trian­ gle rendering rate (for .ve-way interleaved frame buffers). 
time (when in page mode) and its changes over a half-decade period. VRAMs also constrain the ways in 
which a frame buffer can be partitioned into independently addressable banks. Throughout the .ve year 
period in Figure 5, three generations of VRAM technol­ogy have been organized as 256K by 4, 8, and 16-bit 
memories. For contemporary display resolutions of 1280 × 1024, the chips com­prising a minimum frame 
buffer can be organized into no more than .ve separately-addressed interleave banks. Combining this informa­tion, 
a theoretical maximum rendering speed for a primitive can be computed. The second line in Figure 5 is 
the corresponding perfor­mance for rendering 100-pixel Z-buffered triangles, including the overhead for 
entering page mode, content refresh, and video shift register transfers (video refresh). Higher rendering 
rates are only possible if additional redundant memory chips are added, allowing for higher interleaving 
factors, at the price of increased system cost. Even supporting .ve parallel interleaves has a cost: 
at least 305 memory interface pins (.ve banks of (24 RGB + 24 Z + 13 address/ control)) are required, 
more pins than it is currently possible to ded­icate to a memory interface on one chip. Some systems 
have used external buffer chips, but on a minimum cost and board area sys­tem, this costs almost as much 
as additional custom chips. Thus, on the Leo system we opted for .ve separate VRAM control chips (LeoDraws). 
 Triangle scan conversion Traditional shaded triangle scan conversion has typically been via a linear 
pipeline of edge-walking followed by scan interpolation [12]. There have been several approaches to achieving 
higher throughput in rasterization. [2] employed a single edge-walker, but parallel scan interpolation. 
[4][10] employed massively parallel rasterizers. [6] and other recent machines use moderately parallel 
rasterizers, with additional logic to merge the pixel rasterization streams back together. In the Leo 
design we chose to broadcast the identical triangle spec­i.cation to .ve parallel rendering chips, each 
tasked with rendering only those pixels visible in the local interleave. Each chip performs its own complete 
edge-walk and span interpolation of the triangle, biased by the chip's local interleave. By paying careful 
attention to proper mathematical sampling theory for rasterized pixels, the .ve chips can act in concert 
to produce the correct combined rasterized image. Mathematically, each chip thinks it is rasterizing 
the triangle into an image memory with valid pixel centers only every .ve orig­inal pixels horizontally, 
with each chip starting off biased one more pixel to the right. To obtain the speed bene.ts of parallel 
chips, most high perfor­mance graphics systems have split the edge-walk and span-interpo­late functions 
into separate chips. But an examination of the relative amounts of data .ow between rendering pipeline 
stages shows that the overall peak data transfer bandwidth demand occurs between the edge-walk and span-interpolate 
sections, induced by long thin triangles, which commonly occur in tessellated geometry. To mini­mize 
pin counts and PC board bus complexity, Leo decided to rep­licate the edge-walking function into each 
of the .ve span-interpo­lation chips. One potential drawback of this approach is that the edge-walking 
section of each LeoDraw chip will have to advance to the next scan line up to .ve times more often than 
a single rasterization chip would. Thus LeoDraw's edge-walking circuit was designed to oper­ate in one 
single pixel cycle time (160 ns. read-modify-write VRAM cycle), so it would never hold back scan conversion. 
Other usual pipelining techniques were used, such as loading in and buffering the next triangle to be 
drawn in parallel with rasterizing the current triangle. Window clipping, blending, and other pixel post 
processing are handled in later pipelined stages. Line scan conversion As with triangles, the mathematics 
of the line rasterization algo­rithms were set up to allow distributed rendering of aliased and antialiased 
lines and dots, with each LeoDraw chip handling the 1/5 of the frame buffer pixels that it owns. While 
the Leo system uses the X11 semantics of Bresenham lines for window system operations, these produce 
unacceptable motion artifacts in 3D wireframe rendering. Therefore, when rendering 3D lines, Leo employs 
a high-accuracy DDA algorithm, using 32 bits internally for suf.cient subpixel precision. At present 
there is no agreement in the industry on the de.nition of a high quality antialiased line. We choose 
to use the image quality of vector strokers of years ago as our quality standard, and we tested dif­ferent 
algorithms with end users, many of whom were still using cal­ligraphic displays. We found users desired 
algorithms that displayed no roping, angle sensitivities, short vector artifacts, or end-point arti­facts. 
We submitted the resulting antialiased line quality test patterns as a GPC [11] test image. In achieving 
the desired image quality lev­el, we determined several properties that a successful line antialias­ing 
algorithm must have. First, the lines must have at least three pix­els of width across the minor axis. 
Two-pixel wide antialiased lines exhibit serious roping artifacts. Four-pixel wide lines offer no visible 
improvement except for lines near 45 degrees. Second, proper end­point ramps spread over at least two 
pixels are necessary both for seamless line segment joins as well as for isolated line-ends. Third, proper 
care must be taken when sampling lines of subpixel length to maintain proper .nal intensity. Fourth, 
intensity or .lter adjustments based on the slope are necessary to avoid artifacts when rotating wireframe 
images. To implement all this, we found that we needed at least four bits of subpixel positional accuracy 
after cumulative inter­polation error is factored in. That is why we used 32 bits for XY co­ordinate 
accuracy: 12 for pixel location, 4 for subpixel location, and 16 for DDA interpolation error. (The actual 
error limit is imposed by the original, user-supplied 32-bit IEEE .oating-point data.) Because of the 
horizontal interleaving and preferred scan direction, the X-major and Y-major aliased and antialiased 
line rasterization algorithms are not symmetric, so separate optimized algorithms were employed for each. 
 Antialiased dots Empirical testing showed that only three bits of subpixel precision are necessary for 
accurate rendering of antialiased dots. For ASIC implementation, this was most easily accomplished using 
a brute­force table lookup of one of 64 precomputed 3 × 3 pixel dot images. These images are stored in 
on-chip ROM, and were generated using a circular symmetric Gaussian .lter. Triangle, line, and dot hardware 
Implementation of the triangle and antialiased vector rasterization algorithms require substantial hardware 
resources. Triangles need single pixel cycle edge-walking hardware in parallel with RGBZ span interpolation 
hardware. To obtain the desired quality of anti­aliased vectors, our algorithms require hardware to apply 
multiple waveform shaping functions to every generated pixel. As a result, the total VLSI area needed 
for antialiased vectors is nearly as large as for triangles. To keep the chip die size reasonable, we 
reformu­lated both the triangle and antialiased vector algorithms to combine and reuse the same function 
units. The only difference is how the separate sequencers set up the rasterization pipeline. Per-pixel 
depth cue Depth cueing has long been a heavily-used staple of wireframe ap­plications, but in most modern 
rendering systems it is an extra time expense feature, performed on endpoints back in the .oating-point 
section. We felt that we were architecting Leo not for benchmarks, but for users, and many wireframe 
users want to have depth cueing on all the time. Therefore, we built a parallel hardware depth cue function 
unit into each LeoDraw. Each triangle, vector, or dot ren­dered by Leo can be optionally depth cued at 
absolutely no cost in performance. Another bene.t of per-pixel depth cueing is full com­pliance with 
the PHIGS PLUS depth cueing speci.cation. For Leo, per-pixel depth cueing hardware also simpli.es the 
LeoFloat mi­crocode, by freeing the LeoFloats from ever having to deal with it. Picking support Interactive 
graphics requires not only the rapid display of geometric data, but also interaction with that data: 
the ability to pick a partic­ular part or primitive within a part. Any pixels drawn within the bounds 
of a 3D pick aperture result in a pick hit, causing the current pick IDs to be automatically DMAed back 
to host memory. Window system support Many otherwise sophisticated 3D display systems become some­what 
befuddled when having to deal simultaneously with 3D ren­dering applications and a 2D window system. 
Modern window sys­tems on interactive workstations require frequent context switching of the rendering 
pipeline state. Some 3D architectures have tried to minimize the overhead associated with context switching 
by sup­porting multiple 3D contexts in hardware. Leo goes one step fur­ther, maintaining two completely 
separate pipelines in hardware: one for traditional 2D window operations; the other for full 3D ren­dering. 
Because the majority of context switch requests are for 2D window system operations, the need for more 
complex 3D pipeline context switching is signi.cantly reduced. The 2D context is much lighter weight 
and correspondingly easier to context switch. The two separate graphics pipelines operate completely 
in parallel, al­lowing simultaneous access by two independent CPUs on a multi­processor host. 2D functionality 
abstracts the frame buffer as a 1-bit, 8-bit, or 24-bit pixel array. Operations include random pixel 
access, optimized char­acter cell writes, block clear, block copy, and the usual menagerie of boolean 
operations, write masks, etc. Vertical block moves are spe­cial cased, as they are typically used in 
vertical scrolling of text windows, and can be processed faster than the general block move because the 
pixel data does not have to move across LeoDraw chip interleaves. Rendering into non-rectangular shaped 
windows is supported by special clip hardware, resulting in no loss in perfor­mance. A special block 
clear function allows designated windows (and their Z-buffers) to be initialized to any given constant 
in under 200 microseconds. Without this last feature, 30 Hz or faster anima­tion of non-trivial objects 
would have been impossible.  7 VIDEO OUTPUT: LEOCROSS Leo's standard video output format is 1280 × 1024 
at 76 Hz refresh rate, but it also supports other resolutions, including 1152 × 900, interlaced 640 × 
480 RS-170 (NTSC), interlaced 768 × 576 PAL timing, and 960 × 680 113 Hz .eld sequential stereo. LeoCross 
contains several color look-up tables, supporting multiple pseudo color maps without color map .ashing. 
The look-up table also sup­ports two different true color abstractions: 24-bit linear color (needed by 
rendering applications), and REC-709 non-linear color (required by many imaging applications). Virtual 
reality support Stereo output is becoming increasingly important for use in Virtual Reality applications. 
Leo s design goals included support for the Virtual Holographic Workstation system con.guration described 
in [5]. Leo s stereo resolution was chosen to support square pixels, so that lines and antialiased lines 
are displayed properly in stereo, and standard window system applications can co-exist with stereo. Ste­reo 
can be enabled on a per-window basis (when in stereo mode win­dows are effectively quad-buffered). Hooks 
were included in Leo-Cross to support display technologies other than CRT's, that may be needed for head-mounted 
virtual reality displays. 8 NURBS AND TEXTURE MAP SUPPORT One of the advantages to using programmable 
elements within a graphics accelerator is that additional complex functionality, such as NURBS and texture 
mapping, can be accelerated. Texture map­ping is supported through special LeoFloat microcode and features 
of LeoCommand. LeoFloat microcode also includes algorithms to accelerate dynamic tessellation of trimmed 
NURBS surfaces. The dynamic tessellation technique involves reducing trimmed NURBS surfaces into properly 
sized triangles according to a display/pixel space approximation criteria [1]; i.e. the .neness of tessellation 
is view dependent. In the past, dynamic tessellation tended to be mainly useful as a compression technique, 
to avoid storing all the .attened triangles from a NURBS surface in memory. Dynamic tes­sellation was 
not viewed as a performance enhancer, for while it might generate only a third as many triangles as a 
static tessellation, the triangles were generated at least an order of magnitude or more slower than 
brute force triangle rendering. In addition it had other problems, such as not handling general trimming. 
For many cases, Leo's dynamic tesselator can generate and render triangles only a small integer multiple 
slower than prestored triangle rendering, which for some views, can result in faster overall object rendering. 
9 RESULTS Leo is physically a-two board sandwich, measuring 5.7 × 6.7 × 0.6 inches, that .ts in a standard 
2S SBus slot. Figure 6 is a photo of the two boards, separated, showing all the custom ASICs. Figure 
7 is a photo of the complete Leo workstation, next to two of our units of scale and the board set. Leo 
can render 210K 100-pixel isolated, lighted, Gouraud shaded, Z-buffered, depth cued triangles per second, 
with one in.nite dif­fuse and one ambient light source enabled. At 100 pixels, Leo is still VRAM rendering 
speed limited; smaller triangles render faster. Isolated 10-pixel antialiased, constant color, Z-buffered, 
depth cued lines (which are actually 12 pixels long due to endpoint ramps, and three pixels wide) render 
at a 422K per second rate. Corresponding aliased lines render at 730K. Aliased and antialiased constant 
color, Z-buffered, depth cued dots are clocked at 1100K. 24-bit image ras­ters can be loaded onto the 
screen at a 10M pixel per second rate. Screen scrolls, block moves, and raster character draws all also 
have competitive performance. Figure 8 is a sample of shaded tri­angle rendering. 10 SIMULATION A system 
as complex as Leo cannot be debugged after the fact. All the new rendering mathematics were extensively 
simulated before being committed to hardware design. As each chip was de.ned, high, medium, and low level 
simulators of its function were written and continuously used to verify functionality and performance. 
Com­plete images of simulated rendering were generated throughout the course of the project, from within 
weeks of its start. As a result, the window system and complex 3D rendering were up and running on a 
complete board set within a week of receiving the .rst set of chips. 11 CONCLUSIONS By paying careful 
attention to the forces that drive both perfor­mance and cost, a physically compact complete 3D shaded 
graphics accelerator was created. The focus was not on new rendering fea­tures, but on cost reduction 
and performance enhancement of the most useful core of 3D graphics primitives. New parallel algo­rithms 
were developed to allow accurate screen space rendering of primitives. Judicious use of hardware to perform 
some key tradi­tional software functions (such as format conversion and primitive vertex reassembly) 
greatly simpli.ed the microcode task. A spe­cialized .oating-point core optimized for the primary task 
of pro­cessing lines and triangles also supports more general graphics pro­cessing, such as rasters and 
NURBS. The .nal system performance is limited by the only chips not custom designed for Leo: the stan­dard 
RAM chips.  ACKNOWLEDGEMENTS The authors would like to thank the entire Leo team for their efforts in 
producing the system, and Mike Lavelle for help with the paper.  REFERENCES 1. Abi-Ezzi, Salim, and 
L. Shirman. Tessellation of Curved Surfaces under Highly Varying Transformations. Proc. Euro­graphics 
'91 (Vienna, Austria, September 1991), 385-397. 2. Akeley, Kurt and T. Jermoluk. High-Performance Polygon 
Rendering, Proceedings of SIGGRAPH '88 (Atlanta, GA, Aug 1-5, 1988). In Computer Graphics 22, 4 (July 
1988), 239-246. 3. Anido, M., D. Allerton and E. Zaluska. MIGS - A Multipro­cessor Image Generation 
System using RISC-like Micropro­cessors. Proceedings of CGI 89 (Leeds, UK, June 1989), Springer Verlag 
1990. 4. Deering, Michael, S. Winner, B. Schediwy, C. Duffy and N. Hunt. The Triangle Processor and 
Normal Vector Shader: A VLSI system for High Performance Graphics. Proceedings of SIGGRAPH '88 (Atlanta, 
GA, Aug 1-5, 1988). In Computer Graphics 22, 4 (July 1988), 21-30.  5. Deering, Michael. High Resolution 
Virtual Reality. Proceed­ings of SIGGRAPH '92 (Chicago, IL, July 26-31, 1992). In Computer Graphics 26, 
2 (July 1992), 195-202. 6. Dunnett, Graham, M. White, P. Lister and R. Grimsdale. The Image Chip for 
High Performance 3D Rendering. IEEE Computer Graphics and Applications 12, 6 (November 1992), 41-52. 
7. Foley, James, A. van Dam, S. Feiner and J Hughes. Com­puter Graphics: Principles and Practice, 2nd 
ed., Addison-Wesley, 1990. 8. Kelley, Michael, S. Winner, K. Gould. A Scalable Hardware Render Accelerator 
using a Modified Scanline Algorithm. Proceedings of SIGGRAPH '92 (Chicago, IL, July 26-31, 1992). In 
Computer Graphics 26, 2 (July 1992), 241-248. 9. Kirk, David, and D. Voorhies. The Rendering Architecture 
of the DN10000VS. Proceedings of SIGGRAPH '90 (Dallas, TX, August 6-10, 1990). In Computer Graphics 24, 
4 (August 1990), 299-307. 10. Molnar, Steven, J. Eyles, J. Poulton. PixelFlow: High-Speed Rendering 
Using Image Composition. Proceedings of SIG-GRAPH '92 (Chicago, IL, July 26-31, 1992). In Computer Graphics 
26, 2 (July 1992), 231-240. 11. Nelson, Scott. GPC Line Quality Benchmark Test. GPC Test Suite, NCGA 
GPC committee 1991. 12. Torborg, John. A Parallel Processor Architecture for Graph­ics Arithmetic Operations. 
Proceedings of SIGGRAPH '87 (Anaheim, CA, July 27-31, 1987). In Computer Graphics 21, 4 (July 1987), 
197-204.     
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166131</article_id>
		<sort_key>109</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Reality Engine graphics]]></title>
		<page_from>109</page_from>
		<page_to>116</page_to>
		<doi_number>10.1145/166117.166131</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166131</url>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Parallel processing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Antialiasing**</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010386</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Antialiasing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39073268</person_id>
				<author_profile_id><![CDATA[81100563035]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kurt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akeley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>378516</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AKELEY, KURT AND TOM JERMOLUK. High-Performance Polygon Rendering. In Proceedings of SIGGRAPH '88 (August 1988), pp. 239-246.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808585</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[CARPENTER, LOREN. The A-buffer, An Antialiased Hidden Surface Method. In Proceedings of SIGGRAPH '84 (July 1984), pp. 103-108.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97913</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[HAEBERLI, PAUL AND KURT AKELEY. The Accumulation Buffer: Hardware Support for High-Quality Rendering. In Proceedings of SIGGRAPH ' 90 (August 1990), pp. 309-318.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97912</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[KIRK, DAVID AND DOUGLAS VOORHIES. The Rendering Architecture of the DN 10000VS. In Proceedings of SIGGRAPH ' 90 (August 1990), pp. 299-308.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>897993</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[MOLNAR, STEVEN. Image-Composition Architectures for Real-Time Image Generation. University of North Carolina at Chapel Hill, Chapel Hill, NC, 1991.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134067</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[MOLNAR, STEVEN, JOHN EYLES AND JOHN POULTON. PixelFlow: High-Speed Rendering Using Image Composition. In Proceedings of SIGGRAPH '92 (July 1992), pp. 231-240.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[NEIDER, JACQUELINE, MASON WOO AND TOM DAVIS. OpenGL Programming Guide. Addison Wesley, 1993.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[OPENGL ARCHITECTURE REVIEW BOARD. OpenGL Reference Manual. Addison Wesley, 1992.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378457</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[PINEDA, JUAN. A Parallel Algorithm for Polygon Rasterization. In Proceedings of SIGGRAPH '88 (August 1988), pp. 17-20.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122733</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[SCHILLING, ANDREAS. A New Simple and Efficient Antialiasing with Subpixel Masks. In Proceedings of SIGGRAPH '91 (July 1991), pp. 133-141.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[SILICON GRAPHICS, INC. Iris 4DGT Technical Report. Silicon Graphics, Inc., Mountain View, CA, 1988.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[SILICON GRAPHICS, INC. Technical Report - Power Series. Silicon Graphics, Inc., Mountain View, CA, 1990.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801126</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[WILLIAMS, LANCE. Pyramidal Parametrics. In Proceedings of SIGGRAPH '83 (July 1983), pp. 1-11.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 RealityEngine Graphics Kurt Akeley . Silicon Graphics Computer Systems Abstract The RealityEngineTM 
graphics system is the .rst of a new genera­tion of systems designed primarily to render texture mapped, 
an­tialiased polygons. This paper describes the architecture of the RealityEngine graphics system, then 
justi.es some of the decisions made during its design. The implementation is near-massively par­allel, 
employing 353 independent processors in its fullest con.gura­tion, resulting in a measured .ll rate of 
over 240 million antialiased, texture mapped pixels per second. Rendering performance exceeds 1 million 
antialiased, texture mapped triangles per second. In ad­dition to supporting the functions required of 
a general purpose, high-end graphics workstation, the system enables realtime, out­the-window image generation 
and interactive image processing. CR Categories and Subject Descriptors: I.3.1 [Computer Graphics]: Hardware 
Architecture; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism -color, shading, shad­owing, 
and texture 1 Introduction This paper describes and to a large extent justi.es the architecture chosen 
for the RealityEngine graphics system. The designers think of this system as our .rst implementation 
of a third-generation graphics system. To us a generation is characterized not by the scope of capabilities 
of an architecture, but rather by the capabili­ties for which the architecture was primarily designed 
 the target capabilities with maximized performance. Because we designed our .rst machine in the early 
eighties, our notion of .rst generation corresponds to this period. Floating point hardware was just 
be­comingavailableatreasonableprices,framebuffer memorywasstill quite expensive, and application-speci.cintegrated 
circuits (ASICs) were not readily available. The resulting machines had workable transformation capabilities, 
but very limited framebuffer process­ing capabilities. In particular, smooth shading and depth buffering, 
which require substantial framebuffer hardware and memory, were not available. Thus the target capabilities 
of .rst-generation ma­chines were the transformation and rendering of .at-shaded points, lines, and polygons. 
These primitives were not lighted, and hidden surface elimination, if required, was accomplished by algorithms 
implemented by the application. Examples of such systems are the e 2011 N. Shoreline Blvd., Mountain 
View, CA 94043 USA, kurt@sgi.com Permission to copy without fee all or part of this material is granted 
 provided that the copies are not made or distributed for direct provided that the copies are not made 
or distributed for direct commercial advantage, the ACM copyright notice and the title of the commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by publication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinery. permission of the Association for Computing Machinery. To 
copy otherwise, or to republish, requires a fee and/or specific permission. otherwise, or to republish, 
requires a fee and/or specific permission. &#38;#169;1993 8/93/008/0015 $1.50 &#38;#169;1993 ACM - 0 
- 89791 - 601 - 8/93/008 $1.50 Silicon Graphics Iris 3000 (1985) and the Apollo DN570 (1985). Toward 
the end of the .rst-generation period advancesin technology allowed lighting, smooth shading, and depth 
buffering to be imple­mented, but only with an order of magnitude less performance than was available 
to render .at-shaded lines and polygons. Thus the target capability of these machines remained .rst-generation. 
The Silicon Graphics 4DG (1986) is an example of such an architecture. Because .rst-generation machines 
could not ef.ciently eliminate hidden surfaces, and could not ef.ciently shade surfaces even if the application 
was able to eliminate them, they were more effective at rendering wireframe images than at rendering 
solids. Begin­ning in 1988 a second-generation of graphics systems, primarily workstations rather than 
terminals, became available. These ma­chines took advantage of reduced memory costs and the increased 
availability of ASICs to implement deep framebuffers with multiple rendering processors. These framebuffers 
had the numeric ability to interpolate colors and depths with little or no performance loss, and the 
memory capacity and bandwidth to support depth buffering with minimal performance loss. They were therefore 
able to render solidsandfull-frame scenesef.ciently,aswellaswireframeimages. The Silicon Graphics GT 
(1988)[11] and the Apollo DN590 (1988) are early examples of second-generation machines. Later second­generation 
machines, such as the Silicon Graphics VGX[12] the Hewlett Packard VRX, and the Apollo DN10000[4] include 
texture mapping and antialiasing of points and lines, but not of polygons. Their performances are substantially 
reduced, however, when tex­ture mapping is enabled, and the texture size (of the VGX) and .ltering capabilities 
(of the VRX and the DN10000) are limited. The RealityEngine system is our .rst third-generation design. 
Its target capability is the rendering of lighted, smooth shaded, depth buffered, texture mapped, antialiased 
triangles. The initial target performance was 1/2 million such triangles per second, assuming the triangles 
are in short strips, and 10 percent intersect the viewing frustum boundaries. Textures were to be well 
.ltered (8-sample lin­ear interpolation within and between two mipmap[13] levels) and large enough () 
to be usable as true images, rather 1024 ( 1024 than simply as repeated textures. Antialiasing was to 
result in high­quality images of solids, and was to work in conjunction with depth buffering, meaning 
that no application sorting was to be required. Pixels were to be .lled at a rate suf.cient to support 
30Hz ren­dering of full-screen images. Finally, the performance on second­generation primitives (lighted, 
smooth shaded, depth buffered) was to be no lower than that of the VGX, which renders roughly 800,000 
such mesh triangles per second. All of these goals were achieved. The remainder of this paper is in four 
parts: a description of the architecture, some speci.cs of features supported by the architec­ture, alternatives 
considered during the design of the architecture, and .nally some appendixes that describe performance 
and imple­mentation details.  2 Architecture The RealityEngine system is a 3, 4, or 6 board graphics 
accelerator that is installed in a MIPS RISC workstation. The graphics system and one or more MIPS processors 
are connected by a single system bus. Figure 1 is a board-level block diagram of the RealityEngine graphics 
accelerator. The geometry board comprises an input FIFO, the Command Processor, and 6, 8, or 12 Geometry 
Engines. Each raster memory board comprises 5 Fragment Generators (each with its own complete copy of 
the texture memory), 80 Image Engines, and enough framebuffer memory to allocate 256 bits per pixel to 
a  1280 ( 1024 framebuffer. The display generator board supports all video functions, including video 
timing, genlock, color mapping, and digital-to-analog conversion. Systems can be con.gured with 1, 2, 
or 4 raster memory boards, resulting in 5, 10, or 20 Fragment Generators and 80, 160, or 320 Image Engines. 
To get an initial notion of how the system works, let s follow a single triangle as it is rendered. The 
position, color, normal, and texture coordinate commands that describe the vertexes of the triangle in 
object coordinates are queued by the input FIFO, then interpreted by the Command Processor. The Command 
Processor directs all of this data to one of the Geometry Engines, where the coordinates and normals 
are transformed to eye coordinates, lighted, transformed to clip coordinates, clipped, and projected 
to window coordinates. The associated texture coordinates are transformed by a third matrix and associated 
with the window coordinates and colors. Then window coordinate slope information regarding the red, green, 
blue, alpha, depth, and texture coordinates is computed. The projected triangle, ready for rasterization, 
is then output from the Geometry Engine and broadcast on the Triangle Bus to the 5, 10, or 20 Fragment 
Generators. (We distinguish between pixels generated by rasterization and pixels in the framebuffer, 
referring to the former as fragments.) Each Fragment Generator is responsible for the rasterization of 
1/5, 1/10, or 1/20 of the pixels in the frame­buffer, with the pixel assignments .nely interleaved to 
insure that even small triangles are partially rasterized by each of the Fragment Generators. Each Fragment 
Generator computes the intersection of the set of pixels that are fully or partially covered by the triangle 
and the set of pixels in the framebuffer that it is responsible for, gener­ating a fragment for each 
of these pixels. Color, depth, and texture coordinates are assigned to each fragment based on the initial 
and slope values computed by the Geometry Engine. A subsample mask is assigned to the fragment based 
on the portion of each pixel that is covered by the triangle. The local copy of the texture memory is 
indexed by the texture coordinates, and the 8 resulting samples are reduced by linear interpolation to 
a single color value, which then modulates the fragment s color. The resulting fragments, each comprising 
a pixel coordinate, a color, a depth, and a coverage mask, are then distributed to the Image Engines. 
Like the Fragment Generators, the Image Engines are each assigned a .xed subset of the pixels in the 
framebuffer. These subsets are themselves subsets of the Fragment Generator allocations, so that each 
Fragment Generator communicates only with the 16 Image Engines assigned to it. Each Image Engine manages 
its own dynamic RAM that implements its subset of the framebuffer. When a fragment is received by an 
Image Engine, its depth and color sample data are merged with the data already stored at that pixel, 
and a new aggregate pixel color is immediately computed. Thus the image is complete as soon as the last 
primitive has been rendered; there is no need for a .nal framebuffer operation to resolve the multiple 
color samples at each pixel location to a single displayable color. Before describing each of the rendering 
operations in more detail, we make the following observations. First, after it is separated by the Command 
Processor, the stream of rendering commands merges only at the Triangle Bus. Second, triangles of suf.cient 
size (a function of the number of raster memory boards) are processed by almost all the processors in 
the system, avoiding only 5, 7, or 11 Geometry Engines. Finally, small to moderate FIFO memories are 
included at the input and output of each Geometry Engine, at the input of each Fragment Generator, and 
at the input of each Image Engine. These memories smooth the .ow of rendering commands, helping to insure 
that the processors are utilized ef.ciently. 2.1 Command Processor That the Command Processor is required 
at all is primarily a func­tion of the OpenGLTM [8][7] graphics language. OpenGL is modal, meaning that 
much of the state that controls rendering is included in the command stream only when it changes, rather 
than with each graphics primitive. The Command Processor distinguishes between two classes of this modal 
state. OpenGL commands that are expected infrequently, such as matrix manipulations and light­ing model 
changes, are broadcast to all the Geometry Engines. OpenGL commands that are expected frequently, such 
as vertex colors, normals, and texture coordinates, are shadowed by the Com­mand Processor, and the current 
values are bundled with each ren­dering command that is passed to an individual Geometry Engine. The 
Command Processor also breaks long connected sequences of line segments or triangles into smaller groups, 
each group passing to a single Geometry Engine. The size of these groups is a trade­off between the increased 
vertex processing ef.ciency of larger groups (due to shared vertexes within a group) and the improved 
load balancing that results from smaller groups. Finally, because the Command Processor must interpret 
each graphics command, it is also able to detect invalid command sequences and protect the FromCommand 
Processor 48  i860XP 64 256Kx64 48 DRAM To Triangle Bus Figure 2. Individual Geometry Engine. subsequent 
processors from their effects. Non-broadcast rendering commands are distributed to the Ge­ometry Engines 
in pure round-robin sequence, taking no account of Geometry Engine loading. This approach was chosen 
for its simplicity, and is ef.cient because the processing requirements of primitives are usually very 
similar, and because the input and out­put FIFOs of each Geometry Engine smooth the imbalances due to 
data-dependent processing such as clipping.  2.2 Geometry Engines The core of each Geometry Engine is 
an Intel i860XP processor. Operating at 50MHz, the combined .oating point multiplier and ALU can achieve 
a peak performance of 100 MFLOPS. Each Intel processor is provided 2 Mbytes of combined code/data dynamic 
memory, and is supported by a single ASIC that implements the in­put and output FIFOs, a small register 
space from which the i860XP accesses incoming commands, and specialized data conversion fa­cilities that 
pack computed slope data into a format accepted by the Fragment Generators. (Figure 2.) All Geometry 
Engine code is .rst developed in C, which is cross compiled for the i860XP on MIPS RISC development systems. 
Code that is executed frequently is then re-coded in i860XP assem­bly code, showing the greatest improvement 
in performance where scheduling of the vector .oating point unit is hand optimized. The assembly code 
is written to conform to the compiler s link conven­tions, so that hand-codedand compiled modules are 
interchangeable for development and documentation purposes. Most .oating point arithmetic is done in 
single precision, but much of the texture arithmetic, and all depth arithmetic after projec­tion transformation, 
must be done in double precision to maintain the required accuracy. After transformation, lighting, and 
clipping, the rasterization setup code treats each parameter as a plane equa­tion, computing its signed 
slope in the positive X and Y screen directions. Because the parameters of polygons with more than 3 
vertexes may be non-planar, the Geometry Engine decomposes all polygons to triangles. 2.3 Triangle Bus 
The Triangle Bus acts as a crossbar, connecting the output of each Geometry Engine to the inputs of all 
the Fragment Generators. Because all Geometry Engine output converges at this bus, it is a potential 
bottleneck. To avoid performance loss, the Triangle Bus was designed with bandwidth to handle over one 
million shaded, depth buffered, texture mapped, antialiased triangles per second, more than twice the 
number of primitives per second that were anticipated from an 8 Geometry Engine system. This performance 
cushion allows the later-conceived 12 Geometry Engine system to render at full performance, in spite 
of the greater than expected performance of the individual engines. In addition to broadcasting the rasterization 
data for triangles to the Fragment Generators, the Triangle Bus broadcasts point and line segment descriptions, 
texture images, and rasterization mode changes such as blending functions. 2.4 Fragment Generators Although 
each Fragment Generator may be thought of as a single processor, the data path of each unit is actually 
a deep pipeline. This pipeline sequentially performs the initial generation of fragments, generation 
of the coverage mask, texture address generation, texture lookup, texture sample .ltering, texture modulation 
of the fragment color, and fog computation and blending. These tasks are distributed among the four ASICs 
and eight dynamic RAMs that comprise each Fragment Generator. (Figure 3.) Fragments are generated using 
Pineda arithmetic[9], with the algorithm modi.ed to traverse only pixels that are in the domain of the 
Fragment Generator. A coverage mask is generated for 4, 8, or 16 sample locations, chosen on a regular 
subsample grid 8 ( 8 within the square boundaries of the pixel. The hardware imposes no constraints on 
which subset of the 64 subsample locations is chosen, except that the same subset is chosen for each 
pixel. The subset may be changed by the application between frames. Depth and texture coordinate sample 
values are always computed at the center-most sample location, regardless of the fragment cov­erage mask. 
The single depth sample is later used by the Image Engines to derive accurate depth samples at each subpixel 
location, using the X and Y depth slopes. Taking the texture sample at a consistent location insures 
that discontinuities are avoided at pixels that span multiple triangles. Color sample values are computed 
at the center-most sample location only if it is within the perimeter of the triangle. Otherwise the 
color sample is taken at a sample location within the triangle perimeter that is near the centroid of 
the covered region. Thus color samples are always taken within the triangle perimeter, and therefore 
never wrap to inappropriate values. Based on a level-of-detail (LOD) calculation and the texture co­ordinate 
values at the fragment center, the addresses of the eight texels nearest the sample location in the mipmap 
of texture images are produced. Eight separate banks of texture memory are then accessed in parallel 
at these locations. The 8 16-bit values that result are merged with a trilinear blend, based on the subtexel 
co­ordinates and the LOD fraction, resulting in a single texture color that varies smoothly from frame 
to frame in an animation. The entire bandwidth of the 8-bank texture memory is consumed by a single Fragment 
Engine, so each Fragment Engine includes its own complete copy of all texture images in its texture memory, 
allowing all Fragment Generators to operate in parallel. Separate FIFO mem­ories on the address and data 
ports of each texture memory bank insure that random page boundary crossings do not signi.cantly degrade 
the bandwidth available from the dynamic RAMs. The last ASIC in the Fragment Generator applies the texture 
color to the fragment s smooth shaded color, typically by modulation. It then indexes its internal fog 
table with the fragment s depth value and uses the resulting fog blend factor (computed by linear interpolation 
between the two nearest table entries) to blend the fragment color with the application-de.ned fog color. 
 2.5 Image Engines Fragments output by a single Fragment Generator are distributed equally among the 
16 Image Engines connected to that generator. When the triangle was .rst accepted by the Fragment Generator 
for processing, its depth slopes in the X and Y screen directions were broadcast to each Image Engine, 
which stored them for later use. When an Image Engine accepts a fragment, it .rst uses these two slope 
values and the fragment s depth sample value to reconstruct the depth values at each subpixel sample 
location. The arithmetic required for this operation is simpli.ed because the subpixel sam­ple locations 
are .xed to a regular grid. The calculations are 8 ( 8 linear because depth values have been projected 
to window coor­dinates just like the X and Y pixel coordinates. At each sample location corresponding 
to a 1 in the fragment s coverage mask, the computed depth value is compared to the depth value stored 
in the framebuffer. If the comparison succeeds, the framebuffer color at that subsample location is replaced 
by the fragment color, and the framebuffer depth is replaced by the derived fragment depth. If any change 
is made to the pixel s contents, the aggregate pixel color is recomputed by averaging the subpixel sample 
colors, and is immediately written to the displayable color buffer that will contain the .nal image. 
Each Image Engine controls a single 256K 16 dynamic RAM ( that comprises its portion of the framebuffer. 
(Figure 4.) When the framebuffer is initialized, this memory is partitioned equally among 4K, 8K, or 
16K pixels, resulting in pixels with 1024, 512, or 256 bits. All subsample depth and color samples, as 
well as the one, two, or four displayable color buffers and other auxiliary buffers, are stored in this 
memory. By default, colors are stored From Fragment Generator 4  256K x 16 16 DRAM 1 To Display Generator 
Figure 4. Individual Image Engine. with 12 bits per red, green, blue, and alpha component in both the 
displayable buffers and the subpixel samples. Depth values are 32 bits each, and are normally required 
only for each subpixel sample, not for the displayable color buffer or buffers. Color and depth sample 
resolutions can be reduced to 8,8,8 and 24 bits to allow more samples to be stored per pixel. The 4K 
partition stores 8 high­resolution samples per pixel, or 16 low-resolution samples per pixel, in addition 
to two displayable color buffers of the same resolution. The 8K partition stores 4 high-resolution samples 
per pixel, or 8 low-resolution samples per pixel, again with two displayable color buffers of the same 
resolution. The 16K partition cannot be used to support multisample antialiasing. Because the number 
of raster memory boards (1, 2, or 4) and the number of pixels per Image Engine (4K, 8K, or 16K) are in­dependent, 
the RealityEngine system supports a wide variety of framebuffer dimensions, color and depth resolutions, 
and subpixel samples. For example, a single raster board system supports 16­sample antialiasing at resolution 
or aliased rendering at 1280 ( 10241280 ( 6401024 ( 1920512 ( 1035 resolution, and a 4-board system supports 
8-sample antialiasing at true HDTV () resolution or 16-sample antialiasing at resolution. 2.6 Display 
Hardware Each of the 80 Image Engines on the raster memory board drives a single-bit, 50 MHz path to 
the display board, delivering video data at 500 MBytes per second. All 160 single-bit paths of a two 
raster memory board con.guration are active, doubling the peak video data rate. The paths are time multiplexed 
by pairs of raster memory boards in the four board con.guration. Ten crossbar ASICs on the display board 
assemble the 80 or 160 single-bit streams into individual color components or color indexes. Color components 
are then dithered from 12 bits to 10 bits and gamma corrected using  1024 ( 8 lookup tables. The resulting 
8-bit color components drive digital-to-analog converters and are output to the monitor. Color indexes 
are dereferenced in a 32K-location lookup table, supporting separate color lookup tables for each of 
up to 40 windows on the screen. Per-pixel display modes, such as the color index offset, are supported 
by a combination of Image Engine and display board hardware, driven by window ID bits stored in the framebuffer 
[1]. 3 Features This section provides additional information regarding the architec­ture s antialiasing, 
texture mapping, stereo, and clipping capabili­ties. 3.1 Antialiasing The architecture supports two fundamentally 
different antialiasing techniques: alpha and multisample. Alpha antialiasing of points and lines is common 
to second generation architectures. Alpha antialiasing is implemented using subpixel and line-slope indexed 
tables to generate appropriate coverage values for points and lines, compensating for the subpixel position 
of line endpoints. Polygon coverage values are computed by counting the 1 s in the full pre­cision coverage 
mask. The fragment alpha value is scaled by 8 ( 8 the fractional coverage value, which varies from 0.0, 
indicating no coverage, to 1.0, indicating complete coverage. If pixel blending is enabled, fragments 
are blended directly into the color buffer no subpixelsamplelocationsareaccessedorrequired. Alphaantialias­ing 
results in higher quality points and lines than does multisample antialiasing, because the resolution 
of the .lter tables is greater than the 4 bit equivalent of the 16-sample mask. While alpha antialiased 
primitives should be rendered back-to-front or front-to-back (de­pending on the blend function being 
used) to generate a correct image, it is often possible to get an acceptable point or line image without 
such sorting. Alpha antialiased polygons, however, must be sorted near to far to get an acceptable image. 
Thus this technique is ef.ciently applied to polygons only in 2D scenes, such as instru­ment panels, 
where primitive ordering is .xed and a slight increase in quality is desired. Multisample antialiasing 
has already been described. Its princi­pal advantage over alpha antialiasing is its order invariance 
-points, lines, and polygonscan be drawn into a multisample buffer in any or­der to produce the same 
.nal image. Two different mask generation techniques are supported in multisample mode, each with its 
own advantages and disadvantages. The default mask generation mode is called point sampled; the alternate 
mode is area sampled. A point sampled mask is geometrically accurate, meaning that each mask bit is set 
if and only if its subpixel location is within the perimeter of the point, line, or polygon outline. 
(Samples on the primitive s edge are included in exactly one of the two adjacent primitives.) Such masks 
insure the correctness of the .nal image, at the expense of its .ltered quality. The .nal image is correct 
because all the samples that comprise it are geometrically valid -none having been taken outside their 
corresponding primitives. It is poorly sampled because the number of bits set in the mask may not closely 
correspond to the actual area of the pixel that is covered by the primitive, and the .nal .ltering quality 
depends on this correspondence. Area sampling attempts to insure that the number of 1 s in the sample 
mask is correct plus or minus 1/2 a sample, based on the actual coverage of pixel area by the primitive. 
(Figure 5.) In order to accomplish this, area sampled masks necessarily include samples that are outside 
the primitive outline, resulting in image artifacts such as polygon protrusions at silhouettes and T-junctions. 
Area sampled masks are implemented with a technique that is related to the one described by Andreas Schilling[10]. 
Point and area sampling can be selected by the application program on a per-primitive basis. The desirable 
multisample property of order invariance is lost if alpha transparency and pixel blending are used. Alpha 
does sometimes carry signi.cant information, usually as a result of the alpha channel in the texture 
application. For example, trees are The single sample selected by the The three samples selected by 
the point sample method is darkened. area sample method are darkened. Figure 5. A narrow triangle intersected 
with a single, 16-sample pixel. The three samples selected by the area sample method accurately represent 
the fact that almost 20 percent of the pixel is covered by the triangle. often drawn as single polygons, 
using an alpha matte to express their shape. In order to handle alpha transparency without requiring 
pixel blending, the Image Engines have the ability to convert fragment alpha values to pseudo-random 
masks, which are then logically ANDed with the fragment s coverage mask. This method, while not geometrically 
accurate, provides usable antialiasing of texture mattes, and is order invariant. 3.2 Texture Mapping 
In addition to the 2-dimension texture maps described in the archi­tecture section, 1-and 3-dimension 
maps are also supported. The eight million texel memory associated with each Fragment Genera­tor stores 
2D mipmapped images up to , and 3D non­ 256 ( 256 ( 641024 ( 1024 mipmapped images up to . Thus 3D textures 
can be used to render volumetric images of substantial resolution, at rates up to 30 frames per second. 
The S, T, and R texture coordinates of each fragment are computed by interpolating S/W, T/W, R/W, and 
1/W, then doing the correct divisions at each pixel, resulting in perspective-corrected mapping. Level-of-detail 
is also computed for each pixel, based on the worst-case of the four pixel-to-texel X and Y ratios. Linear 
.ltering of the nearest texels and mipmap levels is sup­ported for 1D, 2D, and 3D textures, blending 
a total of 16 texel colors in the 3D mode. In the 2D case such linear .ltering is com­monly known as 
trilinear. Bicubic interpolation is supported for 2D, nonmipmapped textures, again blending 16 texels. 
There is no sup­port for cubic .ltering of 1D or 3D textures, or of any mipmapped textures. The default 
16-bit texel size supports RGBA texels at 4­bits per component, RGB texels at 5-bits per component (6 
bits for green), intensity-alpha texels at 8-bits per component, and intensity texels at 12-bits per 
component. 32-bit and 48-bit texels can be speci.ed by the application with proportional loss of performance. 
The maximum RBGA texel resolution is 12-bits per component, equal to the maximum framebuffer color resolution. 
Texture magni.cation can be done by extrapolation of mipmap levels, resulting in a sharpening of the 
highest resolution mipmap image, or the highest resolution image can be blended with a repli­cated detail 
image, greatly increasing the apparent res­256 ( 256 olution of the texture without requiring excessive 
texture storage. Filter functions for RGB and for alpha can be speci.ed separately to improve the quality 
of texture mattes. Finally, texture memory can be loaded from the application processor s memory at the 
rate of 80 million 16-bit texels per second, allowing the application to treat texture memory as a managed 
cache of images. 3.3 Stereo in a Window Image Engine memory can be con.gured with separate left and 
right color buffers for both the visible and nonvisible displayable color buffers, resulting in a total 
of four 48-bit color buffers per pixel. The display hardware alternately displays the left and right 
buffer con­tents of the visible buffers of all windows so con.gured, and drives a sync signal that can 
be used to control screen or head-mounted shutters. This stereo-in-a-window capability is both formally 
and practically compatible with the X protocol: formally because neither framebuffer dimensions nor pixel 
aspect ratio are changed when it is enabled or disabled, and practically because it allows monoscopic 
windows such as menus to be rendered and displayed correctly. To reduce eye fatigue, it is advisable 
to select a reduced-dimension framebuffer when the window system is initialized, allowing the frame display 
rate to be increased to 90+ Hz within the 140 MHz pixel limit of the display board. 3.4 Fast Clipping 
RealityEngine polygon clipping is faster than that of our earlier designs for two fundamental reasons: 
it is implemented more ef.­ciently, and it is required less often. Higher ef.ciency results from the 
MIMD Geometry Engine architecture. Because each of the en­gines executes an independent code sequence, 
and because each has signi.cant input and output FIFOs, random clipping delays affect only a single engine 
and are averaged statistically across all the en­gines. Also,becauseeachGeometryEnginecomprisesonlyasingle 
processor, all of that engine s processing power can be devoted to the clipping process. SIMD architectures 
are less ef.cient because all processors are slowed when a single processor must clip a poly­gon. Pipelines 
of processors, and even MIMD arrangements of shortpipelines, arelessef.cient becauseonly afraction ofavailable 
processing power is available to the clipping process. The requirement for clipping is reduced through 
a technique we call scissoring. Near and far plane clipping are done as usual, but the left, right, bottom, 
and top frustum edges are moved well away from the speci.ed frustum, and all triangles that fall within 
the expanded frustum are projected to extended window coordinates. If culling is done by the application, 
almost no triangles will actually intersect the sides of the expanded frustum. Projected triangles that 
are not fully within the viewport are then scissored to match the edges of the viewport, eliminating 
the portions that are not within the viewport. The Pineda rasterization algorithm that is employed easily 
and ef.ciently handles the additional rectilinear edges that result, and no fragment generation performance 
is lost on scissored regions.  4 Design Alternatives We think that the most interesting part of design 
is the alternatives considered, and the reasons for choices, rather than the details of the result. This 
section highlights some of these alternatives, in roughly decreasing order of signi.cance. 4.1 Single-pass 
Antialiasing Multi-pass accumulation buffer antialiasing using an accumulation buffer [3] is order invariant, 
and produces high-quality images in 10 to 20 passes. Further, a system that was fast enough to render 
10 to 20 full scene images per frame would be a fantastic generator of aliased images. So why design 
a complex, multisample frame­buffer to accomplish the same thing in one pass? The answer is that signi.cantly 
more hardware would be required to implement a multi-pass machine with equivalent performance. This is 
true not only because the multi-pass machine must traverse and transform the object coordinates each 
pass, but in particular because texture mapping would also be performed for each pass. The component 
costs for traversal, transformation, parameter interpolation, and tex­ture mapping constitute well over 
half of the multisample machine cost, and they are not replicated in the multisample architecture. A 
competing multi-pass architecture would have to replicate this hard­ware in some manner to achieve the 
required performance. Even the PixelFlow architecture[6], which avoids repeated traversal and transformation 
by buffering intermediate results, must still rasterize and texture map repeatedly. 4.2 Multisample 
Antialiasing Multisample antialiasing is a rather brute-force technique for achieving order invariant 
single-pass antialiasing. We investi­gated alternative sorting buffer techniquesderived from the A-buffer 
algorithm[2], hoping for higher .lter quality and correct, single-pass transparency. These techniques 
were rejected for several reasons. First, sort buffers are inherently more complex than the multisam­ple 
buffer and, with .nite storage allocations per pixel, they may fail in undesirable ways. Second, any 
solution that is less exact than multisampling with point sampled mask generation will ad­mit rendering 
errors such as polygon protrusions at silhouettes and T-junctions. Finally, the multisample algorithm 
matches the single­sample algorithm closely, allowing OpenGL pixel techniques such as stencil, alpha 
test, and depth test to work identically in single or multisample mode. 4.3 Immediate Resolution of 
Multisample Color Our initial expectation was that rendering would update only the multisample color 
and depth values, requiring a subsequent res­olution pass to reduce these values to the single color 
values for display. The computational expense of visiting all the pixels in the framebuffer is high, 
however, and the resolution pass damaged the software model, because OpenGL has no explicit scene demarca­tions. 
Immediate resolution became much more desirable when we realized that the single most common resolution 
case, where the fragment completely replaces the pixel s contents (i.e. the fragment mask is all ones 
and all depth comparisons pass) could be imple­mented by simply writing the fragment color to the color 
buffer, making no change to the 4, 8, or 16 subsample colors, and spe­cially tagging the pixel. Only 
if the pixel is subsequently partially covered by a fragment is the color in the color buffer copied 
to the appropriate subsample color locations. This technique increases the performance in the typical 
rendering case and eliminates the need for a resolution pass. 4.4 Triangle Bus All graphics architectures 
that implement parallel primitive pro­cessing and parallel fragment/pixel processing must also implement 
a crossbar somewhere between the geometry processors and the framebuffer[5]. While many of the issues 
concerning the placement of this crossbar are beyond the scope of this paper, we will men­tion some of 
the considerations that resulted in our Triangle Bus architecture. The RealityEngine Triangle Bus is 
a crossbar between the Geometry Engines and the Fragment Generators. Described in RealityEngine terms, 
architectures such as the Evans &#38; Suther­land Freedom SeriesTM implement Geometry Engines and Fragment 
Generators in pairs, then switch the resulting fragments to the ap­propriate Image Engines using a fragment 
crossbar network. Such architectures have an advantage in fragment generation ef.ciency, due both to 
the improved locality of the fragments and to only one Fragment Generator being initialized per primitive. 
They suffer in comparison, however, for several reasons. First, transformation and fragment generation 
rates are linked, eliminating the possibil­ity of tuning a machine for unbalanced rendering requirements 
by adding transformation or rasterization processors. Second, ultimate .ll rate is limited by the fragment 
bandwidth, rather than the prim­itive bandwidth. For all but the smallest triangles the quantity of data 
generated by rasterization is much greater than that required for geometric speci.cation, so this is 
a signi.cant bottleneck. (See Appendix 2.) Finally, if primitives must be rendered in the order that 
they are speci.ed, load balancing is almost impossible, because the number of fragments generated by 
a primitive varies by many orders of magnitude, and cannot be predicted prior to processor assignment. 
Both OpenGL and the core X renderer require such ordered rendering. The PixelFlow[6] architecture also 
pairs Geometry Engines and Fragment Generators,but the equivalent of Image Engines and mem­ory for a 
pixel tile are also bundled with each Geome­ 128128 ( 128 ( 128 try/Fragment pair. The crossbar in this 
architecture is the composit­ing tree that funnels the contents of rasterized tiles to a .nal display 
buffer. Because the framebuffer associated with each processor is smaller than the .nal display buffer, 
the .nal image is assembled as asequenceof logicaltiles. Ef.cientoperationisachieved only when each logical 
tile is rasterized once in its entirety, rather than being revisited when additional primitives are transformed. 
To insure that all primitives that correspond to a logical tile are known, all primitives must be transformed 
and sorted before rasterization can begin. This substantially increases the system s latency, and requires 
that the rendering software support the notion of frame de­marcation. Neither the core X renderer nor 
OpenGL support this notion. 4.5 12-bit Color Color component resolution was increased from the usual 
8 bits to 12 bits for two reasons. First, the RealityEngine framebuffer stores color components in linear, 
rather than gamma-corrected, format. When 8-bit linear intensities are gamma corrected,single bit changes 
at low intensities are discernible, resulting in visible banding. The combination of 12-to-10 bit dithering 
and 10-bit gamma lookup ta­bles used at display time eliminates visible banding. Second, it is intended 
that images be computed, rather than just stored, in the RealityEngine framebuffer. Volume rendering 
using 3D textures, for example, requires back-to-front composition of multiple slices through the data 
set. If the framebuffer resolution is just suf.cient to displayanacceptableimage,repeatedcompositionswill 
degradethe resolution visibly. The 12-bit components allow substantial frame­buffer composition to take 
place before artifacts become visible.  Conclusion The RealityEngine system was designed as a high-end 
workstation graphics accelerator with special abilities in image generation and image processing. This 
paper has described its architecture and capabilities in the realm of image generation: 20 to 60 Hz anima­tions 
of full-screen, fully-textured, antialiased scenes. (Figures 6 and 7.) The image processing capabilities 
of the architecture have not been described at all; they include convolution, color space conversion, 
table lookup, histogramming, and a variety of warping and mapping operations using the texture mapping 
hardware. Fu­ture developments will investigate additional advanced rendering features, while continually 
reducing the cost of high-performance, high-quality graphics. Acknowledgments It was a privilege to 
be a part of the team that created RealityEngine. While many team members made important contributions 
to the de­sign, I especially acknowledge Mark Leather for developing the mul­tisample antialiasing technique 
that was eventually adopted, and for designing a remarkable integrated circuit (the Image Engine) that 
implemented his design. Also, special thanks to Doug Voorhies, who read and carefully marked up several 
drafts of this paper, Fi­nally, thanks to John Montrym, Dan Baum, Rolf van Widenfelt, and the anonymous 
reviewers for their clari.cations and insights. Appendix 1: Measured Performance The two most signi.cant 
performance categories are transform rate: the number of primitives per second that can be processedby 
the Ge­ometry Engines, and .ll rate: the number of fragments per second that can be generated and merged 
into the framebuffer. Running in third-generation mode (lighting, smooth shading, depth buffering, texturing 
and multisample antialiasing) a 12 Geometry Engine sys­tem can process 1.5 million points, 0.7 million 
connected lines, and 1.0 million connected triangles per second. In second-generation mode (lighting, 
smooth shading, and depth buffering) the same sys­tem can process 2.0 million points, 1.3 million connected 
lines, and 1.2 million connected triangles per second. Measured third­generation .ll rates for 2 and 
4 raster board systems are 120 and 240 million fragments per second. Measured second-generation .ll rates 
for 1, 2, and 4 raster board systems are 85, 180, and 360 million fragments per second. The third-generation 
.ll rate num­bers are somewhat dependent on rendering order, and are therefore chosen as averages over 
a range of actual performances. Appendix 2: Bandwidth and other Statistics Triangle Bus, fragment transfer 
path, and Image Engine to frame­buffer memory bandwidths are in roughly the ratios of 1:10:20. Speci.c 
numbers for the typical two raster board con.guration are 240 Mbyte/sec on the Triangle Bus, 3,200 Mbyte/sec 
aggregate on the 160 Fragment Generator to Image Engine busses, and 6,400 Mbyte/sec aggregate on the 
160 Image Engine to framebuffer con­nections. Because the 6,400 Mbyte/sec framebuffer bandwidth is so 
much larger than the bandwidth required to refresh a monitor (roughly 800 Mbyte/sec at Hz) we implement 
the framebuffer 1280 ( 1024 ( 76 memory with dynamic RAM rather than video RAM, accepting the 12 percent 
.ll rate degradation in favor of the lower cost of com­modity memory. Geometry Engine memory and texture 
memory are also implemented with commodity, 16-bit data path dynamic RAM. Total dynamic memory in the 
maximally con.gured system is just over 1/2 Gigabyte. References [1] AKELEY, KURT AND TOM JERMOLUK. High-Performance 
Poly­ gon Rendering. In Proceedings of SIGGRAPH 88 (August 1988), pp. 239 246. [2] CARPENTER, LOREN. 
The A-buffer, An Antialiased Hidden Surface Method. In Proceedings of SIGGRAPH 84 (July 1984), pp. 103 
108. [3] HAEBERLI, PAUL AND KURT AKELEY. The Accumulation Buffer: Hardware Support for High-Quality Rendering. 
In Proceedings of SIGGRAPH 90 (August 1990), pp. 309 318. [4] KIRK, DAVID AND DOUGLAS VOORHIES. The Rendering 
Ar­chitecture of the DN10000VS. In Proceedings of SIGGRAPH 90 (August 1990), pp. 299 308. [5] MOLNAR, 
STEVEN. Image-Composition Architectures for Real-Time Image Generation. University of North Carolina 
at Chapel Hill, Chapel Hill, NC, 1991. [6] MOLNAR, STEVEN, JOHN EYLES AND JOHN POULTON. Pix­elFlow: High-Speed 
Rendering Using Image Composition. In Proceedings of SIGGRAPH 92 (July 1992), pp. 231 240. [7] NEIDER, 
JACQUELINE, MASON WOO AND TOM DAVIS. OpenGL Programming Guide. Addison Wesley, 1993. [8] OPENGL ARCHITECTURE 
REVIEW BOARD. OpenGL Reference Manual. Addison Wesley, 1992. [9] PINEDA, JUAN. A Parallel Algorithm for 
Polygon Rasteri­zation. In Proceedings of SIGGRAPH 88 (August 1988), pp. 17 20. [10] SCHILLING, ANDREAS. 
A New Simple and Ef.cient Antialias­ing with Subpixel Masks. In Proceedings of SIGGRAPH 91 (July 1991), 
pp. 133 141. [11] SILICON GRAPHICS, INC. Iris 4DGT Technical Report. Silicon Graphics, Inc., Mountain 
View, CA, 1988. [12] SILICON GRAPHICS, INC. Technical Report -Power Series . Silicon Graphics, Inc., 
Mountain View, CA, 1990. [13] WILLIAMS, LANCE. Pyramidal Parametrics. In Proceedings of SIGGRAPH 83 (July 
1983), pp. 1 11. RealityEngine and OpenGL are trademarks of Silicon Graphics, Inc. Freedom Series is 
a trademark of Evans &#38; Sutherland Computer Corporation. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166132</article_id>
		<sort_key>117</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[VIEW]]></title>
		<subtitle><![CDATA[an exploratory molecular visualization system with user-definable interaction sequences]]></subtitle>
		<page_from>117</page_from>
		<page_to>126</page_to>
		<doi_number>10.1145/166117.166132</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166132</url>
		<keywords>
			<kw><![CDATA[data-constrained sketching]]></kw>
			<kw><![CDATA[graphical debugging]]></kw>
			<kw><![CDATA[molecular graphics]]></kw>
			<kw><![CDATA[scientific visualization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>D.2.6</cat_node>
				<descriptor>Interactive environments</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Biology and genetics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.2.2</cat_node>
				<descriptor>User interfaces</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.3.2</cat_node>
				<descriptor>Extensible languages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.3.2</cat_node>
				<descriptor>Specialized application languages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010095</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Systems biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010087</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Computational biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010935</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Genetics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011066.10011069</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Development frameworks and environments->Integrated and visual development environments</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011023</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Specialized application languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011009.10011019</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language types->Extensible languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011066</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Development frameworks and environments</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31070231</person_id>
				<author_profile_id><![CDATA[81100245096]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lawrence]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Bergman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P134644</person_id>
				<author_profile_id><![CDATA[81332523548]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jane]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Richardson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14035191</person_id>
				<author_profile_id><![CDATA[81332523393]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Richardson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14037393</person_id>
				<author_profile_id><![CDATA[81100077256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Frederick]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Brooks]]></last_name>
				<suffix><![CDATA[Jr.]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[B.H. McCormick, T.A. DeFanti, and M.D. Brown, eds., "Visualization in Scientific Computing," Computer Graphics, Vol. 21, No. 6, Nov. 1987.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[M. Pique, J.S. Richardson, and F.P. Brooks, Jr., "What Does a Protein Look Like?" Invited videotape presented at 1982 SIGGRAPH Conference, July 1982.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[J.S. Richardson et al, "Looking at Proteins: Representations, Folding, Packing, and Design,"Biophys.J., Vol. 63, Nov. 1992, pp. 1186-1209.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bergman, et al, "VIEW- Visualization Impromptu Evaluation Workbench," abstract in J. Mol. Graphics, Vol. 6, Dec. 1988, pp. 223.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617498</ref_obj_id>
				<ref_obj_pid>616006</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[C. Upson et al, "The Application Visualization System: A Computational Environment for Scientific Visualization,"IEEE Computer Graphics &amp; Applications, Vol. 9, No. 4, July 1989, pp. 30-42.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617576</ref_obj_id>
				<ref_obj_pid>616012</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[D.S. Dyer, "A Dataflow Toolkit for Visualization," IEEE Computer Graphics &amp; Applications, Vol. 10, No. 4, July 1990, pp. 60-69.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[IRIS Explorer (TM) User's Guide, Document Number 007- 1371-010, Silicon Graphics, Inc., Mountain View, CA, Jan. 1992.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949708</ref_obj_id>
				<ref_obj_pid>949685</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[B. Lucas et al, "An Architecture for a Scientific Visualization System," Proc. Visualization '92 (Oct. 1992), pp. 107-114.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617748</ref_obj_id>
				<ref_obj_pid>616023</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[T.C. Palmer, "A Language for Molecular Visualization," IEEE Computer Graphics &amp; Applications, Vol. 12, No. 3, May 1992, pp. 23-32.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949729</ref_obj_id>
				<ref_obj_pid>949685</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[J.P. Hultquist and E.L. Raible, "SuperGlue: A Programming Environment for Scientific Visualization," Proc. Visualization '92 (Oct. 1992), pp. 243-251.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949713</ref_obj_id>
				<ref_obj_pid>949685</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[W. Hibbert, C.R. Dyer, and B. Paul, "Display of Scientific Data Structures for Algorithm Visualization," Proc. Visualization 92 (Oct. 1992), pp. 139-146.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806799</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[T.J. O'Donnell and A.J. Olson, "Gramps- A Graphics Language Interpreter for Real-Time Interactive Three-Dimensional Picture Editing and Animation," Computer Graphics (Proceedings ofSIGGRAPH 1981), Vol. 15, No. 3, Aug. 1981, pp. 133- 142.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[M.C. Connolly and A.J. Olson, "Granny, a Companion to Gramps for the Real-Time Manipulation of Macromolecular Models." Computers and Chemistly, Vol. 9, No. 1, 1985, pp. 1-6.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[D.C. Richardson and J.S. Richardson, "The Kinemage: A Tool for Scientific Communication," Protein Science, Vol. 1, 1992, pp. 3-9.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 VIEW An Exploratory Molecular Visualization System with User-Definable Interaction Sequences Lawrence 
D. Bergman*, Jane S. Richardson , David C. Richardson , and Frederick P. Brooks, Jr.* * GRIP Molecular 
Graphics Research Resource Department of Computer Science University of North Carolina at Chapel Hill 
 Department of Biochemistry Duke University ABSTRACT VIEW is an exploratory visualization system for 
studying the struc­tures of molecules. The system supports a high degree of complex user interaction 
with the image. Visualizations are constructed by selecting drawing tools from a library. Each tool uses 
parameters obtained from interactive selection of on-screen geometry by the user, and from a molecular 
database. The system is based on a tight coupling of on-screen geometry with the underlying database. 
Using these links, tools can create true­scale drawing elements that are constrained to database values. 
VIEW is highly extensible by the user or a paraprogrammer associ­ated with the user. Drawing tools are 
written in a C-like program­ming language with constructs for managing databases, constructs for creating 
and altering geometry, as well as standard statements such as If-Else and For loops. An event-definition 
mechanism allows the user to describe actions to be performed when keys are depressed or dials turned. 
In addition, the user is able to specify conditional events actions that are to be taken whenever a 
user-defined condition becomes true. These conditions are automatically evaluated by the system as part 
of event processing. Such conditional events allow simple simulations to be readily pro­grammed. Applications 
of conditional events have included anima­tions of protein binding activity, and an interactive flashlight 
which highlights structures as a cursor is steered through a molecule. The system includes a development 
environment complete with a WYSIWYG editor, an interactive debugger, and a set of innovative graphical 
debugging features. * CB 3175, UNC, Chapel Hill NC 27599-3175. (919) 962-1932 bergman@cs.unc.edu (919) 
962-1931 brooks@cs.unc.edu  Department of Biochemistry, Duke University 27710 (919) 684-6010 jsr@suna.biochem.duke.edu 
dcr@suna.biochem.duke.edu  Permission to copy without fee all or part of this material is granted provided 
that the copies are not made or distributed for direct provided that the copies are not made or distributed 
for direct commercial advantage, the ACM copyright notice and the title of the commercial advantage, 
the ACM copyright notice and the title of the publication and its date appear, and notice is given that 
copying is by publication and its date appear, and notice is given that copying is by permission of the 
Association for Computing Machinery. To copy permission of the Association for Computing Machinery. To 
copy otherwise, or to republish, requires a fee and/or specific permission. otherwise, or to republish, 
requires a fee and/or specific permission. &#38;#169;1993 ACM-0-89791-601-8/93/008/0015 $1.50 &#38;#169;1993 
ACM-0-89791-601-8/93/008 $1.50 VIEW has been installed for over a year in a protein crystallography laboratory 
at Duke University. Graduate students and faculty have used the system both for exploring molecular structures 
and for producing presentation graphics. These users have developed their own set of tools and made extensive 
use of the tool library. In January 1993, a beta-version of the software was released to a small set 
of laboratories in the US and Europe. It is now generally available. CR Categories and Subject Descriptors: 
D.2.2 [Software Engi­neering]: Tools and Techniques Programmer workbench, Soft­ware libraries, User 
interfaces; D.2.6 [Software Engineering]: Programming Environments Interactive; D.3.2 [Software Engi­neering]: 
Language Classifications Design languages, Extensible languages, Specialized application languages; 
I.3.6 [Computer Graphics]: Methodology and Techniques Interaction techniques; I.3.8 [Computer Graphics]: 
Applications; J.3 [Computer Appli­cations]: Life and Medical Sciences Biology. Additional keywords: 
scientific visualization, graphical debug­ging, molecular graphics, data-constrained sketching.  MOTIVATION 
Visualization is powerful. Over the past few years, scientific visualization has received a great deal 
of attention and is widely acknowledged as an important tool for the exploration of scientific data. 
Through visualization, a scientist assimilates large quantities of data, and may acquire new insights 
[1]. The visualization design space is large. Different representations of a dataset highlight and reveal 
different properties (see [2] for an excellent example). The number of possible exploratory visualiza­tions 
of any dataset is limitless. Some of them will reveal or emphasize certain properties of the data; others 
will reveal or emphasize other properties; most will be uninformative. For this reason, a great deal 
of guidance by the scientist is usually required in constructing useful visualizations. Creating new 
geometric representations is important but difficult. Graphical representations consist of a display 
of geometry with associated surface attributes such as color and texture. The shapes used to represent 
data entities and their relative position, size, and orientation tell us a great deal they often contain 
most of the information in an image. Designing new geometry is difficult. The user must specify the algorithm 
used to convert database information into geometric parameters. Using existing visualization systems, 
the user writes new code, usually in C, for each new geometric representation. Little support is provided 
for this code development. The user usually works outside the boundary of the visualization system, employing 
a cumbersome code-compile-link-test-recode cycle. The problem. The problem addressed in this work is: 
How can we facilitate the design of new visual representations of scientific data, particularly new forms 
of geometry? The design process. A system for visualization should be based on the process by which a 
scientist or programmer designs a new visualization: Design usually starts with some sketching, done 
outside the confines of any visualization system what we call the paper napkin stage.  Design is an 
iterative process the user repeatedly tries new approaches and gradually refines her notion of what 
is needed to understand or emphasize the data. Most of the tries are unsuccessful.  When a satisfactory 
sketch is achieved, a scale drawing reflect­ing the actual data is made.  Visual feedback is crucial 
in guiding the design process. The user usually determines what she wants based on what she sees; she 
rarely knows exactly what she wants when she begins.  New designs usually start with an existing design. 
Simple and aggregate elements from one design are often reused in a series of designs.  Examination 
of the design process tells us that the user needs a sketching facility, and an easy way to get from 
sketch to scale drawing. The ability to interact with a partially complete image, to try and discard 
a number of alternatives, is critical. Users must also be able to customize the tools or craft their 
own. Lack of design-process-based systems. Existing visualization systems fall into two categories, neither 
of which have heretofore tried to support the design process as described. Application-specific software 
systems (such as commercial molecular modeling pack­ages, geographic information systems, or flow visualization 
sys­tems) often provide for interactive design, but the toolkit is fixed and often small. General-purpose 
visualization systems (such as AVS or Explorer) provide for user-specification of a visualization in 
a highly interactive fashion, but do not allow the user to directly interact with the visualization itself. 
 THE VIEW SOLUTION A design-process-based exploratory system. VIEW is a molecular visualization system 
designed to provide an exploratory environ­ment. The goals of VIEW are to bring the napkin on-line, to 
support an iterative design process, to provide immediate visual feedback, to allow user-extension of 
the design tools, and to promote reuse of the design components. The data-drawing model. Sketching for 
visualization is distin­guished from that for most other forms of design in one important respect the 
visualization represents an underlying database. We want geometric parameters of the visualization, primarily 
positions, to be specified using information from the database. Unlike a free­hand sketch, which can 
only portray the topology of a form, a database-driven sketch may be constrained to data values, providing 
a true-scale representation of the geometry. The VIEW system supplies a method for interactive visualization 
design that we call data-drawing. With data-drawing, the user specifies database parameters by selecting 
geometric objects on­screen. These geometric objects serve as stand-ins for records in the database. 
Design tools that incorporate the data-drawing method operate as follows: 1) The user picks a graphical 
element. 2) The tool retrieves data associated with that element and addi­tional related data. 3) The 
tool creates new geometry based on the database informa­tion and associates that information with the 
geometry. This new geometry is in turn available as a visual template for future data-drawing. With the 
language that specifies drawing tools, the user associates database records with on-screen geometry and 
retrieves this informa­tion from picks. This uniform philosophy of tool design and use incorporates two 
desirable features: As the user constructs geometries, the new forms are available for data-drawing. 
 The tool-user may select any form of geometry that represents  the desired database element. She is 
not restricted to a small set of special representations. Use of VIEW. A user of the VIEW system starts 
a session with a simple representation of a molecule. An initial tool creates geometry from molecular 
data such as Brookhaven Protein Databank atomic coordinates. Often vectors are chosen to represent atomic 
bonds. Using a variety of drawing tools from a library provided, the user sketches in additional geometry. 
For example, a user may sketch: individual amino acids with the bonds represented as small cylinders, 
larger cylinders that represent the axes of helices within a protein, a spline-like representation of 
a portion of the backbone, or any of a number of other representations. Interaction with the image is 
crucial. We recognize that each user has preferred interaction styles. For this reason, the user may 
customize interaction sequences. The drawing tool language pro­vides a facility known as interactive 
events-monitors for defining actions that are to be performed based on mouse movement, dial movement, 
and key presses. Interactive sequences that have been coded to date include: moving a small molecule 
with the mouse, changing sphere and cylinder radii using a dial, moving atoms using dials while maintaining 
bond connections, and triggering actions on key depressions. Geometry and data closely linked. Central 
to the data-drawing model is a tight coupling of on-screen geometry with the underlying data. This allows 
on-screen picks to return database records directly to the drawing routines. Design of new drawing tools. 
The VIEW user can code new drawing tools which become members of the library. This ability to design 
new tools is an important feature in an impromptu visualiza­tion system. Tool development environment. 
VIEW supports the user in extending the toolkit by providing a development environment that includes 
a Macintosh-like text editor and a visual debugger that interacts with the on-screen image.  AN EXAMPLE 
Most of the visualizations produced using VIEW have displayed protein molecules. Proteins consist of 
a linear chain of amino acid residues which fold into a few well-defined 3-D structures such as beta 
sheets and alpha helices. These in turn form larger motifs such as beta barrels. A protein has a mainchain 
consisting of carbon, oxygen, and nitrogen atoms. Extending from the mainchain are sidechains. Each amino 
acid type, of which there are approximately twenty, has a distinctive sidechain. Hydrogen bond connections 
between non-sequential amino acids define the topology of the protein. Jane Richardson produced the visualization 
of the protein Con­canavalin A shown in Figure 1 using VIEW drawing tools. A variation of this image 
appeared in Biophysical Journal [3]. The image shows the orientation of a phenylalanine amino acid and 
two possible but less favorable orientations. The steps in constructing the visualization were: 1) Richardson 
selected a tool that creates initial geometry starting with atomic coordinates. The tool produces vectors 
connect­ing just the alpha carbons of adjacent amino acids (Figure 2a). This representation gives a clear 
global view of the structure. 2) Using a mainchain drawing tool, she sketched in atomic-level detail 
for three strands of the chain. She specified starting and ending points by picking atom positions in 
the original repre­sentation. On each pick, the tool drew a small red sphere to mark the selection. After 
both ends were selected, the tool drew the connecting main chain at the atomic level automati­cally, 
using atom coordinates fetched from the molecular data. After drawing the main chain, the tool removed 
the marker spheres. Figure 2b shows the drawing after specification of the second strand, just before 
the system removed the red markers. Only one tool selection and four datapoint selections were required 
to produce this detailed scale drawing.  2a 2b Figure 1: A VIEW visualization 3) Using a line drawing 
tool, she sketched in hydrogen bonds that couple the strands together. The particular tool employed knows 
nothing about hydrogen bonds; Richardson selected the termini of each bond. We could have written a new 
tool to automatically draw in all hydrogen bonds for the molecule. Since we wanted only a small, selected 
set, manual specifica­tion of each bond seemed reasonable. The line drawing tool bases each drawing operation 
on two atom selections. This time, the selections were made using the geometry produced in step 2; display 
of the original represen­tation was toggled off. VIEW users often turn off individual groups of geometry 
by pressing virtual buttons in the interface. Users frequently switch between sparse global views and 
detailed local views. 2c  2d 2e2f Figure 2. Construction of a visualization using VIEW drawing tools 
With the line drawing tool, the user may lengthen or shorten lines that connect the centers of selected 
atoms. By pressing the l key, Richardson triggered an interactive event for defining the length scaling. 
The event popped up a query window requesting a scaling factor. She specified .65 before proceeding with 
the drawing shown in Figure 2c. 4) Next, she sketched a single sidechain using a tool which draws an 
entire sidechain based on one atom selection and informa­tion from the molecular database. Once the bonds 
of the sidechain were drawn, Richardson used a marker-sphere tool to mark the sidechain atoms. Figure 
2d shows the result of a single selection with the sidechain tool and eight selections with the marker-sphere 
tool. 5) In order to produce the two rotated positions of the sidechain, Richardson created a duplicate 
of the marker spheres using a group duplication tool. VIEW places the geometry created by each drawing 
tool in a separate geometry group labeled with the name of the tool that produced it. These groups may 
be individually manipulated by other tools. The duplication tool requested that she select any element 
of geometry from the group to be duplicated, and then queried her for a name for the new group. This 
specification of group by identifying a member is a common theme in VIEW drawing tools. 6) Once the group 
had been duplicated, she used a rotation tool to rotate the duplicate into position. The rotation tool 
requests selection of a rotation axis, followed by requests for selection of one or more geometry groups 
to be rotated. Rotation may then be performed with a dial or by key presses. In this case, Richardson 
wanted to rotate the group by a precise amount. An event triggered by pressing the d key allowed her 
to type in the desired angle (120 degrees). She applied the rotation by pressing the "r" key. Figure 
2e shows the rotated markers. The rotation axis is highlighted in white. 7) Steps 4 and 5 were repeated 
to produce a second duplicate rotated to 240 degrees. 8) In order to generate the open framework of lines 
connecting markers at the rotated positions, Bergman coded a new draw­ing tool that connects a sequence 
of selected positions with wireframe cylinders. The tool was created by merging and modifying two prior 
tools. The first of these tools connected a sequence of positions with solid cylinders using the system 
cylinder primitive. The other produced a tessellated cylinder not using the system cylinder primitive. 
The new tool was developed in under a half-hour, including testing. Figure 2f shows the open cylinders 
being drawn; the last two selected positions are highlighted white. 9) Richardson used a group recoloring 
tool to finalize colors in the image, shown in Figure 1. The sequence of operations described (excluding 
the tool development in step 8) can be carried out in about fifteen minutes. In actuality, Richardson 
spent a couple of hours producing the visualization. A large amount of trial-and-error is required to 
design a useful image. She tried a score of possibilities before settling on the above result, including: 
changing colors, radii, lengths, and number of facets for the wireframe cylinders.  SYSTEM DESCRIPTION 
The VIEW system (Figure 3) is written in C++ and runs on SGI 4D, Indigo, and Crimson workstations. Three-dimensional 
manipulation of geometry is performed using a mouse-based virtual trackball or a dialbox. Drawing tools. 
The toolkit supplied with the VIEW system contains Figure 3: The VIEW system  about fifty interactive 
drawing tools. Several tools are provided to generate initial representations of a database; the majority 
of the tools are used for click-and-draw geometry generation or modification. Drawing operations are 
reversible using a multi-level undo feature. Tool language. Drawing tools are specified in a C-like language 
with most of the standard C datatypes and control structures. Addi­tionally, the language includes geometric 
and database datatypes, constructs for database access and modification, and constructs for selecting 
and manipulating individual geometric elements and groups of geometry. A key feature is the ability to 
associate database records with individual geometric primitives. Picking a geometric primitive will retrieve 
its associated data properties. The drawing tool language was designed for ease of use. Ability to prototype 
quickly was given higher priority than runtime speed. For this reason the language is dynamically typed 
with no type declaration statements. Objects are sized dynamically no size declarations are given for 
arrays, sets, geometry groups or databases. Scope rules are very simple. All variables are global within 
a routine, and they are also global to all event-monitors defined within the routine. Variables are only 
available outside a routine if passed as subroutine parameters. The language is interpreted, allowing 
changes to a tool to be quickly retried. The tools in the library are all coded in the tool language, 
not in C++. The prompts, highlighting, and final geometry created are specified by tool language statements. 
The user can readily change the interaction sequence and any of the intermediate or final repre­sentations 
produced by these tools. Several considerations led to development of a new, interpreted language: 1) 
We wanted a procedural language similar to C or FORTRAN. Scientists are understandably reluctant to invest 
in learning new programming styles. This consideration ruled out popular interpreted languages such as 
LISP and Smalltalk. 2) We wanted to supply special syntax to simplify expression of certain commonly 
used constructs, particularly database and geometry access. This could have been accomplished by supplying 
a subroutine library for a language such as C. However, code developed with such a library will be less 
concise and less readable than if special constructs are available. 3) To simplify coding, we wanted 
to avoid type statements and size declarations. We also wanted to avoid certain constructs such as pointers 
that provide flexibility at the expense of code comprehensibility. 4) We wanted to support interactive 
event definition. 5) We wanted interpretation of the language to be closely inte­grated with debugging 
functions, particularly graphical de­bugging facilities. Close connection between geometry and databases. 
One of the characteristics that most distinguishes VIEW from existing general­purpose visualization systems, 
is the intimate connection between on-screen geometry and an underlying database that the geometry represents. 
This connection is fundamental to on-screen sketching in a representation of the database  the basic 
notion of the data­drawing model. VIEW drawing tools typically associate atom or bond records with each 
element of geometry created. This association allows other tools to use these elements as visual stand-ins 
for database entries; the user may select atoms or bonds by clicking on geometry that represents them. 
Although this is a tried-and-true technique, VIEW is unique in allowing the user to specify the connections; 
she is not tied down to a system-defined schema. The following code fragment presents an example of establishing 
links between a geometric object and database records: cyl = CYLINDER (pnt1, pnt2, radius); (1) cyl.DB_PTR 
= atom_rec; (2)  cyl.DB_PTR = bond_rec; (3) Statement (1) creates a cylinder with the variable name 
cyl. State­ment (2) establishes a connection between the cylinder and the atom record stored in atom_rec. 
Statement (3) assigns an additional database pointer to the cylinder, this time to a bond record. Another 
tool can access the database information associated with this on-screen geometry as follows: SELECT (item, 
Select an object ); (1)  selected_atom_rec = item.atom; (2) Statement (1) is a pick. The user is told 
to select a geometric object on-screen using the mouse. The selected object will be returned in the variable 
item. Statement (2) specifies that the atom record pointed to by that object is to be assigned to the 
variable selected_atom_rec. If the geometric object selected happens to be the cylinder created in the 
previous example, the value of selected_atom_rec will be the record contained in atom_rec. Databases. 
VIEW databases are stored in a non-application specific format. In fact, the only portion of the VIEW 
system that is specific to molecular visualization is certain drawing tools in the library. We convert 
molecular data from Brookhaven Protein Databank format to the more generic VIEW format using a filter 
run outside of VIEW. Databases are stored in ASCII files, each consisting of one or more named subsets. 
Our molecular databases have two subsets an atom subset, and a bond subset. A subset consists of a header 
which describes the record format, followed by a sequence of records. All records in a subset have the 
same number and ordering of fields. Record fields may be integers, floating point numbers, or strings. 
Each record contains a single integer- or string-valued key field used for key-access. Record access 
from a database requires naming the subset to be accessed and the retrieval key. For example: rec = 
dbase.atom(num); will retrieve the record that has atom number num from the atom subset of the database 
dbase. Fields can be retrieved from a record by naming the field. For example: type = rec.atom_type; 
will retrieve the atom_type field from the record rec. These forms may be combined. For example:  type 
= dbase.atom(num).atom_type; A special iterator, FOREACH, allows iteration through a subset s records 
in the order in which they are stored in the file (FOREACH is also used for iterating on arrays, sets, 
and geometry groups). The NEXT_RECORD statement retrieves the record following a given record, allowing 
manual control of record access. Similarly a PREV_RECORD statement allows backward movement through a 
subset. The VIEW drawing tool language allows the user a great deal of flexibility in modifying databases 
including: 1) changing fields, 2) adding or deleting fields or records, and 3) writing and reading databases 
to and from files. Additionally, the user may define her own database formats. Geometry groups. The drawing 
tool language provides construc­tors for geometric objects including spheres, triangles, lines, cylin­ders, 
and text. Geometric objects, created by drawing tools, or read from files are stored in geometry groups. 
Groups allow named access to related sets of geometric objects. The system provides a mecha­nism for 
controlling which groups are displayed on-screen. Other management functions are available including: 
1) removing groups from the system, 2) writing groups to file, and 3) renaming groups. Each geometric 
object is contained in one and only one geometry group. There is no nesting of groups. These properties 
were dictated by two simple design rules. 1) We wanted all geometric objects to be contained in a named 
geometry group. This ensures that display of any object may be turned on and off using a group display 
function in the interface. 2) We wanted to be able to access a group through a geometric object. This 
allows the user to specify a geometry group by selecting a member of that group. The geometry group by 
example model focuses the user s attention on on-screen geometry, not on interface buttons or menus. 
For this reason, we wanted a tool language construct that would query an object: what group are you in? 
To make this construct simple, both syntactically and semantically, we restricted objects to membership 
in a single group. The contents of each geometry group is thus distinct. This property is highly desirable. 
The semantics of group display and group removal are thereby simple and intuitive. If groups are permitted 
to overlap, these semantics become more involved and may be counterintuitive. By default, each drawing 
tool adds geometry to a group that has the name of the tool. If no such group exists, the system automatically 
creates it the first time the tool generates display geometry. In addition, the tool writer may define 
groups with other names in which geometry is to be placed. Tools for duplicating geometry groups and 
merging the contents of geometry groups are provided in the tool library. Interface operations. Another 
design criterion for the language was that tools be able to specify any operation that can be performed 
from the user interface. Statements are available to provide tool control over user interface functions 
such as toggling the display of geometry groups, removing groups, reading and writing databases from 
and to files, etc. Event-monitor definition. The tool language provides a mecha­nism which allows the 
tool creator to specify blocks of code that are to be executed when specified keyboard keys are pressed 
or when dials are rotated. These definitions, known as interactive event­monitors, consist of two portions 
 a monitor, which watches the specified device; and an event body, which is the code to be executed when 
the monitor is triggered. A tool may define a suite of event­monitors on different devices which communicate 
through a com­mon symbol table; the tool builder can readily design a sophisticated interactive interface 
to her tools. The following example specifies a dynamic radius-changing tool. SELECT (obj, Select a geometric 
object to be changed ); (1) EV{ } ENT ( change_radius ; ON DIAL 7) obj.RADIUS = obj.RADIUS * (1 + DIALRATE/50 
); IF (obj.RADIUS < 0.01) obj.RADIUS = 0.01; REDRAW(); (2) (3) (4) (5) Line (1) is a pick specification. 
Line (2) defines an event named change_radius which will be executed whenever dial 7 is rotated. Line 
(3) modifies the selected object s radius, using a system-defined variable, DIALRATE, which contains 
the angular change in dial position, positive for clockwise movement, negative for counter­clockwise. 
Line (4) ensures that the radius remains positive, and line (5) redraws the screen. With this tool, the 
user selects an object and then rotates dial 7 to increase or decrease its radius. The radius will alter 
smoothly as the dial is rotated, because the system continually reexecutes the event body as long as 
the dial state is changing. Although only one drawing tool may execute at a time, the event­monitors 
that it defines persist. Thus, a whole set of event-monitors defined by different tools can be active 
simultaneously, each moni­toring a different device. For example, the rotation tool described above might 
be used in conjunction with a translation tool that translates geometry along a selected axis using a 
different dial. The two dials may be used to rotate and translate concurrently. A panel in the interface 
gives a summary of all currently active event­monitors. Conditional event-monitors. In addition to event-monitors 
that are bound to dials and keys, the system supports conditional event­monitors. The monitor is a conditional 
expression; the event is triggered when the monitor expression is True. The system event manager stores 
a parse tree for each of these expressions, and the interpreter evaluates them on each iteration of the 
inner event loop. When any of the conditionals is True, the event body is executed. Thus, the tool builder 
defines actions to be taken based on certain conditions without coding a polling loop. The conditional 
evaluation slows the system, but it still responds at interactive rates even when several event-monitors 
are defined. A simple conditional event­monitor is shown in the following example. This code segment 
will turn a predefined object red when a probe is within a specified distance. EVENT ( highlight_dist 
; DIST(probe.CENTER, obj.CENTER) < 5.0) (1) { obj.COLOR = COLOR(255,0,0); (2) REDRAW(); STOP_EVENT ( 
highlight_dist ); (3) } Statement (1) is a conditional event-monitor definition. This statement creates 
an event, highlight_dist , which will be triggered whenever the distance between the center of the object 
stored in probe and the object stored in obj is less than 5 units (probe and obj would be defined elsewhere 
in the tool, and additional event-monitors pro­vided for moving probe through the scene). Line (2) sets 
the color of obj to red, and line (3) deactivates the monitor so that the color change is only applied 
once. Spatial search. Frequently we wish a tool to simultaneously monitor a group of 3-D points. When 
any point is near a specified location, an event is to be triggered. This function occurs so often that 
it calls for its own underlying mechanism. Simultaneous monitoring is implemented by means of a spatial 
search function. The function takes as input an array of points to be checked, a probe location and a 
radius. Any points from the check list that fall within the search radius are placed in an output array. 
If the output array contains any points, the function returns True. This mechanism allows the tool to 
trigger events at any of a large number of positions. Programmable undo. The language allows the developer 
to define the scope of the system undo function. The keyword UNDOABLE may be supplied as an argument 
to either the pick function or in an event-monitor header. This keyword indicates that a checkpoint is 
to be created prior to execution of the statement. Whenever undo is clicked, the system restores the 
state of the latest checkpoint. The undo stack stores up to twenty-five checkpoints, allowing the user 
to backup through a number of drawing operations. Tool development environment. The VIEW development 
environ­ment is modeled on that of Smalltalk-80. Code may be modified and executed from editors or pop-up 
debuggers, allowing a rapid code­test-recode cycle. The debugger supports many features of tradi­tional 
interactive debuggers including setting breakpoints, step, next, and print. Graphical debugging. Several 
graphical debugging features are provided that go beyond those provided in Smalltalk or interactive debuggers 
such as dbx. Using a construction facility, the developer automatically views graphical representations 
of intermediate con­struction points and lines as they are created by the code. This graphical auto-print 
makes it easy to follow the progress of algo­rithms that construct geometry. The display function within 
the debugger highlights the representation corresponding to any selected geometric variable. Graphical 
breakpoints are also available. These are similar to the conditional breakpoints provided in interactive 
debuggers such as dbx. Rather than providing an expression that must be True for execution to pause, 
however, the user selects a graphical entity, whose display is the condition on which execution is to 
pause. With this facility, the user selects an object at which the algorithm is to stop. The system will 
pause on reexecution, allowing her to display variable values (graphically or textually) or manually 
control the execution.  SAMPLE APPLICATIONS Interactive superposition. We have developed several interactive 
exploratory applications using VIEW. Kim Gernert, a biochemist at Duke University, has been studying 
the geometry of close atomic contacts within protein molecules. She wished to superimpose similar structures 
from a number of proteins, and then measure geometric parameters from each. Bergman and Gernert prototyped 
the superpositioning procedure using a sequence of tools. We began by sketching an axis in each of two 
structures to be superimposed. We then selected a tool that computes a transformation to superimpose 
two selected axes. The tool applies the transformation to the geometry of the structure to be superimposed. 
Once the two structures were oriented on a common axis, the rotation tool described previously was used 
to rotate one of the structures around that axis. A dial controlled the rotation. Using a translation 
tool, we moved the structure along the common axis under control of a different dial. The rotation and 
translation tools allowed us to manually superimpose the structures. Figure 4 shows the completed superposition. 
  Interactive structure highlighting. Using the conditional event mechanism with spatial search, we 
have constructed a flashlight tool for exploring proteins. Using the mouse, we steered a probe sphere 
through a skeletal representation of a protein. As the probe comes near portions of the molecule, more 
detailed representations are generated. The flashlight has several lenses selected by key toggles. With 
one lens, amino acid sidechains near the probe are dynamically drawn in. With another lens, the mainchain 
is high­lighted. Yet another lens displays pinwheel icons representing close contacts between neighboring 
atoms. Figure 5 shows a protein with sections of the molecule traced by the flashlight. Algorithm construction 
and visualization. VIEW has proven to be quite useful for visualizing geometric algorithms. We used a 
set of 3-D ruler-and-compass construction tools to develop a parameteriza­tion of the helix contact geometry 
discussed above. Using four tools: project a point onto a line , construct a plane normal to a line through 
a given point , project a point onto a plane , and connect two points , we were able to construct the 
geometry in Figure 6. The figure displays in a plane the angles that we decided to use for the study. 
 VIEW has also allowed us to visualize the workings of existing algorithms. Figure 7a shows the technique 
used for constructing axes of alpha helices in proteins (shown in Figure 7b). The red spheres mark user-selected 
atoms. The yellow spheres mark positions obtained from the database. The blue spheres mark positions 
computed by the tool. Display statements to generate the construc­tion spheres and cylinders that illustrate 
the algorithm were added to the already working tool in about 15 minutes. Interactive topology tracing. 
Several educational applications have been constructed using VIEW. Figure 8 shows a tool that is used 
to interactively outline the topology of protein backbone. With a sequence of events on keyboard keys, 
the user guides a cursor along the backbone, indicating where segments of interest begin and end (Figure 
8a). As each segment is identified, a simplified representa­tion replaces the backbone. Once all segments 
are identified, an event is available to specify ordering and orientation of the segments (Figure 8b). 
Finally, the tool flattens the connected segments into a map of the chain topology (Figure 8c). Interactive 
simulation of binding activity. Another educational application is simulation of the binding activity 
of enzymes. Several interactive applications have been constructed that allow a student to steer a small 
molecule into a protein s active site, triggering an animated conformational change. Binding of a dipeptide 
in the active site of the protein carboxypepsidase (Figure 9) requires that the dipeptide be close to 
the ideal position, and oriented properly. The simulation is implemented using a conditional event-monitor 
that checks the distance between the dipeptide and the binding site and also evaluates orientation by 
checking two dot products. When the distance and both dot products are within specified limits, the event 
is triggered. The dipeptide is rotated and translated into the exact binding alignment, while the 7a 
7b Figure 7: Display of helix axis tool algorithm conformational change is animated by displaying a series 
of precom­puted frames. The dipeptide may be driven in and out of the binding sites with alternate conformational 
changes. RELATED WORK The vision driving development of the VIEW system was first described by Brooks 
[4]. General purpose visualization systems (such as AVS [5], ApE [6], Explorer [7], Data explorer [8]) 
allow the user to configure their own applications using a data-flow programming model. These systems 
tend towards a batch visualization pipeline the user s ability to interact with the image (beyond viewing 
manipulations) is limited. VIEW extends the capability of these systems by adding a high degree of interaction 
with the image; allowing the user to direct the visualization process on-the-fly. VIEW also goes beyond 
these systems by supporting new module development within the confines of the system (although IRIS Explorer 
has recently introduced several embedded languages [7]). The use of an interpreted lan­guage, with built-in 
graphical debugging, greatly facilitates tool development. Several visualization programming languages 
and systems have been developed in recent years. Palmer s pdbq language [9] provides support for visualization 
of molecular structures. Hultquist s LISP­based system for flow-visualization [10] allows rapid prototyping 
of new algorithms. The VIEW language extends these systems by adding interaction with the image. With 
Hibbert s system for developing algorithms to process meteorological data [11], the user steps through 
an algorithm, choosing a variety of display represen­tations for intermediate values. The selection of 
locations to be examined is performed on-screen. VIEW provides a similar facility, with the addition 
of user-specified geometries and interaction se­quences. Gramps [12] is a general purpose graphics language 
which has been extended for molecular modeling [13]. VIEW goes beyond Gramps by providing a general-purpose 
programming language and scriptable interaction sequences. The MAGE system, developed by D.C. and J.S. 
Richardson, pio­neers a new concept in scientific visualization [14]. Authors in the journal Protein 
Science publish not only their visual images, but also the associated 3-D display lists on a diskette. 
The diskette also contains the MAGE software for the Macintosh and the PC. Ani­mated visualizations are 
pre-scripted using a scripting language available to any reader. In addition to viewing the animation, 
the reader may use a fixed set of database query and visualization tools to explore the images.  WHAT 
S NEW In summary, the VIEW system goes beyond previous work by providing: data-drawing  8a 8b8c Figure 
8. Steps in tracing chain topology. The final figure shows a classic Greek-key barrel motif. Figure 9. 
Interactive simulation of dipeptide binding  image-based interaction in which the user changes the visual­ization 
by touching parts of it.  user-customizable interaction sequences.  conditional event-monitors that 
allow parts of the visualization to respond automatically to user actions on other parts.  multiple 
event-monitors based on spatial search that allow simultaneous evaluation of potential actions at a large 
number of 3-D locations.  graphical debugging in which the image is treated as a trace of a routine 
s execution state.  APPLICATION AND SYSTEM EXTENSIONS The VIEW system as is could be readily applied 
to other datasets that are naturally represented by discrete geometric structures. Air­frames or finite-element 
models are good examples. Users com­monly superimpose analytical artifacts such as grids, region bound­aries, 
and isovalue surfaces into scientific databases. These artifacts provide structured geometry to which 
the VIEW approach could be applied. Volume data presents difficulties for current visualization systems; 
the sheer volume of information precludes real-time specification of parameters. An interactive exploratory 
system might provide a solution. We can imagine starting with a skeletal representation of a volume dataset, 
perhaps a sparse cloud of points. A flashlight tool could be used to produce a higher quality rendering 
for user-selected portions of the data set. Selection of regions of interest is a common task in studying 
a dataset. A scripting capability would allow users to build tools to display anomalies in the data, 
search for and highlight extrema, and construct a variety of other interactive filters to limit the amount 
of visual information displayed. A language tailored to volume data, containing commonly used datatypes 
and operations, should provide powerful exploratory capabilities. The ability to script interaction demonstrated 
in VIEW might prove extremely useful in dealing with fluid-flow data and other time­varying datasets. 
Interactive placement of trace particles and other types of probes is already common in systems for studying 
flow fields. The conformational animations produced using VIEW show simple cases of scripting interaction 
with time-varying datasets. Additional language support for handling time-varying data would greatly 
enhance this capability. Scientists would be able to readily tailor the visualization process to accommodate 
a variety of datasets and interaction styles. A useful extension of VIEW would be an ability to sketch 
geometry into an animation. After a series of animation frames have been created, the user might wish 
to sketch new geometry into a single frame using any of the VIEW drawing tools. The tools would automatically 
add the same geometric forms to all frames, with geometric parameters properly updated to account for 
between­frame positional changes. The conditional event-monitor mechanism has proved to be a pow­erful 
tool for specifying interaction sequences. The shortcoming of the technique, however, is that evaluation 
of the conditionals slows the overall response of the system. A natural solution would be to implement 
a multi-processor architecture for the system. Each processor would be assigned evaluation of one or 
more monitors, with access to the required symbol tables and parse trees through shared memory. Any processor 
that detects a monitor trigger would set a global flag, with an associated record indicating the event 
to be executed. The main control process would simply check the flag as part of its polling loop and 
when appropriate, would initiate event processing. Conditional event-monitors are also limited by the 
restriction of the header to a single relational expression. We would like to specify an arbitrary code 
block to be evaluated continually, with some portion of that block serving as the conditional expression. 
We have not taken that approach in the current implementation because read-only relational expressions 
are easier to deal with than general blocks of code. In the latter case, we would need to implement a 
critical section mechanism, ensuring that only a single event-monitor or tool at­tempts to update the 
symbol table at a time. Doing so would make the conditional event-monitor mechanism much more powerful. 
 SYSTEM AVAILABILITY VIEW is available for public use via anonymous ftp. The ftp site is ftp.cs.unc.edu 
(152.2.128.159). Executables, data files, and docu­mentation are located in the pub/VIEW directory. More 
extensive documentation is available from the UNC Department of Computer Science. ACKNOWLEDGMENTS This 
work is supported by the Biotechnology Research Program, National Center for Research Resources, NIH, 
grant number RR02170. Our thanks to the many graduate research assistants who worked on previous versions 
of the VIEW system, and to Daniel Aliaga for assistance in implementing the current version. Tom Palmer 
and Dave Bock offered many useful suggestions on the applicability of VIEW concepts to other datatypes. 
Thanks to Amitabh Varshney, and Mike Bajura for assistance in preparing the manuscript, and to Laura 
Bollinger for careful editing. We especially thank collaborating biochemist Kim Gernert of Duke University, 
for numerous contributions to this research.  REFERENCES [1] B.H. McCormick, T.A. DeFanti, and M.D. 
Brown, eds., Visu­alization in Scientific Computing, Computer Graphics, Vol. 21, No. 6, Nov. 1987. [2] 
M. Pique, J.S. Richardson, and F.P. Brooks, Jr., What Does a Protein Look Like? Invited videotape presented 
at 1982 SIGGRAPH Conference, July 1982. [3] J.S. Richardson et al, Looking at Proteins: Representations, 
Folding, Packing, and Design, Biophys. J., Vol. 63, Nov. 1992, pp. 1186-1209. [4] Bergman, et al, VIEW 
 Visualization Impromptu Evaluation Workbench, abstract in J. Mol. Graphics, Vol. 6, Dec. 1988, pp. 223. 
[5] C. Upson et al, The Application Visualization System: A Computational Environment for Scientific 
Visualization, IEEE Computer Graphics &#38; Applications, Vol. 9, No. 4, July 1989, pp. 30-42. [6] D.S. 
Dyer, A Dataflow Toolkit for Visualization, IEEE Computer Graphics &#38; Applications, Vol. 10, No. 4, 
July 1990, pp. 60-69. [7] IRIS Explorer (TM) User s Guide, Document Number 007­1371-010, Silicon Graphics, 
Inc., Mountain View, CA, Jan. 1992. [8] B. Lucas et al, An Architecture for a Scientific Visualization 
System, Proc. Visualization 92 (Oct. 1992), pp. 107-114. [9] T.C. Palmer, A Language for Molecular Visualization, 
IEEE Computer Graphics &#38; Applications, Vol. 12, No. 3, May 1992, pp. 23-32. [10] J.P. Hultquist and 
E.L. Raible, SuperGlue: A Programming Environment for Scientific Visualization, Proc. Visualization 92 
(Oct. 1992), pp. 243-251. [11] W. Hibbert, C.R. Dyer, and B. Paul, Display of Scientific Data Structures 
for Algorithm Visualization, Proc. Visualization 92 (Oct. 1992), pp. 139-146. [12] T.J. O Donnell and 
A.J. Olson, Gramps A Graphics Lan­guage Interpreter for Real-Time Interactive Three-Dimensional Picture 
Editing and Animation, Computer Graphics (Proceed­ings of SIGGRAPH 1981), Vol. 15, No. 3, Aug. 1981, 
pp. 133­ 142. [13] M.C. Connolly and A.J. Olson, Granny, a Companion to Gramps for the Real-Time Manipulation 
of Macromolecular Models. Computers and Chemistry, Vol. 9, No. 1, 1985, pp. 1-6. [14] D.C. Richardson 
and J.S. Richardson, The Kinemage: A Tool for Scientific Communication, Protein Science, Vol. 1, 1992, 
pp. 3-9.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166133</article_id>
		<sort_key>127</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[The nanomanipulator]]></title>
		<subtitle><![CDATA[a virtual-reality interface for a scanning tunneling microscope]]></subtitle>
		<page_from>127</page_from>
		<page_to>134</page_to>
		<doi_number>10.1145/166117.166133</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166133</url>
		<keywords>
			<kw><![CDATA[force]]></kw>
			<kw><![CDATA[haptic]]></kw>
			<kw><![CDATA[interactive graphics]]></kw>
			<kw><![CDATA[scanning tunneling microscopy]]></kw>
			<kw><![CDATA[scientific visualization]]></kw>
			<kw><![CDATA[teleoperation]]></kw>
			<kw><![CDATA[telepresence]]></kw>
			<kw><![CDATA[virtual worlds]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.2</cat_node>
				<descriptor>Chemistry</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010436</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Chemistry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39046690</person_id>
				<author_profile_id><![CDATA[81405594834]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Russell]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Taylor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P296785</person_id>
				<author_profile_id><![CDATA[81100253164]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Warren]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Robinett]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP311187100</person_id>
				<author_profile_id><![CDATA[81541220756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Vernon]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Chi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14037393</person_id>
				<author_profile_id><![CDATA[81100077256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Frederick]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Brooks]]></last_name>
				<suffix><![CDATA[Jr.]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39081299</person_id>
				<author_profile_id><![CDATA[81332536188]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[V.]]></middle_name>
				<last_name><![CDATA[Wright]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP36043603</person_id>
				<author_profile_id><![CDATA[81336493963]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[R.]]></first_name>
				<middle_name><![CDATA[Stanley]]></middle_name>
				<last_name><![CDATA[Williams]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P78871</person_id>
				<author_profile_id><![CDATA[81100167603]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Erik]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Snyder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>134061</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bajura, Michael, Henry Fuchs, and Ryutarou Ohbuchi. Merging Virtual Objects with the Real World: Seeing Ultrasound Imagery within the Patient. Proceedings of SIGGRAPH '92 (Chicago, Illinois, July 26-31, 1992), In Computer Graphics 26, 2 (July 1992), 203-210.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Becker, R.S., J.A. Golovchenko and B.S. Swartzentruber. Atomic-Scale Surface Modifications Using a Tunnelling Microscope. Nature 325(1987), 419.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Besenbacher, F., F. Jensen, E. La~gsgaard, K. Mortensen, and I. Stensgaard. Visualization of the Dynamics in Surface Reconstructions. Journal of Vacuum Science Technology. B 9 (2), Mar/Apr 1991, 874-877.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Binnig, G. and H. Rohrer. Scanning Tunneling Microscopy. Helvetica Physica Acta. 55 (1982), 726-735.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Binnig, G. and H. Rohrer. Scanning Tunneling Microscopy - From Birth to Adolescence. Reviews of Modern Physics, 59(3) July 1987, 615-625.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97899</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Brooks, F. P., Jr., M. Ouh-Young, J. J. Batter, and P. J. Kilpatrick. Project GROPE - Haptic Displays for Scientific Visualization. Proceedings of SIGGRAPH '90. In Computer Graphics 24, 4 (August 1990), 177-185.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Edstrom, Ronald D. and Maria A. Miller. Scanning Tunneling Microscopy and Atomic Force Microscopy Visualization of the Components of the Skeletal Muscle Glycogenolytic Complex. Journal of Vacuum Science Technology, B 9 (2) (Mar/Apr 1991), 1248-1252.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Eklund, E.A. Correlation from Randomness: Scanning Tunneling Microscopy Applied to the Quantitative Analysis of Sputtered Graphite Surfaces. Ph. D. Thesis, UCLA. 1991.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74341</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Fuchs, Henry, John Poulton, John Eyles, Trey Greer, Jack Goldfeather, David Ellsworth, Steve Molnar, Greg Turk, Brice Tebbs, and Laura Israel. Pixel-Planes 5: A Heterogeneous Multiprocessor Graphics System Using Processor-Enhanced Memories. Proceedings of SIGGRAPH '89. In Computer Graphics, 19, 3 (1989). 79-88.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Holloway, Richard, Henry Fuchs, and Warren Robinett. Virtual-Worlds Research at the University of North Carolina at Chapel Hill. Proc. Computer Graphics '91, London.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Kobayashi, A., F. Grey, R. S. Williams, and M. Aono. Nanometer-Scale Silicon Groove Formation by STM. Science, in press, 1993.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>45995</ref_obj_id>
				<ref_obj_pid>45948</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Komuro, M., S. Okayama, O. Kitamura, W. Mizutani, H. Tokumoto, and K. Kajimura. Nanometer Structure Fabricated by FIB and its Observation by STM. Microelectronic Engineering 6 (1987), 343-348.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Lyding, J. W. S. Skala, J. S. Hubacek, R. Brockenbrough, and G. Gammie. Variable-temperature scanning tunneling microscope. Rev. Sci. Instrum. 59 (9), (September 1988), 1897-1902.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Lyo, I.W. and Ph. Avouris. Field-Induced Nanometer to Atomic Scale Manipulation of Si Surfaces with the Scanning Tunneling Microscope. Science 253 (1991), 173.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Magonov, S. N., G. Bar, E. Keller, E. B. Yagubskii, and H. J. Cantow. Atomic Scale Surface Studies of Conductive Organic Compounds. Synthetic Metals, 40 (1991), 247-256.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Mamin, H. J., S. Chiang, H. Birj, P. H. Guethner, and D. Rugar. Gold deposition from a scanning tunneling microscope tip. J. Vac. Sci. Technol. B 9 (2) (Mar/Apr 1991), 1398-1402.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>917155</ref_obj_id>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Ouh-young, Ming. Force Display In Molecular Docking. Ph.D. Thesis, University of North Carolina at Chapel Hill, 1990.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Robinett, W., R. Taylor, V. Chi, W. V. Wright, F. P. Brooks Jr., R. S. Williams, and E. J. Snyder. The Nanomanipulator: An Atomic-Scale Teleoperator. SIGGRAPH '92 course notes for "Implementation of Immersive Virtual Environments."]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147201</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Robinett, W., and R. Holloway. Implementation of Flying, Scaling, and Grabbing in Virtual Worlds. A C M Symposium on Interactive 3D Graphics, Cambridge MA (1992).]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Snyder, Eric J., Mark S. Anderson, William M. Tong, R. Stanley Williams, Samir J. Anz, Marcos M. Alvarez, Yves Rubin, Frangois N. Diederich, and Robert L. Whetten. Atomic Force Microscope Studies of Fullerene Films: Highly Stable C60 fcc (311) Free Surfaces. Science, 253, 12 (July 1991), 171-173.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>121058</ref_obj_id>
				<ref_obj_pid>121051</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Stoll, E. P. Picture processing and three-dimensional visualization of data from scanning tunneling and atomic force microscopy. IBM Journal of Research and Development, 35,. 1/2 (January/March 1991), 67-77.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166134</article_id>
		<sort_key>135</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[Surround-screen projection-based virtual reality]]></title>
		<subtitle><![CDATA[the design and implementation of the CAVE]]></subtitle>
		<page_from>135</page_from>
		<page_to>142</page_to>
		<doi_number>10.1145/166117.166134</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166134</url>
		<keywords>
			<kw><![CDATA[head-tracking]]></kw>
			<kw><![CDATA[projection paradigms]]></kw>
			<kw><![CDATA[real-time manipulation]]></kw>
			<kw><![CDATA[stereoscopic display]]></kw>
			<kw><![CDATA[virtual reality]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Stereo</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Three-dimensional displays**</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39045197</person_id>
				<author_profile_id><![CDATA[81100487435]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carolina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cruz-Neira]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P59331</person_id>
				<author_profile_id><![CDATA[81100455127]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Sandin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15032129</person_id>
				<author_profile_id><![CDATA[81100432746]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[DeFanti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>142416</ref_obj_id>
				<ref_obj_pid>142413</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bishop, G., Fuchs, H., et al. Research Directions in Virtual Environments. Computer Graphics, Vol. 26, 3, Aug. 1992, pp. 153--177.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>57168</ref_obj_id>
				<ref_obj_pid>57167</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Brooks, F.P. Grasping Reality Through Illusion: Interactive Graphics serving Science. Proc. SIGCHI ' 88, May 1988, pp. 1-11.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>129892</ref_obj_id>
				<ref_obj_pid>129888</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Cruz-Neira, C., Sandin, D.J., DeFanti, T.A., Kenyon, R., and Hart, J.C. The CAVE, Audio Visual Experience Automatic Virtual Environment. Communications of the ACM, June 1992, pp. 64-72.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>142825</ref_obj_id>
				<ref_obj_pid>142750</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Codella, C., Jalili, R., Koved, L., Lewis, B., Ling, D.T., Lipscomb, J.S., Rabenhorst, D., Wang, C.P., Norton, A., Sweeny, P., and Turk, G. Interactive simulation in a multi-person virtual world. ACM Human Factors in Computing Systems, CHI '92 Conf., May 1992, pp. 329- 334.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Chung, J.C., Harris et al. Exploring Virtual Worlds with Head-Mounted Displays. Proc. SPIE, Vol. 1083-05, Feb.1990, pp. 42-52.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134039</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Deering, M. High Resolution Virtual Reality. Computer Graphics, Vol. 26, 2, July 1992, pp. 195-201.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Fisher, S. The AMES Virtual Environment Workstation (VIEW). SIGGRAPH '89, Course #29 Notes, Aug. 1989.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988488</ref_obj_id>
				<ref_obj_pid>988486</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Max, N. SIGGRAPH'84 Call for Omnimax Films. Computer Graphics, Vol 16, 4, Dec. 1982, pp. 208-214.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[McDowall, I.E., Bolas, M., Pieper, S., Fisher, S.S. and Humphries, J. Implementation and Integration of a Counterbalanced CRT-based Stereoscopic Display for Interactive Viewpoint Control in Virtual Environments Applications. Proc. SPIE, Vol. 1256-16.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Plato. The Republic. The Academy, Athens, c.375 BC.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>135302</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Rheingold, H. Virtual Reality. Summit, New York, 1991.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I.E. The Ultimate Display. Proc. IFIP 65, 2, pp. 506-508,582-583.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Teitel, M.A. The Eyephone: A Head-Mounted Stereo Display. Proc. SPIE, Vo1.1256-20, Feb. 1990, pp. 168- 171.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Graphics Library Programming Guide. Silicon Graphics, Inc. 1991.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Surround-Screen Projection-Based Virtual Reality: The Design and Implementation of the CAVE Carolina 
Cruz-Neira Daniel J. Sandin Thomas A. DeFanti Electronic Visualization Laboratory (EVL) The University 
of Illinois at Chicago Abstract This paper describes the CAVE (CAVE Automatic Virtual Environment) virtual 
reality/scientific visualization system in detail and demonstrates that projection technology applied 
to virtual-reality goals achieves a system that matches the quality of workstation screens in terms of 
resolution, color, and flicker-free stereo. In addition, this format helps reduce the effect of common 
tracking and system latency errors. The off-axis perspective projection techniques we use are shown to 
be simple and straightforward. Our techniques for doing multi-screen stereo vision are enumerated, and 
design barriers, past and current, are described. Advantages and disadvantages of the projection paradigm 
are discussed, with an analysis of the effect of tracking noise and delay on the user. Successive refinement, 
a necessary tool for scientific visualization, is developed in the virtual reality context. The use of 
the CAVE as a one-to-many presentation device at SIGGRAPH '92 and Supercomputing '92 for computational 
science data is also mentioned. Keywords: Virtual Reality, Stereoscopic Display, Head-Tracking, Projection 
Paradigms, Real-Time Manipulation CR Categories and Subject Descriptors: I.3.7 [Three-Dimensional Graphics 
and Realism]: Virtual Reality; I.3.1 [Hardware Architecture]: Three-Dimensional Displays.  1. Introduction 
 1.1. Virtual Reality Overview Howard Rheingold [11] defines virtual reality (VR) as an experience in 
which a person is surrounded by a three­dimensional computer-generated representation, and is able to 
move around in the virtual world and see it from different angles, to reach into it, grab it, and reshape 
it. The authors of this paper prefer a definition more confined to the visual domain: a VR system is 
one which provides real-time viewer-centered head­tracking perspective with a large angle of view, interactive 
control, and binocular display. A competing term, virtual environments (VE), chosen for truth in advertising 
[1], has a somewhat grander definition which also correctly encompasses touch, smell, and sound. Although 
VE is part of the CAVE acronym, we will use the initials VR herein to conform to mainstream usage. 851 
S. Morgan, Room 1120 SEO (M/C 154). Chicago, IL 60607-7053. E-mail: cruz@bert.eecs.uic.edu  Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct provided that the copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the commercial advantage, the ACM copyright notice and the 
title of the publication and its date appear, and notice is given that copying is by publication and 
its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. otherwise, or to republish, requires a fee and/or specific permission. 
&#38;#169;1993 ----8/93/008/0015 $1.50 &#38;#169;1993 ACM -0 -89791 -601 -8/93/008 $1.50 Several common 
systems satisfy some but not all of the VR definition above. Flight simulators provide vehicle tracking, 
not head tracking, and do not generally operate in binocular stereo. Omnimax theaters give a large angle 
of view [8], occasionally in stereo, but are not interactive. Head-tracked monitors [4][6] provide all 
but a large angle of view. Head-mounted displays (HMD) [7][13] and BOOMs [9] use motion of the actual 
display screens to achieve VR by our definition. Correct projection of the imagery on large screens can 
also create a VR experience, this being the subject of this paper. Previous work in the VR area dates 
back to Sutherland [12], who in 1965 wrote about the Ultimate Display. Later in the decade at the University 
of Utah, Jim Clark developed a system that allowed wireframe graphics VR to be seen through a head­mounted, 
BOOM-type display for his dissertation. The common VR devices today are the HMD and the BOOM. Lipscomb 
[4] showed a monitor-based system in the IBM booth at SIGGRAPH '91 and Deering [6] demonstrated the Virtual 
Portal, a closet­sized three-wall projection-based system, in the Sun Microsystems' booth at SIGGRAPH 
'92. The CAVE, our projection-based VR display [3], also premiered at SIGGRAPH '92. The Virtual Portal 
and CAVE have similar intent, but different implementation schemes. To distinguish VR from previous developments 
in computer graphics, we list the depth cues one gets in the real world. 1 Occlusion (hidden surface) 
2 Perspective projection 3 Binocular disparity (stereo glasses) 4 Motion Parallax (head motion) 5 Convergence 
(amount eyes rotate toward center of interest, basically your optical range finder) 6 Accommodation (eye 
focus, like a single-lens reflex as range finder) 7 Atmospheric (fog) 8 Lighting and Shadows Conventional 
workstation graphics gives us 1, 2, 7, and 8. VR adds 3, 4, and 5. No graphics system implements accommodation 
clues; this is a source of confusion until a user learns to ignore the fact that everything is in focus, 
even things very close to the eyelash cutoff plane that should be blurry. The name of our virtual reality 
theater, CAVE, is both a recursive acronym (CAVE Automatic Virtual Environment) and a reference to The 
Simile of the Cave found in Plato's Republic [10], in which the philosopher discusses inferring reality 
(ideal forms) from projections (shadows) on the cave wall. The current CAVE was designed in early 1991, 
and it was implemented and demonstrated to visitors in late 1991. This paper discusses details of the 
CAVE design and implementation.  1.2. CAVE Motivation Rather than having evolved from video games or 
flight simulation, the CAVE has its motivation rooted in scientific visualization and the SIGGRAPH '92 
Showcase effort. The CAVE was designed to be a useful tool for scientific visualization. Showcase was 
an experiment; the Showcase chair, James E. George, and the Showcase committee advocated an environment 
for computational scientists to interactively present their research at a major professional conference 
in a one-to­many format on high-end workstations attached to large projection screens. The CAVE was developed 
as a Virtual Reality Theater with scientific content and projection that met the criteria of Showcase. 
The Showcase jury selected participants based on the scientific content of their research and the suitability 
of the content to projected presentations. Attracting leading-edge computational scientists to use VR 
was not simple. The VR had to help them achieve scientific discoveries faster, without compromising the 
color, resolution, and flicker-free qualities they have come to expect using workstations. Scientists 
have been doing single-screen stereo graphics for more than 25 years; any VR system had to successfully 
compete. Most important, the VR display had to couple to remote data sources, supercomputers, and scientific 
instruments in a functional way. In total, the VR system had to offer a significant advantage to offset 
its packaging. The CAVE, which basically met all these criteria, therefore had success attracting serious 
collaborators in the high-performance computing and communications (HPCC) community. To retain computational 
scientists as users, we have tried to match the VR display to the researchers' needs. Minimizing attachments 
and encumbrances have been goals, as has diminishing the effect of errors in the tracking and updating 
of data. Our overall motivation is to create a VR display that is good enough to get scientists to get 
up from their chairs, out of their offices, over to another building, perhaps even to travel to another 
institution. 1.3. CAVE Design The CAVE we exhibit at conferences is a theater 10'x10'x10' made up of 
three rear-projection screens for walls and a down­projection screen for the floor, as shown in Figure 
1. (Our development system at EVL is actually 7'x7'x7' due to ceiling height limitations.) Projectors 
throw full-color workstation fields (1280x512 stereo) at 120Hz onto the screens, giving between 2,000 
and 4,000 linear pixel resolution to the surrounding composite image. Computer-controlled audio provides 
a sonification capability to multiple speakers. A user's head and hand are tracked with Polhemus or Ascension 
tethered electromagnetic sensors. Stereographics' LCD stereo shutter glasses are used to separate the 
alternate fields going to the eyes. Four Silicon Graphics high-end workstations create the imagery (one 
for each screen); they are tied to a fifth for serial communications to input devices and synchronization 
via fiber­optic reflective memory by Systran Corporation. The CAVE's theater area sits in a 30'x20'x13' 
room, provided that the projectors' optics are folded by mirrors. Conference use thus far has necessitated 
the building of a light-tight structure of this size on site to house the screens and projectors. Goals 
that inspired the CAVE engineering effort include: 1 The desire for higher-resolution color images and 
good surround vision without geometric distortion. 2 Less sensitivity to head-rotation induced errors 
3 The ability to mix VR imagery with real devices (like one's hand, for instance) 4 The need to guide 
and teach others in a reasonable way in artificial worlds 5 The desire to couple to networked supercomputers 
and data sources for successive refinement Figure 1: CAVE diagram. Graphics by Milana Huang, University 
of Illinois at Chicago Significant barriers, now hurdled, include eliminating the lag inherent in common 
green video projector tubes, corner detailing, and frame accurate synchronization of the workstations; 
our solutions to these problems are described in detail in section 3. The electromagnetic trackers required 
building the CAVE screen support structure out of non-magnetic stainless steel (which is also relatively 
non-conductive), but non­linearities are still a problem, partially because conductive metal exists on 
the mirrors and in the floor under the concrete. Wheelchairs, especially electric ones, increase tracker 
noise and non-linearities as well. Unsolved problems to date include removing the tracking tether so 
the user is less encumbered, moving the shutters from the eyes to the projectors so cheap cardboard polarizing 
glasses can be used, incorporating accurate directional sound with speakers, and bringing down the cost. 
These, and other problems we've encountered, are described in section 6. The implementation details fall 
mainly into two categories: projection and stereo. These will be presented next.  2. Projection Details 
2.1. Cube Sides As Projection Planes One rarely noted fact in computer graphics is that the projection 
plane can be anywhere; it does not have to be perpendicular to the viewer (as typical on workstations, 
the HMD, and the BOOM). An example of an unusual projection plane is the hemisphere (like in Omnimax 
theaters or some flight simulators). However, projection on a sphere is outside the real-time capability 
of the ordinary high-end workstation. And, real-time capability is a necessity in VR. The CAVE uses a 
cube as an approximation of a sphere. This simplification greatly aids people trying to stand in the 
space, and fits the capabilities of off-the-shelf graphics and high­resolution projection equipment, 
both of which are made to create and project imagery focused on flat rectangles. The defects one encounters 
in attempting to build a perfect cube are fortunately within the range of adjustment by standard video 
projectors; in particular, keystoning and pincushion corrections can be utilized. Thus, the ability to 
match projected images at the seams and corners is effectively perfect, with tuning effort. 2.2. Window 
Projection Paradigm The most common computer graphics projection paradigm is the camera view. This type 
of projection simulates the way an image is captured on film, and includes the direction the camera is 
pointed and the focal length, position, and twist angle of the lens. In the camera paradigm, stereo is 
typically achieved by using two cameras; this is the technique used by the HMD and BOOM. The CAVE instead 
uses a window projection paradigm in which the projection plane and projection point relative to the 
plane are specified, thus creating an off-axis perspective projection. Fortunately, the Silicon Graphics' 
Graphics Library (GL) [14] provides a window projection function. Since this function can also be performed 
by two shears and a standard perspective projection, or, alternatively, by a translation, a standard 
perspective projection and a translation back, the window projection function can easily be constructed 
from more primitive functions, if not available in another graphics library. In the CAVE, the projection 
plane locations correspond to the locations of the actual walls. Therefore, as the viewer moves around 
in the environment, the off-axis stereo projection is calculated according to his/her position with respect 
to the walls (see Figure 2). Front wall Left Right wall wall Viewer Figure 2: Off-axis projection For 
the simplicity of the calculations, we assume that all the walls share the same reference coordinate 
system as shown in Figure 3. The origin of the coordinate system is placed in the center of the CAVE 
and it is a right-handed system with respect to the front wall. All the measurements from the trackers 
(position and orientation) are transformed to match this convention. Left wall Front wall Right wall 
 Figure 3: CAVE reference system. Figure 4 shows a top diagram of the CAVE. The point Q' is the projection 
of the point Q. PP is the distance from the center of the CAVE to the front wall (5' for the 10'x10'x10' 
CAVE). Right wall Left wall Figure 4: CAVE projection diagram Using straightforward algebra and following 
the conventions in Figure 4, the projection Q' of a point Q(Qx, Qy, Qz) on the front wall is given by: 
(PP- Qz )(ex - Qx ) Q'x = Qx + ez - Qz (PP -Qz)(ey- Qy ) Q'y = Qy + ez - Qz Thus, the general projection 
matrix is: . 1 0 0 0 . . 0 1 0 0 . -. ex - ey 1 - 1 . . .. ez - PP exPP ez - PP ez - PP eyPP ez - PP 
0 ez - PP ez ez - PP . .. One important issue to mention is that, in the CAVE, the eyes are not assumed 
to be horizontal and in a plane that is perpendicular to the projection plane. A clear example of this 
is a situation in which the viewer is looking at one of the corners of the CAVE with his/her head tilted. 
Our tracker is mounted on top of the stereo glasses; it is raised 5.5" from the glasses to minimize interference 
and centered between the eyes. From the values obtined from the tracker, and assuming an interpupilar 
distance of 2.75", we can determine the position of each eye and its orientation with respect to each 
one of the walls before applying the projection matrix. The reader can easily derive the matrices for 
the other walls of the CAVE. Notice that, since the walls of the CAVE are at exactly 90° from each other, 
the viewer's position with respect to the other walls are: Left wall: (ez, ey, ex) Right wall: (-ez, 
ey, ex) Floor wall: (ex, ez, -ey)  3. Stereo Vision Details 3.1. Convergence To achieve stereo vision 
in the CAVE, we, in principle, do two off-axis stereo projections per screen, one for each eye. We need 
to obtain information from the tracker to accurately place each eye. We assume that the center of rotation 
of the eye is close enough to the nodal point (projection point) of the eye to not introduce significant 
error. Thus, as with other VR systems, where the eyes are looking does not enter into the calculations. 
 3.2. Frame Sequential Stereo To get a different image to each eye, we use frame sequential stereo with 
synchronized shutter glasses. Infrared transmitters cause the lens for each eye to stay transparent for 
the proper 512 lines of the 1280x1024 image per screen, switching during vertical retrace time. We produce 
120 fields per second, thus updating the whole image at 60Hz, producing a flicker-free image. Note, however, 
that the green phosphor used in commercially available projection tubes has a persistence that is too 
long, so a user always sees both images anyway, destroying the stereo effect. Until Stereographics provided 
us with P43 coated green tubes by special order, we did our experiments (in 1991) in blue and red and 
shades of magenta. With luck, tube manufacturers will be motivated to add such tubes to their catalogs 
soon. 3.3. Distortion Correction The HMD, BOOM, and monitor VR systems have significant geometric distortion 
inherent in their optics. Modern data projectors have extensive electronic adjustments to accurately 
correct geometric distortions. 3.4. Minimizing User Shadows The three wall screens are rear projected 
so that the participants in the CAVE do not cast shadows. The floor is down projected so shadows are 
cast. We off-axis project the image from the front top instead of directly overhead, so the shadow of 
the user falls mainly behind him/her. 3.5. Frame Accurate Synchronization Another problem we had to 
solve was the perfect synchronization of the screen updates. If the images are even one frame out of 
sync, the images in the corners crease and start to look sucked in like sofa cushions. We were unable 
to get adequate response from the UNIX system to synchronize within the 8ms needed, so (at the suggestion 
of Silicon Graphics staff) we went to reflective memory, a sort of shared cache arrangement among all 
the workstations. Reflective memory allows C-pointers to directly access chunks of memory, neatly bypassing 
the operating system. We intend to use the reflective memory for more sophisticated data sharing, including 
broadcasting of meshes, textures, and polygon lists. For now, however, reflective memory solves a nasty 
problem. 3.6. Edge Matching Particular attention is paid to the edges and corners of the screen to avoid 
occlusion of stereo objects inside the room. We minimize the seams by stretching a 10'x30' plastic screen 
over 1/8" stainless steel cable under tension. This gives a seam of about a pixel or so in width, which 
can be seen but can also be easily ignored. Hence, the illusion of stereo in the CAVE is extremely powerful 
to the viewer. The floor butts up against the screen fairly perfectly (1/16") and presents no problem. 
In the case of 3D movies and workstation screens, stereo objects in front of the screen (often the most 
interesting ones) have to stay pretty much centered. When a stereo object in front of a screen hits the 
edge (called frame violation in the jargon), it collapses the depth illusion since occlusion is a stronger 
depth cue than binocular disparity. The CAVE's screen edges are basically out of view (one can see the 
tops of the screens, but they are high up) so the stereo objects can be anywhere. We were amazed at how 
much the floor adds to the experience; a user can walk around convincing objects that are being projected 
into the room. Since the tracker provides six degrees of information, the user's head can tilt as well, 
a natural way to look at objects. The HMD provides this capability, but BOOM hardware does not. 3.7 
Minimizing Occlusion by Participants A user's hand can cause stereo violation if an object is between 
the eyes and the hand, a rare enough situation. People are very eager to resolve stereo violation whenever 
it's easy so, in these instances, the user simply moves his/her hand out of the way. A much more serious 
situation occurs with multiple people in the CAVE. If someone gets in the way of another viewer and an 
object is supposed to be projected between the two of them, the stereo collapses. We avoid this by having 
a teacher or guide control the navigation, but let the student or tourist be tracked and stand in front, 
thereby getting the best stereo experience without first having to learn to be an expert navigator of 
the data space, whatever it is. At conferences, we often jam a dozen people at a time in the CAVE and 
try to keep the images in front of the crowd. Since people more or less have to stay still or move together, 
the VR experience for all, however limited, is nevertheless pleasing. 3.8. Motion Sickness Seeing one's 
own body or those of other people may in fact be a good idea. Of 9,000 or so people who have been in 
the CAVE, two have experienced enough nausea to complain about it, a very low ratio (apparently) for 
VR [1]. We don't yet know why the CAVE doesn't make people nauseous; perhaps it is content related. Our 
images primarily have to do with scientific data that changes over time, not roller coaster type motions 
with fast tilting horizons typical of many VR applications. Another explanation may be our better coping 
with fast head rotation (see next section).   4. Quantitative Analysis of the Effect of Tracking Noise 
and Latency 4.1. Introduction Different VR modes have different responses to errors in tracking viewer 
position. One reason for the differences depends on whether the projection plane moves with the viewer 
(as with BOOMs and HMDs) or not (in the case of the monitor and CAVE). A second reason is the difference 
in the distance of the projection plane to the eye, which distinguishes the monitor implementation from 
the CAVE's. 4.2. Rotation errors Tracking errors can be resolved into displacement errors and rotation 
errors. Actual problems are often a combination of the two. In the monitor and CAVE paradigms, since 
the projection plane does not move with the viewer's position and angle, a rotation about the projection 
point in the eye creates zero error. In the HMD/BOOM paradigm, a given rotational tracking error produces 
the same magnitude of rotational error in the image, but of opposite sign. This is a serious problem 
if the user's head rotates quickly because the whole visual scene first rotates with the head and then 
steps back into the proper place.  4.3. Analysis of displacement errors in the CAVE and monitor paradigms 
The effect of displacement error for both the CAVE and the monitor paradigms is illustrated in Figure 
8. The displacement error in eye tracking is .P (in a plane parallel to the projection plane), the distance 
from the eye to the projection plane is PD, and the distance to the object is Z. DISP is the distance 
error on the projection plane. a is the angular error. ProjectionTracked  Z Figure 8: Effect of displacement 
error for both the CAVE and the monitor paradigms Z -PD . . DISP =.P . Z . DISP . . a=arctan . PD . DISP 
a. for small angles PD therefore, (Z -PD) .P Z (1) a. PD (Z -PD) For large Z, .1 Z therefore, .P (2) 
a. PD (Z -PD) PD For small Z, .- ZZ therefore, .P (3) a .- Z For Z = PD (when the object is on the projection 
plane), (Z -PD) =0 Z therefore, (4) a=0 Equation (1) represents the approximate angular error a for a 
displacement tracking error .P in the monitor and CAVE paradigms. Equation (2) shows that the larger 
projection distance PD associated with the CAVE, as compared to the monitor, makes angular error a due 
to displacement .P smaller for large distances Z to the object viewed. Equation (3) shows that for very 
small Z values, the monitor and CAVE have similar responses. Equation (4) shows that when objects are 
on the projection planes of the monitor or CAVE, the angular error a due to displacement is zero.  4.4. 
Analysis of displacement errors in the BOOM and HMD A similar analysis for the BOOM and HMD is indicated 
in Figure 9. Perceived Projection objectplane -.P DISP  Eye Actual object Figure 9: Effect of displacement 
error for both the HMD and the BOOM paradigms A displacement error in tracking head position results 
in identical errors in both the eye position and the projection plane position. This results in a negative 
displacement of the object being viewed. -.P . . a=arctan . Z . For small angles, -.P (5) a. Z Equation 
(5) shows that the angular error a is independent of the projection distance PD to the projection plane. 
Comparing equation (5) with (2), we see that the BOOM and HMD have less angular error a for displacement 
errors .P for large object distances Z than the CAVE/monitor models. Comparing equation (5) with (3), 
we see that the BOOM and HMD have similar angular errors a for small object distance Z.  Figure 10 graphs 
the angular error a due to a tracker displacement error .P of 3cm for object distances Z. This case represents 
a tracking error due to latency of a person moving 30cm/second combined with a display rate of 10 frames/second. 
For large object viewing distances (Z=500cm), the HMD/BOOM have the best performance, the CAVE has 2-1/2 
times the error, and the monitor has 9 times the error. For small object viewing distances (Z=20cm), 
the monitor has the best performance, and the CAVE and HMD/BOOM have only slightly worse error magnitudes. 
 4.5. Examples of combined rotation and displacement tracking errors Normal head motions like nodding 
and panning involve both rotation and displacement of the eyes. The combined effect of these errors may 
be approximated by summing the individual angular errors a. The assumed projection distances PD for the 
monitor and 10' CAVE are 50cm and 150cm, respectively. Figure 11 graphs the angular error a as a function 
of eye/object distance Z due to a head rotation (pan) of 90 degrees/second and a display rate of 10 frames/second. 
It is assumed that the eyes are 5cm from the center of rotation. For large Z, the CAVE is 43 times better 
than the HMD/BOOM and 4 times better than the monitor. For small Z, the CAVE and monitor are 6 times 
better than the HMD/BOOM. Figure 11: Tracking errors introduced by head panning Figure 12: Tracking errors 
introduced by head nodding Figure 12 graphs the angular error a as a function of eye/object distance 
Z due to a head rotation (nod) of 90 degrees/second and a display rate of 10 frames/second. It is assumed 
that the eyes are 15cm from the center of rotation. For large Z, the CAVE is 15 times better than the 
HMD/BOOM and 4 times better than the monitor. For small Z, the CAVE and monitor are 3 times better than 
the HMD/BOOM. The examples above are all due to tracking errors caused by latency. Tracking errors from 
other sources, such as electrical interference, tend to be about an order of magnitude smaller, but the 
ratios are the same and we can draw the same conclusions. For the head-panning example in section 4.5, 
the problem was caused by normal head motion; if, however, we divide the angular error a by 20, we could 
interpret the graph as representing the case of a 0.5-degree tracking error combined with a tracking 
receiver mounted 5cm from the eye. 5. Successive Refinement One benefit of the wrap-around screens in 
the CAVE is the potential for successive refinement of images. It is fair to say that we will never, 
in our lifetimes, have enough computing power to create complex models and display them in real time. 
Successive refinement trades off motion for time, freezing the image and filling it in, a now common 
computer graphics technique. Yet, one cannot freeze the image in a HMD without major disorientation. 
In the BOOM, successive refinement is possible but the user cannot look around. In the CAVE, one can 
navigate to a place in real time and then send off to a supercomputer for a highly detailed set of four 
images, still in stereo. When the images come back, the user can still pan around, although he/she cannot 
navigate while in this mode. The best stereo is achieved when looking in the last interactively tracked 
direction. Optimizing for this mode is the subject of active ongoing research. Making VR usable in less-than-real-time 
situations is important. Supercomputers are essentially floating-point machines. One popular vector machine 
we use cannot create 1280x1024 pixel maps in real time because the floating-to-fixed conversions are 
done by non-vectorized subroutine calls (at three conversions, one for each pixel color component, it 
gets time consuming). There are no floating-point frame buffers for sale. In addition, the desire to 
transmit a 1280x1024 24-bit image to a workstation 60 times a second requires nearly 2 gigabits of network 
throughput! Multiply that by 4 for the CAVE screens. Since an update rate of only 10 times a second is 
closer to VR industry standards, divide by 6, which results in a need for 1.25 gigabits/second. Clearly, 
we try to transmit polygon lists and meshes in floating point and let the workstation's graphics engine 
do its job whenever possible. Naturally, it is important to consider more than image complexity; the 
basic science being computed often is extremely complex and will not respond in real time. Sometimes 
large stores of precomputed data are meaningful to explore; perhaps disk-based playback will be useful. 
The CAVE is a research resource now being used by scientists at the University of Illinois at Chicago, 
the National Center for Supercomputing Applications, Argonne National Laboratory, University of Chicago, 
California Institute of Technology, and the University of Minnesota. The overall goal is to match the 
capabilities of supercomputing, high-speed networking, and the CAVE for scientific visualization applications. 
  6. CAVE Shortcomings 6.1. Cost The CAVE is big and expensive, although, given inflation, it is no 
more expensive than the PDP-11/Evans &#38; Sutherland single­user display system was 20 years ago. Also, 
considering that up to 12 people can space-share the CAVE, the cost per person comes down in some circumstances. 
Cheap wall-sized LCD screens with low latency that one could stand on would be great to have, if they 
only existed. The desire for the rendering afforded by $100,000 state-of-the-art graphics engines will 
not abate; however, current effects will be achievable at more modest cost as time goes on. 6.2. Ability 
to Project on All Six Sides of the CAVE Six screens would make a better CAVE. We originally planned to 
do both floor and ceiling rear projections, which would have necessitated raising the CAVE structure 
10'. A hole in the floor and a large sheet of strong glass or plastic would be a better solution, but 
not one easily achieved at conferences or universities. A rear screen for the fourth wall might be possible, 
although the details for human entrance and exit would have to be worked out, especially if the cable-stretched 
screen technique were used. Four screens work very well, yielding large surround views for both panning 
actions and looking down. Consequently, objects inside the room can be walked around and virtually beg 
to be touched. 6.3. Light Spillage One problem is the light spillage from the screen on the floor (the 
wall screens are fortunately not very reflective). Our floor screen is simply a painted floor board; 
the floor paint was quickly chosen by using the color-matching computer at the local paint distributor 
to duplicate the wall screens' color as a first approximation. The only time there would be a problem 
having one screen brighter than the others would be when the center of interest is not an object on the 
brightest screen, an unusual case. Very bright screens all around do tend to reduce image contrast somewhat, 
but this, too, has not been an issue. Naturally, good graphic design optimizes for the strengths and 
weaknesses of any medium. 6.4. Utilizing the CAVE Medium to Its Full Potential The CAVE, like Omnimax, 
represents a different visual paradigm: inside out instead of outside in. From working with students 
and colleagues, we realize that getting people to design visualizations and think in terms of inside-out 
is difficult, especially since the CAVE simulator used in the early stages of application development 
has an outside-in presentation on the workstation screen. Nonetheless, it is a concept into which it 
is fairly easy to incorporate data. 6.5. Fragility The CAVE is not museum hardy. The screens, tracker, 
and glasses are not kid-proof, thereby limiting use in museums, malls, arcades, and so on. More research 
is needed. 6.6. New Control Paradigms As the computing community went from command-line terminals to 
2D raster systems, the pull-down menu and mouse provided an alternative to the command line and keyboard. 
The CAVE has not produced any significant new control paradigms to date, although step-on menus have 
been proposed. One graduate student (Randy Hudson) has achieved a nice way to control rotation by having 
the user stroke a barely perceptible tessellated wireframe sphere with his/her hand. We look forward 
to the challenge of finding the next control models and encourage anyone with ideas to come and discuss 
collaboration. 6.7. Directional Sound Another issue to address is the effective implementation of directional 
sound. In theory, with speakers in all corners, one should be able to achieve good directionality with 
the proper audio synthesis gear. In practice, however, sound localization is compromised by reflections 
off the screens. 6.8. Ability to Document The CAVE is very hard to photograph. Imaginations soar when 
readers are presented with excellent suggestive 2D photos of other VR devices in use. We have not been 
able to compete in this domain. However, the CAVE and monitor are both amenable to video documentation 
if the tracking device is attached to the camera and the interoccular distance is adjusted to zero. 
 7. Conclusions The CAVE has proven to be an effective and convincing VR paradigm that widens the applicability 
and increases the quality of the virtual experience. The CAVE achieves the goals of producing a large 
angle of view, creating high-resolution (HDTV to twice HDTV) full-color images, allowing a multi-person 
(teacher/student or salesperson/client) presentation format, and permitting some usage of successive 
refinement. Furthermore, the flatness of the projection screens and the quality of geometric corrections 
available in projectors allow presentations of 3D stereo images with very low distortion as compared 
to monitor­based, HMD, and BOOM VR systems. The user is relatively unencumbered given that the required 
stereo glasses are lightweight and the wires to the head and hand trackers for the tracked individual 
are very thin. Since the projection plane does not rotate with the viewer, the CAVE has dramatically 
minimized error sensitivity due to rotational tracking noise and latency associated with head rotation, 
as compared to the HMD and BOOM. At SIGGRAPH '92 and Supercomputing '92, more than a dozen scientists, 
in fields as diverse as neuroscience, astrophysics, superconductivity, molecular dynamics, computational 
fluid dynamics, fractals, and medical imaging, showed the potential of the CAVE for teaching and communicating 
research results. Collaborative projects are currently underway in non-Euclidean geometries, cosmology, 
meteorology, and parallel processing. The CAVE is proving itself a useful tool for scientific visualization, 
in keeping with our Laboratory's goal of providing scientists with visualization tools for scientific 
insight, discovery, and communication. 8. Future Work Further research efforts will tie the CAVE into 
high-speed networks and supercomputers. We have interest in adding motion-control platforms and other 
highly tactile devices. Hardening and simplifying the CAVE's design for the nation's science museums, 
schools, and shopping malls is a goal as well. Design and implementation of quantitative experiments 
to measure CAVE performance are also planned. 9. References [1] Bishop, G., Fuchs, H., et al. Research 
Directions in Virtual Environments. Computer Graphics, Vol. 26, 3, Aug. 1992, pp. 153--177. [2] Brooks, 
F.P. Grasping Reality Through Illusion: Interactive Graphics serving Science. Proc. SIGCHI 88, May 1988, 
pp. 1-11. [3] Cruz-Neira, C., Sandin, D.J., DeFanti, T.A., Kenyon, R., and Hart, J.C. The CAVE, Audio 
Visual Experience Automatic Virtual Environment. Communications of the ACM, June 1992, pp. 64-72. [4] 
Codella, C., Jalili, R., Koved, L., Lewis, B., Ling, D.T., Lipscomb, J.S., Rabenhorst, D., Wang, C.P., 
Norton, A., Sweeny, P., and Turk, G. Interactive simulation in a multi-person virtual world. ACM Human 
Factors in Computing Systems, CHI 92 Conf., May 1992, pp. 329­ 334. [5] Chung, J.C., Harris et al. Exploring 
Virtual Worlds with Head-Mounted Displays. Proc. SPIE, Vol. 1083-05, Feb.1990, pp. 42-52. [6] Deering, 
M. High Resolution Virtual Reality. Computer Graphics, Vol. 26, 2, July 1992, pp.195-201. [7] Fisher, 
S. The AMES Virtual Environment Workstation (VIEW). SIGGRAPH 89, Course #29 Notes, Aug. 1989. [8] Max, 
N. SIGGRAPH'84 Call for Omnimax Films. Computer Graphics, Vol 16, 4, Dec. 1982, pp. 208-214. [9] McDowall, 
I.E., Bolas, M., Pieper, S., Fisher, S.S. and Humphries, J. Implementation and Integration of a Counterbalanced 
CRT-based Stereoscopic Display for Interactive Viewpoint Control in Virtual Environments Applications. 
Proc. SPIE, Vol. 1256-16. [10] Plato. The Republic. The Academy, Athens, c.375 BC. [11] Rheingold, H. 
Virtual Reality. Summit, New York, 1991. [12] Sutherland, I.E. The Ultimate Display. Proc. IFIP 65, 2, 
pp. 506-508, 582-583. [13] Teitel, M.A. The Eyephone: A Head-Mounted Stereo Display. Proc. SPIE, Vol.1256-20, 
Feb. 1990, pp. 168­ 171. [14] Graphics Library Programming Guide. Silicon Graphics, Inc. 1991. Acknowledgments 
CAVE research is being conducted by the Electronic Visualization Laboratory of the University of Illinois 
at Chicago, with extraordinary support from Argonne National Laboratory and the National Center for Supercomputing 
Applications at the University of Illinois at Urbana-Champaign. Equipment support is provided by Ascension 
Technology Company, DataDisplay Corporation, Electrohome Projection Systems, Polhemus, Silicon Graphics 
Computer Systems, Stereographics Corporation, and Systran Corporation. Major funding is provided by the 
National Science Foundation (NSF) grant ASC-92113813, which includes support from the Defense Advanced 
Research Projects Agency and the National Institute for Mental Health, NSF grant IRI­9213822, and the 
Illinois Technology Challenge Grant.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166135</article_id>
		<sort_key>143</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[Painting with light]]></title>
		<page_from>143</page_from>
		<page_to>146</page_to>
		<doi_number>10.1145/166117.166135</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166135</url>
		<keywords>
			<kw><![CDATA[global illumination]]></kw>
			<kw><![CDATA[inverse problems]]></kw>
			<kw><![CDATA[lighting design]]></kw>
			<kw><![CDATA[radiosity]]></kw>
			<kw><![CDATA[ray tracing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.6</cat_node>
				<descriptor>Least squares methods</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003716</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003716</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P46326</person_id>
				<author_profile_id><![CDATA[81100402692]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schoeneman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P150906</person_id>
				<author_profile_id><![CDATA[81100369597]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Julie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dorsey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31097510</person_id>
				<author_profile_id><![CDATA[81408593457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Smits]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14183802</person_id>
				<author_profile_id><![CDATA[81100529394]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Arvo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P68461</person_id>
				<author_profile_id><![CDATA[81100196982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Donald]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Greenberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baltes, H. P., editor. Inverse Source Problems in Optics, Springer-Verlag, New York, 1978.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122723</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Dorsey, Julie O'B., Francois X. Sillion, and Donald P. Greenberg. "Design and Simulation of Opera Lighting and Projection Effects," in Computer Graphics, 25(4), August 1991, pages 41-50.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Evans, Ralph M. Eye, Film, and Camera in Color Photography, John Wiley &amp; Sons, New York, 1959.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808601</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Goral, Cindy M., Kenneth E. Torrance, Donald P. Greenberg, and Bennett Battaile. "Modeling the Interaction of Light Between Diffuse Surfaces," in Computer Graphics, 18(3), July 1984, pages 213-222.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97903</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Hanrahan, Pat and Paul Haeberli. "Direct WYSIWYG Painting and Texturing on 3D Shapes," in Computer Graphics, 24(4), August 1990, pages 215-223.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Lawson, Charles L. and Hanson Richard J. Solving Least Squares Problems, Prentice-Hall, Englewood Cliffs, 1974.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>524037</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Luenberger, David G. Optimization by Vector Space Methods, John Wiley &amp; Sons, New York, 1969.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147160</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Poulin, PierreandAlainFournier. "Lights from Highlights and Shadows," Proceedings of the 1992 Symposium on Interactive 3D Graphics, in Computer Graphics, April 1992, pages 31- 38.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Tumblin, Jack and Holly Rushmeier. "Tone Reproduction for Realistic Computer Generated Images," in Radiosity Course Notes of SIGGRAPH'91, ACM, August 1991, pages 229- 257.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Whitted, Turner. "An Improved Illumination Model for Shaded Display," CACM, 32(6), June 1980, pages 343-349.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Painting with Light Chris Schoeneman Julie Dorsey Brian Smits James Arvo Donald Greenberg Program of 
Computer Graphics Cornell University Ithaca, NY 14853 ABSTRACT We present a new approach to lighting 
design for image synthesis. It is based on the inverse problem of determining light settings for an environment 
from a description of the desired solution. The method is useful for determining light intensities to 
achieve a desired effect in a computer simulation and can be used in conjunction with any rendering algorithm. 
Given a set of lights with .xed positions, we determine the light intensities and colors that most closely 
match the target image painted by the designer using a constrained least squares approach. We describe 
an interactive system that allows .exible input and display of the solution. CR Categories and Subject 
Descriptors: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism; I.3.3 [ Computer Graphics]: 
Picture/Image Generation; I.3.6 [Computer Graphics]: Methodology and Techniques -Interaction techniques. 
Additional Key Words: simulation, global illumination, radiosity, ray trac­ing, lighting design, inverse 
problems. 1 INTRODUCTION Although global illumination algorithms can produce strikingly re­alistic 
images, these algorithms can be dif.cult to use for lighting design. Currently the only tools available 
to designers are based upon direct methods those that determine an image from a com­plete description 
of an environment and its lighting parameters. This forces a designer to begin with a geometric model, 
position the lights, assign their colors and intensity distributions, and .­nally compute a solution. 
The process is repeated until the so­lution matches the desired effect. This method is generally time­consuming, 
tedious, and often counter-intuitive. Given that we usu­ally begin with a notion of the .nal appearance, 
a more natural, al­beit more dif.cult, approach is to solve the inverse problem that is, to allow the 
user to create a target image and have the algo­rithm work backwards to establish the lighting parameters. 
Inverse problems infer parameters of a system from observed or desired data [1] in contrast with direct 
problems, which simulate the ef­fects given all parameters. Although inverse problems are common Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct provided that the copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the commercial advantage, the ACM copyright notice and the 
title of the publication and its date appear, and notice is given that copying is by publication and 
its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy permission of the Association for Computing Machinery. otherwise, or to republish, requires 
a fee and/or specific permission. otherwise, or to republish, requires a fee and/or specific permission. 
&#38;#169;1993 ACM -0-89791 -601 -8/93/008 $1.50 &#38;#169;1993 -0---8/93/008/0015 $1.50 in radiative 
transfer, thus far the .eld of computer graphics has been almost exclusively concerned with direct problems. 
Yet, inverse problems match a central goal of lighting design determining how to achieve a desired effect. 
In this paper, we present an approach that allows a designer to paint a scene as it is desired to appear. 
Given static geometry and a set of lights with .xed positions, a constrained least squares approach is 
used to determine the light intensities and colors that most closely match the target image painted by 
the designer. In the domain of lighting design, geometry often constrains the placement of the lights 
[2]; the designers frequently know about where to put the lights but not how the lights will combine 
or how bright to make them. Consequently, the task of selecting appropriate intensities for static lights 
is a useful subproblem of lighting design, and this is our focus. We do not address the automatic placement 
of lights, nor the mapping of simulated intensities to physical properties of the lights [3, 9]. 2 INVERSE 
PROBLEM The problem can be phrased more formally as follows: given static scene geometry and a desired 
appearance, determine the lights that will most closely match the target. There are constraints on pos­sible 
solutions: only certain objects can emit light and only posi­tive energy can be emitted keeping us in 
the realm of physically meaningful solutions. The existence of constraints implies that not every target 
is realizable. The most general problem of determining how many lights to use, where the lights should 
be placed, as well as the distribution, color, and intensity of the lights is a non-linear optimization 
problem. However, if all possible lights have been po­sitioned, and their distributions have been .xed, 
the determination of which lights to use and what their colors and intensities should be is a linear 
optimization problem. 2.1 Constrained Least Squares Suppose {81 ,...,8n}is the set of functions resulting 
from n distinct light sources illuminating an environment independently. These functions can be computed 
by any illumination algorithm, including those that account for interre.ection and shadows. For example, 
they may be ray traced images [10] of a scene for each light from the same viewpoint, or radiance functions 
over surfaces in the environment computed via radiosity [4]. Let 9be the target function we wish to approximate. 
To formulate the approximation problem we require some minimal structure on the space of func­tions. 
In particular, we require vector addition and scaling, which we de.ne pointwise, as well as an inner 
product de.ned on pairs of functions (i.e. a symmetric positive de.nite bilinear form). From the inner 
product we gain the useful notion of the size of a func­tion via the norm . II8II= 8,8 , (1) which provides 
a measure of error. The approximation problem can then be stated in terms of .nding non-negative weights 
w1,...,wn such that the function n $ i 9= wi8(2) i=1 minimizes the objective function II909$II. Stated 
in this way, the problem is one of least squares. Its unique solution is easily ex­pressed in terms of 
the inner products:   ()() () 111 8n 1 8,8111 8,w1 8,9 .. . . .. . = . .(3) . ... () 8n 1 8n 8n 
8n ,8111, wn,9 Mwb The n 2n matrix M is the Gram matrix of the inner product, which consists of the 
coef.cients of the normal equations [7]. The Gram 1 8n matrix is non-singular if and only if the functions 
{8,...,}are linearly independent, which will normally be the case if all n lights produce distinct effects 
on the environment. Naturally, this excludes coincident light sources. The remaining task is to de.ne 
an appropriate inner product on the space of functions. Here we make use of the exact nature of the functions. 
If the functions assign intensities to a set of p discrete points, such as images consisting of p pixels, 
then the natural inner product is the p-dimensional vector dot product. Alternatively, if the functions 
de.ne surface radiance, the most natural inner product is the integral of the pointwise product of the 
functions. We further assume that the functions are piecewise linear, de.ned by interpolating a .nite 
set of patch vertices. This represen­ tation is easily integrated yielding v (j) ii 2 j 8,8= 8kak 8(4) 
k k=1 where v is the number of patch vertices, ak is proportional to the sum of all patch areas adjacent 
to the kth vertex, and 8ki is the radiosity at vertex k due to light i. Under these assumptions, the 
normal equa­tions can be written ATDAw = ATD9 (5) where A is the v 2n matrix of the n vectors 8i, and 
D is the v 2v diagonal matrix diag(a12 ,...,av2) of the weights used for the inner product. With this 
de.nition, II8IIis proportional to the total power leaving all surfaces. Also, changes to the inner product 
are easily expressed as changes to D. 2.2 Solving the Normal Equations The problem now is to solve the 
system of equations from Equa­tion 5. This system contains n equations in n unknowns where n, the number 
of lights, is generally much smaller than the number of vertices in the environment or pixels in the 
image. Let M = AT DA and b = ATD9as in Equation 3. We chose to solve the system Mw = b using a modi.ed 
Gauss-Seidel iteration. There is no guarantee that the solution to the system has only positive entries. 
Simply clipping to zero after convergence is not a viable approach because negative values counteract 
some of the pos­itive energy; ignoring them causes the environment to be too bright. To avoid this dif.culty, 
we modify the Gauss-Seidel algorithm so that negative values are clipped to zero during each iteration. 
On the k + 1 iteration of the modi.ed algorithm, the updated value of wi is  i01(k+1) n (k) bi 0Mijw0Mijw 
(k+1) j=0 jj=i+1 j w= max,0.(6)i Mii Since a zero value does not in.uence other entries of w, we are 
ef­fectively ignoring that light while the iteration is producing a nega­tive value for it. In practice, 
this approach always converges in the sense that the difference between two iterations goes to zero. 
An alternative method may be found in [6]. 3 IMPLEMENTATION Our implementation is based on surface radiance 
functions as op­posed to images. The system is therefore view-independent, solving for light intensities 
that are meaningful in a global sense, not simply for a given view. Although the system does no automatic 
placement of lights, the user may modify light source positions and distribu­tions at any time. However, 
any such change requires that a new solution 8i be computed. To keep these operations fast, we have currently 
limited the solutions to direct illumination from each of the lights, accounting for distance and visibility 
but not secondary re.ections. Similarly we restrict surfaces to be ideal diffuse re.ec­tors. Using more 
complex techniques to .nd the light source func­tions makes moving a light more expensive, but does not 
affect the algorithm. By solving for the intensity of each color channel sepa­rately, the colors are 
determined as well as the intensities. The user modi.es the radiance function of the target by paint­ 
 ing light onto surfaces. We also adjust the matrix D so that painted surfaces have more weight (or more 
area) in the solution, causing the system to try harder to match painted surfaces than unpainted ones. 
This is necessary in complex environments where the large unpainted areas can overwhelm the effect of 
small painted areas. To achieve interactive speeds while painting we use the method introduced by Hanrahan 
and Haeberli [5] to quickly .nd which patch the brush is currently affecting. Object id s and the patch 
uv coordinatesarerenderedintoauxiliarybuffers. Alookupatthepaint brush position in these buffers quickly 
identi.es the patch being painted. Only painted patches are redrawn. Since very few patches change at 
once, updates are easily made in real time. The patch s re.ectance function modi.es the light as it gets 
 painted on a surface. This prevents a surface from being painted with physically unattainable colors. 
For example, a purely red sur­ face cannot be painted blue. The modi.ed light then gets distributed to 
the patch s vertices according to their proximity to the paint brush. We restrict the radiosity at a 
vertex to between zero and one and lin­ early map this to the full dynamic range of the display. The 
system recomputes the closest .tting combination of lights after each brush stroke. All vertices painted 
between a button press and release comprise a stroke. To maintain interactivity, we perform all the updates 
incrementally. Instead of completely rebuilding 9 (the target radiosities) and re-solving, however, we 
only change the elements corresponding to painted vertices and make incremental changes to the inner 
products. If 19is a vector of the changes to the radiosities with p non-zero terms, then bnew = ATD(9+ 
19)= bold + ATD19. (7) Since 19is typically very sparse, we can update b with O(np) op­erations by ignoring 
all zero entries of 19. Since most of the en­vironment hasn t changed, the old intensities provide a 
good ini­tial guess for the modi.ed Gauss-Seidel iteration and it converges quickly. We can similarly 
update the weight (i.e. effective area) of vertices. Consider changing the importance of one vertex. 
Let 1D be the diagonal matrix with its sole non-zero entry being the change in weight of the vertex. 
Then bnew = AT(D + 1D)9= bold + AT1D9. (8) Because 1D has only one non-zero entry, 1D9has only one non­zero 
entry and bold can be updated with O(n) operations. Changing the inner product, though, requires that 
M be updated as well. This can be done incrementally, observing that Mnew = AT(D + 1D)A = Mold + AT1DA. 
(9) Since 1DA has only one non-zero row, we need to look at only one column of AT so we can do the multiplication 
in O(n2) steps. In addition to painting, the user can also interactively move and aim light sources. 
Changing a light requires recomputing the di­rect illumination due to that light. Since A changes, M 
must be re­computed as well; however the cost of recomputing a column of A greatly overshadows the matrix 
multiply used to determine M. Because this can take time for large environments, the user can de­fer 
these computations until all the lights have been satisfactorily placed. The user may also move the camera 
interactively. Because we paint directly onto the geometry, painted surfaces are view­ independent. Also, 
since no directional effects are accounted for, the functions 8i for each light are independent of the 
position and orientation of the camera. Therefore we need not recompute A = I81 1118nIor re-solve for 
the light intensities as a result of moving the camera.  4 RESULTS We tested the system on a moderately 
complex environment con­ sisting of polygonal meshes with about 19,000 polygons, 27,000 vertices, and 
12 lights. Figure 1 shows the user s painted environ­ ment at the top and the system s solution on the 
bottom. A user can see both views at once while working to get immediate feedback on how closely the 
design is being met. Figure 2 shows the same en­ vironment with the same light positions but with different 
painted intensities and colors (left) and a distinct best approximation (mid­ dle). The lighting parameters 
determined by the interactive lighting design were then used to compute a ray traced solution, which 
is shown in Figure 2 (right). The large scale washes of color and illu­ mination levels are captured 
well in the rendered image. The user can quickly and easily modify a design to have a very different 
ap­ pearance. Figure 3 shows the screen during a painting session. The window in which the user paints 
is on the left and the best .t solution is on the right. Some of the support tools for choosing light 
to paint and positioning lights are also shown. In this design, 14 lights were placed in another environment 
of similar complexity.  Figure 1: Design (top) and associated best approximation (bottom). 5 CONCLUSIONS 
AND FUTURE WORK We have created an interactive system to help with lighting design in image synthesis 
by solving a restricted inverse lighting problem. The user paints an approximation of the desired result 
and the sys­tem computes light intensities and colors to match it. This approach can be more intuitive 
and easier to use than the usual direct edit render cycle. Given .xed geometry and a desired target, 
the problem of deter­mining light intensities and colors can be solved in the least squares sense using 
a modi.ed Gauss-Seidel algorithm. The method can be made more interactive by using incremental updates 
to the matrices and vectors involved in the solution process. Magnifying the effect of each brush stroke 
by increasing the weight of the affected vertices allows the user to make changes to the environment 
with relatively little effort. Although they have received little attention in computer graph­ics, inverse 
lighting algorithms have great potential as design tools. Clearly there is much to do beyond automatic 
selection of light source intensities. Automatic light source placement would greatly increase the utility 
of the technique, but will require more elabo­ Figure 2: Design (left); best approximation (middle); 
ray tracing (right). Figure 3: Interactive system. rate optimization methods, as this requires solving 
non-linear con­strained optimization problems. Any rendering technique will work for determining the 
contribu­tions from each of the lights. Our use of direct illumination only was motivated by a desire 
to allow interactive light placement. A more elaborate implementation might compute more accurate solutions 
for those lights that were unlikely to change position or distribution. In order to make the system usable 
for lighting designers, some way of mapping screen intensities to physical units in the system must be 
found. Since the system is being driven by the user s per­ception of what is being painted, the lighting 
conditions of the user s environment must be accounted for, as well as the non-linearities of the monitor, 
the reproduction of color on the monitor, and most im­portantly, the extremely limited dynamic range 
of the monitor.   ACKNOWLEDGEMENTS We would like to thank Jed Lengyel for his helpful comments and 
Kurk Dorsey and Suzanne Smits for their help assembling the pa­per. Much thanks to Matthew Bannister 
who created the model and the lighting designs. This work was supported by the NSF grant Interactive 
Computer Graphics Input and Display Techniques (CCR-8617880), and by the NSF/DARPA Science and Technology 
Center for Computer Graphics and Scienti.c Visualization (ASC­8920219). The authors gratefully acknowledge 
the generous equip­ment grant from Hewlett Packard Corporation on whose worksta­tions the research was 
conducted.  REFERENCES [1] Baltes, H. P., editor. Inverse Source Problems in Optics , Springer-Verlag, 
New York, 1978. [2] Dorsey, Julie O B., Fran¸cois X. Sillion, and Donald P. Green­berg. Design and Simulation 
of Opera Lighting and Pro­jection Effects, in Computer Graphics, 25(4), August 1991, pages 41 50. [3] 
Evans, Ralph M. Eye, Film, and Camera in Color Photogra­phy, John Wiley &#38; Sons, New York, 1959. [4] 
Goral, Cindy M., Kenneth E. Torrance, Donald P. Greenberg, and Bennett Battaile. Modeling the Interaction 
of Light Be­tween Diffuse Surfaces, in Computer Graphics, 18(3), July 1984, pages 213 222. [5] Hanrahan, 
Pat and Paul Haeberli. Direct WYSIWYG Paint­ing and Texturing on 3D Shapes, in Computer Graphics, 24(4), 
August 1990, pages 215 223. [6] Lawson, Charles L. and Hanson Richard J. Solving Least Squares Problems 
, Prentice-Hall, Englewood Cliffs, 1974. [7] Luenberger, David G. Optimization by Vector Space Methods 
, John Wiley &#38; Sons, New York, 1969. [8] Poulin, Pierre and Alain Fournier. Lights from Highlights 
and Shadows, Proceedings of the 1992 Symposium on Interactive 3D Graphics, in Computer Graphics, April 
1992, pages 31 38. [9] Tumblin, Jack and Holly Rushmeier. Tone Reproduction for Realistic Computer Generated 
Images, in Radiosity Course Notes of SIGGRAPH 91, ACM, August 1991, pages 229 257. [10] Whitted, Turner. 
An Improved Illumination Model for Shaded Display, CACM, 32(6), June 1980, pages 343 349. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166136</article_id>
		<sort_key>147</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[Radioptimization]]></title>
		<subtitle><![CDATA[goal based rendering]]></subtitle>
		<page_from>147</page_from>
		<page_to>154</page_to>
		<doi_number>10.1145/166117.166136</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166136</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.6</cat_node>
				<descriptor>Constrained optimization</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor>Computer-aided design (CAD)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010472.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Architecture (buildings)->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003716</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003716</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P144522</person_id>
				<author_profile_id><![CDATA[81539715256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Kawai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39076897</person_id>
				<author_profile_id><![CDATA[81100400443]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Painter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP77032428</person_id>
				<author_profile_id><![CDATA[81406592138]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Cohen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[J. J. Chang and J. D. Carroll. How to use INDSCAL: a computer program for canonical decomposition of N- way tables and individual differences in multidimensional scaling. Technical report, Bell Telephone Laboratories, 1972.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378487</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[M. F. Cohen, S. E. Chen, J. R. Wallace, and D. P. Greenberg. A progressive refinement approach to fast radiosity image generation. Computer Graphics (,gIG- GRAPH '88 Proceedings), 22(4):75-82, July 1988.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325171</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[M. F. Cohen and D. P. Greenberg. The hemi-cube: A radiosity for complex environments. Computer Graphics (SIGGRAPH '85 Proceedings), 19(3):31-40, July 1985.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[J. g. Flynn. A study of subjective responses to low energy and nonuniform lighting systems. Lighting Design and Application, Feb. 1977.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[J. g. Flynn, C. Hendrick, T. J. Spencer, and O. Martyniuk. A guide to methodology procedures for measuring subjective impressions in lighting. Journal of the IES, Jan. 1979.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[J. E. Flynn, T. J. Spencer, O. Martyniuk, and C. Hendrick. Interim study of procedures for investigating the effect of light on impression and behavior. Journal of the IES, Oct. 1973.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[P. E. Green, F. J. Carmone, Jr., and S. M. Smith. Multidimensional Scaling Concepts and Applications. Smith, Allyn, and Bacon, 1989.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122740</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[P. Hanrahan, D. Salzman, and L. Aupperle. A Rapid Hierarchical Radiosity Algorithm. Computer Graphics (SIGGRAPH '91 Proceedings), 25(4):197-206, July 1991.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[J.T. Kajiya. The rendering equation. Computer Graphics (SIGGRAPH '86 Proceedings), 20(4):143-150, Aug. 1986.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[H. N. McKay. Energy optimization and quality lighting design. Lighting Design and Application, Mar. 1986.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[P. Y. Papalambros and D. J. Wilde. Principles of Optimal Design. Cambridge University Press, Cambridge, England, 1988.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147160</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[P. Poulin and A. Fournier. Lights from highlights and shadows. 1992 Symposium on Interactive 3D Graphics, pages 31-38, Mar. 1992.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. Numerical Recipes. Cambridge University Press, New York, 1986.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[J. B. Rosen. The gradient projection method for nonlinear programming, part i: Linear constraints. SIAM, 8:181-217, 1960.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[J. B. Rosen. The gradient projection method for nonlinear programming, part ii: Non-linear constraints. SIAM, 9:514-532, 1961.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[P. C. Sorcar. Architectural Lighting for Commercial Interiors. John Wiley and Sons Inc., 1987.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[S. S. Stevens and J. C. Stevens. Brightness function: Effects of adaptation. Journal of the Optical Society of America, 53(3), Mar. 1963.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[J. Tumblin and H. Rushmeier. Tone reproductions for realistic computer generated images. Technical Report GIT-GVU-91-13, Graphics, Visualization, and Usability Center, Georgia Institute of Technology, July 1991.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Radioptimization Goal Based Rendering John K. Kawai James S. Painter Michael F. Cohen Department of 
Computer Science Department of Computer Science Department of Computer Science University of Utah University 
of Utah Princeton University Abstract This paper presents a method for designing the illumination in 
an environment using optimization techniques applied to a radiosity based image synthesis system. An 
optimization of lighting parameters is performed based on user speci.ed constraints and objectives for 
the illumination of the envi­ronment. The Radioptimization system solves for the best possible settings 
for: light source emissivities, element re.ec­tivities, and spotlight directionality parameters so that 
the design goals, such as to minimize energy or to give the room an impression of privacy , are met. 
The system absorbs much of the burden for searching the design space allow­ing the user to focus on the 
goals of the illumination design rather than the intricate details of a complete lighting spec­i.cation. 
The system employs an object space perceptual model based on work by Tumblin and Rushmeier to account 
for psychophysical e.ects such as subjective brightness and the visual adaptation level of a viewer. 
This provides a higher .delity when comparing the illumination in a computer sim­ulated environment against 
what would be viewed in the real world. Optimization criteria are based on subjective impressions of 
illumination with qualities such as pleasant­ness , and privateness . The qualities were selected based 
on Flynn s work in illuminating engineering. These crite­ria were applied to the radiosity context through 
an experi­ment conducted with subjects viewing rendered images, and the respondents evaluated with a 
Multi-Dimensional Scaling analysis. Introduction Historically, lighting design has been a black art. 
The light­ing designer .rst received a design speci.cation of the cus­tomer s expectations and of the 
room s function. The de­signer then made a lighting lay out and from experience would sketch what the 
room would look like from rough lighting calculations. With the advent of computer aided rendering, this 
process has been simpli.ed allowing the de­signer to model lighting speci.cations with a CAD system and 
have it simulate the lighting calculations giving the de­signer a quick design check of what the room 
would look Permission to copy without fee all or part of this material is granted provided that the 
copies are not made or distributed for directprovided that the copies are not made or distributed for 
direct commercial advantage, the ACM copyright notice and the title of thecommercial advantage, the ACM 
copyright notice and the title of the publication and its date appear, and notice is given that copying 
is bypublication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Machinery. permission of the Association for Computing Machinery. To copy otherwise, or 
to republish, requires a fee and/or specific permission. otherwise, or to republish, requires a fee and/or 
specific permission. &#38;#169;1993 8/93/008/0015 $1.50 &#38;#169;1993 ACM - 0 - 89791 - 601 - 8/93/008 
$1.50 like. This also provides the customer who has no experience with lighting units a realistic preview 
of the .nished room early in the design cycle [16]. Progress in rendering to date has mainly focused 
on improving the realism of the physical simulation and the development of algorithms with faster performance. 
Although great advances have been made in these areas, little work has been done on addressing the de­sign 
problems in creating better quality lighting, except for a few systems that determine lighting placement 
by indicating desired areas of highlights and/or shadow [12]. Lighting designers base their art on the 
belief that spa­tial lighting patterns are a visual communicative medium, in which some patterns of light 
suggest or reinforce shared at­titudes and impressions to people of the same cultural back­ground [5]. 
In addition, the designer must be aware of the need to conserve the electrical energy used in implementing 
their designs. An over-reaction to the wasteful energy con­sumption of the 1960s and 1970s often led 
to buildings which were inadequately lit for their designed purposes, hampering the productivity of the 
residents. A better balance of goals between energy conservation and the quality of the lighting is needed 
[10]. This paper proposes a goal based illumination design ap­proach, that has been termed Radioptimization, 
to help a lighting designer search the space of possible lighting spec­i.cations. Though computers will 
never replace artists, the system may generate con.gurations not previous considered or optimize on an 
already considered con.guration. The ap­proach allows the designer to concentrate on high level goals 
such as visual clarity and specify constraints such as min­imum lighting levels in speci.c locations. 
The system then determines optimal settings for the lighting parameters of the modeled environment by 
searching for the best pos­sible settings for light source emissivities, surface re.ectiv­ities, and 
spotlight directionality. Unconstrained optimiza­tion techniques are employed in conjunction with classical 
radiosity [3, 2, 8] to simulate global illumination. Creating an appropriate two-way link between the 
de­signer and the rendering system requires two important en­hancements to basic rendering methods. First, 
since the de­signer is asked to iteratively evaluate the visual impression from a rendered image, the 
images must provide (as much as possible) a subjective match to a real environment. The work of Tumblin 
and Rushmeier [18] on the psycho-physical quantities of subjective brightness has been applied to map 
luminance values to brightness values to provide higher .­delity for comparing the illumination of a 
computer gener­ated scene. Secondly, the optimization objectives presented to the de­signer are based 
on John Flynn s work [5] whose experiments allowed one to measure impressions of lighting patterns. To 
develop the objective functions, experiments were conducted with subjects viewing computer generated 
images to create a mapping from Flynn s criteria to quanti.able qualities in the radiosity simulations. 
There are three bodies of technology and related literature that are central to the work reported here: 
numerical opti­mization, radiosity based image synthesis, and knowledge about human perception as it 
relates to subjective impres­sions of lighting and to subjective impressions from images presented on 
a CRT. We will brie.y review each of these areas concentrating on the pertinent subtopics in each that 
relate directly to this work. 1.1 Optimization The basic constrained optimization problem is to minimize 
the scalar quantity of an objective function of n system pa­rameters while satisfying a set of constraints. 
Although this is a well researched area, to date there is no computational algorithm for optimization 
which will always .nd the global minimum of a general non-linear objective function. Most methods for 
dealing with constraints transform the constrained problem to an (approximately) equivalent un­constrained 
optimization by either removing the constraints by explicitly solving for one optimization variable, 
or by adding a new function to the objective [14, 15]. In the sim­plest case a constraint can be transformed 
into a penalty function, which when added to the objective returns a high value on a constraint violation. 
Once the constraints are removed or transformed, the problem reduces to .nding a minimum of the objective. 
Most optimization methods are performed iteratively from a starting point, in the multidimensional search 
space. Local information about the value, gradient, and Hessian (matrix of second order partial derivatives) 
of the function is gath­ered and a search direction is selected to move the solution to a new guess. 
One such technique for selecting a search di­rection is Newton s Method which solves for a step direction 
as the inverse of the Hessian times the negative gradient, i.e, .X = -(v2f )-1 ·vf. Although Newton s 
method can have great success, a number of Quasi-Newton methods have been developed to numerically approximate 
the Hessian from a series of gradients for applications where it is either ine.cient or impossible to 
derive the Hessian directly. These include the Broyden-Fletcher-Goldfarb-Shanno (BFGS) method [13, 11], 
which due to non-linearities does a series of one dimen­sional line searches until it converges on a 
local minimum. 1.2 Radiosity Radiosity methods simulate the illumination of Lambertian di.use environments 
by deriving an energy balance equa­tion. Discretizing the environment into a set of elements with an 
assumed functional form, typically a constant value, for the radiosity across the surface, the balance 
of energy be­tween elements is de.ned as through a set of interdependent linear constraints in the form: 
n Bi = Ei + .i Fi,j Bj (1) j=1 where Bi is the radiosity of element i, Ei is the emission of element 
i, .i is the re.ectivity of element i, and Fi,j is the form factor from element i to element j. The form 
factor is the fraction of light leaving one element (i) that arrives at another (j) and is given by: 
 1 cos(fi)cos(fj ) Fi,j = d(pi,pj ) dAidAj Aipr2 pi.Aipj .Aj ij where Ai and Aj are the area of the element 
surfaces, pi and pj are points on elements i and j respectively, d(pi,pj ) returns 1 if pi and pj are 
mutually visible and 0 otherwise, fi is the angle between the normal vector at pi and the vector from 
pi to pj , fj is the angle between the normal vector at pj and the vector from pj to pi, and rij is the 
distance from pi to pj . For an environment of n patches, equaton 1 can be expressed as a set of n simultaneous 
linear equations. This system of equations can be solved numerically by gathering or shooting methods 
[3, 2]. The solution to this system yields the element radiosities, Bi, which can be projected from any 
view point onto the view plane for a .nal image. At .rst glance a direct solution to the radiosity equa­tion 
appears to require at least O(n 2) space and time, given n elements. Hanrahan et al. have shown, however, 
that an equivalent to the form factor matrix can be computed and stored in O(n) space and time by exploiting 
the cohenerent structure of the matrix [8]. Directional lighting e.ects such as spotlights can be added 
to the radiosity equation by replacing the cos(fi) term in the form factor equation with a di.erent distribution 
function: 1 cos(fj ) Fi,j = d(pi,pj )s(fi) dAidAj Aipr2 pi.Aipj .Aj ij where s(fi) is the directionality 
distribution weight for the light source as a function of the angle between the direc­tion vector of 
the light (element i) and the vector between the points pi and pj . Here we restrict ourselves to distribu­tions 
of the form, sn(f)= w(n) cos n(f) for values n>= 1. It is useful to be able to change the beam width 
without a.ecting the total energy emitted by the light. This re­quires a normalization factor, w(n), 
in the emission func­tion sn. The normalization factor w(n) must be chosen so that the total energy emitted 
over the hemisphere is con­stant, independent of n, as the beam width is adjusted. The value of the constant 
is chosen so that w(1) = 1. That is, v snd. = p, where d. is the di.erential solid angle hemisphere on 
the sphere. Carrying out the integration in spherical co­ordinates yields the normalization weight, w(n)=(n+1)/2. 
 1.3 Human Perception 1.3.1 Brightness Brightness is a measure of the subjective sensation produced by 
visible light. Brightness, measured in units of brils, re­lates linearly to human visual response. For 
example, if two light sources are compared and one appears to be twice as bright as the other, the brightness 
of the .rst, in brils, will be twice that of the second. The human eye is sensitive to a luminance range 
of ap­proximately ten orders of magnitude. However, at any one time the eye can only detect a brightness 
range of 100 to 1 with good accuracy. The iris adjusts, limiting the amount of light entering the eye, 
in order to seek a state of equilibrium that is appropriate for the general bright­ness conditions. Tumblin 
and Rushmeier [18] studied work by Stevens [17] who theorized that the adaptation level of a scene can 
be estimated by the expected value (mean) of the log10 of the luminances visible on the retina, i.e., 
EXPp.retina{log10(L(p))} where L(p) is the luminance at a point p on the retina. Miller et al. also theorized 
that di.ering adaptations of the eye result in a family of curves relating luminance and brightness values 
in the form, log10(P )= aa * log10(L)+ bb where P is the brightness value speci.ed in brils, L is the 
luminance value speci.ed in nits, bb is - 0.4(log10(Lw))2 +(-2.58log10(Lw)) + 2.02, aa is 0.4log10(Lw)+2.92, 
and Lw is the white adapt­ing luminance which can be approximated by the equation log10(Lw) = EXP{log10(Li)} 
+0.84. This perceptual model accepts luminance values in units of nits which in photometric units are 
related to lux on a di.use surface by, 1 lux = 1 nit / 10, 000. Thus solving for brils in terms of an 
element radiosity of B lux yields: 10aa*log10(B/10,000)+bb P = (2) Since the adaptation of the eye is 
a.ected only by what is visible to the retina, perceptual processing is usually done as a view dependent 
process in screen space. This assumes that the viewer adapts to a single view rather than to an entire 
environment. In practice, we are constantly moving our head and eyes to scan a room and hence adapt to 
the overall room lighting rather than to a single view. In our work we propose a view independent approach 
to lighting design, since the designer s goal is to optimize on the overall impression of a room rather 
than a particular view of the room. Therefore, the conversion from luminance units into perceptual units 
is performed in object space. Each element is considered to contribute to the adaptation proportional 
to its physical size. This neglects the view dependent e.ects of perspective foreshortening and occlusion 
but has the advantage that it yields view independent results. We have found that the object space, view 
independent, method gives results that are nearly identical to view dependent screen space methods for 
typical, single room, architectural models. In addition to the view independence, calculating perception 
in object space has the added advantage of faster performance if the number of elements is much smaller 
than the number of screen pixels. 1.3.2 Subjective Impressions of Illumination In the 1970 s, John Flynn 
published a series of articles [6, 4, 5], introducing a methodology with which to quan­tify parameters 
that elicit a shared human behavioral re­sponse and subjective impression. In particular, Flynn ex­amined 
how non-uniform, peripheral, and bright lighting af­fects impressions of visual clarity, spaciousness, 
relaxation, and privacy. Flynn created six di.erent light settings for a conference room and subjectively 
associated each room with a non-uniform, peripheral, and brightness value so that each room corresponded 
to a point in a 3 dimensional space of the di.erent lighting characteristics. Flynn also associ­ated 
a set of semantic di.erential (SD) rating scales such as large-small and spacious-cramped with each category 
of impression. Test subjects were then asked to make pair wise comparisons of the di.erences between 
each room from the set of SD rating scales where 0 meant no di.erence and 10 meant a large di.erence. 
The data gathered resulted in a 6x6 symmetric dissimi­larity matrix comparing the 6 rooms for each subject 
tested and each SD comparison made, e.g. large-small. The mul­tidimensional scaling program INDSCAL [1, 
7], was used to determine how each subjected weighted the non-uniformity, peripheral and brightness values 
in making each SD com­parison. A weighting of each dimension for each subject was determined that best 
.t the data. The results showed a correlation between the room positions hypothesized by Flynn and the 
positions computed by INDSCAL, support­ing Flynn s hypothesis that brightness, non-uniformity, and peripheral 
lighting reinforce particular impressions. In ad­dition, there also was a correlation for the weights 
for each parameter among all the subjects, supporting the concept that particular lighting patterns elicit 
a shared impression. By this process, Flynn was not only able to demonstrate that there is a de.nite 
correlation between the measurable quantities (non-uniform, peripheral, and bright lighting) and the 
subjective impressions (visual clarity, spaciousness, and relaxation), but was able to quantify how much 
each of the measurable dimensions a.ects each subjective impression. As described shortly, we have adapted 
this work through an additional level of experimentation in which subjects re­ported impressions from 
computer generated images. 2 Problem Formulation To pose the illumination design task as a constrained 
opti­mization problem we must identify: the variables involved in the optimization process, the constraints 
that must be satis.ed, and the objective function.  2.1 Optimization Variables In a normal radiosity 
based renderer, the element radiosi­ties Bi are the unknowns to be computed in terms of .xed material 
and light property parameters. In the optimiza­tion setting the material and light properties are no 
longer .xed and must also be considered as variables. Constraints may be imposed on any of these variables 
and the objective function may involve any or all of them. In the illumination design problem the optimization 
vari­ables are light source speci.cation parameters (emissions, spotlight directions, spotlight focus), 
element radiosities, Bi, and element re.ectivities, .i. Two types of light sources are considered: di.usely 
emitting elements described by a sin­gle emissivity parameter Ei, and directional lights idealized as 
spotlights described by a position, direction, and a cos n directional distribution. Light source positions 
are assumed to be .xed and only the direction and distribution pattern is allowed to change during optimization. 
Every light source emission Ei, light direction vector Vi, cosine distribution exponent ni, element radiosity 
Bi and re.ectivity .i, has the potential to be a variable in the op­timization problem. If all are treated 
explicitly as domain variables in the optimization an intractably large system will result. Fortunately, 
the Bi s can be eliminated by direct sub­stitution of the radiosity equation, and typically only a small 
number of the elements will have variable emission, re.ectiv­ity or directionality parameters. These 
remaining variables are called the free variables of the optimization problem. 2.2 Constraints Constraints 
fall into three categories. Physical constraints specify the relationships between light emission and 
element radiosities that are dictated by the physics of light transport. The constraints are captured 
in the rendering equation [9]. We assume perfect di.use surfaces and a discretized environment yielding 
the radios­ity approximation given in equation 1. Design goals are constraints provided by the user. 
These may be either equality or inequality constraints and may apply to a single element, or a conglomeration 
of elements. For example, the requirement that a particular element s radiosity is a given constant, 
Bi = K for some constant K is an equality constraint on a single element that expresses a .xed radiosity 
for the element. Inequality constraints such as Klow = Bi = Khigh can also be speci.ed (in essence two 
inequality constraints) requiring the radiosity of element i to stay within the bounds Klow and Khigh. 
Barrier constraints are hard bounds on the allowable ranges of the optimization variables that must be 
satis.ed to insure that the model is physically realizable. For example, light emissions must remain 
positive and element re.ectivi­ties must remain in the range 0 <= .i <= 1. Barrier con­straints are conceptually 
similar to inequality design goals. The main di.erence is that a barrier constraint must be sat­is.ed 
in order to produce a valid model. Design goals are desires that need not be satis.ed exactly. 2.3 Objective 
Function In general, radioptimization problems are under­constrained. There may be an in.nite number 
of possible solutions that satisfy the problem constraints. The objec­tive function is used to select 
between the many possible solutions. The simplest, directly measurable objective is the minimization 
of energy, fenergy = i BiAi. In theory, any user speci.ed function of the optimization variables could 
be used as an objective function. An al­ternative is to provide a .xed library of objective functions 
and allow the user to construct an objective function via linear combinations of the library functions. 
Each individ­ual objective function in the library has a well de.ned and intuitive behavior. The user 
can then control the weights of the individual objectives to determine the .nal objective function. This 
allows user control without an undo amount of complexity. A variation of Flynn s work, described in the 
previous section, was used to develop a way of quantifying subjective impressions. Flynn s experiment 
was duplicated except, in­stead of having the subjects judge actual rooms with di.er­ent lighting characteristics, 
they were shown rendered im­ages of an identical room with di.erent light patterns (see .gure 4). Once 
the data set was collected, it was processed by INDSCAL with the brightness, non-uniform, and periph­eral 
values for each room computed by the following func­tions: i.. PiAi fbrightness(P, A)= - . i.. Ai .! 
.1 2 i..(Pavg,i-Pi)2Ai fnon-uniform(P, A)= - . i.. Ai) PiAi PiAi i.µi.. fperipheral(P, A)= ! - ! Ai 
Ai i.µi.. where . is the set of all elements in the environment, . is the set of elements that make up 
the walls, µ is the set of all horizontally oriented elements, Pi is the brightness of element i, Ai 
is the area of element i, and Pavg,i is the average brightness of the elements around element i. The 
functions are de.ned in terms of perceptual values because humans subjectively quantify illumination 
by brightness not by actual luminance. The results from INDSCAL showed that there was a cor­relation 
among those tested in the relationship between the measurable quantities, brightness, non-uniform, and 
periph­eral lighting, and the subjective impressions of visual clarity, privacy, and pleasantness. A 
linear transformation was .t to the INDSCAL data resulting in linear relationships between the subjective 
impressions and the measured values: fclear =0.90 · fbrightness - 0.38 · fnon-uniform - 0.58 · fperipheral 
fpleasant =0.78 · fbrightness - 0.53 · fnon-uniform + 0.24 · fperipheral fprivate =0.90 · fbrightness 
+0.32 · fnon-uniform - 0.09 · fperipheral  2.4 Conversion of the Constrained Problem to an Unconstrained 
Problem The design goal constraints can be included in the objec­tive function through the penalty method 
[11] by penalizing deviations from constraints through explicit terms in the ob­jective function. The 
penalty imposed on the objective is de­.ned as the square of the constraint violation. For example, if 
the jth constraint, Cj , is an equality constraint specifying a particular radiosity1 to be a given constant, 
(Bij = Kj ), this will result in a penalty term fCj in the cost function given by fCj = Aij (Kj - Bij 
)2 . Inequality constraints can be handled through a penalty function that turns on when the constraint 
is not satis.ed. For example, the inequality constraint Cj given by (Bij <Kj ) results in a penalty term 
fCj = Aij (Kj - Bij )2 when Bij is greater than Kj and is zero otherwise. Barrier Constraints are handled 
in a similar fashion to impose hard physical restrictions on certain values, for ex­ample, the emission 
variables must always remain positive. Similarly, re.ectivities must remain between 0 and 1. A bar­rier 
term is added to the objective function for each barrier constraint to avoid violations of these constraints. 
The bar­rier constraint Gj given by (Xj >Kj ) for some free variable Xj results in a barrier term fGj 
=(Xj -Kj )-4 for Xj >Kj . In addition, the optimization search explicitly enforces the constraint (Xj 
>Kj ) by clamping the Xj to Kj + . when Xj drops below Kj , where . is a small positive constant. This 
will yield a large barrier term in the objective function tending to lead the search away from the barrier 
in the next iteration. The remaining constraints are the physical constraints speci.ed by the radiosity 
equation (equation 1). These are dealt with by direct substitution. The radiosity equation implicitly 
de.nes each Bi in terms of all the E, V, n and . s. The Bi s are calculated via a radiosity solution 
algorithm [8]. The values for the Pi s can then be computed directly from the Bi s by equation 2. The 
Bi and Pi values can be directly substituted into the objective function. This e.ectively eliminates 
all the Bi s and Pi s from the set of optimization domain variables. Thus the modi.ed optimization problem 
is given by: 1Bij indicates the radiosity of the ith element, where i was selected by the jth constraint, 
Cj . f(X)= Wenergy fenergy + (3) Wbrightness fbrightness + Wnon-uniform fnon-uniform + Wperipheral fperipheral 
+ Wclear fclear + Wpleasant fpleasant + Wprivate fprivate + ! Wdesigngoals !j fCj + j fGj where X is 
a point in the multidimensional space spanned by the remaining free variables, Ei, Vi, ni, and .i. Through 
the use of the penalty method, barrier func­tions, and substitution of physics constraints, the optimiza­tion 
problem can now be stated as a simple unconstrained, multidimensional minimization problem. Let X be 
a multi­dimensional vector in the design space , the space spanned by the free variables in the design. 
We must identify a point in the design space, X*, such that the objective function f(X*) is (at least 
locally) minimized. There are many solu­tion methods for such a minimization problem. We use the well 
known BFGS method described above [13]. 3 Implementation The user provides an initial model that is rendered 
to pro­vide a baseline rendering. The user can select elements in­teractively from an image generated 
from the baseline so­lution to specify the free variables in the optimization pro­cess. The user can 
also specify the objective function weights Wenergy,Wbrightness,Wnon-uniform, etc. to direct the opti­mization 
process. After all the design goals and objective weights are speci.ed, the optimization process is run 
until convergence is achieved. This process can be described in Pseudo code by: Compute baseline rendering. 
Establish constraints and objectives. REPEAT Evaluate partial derivatives. Compute search direction .X 
using BFGS. Perform line search in the direction .X. Display results, and allow user to modify constraints 
and objectives UNTIL convergence. 3.1 Baseline rendering The initial model is rendered and displayed 
by the hierarchi­cal radiosity solution algorithm of Hanrahan et al. [8]. Dur­ing baseline rendering, 
the input model is subdivided into a hierarchical structure and links are established between nodes in 
the hierarchy to establish the block structured form factor matrix as described in [8]. 3.2 Establishing 
Constraints and Objectives Once an image is displayed the user can select elements di­rectly from the 
screen with the mouse and set constraints via the user interface shown in .gure 5. In this example, the 
desk top has been selected as indicated by the green outline. Current illumination information for the 
selected element is displayed in the lower right corner of the interface. Through a set of buttons in 
the interface, the user can elect to impose a constraint on the element radiosity, and/or specify that 
the element re.ectivity or emission should be a free variable in the optimization process. Spot lights 
are handled with a similar interface that allows the light direction vector and/or distribution parameter 
n to be marked as free variables in the optimization. The objective function weights can also be adjusted 
with slider bars in this interface. 3.3 Partial Derivative Estimation Evaluation of partial derivatives 
of the modi.ed objective with respect to each free variables is required by the op­timization process. 
For example, to compute the partial derivative of the objective function with respect to a light emission, 
Ek, we must evaluate: ! .f/.Ek = Wenergy j .Ej /.Ek Aj + Wbrightness .fbrightness/.Ek + Wnon-uniform 
.fnon-uniform/.Ek + Wperipheral .fperipheral/.Ek + Wclear .fclear/.Ek + Wpleasant .fpleasant/.Ek + Wprivate 
.fprivate/.Ek + ! Wdesign . !j fCj /.Ek + . j fGj /.Ek (4) The partial derivative of the constraint function 
fCj for .fCjan equality constraint Cj :(Bij = Kj ) is: .Ek = -2Aij · .Bij (Kj - Bij ) .Ek . For an inequality 
constraint, the par­tial .fCj /.Ek is zero when the constraint is satis.ed and is given by the above 
equation otherwise. The partial of a barrier function fGj can also be expressed directly as: .fGj = -4(Ej 
- Gj )-5 .Ej . .Ek .Ek The partials of the form .Ej /.Ek are 1 if j = k and zero otherwise. The partials 
in the form .Bj /.Ek repre­sent the in.uence that the free variable Ek has on each element radiosity 
Bj . These in.uence factors are equivalent to entries in the inverse of the form factor matrix. Once 
the in.uence factors are known, the scene can be rerendered with new light source emissivities without 
resolving the ra­diosity equations. Besides providing the partial derivatives necessary for the optimization 
process, explicit storage of the in.uence factors also allows interactive, near real time, user adjustments 
to the lighting. Rather than perform an explicit inversion of the block structured system, the partial 
derivatives can be estimated .E or ...k ..j ..j .E or ..  Figure 1: Estimation of .Bj /.Ek or .Bj /..k 
by shooting a delta emission from source k. k - cosnkfvk) .Bj .V j Figure 2: Estimation of .Bj /.Vk 
by shooting a delta emis­sion from source k. nk+1 cosnkfvkk 2 nk+.n+1 nk+1 cosnk+.nfvk- cosnkfvk) 22 
.Bj .n nk+.n+1 j 2 cosnk+.nfvk Figure 3: Estimation of .Bj /.nk by shooting a delta emis­sion from source 
k. by .nite di.erences. A small delta emission, .E, is shot from the variable emission light source as 
indicated in .g­ure 1 and allowed to interre.ect. The iterative shooting operations are very rapid since 
the links representing the form factors are precomputed during the baseline rendering. The result of 
shooting a small amount of energy through the network of links results in an e.ect on each element radiosity, 
.Bj , thus providing all the derivative estimates .Bj /.E. If the only free variables in the optimization 
are light emissions, these in.uence factors need only be eval­uated once, due to linearity. On the other 
hand, if any spotlight directionality or element re.ectance is allowed to be variable, light emission 
in.uence factors must be updated each iteration. The partial derivative of the objective with respect 
to a variable element re.ectivity is handled in a similar fashion. The element re.ectivity .k is adjusted 
by a small delta ... The e.ect on all other elements can be evaluated by shoot­ing the unshot radiosity 
due to the change in re.ectivity: Bk... As with light sources, several shooting iterations may be necessary 
to account for multiple bounce e.ects. Once convergence has been achieved, the e.ect of .. on element 
radiosity .Bj is available and the in.uence factor estimate .Bj /.. can be recorded. In.uence factors 
for spotlight directionality variables, Vk and nk, are also approximated through .nite di.erences. For 
example, a small change, .V, can be made to the di­rection vector Vk and the e.ect on each element radiosity 
can be determined by a series of shooting steps. The .rst shooting step, illustrated in .gure 2, shoots 
a delta emission from the modi.ed spotlight to all other elements. The delta emission is determined according 
to the change in the direc­ (nk+1) tionality parameter, in this case, Ek 2 (cos nk (fvk+.v)- cos nk (fvk 
)) where fvk is the angle between the original di­rection vector of the light and the direction of the 
element and fvk+.v is the angle between the new spotlight direc­tion vector and the direction of the 
element. Subsequent shooting steps proceed in the normal fashion in order to handle multiple bounce e.ects. 
The same technique can be used when the distribution pattern parameter nk is changed as illustrated in 
.gure 3. In this case the radiosity cast is (nk+.n+1) ) - (nk+1) Ek(2 cos nk (fvk2 cos nk (fvk)). The 
cost functions that measure patterns of light or sub­jective impressions are de.ned in terms of perception. 
The partial derivatives of the functions examining lighting pat­terns with respect to light emission 
Ek are: ! .Pi Ai .fbrightness i .Ei = - ! .Ek i Ai ! 2 - 1 2 .fnon-uniform (Pavg,i - Pi)Ai = -i ! * .Ek 
Ai i . . ! .Pavg,i - .Pi i .Ek .Ek (Pavg,i - Pi) .! . i Ai !! .Pi .Pj Ai Aj .fperipheral i .Ek j .Ek 
=! - ! .Eki Aij Aj The partials of the subjective impressions are just a lin­ear combination of the partial 
derivatives of fbrightness, fnon-uniform, and fperipheral. The partials .Pj /.Ek are derived by di.erentiating 
equa­tion 2 giving, .Pj aa .Bj .a = 10.+ .(5) .Ek Bj .Ek .Ek where a is the adaption level which can 
be approx­ !! imated by ( i log10(Bi/10, 000)Ai)/ i Ai, . is aa * log10(Bi/10, 000) + bb, and . is 0.4log10(Bi/10, 
000) - ln(10)(0.8a +2.6). If the the adaptation level is assumed constant with re­spect to a change in 
emission Ek, .a/.Ek = 0, otherwise .a Aj .Bj = ! .Ek Bj ln(10) i(Ai) .Ek  3.4 Optimization The optimization 
process uses the BFGS algorithm, which evaluates the objective function and gradient at a current step 
in the design space in order to compute a search di­rection. Once a search direction is derived, a line 
search is performed in this direction. Each step in the line search involves a reevaluation of the objective 
function, hence a reevaluation of the element radiosities which are displayed, allowing the user to watch 
the progress of the optimization. This process is repeated until the system has converged to a minimum. 
4 Experiences and Results The .rst implementation of the Radioptimization system al­lowed an objective 
function based only on photometric mea­sures and did not take into account the psychophysical prop­erties 
of lighting. The system could successfully optimize lighting but required quite a bit of unintuitive 
tweaking of the objective function weights in order to achieve light­ing that had the right subjective 
appearance. These early experiences led to the investigation of the psychophysical objective functions. 
Figure 6 shows the e.ects that the subjective impressions have on an optimization. The top image constrains 
the table to have a small amount of illumination while conserving en­ergy and creating an overall impression 
of visual clarity. To improve e.ciency the optimization was run at a low resolu­tion on a simpli.ed model, 
without the chairs and television set. The optimization process took 1 minute and 21 seconds on an IBM 
Model 550 RISC System 6000. The bottom im­age has the same design goals as the top image except that 
it tries to elicit an impression of privateness. This optimiza­tion took 2 minutes and 11 seconds. It 
took two or three hours of performing design iterations before developing an intuitive feel for the optimization 
process and the e.ects of the weights on the objective func­tion. One of the problems with the design 
cycle is that there may be local minima of the speci.ed objective that are vi­sually unattractive. For 
example, in addition to the design goals mentioned above for .gure 6, we needed to add an additional 
constraint limiting the illumination of the ceil­ing because pointing the lights directly at the ceiling 
was an optimal way of increasing the overall brightness of the room. One drawback of the system at this 
point is that it is not fast enough to allow a highly interactive feedback cycle for complex models. 
However since the system allows a designer to think in terms of their own design goals, it requires fewer 
design iterations to achieve the desired result. 5 Conclusions This paper has presented a new method 
of designing illumi­nation in a computer simulated environment, based on goal directed modeling. A library 
of functions were developed that approximate a room s success in meeting certain light­ing design goals 
such as minimizing energy or evoking an im­pression of privacy. The objective functions were developed 
through an experiment in which subjects ordered a set of im­ages according to a particular impression. 
Processing this data with INDSCAL, showed a correlation between quanti­tative lighting patterns and subjective 
measures of visually clarity, pleasantness, and privacy. Once the lighting design goals have been set, 
the software system searches the space of lighting con.gurations for the illumination pattern that best 
meets the design speci.cations. The system absorbs much of the burden for searching the design space 
allow­ing the user to focus on the goals of the illumination design rather than the intricate details 
of a complete illumination speci.cation. The radioptimization system explores only one possi­ble path 
in the application of optimization techniques to image synthesis design problems. Constrained optimiza­tion 
techniques may be more suitable than the uncon­strained penalty method technique used here when the de­sign 
goals must be satis.ed precisely. Discrete optimization methods may be appropriate in some instances, 
for exam­ple when emissivities are constrained to a .nite set, e.g. {60 Watts , 100 Watts , · · ·}. Geometric 
properties of the model, such as the position of the lights or the size and po­sition of the windows, 
could be allowed as free variables. More general image synthesis methods could be applied to account 
for non-di.use e.ects such as glare. Acknowledgments Pat Hanrahan, Larry Aupperle and David Salzman pro­vided 
the radiosity software used as a basis for this work. Greg Ward o.ered suggestions on useful objective 
functions for lighting design. Shinichi Kasahara participated in dis­cussions about the work as it developed. 
The .rst and second author s work was supported by NFS (CCR-9210587). All opinions, .ndings, conclusions, 
or rec­ommendations expressed in this document are those of the authors and do not necessarily re.ect 
the views of the spon­soring agencies. References [1] J. J. Chang and J. D. Carroll. How to use INDSCAL: 
a computer program for canonical decomposition of N­way tables and individual di.erences in multidimen­sional 
scaling. Technical report, Bell Telephone Labo­ratories, 1972. [2] M. F. Cohen, S. E. Chen, J. R. Wallace, 
and D. P. Greenberg. A progressive re.nement approach to fast radiosity image generation. Computer Graphics 
(SIG-GRAPH 88 Proceedings), 22(4):75 82, July 1988. [3] M. F. Cohen and D. P. Greenberg. The hemi-cube: 
A radiosity for complex environments. Computer Graph­ics (SIGGRAPH 85 Proceedings), 19(3):31 40, July 
1985. [4] J. E. Flynn. A study of subjective responses to low en­ergy and nonuniform lighting systems. 
Lighting Design and Application, Feb. 1977. [5] J. E. Flynn, C. Hendrick, T. J. Spencer, and O. Mar­tyniuk. 
A guide to methodology procedures for mea­suring subjective impressions in lighting. Journal of the IES, 
Jan. 1979. [6] J. E. Flynn, T. J. Spencer, O. Martyniuk, and C. Hen­drick. Interim study of procedures 
for investigating the e.ect of light on impression and behavior. Journal of the IES, Oct. 1973. [7] P. 
E. Green, F. J. Carmone, Jr., and S. M. Smith. Multidimensional Scaling Concepts and Applications. Smith, 
Allyn, and Bacon, 1989. [8] P. Hanrahan, D. Salzman, and L. Aupperle. A Rapid Hierarchical Radiosity 
Algorithm. Computer Graph­ics (SIGGRAPH 91 Proceedings), 25(4):197 206, July 1991. [9] J. T. Kajiya. 
The rendering equation. Computer Graph­ics (SIGGRAPH 86 Proceedings), 20(4):143 150, Aug. 1986. [10] 
H. N. McKay. Energy optimization and quality lighting design. Lighting Design and Application, Mar. 1986. 
[11] P. Y. Papalambros and D. J. Wilde. Principles of Op­timal Design. Cambridge University Press, Cambridge, 
England, 1988. [13] W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. Numerical Recipes. 
Cambridge University Press, New York, 1986. [14] J. B. Rosen. The gradient projection method for non­linear 
programming, part i: Linear constraints. SIAM, 8:181 217, 1960. [15] J. B. Rosen. The gradient projection 
method for non­linear programming, part ii: Non-linear constraints. SIAM, 9:514 532, 1961. [16] P. C. 
Sorcar. Architectural Lighting for Commercial Interiors. John Wiley and Sons Inc., 1987. [17] S. S. Stevens 
and J. C. Stevens. Brightness function: E.ects of adaptation. Journal of the Optical Society of America, 
53(3), Mar. 1963. [18] J. Tumblin and H. Rushmeier. Tone reproductions for realistic computer generated 
images. Technical Report GIT-GVU-91-13, Graphics, Visualization, and Usabil­ity Center, Georgia Institute 
of Technology, July 1991.  Figure 5: Sample interface which allows the user to set the weights of the 
objective and/or specify constraints.  Figure 6: The top image constrains the table to have a small 
Figure 4: Computer generated rooms used to test subjects amount of illumination while preserving energy 
and creat­on which illumination patterns illicit particular subjective ing an overall impression of visual 
clarity. The bottom image impressions. also constrains the table to have a small amount of illumi­nation 
while preserving energy. In addition, it trys to create a feeling of privacy.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166137</article_id>
		<sort_key>155</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[A hierarchical illumination algorithm for surfaces with glossy reflection]]></title>
		<page_from>155</page_from>
		<page_to>162</page_to>
		<doi_number>10.1145/166117.166137</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166137</url>
		<keywords>
			<kw><![CDATA[adaptive meshing]]></kw>
			<kw><![CDATA[global illumination]]></kw>
			<kw><![CDATA[radiosity]]></kw>
			<kw><![CDATA[raytracing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P168375</person_id>
				<author_profile_id><![CDATA[81100289210]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Larry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aupperle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15033698</person_id>
				<author_profile_id><![CDATA[81100482576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanrahan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>155305</ref_obj_id>
				<ref_obj_pid>155294</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Amanatides, J. (1992) Algorithms for the detection and elimination of specular aliasing. Proc. Graphics Intelface '92, 86-93.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>193993</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Aupperle, L. (1993) Hierarchical algorithms for illumination. Doctoral Dissertation, Princeton University.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563893</ref_obj_id>
				<ref_obj_pid>965141</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Blinn, J.F. (1977) Models of light refection for computer synthesized pictures. Computer Graphics 11 (2), 192-198.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122737</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Chen, S.E., Rushmeier, H.E., Miller, G., Turner, D. (1991) A progressive multi-pass method for global illumination. Computer Graphics 25 (4), 165-174.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>8927</ref_obj_id>
				<ref_obj_pid>7529</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Cook, R.L. (1986) Stochastic sampling in computer graphics. ACM Transactions on Graphics 5 (1), 51-72.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>63450</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Hall, R. (1989) Illumination and color in computer generated imagery. Springer-Verlag, New York.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122740</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Hanrahan, E, Salzman, D., Aupperle, L. (1991) A rapid hierarchical radiosity algorithm. Computer Graphics 25 (4), 197-206.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15901</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Immel, D.S., Cohen, M.F., Greenberg, D.E (1986) A radiosity method for non-diffuse environments. Computer Graphics 20 (4), 133-142.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Kajiya, J.T. (1986) The rendering equation. Computer Graphics 20 (4), 143-150.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Mitchell, D. (1992) Manuscript.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Nicodemus, EE., Richmond, J.C., Hsia, J.J., Ginsberg, I.W., Limperis, T. (1977) Geometrical considerations and nomenclature for reflectance. National Bureau of Standards monograph, no. 160.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>93316</ref_obj_id>
				<ref_obj_pid>93267</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Shirley, E (1990) A ray tracing method for illumination calculation in diffuse-specular scenes. Proc. Graphics Interace '90, 205-212.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122739</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Sillion, EX., Arvo, J.R., Westin, S.H., Greenberg, D.E (1991) A global illumination solution for general reflectance distributions. Computer Graphics 25 (4), 187-196.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134080</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Smits, B.E., Arvo, J.R., Salesin, D.H. (1992) An importancedriven radiosity algorithm. Computer Graphics 26 (2), 273-282.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Torrance, K.E., Sparrow, E.M. (1967) Theory for off-specular reflection from roughened surfaces. J. of the Optical Society of America 57 (9), 1105-1114.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378490</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Ward, G.J., Rubinstein, F.M., Clear, R.D. (1988) A ray tracing solution for diffuse environments. Computer Graphics 22 (3), 85- 92.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Hierarchical Illumination Algorithm for Surfaces with Glossy Re.ection Larry Aupperle Pat Hanrahan 
Department of Computer Science Princeton University Abstract We develop a radiance formulation for discrete 
three point transport, and a new measure and description of re.ectance:area re.ectance. This formulation 
and associated re.ectance allow an estimate of er­ror in the computation of radiance across triples of 
surface elements, and lead directly to a hierarchical re.nement algorithm for global illumination. We 
have implemented and analyzed this algorithm over surfaces exhibiting glossy specular and diffuse re.ection. 
Theoretical growth in light transport computation is shown to befor suf.cient nO e n i kk 3 h re.nement, 
where is the number of elements at the .nest level of subdivision over an environment consisting ofinput 
polygonal patches this growth is exhibited in experimental trials. Naive application of three point 
transport would require computation overO e n 3 h element-triple interactions. CR Categories and Subject 
Descriptors: I.3.7 [Computer Graph­ ics]: Three-Dimensional Graphics and Realism. Key Words: adaptive 
meshing, global illumination, radiosity, ray tracing.  1 Introduction A major open problem in image 
synthesis is the ef.cient solution of the rendering equation. Radiosity methods have been quite success­ful 
over environments containing surfaces that exhibit only diffuse re.ection. Unfortunately, very few materials 
are purely Lamber­tian re.ectors, and ef.cient solution techniques have not yet been developed for more 
general specular or glossy re.ection functions. The rendering equation is an integral equation, and the 
solutions to complicated integral equations are generally obtained using either Monte Carlo or .nite 
element techniques. Monte Carlo algorithms sometimes go under the name of distributed or stochastic ray 
tracing and are the most commonly employed in computer graphics (e.g. see [4, 5, 9, 12, 16]). Monte Carlo 
techniques have the advantage that they are easy to implement and can be used for complicated geometries 
and re.ection functions. Unfortunately, their disadvan­tage is that they are notoriously inef.cient. 
The second approach, the .nite element method, has been very successfully applied to the rendering equation 
under the radiosity assumption, but has only begun to be employed in the general case, and with limited 
success. For example, Immel et al. [8] discretized radiance into a lattice of cubical environment maps, 
and solved the resulting system. More recently, Sillion et al. [13] used a mesh of spherical harmonic 
func­tions to represent radiance, and solved the resulting system using a shooting algorithm. Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct provided that the copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the commercial advantage, the ACM copyright notice and the 
title of the publication and its date appear, and notice is given that copying is by publication and 
its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy permission of the Association for Computing Machinery. otherwise, or to republish, requires 
a fee and/or specific permission. otherwise, or to republish, requires a fee and/or specific permission. 
&#38;#169;1993 ACM -0 -89791 -601 -8/93/008 $1.50 &#38;#169;1993 ----8/93/008/0015 $1.50 There are many 
ways to parameterize the rendering equation, and each leads to a different choice of basis functions. 
In the transport theory community two techniques are common: directional sub­division (the method of 
discrete ordinates or ), and spherical P N S N harmonics (). These two techniques roughly correspond 
to the methods of Immel et al. and Sillion et al., although many interest­ing variations are possible. 
Our approach is somewhat different, and based on Kajiya s original formulation of the rendering equation 
[9]. Under this formulation, the rendering equation is expressed in terms of three point transport. That 
is, the kernel of the integral expresses the transport of light from a point on the source to a point 
on the receiver, via a point on a re.ector. Given this formulation, the three point rendering equation 
can be discretized over pairs of elements to form a linear system of equations. Solving this system yields 
the radiance transported between elements. Note that this approach is very similar to the radiosity formulation. 
The problem with .nite element methods is that the matrix of interactions is very large for interesting 
environments. For a given environment of input polygonal patches containing elements k 3 n at the .nest 
level of re.nement, the three point discretization that we are proposing generates an matrix of interactions. 
However, in this paper we show that we can accurately approximate the O e n i nk 3 h n 3 re.ectance matrix 
with blocks, in a way very similar to our recent hierarchical radiosity algorithm [7]. In that paper 
we showed how the form factor matrix could be approximated withO e n i k 2 h n 2 blocks, resulting in 
a very ef.cient algorithm in both space and time. Although the results presented in this paper are preliminary, 
we believe a hierarchical .nite element approach along these lines will ultimately lead to a fast, ef.cient 
algorithm. In the following section we describe our application of the .­nite element element method 
to the three point rendering equation, yielding a radiance formulation for discrete transport. In Section 
3 we present a simple adaptive re.nement algorithm for computation over this formulation, and the iterative 
solution technique employed for the actual calculation of transport. In Section 4 we discuss our implementation 
of the algorithm over glossy re.ection, and in Sec­tion 5 we present some experiments and results. An 
appendix to this paper contains details of our error analysis for discrete transport under the glossy 
model. 2 Discrete Three Point Transport The algorithm presented in this paper operates through two func­tions: 
re.nement of the environment to form a hierarchy of discrete interactions, patches and elements, and 
the actual computation of illumination over this hierarchy. In this section we develop the basis for 
both discretization and transport. We derive a radiance formulation for three point transport, and a 
new measure and description of re.ectance, area re.ectance. This radiance formulation and associated 
re.ectance provide a natu­ral criterion for discretization under illumination and re.ection, and allow 
both the computation of radiance across triples of individual surface elements, and the expression and 
computation of all light transport over all surfaces. 2.2 Area Re.ectance The quantity has a natural 
and satisfying physical signi.cance it is an expression of re.ectance over areas , , and . Consider the 
fraction of the radiant .ux transported from incident to that is re.ected in the direction of area : 
R A i R A j R A k R j f krj e i R x A i x R 0 A x j 00 L he Lx e x x 0 x h 0 G h G e x e x x 0 x h dx 
0 h G 0 Adx e x i 0 Ax j 00 A h dx k 00 Adx k 0 dxA i If we assume that incident radiance is uniform 
and isotropic over both (as induced by ) and , we may divide through by , yielding: L e jx e Ax i 0 i 
0 h R A Aj i j R A e A A k j i h R i A Aj k fA rk e R x h A A i i x R 0 A x j 00 G h AG e x j e x x 0 
x h 0 dx h G 0 dx e x 0 x 00 h dx 00 dx 0 dx We de.ne to be area re.ectance. Note that area re.ectance 
is similar to biconical re.ectance [11], save that it is also integrated over the re.ecting surface. 
By de.nition of : R ij R kijk j e A i A j A k h Conservation of energy over re.ection, and the reciprocity 
rela­tion derived for above, constitute fundamental properties of area re.ectance: X R ijk n R 1 ijk 
ij 1. ,for.xed , . A ki F ij R ijk A k F kj R kji 1 where equality is achieved in property over complete 
enclosures and perfect re.ectivity. 2.3 Evaluation of  A i A j A k R kji R kji In this section we examine 
the evaluation ofover given patches ,, . Recall: R 2. kji R A k R A j R A i f R r e A x k 00 R A x 0 
. Gx he Gx 00 e xx 000 h xdx 0 h 0 Gdx e x 000 x h dxdx 0 dx 00 We assume that discrete areas , , are 
of small enough scale that and are relatively constant over their surfaces. Then: f r RG kji S kj i 
GAG kikjj GAA jjik A A kjk A j A i where is the discretized value of , . FF jij i AS i S kji A i R k 
A jj S ik GA jikj G i FA j f ijr GA i S e i S k x ijj 0 ik xF h ji S kji A i S x k x j x AR i jkji Note 
that the average value of over and is we thus estimate by , and compute as: In practice, it will not 
be possible to compute the exact values of and over , , . We assume that we are able to estimate these 
values, along with error bounds for each estimation. Letandbe error estimates for computed and , respectively. 
We then have an estimate for area re.ectance in the form: R kj i F j v i F e j F i S j i ki S j iF ik 
j i i F j S i he k F j SR j iki i k S jjik i iS j i k i ji S e k S j F ik hj i SF kj i j h i ii F j 
F i S jik ji FSS kjkij h jii h Assuming,, we have neglected the last term and estimate the error in 
as . In general, and as is shown for glossy re.ection in Section 4, the accuracy of estimators for and 
is dependent on the F ji S kji size of the patches over which re.ectance is computed, relative to their 
distance apart. As relative size decreases, so does error in computation, leading directly to the adaptive 
re.nement strategy for illumination presented in Section 3 below. 3 Algorithms for Three Point Transport 
3.1 Introduction Recall equation (2):  L jk E jk i X i L ij R kji This equation suggests both a solution 
strategy for radiance under three point transport, and a natural representation for illumination within 
the solution system. We may interpret equation (2) as a gathering iteration similar to that employed 
for radiosity under diffuse re.ection: the radianceat patch in the direction of patch is found by gathering 
radiances in the direction of at patches . We may solve for transport by gathering radiance for each 
, and successively iterating to capture all signi.cant re-re.ection. We are left with the question of 
what structure we are gathering over and iterating upon. Note that all illumination is expressed as the 
radiance at a given patch in the direction of another it is these patch-patch interactions that form 
the primary structure within the solution system. All operation is over interactions: both the representation 
and transport of radiance, and the iteration and solution for illumination. Consider the following structure: 
 L jk L ij A j A j A k L jk A i typedef struct _interaction { Patch *from; Patch *to; Color L; Color 
Lg; List *gather; struct _interaction *nw, *sw, *se, *ne; } Interaction; A given interaction is de.ned 
by two patches ij->fromand ij ij->to, and represents the radiance at from in the direction of to. This 
radiance is stored within the interaction as attribute L. Lg is radiance gathered during the current 
solution iteration from interactions contained in the list gather. Subinteractions nw, sw, se, ne are 
the children of , induced by subdivision over either ij from or to. The structure assumes quadtree re.nement, 
leaving northwest, southwest, southeast, and northeast descendants. In the following sections we will 
present an algorithm for the re.nement and computation of illumination over a hierarchy of interactions. 
The algorithm will operate by re.ning pairs of inter­actions , (such that ij->to == jk->from), to ensure 
that ijjkijjkijjk computed re.ectance across the interaction pairs, and associated patch triples, satis.es 
user speci.ed error bounds. If a given in­teraction pair , is satisfactory, the interactions are linked 
to record that radiance may be gathered from to , otherwise one or both interactions are subdivided and 
re.nement applied to their descendants. After re.nement, a gathering iteration may be carried out, each 
interaction gathering radiance from interactions to which it has been linked. The gathered radiances 
are then distributed within each re­ceiving interaction hierarchy, and subsequent iterations computed 
until satisfactory convergence has been achieved. Note that, within this system, the eye may be regarded 
as simply another object with which patches may interact. The radiance along interactions to the eye 
provides the resulting view. 3.2 Adaptive Re.nement Consider the following procedure: Refine(Interaction 
*ij, Interaction *jk, float Feps, float Seps, float Aeps) { float feps, seps; feps = GeometryErrorEstimate(ij); 
seps = ReflectionErrorEstimate(ij, jk); if (feps < Feps &#38;&#38; seps < Seps) Link(ij, jk); else 
if (seps >= Seps) { switch(SubdivS(ij, jk, Aeps)) { case PATCH_I: Refine(ij->nw, jk, Seps, Feps, Aeps); 
Refine(ij->sw, jk, Seps, Feps, Aeps); Refine(ij->se, jk, Seps, Feps, Aeps); Refine(ij->ne, jk, Seps, 
Feps, Aeps); break; case PATCH_J: /* refine over children of ij and jk */ case PATCH_K: /* refine over 
children of jk */ case NONE: Link(ij, jk); } } else { /* feps >= Feps */ switch(SubdivG(ij, jk, Aeps)) 
{ /* refine over children, or link, as */ /* directed by PATCH_I, J, K, or NONE. */ } } This procedure 
computes over pairs of interactions, and associated patch triples, subdividing and recursively re.ning 
if estimated error exceeds user speci.ed bounds, linking the interactions for gathering if the bounds 
are satis.ed, or if no further subdivision is possible. Fepsand Sepsare the bounds for geometric and 
re.ection error, respectively; Aepsspeci.es the minimum area a patch may possess and still be subdivided. 
GeometryErrorEstimate and Re­ flectionErrorEstimateprovide estimations for and . SubdivS and SubdivG 
control re.nement for re.ection and geometry error, respectively. Both routines select a patch for re.ne­ment, 
subdividing the patch and associated interaction(s) if required. An identi.er for the selected patch 
is returned if no patch may be subdivided, then NONE is passed back. Note that a given in­teraction/patch 
may be re.ned against many different interactions within the system, and thus may have already been subdivided 
when selected by a Subdiv routine in this case, the routine simply returns the proper identi.er. The 
Subdiv routines should select for re.nement patches that are of large size relative to their distance 
from their partner(s) in the transport triple. Form factor estimation is a convenient criterion for the 
determination of such patches a large differential to area form factor indicates that patch is of large 
relative size. Care must S } kji F ji F ji S kji F dpq q be taken in subdivision, however, to ensure 
that each interaction is always subdivided in the same way for all re.nements involving that interaction. 
The Subdiv routines thus choose for re.nement the patch of size at least Aepsthat is of greatest form 
factor within and/or that will not induce multiple sets of children over either interaction.p k p k p 
j p j p j ijijjkp i jpk i If patch is of greatest form factor over both and , and of area greater than 
Aeps, then it is chosen for re.nement (Figure 3 at middle). Otherwise, if is selected over one interaction, 
but or is selected over the other, then the outside patch is chosen for re.nement. Given two selected 
outside patches, SubdivS selects the one of greater form factor relative to ; SubdivG selects over , 
as has no direct effect on geometric accuracy. Note, however, that even under SubdivG, if only and are 
allowed p j p k p j p k subdivision, will be selected, although with further subdivision the triple will 
eventually balance suf.ciently to allow re.nement over . Figure 3: Re.nement and Subdivision 3.3 Gathering 
Radiance Gathering radiance over interactions may be written as a simple procedure: Gather(Interaction 
*jk) { Interaction *ij; if (jk) { jk->Lg = 0; ForAllElements(ij, jk->gather) jk->Lg += ij->L * Reflectance(ij, 
jk); Gather(jk->nw); Gather(jk->sw); Gather(jk->se); Gather(jk->ne);  } } We gather radiance into 
jk->Lg rather than directly into jk->L to avoid the necessity of a push/pull with every invocation of 
the procedure (see Section 3.4). The solution method is thus simple Jacobi iteration, as opposed to Gauss-Seidel, 
as the hierarchical structure imposes simultaneous rather than successive displacement. 3.4 Radiance 
within a Hierarchy A gathering iteration results in received radiance scattered through­out each interaction 
hierarchy. This gathered radiance must be dis­tributed and accounted for over all ancestors and descendants 
of each receiving interaction, in order to maintain the consistency and correctness of the hierarchical 
representation of radiance between patches. We employ a distribution algorithm similar to that presented 
in [7] for radiosity over patch/element hierarchies: gathered radiance is pushed to the leaf interactions 
within each hierarchy to ensure propagation to all descendants, and then pulled and distributed back 
up from the leaves through all higher level interactions to their common ancestor at the root. As is 
shown in [2], radiance may be pushed unchanged within the interaction hierarchy, and area averaged as 
it is pulled from child to parent.  4 Application over Glossy Re.ection In this section we discuss our 
implementation of the above algo­rithms over glossy re.ection. 4.1 The Re.ection Function We employ 
a highly simpli.ed Torrance-Sparrow [15] model for our glossy re.ection function: f g e n i n r h b 8i2coscos 
i. cos mr sh e ir hcos Fm This function incorporates the facet distribution function developed by Blinn 
[3], normalized for projected facet area under  Figure 4: Estimating Cones . r Figure 5: , , and C 
m nn mi [10]. Angle is that made to the mean surface normal by , the microfacet mirror orientation normal 
lying halfway betweenand f g .  n r sh m e ir .h i sh i 1 r 0 ir Function expresses self-shadowing over 
microfacets for near specular surfaces, such self-shadowing or masking does not become critical until 
relatively high or [6]. The imple­mented system thus simply clamps from to when or exceeds a preset bound 
near the horizon. This scheme serves as a crude approximation to the shadowing function; however, a better 
strategy would be to employ a much fuller tabulation of the func­tion, incorporated into the error analysis 
presented below. A more complete discussion of shadowing and conservation of energy over is presented 
in [2]. 4.2 Error Estimation Recall the general expression for error derived in Section 2.3: A i e F 
ji S kji i S kji FA jji h F ji F d A ji In implementation we have estimated the form factor by , the 
form factor from a differential area at to a disk of area centered at , as was employed in [7]. As discussed 
in [7], the relative error in this estimate is proportional to the estimate itself. In our implementation 
we have thus estimated absolute error as at most proportional to .A brief discussion of relative and 
absolute error over hierarchical methods is presented in [2]. We now consider the error estimate . As 
discussed in the appendix to this paper, we may compute bounding cones , , and over all possible incident, 
re.ected, and mirror orientation directions induced at by and (Figures 4 and 5 these .gures are discussed 
more fully in the appendix). We may then  F d 2 ji F ji C m bA j A i cos A Fkm S k cos jii cos r C 
i C r compute maximum and minimum, , over these cones, and estimate error by interval width. The full 
expression for estimated error over transport is given in the appendix. 4.3 Clamping and Visibility Evaluation 
of glossy re.ectance over three surface areas, as required by the gather iteration, may be dif.cult, 
particularly if surface sub­division has been limited by Aepsrather than satisfaction of error bounds, 
and if , the facet distribution exponent, has high value. In this case we must estimate the integral 
of a spikey function over a relatively broad area. Our solution is to band limit the BRDF in a fashion 
similar to that presented by Amanatides [1]. We employ the cone estimation techniques of the previous 
section to determine if the BRDF varies signi.cantly over the given patches if this variance exceeds 
a set bound, we roughen the re.ecting surface, loweringto b broaden the resulting re.ection over the 
estimated cones. We then renormalize the resulting blurred function, as described in [1], to Figure 
6: Geometric Con.gurations prevent ampli.cation of its low frequency components. We note that the resulting 
antialiasing is relatively aggressive, signi.cantly dimming or eliminating re.ections requiring overmuch 
blurring. In implementation, we have computed visibility via jittered ray casting and inheritance similar 
to that of [7], storing visibility data in interactions as it is computed.  5 Results 5.1 Growth in 
Transport We have measured the growth in transport triples (linked interac­tions) versus , the maximum 
number of elements at the .nest level n of subdivision, over parallel, perpendicular, and oriented patches 
(Figure 7). The corresponding geometries are shown in Figure 6. The graphs show linear or near linear 
behavior over each range the graph of triples vs. for the perpendicular case is slightly con­ n cave 
over the lower data points, but subsides to linear with further re.nement. In previous work [7] on hierarchical 
re.nement for radiosity, it was shown that for error estimate proportional to, and suf.cient cnkcF dji 
re.nement, each subpatch may only interact with other patches in a limited local neighborhood. As discussed 
in [7], each patch may thus participate in at most interactions, for some constant independent of and 
. Adaptive re.nement thus generates at most transport interactions. We will show a similar bound for 
discrete three point transport under glossy re.ection. Recall that the estimate for error in computed 
transport is pro­portional to . Our argument depends on two assumptions:  O e n h F ji S kji i S k S 
jikji F ji S kj S i S 1. We may bound both and by some max. As discussed below, the lower this max, the 
smaller the magni­tude of the leading coef.cient underling the resulting bound. Note that our argument 
thus does not apply to perfect specular re.ection, as the corresponding BRDF incorporates the Dirac delta 
function [11]. Equivalently, the argument does not hold overfor b 1 bS f g (inducing mirror re.ection), 
as we can not provide a .nite bound for in this case. For .nite , however, the desired bound over glossy 
re.ection is achieved by:  b i82maxecos Fm hmaxesec i hmaxesec r h The maxima over the secant terms 
are bounded by microfacet self­shadowing. 2. and within our error estimate are at most proportional F 
jdiji F ji F ji F dji F ji F d 2 ji to . Recall that we estimate as , and as , thus satisfying this assumption. 
Given these assumptions, estimated error is at most proportional tomax. We may now show growth, for suf.cient 
re.nement. Con­sider re.nement over interaction under an error estimate at worst proportional to max. 
The error estimate is thus proportional to , and therefore, for suf.cient re.nement, there are at mostsuch 
interactions, as discussed in [7]. Consider now an error satis.ed link from to an interaction . For suf.cient 
re.nement under our subdivision scheme, we may F dj Sn i Fk dji SF d O ji e nF h ij F ji ijF jk F kj 
ijp i p j pO k e jnk h assume that form factors , , , over , , and are roughly equal. Furthermore, these 
satisfying form factors depend only on the error estimate, re.ection function, and error bounds, not 
on or. 6000000 b Triples   8000000 6000000 4000000 2000000 Triples  10000000 1500000 8000000 Triples 
1000000 500000 2000000 500 1000 1500 2000 2500 N 1000 1500 N Figure 7: Triples vs. 500 Parallel Configuration 
N   500 1000 1500 2000 2500 3000 N Perpendicular Configuration 0 1 0 005 . 4000000 Oriented Configuration 
e e over Geometry. Error bounds . Glossy exponent Figure 10: Meshing for glossy and diffuse re.ection 
1500000 . = 500  . = 100 Triples 1000000 . = 50 . = 25  . = 5  500000 . The graph is over parallel 
polygons for which the error bounds and interpolygon distance have been F Si max Figure 8: Triples 
vs. F ij 200 300 400 500 N Specular Configurations over (ie. doubled. above, S j 0 At worst the 
above form factors are such that where Eps is the most restrictive error bound. Note that, as stated 
depends only on the error estimate, re.ection function max), and error bounds. Only some constant number 
of such form factors may be .tted over the directional hemisphere abovemay only be linked to some constant 
number of interac­. The total number of linked interactions, and corresponding kO e n h S . Eps, p 
 ij may not achieve ,  and thus j transport triples, is thus For a given pb j S  tions over As Note 
that the above argument, although it establishes the desired bound, may overstate the potential for links 
at a given interaction. , much of the directional re.ection into the hemisphere max, and may even be 
of maximum That is, the analysis ignores the modulation between the paired error increases in magnitude, 
the corresponding boundincrease as well. We may thus expect greater growth in transport computation with 
higher specular exponent, as shown in Figure 8. 2000 b 500  and value terms within the error estimate. 
Within this graph, growth is superlinear for 500 . max must trials over a higher range of n n , though 
further have shown that the increases, allowing suf.cient re.nement Finally, we note that under specular 
re.ection each element is re.ected across every other element perfectly, and to a .rst approx­imation 
is visible from a constant number of other elements in the environment (at least in the case of a convex 
enclosed room; the analysis is complicated by occlusion and certain worst case align- O e n 2 h we Table 
1: Image Statistics eye/offset views for the re.ection of a garish checkerboard. The image in Figure 
10 shows contrasting illumination and mesh­ing induced by diffuse and glossy re.ection. Note the distinct 
mesh­ing for each highlight. Glossy re.ection is at a less oblique angle, and thus both the highlight 
and meshing exhibit less distortion in the direction of the eye. Note that these scenes are extremely 
simple application to more complex environments is still very expensive, despite the em­rate subsides 
to linear as for the local neighborhood property to obtain. ments). Thus, the number of interactions 
is at least conjecture that it is no worse than this bound. 5.2 Illumination and Re.nement Figure 9 shows 
illumination and meshing over surfaces of varying glossiness (specular exponent). Within each image, 
the re.ecting surface is perpendicular to the diamond shaped light source, and we see the resulting re.ection 
in the direction of the eye. Note the conformation of meshing to the highlight over each surface. The 
stretched nature of the highlight along the axis to the eye is characteristic of Torrance-Sparrow re.ection 
over fairly oblique angles, and accounts for the increased sensitivity of meshing along this axis. The 
rightmost three images in the .gure show the meshing from above. The illumination shown in these images 
is somewhat unusual -it shows the re.ection to the eye as though it had been painted on the re.ecting 
surface, and then viewed from a different location, directly above. The images in Figure 11 show similar 
ployment of hierarchical methods. Motivated by the work of Smits et al. [14] in hierarchical radiosity, 
we are currently experimenting with importance and radiance weighting over three point transport preliminary 
results of this work are shown in Figure 12. The given environment contains four re.ectors: the broad 
face of each of the three slabs and the top of the central cube. In addition to the re.ections seen in 
the slabs, note the play of light originating at the lamp at left, re.ected off the cube top, and over 
the upper part of the green wall at right. Total potential transport triples over this environment at 
the .nest level of subdivision is just over 222 billion our system, under importance and radiance weighting, 
employs 70,995, a reduction to 3 hundred-thousandths of 1 percent. Table 1 provides further statistics 
for the images. Timings are given for a Silicon Graphics indigo workstation with a single 50 MHz R4000 
processor. The image shown in Figure 12 was generated after seven complete iterations (gathers to all 
interactions), and total time just over three minutes.  Figure 9: Illumination and Re.nement 6 Discussion 
Recall the matrix formulation shown in Figure 2. For any of n 3 n reasonable size, the resulting matrix 
will be unmanageable we have shown, however, that for suf.cient re.nement theentries in the matrix may 
be approximated to within user speci.ed bounds O e n h n 3 by subblocks. The gather and push/pull procedures 
described in preceding sections allow manipulation and solution over this representation. As discussed 
in [2], the resulting system may be shown to converge. Growth in transport is more accurately described 
as , kk 3 O e n i k 3 h where is the number of input polygonal patches within the en­vironment, as opposed 
to elements. The term is generated by the initial examination of all polygon triples for re.ection, and 
is subsumed by as the number of elements increases. As the number nk 3 of polygons in an environment 
grows, however, the term will become prohibitively large. As discussed in [14] with respect to the related 
problem under hierarchical radiosity, the capability to cluster as well as re.ne polygons would reduce 
the dif.culty of unneces­sary initial interactions. Clustering is arguably the most important open problem 
in the computation of global illumination. The hierarchical approach described in this paper was derived 
by writing the rendering equation in a three point transport formulation. Another option would be to 
parameterize radiance by position and direction we believe that a similar hierarchical approach could 
be employed with the method of discrete ordinates or spherical harmonics. Finally, we note that, similarly 
to other algorithms for hierar­chical illumination [7, 14], the algorithm described in this paper bounds 
estimated error over individual transport computations. As discussed in [14], bounding estimated error 
over individual trans­port does not easily or necessarily provide a rigorous bound for overall error 
in the solution. An analysis and means of computing such a bound over hierarchical illumination remains 
an interesting open problem. 7 Acknowledgements This research was partially supported by equipment grants 
from Apple and Silicon Graphics Computer Systems and a research grant from the National Science Foundation 
(CCR 9207966). The authors would like to thank Dr. P. Prusinkiewicz for access to the graph­ics research 
facilities at the University of Calgary during the .nal stages of this work, and Deborah Fowler for her 
crucial assistance in shooting test images, paste up and much other support and en­couragement. Thanks 
to Cullen Jennings and David Laur for all of their help recording images. We especially thank the anonymous 
referees for their many helpful comments and suggestions. 8 References [1] Amanatides, J. (1992) Algorithms 
for the detection and elimi­nation of specular aliasing. Proc. Graphics Interface 92, 86-93. [2] Aupperle, 
L. (1993) Hierarchical algorithms for illumination. Doctoral Dissertation, Princeton University. [3] 
Blinn, J.F. (1977) Models of light refection for computer syn­thesized pictures. Computer Graphics 11 
(2), 192-198. [4] Chen, S.E., Rushmeier, H.E., Miller, G., Turner, D. (1991) A progressive multi-pass 
method for global illumination. Computer Graphics 25 (4), 165-174. [5] Cook, R.L. (1986) Stochastic sampling 
in computer graphics. ACM Transactions on Graphics 5 (1), 51-72. [6] Hall, R. (1989) Illumination and 
color in computer generated imagery. Springer-Verlag, New York. [7] Hanrahan, P., Salzman, D., Aupperle, 
L. (1991) A rapid hierar­chical radiosity algorithm. Computer Graphics 25 (4), 197-206. [8] Immel, D.S., 
Cohen, M.F., Greenberg, D.P. (1986) A radiosity method for non-diffuse environments. Computer Graphics 
20 (4), 133-142. [9] Kajiya, J.T. (1986) The rendering equation. Computer Graphics 20 (4), 143-150. [10] 
Mitchell, D. (1992) Manuscript. [11] Nicodemus, F.E., Richmond, J.C., Hsia, J.J., Ginsberg, I.W., Limperis, 
T. (1977) Geometrical considerations and nomenclature for re.ectance. National Bureau of Standards monograph, 
no. 160. [12] Shirley, P. (1990) A ray tracing method for illumination cal­culation in diffuse-specular 
scenes. Proc. Graphics Interace 90, 205-212. [13] Sillion, F.X., Arvo, J.R., Westin, S.H., Greenberg, 
D.P. (1991) A global illumination solution for general re.ectance distributions. Computer Graphics 25 
(4), 187-196. [14] Smits, B.E., Arvo, J.R., Salesin, D.H. (1992) An importance­driven radiosity algorithm. 
Computer Graphics 26 (2), 273-282. [15] Torrance, K.E., Sparrow, E.M. (1967) Theory for off-specular 
re.ection from roughened surfaces. J. of the Optical Society of America 57 (9), 1105-1114. [16] Ward, 
G.J., Rubinstein, F.M., Clear, R.D. (1988) A ray tracing solution for diffuse environments. Computer 
Graphics 22 (3), 85­ 92. Appendix: Error Analysis Recall the error expression derived in Section 2.3: 
In implementation, we have divided into separate compo­nents for each subfactor of . We thus have: e 
82mmm F secji cose i cos F F ji Fji f S cos gmk Fjr ir m i m cosm S k S secjFik F j mijri FF h jjii cos 
cosF i 1cosim r In implementation, the re.nement procedure of Section 3.2 takes an additional argument, 
Ceps, against which the two estimates of error in reciprocal cosine are tested. We are left with the 
computation of , , andn i cos n rFm A i A jk sec i 0 sec r . The variance (and associated error) in these 
cosine terms over given patches , , is determined by the set of possible , lying between the patches 
(we dispense with notation in this section). Consider patches , and (Figure 4): we enclose these patches 
S i S j A i A j c i c j r i r j S i S j in spheres , with centers , , and radii , , respectively. For 
the moment we will assume that the interiors of and do not intersect, and thus there exists a tangent 
cone lying between the spheres. Note that this cone is a right circular cone centered on the line joining 
and . Consider the nappe containing : it may be regarded as a cone of direction vectors centered about 
the vector . We will call this vector cone . If and are any two c i x cmm jri c i cS ji S j c i x n i 
c j CAp iij x mp ij A k pS ji C i C ri C i points on or in , , then the vector lies within . thus bounds 
the set of possible . We may characterize by the angle de.ned by its axis, , and boundary cone and angle 
may be similarly de.ned over and . If either pair of spheres intersect, we set the corresponding. We 
may easily compute maxima and minima for and given andC r , and may then compute error in estimation 
as sec i secemax r x min C i h2 . The cones and centered about and induce a similar C i Cn rm n i n r 
cone of variation about(Figure 5). Application of basic spherical trigonometry yields [2]:  m m n arcsinminesine 
m i 2hisine m r 2h10h Given m m , determination of maxecos n i i F n mm h , minecos Fm h , and thus 
immediately follows. Having computed these estimates and maxima, and incorporat­ing the estimates for 
form factor computation, we may bound and estimate error in transport as: cos F O28 F m F m m d 2 j cossec 
i max F ir F m mcos d F j i d maxmcosFmax j i maxmsecm mcosmaxmsecF i i max i msec max msec rir r 
 It is this error measure that we employ in our implementation.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166138</article_id>
		<sort_key>163</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[On the form factor between two polygons]]></title>
		<page_from>163</page_from>
		<page_to>164</page_to>
		<doi_number>10.1145/166117.166138</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166138</url>
		<keywords>
			<kw><![CDATA[closed form solution]]></kw>
			<kw><![CDATA[form factor]]></kw>
			<kw><![CDATA[polygons]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP40023875</person_id>
				<author_profile_id><![CDATA[81100117380]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schr&#246;der]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15033698</person_id>
				<author_profile_id><![CDATA[81100482576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanrahan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1098650</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ABRAMOWITZ, M., AND STEGUN, I. A. Handbook of Mathematical Functions, 9th ed. Dover Publications, 1970.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74367</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BAUM, D. R., RUSHMEIER, H. E., AND WINGET, J. M. Improving Radiosity Solutions Through the Use of Analytically Determined Form-Factors. Computer Graphics 23, 3 (July 1989), 325- 334.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325171</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[COHEN, M. F., AND GREENBERG, D. P. The Hemi-Cube: A Radiosity Solution for Complex Environments. Computer Graphics 19, 3 (July 1985), 31-40.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808601</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[GORAL, C. M., TORRANCE, K. E., GREENBERG, D. P., AND BATTAILE, B. Modelling the Interaction of Light between Diffuse Surfaces. Computer Graphics 18, 3 (July 1984), 212-222.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122740</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[HANRAHAN, P., SALZMAN, D., AND AUPPERLE, L. A Rapid Hierarchical Radiosity Algorithm. Computer Graphics 25, 4 (July 1991), 197-206.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[HERMAN, R. A. A Treatise on Geometrical Optics. Cambridge University Press, 1900.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[LAMBERT. Photometria sive de mensura et gradibus luminis, colorum et umbrae. 1760. German translation by E. Anding in Ostwald's Klassiker der Exakten Wissenschaften, Vol. 31-33, Leipzig, 1892.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325169</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[NISHITA, T., AND NAKAMAE, E. Continuous Tone Representation of Three-Dimensional Objects Taking Account of Shadows and Interreflection. Computer Graphics 19, 3 (July 1985), 23-30.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[SCHRCIDER, P., AND HANRAHAN, P. A Closed Form Expression for the Form Factor between Two Polygons. Tech. Rep. CS-404- 93, Department of Computer Science, Princeton University, January 1993.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74366</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[WALLACE, J. R., ELMQUIST, K. A., AND HAINES, E. A. A Ray Tracing Algorithm for Progressive Radiosity. Computer Graphics 23, 3 (July 1989), 315-324.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[WOLFRAM, S. Mathematica. Addison-Wesley, 1988.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 On the Form Factor between Two Polygons Peter Schröder Pat Hanrahan Department of Computer Science Princeton 
University  Abstract Form factors are used in radiosity to describe the fraction of dif­fusely re.ected 
light leaving one surface and arriving at another. They are a fundamental geometric property used for 
computation. Many special con.gurations admit closed form solutions. How­ever, the important case of 
the form factor between two polygons in three space has had no known closed form solution. We give such 
a solution for the case of general (planar, convex or concave, In this paper we present a formula for 
the form factor integral between two general polygons. The derivation of this formula is quite involved, 
and the interested reader is referred to [9] for a detailed derivation. The purpose of this paper is 
to bring this result to the attention of the graphics community. 2 Closed form solution The form factor 
integral can be reduced to a double contour inte­ w w w w gral by two applications of Stokes theorem 
[6] cos .1 cos .2 CR Categories and Subject Descriptors: I.3.7 [Computer Graphics]: pA1F12 = dA2 dA1 
A1 A2 possibly containing holes) polygons. IIrI2 Three-Dimensional Graphics and Realism Radiosity; 
J.2 [Physical Sci­ 1 ln(rr · rr) drx2 · drx1 ences and Engineering]: Engineering.= 4 .A1 .A2 Additional 
Key Words and Phrases: Closed form solution; form factor; polygons. 1 Introduction When using the radiosity 
technique to create images the form factor plays a central role. It describes the fraction of radia­tion 
diffusely emitted from one surface reaching another surface. where .1, .2 are the angles between the 
normal vector of the respective surface and a radius vector rr, which connects two points on the surfaces. 
The above equation holds for all surfaces such that every point on either surface sees the same contour 
of the other surface. In the case of polygons P1 and P2 the contour integral reduces to a sum of double 
line integrals over all pairwise combinations The accurate computation of form factors is the central 
theme of edges in many recent papers. Goral et al. [4], who introduced radios­   ity to the computer 
graphics community, used numerical contour 4pAP1 FP1P2 =cosEiEjln(rr · rr) dsj dti integration to compute 
form factors between polygons. Cohen EiEj Ei Ej and Greenberg [3] took visibility into account with 
their hemi­ ww EiEj we are left with the task of giving c2 c0 a solution to integrals of the general 
form 0 0 ln f (s, t) ds dt. Ignoring the factor cos  cube algorithm. More recent hierarchical and adaptive 
algorithms compute still more accurate form factors [10; 5]. Nishita and Nakamae [8] and Baum et al. 
[2] have used an exact solution for the form factor between a differential surface element and a c0 and 
c2 are the lengths of the edges over which a given double contour integral is taken and f(s, t)= s 2+ 
c1st + t2+ c3s + c4t + c5 is the bi-quadratic form which arises from the expansion of the dot product 
(see Table 2 for de.nitions of all variables). If the two line segments lie in a common plane we can 
factor f(s, t) into two bi-linear forms and a solution is readily obtained with standard integration 
tables (see [9]). Lines in general position polygon. Most radiosity algorithms are restricted to polygonal 
environments, and so a closed form solution for the form factor between polygons is potentially of great 
utility. The history of computing form factors is very long. A closed form expression for the form factor 
between a differential surface element and a polygon was found by Lambert in 1760 [7]. Lam­   lead 
to the following result: c2 c0 bert proceeded to derive the form factor for a number of special ln f(s, 
t) ds dt con.gurations among them the form factor between two perpen-00 [[[[[[  dicular rectangles 
sharing a common edge. He writes about the t=c2  latter derivation: c3 c1 =(s + )G(f(., t))(s)+ H(f(., 
t))(s) 22 [[ [[ s=c0 s=0 Although this task appears very simple its solution t=0 is considerably more 
knotted than one would expect. For it would be very easy to write down the differential -2c0c2+ c14c15 
p(2k(s) + 1)M(t) expression of fourth order, which one would need to integrate four fold; but the highly 
laborious computa­tion would .ll even the most patient with disgust and [[ [[ -iL(-c17(s))(t) + L(-c18(s))(t) 
 s=c0 s=0 drive them away from the task.  [[[[[[ t= of a closed form solution for the form factor between 
two general t= c13+c2 ¯ c13+c2  factors between many different geometric con.gurations and these -L(c17(s))(t) 
- L(c18(s))(t)Other workers have derived closed form solutions for the form can be found in standard 
textbooks. However, we are not aware c13 c¯13 polygons. Thus, this problem has remained open for over 
230 years. Permission to copy without fee all or part of this material is granted provided that the 
copies are not made or distributed for direct provided that the copies are not made or distributed for 
direct commercial advantage, the ACM copyright notice and the title of the commercial advantage, the 
ACM copyright notice and the title of the publication and its date appear, and notice is given that copying 
is by publication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Machinery. permission of the Association for Computing Machinery. To copy otherwise, or 
to republish, requires a fee and/or specific permission. otherwise, or to republish, requires a fee and/or 
specific permission. &#38;#169;1993 -0---8/93/008/0015 $1.50 &#38;#169;1993 ACM -0-89791 -601 -8/93/008 
$1.50 where k(s) . {-1, 0, 1} according to the particular branchcut of the complex logarithm choosen 
in L. The auxiliary functions G, H, L, and M are given in Table 1. 3 An example We have implemented 
our closed form solution in Mathemat­ica [11] (this code is available from ps@princeton.edu). The w 
1 -b ln(y-1) - b ln(1+y) 2(b+y)(1+by)[(b-y)2+(by-1)2] L(b)(y) := y t2(1 - t2)-3 ln(b + t) dt = + +ln 
(1-y)(1-b) ln(b + y) 16 (b+1)2 (b-1)2 (b2-1)2(y2-1)2 (1+y)(1+b)2(b-y)1-y 1+y + + Li2 - Li2 (b2-1)(y2-1) 
1+b 1-b w y 2)-31 M(y) := t2(1 - tdt =4y(y 2 - 1)-2+2y(y 2 - 1)-1 +ln y-1 16 y+1 w q(y) J(y) G(q)(y) 
:= y ln q(t) dt = Jln q(y) - 2y + d tan-1 q 2a ad w y y 2 cb2 bd J(y) H(q)(y) := t ln q(t) dt =+ - 2ln 
q(y) - y(ay-b) - 2 tan-1 q 2 2a 4a2a 2ad 8 k Table 1: Four auxiliary integrals needed in the solution. 
Notice that L(b)(y) uses the dilogarithm [1], Li2(z)=zk2, d Li2(z)= 1 dz - ln(1-z) . In G and H the 
argument q is an arbitrary quadratic polynomial q(t)= at2+ bt + c and d = v 4ac - b2. z c0= lEj l c1= 
-2dri · drj c2= lEil c3= -2drj · (rpi - prj ) c4=2dri · (pri - prj ) c5= lrpi - prjl2 2 c10 =4 - c1 c11 
=4c4 - 2c1c3 c12 =4c5 - c32 c c11- 2 -4c10c12 11 c13 = 2c10 c 2 -4c10c12 11 c14 = c10 v c15 = c10c14 
c16(s)= c1c13 - c3 - 2s 2 -c15+ -4|c16(s)|2 15 c17(s)= c 2ic¯16(s) 2 -c15--4|c16(s)|2 15 c18(s)= c 2ic¯16(s) 
 Table 2: All expressions for two edges Eij with parameterization rxi(t)= rpi + tdri and rxj(s)= prj 
+ sdrj (ldri,jl = 1). implementation requires some care because of the complexities of the functions 
that are involved. A simple example, which requires the full power of our for­mula, concerns the form 
factor between two equal width rectangles sharing an edge with an enclosing angle . . [0,p]. The con.g­uration 
is illustrated in Figure 1 together with the form factor as a function of . for different aspect ratios 
l = ab (common edge length b).  Conclusion We have given a closed form solution for the form factor 
between two general polygons. This solution is non-elementary since it involves the dilogarithm function. 
The principal value of our solution is in determining exact answers for general polygonal con.gurations. 
This can be used in practice for reference solutions to check more ef.cient approximations. Baum et al. 
[2] have also shown that the error in the computed solution can be reduced signi.cantly when using a 
closed form solution near singularities of the integrand. There has been a long history of computing 
closed form ex­pressions for form factors starting with Lambert in 1760. The literature lists many special 
cases for which closed form solu­tions exist, but hitherto no solution had been given for general polygonal 
con.gurations. The present paper closes this gap. Acknowledgements The .rst author would like to thank 
the Sci-Vis group at HLRZ for their support. Other support came from Apple, Silicon Graphics and the 
NSF (contract no. CCR 9207966). References [1] Abramowitz, M., and Stegun, I. A. Handbook of Mathemat­ical 
Functions, 9th ed. Dover Publications, 1970. [2] Baum, D. R., Rushmeier, H. E., and Winget, J. M. Im­proving 
Radiosity Solutions Through the Use of Analytically Deter­mined Form-Factors. Computer Graphics 23, 3 
(July 1989), 325 334. [3] Cohen, M. F., and Greenberg, D. P. The Hemi-Cube: A Radiosity Solution for 
Complex Environments. Computer Graphics 19, 3 (July 1985), 31 40. [4] Goral, C. M., Torrance, K. E., 
Greenberg, D. P., and Battaile, B. Modelling the Interaction of Light between Diffuse Surfaces. Computer 
Graphics 18, 3 (July 1984), 212 222. [5] Hanrahan, P., Salzman, D., and Aupperle, L. A Rapid Hierarchical 
Radiosity Algorithm. Computer Graphics 25, 4 (July 1991), 197 206. [6] Herman, R. A. A Treatise on Geometrical 
Optics. Cambridge University Press, 1900. [7] Lambert. Photometria sive de mensura et gradibus luminis, 
colo­rum et umbrae. 1760. German translation by E. Anding in Ostwald s Klassiker der Exakten Wissenschaften, 
Vol. 31-33, Leipzig, 1892. [8] Nishita, T., and Nakamae, E. Continuous Tone Representa­tion of Three-Dimensional 
Objects Taking Account of Shadows and Interre.ection. Computer Graphics 19, 3 (July 1985), 23 30. [9] 
Schr¨ oder, P., and Hanrahan, P. A Closed Form Expression for the Form Factor between Two Polygons. Tech. 
Rep. CS-404­93, Department of Computer Science, Princeton University, January 1993. [10] Wallace, J. 
R., Elmquist, K. A., and Haines, E. A. A Ray Tracing Algorithm for Progressive Radiosity. Computer Graphics 
23, 3 (July 1989), 315 324. [11] Wolfram, S. Mathematica. Addison-Wesley, 1988. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166139</article_id>
		<sort_key>165</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[Reflection from layered surfaces due to subsurface scattering]]></title>
		<page_from>165</page_from>
		<page_to>174</page_to>
		<doi_number>10.1145/166117.166139</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166139</url>
		<keywords>
			<kw><![CDATA[Monte Carlo]]></kw>
			<kw><![CDATA[integral equations]]></kw>
			<kw><![CDATA[reflection models]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Monte Carlo</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003682</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Sequential Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003727.10003729</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Differential equations->Partial differential equations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003677</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Markov-chain Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP73023120</person_id>
				<author_profile_id><![CDATA[81100482576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanrahan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39074766</person_id>
				<author_profile_id><![CDATA[81332510462]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wolfgang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Krueger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BECKMANN, P., AND SPIZZICHINO, A. The scattering of electromagnetic waves from rough surfaces. Pergamon, Oxford, 1963.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801255</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BLINN, J. F. Light Reflection Functions for Simulation of Clouds and Dusty Surfaces. Computer Graphics 16, 3 (July 1982), 21-29.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325249</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BLOOMENTHAL, J. Modeling the Mighty Maple. Computer Graphics 19, 3 (July 1985), 305-311.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BOUCUEa, P. The Gradation of Light. University of Toronto Press, 1960.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37434</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CABRAL, B., MAX, N., AND SPRINGMEYER, R. Bidirectional reflection functions from surface bump maps. Computer Graphics 21, 4 (July 1990), 273-281.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[CAaTSa, L., AND CASHWELL, E. Particle Transport Simulation with the Monte Carlo Method. Energy Research and Development Administration, 1975.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[CHANDRASEKHAR, S. Radiative Transfer. Dover, New York, 1960.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357293</ref_obj_id>
				<ref_obj_pid>357290</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[CooK, R. L., AND TORRANCE, K. E. A Reflection Model for Computer Graphics. ACM Transactions on Graphics 1, 1 (1982), 7-24.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[f'ANTE, R. Relationship between Radiative Transport Theory and Maxwell's Equations in Dielectric Media. J. Opt. Soc. Am. 71, 4 (April 1981), 460-468.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[GRAWBOSKI, L. Astrophysics J. 39 (1914), 299.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[HANRAHAN, P. From Radiometry to the Rendering Equation. SIGGRAPH Course Notes: An Introduction to Radiosity (1992).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122738</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[HE, X. D., TORRANCE, K. E., SILLION, F. X., AND GaSSNBSaC, D. P. A Comprehensive Physical Model for Light Reflection. Computer Graphics 25, 4 (July 1991), 175-186.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[HENYEY, L. G., AND GREENSTEIN, J. L. Diffuse radiation in the galaxy. Astrophysics J. 93 (1941), 70-83.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[ISHIMURA, A. Wave Propagation and Scattering in Random Media. Academic Press, New York, 1978.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[JERLOV, N. G. Optical Oceanography. Elsevier, Amsterdam, 1968.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[KAJIYA, J. Radiometry and Photometry for Computer Graphics. SIGGRAPH Course Notes: State of the Art in Image Synthesis (1990).]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325167</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[KAJIYA, J. Anisotropic Reflection Models. Computer Graphics 19, 3 (July 1985), 15-22.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[KORTUM, G. Reflectance Spectroscopy. Springer-Verlag, Berlin, 1969.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Kauscsa, W. The Application of Transport Theory to the Visualization of 3-D Scalar Fields. Computers in Physics 5 (April 1991), 397-406.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[MA, Q., ISHIMURA, A., PHU, P., AND KUCA, Y. Transmission, Reflection and Depolarization of an Optical Wave For a Single Leaf. IEEE Transactions on Geoscience and Remote Sensing 28, 5 (September 1990), 865-872.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[MARCHUK, G., MIKHAILOV, G., NAZARALIEV, M., DARBINJAN, R., KARGIN, B., AND ELEPOV, B. The Monte Carlo Methods in Atmospheric @tics. Springer Verlag, Berlin, 1980.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97922</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[NAKAMAE, E., KANEDA, K., OKAMOTO, T., AND NISHITA, T. A Lighting Model Aiming at Drive Simulators. Computer Graphics 24, 4 (August 1990), 395-404.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[NICODEMUS, F. E., RICHMOND, J. C., AND HSlA, J. J. Geometrical Considerations and Reflectance. National Bureau of Standards, October 1977.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97909</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[POULIN, P., AND FOURNIER, A. A Model for Anisotropic Reflection. Computer Graphics 24, 4 (August 1990), 273- 282.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[SEELIGER, R. Munch. Akad. H. KI. Sitzungsber 18 (1888), 201.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[TORRANCE, K. E., AND SPARROW, E. M. Theory of Off- Specular Reflection From Roughened Surfaces. Journal of the Optical Society of America 57 (September 1967), 1104- 1114.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[VAN GEMERT, M. F. C., JACQUES, S. L., STEREN- BERG, H. J. C. M., AND STAR, W. M. Skin Optics. IEEE Transactions on Biomedical Engineering 36, 12 (December, 1989), 1146-1154.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134075</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[WESTIN, S. H., ARVO, J. R., AND TORRANCE, K. E. Predicting Reflectance Functions from Complex Surfaces. Computer Graphics 26, 2 (July 1992), 255-264.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[WOOLLEY, J. T. Reflectance and Transmittance of Light by Leaves. Plant Physiology 47 (1971), 656-662.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Re.ection from Layered Surfaces due to Subsurface Scattering Pat Hanrahan Wolfgang Krueger Department 
of Computer Science Department of Scienti.c Visualization Princeton University German National Research 
Center for Computer Science Abstract The re.ection of light from most materials consists of two ma­jor 
terms: the specular and the diffuse. Specular re.ection may be modeled from .rst principles by considering 
a rough surface consisting of perfect re.ectors, or micro-facets. Diffuse re.ection is generally considered 
to result from multiple scattering either from a rough surface or from within a layer near the surface. 
Ac­counting for diffuse re.ection by Lambert s Cosine Law, as is universally done in computer graphics, 
is not a physical theory based on .rst principles. This paper presents a model for subsurface scattering 
in layered surfaces in terms of one-dimensional linear transport theory. We derive explicit formulas 
for backscattering and transmission that can be directly incorporated in most rendering systems, and 
a gen­eral Monte Carlo method that is easily added to a ray tracer. This model is particularly appropriate 
for common layered materials appearing in nature, such as biological tissues (e.g. skin, leaves, etc.) 
or inorganic materials (e.g. snow, sand, paint, varnished or dusty surfaces). As an application of the 
model, we simulate the appearance of a face and a cluster of leaves from experimental data describing 
their layer properties. CR Categories and Subject Descriptors: I.3.7 [Computer Graphics]: Three-Dimensional 
Graphics and Realism. Additional Key Words and Phrases: Re.ection models, integral equations, Monte Carlo. 
  Motivation An important goal of image synthesis research is to develop a comprehensive shading model 
suitable for a wide range of ma­terials. Recent research has concentrated on developing a model of specular 
re.ection from rough surfaces from .rst principles. In particular, the micro-facet model .rst proposed 
by Bouguer in 1759 [4], and developed further by Beckmann[1], Torrance &#38; Sparrow[26], and others, 
has been applied to computer graphics by Blinn [2] and Cook &#38; Torrance[8]. A still more comprehen­sive 
version of the model was recently proposed by He et al[12]. These models have also been extended to handle 
anisotropic mi­crofacets distributions[24, 5] and multiple scattering from complex microscale geometries[28]. 
Another important component of surface re.ection is, however, diffuse re.ection. Diffuse re.ection in 
computer graphics has al­most universally been modeled by Lambert s Cosine Law. This law states that 
the exiting radiance is isotropic, and proportional to the surface irradiance, which for a light ray 
impinging on the surface from a given direction depends on the cosine of the angle Permission to copy 
without fee all or part of this material is granted  provided that the copies are not made or distributed 
for direct provided that the copies are not made or distributed for direct commercial advantage, the 
ACM copyright notice and the title of the commercial advantage, the ACM copyright notice and the title 
of the publication and its date appear, and notice is given that copying is by publication and its date 
appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
permission of the Association for Computing Machinery. To copy  otherwise, or to republish, requires 
a fee and/or specific permission. otherwise, or to republish, requires a fee and/or specific permission. 
 &#38;#169;1993 -0---8/93/008/0015 $1.50 &#38;#169;1993 ACM -0-89791 -601 -8/93/008 $1.50 of incidence. 
Diffuse re.ection is qualitatively explained as due to subsurface scattering [18]: Light enters the material, 
is absorbed and scattered, and eventually exits the material. In the process of this subsurface interaction, 
light at different wavelengths is differ­entially absorbed and scattered, and hence is .ltered accounting 
for the color of the material. Moreover, in the limit as the light ray is scattered multiple times, it 
becomes isotropic, and hence the di­rection in which it leaves the material is essentially random. This 
qualitative explanation accounts for both the directional and col­ormetric properties of diffuse materials. 
This explanation is also motivated by an early proof that there cannot exist a micro-facet distribution 
that causes equal re.ection in all outgoing directions independent of the incoming direction [10]. The 
above model of diffuse re.ection is qualitative and not very satisfying because it does not refer to 
any physical param­eter of the material. Furthermore, there is no freedom to adjust coef.cients to account 
for subtle variations in re.ection from dif­ferent materials. However, it does contain the essential 
insight: an important component of re.ection can arise from subsurface scattering. In this paper, we 
present a model of re.ection of light due to subsurface scattering in layered materials suitable for 
com­puter graphics. The only other work in computer graphics to take this approach is due to Blinn, who 
in a very early paper presented a model for the re.ection and transmission of light through thin clouds 
of particles in order to model the rings of Saturn[2]. Our model differs from Blinn s in that it is based 
on one-dimensional linear transport theory a simpli.cation of the general volume rendering equation [19] 
and hence is considerably more general and powerful. Of course, Blinn was certainly aware of the trans­port 
theory approach, but chose to present his model in a simpler way based on probabilistic arguments. In 
our model the relative contributions of surface and subsur­face re.ection are very sensitive to the Fresnel 
effect (which Blinn did not consider). This is particularly important in biological tis­sues which, because 
cells contain large quantities of water, are translucent. A further prediction of the theory is that 
the sub­surface re.ectance term is not necessarily isotropic, but varies in different directions. This 
arises because the subsurface scattering by particles is predominantly in the forward direction. In fact, 
it has long been known experimentally that very few materials are ideal diffuse re.ectors (for a nice 
survey of experiments pertaining to this question, see [18]). We formulate the model in the currently 
emerging standard terminology for describing illumination in computer graphics [16, 11]. We also discuss 
ef.cient methods for implementation within the context of standard rendering techniques. We also describe 
how to construct materials with multiple thin layers. Finally, we apply the model to two examples: skin 
and leaves. For these examples, we build on experimental data collected in the last few years, and provide 
pointers to the relevant literature. Another goal of this paper is to point out the large amount of recent 
work in the applied physics community in the application of linear transport theory to modeling appearance. 
 z Figure 1: The geometry of scattering from a layered surface (.i,fi) Angles of incidence (incoming) 
(.r,fr) Angles of re.ection (outgoing) (.t,ft) Angles of transmission L(z; ., f) Radiance [W / (m2 sr)] 
Incident (incoming) radiance Li Re.ected (outgoing) radiance Lr Transmitted radiance Lt forward-scattered 
radiance L+ backward-scattered radiance L- fr(.i,fi; .r,fr) BRDF ft(.i,fi; .t,ft) BTDF Surface or boundary 
BRDF fr,s(.i,fi; .r,fr) Surface or boundary BTDF ft,s(.i,fi; .t,ft) Volume or subsurface BRDF fr,v(.i,fi; 
.r,fr) Volume or subsurface BTDF ft,v(.i,fi; .t,ft) n Index of refraction Scattering cross section [mm-1] 
ss(z; .) Absorption cross section [mm-1] sa(z; .) Total cross section (st = sa + ss) [mm-1] st(z; .) 
W Albedo (W = ss ) st d Layer thickness [mm] p(z; ., f; .',f'; .) Scattering phase function ((.',f') 
to (., f)) Table 1: Nomenclature  Re.ection and Transmission due to Layered Surfaces As a starting point 
we will assume that the re.ected radiance Lr from a surface has two components. One component arises 
due to surface re.ectance, the other component due to subsurface volume scattering. (The notation used 
in this paper is collected in Table 1 and shown diagramatically in Figure 1.) Lr(.r,fr)= Lr,s(.r,fr)+ 
Lr,v (.r,fr) where: Lr,s -re.ected radiance due to surface scattering Lr,v -re.ected radiance due to 
volume or subsurface scattering The models developed in this paper also predict the transmis­sion through 
a layered surface. This is useful both for materials made of multiple layers, as well as the transmission 
through thin translucent surfaces when they are back illuminated. The transmit­ted radiance has two components. 
The .rst component is called the reduced intensity; this is the amount of incident light trans­mitted 
through the layer without scattering inside the layer, but accounting for absorption. The second is due 
to scattering in the volume. Lt(.t,ft)= Lri(.t,ft)+ Lt,v(.t,ft) where: Lri -reduced intensity Amplitude 
1.0 0.8 0.6 0.4 0.2 0.0 Figure 2: Fresnel transmission and re.ection coef.cients for a ray leaving air 
(n =1.0) and entering water (n =1.33). Lt,v -transmitted radiance due to volume or subsurface scat­tering 
The bidirectional re.ection-distribution function (BRDF) is de­.ned to the differential re.ected radiance 
in the outgoing direction per differential incident irradiance in the incoming direction [23]. Lr(.r,fr)fr(.i,fi; 
.r,.r) = Li(.i,fi) cos .id.i The bidirectional transmission-distribution function (BTDF) has a similar 
de.nition: Lt(.t,ft)ft(.i,fi; .t,.t) = Li(.i,fi) cos .id.i Since we have separated the re.ected and transmitted 
light into two components, the BRDF and BTDF also have two components. fr = fr,s + fr,v ft = fri + ft,v 
If we assume a planar surface, then the radiance re.ected from and transmitted across the plane is given 
by the classic Fresnel coef.cients. Lr(.r,fr)= R12(ni,nt; .i,fi . .r,fr)Li(.i,fi) Lt(.t,ft)= T 12(ni,nt; 
.i,fi . .t,ft)Li(.i,fi) where R12(ni,nt; .i,fi . .r,fr)= R(ni,nt, cos .i, cos .t) 22 nn tt T 12(ni,nt; 
.i,fi . .t,ft)= 2 T = 2(1 - R) nn ii where R and T are the Fresnel re.ection formulae and are de­scribed 
in the standard texts (e.g. Ishimura[14]) and .t is the angle of transmission. Besides returning the 
amount of re.ection and transmission across the boundary, the functions R12 and T 12, as a side effect, 
compute the re.ected and refracted angles from the Re.ection Law (.r = .i) and Snell s Law (ni sin .i 
= nt sin .t). Note also the factor of (nt/ni)2) in the transmitted coef.cient of the above formula; this 
arises due to the change in differential solid angle under refraction and is discussed in Ishimura[pp. 
154­155]. Plots of the Fresnel functions for the boundary between air and water are shown in Figure 2. 
In our model of re.ection, the relative contributions of the sur­face and subsurface terms are modulated 
by the Fresnel coef.­cients. fr = Rfr,s + Tfr,v = Rfr,s + (1 - R)fr,v Thus, an immediate prediction of 
the model is that re.ection due to subsurface scattering is high when Fresnel re.ection is low, since 
more light enters the surface layer. Notice in Figure 2 that the percentage of transmission is very high 
for a quite wide range of angles of incidence. Thus, the re.ectance properties of materials impregnated 
with water or oil (dielectrics with low indices of refraction) are dominated by subsurface re.ectance 
components at near perpendicular angles of incidence, and surface components at glancing angles of incidence. 
Actually, light returning from the subsurface layers must refract across the boundary again. Thus, it 
will be attenuated by yet an­other Fresnel transmission factor. Recall that if light returns from a media 
with a higher index of refraction, then total internal re.ec­tion may occur. All light with an incident 
angle greater than the critical angle (.c = sin-1 ni/nt) will not be transmitted across the boundary. 
By assuming an isotropic distribution of returning light, we can compute the percentage that will be 
transmitted and hence considered re.ected. This sets an upper bound on the subsurface re.ectance of 1 
- (ni/nt)2 (remember, nt >ni). For example, for an air-water boundary, the maximum subsurface re.ectance 
is approximately .44.  Description of Materials The aim of this work is to simulate the appearance of 
natural ma­terials such as human skin, plant leaves, snow, sand, paint, etc. The surface of these materials 
is comprised of one or more layers of material composed of a mixture of randomly distributed parti­cles 
or inhomogeneities embedded in a translucent media. Particle distributions can also exist, in which case 
the properties are the material are given by the product of each particle s properties times the number 
of particles per unit volume. The layers of such materials can be described by a set of macro­scopic 
parameters as shown in the following table. Measurements of these properties have been made for a large 
variety of natural materials. Symbol Property n index of refraction sa [mm-1] absorption cross section 
ss [mm-1] scattering cross section d [mm] depth or thickness p(cos j) scattering phase function g mean 
cosine of phase function Index of Refraction The materials considered are dielectrics where n is on 
the order of the index of refraction of water (1.33). Absorption and scattering cross section The intensity 
of the backscattered and transmitted light de­pends on the absorption and scattering properties of the 
mate­rial. The cross section may be interpreted as the probability per unit length of an interaction 
of a particular type. The total scattering cross section st = sa + ss. The mean free path is equal to 
the reciprocal of the total cross section. An important quantity is the albedo, which equals W = ss/st. 
If the albedo is close to 1, the scattering cross section is much greater than the absorption cross section, 
whereas if the albedo is close to 0, absorption is much more likely than scattering. Scattering phase 
function The phase function, p(xx; ., f; .',f') represents the direc­tional scattering from (.',f') to(., 
f) of the light incident onto a particle. This function depends on the nature of the scattering medium. 
The form of p is affected by the size, Figure 3: Henyey-Greenstein phase function for g = -.3 and g = 
.6. form and orientation of the suspended particles, the dielectric properties of the particles, and 
the wavelength of the incident light. The scattering of light from particles small compared to the wavelength 
of light is given by the Rayleigh scatter­ing formula, and the scattering due to dielectric spheres of 
different radii by the Mie formula. However, most materials contain distributions of particles of many 
different sizes, so simple single particle phase func­tions are not applicable. For this reason, we describe 
the ma­terial phase function with the empirical formula, the Henyey-Greenstein formula[13]. 11 - g 2 
pHG(cos j)= 4p (1 + g2 - 2g cos j)3/2 where j is the angle between the incoming and the outgoing direction 
(if the phase function depends only on this an­gle the scattering is symmetric about the incident direction). 
The Henyey-Greenstein formula depends on a single param­eter g, the mean cosine of the scattered light. 
The Henyey-Greenstein phase function for different values of g is shown in Figure 3. Note that if g = 
0 the scattering is isotropic, whereas positive g indicates predominantly forward scatter­ing and negative 
g indicates predominantly backward scat­tering. In the model employed in this paper, material properties 
are described macroscopically as averages over the underlying mi­croscopic material property de.nitions. 
If the material is made of several components, the resulting properties of the composite materials can 
be computed by simple summation. n sa = wi sa,i i=1 n ss p(cos j, g)= wi ss,i p(cos j, gi) i=1 and 
so on. Here wi is the volume fraction of the volume occupied by material i. Another very important property 
of real materials is that the properties randomly vary or .uctuate. Such .uctuations cause variation 
in the appearance of natural surfaces. This type of .uc­tuation is easy to model with a random noise 
function or a texture map. Optical propagation in random media has been studied in a va­riety of applications, 
including blood oximetry, skin photometry, plant physiology, remote sensing for canopies and snow, the 
paint and paper industry, and oceanic and atmospheric propagation. For many examples the macroscopic 
parameters have been measured across many frequency bands. A major attempt of our work is the simulation 
of the appearance of natural surfaces by using mea­sured parameters to be inserted into the subsurface 
re.ection and transmission formulas. This approach is similar to the attempt of Cook &#38; Torrance [8] 
to simulate the appearance of metallic sur­faces by using appropriate values for the refractive index 
and the roughness parameters. 4 Light Transport Equations Linear transport theory is a heuristic description 
of the propagation of light in materials. Transport theory is an approximation to elec­tromagnetic scattering 
theory, and hence cannot predict diffrac­tion, interference or quantum effects. In particular, the specular 
re.ection of light from rough surfaces whose height variation is comparable in size to the wavelength 
of incident light requires the full electromagnetic theory as is done in He et al[12]. A nice discussion 
of the derivation of transport theory from electromag­netism and the conditions under which it is valid 
is contained in an recent article by Fante[9]. The applicability of transport theory, however, has been 
veri.ed by its application to a large class of practical problems involving turbid materials, including 
inorganic materials such as ponds, atmospheres, snow, sand and organic materials such as human skin and 
plant tissue[14]. Transport theory models the distribution of light in a volume by a linear integro-differential 
equation. .L(xx, ., f) = .s ' '' '' -stL(xx, ., f)+ ss p(x ; ., f; . ,f ' )L(xx,. ,f ) d. df This equation 
is easily derived by accounting for energy balance within a differential volume element. It simply states 
that the change in radiance along a particular in.nitesimal direction ds consists of two terms. The .rst 
term decreases the radiance due to absorption and scattering. The second term accounts for light scattered 
in the direction of ds from all other directions. Thus, it equals the integral over all incoming directions. 
For layered media, the assumption is made that all quantities only depend on z and not on x and y. This 
assumption is valid if the incoming illumination is reasonably constant over the region of interest. 
It is also roughly equivalent to saying the re.ected light emanates from the same point upon which it 
hits the surface. With this assumption, the above equation simpli.es to .L(., f) cos . = .z . ' ' ' ' 
 ' ) L(. ' ) d. -stL(., f)+ ss p(z; ., f; . ,f ,f df The above equation is an integro-differential equation. 
It can be converted to an equivalent double integral equation, whose solution is the same as the original 
integro-differential equation. L(z; ., f)= z . . dz.. . z - st cos . e 0 ss(z .) p(z .;.,f;..,f.) L(z 
.;..,f.) d.. dz. 0 cos . This is the basis of most current approaches to volume rendering. The 1-dimensional 
linear transport equation must also satisfy certain boundary conditions. This is most easily seen by 
consid­ering the forward and the backward radiance separately. L(., f)= L+(., f)+ L-(p - ., f) Where 
L+ is energy propagating in the positive z direction, and L- in the negative direction. Note that L- 
is de.ned to be a function of of p - ., the angle between the backward direction of propagation and the 
negative z axis. It is important to re­member this convention when using formulas involving backward 
radiances. At the top boundary the forward radiance is related to the inci­dent radiance. ' '' L+(z = 
0; . ,f ' )= ft,s(.i,fi; .,f ) Li(.i,fi) d.i This simply states that the forward component of radiance 
entering the volume at the boundary is due to light transmitted across the surface. If we assume a planar 
surface and parallel incident rays, then ft,s equals the Fresnel transmission term times a d-function 
that picks up the appropriate angle of incidence. '' L+(z = 0; . ,f ' )= T 12(ni,nt; .i,fi . . ,f ' )Li(.i,fi) 
In the more general case of a rough surface, ft,s is given by a transmission coef.cient times the probability 
that light will refract in the desired direction. The boundary conditions at the top let us formally 
state the contribution to re.ection due to subsurface scattering in terms of the solution of the integral 
equation at the boundary z = 0. Lr,v(.r,fr)= ft,s(., f; .r,fr) L-(z = 0; ., f) d. Assuming a planar surface, 
this integral simpli.es to Lr,v(.r,fr)= T 21(ni,nt; ., f . .r,fr)L-(z = 0; ., f) Similar reasoning allows 
the transmitted radiance to be deter­mined from the boundary conditions at the bottom boundary. Lt,v(.t,ft)= 
ft,s(., f; .t,ft) L+(z = d; ., f) d. Once again, assuming a smooth surface, Lt,v(.t,ft)= T 23(n2,n3; 
., f . .t,ft)L+(z = d; ., f) Thus, the determination of the re.ection functions has been reduced to the 
computation of L-(z = 0) and L+(z = d) the solution of the one-dimensional transport equation.  5 Solving 
the Integral Equation There are very few cases in which integro-differential equations can be directly 
solved. The most famous solution is for the case of isotropic scattering and was derived by Chandrasekhar[7, 
p. 124]. Even for this simple phase function the solution is anisotropic. The classic way to solve such 
an equation is to write it in terms of the Neumann series. Physically, this can be interpreted as expanding 
the solution in terms of the radiance due to an integer number of scattering events. That is, 8 (i) L 
= L i=0 where L(0) is the direct radiance assuming no scattering, L(1) is the radiance due to a single 
scattering event, and L(i) is the radiance due to i scattering events. Similar equations apply to the 
forward (i)(i) and backward radiances, L+ and L- . The radiance due to the i scattering events can be 
written using the following recurrence. L(i+1)(z; ., f)= . dz.. z . z - st . cos . e 0 ss(z .) p(z .;.,f;..,f.) 
L(i)(z . ;..,f.) d.. dz. 0 cos . This is the basis for most iterative approaches for numerically calculating 
transport quantities.  Figure 4: Solutions for f (1) and f (1) for different values of g and r,v t,v 
td. From left to right the phase function shifts from predominately backward scattering (g = -0.3) to 
isotropic scattering (g =0.0) to forward scattering (g =0.6). From top to bottom the optical depth of 
the layer increases from 0.5 to 1.0 to 2.0. 5.1 First-Order Approximation Another classic result in 
radiative transport, also derived by Chan­drasekhar[7], is the analytic solution to the integral equation 
as­suming only a single scattering event. As mentioned previously, this is equivalent to the method described 
by Blinn but derived using a completely different technique [2]. The 0th-order solution assumes that 
light is attenuated by the scattering and absorption, but not scattered. The attenuated inci­dent light 
is called the reduced intensity and equals (0) -t/ cos . L+(z)= L+(z = 0)e Here, . z t(z)= st dz 0 is 
called the optical depth. If st is constant, then td = std. Using the boundary conditions for incident 
and re.ected light, and also rewriting the above equation in terms of the angles of incidence and re.ection, 
we arrive at the following formula for the 0th-order transmitted intensity (0) 12 23 Lt,v(.t,ft)= TTe 
-td Li(.i,fi) By substituting the 0th-order solution, or reduced intensity, into the integral equation, 
the 1st-order solutions for forward and back­ward scattering can be calculated. The details of this calculation 
are described in Chandrasekhar and Ishimura and there is no need to repeat them here. Using the boundary 
conditions for incident and re.ected light, and also rewriting in terms of the angles of incidence and 
re­.ection, we arrive at the following formula for the backscattered radiance: (1) Lr,v(.r,fr)= 1221 
cos .i -td(1/ cos .i+1/ cos .r))Li(.i,fi) WT T p(p-.r,fr;.i,fi) (1-e cos .i+cos .r This general formula 
shows that the backscattered light intensity depends on the Fresnel transmission coef.cients, the albedo, 
the layer depth, and the backward part of the scattering phase function. Figure 5: Solutions for fr and 
ft. In the left column is the surface specular re.ection and in the middle is the subsurface re.ection 
and transmission. On the right is the sum of surface and subsurface modulated by the Fresnel coef.cients. 
From top to bottom the angle of incidence increases from 10 to 40 to 65 degrees. A special case of this 
equation is Seeliger s Law, the .rst at­tempt to model diffuse re.ection from .rst principles[25]. Seel­iger 
s Law can be derived by assuming a semi-in.nite layer (td = 8) and ignoring Fresnel effects. cos .i Lr,v(.r,fr)= 
Li(.i,fi) cos .i + cos .r At the boundary z = d, the forward scattered radiance is given by (1) Lt,v(.t,ft)= 
12cos .i -td/ cos .i -e -td/ cos .t )Li(.i,fi) WT T 23 p(.t,ft;.i,fi)(e cos .i-cos .t For cos .t = cos 
.i, the singular factors can be avoided by using L Hospital s rule, yielding (1) 1223 td -td/ cos .t 
Li(.t,ft) Lt,v(.t,ft)= WT T p(.t,ft; .t,ft) e cos .t Figure 4 shows fr,v and ft,v for various values 
of g and d. Figure 5 shows the surface and subsurface components of the re.ection model for various angles 
of incidence. These re.ection and transmission distribution functions have several interesting properties: 
1. The re.ection steadily increases as the layer becomes thicker; in contrast, the transmission due to 
scattering increases to a point, then begins to decrease because of further scattering events. 2. Subsurface 
re.ection and transmission can be predominately backward or forward depending on the phase function. 
 3. As the angle of incidence becomes more glancing, the surface scattering tends to dominate, causing 
both the re.ection and the transmission due to subsurface scattering to decrease. 4. Due to the Fresnel 
effect, the re.ection goes to zero at the horizons. Also, the re.ection function appears .attened relative 
to a hemicircle. Thus, re.ection for near normal angles of incidence varies less than Lambert s Law predicts. 
 5. The distributions vary as a function of re.ection direction. Lambert s Law predicts a constant re.ectance 
in all direc­tions (which would be drawn as a hemicircle in these dia­grams).  Figure 6: Determining 
.rst-order solutions for multiple layers. On the left, the contribution to the .rst order solution for 
a single layer. One the right, the contribution to the .rst order solution due to re.ectance off a single 
layer. The above formulas can be used to generate .rst-order solutions for multiple layers. (This is 
shown diagrammatically in Figure 6.) The total .rst-order scattering will be the sum of the .rst-order 
scattering from each layer, weighted by the percentage of light making it to the layer and returning 
from the layer. The percentage of light making it to the layer is the product of the 0th-order transmission 
functions (or reduced intensity) for a path through the layers above the re.ecting layer. Similarly, 
the percentage of light leaving the entire layer after re.ection is equal to the product of the 0th-order 
transmission functions for the path taken on the way out. Note that across each boundary the light may 
refract, and thus change direction and be attenuated by the Fresnel coef.cient, but this is easy to handle. 
The process simpli.es, of course, if each layer has the same index of refraction, since no re.ection 
or change of direction occurs between layers. Given the above formulas it is very easy to construct a 
procedure to perform this calculation and we will make use of it in the results section. The above formula 
can also be generalized to include re.ection from a boundary between layers. In many situations re.ection 
can only occur from the bottom layer. In this case, we add a single term accounting for the reduced intensity 
to reach the lower bound­ary, and also weight the returning light from that boundary. Such a model is 
commonly employed to model the re.ection of light from a pool of water[15], and has been employed by 
Nishita and Nakamae[22]. Further generalizations of this type are described in Ishimura[14, p. 172]. 
  Multiple Scattering The above process of substituting the ith-order solution and then computing the 
integral to arrive at the (i+1)th-order solution can be repeated, but is very laborious. Note that subsequent 
integrals now involve angular distributions, because, although the input ra­diance is non-zero in only 
a single direction, the scattered radi­ance essentially comes from the directional properties of the 
phase function. Thus, this approach to solving the system analytically quickly becomes intractable. We 
have implemented a Monte-Carlo algorithm for computing light transport in layered media. This algorithm 
is described in Figure 7. A thorough discussion of the application of Monte Carlo algorithms for layered 
media is discussed in the book [21], and the techniques we are using are quite standard. To investigate 
the effects of multiple scattering terms, we sim­ulated a semi-in.nite turbid media with different albedos. 
The re­.ectance was computed and when the particles returning from the media are scored, we keep track 
of how many scattering events they underwent. Figure 8 shows the results of this experiment. The top 
curve is the total re.ectance, and the lower curves rep­ 1 Initialize: A particle enters the layer at 
the origin. Initialize 8p to the origin and the direction 8s to the direction at which the ray enters 
the layer. Set the weight w = 1. 2 Events: Repeat the following steps until the ray weight drops below 
some threshold or the ray exits the layer. 2A Step: First, estimate the distance to the next interaction: 
log r d = - st Where r in this and the following formulas is a uniformly distributed random number between 
0 and 1. Then, com­pute the new position: p8= 8p + d 8s And, .nally set the particle weight to ss w = 
w ss + sa Note: If d causes the particle to leave the layer, break from the repeat loop and adjust the 
weight using the distance to the boundary. 2B Scatter: First, estimate the cosine of the scattering angle 
for the Henyey-Greenstein phase function using the following formula. 11 - g2 cos j = (1+ g 2 - ( )2) 
|2g| 1 - g +2gr and cos f and sin f with f =2pr. Then, compute the new direction: (8s.x cos f cos . 
- 8s.y sin f)/ sin . 8t =(8s.y cos f cos . + 8s.x sin f)/ sin . sin . 8s = 8s cos j + 8t sin j Here, 
cos . = 8s.z and sin . =1 - 8s.z2. Note: Care must be taken if sin . = 0. 3 Score: Divide the sphere 
into regions of equal solid angle and add the weight of the particle to the weight associated with the 
bin in which it is contained. Figure 7: Basic Monte Carlo algorithm for layered media resent scattering 
up to some order. Note that when the albedo is high, implying that ss >> sa, the .rst order term is only 
a small percentage of the total re.ectance. However, as the albedo decreases, corresponding to greater 
absorption, a few low-order terms accurately approximate the re.ectance. This effect can be explained 
by recalling that each term in the Neumann series rep­resenting the re.ection is on the order of W i, 
and since W is always less than one, the magnitude of higher-order terms quickly goes to zero. We have 
also computed the BRDF as a function of the angle of re.ection using our Monte Carlo algorithm for the 
same con.gu­ration as described in the last experiment. The results are shown in Figure 9. Recall that 
the 1st-order re.ection due to a semi­in.nite media is given by Seeliger s Law: cos .i/(cos .i + cos 
.r). The computed 1st-order BRDF matches the theoretical result quite well. In this .gure we also plot 
the total BRDF due to any num­ber of scattering events, and the difference between the total and the 
1st-order BRDF. Note as in the previous experiment when the albedo W is small, the BRDF is closely approximated 
by the 1st­order term. However, note that the shape of the re.ection function is also largely determined 
by the shape of the 1st-order re.ection, which in turn is largely determined by the phase function. Fur­ 
0.10 0.10 1.0 0.08 0.08 0.9 0.8 0.06 0.06 0.7 Reflectance 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Figure 8: A plot 
of re.ectance versus albedo for a semi-in.nite  0.04 0.04 0.02 0.02 0.00 0.00 Figure 9: Graphs of the 
BRDF (fr) as a function of the angle of re.ection for a semi-in.nite slab with different albedos (on 
the left W =0.4 and on the right W =0.8) and an angle of inci­dence of 45. . The solid line is the theoretical 
BRDF as given by Seeliger s Law (the superimposed dashed line is the computed 1st-order BRDF showing 
a good match). The top dashed curve is the total computed BRDF; The bottom dotted curve is the dif­ference 
between the total BRDF due to multiple scattering events media. The top curve is the total re.ectance 
(the total radiant energy per unit area re.ected divided by the incident irradiance). The bottom curve 
is the re.ectance assuming only a single scat­tering event. Moving upward is a sequence of curves consisting 
of additional terms corresponding to a single additional scatter­ing event. The .rst 10 terms in the 
solution are shown; In our simulations, we recorded terms involving thousands of scattering events. ther, 
observe that the difference between the 1st-order solution and the full solution is approximately independent 
of the angle of re.ection. Thus, the sum of the higher order terms roughly obeys Lambert s Law. For this 
reason it is often convenient to divide the subsurface re.ection into two terms: Lr,v(.r,fr)= L(1)(.r,fr)+ 
Lm where Lm is constant and represents the sum of all the multiple scattering terms. Finally, we have 
begun preliminary experiments where we in­corporate a Monte Carlo subsurface ray tracer within a standard 
ray tracer. When the global ray tracer calls the subsurface ray tracer it attempts to estimate the BRDF 
and BTDF to a particular light source. This is done by biasing the Monte Carlo procedure to estimate 
the energy transported to the light. A simple method to do this is to send a ray to the light at each 
scattering event, as described in Carter and Cashwell[6]. This ray must be weighted by the phase function 
and the attenuation caused by the traversal through the media on the way to the light. If the albedo 
is less than 1, then only a few scattering events are important, and thus the subsurface ray tracer consumes 
very little time on average (the cost is proportional the the mean number of scattering events). Also, 
since the subsurface ray tracer does not consider the global environment when tracing its rays, the cost 
of subsurface Monte Carlo simulation at every shading calculation is relatively low. The advantage of 
this approach is that the BRDF s do not have to precomputed, and so if material parameters are varying 
across the surface, the correct answer is still estimated correctly at each point. Results The subsurface 
scattering models developed in this paper has been tested on two common natural surfaces: human skin 
and plant leaves. The goal of these experiments are twofold: First, to compare our anisotropic diffuse 
re.ection model with Lambertian shading. Second, to attempt to simulate the optical appearance from measured 
parameters. Our experiments are meant to be sug­ and the 1st-order BRDF. Property Epidermis Dermis Pigment 
Blood n sa [mm-1 ] ss [mm-1] d [mm] g 1.37-1.5 3.8 50.0 0.001-0.15 0.79 1.37-1.5 0.3 21.7 1-4 0.81 1.37-1.5 
.79 1.37-1.5 32.6 0.96 .0 Table 2: Two Layer Skin Model Properties. Pigment coef.cients are mixed with 
epidermal coef.cients to compute the properties of the outer layer. Blood coef.cients are mixed with 
dermal co­ef.cients to compute the properties of the inner layer. gestive of the power of this approach; 
we do not claim to have an experimentally validated model. 7.1 Skin Human skin can be modeled as two 
layers with almost homo­geneous properties. Both layers are assumed to have the same refractive index 
but a different density of randomly distributed absorbers and scatterers. The outer epidermis essentially 
consists of randomly sized tissue particles and imbedded pigment parti­cles containing melanin. The pigment 
particles act as strongly wavelength dependent absorbers causing a brown/black coloration as their density 
increases. The inner dermis is considered to be a composition of weakly absorbing and strongly scattering 
tissue material and of blood which scatters light isotropically and has strong absorption for the green 
and blue parts of the spectrum. Experimental evidence also supports the hypothesis that light scat­tering 
in the skin is anisotropic with signi.cant forward scattering. A comprehensive study of optical properties 
of human skin can be found in van Gemert et al.[27]. The values chosen for our test pictures are given 
in Table 2. We also add a thin outer layer of oil that re.ects light using the Torrance-Sparrow model 
of rough surfaces. A head data set was acquired using a medical MRI scanner. Unfortunately, the ears 
and the chin were clipped in the process, but enough of the head is visible to test our shading models. 
A volume ray tracer was adapted to output the position and normal vector of the skin layer for each pixel 
into a .le, and this input was used to evaluate the shading models described in this paper. The in.uence 
of the various factors appearing in the subsur­face re.ection formula are shown on Plate 1. These pictures 
are On the right, an specular surface term is added to simulate an oily coat. In these pictures g = .65. 
 Plate 1. Plate 2. not shaded in the conventional way. In particular, a Lambertian shading model would 
yield a constant image. The .rst picture (upper left) shows the in.uence of the Fresnel factors. Observe 
that the intensity is almost .at, but strongly attenuated for glancing incident and viewing angles. The 
second picture (upper middle) shows the action of Seeliger s Law alone. Seeliger s Law leads to very 
little variation in shading, which makes the surface appear even more chalky or dusty. The third picture 
(upper right) demon­strates the action of the factor accounting for the .nite layer depth giving only 
weak enhancements for glancing angles. This is a mi­nor effect. The fourth picture (lower left) shows 
the in.uence of the Henyey-Greenstein scattering phase function for small back­ward scattering (g = -.25) 
and the .fth picture (lower middle) shows the effect of large forward scattering (g = .75). The result 
is strong enhancement of glancing re.ection for low angles of in­cidence and viewing, assuming they are 
properly aligned. The last picture (lower right) shows the superposition of these four factors with g 
= .75 giving a complex behavior. An overall smoothing of the re.ection appears; the surface appears to 
be more silk-like (see also Plate 3). Although these effects are all subtle, their com­bination when 
controlled properly can create a wide variation in appearance. The appearance of the face with the new 
subsurface re.ection model is compared to the Lambertian diffuse re.ection model for different angles 
of incidence in Plate 2. The left column shows the results for the Lambert scattering for angles 0 and 
45 degrees, and the middle column is rendered for the new model. Again, Plate 4: Human face with variation 
in subsurface blood concen­tration, an oily outer layer and Gaussian variation in parameters to create 
the freckles. notice a much smoother silk-like appearance. The right column gives the relative difference 
of both models, red indicates more re.ection from the new model, and blue vice versa. To illustrate the 
degrees of freedom of the model, we rendered several faces with their parameters controlled by texture 
maps. One texture map controls the relative concentration of blood in the dermis; another texture map 
controls the concentration of melanin in the epidermal layer. These faces are shown in Plates 3 and 4. 
To create a dark complexion we modulate the percentage of pigment in the otherwise transparent epidermis. 
This creates a dark brown appearance due to the strong absorption of melanin (in this case we set the 
absorption to .6). For the lips the epidermis is set to be very thin such that the appearance is dominated 
by the re.ection from the dermis which has for the lips a large blood content (strong absorption for 
green and blue light component). The epidermis pigment part also has been varied locally with about 20% 
with a Gaussian process. This allows us to create a wide variety of skin colors, from black to suntanned 
to Caucasian, and from .ushed to burnt to relaxed. The pictures in Plate 3 also show the effect of an 
additional specular term due to a thin layer of oil on the skin. Finally, Plate 4 shows another picture 
created by our program.  Plate 5: Leaf Model. On the left is the albedo image and on the right is a 
thickness image (white indicates thick) This picture took approximately 20 seconds to render on a Silicon 
Graphics Personal Iris. 7.2 Leaves Figure 10 shows an idealized leaf in cross-section. The leaf is composed 
of several layers of cells. On the top and bottom are epidermal cells with a thin smooth, waxy cuticular 
outer layer. The waxy cuticular layer is largely responsible for specularly re.ected light. Below the 
upper epidermal cells there are a series of long palisaide cells which are highly absorbing due to the 
numerous chloroplasts contained within them. Below the palisaide cells are a loosely packed layer of 
irregularly shaped spongy cells. The spaces between the spongy cells are .lled with air, which causes 
them to scatter light. Both the palisaide and the spongy cells are quite large (approximately 20 µm) 
compared to the wavelength, so their scattering phase function is forward directed. Furthermore, the 
cells are high in water content, so the index of refraction of the leaf is approximately equal to that 
of water 1.33. A typical leaf is .5 to 1 mm thick, with an optical depth of 5 to 10. To test our model 
on a leaf, we constructed a leaf model us­ing the technique described in Bloomenthal[3]. Although spectral 
transmission and re.ectance curves are available for leaves[29], we have set the color of the leaf from 
an image acquired from a digital scanner. An albedo image is texture mapped onto a series of simply-shaped, 
bent polygons to create the leaf. Where the tex­ture map is transparent the polygon is considered transparent 
and the leaf is not visible. We also modulate the thickness of the leaf with a thickness map drawn on 
top of the original leaf image. The texture maps we used are shown in Plate 5. The waxy cuticle is modeled 
using a rough specular surface with a specular exponent of 10. The interior of the leaf is modeled as 
a single homogeneous layer with an optical depth of 5 and a mean scattering cosine of .3[20]. Pictures 
were generated by modifying a conventional ray tracer Plate 6: A cluster of leaves. A series of leaf 
images under dif­ferent simulated lighting conditions. On the left are two backlit images, on the right, 
front lit. to account for subsurface re.ection and transmission. When a ray encounters a leaf, the BRDF 
and BTDF are evaluated for direct illumination from light sources. Shadow rays are cast to the light 
source, and if the ray stabs any other leaves the light intensity is attenuated by the 0th-order transmission 
function through each leaf. Plate 6 shows a picture of a cluster of leaves with the sun in different 
positions. Note that the re.ection from leaves is largely determined by specular re.ection due to the 
waxy cuticle; there is very little diffuse re.ection and hence when the light source is on the same side 
of the leaf as the viewer, the leaf is quite dark. The transmission term, however, can be quite large, 
and therefore the leaves may actually be brighter when the are illuminated from behind. Note also that 
the increased thickness of the veins cause dark shadows to be cast on other leaves. The veins also appear 
dark when the leaf is back lit because they absorb more light, and bright when the leaf is front lit 
because their increased thickness causes more light to be re.ected. 8 Summary and Discussion We have 
presented a re.ectance model consisting of two terms: the standard surface re.ectance and a new subsurface 
re.ectance due to backscattering in a layered turbid media. This model is applicable to biological and 
inorganic materials with low indices of refraction, because their translucent nature implies that a high 
percentage of the incident light enters the material, and so the subsurface re.ection is quite large. 
This model incorporates di­rectional scattering within the layer, so the resulting subsurface re.ection 
is not isotropic. This model can be interpreted as a the­oretical model of diffuse re.ectance. Thus, 
this model predicts a directionally varying diffuse re.ection, in contrast to Lambert s Law. However, 
if multiple scattering contributes signi.cantly to the re.ection, then the higher scattering terms contribute 
to a re­.ection function with roughly the same shape. As in any model, our model makes many assumptions. 
The two most important are that the physical optics may be approximated with transport theory, and that 
the material can be abstracted into layered, turbid media with macroscopic scattering and absorption 
properties. An exact model of biological tissues would explicitly model individual cells, organelles 
and so on, in considerably more detail. The Monte-Carlo algorithm for simulating re.ection by Westin 
et al.[28] is an example of such an approach. Although such an approach may seem more accurate, often 
the experimental data needed to describe the arrangements of these structures is simply not available, 
and so in the end the results may be dif.cult to validate. An advantage of the transport theory approach 
is that the parameters of the model often may be directly extracted from experimental data. A legitimate 
criticism of our work is that we did not directly compare the predictions of our model with experiment. 
The pre­dictions of our model and the in.uence of measured material pa­rameters should be checked carefully. 
However, we believe that this model has many applications in computer graphics even if it does not perfectly 
predict measured re.ection functions. The metaphor of layered surfaces is very easy for users to understand 
because is a natural way to describe phenomenologically the ap­pearance of many materials. It also .ts 
easily into most rendering systems and can be implemented ef.ciently. Finally, transport theory is a 
heuristic theory based on abstract­ing microscopic parameters into statistical averages. Transport theory 
is also the basis of the rendering equation, which is widely viewed as the correct theoretical framework 
for global illumina­tion calculations. In this paper we propose to model surface re­.ection from layered 
surfaces with transport theory. Thus, when our re.ectance model for layered surfaces is incorporated 
into a ray tracer, there is a hierarchy of transport calculations being per­formed. Within this hierarchy, 
the lower level transport equation computes the re.ectance for the higher level transport equation. When 
performing this calculation, the lower level transport equa­tion uses as its initial conditions the values 
from the higher level transport solution. Thus the two levels are coupled in a very sim­ple way. In fact, 
it is possible to reformulate transport theory entirely in terms of re.ection functions, the result is 
an integral equation for the re.ection function itself; in this formulation the radiance does not appear 
at all. Coupling transport equations at different levels of detail in this manner is a promising approach 
to tackling the problem of constructing representations with many different levels of detail as proposed 
by Kajiya[17]. Acknowledgements We would like to thank Craig Kolb for his help with RayShade and the 
leaf pictures. We would also like to thank David Laur for his help with the color plates. This research 
was partially supported by Apple, Silicon Graphics Computer Systems, David Sarnoff Research Center, and 
the National Science Foundation (CCR 9207966). References [1] Beckmann, P., and Spizzichino, A. The scattering 
of electromagnetic waves from rough surfaces. Pergamon, Oxford, 1963. [2] Blinn, J. F. Light Re.ection 
Functions for Simulation of Clouds and Dusty Surfaces. Computer Graphics 16, 3 (July 1982), 21 29. [3] 
Bloomenthal, J. Modeling the Mighty Maple. Computer Graphics 19, 3 (July 1985), 305 311. [4] Bouguer, 
P. The Gradation of Light. University of Toronto Press, 1960. [5] Cabral, B., Max, N., and Springmeyer, 
R. Bidi­rectional re.ection functions from surface bump maps. Com­puter Graphics 21, 4 (July 1990), 273 
281. [6] Carter, L., and Cashwell, E. Particle Transport Sim­ulation with the Monte Carlo Method. Energy 
Research and Development Administration, 1975. [7] Chandrasekhar, S. Radiative Transfer. Dover, New York, 
1960. [8] Cook, R. L., and Torrance, K. E. A Re.ection Model for Computer Graphics. ACM Transactions 
on Graph­ics 1, 1 (1982), 7 24. [9] Fante, R. Relationship between Radiative Transport The­ory and Maxwell 
s Equations in Dielectric Media. J. Opt. Soc. Am. 71, 4 (April 1981), 460 468. [10] Grawboski, L. Astrophysics 
J. 39 (1914), 299. [11] Hanrahan, P. From Radiometry to the Rendering Equa­tion. SIGGRAPH Course Notes: 
An Introduction to Radiosity (1992). [12] He, X. D., Torrance, K. E., Sillion, F. X., and Greenberg, 
D. P. A Comprehensive Physical Model for Light Re.ection. Computer Graphics 25, 4 (July 1991), 175 186. 
[13] Henyey, L. G., and Greenstein, J. L. Diffuse radia­tion in the galaxy. Astrophysics J. 93 (1941), 
70 83. [14] Ishimura, A. Wave Propagation and Scattering in Random Media. Academic Press, New York, 1978. 
[15] Jerlov, N. G. Optical Oceanography. Elsevier, Amster­dam, 1968. [16] Kajiya, J. Radiometry and Photometry 
for Computer Graphics. SIGGRAPH Course Notes: State of the Art in Im­age Synthesis (1990). [17] Kajiya, 
J. Anisotropic Re.ection Models. Computer Graphics 19, 3 (July 1985), 15 22. [18] Kortum, G. Re.ectance 
Spectroscopy. Springer-Verlag, Berlin, 1969. [19] Krueger, W. The Application of Transport Theory to 
the Visualization of 3-D Scalar Fields. Computers in Physics 5 (April 1991), 397 406. [20] Ma, Q., Ishimura, 
A., Phu, P., and Kuga, Y. Trans­mission, Re.ection and Depolarization of an Optical Wave For a Single 
Leaf. IEEE Transactions on Geoscience and Remote Sensing 28, 5 (September 1990), 865 872. [21] Marchuk, 
G., Mikhailov, G., Nazaraliev, M., Darbinjan, R., Kargin, B., and Elepov, B. The Monte Carlo Methods 
in Atmospheric Optics. Springer Ver­lag, Berlin, 1980. [22] Nakamae, E., Kaneda, K., Okamoto, T., and 
Nishita, T. A Lighting Model Aiming at Drive Simu­lators. Computer Graphics 24, 4 (August 1990), 395 
404. [23] Nicodemus, F. E., Richmond, J. C., and Hsia, J. J. Geometrical Considerations and Re.ectance. 
National Bu­reau of Standards, October 1977. [24] Poulin, P., and Fournier, A. A Model for Anisotropic 
Re.ection. Computer Graphics 24, 4 (August 1990), 273 282. [25] Seeliger, R. Munch. Akad. II. Kl. Sitzungsber 
18 (1888), 201. [26] Torrance, K. E., and Sparrow, E. M. Theory of Off-Specular Re.ection From Roughened 
Surfaces. Journal of the Optical Society of America 57 (September 1967), 1104 1114. [27] van Gemert, 
M. F. C., Jacques, S. L., Steren­berg, H. J. C. M., and Star, W. M. Skin Optics. IEEE Transactions on 
Biomedical Engineering 36, 12 (De­cember, 1989), 1146 1154. [28] Westin, S. H., Arvo, J. R., and Torrance, 
K. E. Predicting Re.ectance Functions from Complex Surfaces. Computer Graphics 26, 2 (July 1992), 255 
264. [29] Woolley, J. T. Re.ectance and Transmittance of Light by Leaves. Plant Physiology 47 (1971), 
656 662.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166140</article_id>
		<sort_key>175</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[Display of the earth taking into account atmospheric scattering]]></title>
		<page_from>175</page_from>
		<page_to>182</page_to>
		<doi_number>10.1145/166117.166140</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166140</url>
		<keywords>
			<kw><![CDATA[atmospheric scattering]]></kw>
			<kw><![CDATA[color of water]]></kw>
			<kw><![CDATA[earth]]></kw>
			<kw><![CDATA[optical length]]></kw>
			<kw><![CDATA[photo-realism]]></kw>
			<kw><![CDATA[radiative transfer]]></kw>
			<kw><![CDATA[sky light]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.2</cat_node>
				<descriptor>Earth and atmospheric sciences</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010437</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Earth and atmospheric sciences</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP36042211</person_id>
				<author_profile_id><![CDATA[81100539710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tomoyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P277694</person_id>
				<author_profile_id><![CDATA[81332528595]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sirai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P158160</person_id>
				<author_profile_id><![CDATA[81100531919]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Katsumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tadamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P75905</person_id>
				<author_profile_id><![CDATA[81100145250]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Eihachiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakamae]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>801255</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[J.F. Blinn, "Light Reflection Functions for Simulation of Clouds and Dusty Surfaces," Computer Graphics, Vol. 16, No. 3 (1982), pp. 21-29.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15894</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[A. Fournier, "A Simple Model of Ocean Waves," Computer Graphics, Vol. 20, No. 4 (1986), pp. 75-84.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[H.R. Gordon, "Simple Calculation of the Diffuse Reflectance of the Ocean," Applied Optics, Vol. 12, No. 12 (1973), pp. 2803-2804.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808594</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[J.T. Kajiya, "Raytracing Volume Densities," Computer Graphics, Vol. 18, No. 3 (1984), pp.165-174.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>35071</ref_obj_id>
				<ref_obj_pid>35068</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[R V Klassen, "Modeling the Effect of the Atmosphere on Light," ACM Transaction on Graphics~ V ol. 6, No. 3 (1987), pp. 215-237.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[LINKS Corporation, leaflet of"LINKS CG LIBRARY", (1991).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>30658</ref_obj_id>
				<ref_obj_pid>30657</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[G.A. Mastin, P .A. W atterberg, and J.F. Mareda, "Fourier Synthesis of Ocean Scenes," IEEE Computer Graphics &amp; Applications, Vol. 7, No. 3 (1987), pp. 16-23.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5515</ref_obj_id>
				<ref_obj_pid>5513</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[N. Max, "Light Diffusion through Clouds and Haze," Graphics and Image Processing, Vol. 33, No. 3 (1986), pp.280-292.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[McCluney, W.R. "Ocean Color Spectrum Calculations." Applied Optics, Vol. 13, No. 10 (1974), pp. 2422-2429.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[N.G. Jerlov, "Optical Oceanography," Elsevier, Amsterdom (1968).]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[T. Nishita, K. Kaneda, E. Nakamae, "A Scanline Algorithm for Displaying Trimmed Surfaces by Using B6zier Clipping," The Visual Computer, Vol.7, No.5 (1991) pp.269-279.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37437</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[T. Nishita, Y. Miyaw aki, E. Nak amae, "A Shading Model for Atmospheric Scattering Considering Distribution of Light Sources," Computer Graphics, Vol. 21, No. 4 (1987), pp. 303-310.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15900</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[T. Nishita, E. Nakamae, "Continuous tone Representation of Three-Dimensional Objects Illuminated by Sky Light," Computer Graphics, V ol. 20, No. 4 (1986), pp. 125-132.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[T. Nishita, E. Nakamae, "Half-Tone Representation of 3D Objects with Smooth Edge by Using a Multi-Scanning Method," J. Information Processing (in Japanese), Vol. 25, No.5 (1984), pp.703-711.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[K. Sato, "Fractal Graphics," Rassel Co.(in Japanese) (1989) p.74]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[S. Sekine, "Optical characteristics of turbid atmosphere,"J Illum Eng Int Jpn, Vol. 71, No. 6, (1987) pp. 333.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>35070</ref_obj_id>
				<ref_obj_pid>35068</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[P.Y. Ts'o, B. A Barsky, "Modeling and Rendering Waves: Wave-Tracing Using Beta-Splines and Reflective and Refractive Texture Mapping," ACM Tr ansactions on Graphics, Vol. 6, No. 3 (1987), pp. 191-214.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[W.M. Cornette, J.G. Shanks, "Physical reasonable analytic expression for the single-scattering phase function," Applied Optics, Vol. 31, No.16 (1992), pp.3152- 3160.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Display of The Earth Taking into Account Atmospheric Scattering Tomoyuki Nishita Takao Sirai Fukuyama 
University Higashimura-cho, Fukuyama, 729-02 Japan  Abstract A method to display the earth as viewed 
from outer space (or a spaceship) is proposed. The intention of the paper is application to space flight 
simulators (e.g., reentry to the atmosphere) and the simulation of surveys of the earth (comparisons 
with observations from weather satellites and weather simulations); it is not for geometric modeling 
of terrains and/or clouds viewed from the ground, but for displaying the earth including the surface 
of the sea viewed from outer space taking into account particles (air molecules and aerosols) in the 
atmosphere and water molecules in the sea. The major points of the algorithm proposed here are the efficient 
calculation of optical length and sky light, with lookup tables taking advantage of the facts that the 
earth is spherical, and that sunlight is parallel. CR Categories and Subject Descriptors: I.3.3 [Computer 
Graphics]: Picture/Image Generation I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism 
Key Words: Earth, Atmospheric Scattering, Optical Length, Sky light, Color of Water, Photo-realism, Radiative 
Transfer  1 INTRODUCTION Research on image synthesis of realistic 3-D models is one of the most popular 
fields these days. Displays of natural scenes such as mountains, trees, sea, clouds have been attractively 
rendered, and an image synthesis of the earth has also been developed. Images of the earth are widely 
used in movies or TV commercials, e.g., the CG library of earth images [6] was recently released for 
use in this field. These images, however, are focused on how to create attractive images without any 
requirement of physical based accuracy. However, physically-based images are required for the study of 
the simulation of surveys of the earth, such as observation from weather satellites in comparison to 
weather simulation, and flight simulators in space. The color of the earth when viewed from space varies 
according to the relationship between the view direction and the position of the sun. In the famous words 
of the astronaut, the earth was blue . When we observe the earth from relatively close to the atmosphere, 
the atmosphere surrounding the earth appears as blue, and the atmosphere near the boundary of the shadow 
due to the sun appears red (i.e., sunset). The color of clouds Permission to copy without fee all or 
part of this material is granted provided that the copies are not made or distributed for direct provided 
that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice 
and the title of the commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. permission of the 
Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific 
permission. otherwise, or to republish, requires a fee and/or specific permission. &#38;#169;1993 8/93/008/0015 
$1.50 &#38;#169;1993 ACM - 0 - 89791 - 601 - 8/93/008 $1.50 Katsumi Tadamura Eihachiro Nakamae Hiroshima 
Prefectural University Nanatsuka-cho, Shoubara City , 727 Japan also varies according to the sun s position. 
These phenomena are optical effects caused by particles in the atmosphere, and cannot be ignored. The 
color of the surface of the sea is not uniform, such as navy blue; it has various colors which depend 
on incident light to the sea and absorption/scattering effects due to water molecules. This paper proposes 
an algorithm of physically-based image synthesis of the earth viewed from space. The method proposed 
here has the following advantages: (1) Calculation of the spectrum of the earth viewed through the atmosphere; 
the earth is illuminated by direct sunlight and sky light affected by atmospheric scattering. (2) Calculation 
of the spectrum of the atmosphere taking account of absorption/scattering due to particles in the atmosphere. 
 (3) Calculation of the spectrum on the surface of the sea taking into account radiative transfer of 
water molecules. The major parts in 1) and 2) are concerned with the calculation  of optical length 
and sky light. For these calculations, numerical integrations taking into account atmospheric scattering 
are required, but they are effectively solved by using several (various) lookup tables making good use 
of the facts that the shape of the earth is a sphere and that sunlight is a parallel light. For 3), we 
show that an analytical solution is available instead of numerical integrations. In the following sections, 
the basic idea of the lighting model for rendering the color of the earth taking into account atmospheric 
scattering, rendering the color of clouds, and spectrum calculation of the sea is described. Finally, 
several examples are demonstrated in order to show the effectiveness of the method proposed here. 2 
BASIC IDEAS In order to render the earth, the following elements should be taken into account: a geometric 
model of the earth, the atmosphere (air molecules, aerosols), sea, clouds, and the spectrum of the sunlight. 
This paper discusses rendering an algorithm of the earth, the atmosphere, sea, and clouds viewed from 
outer space or various positions within the atmosphere; the following optical characteristics should 
be considered: (1) The color of the atmosphere: the atmosphere contains air molecules and aerosols, and 
scattered sunlight from those particles reaches the viewpoint; the intensity of the light reaching the 
viewpoint is obtained by integrating scattered light from every particle on the ray, and the light scattered 
from the atmosphere around the earth also reaches this viewpoint. (2) The color of the earth s surface: 
the earth is illuminated by both direct sunlight and sky light. Sunlight is absorbed when light passes 
through the atmosphere, and sky light consists of light scattered by particles in the air. On the way, 
passing through the atmosphere the light is attenuated, and its spectrum changes.  (3) The color of 
the sea: sunlight reaching the surface of the sea is divided into reflected light at the surface and 
light scattered from water molecules. Both of them pass through the atmosphere and reach the viewpoint. 
 (4) The color of clouds: sunlight is scattered from particles of clouds, the scattered light is attenuated 
and reaches the viewpoint. These phenomena should be simulated as precisely as possible  in the calculation 
of the spectrum of the earth and the atmosphere. As we intend to concentrate on close views of the earth, 
the bumped terrain model of the earth is used instead of a simple sphere; the continents are modeled 
by 3D fractals, and the sea is expressed by a sphere consisting of some curved surfaces. Geometric models 
such as a spaceship are also dealt with. For hidden surface removal, the scanline algorithm for free 
form surfaces developed by the authors is employed [11]; the surfaces are expressed by Bézier surfaces. 
 3 MODELING OF THE EARTH Even though we may use a modeling in which the earth is treated as a sphere 
and the land is modeled by bump mapping, we consider the earth as having two components, land and sea: 
the sea consists of eight cubic Bézier patches, and the land consists of a set of curved surfaces. The 
land data is made by mapping small patches onto the sphere, which are subdivided by using fractals after 
giving the altitude data for each mesh point overlapped onto a world map: the random midpoint displacement 
algorithm is employed as a fractal. A scanned image of the map is used as the texture of the land. Therefore 
the color is not the real color of the earth. 4 SPECTRUM OF THE ATMOSPHERE Previous work taking account 
scattering/absorption due to particles include; a) the display of Saturn s rings (reflective ice particles) 
[1], b) for light scattering from particles in the air, shafts of light caused by spot lights [12], and 
light beams passing through gaps in the clouds or through trees [8], c) scattered light due to nonuniform 
density particles such as clouds and smoke [12][4], d) sky color taking account atmospheric scattering 
[5]. In this paper we focus our discussion on the atmosphere. On this topic, Klassen [5] approximated 
the atmosphere as multiple layers of plane-parallel atmosphere with uniform density; however, this method 
results in a large error near the horizon. We discuss here a spherical-shell atmosphere with continuous 
variation of density in order to improve accuracy. Though his method can only render the color of the 
sky viewed from a point on the earth, the method discussed here can render the color of the atmosphere 
viewed from space. The color of the atmosphere is much influenced by the spectrum of the sunlight, scattering/absorption 
effects due to particles in the air, reflected light from the earth s surface, and the relationship between 
the sun s position and the viewpoint (and direction). The sunlight entering the atmosphere is scattered/ 
absorbed by air molecules and aerosol, and ozone layers. The characteristics of scattering depend on 
the size of particles in the atmosphere. Scattering by small particles such as air molecules is called 
Rayleigh scattering, and scattering by aerosols such as dust is called Mie scattering. Light is attenuated 
by both scattering and absorption. Figure 1: Intensity calculation for the ray intersecting only with 
the atmosphere. 4.1 Assumptions for Spectrum Calculation For the spectrum calculation, we use the following 
assumptions: (1) The multiple scattering of light between air molecules and aerosols in the atmosphere 
is ignored because of its negligible values and large computational cost, so only single scattering is 
considered. The interreflection of light between the earth s surface and particles in the air is also 
neglected because of the same reasons. (2) For visible wavelengths, absorption in the ozone layer is 
negligible compared to absorption by air molecules and aerosols. (3) The density distributions of air 
molecules and aerosols are taken into account; their densities vary exponentially with altitude [16]. 
 (4) It is assumed that light travels in a straight line even though the actual path is curved due to 
the variation of index of refraction with altitudes.  4.2 Atmospheric Scattering Let s consider scattering 
due to air molecules and aerosols. First, single scattering due to air molecules is described. The light 
reflected due to Rayleigh scattering, I, is generally given by the following equation; I( ., .) = I0(. 
)K. Fr ( .)/ .4 2p 2(n2 -1)2 (1) K = 3Ns where I0 is the intensity of incident light, K is a constant 
for the standard atmosphere (molecular density at sea level), . the scattering angle (see Fig. 1), Fr 
the scattering phase function indicating the directional characteristic of scattering (given by 3/4(1+ 
cos2 .) ), . the wavelength of incident light, n the index of refraction of the air, Ns the molecular 
number density of the standard atmosphere, and . the density ratio. . depends on the altitude h (. = 
1 at sea level) and is given by - h .= exp(), (2) H0 where H0 is a scale height ( H0 = 7994m), which 
corresponds to the thickness of the atmosphere if the density were uniform. Eq. (1) indicates that the 
intensity of scattering is inversely proportional to the 4th power of the wavelength. Short wavelength 
light is very strongly attenuated by traversing the atmosphere, but long wavelength light is scarcely 
affected. This is why the sky appears blue in the daytime. Conversely, at sunset or sunrise, the distance 
traversed by the light increases, and the color of sky changes to red because of increased scattering 
of short wavelengths. The attenuation coefficient ß (i.e., the extinction ratio per unit length) is given 
by 8p3(n 3 -1)2 4pK ß= = (3) .4 . 4 3Ns As shown in Fig.1, the light reaching viewpoint Pv can be obtained 
as the remainder after scattering and absorption due to air molecules along the path between Pb and Pv 
. The light at P has been attenuated due to travel in the atmosphere ( PcP ), and the light scattering 
from P is also attenuated before reaching Pv . To calculate the attenuation caused by particles for light 
of wavelength . traversing distance s, we use the optical depth, which is obtained by integrating ß of 
Eq. (3) along the path s. Let s denote the integration variable s and the distance S, then the optical 
depth is given by SS . 4 pK . t( S, .) =ß (s).( s)ds =.(s)ds (4) 0 .40 Next, single scattering due to 
aerosols is described. Scattering optics and the density distribution for aerosols differ from air molecules; 
Eq. (4) is different, too. Because the size range of particles of aerosols is very great, Mie scattering 
is applied for the phase function in Eq. (1) which exhibits a strong forward directivity. The Henyey-Greenstein 
function is well known as a phase function. Recently, Cornette [18] improved it, which gives a more reasonable 
physical expression: 3(1- g 2) (1 + cos2 . ) F(. ,g) = (5) 2(2 + g2) (1+ g2 - 2gcos.)3/2 , where g is 
an asymmetry factor and given by 54 - 25 2)x -1/3 + x1/3 g = u -( u , 9 381 5 125 64 325 1250 4)1/2 x 
= u + u3 + (- u 2 + u , 9 729 27 243 2187 where if g = 0 then this function is equivalent to Rayleigh 
scattering. u is determined by the atmospheric condition (e.g., haze) and wavelength; u varies from 0.7 
to 0.85 (see [18]). Like the density distribution of air molecules, the density of aerosols decreases 
exponentially with altitude, but the rate of decrease is different from that of air molecules. The density 
can be obtained by setting the scale height, H0 , of Eq. (2) to 1.2 km [16].  4.3 Intensity Calculation 
due to Atmospheric Scattering Let s discuss a ray from viewpoint Pv to the earth, the light reaching 
the viewpoint has the following three passes: a) the ray passing through only the atmosphere, b) the 
ray intersecting with the earth, c) the ray passing through only space. For c) intensity calculation 
is not required. The calculation methods for a) and b) are described in the following. 4.3.1 Spectrum 
calculation for only the atmosphere Let s discuss light scattering due to air molecules on the ray passing 
just through the atmosphere. The discussion for aerosols is omitted because the optics is similar except 
for 1/ .4 dependence. As shown in Fig. 1, the light reaching Pv can be obtained as the remainder after 
scattering and absorption due to air molecules along the intersection line between the ray and the atmosphere, 
PbPa . The intensity of the light scattered at point P (at distance s from Pv ) in the direction of Pv 
, Ip , is obtained by Eq. (1). The light scattered at P is attenuated before arriving at Pv . The intensity 
of Figure 2: Intensity calculation for the ray intersecting with the earth.  the light arriving at P, 
Ip , can be obtained by setting the integration interval to PcP in Eq. (4) of optical depth, that is 
1 Ip (. )= Is ( .) KFr (.)..4 exp(-t(PPc ,. )), (6) where Is is the solar radiation at the top of the 
atmosphere, and t( PPc, . ) the optical depth from the top of the atmosphere to point P (l is the integration 
variable) and given by t( PPc, . ) =ß (l ).(l) dl. . PPc As the light scattering from P is also attenuated 
before reaching Pv , the intensity of the light reaching Pv , Ipv , can be obtained by multiplying the 
attenuation by the intensity at P, that is Ipv (. ) = Ip (. )exp(-t( PPa ,. )). (7) As the distance to 
the sun can be considered almost infinite, the sunlight can be assumed to be a parallel beam. Thus the 
scattering angle at every point along PaPb can be considered constant. That is, Iv reaching Pv can be 
obtained by integrating scattered light due to air molecules on PaPb : Iv (. ) = Ipv (. )ds .PPb a (8) 
KFr ( .) Pb = Is ( . ) .exp(- t(PP,. ) - t( PPa , .))ds c . 4 . P a  4.3.2 Spectrum calculation of the 
earth Let s consider the ray intersecting with the earth as shown in Fig. 2. The intensity scattered 
due to particles on the path, PaPb , can be obtained in the same manner as the description in 4.3.1. 
When point P coincides with point Pb (i.e., on the earth surface), the light reaching the viewpoint is 
obtained by adding reflected light from the earth to the light scattered due to molecules on PaPb . The 
intensity of light reaching viewpoint Pv , Iv ' , is expressed by Iv ' (. )= Iv (. ) + Ie( .)exp(-t (PaPb 
, . ), (9) where Iv is the scattered light of Eq. (8). Ie is reflected light at the earth; the direct 
component of sunlight and ambient light. The ambient light is mainly sky light. By considering attenuation 
of sunlight reaching the earth surface, Ie is given by Ie (. ) = r (. )(cosa Is (. )exp(- t(PcPb ,. 
)) + Isky ( ., a)), (10) where r( .) is the diffuse reflection of the earth, a the angle between the 
normal vector of the earth and light vector (sunlight), and Isky sky light. The direct component is small 
at the region where a is large (i.e., nearby the boundary of shadow) and tends to be reddish because 
of its long optical length. Sky light is scattered light due to particles in the atmosphere. The radiance 
distribution of sky light can be obtained by setting the viewpoint on the earth in Eq. (8). As we are 
discussing the earth as viewed from space, shadows caused by obstacles on the surface are ignored, even 
though we take into account shadows due to the earth itself. That is, for shadow calculation, the earth 
is assumed to be a sphere with a smooth surface. Sky light due to scattered light from clouds is also 
ignored here. The illuminance at pointQ on the earth due to the whole sky is obtained by using the following 
method: let s consider an element on a hemisphere whose center is Q (see Fig.3), calculate the intensity 
at each element on the hemisphere, and project each element onto the base of the hemisphere, then the 
illuminance is obtained by integrating the intensity of each element by weighting its projected area 
[13]. Isky is calculated as follows: as shown in Fig. 3(a), the base of the hemisphere is divided into 
a mesh. Let s consider point Pij on the hemisphere, which is mapped onto the hemisphere of the mesh point 
p ij inversely, and calculate the intensity in the direction of QPij . The illuminance due to the whole 
sky is obtained by adding intensities at every mesh point within the base circle of the hemisphere. As 
shown in Fig. 3(a), the x-axis is set so that the sun exists on the x z plane; the region in the half 
circle (e.g., y > 0) is enough to get Isky because of symmetry. The radiance distribution of the sky 
is determined by anglea between the normal of the surface of the earth and the direction of the sunlight. 
Even though the direction of the sunlight is different at each point on the earth, the illuminance due 
to sky light (integrated values) at any point with the same angle a has the same value (e.g., Q and Q 
in Fig. 3). This means that the illuminance due to sky light at arbitrary angle a can be obtained by 
linear interpolation of a precalculated lookup table of Isky . Note that Isky is not zero at regions 
where there is no direct sunlight (a > 90 degrees, e.g., Ps in Fig. 3), so that Isky for a = 0 to a = 
110 degrees must be prepared in the lookup table. 4.3.3 Detection of shadow caused by the earth As shown 
in Fig. 3(b), point P on the ray exists in the shadow region caused by the earth (we refer to it as a 
shadow volume), the scattered light in this region is zero because there is no incident light. Therefore 
it is sufficient to consider only attenuation in this region. As the shadow volume is expressed by a 
cylinder, which is obtained by sweeping the circle (i.e., the contour of the earth viewed from the sun), 
the shadow segment on the ray can be calculated as the intersection segment between the cylinder and 
the ray. 4.3.4 Calculation of optical depth The optical length of air molecules is calculated by numerical 
integration of Eq. (4) (in the case of aerosols, the density distribution and the extinction coefficient 
are different). The optical length is calculated by trapezoidal integration of sampled density. The optical 
length at sampling point Pi on the ray is obtained by adding the optical length of interval Pi -1Pi to 
the optical length at Pi -1. Therefore the integration of the optical depth should start from the viewpoint. 
The optical length between the light source and point Pi on the ray is also required (e.g., PPc in Fig. 
1). This calculation is required at every sampling point on the ray; optimization should be considered 
because of computational expense. We use a lookup table to save on computation time. The density distribution 
of particles in the atmosphere varies exponentially with altitude. This means that the errors in the 
numerical integration become large when it is performed with a constant interval. Intervals which are 
inversely proportional to the density are desired; that is small intervals for low altitude and long 
intervals for high altitude. In order to realize this condition, the atmosphere is assumed as multiple 
spherical-shells. The radius of each sphere is set so that the difference in density between every adjacent 
sphere is within a given value. As a result, the difference between the radii of the shell is small for 
low altitude, and is large for high altitude, as shown in Fig. 4. As Rayleigh scattering governs the 
calculation of optical length, the radius of each sphere is determined by the density distribution of 
air molecules. Let s consider N layers of spheres. The radius is given by (see Fig. 4) ri = H0log(.i 
) + R, .i =1 -i / N, (11) where R is the radius of the earth. For i=N , rN is set to the radius of the 
atmosphere. For aerosols, the scale height is smaller than that for air molecules; aerosols mainly exist 
at low altitude. Therefore aerosols exist in the dense radii of shells; this fact assures the correctness 
of the above mentioned algorithm. The sampling points used in the integration are employed as the intersection 
points between the ray (view sight or light ray) and the multi-imaginary spheres and these intersection 
are easily obtained. The density at every sampling point is easily found from the lookup table indexed 
by the index numbers of the sphere, which is easily get from the altitude of the point. The optical length 
between the sun and an arbitrary point on the ray can easily be precalculated because the earth is a 
sphere and sunlight is parallel light. As shown in Fig. 4, let s consider a cylinder defined by sweeping 
the circle which passes through the center of the earth and is perpendicular to the light direction. 
Every optical length at the intersection (i.e. circle) between the cylinder and each one of the multi-imaginary 
spheres is equal (e.g., P and P in figure). The optical lengths at the intersection points between the 
cylinders with radius Cj and the spheres with radius ri is calculated (e.g., PaP in fig.) and are stored 
in the lookup table. The optical depth at arbitrary point P on the ray is easily calculated by linear 
interpolation, after the radius of the cylinder includingP and the radius of the sphere are calculated. 
The lookup table here is 2D array: [ri ,Cj ]. After getting indeces i and j from point P, the optical 
depth can be obtained by linear interpolation from [ri ,Cj ], [ri +1,Cj ][ri +1,Cj +1], [ri ,Cj +1] As 
described above, the light intensity of one wavelength reaching the viewpoint can be calculated by numerical 
integration with respect to pass length. Therefore the light intensity in the range of visible wavelengths 
(r, g, b in this paper) can be calculated.   5 THE COLOR OF CLOUDS Since the geometric modeling of 
clouds is not our main subject, we are displaying the earth as viewed from space, clouds are simply modeled 
by applying 2D fractals. That is, the density distribution of clouds is expressed by mapping the fractal 
images of the necessary Mandelbrot set (0.39032+ 0.23775i is used in this paper) [15]. To take into account 
clouds with various altitudes, multiple imaginary spheres are employed to map fractal images on them. 
Their color is determined by the following two light paths. One is on the light which passes through 
the atmosphere of scattered light due to cloud particles, again passing through the atmosphere, and reaches 
the viewpoint. Another one is on the light which passes through the atmosphere, reflected light at the 
earth s surface is attenuated by Figure 5: Calculation of color of water surface. cloud particles, 
again passing through the atmosphere. Multiple scattering in clouds is ignored here. The size of particles 
in clouds is larger than that of air molecules or of aerosols. Light scattered by such large particles 
is little influenced by wavelength. (However, the spectrum of incident sunlight onto clouds depends fairly 
strongly on the sun position.) The light reflected from clouds depends on the phase function (the angle 
between the view vector and light vector); the phase function is expressed by Eq. (5) (see reference 
[18] on the value u). In the case of clouds not being illuminated by the sunlight because of the shadow 
due to the earth; the shadow detection is executed by using the shadow volume described before. The shadows 
on the earth due to clouds are ignored in this paper. In the near future, a more precise model for clouds 
is slated in order to get images of the earth viewed from relatively close to the earth s surface. 6 
COLOR OF THE SEA Let s consider the light reaching a viewpoint from the surface of the sea. There are 
three paths (see Fig. 5): (1) reflected light on the water surface, (2) scattered light due to particles 
within the water leaving the water surface, (3) attenuated light passing through the sea after reaching 
the bottom of water. Calculation methods of the color of water have been developed by Max [8], Fournier 
[2], Ts o [17], and Mastin [7]. However their methods focused on (1) and shapes of waves, and did not 
refer to (2) (scattered light due to particles in the water). The method proposed here takes into account 
(1) and (2). Furthermore the attenuation of the light passing through the atmosphere is taken into account. 
For (3), the light from the bottom of the sea can be neglected because of the depth of the sea. When 
the light is incident to the water surface, the light path is divided into reflection and refraction. 
The relation between the reflection and refraction on the water surface obeys Fresnel s law of reflection. 
Incident light is refracted at the water surface; the relation between the incident angle and reflection 
angle obeys Snell s law. The refracted light is scattered/absorbed by water molecules in the sea, and 
reaches the viewpoint after refracting at the water surface again. For this phenomena, Gordon and McCluney 
[3, 9] proposed a quasi-single­scattering (QSS) model based on the radiative transfer equation. However, 
in the model the sun s position is limited to the zenith. We improved upon this. The light intensity 
transmitted in water, IPQ , is given by Ii (. )Ti (. ii ,.io )To( .ji ,. jo )ß( d, . ) IPQ (.ii ,. io, 
z ) = n2 (cos.io + cos. ji )c(. )[1-.0(. )F(. )] (12) ×(1- exp(- zc(. )[1 -. 0(. )F(. )](secji .+secio 
)), where . is wave length, z the depth of the sea, .ii the angle between the surface normal at point 
P and the direction of the viewing direction, .io the angle between the direction of the zenith and the 
direction of incident sunlight, . jo the angle between the reverse direction of the zenith and the sunlight 
after refraction, Ii ( .) the irradiance of sunlight just above the water surface, n the refractive index 
of water, Ti and To the transmittance of the incident light at point S andP, respectively, c (. ) the 
attenuation coefficient of light which expresses the ratio of lost energy of light when the light travels 
a unit length, ß a volume scattering function .0 the albedo of water, and F the fraction of the scattering 
coefficient in a forward direction. Data of ß, .0 , and F used in this paper is obtained from [10]. Eq. 
(12) shows that the color of water depends on the depth, the incident angles and viewing direction. The 
surface of the sea is not flat, and is a spherical surface (i.e., the normal vector of each point on 
the surface is different); the color of the sea varies according to the position because the incident 
and viewing angles to the surface normal at each position are different. As described above, both the 
incident light to the sea and the color (intensity) of the sea are attenuated by the atmosphere. By using 
the same method as described in 4.3.2, this effect can be calculated by taking into account two optical 
lengths; from the sun to the surface and from the surface to the viewpoint. 7 EXAMPLES Fig. 6 shows 
an example of the color of the atmosphere. The color of the earth is assumed to be black in order to 
demonstrate the atmospheric color only. The position of the sun is behind and to the left of the observation 
point. Even though the earth is assumed to be a black body, it looks blue, and the boundary of the earth 
is white. Fig. 7 shows the images of the earth with texture-mapped continents viewed from space; the 
location of the observation is at altitude 36,000 km, which corresponds to the altitude of the Japanese 
weather satellite called Himawari, at 135° E 0° N and the direction of the sun is 70° E 20° N. In Fig. 
7(a), the color of the sea, direct sunlight, and sky light are taken into account, but the attenuation 
from the earth to the viewpoint is ignored (i.e., it corresponds to the color when the observer stands 
on the earth). In Fig. 7(b), atmospheric scattering/absorption is also taken into account (i.e., the 
color of the atmosphere is added). In Fig. 7(c), clouds are added. Figs. 8, 9 show examples of the earth 
viewed from relatively close-by; the viewpoint is at altitude 500 km at 0° E 60° N. The direction of 
the sun in Fig. 8 is 0° E 20° N, and the directions of the sun in Fig. 9 are 200° E 20° N and 240° E 
15° N. Fig. 8 corresponds to noon (daytime), and Fig. 9 correspond to evening or dawn sky. In Fig. 9(b), 
one can observe the shadow (the dark part in the red atmosphere) due to the earth. The color of clouds 
changes to red due to the change of color of direct sunlight. These examples depict beautiful variations 
in color of the earth and the atmosphere. The space shuttle in the figure consists of 178 Bézier patches. 
Let s show the photographs taken by the first Japanese astronaut aboard space shuttle, Dr. M. Mouri (NASDA), 
in Fig. 10 (altitude 300 km, September, 1992). Fig. 11 displays the results of our simulation. One may 
observe differences between the photos and the simulation results. One of the reasons on Fig. 11(a) may 
be due to the poor modeling of clouds and lands. In Fig. 11(b) some horizontal layers (e.g., orange color) 
are observed, one of them may be aerosols due to explosion of Volcano in Philippine. These facts suggest 
the necessity for further researching. For hidden surface removal, the scanline algorithm for curved 
surfaces [11] is employed, and for anti-aliasing the multi-scanning algorithm [14] is employed. The calculation 
was done on an IRIS Indigo Elan. The computation times for Fig. 7(c) and Fig. 9 were 3.8 minutes and 
12.0 minutes, respectively (image size=500 × 490). 8 CONCLUSION We have proposed an algorithm for physically-based 
image synthesis of the earth viewed from space. As shown in the examples, the proposed method gives us 
photo-realistic images taking into account the color of the earth, clouds, and the sea. The advantages 
of the proposed method are as follows: (1) The spectrum of the surface of the earth is calculated by 
taking into account direct sunlight and sky light as affected by atmospheric scattering. (2) The spectrum 
of the atmosphere is calculated by taking into account absorption/scattering due to particles in the 
atmosphere. (3) The spectrum on the surface of the sea is calculated by taking into account radiative 
transfer of water molecules. (4) The optical depth and illuminance due to sky light are efficiently 
calculated by using several lookup tables taking advantages of the facts that the earth is spherical 
and that sunlight is parallel.  Acknowledgment: The authors would like to acknowledge A. Wakayama (currently 
Fujitsu Co.) for his help in coding of the prototype of our program. References [1] J.F. Blinn, Light 
Reflection Functions for Simulation of Clouds and Dusty Surfaces, Computer Graphics, Vol. 16, No. 3 (1982), 
pp. 21-29. [2] A. Fournier, A Simple Model of Ocean Waves, Computer Graphics, Vol. 20, No. 4 (1986), 
pp. 75-84. [3] H.R. Gordon, Simple Calculation of the Diffuse Reflectance of the Ocean, Applied Optics, 
Vol. 12, No. 12 (1973), pp. 2803-2804. [4] J.T. Kajiya, Raytracing Volume Densities, Computer Graphics, 
Vol. 18, No. 3 (1984), pp.165-174. [5] R V Klassen, Modeling the Effect of the Atmosphere on Light, ACM 
Transaction on Graphics, V ol. 6, No. 3 (1987), pp. 215-237. [6] LINKS Corporation, leaflet of LINKS 
CG LIBRARY , (1991). [7] G.A. Mastin, P .A. W atterberg, and J.F. Mareda, Fourier Synthesis of Ocean 
Scenes, IEEE Computer Graphics &#38; Applications, Vol. 7, No. 3 (1987), pp. 16-23. [8] N. Max, Light 
Diffusion through Clouds and Haze, Graphics and Image Processing, Vol. 33, No. 3 (1986), pp.280-292. 
[9] McCluney, W.R. Ocean Color Spectrum Calculations. Applied Optics, Vol. 13, No. 10 (1974), pp. 2422-2429. 
[10] N. G. Jerlov, Optical Oceanography, Elsevier, Amsterdom (1968). [11] T. Nishita, K. Kaneda, E. Nakamae, 
A Scanline Algorithm for Displaying Trimmed Surfaces by Using Bézier Clipping, The Visual Computer, Vol.7, 
No.5 (1991) pp.269-279. [12] T. Nishita, Y. Miyaw aki, E. Nak amae, A Shading Model for Atmospheric Scattering 
Considering Distribution of Light Sources, Computer Graphics, Vol. 21, No. 4 (1987), pp. 303-310. [13] 
T. Nishita, E. Nakamae, Continuous tone Representation of Three-Dimensional Objects Illuminated by Sky 
Light, Computer Graphics, V ol. 20, No. 4 (1986), pp. 125-132. [14] T. Nishita, E. Nakamae, Half-Tone 
Representation of 3D Objects with Smooth Edge by Using a Multi-Scanning Method, J. Information Processing 
(in Japanese), Vol. 25, No.5 (1984), pp.703-711. [15] K. Sato, Fractal Graphics, Rassel Co.(in Japanese) 
(1989) p.74 [16] S. Sekine, Optical characteristics of turbid atmosphere, J Illum Eng Int Jpn, Vol. 71, 
No. 6, (1987) pp. 333. [17] P. Y. Ts o, B. A Barsky, Modeling and Rendering Waves: Wave-Tracing Using 
Beta-Splines and Reflective and Refractive Texture Mapping, ACM Tr ansactions on Graphics, Vol. 6, No. 
3 (1987), pp. 191-214. [18] W.M. Cornette, J.G. Shanks, Physical reasonable analytic expression for the 
single-scattering phase function, Applied Optics, Vol. 31, No.16 (1992), pp.3152- 3160.  Figure 8: The 
earth viewed from relatively close-by.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166141</article_id>
		<sort_key>183</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>24</seq_no>
		<title><![CDATA[Smooth transitions between bump rendering algorithms]]></title>
		<page_from>183</page_from>
		<page_to>190</page_to>
		<doi_number>10.1145/166117.166141</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166141</url>
		<keywords>
			<kw><![CDATA[BRDF]]></kw>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[bump map]]></kw>
			<kw><![CDATA[displacement map]]></kw>
			<kw><![CDATA[rendering]]></kw>
			<kw><![CDATA[surface detail]]></kw>
			<kw><![CDATA[volume texture]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP15035904</person_id>
				<author_profile_id><![CDATA[81100548247]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Barry]]></first_name>
				<middle_name><![CDATA[G.]]></middle_name>
				<last_name><![CDATA[Becker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15033556</person_id>
				<author_profile_id><![CDATA[81100480335]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Nelson]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Max]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[B. BECKER. Smooth transitions between bump rendering algorithms during animation. Master's thesis, University of California at Davis, December 1992.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563893</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J. F. BLINK. Models of light reflection for computer synthesized pictures. J. George, Ed., vol. 11,192-198.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37434</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[B. CABRAL, N. MAX, AND R. SPRINGMEYER. Bidirectional reflection functions from surface bump maps. In Computer Graphics (SIGGRAPH '87 Proceedings) (July 1987), M. C. Stone, Ed., vol. 21,273-281.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808602</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[R.L. COOK. Shade trees. In Computer Graphics (SIGGRAPH '84 Proceedings) (July 1984), H. Christiansen, Ed., vol. 18, 223-231.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37414</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[R. L. COOK, L. CARPENTER, AND E. CATMULL. The Reyes image rendering architecture. In Computer Graphics (SIC- GRAPH '87 Proceedings) (July 1987), M. C. Stone, Ed., 95- 102.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806819</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[R. L. COOK AND K. E. TORRANCE. A reflectance model for computer graphics, vol. 15,307-316.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[A. FOURNIER. Normal distribution functions and multiple surfaces. In Graphics Interface '92 Workshop on Local Illumination. 1992, pp. 45-52.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325167</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[J. T. KAJIYA. Anisotropic reflection models. In Computer Graphics (SIGGRAPH '85 Proceedings) (July 1985), B. A. Barsky, Ed., vol. 19, 15-21.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[N. L. MAX. Horizon mapping: shadows for bump-mapped surfaces. The Visual Computer 4, 2 (July 1988), 109-117.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>182467</ref_obj_id>
				<ref_obj_pid>182466</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[N. L. MAX AND B. BECKER. Bump shading for volume textures. IEEE Computer Graphics and Applications (1993). To appear.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[K. PERLIN. A unified textural reflectance model. Advanced Image Synthesis course notes, SIGGRAPH '84, July 1984.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[K. PERLIN. An image synthesizer. In Computer Graphics (SIGGRAPH '85 Proceedings) (July 1985), B. A. Barsky, Ed., vol. 19, 287-296.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[K. TORRANCE AND E. SPARROW. Theory for off-specular reflection from roughened surfaces. Journal of the Optical Society of America 57, 9 (1967), 1105-1114.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134075</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[S. H. WESTIN, J. R. ARVO, AND K. E. TORRANCE. Predicting reflectance functions from complex surfaces. In Computer Graphics (SIGGRAPH '92 Proceedings) (July 1992), E. E. Catmull, Ed., vol. 26,255-264.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[L. WILLIAMS. Casting curved shadows on curved surfaces. In Computer Graphics (SIGGRAPH '78 Proceedings) (August 1978), vol. 12, 270-274.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Smooth Transitions between Bump Rendering Algorithms Barry G. Becker1 Nelson L. Max2 University of 
California, Davis and Lawrence Livermore National Laboratory Abstract visible. Most real surfaces are 
neither purely specular (mirror-like) nor purely diffuse, but rather somewhere in between. To represent 
A method is described for switching smoothly between this non-trivial distribution of light re.ectance 
a BRDF is used. It rendering algorithms as required by the amount of visi-can be represented by a table 
indexed by a lighting direction and ble surface detail. The result will be more realism with a viewing 
direction, to give the re.ectance as a function of these less computation for displaying objects whose 
surface directions. The BRDF used for this research is constructed from detail can be described by one 
or more bump maps. The distributions of normals recorded from various views of a single three rendering 
algorithms considered are a BRDF, displaced surface patch. bump mapping, and displacement mapping. The 
bump Bump-mapping [2] is an inexpensive way to achieve a good mapping has been modi.ed to make it consistent 
with approximation to macroscopic surface roughness. The parameter­the other two. For a given viewpoint, 
one of these ized surface is treated as smooth for the purpose of visible surface algorithms will show 
a better trade-off between qual-determination, while the surface normals are perturbed to a .rst ity, 
computation time, and aliasing than the other two. order approximation of what the actual bump normals 
would be. The decision as to which algorithm is appropriate is a The third algorithm, displacement-mapping 
[4, 5], is used when function of distance, viewing angle, and the frequency any shortcut in computation 
will be noticeable to the eye. Displace­of bumps in the bump map. ment mapping is different in that the 
surface is actually offset by the appropriate bump height so that the full 3-D geometry can be rendered. 
For purposes of maintaining consistent shading, the sameCR Categories: I.3.3 [Computer Graphics]: Picture/Image 
Gen­approximated normal is used to shade the displaced surface as waseration; I.3.5 [Computer Graphics]: 
Three-Dimensional Graphics used in the bump map. However, now it is applied to the displacedand Realism. 
surface rather than to the .at parametric one. Keywords: animation, BRDF, bump map, displacement map, 
Bump-mapping is good for economically rendering bumps which rendering, surface detail, volume texture. 
can be described as a height .eld. Unfortunately it does not account 1 (510) 422-3724 becker@mozart.llnl.gov 
for occlusion. It is necessary to modify .at bump-mapping so that 2 (510) 422-4074 max2@llnl.gov it yields 
images statistically similar to images produced by the other two methods. This revised procedure will 
be termed redistribution bump-mapping because it redistributes the normals in a way that 1 Introduction 
is statistically similar to those seen on the displaced surface viewed from a speci.c direction. Objects 
in animation are sometimes distant specks; at other times a The three methods are blended together so 
that the parts of thetiny part of one will .ll the whole screen. If these objects have rough scene which 
are close to the viewer, or close to the extreme edgesurfaces, the same rendering algorithm should not 
be used in both (silhouette), would be displacement-mapped, since this is wherecases. Almost all real 
materials have a hierarchy of surface detail. missing detail would be noticed most. Smooth silhouette 
edges areWe assume that the macro-structure of all objects is described by an artifact of bump mapping 
which is easy to detect. Parts fartherparameterized patches or a polygonal mesh. The micro-structure 
is away,orwhosenormalsareparalleltotheviewingdirection,will bethen described by one or more bump tables 
for each level of detail bump-mapped. When surfaces have microscopic material-speci.cbelow the geometrical, 
each giving bump height as a function of the qualities or are very far from the viewer, they are rendered 
using a2-D surface parameters. An alternative way to describe the surface BRDF. More speci.cally, for 
a given scene, those features with adetail is through the use of volume textures to specify bump height 
spatial frequency higher than one half cycle per pixel (the Nyquistas a function of 3-D coordinates [10, 
12]. limit) are considered in the BRDF. At the other end of the spectrum,The Bidirectional Re.ection 
Distribution Function or BRDF features that are large enough to cause noticeable occlusion need to[13, 
14, 6] captures the surface properties which are too small to be be displacement-mapped. The parts in 
between are rendered with Permission to copy without fee all or part of this material is granted provided 
that the copies are not made or distributed for direct provided that the copies are not made or distributed 
for direct commercial advantage, the ACM copyright notice and the title of the commercial advantage, 
the ACM copyright notice and the title of the publication and its date appear, and notice is given that 
copying is by publication and its date appear, and notice is given that copying is by permission of the 
Association for Computing Machinery. To copy permission of the Association for Computing Machinery. 
otherwise, or to republish, requires a fee and/or specific permission. otherwise, or to republish, requires 
a fee and/or specific permission. &#38;#169;1993 ACM -0 -89791 -601 -8/93/008 $1.50 &#38;#169;1993 ----8/93/008/0015 
$1.50 varying degrees of redistributed bump-mapping. Most importantly, there is a smooth transition among 
the three. The effect is that the whole scene looks as if it were displacement-mapped, when in fact much 
of it was rendered with cheaper algorithms. Extending this concept we can have high frequency rough surfaces 
on top of low frequency rough surfaces, each bumpy level of detail having three rendered representations. 
 In Figure 1 we see a teapot rendered in the four different ways. All renderings are based on the same 
height function. A major con­sideration for a smooth transitions among these is the consistency of the 
shading between methods. The amount of light emitted by a surface rendered with one method does not necessarily 
equal that amount emitted by the same surface rendered with another. Nor is the distribution of that 
light necessarily equivalent. A key aspect of this research is the determination of how the varying algorithms 
need to be modi.ed in order to have their overall area-averaged light intensity contributions consistent. 
There are .ve reasons why the average re.ected intensity from a bump-mapped image is inconsistent with 
the re.ected intensity from either the BRDF rendered image or the displacement-mapped image of the same 
object. Usually the BRDF is constructed under the assumption that the microfeatures of the surface are 
composed entirely of specular, mirrored facets. Bump-and displacement­mapping contain both specular and 
diffuse components. The easy solution to this inconsistency is to include a diffuse component for each 
microfacet when constructing the BRDF for the high­est frequency bumps. Usually there is an inconsistency 
between bump-and displacement-mapping because actual surface displace­ment creates a geometrically computed 
facet normal for the shader while the perturbed normals for bump maps are only approxima­tions. As previously 
mentioned this is overcome by using the ap­proximated bump-mapped normals on the displaced surface. The 
approximated bump normals also vary more smoothly than the facet normals, especially with our quadratic 
interpolation, which is smoother than Blinn s approximation [2]. Note that if a procedural displacement 
function is employed, it is possible to compute the surface normal analytically. Since the BRDF is constructed 
from a displacement-mapped patch, the same inconsistency may arise for it. Again the solution is remedied 
by using the bump normal for tabulating the BRDF. The most dif.cult consistency problem is caused by 
occlusion. Occlusion, which is the hiding of some bumps by others, can change the distribution of visible 
surface nor­mals. A solution is presented which redistributes bump normals so they match a distribution 
of normals similar to one derived from displacement-mapping. Lastly, there is the problem of consistency 
of shadowing. We have not yet found a general solution for shad­owing, so we draw our images and compute 
our BRDF without it. The concept of blending between methods is not new. The dif.culty in overcoming 
the intensity distribution inconsistencies is perhaps the main reason why there are few coded examples. 
Kajiya [8] mentioned a hierarchy of scale which is appropriate for modelling the complexity of nature. 
He states that each level of detail contains the three subscales discussed above. Westin et al. [14] 
describes these levels as the geometrical, milliscale, and mi­croscale. Perlin [11] proposed a method 
to shift between the BRDF and perturbed normals. Perlin s method does not include an explicit height 
table for determining the new normals,making displacement­mapping dif.cult. Fournier [7] has presented 
a promising approach for .ltering normal maps by recording a discrete number of Phong peaks. The software 
for each of the three algorithms described in this paper has been combined according to the previously 
discussed considerations. The result is an animation which explores a surface from changing distances 
and directions, showing that there are no signi.cant side effects while transitioning between renderers. 
For more detail concerning the implementation refer to Becker [1]. 2 Basic Algorithms 2.1 Bidirectional 
Re.ection Distribution Functions The BRDF is used to capture the microscopic re.ectance properties of 
a surface. The BRDF itself can be a table of re.ectivities or it can be representedby a spherical harmonic 
series approximation [3, 14]. It is a function of either three or four variables representing the polar 
and azimuthal angles of the light rays. The polar angle is called and it measures the angle away from 
the normal. Its domain wn is02. The azimuthal angle is denoted byand has domain02, with 0 and 2both 
in the direction of the viewer. An   w w isotropic surface is one for which the emitted intensity does 
not vary as the surface is rotated radially about its surface normal. If only isotropic textures are 
used, then the arguments to the BRDF reduce to the two polar viewing directions and the difference in 
the azimuthal angle between the viewing and lighting directions. In the most general anisotropic case, 
the BRDF is a function of viewing direction and lighting directions, requiring all four angles. There 
are several different ways to construct a BRDF. Cabral [3] constructed the BRDF directly from a bump 
map using horizon tables. Westin et al. [14] ray traced a generalized 3-D surface sample in order to 
calculate the intensities for their BRDF. Our method uses normal distributions. They are already required 
in order to create redistribution functions for the new bump-mapping method. The same normal distributions 
are used to create the BRDF. Fournier [7] has also discussed normal distributions. A normal distribution 
is obtained by tabulating sampled nor­mals from a projected displacement-mapped .at patch. The range 
of normals is a hemisphere. The hemisphere can be discretized into a .nite number ofbins. When the displacement 
map is N N projected, each pixel of the projected image represents a sample normal, and the count for 
the bin containing that normal is incre­mented. If bump-mapping is used to draw the .at patch, then the 
approximated normal distribution is independent of. However, u when looking from some direction with0, 
self-occlusion may occur in the displacement-mapped image. This occlusion is ac­counted for by rendering 
the displacement-mapped geometry with a hardware z-buffer, coding the normal directions into the pixel 
colors. For grazing angles many potentially occluding patches may have to be rendered in order to get 
the occlusion correct on a single patch. The problem is solved by rendering a single patch using par­allel 
projection, and then using a block read from the screen buffer to copy the patch to all the positions 
where it is needed, in a back to front ordering. In a postprocess the sample normals are scanned in and 
the distributions are created. These distributions will be used to .nd the redistribution functions and 
to make the BRDF. The normal distributions are stored in a 3-D table. The .rst index is the viewing polar 
angle. The second and third indices are theangles specifying the normal direction. For simplicity a table 
access is described by ], where , and N hV distr V V V L N N V uV N N V N NV N V denotes. The difference 
between viewing and lighting s is denoted by. To improve the statistics of the distribu­tion, the patch 
is viewed in manydirections for each. The result is normal distributions for eachwhich account for proper 
occlusion. To use these distributions in constructing the BRDF, the algorithm in Figure 2 is used. Note 
that there are two components to the BRDF, one for the diffuse information and one for the specular. 
This way the amount of diffusivity and specularity chosen can be used as a parameter later. Theandrepresent 
the angles between the viewing or lighting direction and the bin normal , rather than with the .at surface 
patch normal. The angleis the difference between and when projected to the plane perpendicular to the 
bump normal. It is computed by arctan LxV u V 0 V 0 L 0 0 Vu L u L 0 mod VLy  u N 0 V B h 0 V x 
L 0 N L n LVw  wy B h N w arctan21 where 100and 010are the axis directions of the bump table. This 
technique will give the same BRDF as if for each level n from highest to lowest frequency for eachfor 
eachfor each f H ug V  V L HL V N n NL distr V n P j 0 hong V n n0 distr L Vj specn Nn 0   N for 
each for each if highest frequency BRDF incrementby increment by computeand increment by 1 increment 
by gf g B RD F difspec n VB fRD LV0 F dif specn L0 f V V0 V L L L distr V n L V N Figure 2: The algorithm 
to compute the BRDF using a table of normal distributions. the combined displacement maps were used, 
as long as there is no correlation between the bumps at the different levels. A smooth surface patch 
is rendered by interpolating the BRDF trilinearly in the angles, and. The indices for the table V L V 
L u v u are computed from a local coordinate on the patch surface. The smooth surface normal points in 
the direction of0. The origin of the azimuthal angle is the projection of the viewing direction onto 
the surface. For a given patch parameterization,, the partial deriva­tives, and , are rarely the same 
length (causing P elseu u m Pu P vm Pv stretching), and not always perpendicular (causing warping). For 
these reasons special care must be taken when indexing the BRDF to determine an intensity. The method 
for computing the difference in azimuthal angle is as follows: 0 VV L nn1L u VL P uu j VLV n jP vv j 
L n j 0 Farccos2 The stretching will actually change the normal directions mak­ing the BRDF inaccurate. 
The BRDF would need to be recalculated to yield a theoretically correct result, but equation (2) does 
get the occlusion correct and gives nice anisotropic highlight effects in places where they would be 
expected. 2.2 Bump Mapping In Blinn s bump-mapping [2], the surface is not actually altered from its 
smooth parametric form, but it is shaded as though it were. Blinn used a bump height table B to calculate 
a linear approx­imation to the bump normal at a point P on an object surface. If and are the partial 
derivatives as above, the unnormalized surface normal is In the bump map , the partial derivatives and 
at the interpolated point corresponding to can also be computed using .nite differences.  aP u B v Pa 
v BB u u NaBB u v u n Pa u [ B v aP h v cB uB h [ v n a [B P 23 and is similar. Each evaluation of 
uses bilinear interpolation. Truncating insigni.cant terms, Blinn [2] has showed that the new normalized 
normal is very close to Na 0 4Na 0 u j Na n B u Na B Pa v h B v Na B Pa u j We have chosen to compute 
the bump map derivatives by a quadratic rather than linear scheme. Mach bands are eliminated by replacing 
Blinn s linear formula by a 1 partial derivative formula, de.ned by taking the derivative of the 2 cubic 
B spline curve approximation to the bump heights as a function of or of . Let , then 22 2 5 1322B u u 
h u ch cB b u ch v n dun h uduBvuv 22  du u n B h v du b n n du v n cB b u c vC n CdunB b u c n b v 
c 325 22 and is similar. Here each function evaluation requires only a linear interpolation in . This 
method uses the same eight neighbor­ing values in the height table as does (3), but with quadratic rather 
than linear weights. The normals generated by this process do not lie in a distribution consistent with 
the other two algorithms. As previously discussed, must be further modi.ed so that on average it will 
contribute to a normal distribution similar to displacement-map normals. This new algorithm, redistribution 
bump-mapping, is described in detail in Section 3. It should also be noted that Perlin s volume textures 
[12], with the improvement by Max and Becker [10], can be substituted for bump maps when computing height 
values. The advantage of this is that there is no explicit parameterization to be concerned with, and 
thus no stretching to cause singularities or anisotropy. If a square patch has an isotropic texture mapped 
onto it, the texture becomes anisotropic as soon as the patch is stretched unevenly. Many parameterizations 
have singularities which lead to degenerate patches. If anisotropy is undesirable, then volume textures 
should be used. Perlin also used volume textures, and redistributed the normals to make them gaussian 
(personal communication) in his implementation of [11]. 2.3 Displacement Mapping Displacement-mapping 
is the direct approach to rendering surface detail. For parameterized surfaces, each patch in the object 
has a  uvuv and parameterization. The and coordinates are used as indices to look up height values in 
the bump height table. The cor­responding vertex is then displaced along its normal vector by that height 
[4]. The normal generated from the bump approximation is also used on the displaced vertices. There is 
little loss of accuracy in doing this, and continuity during the transition is assured. Oc­clusion, the 
main problem with bump-mapping, is accounted for automatically when the vertices are displaced. Having 
multiple bump maps for many levels of detail means the displaced bumps will be rendered with the BRDF 
constructed from the next bump map of higher frequency. To keep combined displacements consistent with 
BRDFs representing several com­bined bump maps, surface perturbations for the level must be perpendicular 
to the 1 displaced surface. This means that for each vertex, and vectors must be computed for each level 
of detail which has been displaced. Since and are not neces­sarily perpendicular it is recommended that 
the following formula be used to compute them, given that the surface normal is BP uu ilPevB v el v 
Pi u n i u h P v Pi uthth level n B u Pl u evelNiP thv levelNc 1 where are the bump map partial derivatives. 
The equation for is similar. 3 Redistribution Bump Mapping 3.1 Normal Redistribution The problem of eliminating 
inconsistencies between the different rendering models lies at the heart of making smooth transitions 
from one algorithm to another. Primarily we are concerned with keeping the integral of intensities equal 
over a small area on the surface while the rendering method changes. Unfortunately, normals from bump-mapping 
do not yield a dis­tribution similar to that of displacement-mapping or the BRDF. Since the polygon or 
patch itself is not displaced, it is possible to see normals which ought to be hidden by occluding bumps. 
In order to overcome this problem a redistribution function is cre­ q ated. This is a function which 
accepts as input a normal generated by Blinn s [2] bump approximation, and outputs a normal which is 
statistically consistent with the distribution used to form the BRDF. Since the distribution of normals 
on a displacement-mapped .at patch is different for each viewing angle, it is necessary to have redistribution 
functions for each one. When the viewing angle is vertical, the identity function is used. When the viewing 
angle is just above the horizon, the redistribution of bump normals is necessarily quite drastic. The 
effect is to pull forward normals that might be facing away, and push upward those that might be hidden. 
This new scheme for doing bump-mapping might appropriately be termed redistribution bump-mapping. 3.2 
Redistribution Function Construction Suppose a bumpy surface is viewed from a direction with polar  
 fq VV gdistrgNdistrqN angle . Let denote the distribution of normals ) at this .xed , computed as above 
from the displacement map. Let denote the distribution of normals in a (non-displaced) bump­mapped image. 
Note that f is the same as 0 . If is the redistribution function described above, then the requirement 
that take the distribution to the distribution is that for any region in the hemisphere of possible normals, 
5 It is easier to explain how to specify in a 1-D case. So suppose and are two distributions on 0 1 such 
that fxgx ZZ q 11 Rab 2 Z q Z f Rq q f H q f aff f bFq bG xfGfbbdxxxb u dx de uu Z w u F uu bb fg 
Z Z qg Ra x b bb xgg dxcdxdxxxdxc u dxde 16 00 The problem is to .nd : 0 1 0 1 such that 7 where and 
0 1 . It is enough to guarantee that8 00 Let 0 and 0 Then hence 1 qbfbgqbq u FGbcb 9 The redistribution 
function maps a point so that the area under the curve before in is equal to the area under the curve 
before the point in . The problem in 2-D can be handled similarly. One method is to de.ne 1-D redistribution 
functions separately for and . This gives adequate results for most bump maps, whose and distributionsarefairlyindependent. 
Thisindependenceassumption is con.rmed by the animation. For a more precise redistribution function, 
one can .rst redistribute , and then for each .xed , establish a separate redistribution function for 
. For details see Becker [1]. 4 Transitions 4.1 Partial Bump Displacement For control of appearance and 
for smooth transitions we want the ability to change the height of the bumps in the bump map. This will 
alter the normal distribution and occlusion information. By close consideration we can see that the change 
can be accounted for without having to recalculate the redistribution functions every time the bump heights 
are altered. If the heights are multiplied by a factor , then the tangent of the angle between the bump 
normal and the smooth surface normal should also change by a factor ; i.e., tan tan . The normal, , needs 
to be replaced by arctan tan . In order to keep the visibility information the same, the viewing angle, 
, must be replaced with arccot cot See discussion below concerning Figure 3. The height of the bumps 
used to calculate the BRDF and re­distribution functions must be the same as that of the bumps being 
rendered. ThisisbecausetheBRDFischangedinanon-trivialway as the bump heights change. If we were only 
concerned with bump­and displacement-mapping, we could change the indexing on the redistribution functions 
to get the occlusion correct for changing bump heights. Unfortunately there is no easy way to re-index 
the BRDF to account for scale changes. Between the BRDF and redis­tribution bump-mapping, an intensity 
is computed for both methods. The resulting intensity is an interpolation of the two. For the transition 
between bump-and displacement-mapping, intensity interpolation is not used, since it would cause the 
bump shading (particularly the highlights) to cross-dissolve rather than correctly adjust in position. 
As the bumps go from no displacement to full displacement the surface normals do not change, since they 
are always represented by Blinn s bump normal. The visible subset of bump normals does change, however, 
due to changing occlusion. Let be the transition parameter which gives the fraction of the t N t u tN 
Wt uu N t V nt N cN u NNV t  dispdispdisp uu full bump height. With 0 all normals are seen, even those 
on the back of bumps. With 1, only the visible subset of these normals are seen. In Figure 3 the segments 
of the visible surface are shown in bold. The redistribution of normals takes normals from standard bump-mapping 
into this visible subset. For partially displaced bumps there is a different subset of visible normals, 
but there is a relationship between the bump height and this subset which can be exploited to give the 
necessary redistribution. Different redistribution functions for varying heights are not stored, only 
different functions for different viewing s. Fortu­nately the two are equivalent. For the fractional 
bump height, , we can determine a new for which the same distribution of full height bump normals will 
be seen. Figure 3 shows that the distribu­tion of normals for this partially displaced surface, viewed 
from , is identical to the distribution of visible normals for the fully dis­placed surface viewed from 
. The slope of the line in Figure disp WW W V u dispVdisp WV 3 is times the slope of line , so cot 
cot -1 F (N) .  W V -1 F (F (N)) .. VW W V Figure 3: Top: the non-displaced surface. Middle: surfaced 
sis­placed by bump height fraction . Bottom: Fully displaced surface. and the formula for .nding is: 
  u N V dispndisp W c V arccot cot The inverse redistribution function for is applied to take the visible 
bump normal from the partially displaced surface into a dis­tribution similar to one from a .at bump-mapped 
surface. Next the redistribution function for is applied to that normal to take it all the way forward 
to match statistically a full displacement-mapped normal. Thus the change from bump-mapping to displacement­mapping 
is done through two table based function evaluations. Notice that as the bumps decrease in height, the 
new viewing W approaches vertical. This means that the inverse function needs to alter the normals less 
in order to get them back to the bump-map distribution. 4.2 Algorithm Selection Criterion Now that it 
is known how to modify the algorithms so that they will not deviate from a fundamental re.ection model, 
it must be decided when to apply which algorithm. Clearly displacement-mapping should be applied when 
the view is close, and the BRDF when the view is far. The relationship is 1, where is distance, since 
that f V ndnd V df is how the projected size of an object relates to distance. Another variable to consider 
is viewing angle, . If is the wavelength of a feature then cos is the wavelength of the projected feature 
(in the direction of maximum foreshortening), and should be no smaller than two pixels. When the object 
is close, we would like to see a rough silhouette; when it is far, aliasing becomes a problem on the 
edge so use of the BRDF is desirable. This implies that as the object moves away from the viewer, the 
transition from displaced bumps to BRDF will be far more rapid on the object silhouette than on that 
area where the patch normal points toward the viewer. The threshold at which the switch occurs is determined 
by a constant . D Summarizing these properties, we de.ne a transition parameter 1cos 10  dTd V u nd 
h Dn V n [c Here is the distance from the viewpoint to the surface, is the angle between the viewing 
ray and the surface normal, and is dependent on individual bump maps. To avoid an instantaneous transition 
on the silhouette an is added to the cosine term in the denominator. The constant should be large if 
the highest frequency component of the bump map is large. Note that controls where the function changes 
from positive to negative, and thus lies midway between displacement-mapping and the BRDF. The formula 
for determining is freqcuP u vD u PD v c[DfreqSS V DSSD where is the highest frequency in the bump map 
and is the amount the and values are scaled. If is large, then the bump map will be repeated more times 
over the same area, and the partial derivatives, and , are made shorter by a factor of . The constant 
controls computational effort by globally shifting the scene toward more BRDF or alternatively more displacement. 
If shadows are included, the shadow terminator should be treated just like the silhouette. Areas far 
from the terminator are likely to be completely illuminated or shadowed, but on the terminator, displacement-mapping 
will make the shadowing exact. The param­eter given by equation (10) determines the algorithm or algorithms 
used for rendering. Let the threshold values for choice of renderer be 1 2 0 3 4. If 1thenusetheBRDF,if 
 Tuee.e..e.eT.Tee.T.e 4 then use displacement mapping, and if 2 3 use redistribution bump mapping. Values 
of other than these indicate regions where algorithms are blended. Values of -1, -.3, .3, and 1 respectively, 
were found to give good results. 4.3 Multiple Levels of Detail With multiple levels of detail there 
are many more than two pos­sible transition points. Many other cases need to be considered. The displacement-mapped 
image of the layer is rendered using i h th i th the BRDF for the 1 layer. As the camera continues to 
zoom in, the BRDF will switch to bump-mapping and then again to displacement-mapping. Since each bump 
map has its own independent transition re­gions, some areas may have bump-mapping from two or more different 
levels. Perlin [11] suggests that each set of bumps be lim­ited to a narrow range of frequencies. The 
result of implementing two levels of detail is shown in Figure 4. The bump map describing the surface 
detail is broken up into high and low order band-limited frequencies. The low frequencies compose the 
.rst level bump map and the high frequencies compose the second level. The left half of Figure 4 is color 
coded according to the algorithm used to render the most re.ned level of detail visible. Hence one can 
see bumpy sections colored yellow to indicate the BRDF from the next lower level was used to render the 
displaced bumps. 5 Results 5.1 Consistency Comparison In Figure 5 we can see the four rendering methods 
compared. The difference between the lighting and viewing is zero. Note that since the lighting and viewing 
directions are in alignment the patch becomes brighter for grazing angles. The rows are rendered with 
bump-mapping, redistribution bump-mapping, BRDF, and dis­placement mapping respectively. Note that redistribution 
bump­mapping is far more consistent with the BRDF and displacement­mapping than is ordinary bump-mapping. 
Figure 6 is a table which Figure 4: Two levels of bumpy detail. Colors in the bottom half indicate BRDF 
(yellow), redistribution bump mapping (blue), and displacement mapping (red) in the higher frequency 
bumps.  Figure 5: Intensity comparisons. The lighting direction is consis­tently 4. The rows from top 
to bottom represent bump­  u wn  mapping, redistribution bump mapping, BRDF, and displacement mapping. 
T=0 T=/6 T=/3 T=4/9 Bump 128 129 129 129 Redistribution 128 143 170 194 BRDF 129 146 172 192 Displacement 
128 146 175 194  vv w v w v w Figure 6: Area averaged intensities for the diffuse component.  Figure 
7: Transitions on a .at surface. BRDF (yellow) in the back, redistribution bump mapping (blue) in the 
middle, and displacement mappign in the foreground. shows quantitative results for viewing angles corresponding 
to those shown in Figure 5. In Figure 7, a single .at patch is drawn in perspective. Regions in the foreground 
are clearly displacement-mapped. The middle region is redistribution bump-mapped, and the furthest edge 
is al­most completely shaded with the BRDF. It should be apparent that there is no intensity inconsistency 
between methods and that the transition is smooth. 5.2 Conclusions Combining displacement-mapping, 
bump-mapping and a BRDF into one algorithm makes it possible to explore great scale changes, without 
changing the geometrical data base. Using a series of bump maps we can generate a variety of rough surfaces 
simulating different material properties. Objects in the scene will have a complex underlying structure 
but only the minimum amount of effort necessary to give the impression of complete geometrical representation 
will be expended. Current animations are restricted by the amount of geometrically represented detail. 
If the view gets too close to a feature, large drab polygons .ll the display. With hierarchy of detail, 
the polygon level need never be reached, no matter how close the viewer gets. Even at intermediate and 
far distances the light interacts with .at polygonal surfaces as if they were truly composed of millions 
of smaller micro-polygons. As a result the otherwise drab polygons become alive with texture and interesting 
highlights. Those smaller micro-polygons may actually get rendered, but only if the viewer zooms in much 
closer. 5.3 Future Research Shadowing is the main enhancement yet to be considered. One way to do the 
shadowing of displaced bumps is to use the two-pass z-buffer method developed by Williams [15]. Horizon 
mapping [9] has been shown to generate shadows for bump-mapped images. It will also work for redistribution 
bump-mapping since the horizon is determined by the and parameterization, not the normal. How­ uv ever, 
this may cause a problem since the rendering is according to a redistributed normal, and the shadows 
are according to the parame­terization. The shadowing may look inappropriate for the rendered bumps. 
The shadowing for BRDFs can be done using horizon map­ping, as was demonstrated by Cabral [3]. Another 
possibility is to use only the unshadowed normals from a displaced, rendered, and shadowed .at patch 
to generate the distributions for the BRDF and the redistribution function. The result should be consistent 
in terms of average intensity, but may not look qualitatively correct. 5.4 Acknowledgements This work 
was performed under the auspices of the U.S. Depart­ment of Energy by Lawrence Livermore National Laboratory 
under contract No. W-7405-Eng-48. Bibliography [1] B. BECKER. Smooth transitions between bump rendering 
al­gorithms during animation. Master s thesis, University of California at Davis, December 1992. [2] 
J. F. BLINN. Models of light re.ection for computer synthe­sized pictures. J. George, Ed., vol. 11, 192 
198. [3] B. CABRAL, N. MAX, AND R. SPRINGMEYER. Bidirectional re.ection functions from surface bump maps. 
In Computer Graphics (SIGGRAPH 87 Proceedings) (July 1987), M. C. Stone, Ed., vol. 21, 273 281. [4] R. 
L. COOK. Shade trees. In Computer Graphics (SIGGRAPH 84 Proceedings) (July 1984), H. Christiansen, Ed., 
vol. 18, 223 231. [5] R. L. COOK, L. CARPENTER, AND E. CATMULL. The Reyes image rendering architecture. 
In Computer Graphics (SIG-GRAPH 87 Proceedings) (July 1987), M. C. Stone, Ed., 95 102. [6] R. L. COOK 
AND K. E. TORRANCE. A re.ectance model for computer graphics. vol. 15, 307 316. [7] A. FOURNIER. Normal 
distribution functions and multiple surfaces. In Graphics Interface 92 Workshop on Local Illu­mination. 
1992, pp. 45 52. [8] J. T. KAJIYA. Anisotropic re.ection models. In Computer Graphics (SIGGRAPH 85 Proceedings) 
(July 1985), B. A. Barsky, Ed., vol. 19, 15 21. [9] N. L. MAX. Horizon mapping: shadows for bump-mapped 
surfaces. The Visual Computer 4, 2 (July 1988), 109 117. [10] N. L. MAX AND B. BECKER. Bump shading 
for volume tex­tures. IEEE Computer Graphics and Applications (1993). To appear. [11] K. PERLIN. A uni.ed 
textural re.ectance model. Advanced Image Synthesis course notes, SIGGRAPH 84, July 1984. [12] K. PERLIN. 
An image synthesizer. In Computer Graphics (SIGGRAPH 85 Proceedings) (July 1985), B. A. Barsky, Ed., 
vol. 19, 287 296. [13] K. TORRANCE AND E. SPARROW. Theory for off-specular re.ection from roughened surfaces. 
Journal of the Optical Society of America 57, 9 (1967), 1105 1114. [14] S. H. WESTIN, J. R. ARVO, AND 
K. E. TORRANCE. Predicting re.ectance functions from complex surfaces. In Computer Graphics (SIGGRAPH 
92 Proceedings) (July 1992), E. E. Catmull, Ed., vol. 26, 255 264. [15] L. WILLIAMS. Casting curved shadows 
on curved surfaces. In Computer Graphics (SIGGRAPH 78 Proceedings) (August 1978), vol. 12, 270 274. 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166142</article_id>
		<sort_key>191</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>25</seq_no>
		<title><![CDATA[Linear color representations for full speed spectral rendering]]></title>
		<page_from>191</page_from>
		<page_to>198</page_to>
		<doi_number>10.1145/166117.166142</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166142</url>
		<keywords>
			<kw><![CDATA[full spectral rendering]]></kw>
			<kw><![CDATA[linear color representations]]></kw>
			<kw><![CDATA[linear models]]></kw>
			<kw><![CDATA[tristimulus values]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>F.2.1</cat_node>
				<descriptor>Computations on matrices</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003719</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on matrices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31092054</person_id>
				<author_profile_id><![CDATA[81100454294]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Peercy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>122729</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Borges, Carlos. Trichromatic Approximation for Computer Graphics Illumination Models. Proceedings of SIGGRAPH '91 (Las Vegas, Nevada, July 28-August 2, 1991). In Computer Graphics 25,4 (July 1991),101-104.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>917425</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Borges, Carlos. Numerical Methods for Illumination Models in Realistic Image Synthesis. PhD dissertation, University of California, Davis, 1990.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Cohen, Jozef. Dependency of the Spectral Reflectance Curves of the Munsell Color Chips. Psychon. Sci. 1 (1964), 369-370.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801163</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Cowan, William. An Inexpensive Scheme for Calibration of a Color Monitor in Terms of CIE Standard Coordinates. Proceedings of SIGGRAPH '83 (Detroit, Michigan, July 25-29, 1983). In Computer Graphics 17,3 (July 1983), 315-321.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Davis, E and Rabinowitz, E Methods of Numerical Integration. Academic Press, New York, 1975.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>63450</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Hall, Roy. Illumination and Color in Computer Generated Image~7. Springer-Verlag, New York, 1989.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Hall, Roy and Greenberg, Donald. A Testbed for Realistic Image Synthesis. IEEE Computer Graphics and Applications 3 (1983), 10-20.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Judd, Deane, MacAdam, David, and Wyszecki, Gunter. Spectral Distribution of Typical Daylight as a Function of Correlated Color Temperature. J. Opt. Soc. Am. 54,8 (1964), 1031- 1040.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Maloney, Laurence. Evaluation of linear models of surface spectral reflectance with small numbers of parameters. J. Opt. Soc. Am. A 3,10 (1986), 1673-1683.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Maloney, Laurence. ComputationalApproaches to Color Constancy. PhD dissertation, Stanford University, 1985.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Mardia, K., Kent, J., and Bibby, J. Multivariate Analysis. Academic, London, 1979.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Marimont, David and Wandell, Brian. Linear models of surface and illuminant spectra. J. Opt. Soc. Am. A 9,11 (1992), 1905-1913.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[McCamy, C., Marcus, H., and Davidson, J. A Color Rendition Chart. J. Appl. Photographic Engrg. 11,3 (1976), 95-99.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>45601</ref_obj_id>
				<ref_obj_pid>45596</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Meyer, Gary. Wavelength Selection for Synthetic Image Generation. Computer Vision, Graphics, and Image Processing 41 (1988), 57-79.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Parkkinen, J., Hallikainen, J., and Jaaskelainen, T. Characteristic Spectra of Munsell Colors. J. Opt. Soc. Am.A 6,2 (1989), 318-322.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Raso, Maria, and Fournier, Alain. A Piecewise Polynomial Approach to Shading Using Spectral Distributions. Proceedings of Graphics Interface '91. (Calgary, Alberta, June 3-7, 1991), 40-46.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Smith, Brent, Spiekermann, Charles, and Sember, Robert. Numerical Methods for Colorimetric Calculations: A Comparison of Integration Methods. COLOR Research andApplication 17,6 (1992), 384-393.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Smith, Brent, Spiekermann, Charles, and Sember, Robert. Numerical Methods for Colorimetric Calculations: Sampling Density Requirements. COLOR Research and Application 17,6 (1992), 394-401.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Wallis, Robert. Fast computation of tristimulus values by use of Gaussian quadrature. J. Opt. Soc. Am. 65,1 (1975), 91-94.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>28749</ref_obj_id>
				<ref_obj_pid>28748</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Wandell, Brian. The Synthesis and Analysis of Color Images. IEEE Trans. on Pattern Analysis and Machine Intelligence, PAMI-9,1 (1987), 2-13.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Wyszecki, Gunter and Stiles, W.S. Color Science: Concepts and Methods, Quantitative Data and Formulae. John Wiley and Sons, 1982.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Linear Color Representations for Full Spectral Rendering Mark S. Peercy Department of Applied Physics 
Stanford University Abstract We present a general linear transform method for handling full spec­tral 
information in computer graphics rendering. In this framework, any spectral power distribution in a scene 
is described with respect to a set of .xed orthonormal basis functions. The lighting computa­tions follow 
simply from this decision, and they can be viewed as a generalization of point sampling. Because any 
basis functions can be chosen, they can be tailored to the scenes that are to be rendered. We discuss 
ef.cient point sampling for scenes with smoothly vary­ing spectra, and we present the use of characteristic 
vector analysis to select sets of basis functions that deal ef.ciently with irregular spectral power 
distributions. As an example of this latter method, we render a scene illuminated with .uorescent light. 
CR Categories and Subject Descriptors: I.3.3 [Computer Graphics]: Picture/Image Generation Display Algorithms; 
I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism. Additional Keywords: linear color 
representations, full spectral rendering, linear models, tristimulus values. 1 Introduction Accurate 
color rendering in computer graphics must account for the full spectral character of the lights and surfaces 
within a scene. The rendering procedure must preserve enough spectral information to compute .nal values 
for output to some display device, such as an RGB monitor. However, one wishes to minimize the computational 
cost of the rendering to reduce the time required to create an image. Therefore, one desires ef.cient 
methods of handling full spectral information during image synthesis. Author s address: Dept. of Applied 
Physics, Stanford University Stanford, CA 94305-4090 peercy@kaos.stanford.edu (415)725-3301 Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct provided that the copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the commercial advantage, the ACM copyright notice and the 
title of the publication and its date appear, and notice is given that copying is by publication and 
its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy permission of the Association for Computing Machinery. otherwise, or to republish, requires 
a fee and/or specific permission. otherwise, or to republish, requires a fee and/or specific permission. 
&#38;#169;1993 ACM -0-89791 -601 -8/93/008 $1.50 &#38;#169;1993 -0---8/93/008/0015 $1.50 Some suggested 
techniques in dealing with full spectral information include the use of the tristimulus values for the 
lights and surfaces [1], the use of polynomial representations of spectra [16], and the use of linear 
models of surfaces and lights [20] [12]. The typical method employed is point sampling of the surfaces 
and the lights at a given number of wavelengths. These point samples are used in a numerical integration 
method to compute approximate tristimulus values before being transformed to values appropriate for display. 
To minimize the total number of samples, one seeks an ef.cient integration approximation; one approximation 
that has been studied in various forms is Gaussian quadrature [14] [19] [2]. In this paper, we consider 
a more general method for handling full spectral information in synthetic image generation; our technique 
is closely related to the use of linear models presented in [20]. The principal idea is that we describe 
the spectral power distribution of the light at every step of the rendering procedure with respect to 
a single collection of orthonormal basis functions. This formalism encompasses point sampling, which 
uses delta functions as its basis functions. The constraint of describing all of the spectral power distributions 
with respect to the basis functions is advantageous for two reasons. First, it makes the rendering process 
completely linear. Therefore, this technique can be considered a generalization of point sampling and 
can be readily incorporated into standard renderers. Second, one has the freedom to select any orthonormal 
set of basis functions. This freedom can be exploited to increase the ef.ciency of the rendering process. 
The body of this paper is divided into two main sections. In Section 2 we discuss the mathematical formalism 
of linear color represen­tations of the lights and surfaces, and in Section 3 we address the problem 
of selecting appropriate basis functions. In this latter sec­tion, we discuss Riemann summation for ef.cient 
point sampling in scenes with smoothly varying spectra, and we present the use of characteristic vector 
analysis to provide ef.cient basis functions for scenes with complex spectra.  2 Linear Color Representations 
During the rendering process, we demand that any spectral power distribution in the scene be described 
by orthonormal basis func­ m  (and ) is represented by a single   R a R ad R d R s m g m matrix (and 
, respectively). The interaction of light with a surface component assumes the form of simple matrix 
mul­tiplication, converting the coef.cients of the incoming light into the coef.cients of the outgoing 
light. This result is a generalization of the point sampling case; with point samples, the surface matrices 
are diagonal, and the matrix product multiplies respective sample values. Because this technique is linear, 
it can be included without dif.culty in standard renderers. For the general lighting model case, the 
specular matrix is a func­tion of the geometry. Because the elements of the surface matrices are obtained 
through integration over the basis functions, this in­tegration must be performed for each geometry con.guration. 
If, however, one uses a piecewise separable lighting model, the geom­etry and wavelength dependence separate 
in the specular term, o R aa G d R a RR dds GR ss R ss and the three surface matrices, and , can be 
precomputed. The above discussion addresses only surface re.ection, but effects such as transmission 
and attenuation can be included straightfor­wardly in this framework. As with the re.ectance components, 
these terms take the form of matrices that act on the coef.- RGB m g X pm mim cients of the incoming 
light. 2.3 Conversion to The rendering algorithm determines the spectral contributions to a pixel by 
computing multiple re.ection paths from each of the light sources to the viewer. These contributions 
are transform coef.­cients, and by linearity they can be combined to provide a .nal set of coef.cients 
for that pixel, ; 1 . Equation 1 gives the approximation to the spectral power distribution arriving 
at the pixel, (17) 1 To compute appropriate values for display, one .rst computes the tristimulus values, 
, for the pixel by integrating the .nal spec­trum over the three color matching functions [21]  XYZ 
ZZZ zxy XIIIY ppp IZ p d d d i ZXZXZX piiiimmm E XXX iiiimm T m TT zyxiiip iiipiip i zyx EEE iii d 
d d (16) 1 (18) (19) (20)  1 1 1 1 1 In matrix form, this set of equations can be written 1 2 . (21) 
. 12 With , this equation yields(22) The elements and of the matrix are coef.cients that result from 
integration of the basis function over the three color matching functions. For point sampling, these 
elements are modi.ed based on the method of numerical integration. For ex­ample, common Riemann summation 
over evenly spaced samples includes the distance between the sample points [17], and Gaussian quadrature 
has its own unique weights [5]. xXYZXYT xi ZT T yxyzi T xyz xT zi Ti thp T xmyzm 0B T p m 1C A Assuming 
that an display monitor is properly gamma- RGBcR G B T g 1 M corrected [4], the color values, , of a 
given pixel are computed from the tristimulus values by applying a 33 matrix, , derived from the chromaticities 
of the phosphors of the monitor [6] (23) (24) (25)  Therefore, the values can be obtained directly 
through a linearRGB 1 c 2 M CxT pp g .2 mC transformation of the .nal coef.cient values by a 3 matrix 
. Because this step is linear, it can be applied at any time to the separate contributions to the .nal 
pixel values. 3 Selection of Basis Functions It is in the selection of the basis functions that the .exibility 
of the general transform method is demonstrated. In this section, we describe some factors that determine 
the effective selection of basis functions, and we present two methods for determining basis functions 
that are tailored to the spectral power distributions in a scene. As mentioned in Section 2, the lighting 
model is a signi.cant in­.uence on the choice of basis functions. If the lighting model is not piecewise 
separable, the surface matrices must be computed for each geometry con.guration, so the most ef.cient 
basis functions are most likely point samples. If, however, the lighting model is piecewise separable, 
we have another consideration. The compo­nents of the surface re.ectances are represented by matrices.mmm 
g mm Therefore, the re.ection of light from a surface requires, in general,2 multiplies. If the basis 
functions are point samples, though, the surface matrices are diagonal, and the re.ection requires only 
multiplies. Indeed, only multiplies are required for any set of non-overlapping basis functions. Consequently, 
the computational intensiveness of the general transform rises more rapidly than that of point sampling 
as the number of basis functions increases. A third consideration when selecting basis functions is the 
nature of the spectral power distributions in the scene to be rendered. For smoothly varying distributions,point 
sampling can be quite ef.cient, but for complicated spectra, a set of general basis functions can be 
more appropriate. We discuss each of these methods in the following sections. 3.1 Point Sampling Point 
sampling is typically linked to a numerical integration method used in approximating the tristimulus 
integrals, Equations 18-20. Gaussian quadrature, which is optimal for integrating polynomi­als over general 
weighting functions [5], has been applied to this problem [14] [19] [2]. If the spectral power distributions 
are well described by lower order polynomials, Gaussian quadrature can pro­vide suf.cient accuracy with 
a small number of sample points; it was shown in [14] that as few as four point samples are adequate 
for many rendering applications. Here, we discuss the use of simple Riemann summation for approx­imating 
the tristimulus integrals. Rather than being ef.cient for polynomial functions, Riemann summation is 
ef.cient when inte­grating functions that contain a small number of Fourier coef.cients. Riemann Summation 
Riemann summation is the sum over evenly spaced sample values weighted by the distance between the sample 
wavelengths [17]. Given 2 evenly spaced sample points 0 1 1 separated by a distance10 1 and a spectral 
power distribution , Riemann summation gives   NXZYI ZZ zxyIII N d d d g  R NX iN ii zxy iii III 
N iiiN 0 1 An appropriate choice of endpoints, 0 and 1, is the most closely spaced pair of wavelengths 
that can be chosen such that the color matching functions at these wavelengths can be taken to be zero. 
We found that 0 400and 1 700are often reasonable choices; truncation at these limits results in errors 
signi.cantly smaller than those incurred by undersampling the spectra [17] [18]. Taking 0 0 0 0 and 
x N N y N z N xnmyN Nz nm 1 1 1 0, only the interior points, 1 , need to be preserved during the rendering 
process; the basis functions for the spectral power distributions are given by delta functions at these 
wavelengths. With the endpoints of the integrands equal to zero, Riemann summation with points is exact 
for any linear combina­tion of the .rst 22 Fourier functions 1, 20 , 10 20 00 cossind N = pp N NN = 
pp sindN N = pp 0011 cossindNd N p == p (26) 22 10 1010 21 0 . Therefore, if the products of the spec­ 
10 tral power distributions with each of the color matching functions are well described by a small number 
of Fourier coef.cients, Rie­mann summation provides an ef.cient method for integration. For the set of 
spectral power distributions obtained from the Macbeth Color Checker [13] under CIE Standard Illuminant 
C [21], Riemann summation with four point samples at 460nm, 520nm, 580nm, and Spectral Power 300 250 
200 150 100 50 0 450 500 550 600 650 700 Wavelength Figure 1: Spectral power distribution of a .uorescent 
light 640nm results in an average error of less than 5% in the tristimulus values. Rendering with these 
four sampling points is often suf.­cient; if it is not, selecting .ve, six, or more evenly spaced samples 
is straightforward. 3.2 General Basis Functions For scenes with complicated spectral power distributions 
or surface properties, naive point sampling is insuf.cient. One notable exam­ple is the spectral power 
distribution of .uorescent light, which is ubiquitous in indoor scenes. Fluorescent light, an example 
of which is shown in Figure 1 [21], is characterized by narrow emission lines at several wavelengths, 
a factor leading to aliasing with a small num­ber of point samples. For these complicated cases, one 
would like to be able to tailor the basis functions to the complex spectra. One attempt in this direction 
is the use of abutting box functions over the range of wavelengths whose widths are chosen based on the 
spectra within the scene [7] [6]. Another technique for dealing with these scenes is hand-selecting the 
basis functions using knowledge of the spectra in the scene. For example, for .uorescent lights, one 
could ensure that point samples were positioned at the emission lines. Here, we present an alternative 
method for the selection of basis functions, gaining insight from studies done on the construction of 
linear models of surface re.ectances and spectral power distribu­tions [3] [8] [9] [15] [12]. Most of 
these studies have stressed the use of characteristic vector analysis or principal component analysis 
to characterize lights and surfaces. This technique can be applied to the rendering problem to provide 
an automated method for selecting an ef.cient set of basis functions. Characteristic Vector Analysis 
Given a set of spectral power distributions, characteristic vector analysis computes an ordered set of 
functions such that the .rst functions are the best functions for approximating the distri­butions. Here, 
best is measured in terms of least squared error between the actual and the approximating spectra. Formally, 
for the approximation of the spectral power distribution (27) Im X imi E i m 1 the basis functions, , 
are computed such that the sum of the approximation error over all of the lights in the set is minimized 
ErrE X Ii Z . I g X imi E i . d 2(28) 1 In practice, this set can be determined by placing the representative 
spectra in the columns of a matrix and performing a singular value decomposition [10] [11]. The task 
is then to .nd a representative set of spectra on which to per­form the analysis. For the rendering problem, 
the basis functions should describe any spectral power distribution within the scene. The distributions 
contain contributions from the light sources them­selves, from once-re.ected light, and from multiply-re.ected 
light. Therefore, an appropriate set of spectra is that set derived from pos­sible interre.ections within 
the scene. Given the spectral power dis­tributions of the lights and the components of the surface re.ectances 
in a scene, one can construct a tree of possible interre.ection spec­tra (disregarding any geometry). 
The lights themselves would be included, and any number of re.ections and interre.ections could be included. 
The basis functions computed from a characteristic vector analysis of this set would then approximate 
these spectral power distributions. If the number of spectral power distributions to .t is too large, 
this technique can become inef.cient; the cost of computing the basis functions may exceed the savings 
in rendering time. Also, this method is inapplicable if one does not know a priori the spectral character 
of the surfaces and lights in the scene. However, for many scenes, this technique can readily be applied. 
 3.3 Examples To demonstrate the use of characteristic vector analysis in selecting basis functions, 
we present two related examples. Both examples use the .uorescent light in Figure 1 to show the ability 
of this technique to handle complex spectra. In the .rst example, we determine the ef.ciency in computing 
the tristimulus values of a set of spectral power distributions, and in the second, we render a simple 
scene. Tristimulus Values of Test Spectra We select as sample spectra the twenty-four squares of the 
Macbeth Color Checker under the .uorescent light. A set of basis functions can be computed by performing 
a characteristic vector analysis on the set of twenty-.ve spectral power distributions given by the light 
itself and the light re.ected from the twenty-four samples. Figure 2 shows the .rst three basis functions 
for this set; as can be seen, characteristic vector analysis preserves the narrow peaks that are found 
in the spectral power distribution of the light source. From these basis functions, we compute the transform 
coef.cients of the .uorescent light with Equation 2. Assuming only diffuse re.ection and ignoring geometry, 
we use Equation 12 to compute a single matrix for each of the twenty-four surfaces in the color checker. 
The product of the vector of coef.cients with each of these matrices gives column vectors containing 
the coef.cients of the re.ected light. From these vectors, we compute the linear model approximation 
to the tristimulus values of each of the twenty-four Figure 2: First three basis functions computed with 
characteristic vector analysis for .uorescent light re.ected from the twenty-four squares of the Macbeth 
Color Checker. 50 40 30 Average CIE Lab Error 20 10 0 10 20 30 405060 Number of Multiplies Figure 3: 
Average CIE Lab Error for set of spectra as a function of the number of multiplies per re.ection for 
evenly spaced point samples and for the general linear transform computed with charac­teristic vector 
analysis. patches with Equations 18-20. The average CIE Lab error in units of [21] can then be calculated 
as a function of the number E of basis functions. For reference, we also compute this error as a function 
of the number of evenly spaced point samples for Riemann summation. To compare the two methods in terms 
of their compu­tational intensiveness, we plot in Figure 3 the errors as a function of the number of 
multiplies per re.ection. The general linear model is signi.cantly more ef.cient than point sampling; 
the latter shows severe oscillations from the sampling er­ror in computing the narrow peaks in the .uorescent 
light. Clearly, the point sampling method should (and would) be amended for the .uorescent light case. 
The most natural method is to ensure point samples lie on the narrow peaks and are weighted appropriately 
dur­ing the integration. This is tantamount to hand-selecting a general linear model. Characteristic 
vector analysis is attractive because it matches most anomalies in the spectra without the user being 
required to address each one distinctly. Spectral Reflectance 1 0.8 0.6 0.4 0.2 0 Figure4: Foursurfacere.ectancesfromtheMacbethColorChecker 
used in the example image. 450 500 550 600 650 700 Wavelength Figure 5: First three basis functions 
of the general linear model computed with characteristic vector analysis for the example image. Image 
Generation We now apply characteristic vector analysis to select basis functions for ray tracing of a 
simple scene under .uorescent light. The four distinct surface re.ectances in the scene are taken from 
the Macbeth Color Checker and are shown in Figure 4. To compute the basis functions, we perform a characteristic 
vector analysis on the set of spectra consisting of the light source itself, all single re.ections, and 
all second interre.ections from the four surface samples; the .rst three basis functions are shown in 
Figure 5. These functions are used to compute the column vector of the light source and the ambient, 
diffuse, and specular re.ectance matrices for each of the surfaces in the scene. Figure 6 shows the resultant 
images for four different numbers of basis functions. The top left image in the .gure displays the full 
resolution rendering of the scene computed at one nanometer intervals. The two columns display the general 
linear model and evenly spaced point sampling for the same number of multiplies per re.ection. The left 
column shows the general model with 2, 3, 4, and 5 basis functions from top to bottom, and the right 
column shows 4, 9, 16, and 25 evenly spaced point samples from top to bottom. The linear model based 
on characteristic vector analysis is superior for all images; with just three basis functions, it is 
virtually identical to the full resolution image.  5 Conclusions We have presented a general description 
of the use of linear trans­form methods in synthetic image generation. This formalism re­quires that 
all spectral power distributions be described with respect to a set of orthonormal basis functions. The 
spectral power dis­tributions are represented by column vectors, and the surfaces are described by matrices. 
Re.ection during the rendering procedure takes the form of matrix multiplication. Because this process 
is lin­ear, it allows for easy implementation. In addition, this framework guides the choice of basis 
functions for ef.cient rendering. We have discussed two possibilities for the selection of the ba­sis 
functions, Riemann summation for ef.cient point sampling and characteristic vector analysis of a representative 
set of spectra in the scene. Point sampling based on Riemann summation is effective when the spectral 
power distributions in a scene are well described with low-order Fourier components. The method based 
on charac­teristic vector analysis is of comparable ef.ciency to point sampling techniques when the scenes 
contain smoothly varying spectra, and it can be signi.cantly more ef.cient for scenes with complex spec­tra. 
We demonstrated this by rendering a scene illuminated by .uorescent light. A promising direction of future 
work is the investigation of basis functions that make the rendering procedure more ef.cient; the techniques 
in [12] are potentially useful to this end. In addition, we have focussed in this paper on minimizing 
the cost of full spectral rendering, but the .exibility of the general method might be useful for other 
issues in computer graphics, such as texturing, that deal with spectral information during rendering. 
 Acknowledgements This material is based upon work supported under a National Sci­ence Foundation Graduate 
Fellowship and partially supported by the National Science Foundation under Grant NSF ECS 88-15815. The 
author would like to thank Lambertus Hesselink, Marc Levoy, and Paul Ning for helpful discussions. He 
especially would like to thank Brian Wandell for helpful discussions and for providing the spectral data 
used in this work.  References [1] Borges, Carlos. Trichromatic Approximation for Computer Graphics 
Illumination Models. Proceedings of SIGGRAPH 91 (Las Vegas, Nevada, July 28-August 2, 1991). In Computer 
Graphics 25,4 (July 1991),101-104. [2] Borges, Carlos. Numerical Methods for Illumination Models in Realistic 
Image Synthesis. PhD dissertation, University of California, Davis, 1990. [3] Cohen, Jozef. Dependency 
of the Spectral Re.ectance Curves of the Munsell Color Chips. Psychon. Sci. 1 (1964), 369-370. [4] Cowan, 
William. An Inexpensive Scheme for Calibration of a Color Monitor in Terms of CIE Standard Coordinates. 
Pro­ceedings of SIGGRAPH 83 (Detroit, Michigan, July 25-29, 1983). In Computer Graphics 17,3 (July 1983), 
315-321.  Figure 6: Comparison of general linear model with evenly spaced point sampling. The top left 
image is a full resolution image computed at one nanometer steps; the left image in each row is the general 
model with 2, 3, 4, and 5 basis functions from top to bottom; the right image is evenly spaced point 
sampling with 4, 9, 16, and 25 samples. [5] Davis, P. and Rabinowitz, P. Methods of Numerical Integra­tion. 
Academic Press, New York, 1975. [6] Hall, Roy. Illumination and Color in Computer Generated Imagery. 
Springer-Verlag, New York, 1989. [7] Hall, Roy and Greenberg, Donald. A Testbed for Realistic Image Synthesis. 
IEEE Computer Graphics and Applications 3 (1983), 10-20. [8] Judd, Deane, MacAdam, David, and Wyszecki, 
Gunter. Spec­tral Distribution of Typical Daylight as a Function of Corre­lated Color Temperature. J. 
Opt. Soc. Am. 54,8 (1964), 1031­1040. [9] Maloney, Laurence. Evaluation of linear models of surface spectral 
re.ectance with small numbers of parameters. J. Opt. Soc. Am. A 3,10 (1986), 1673-1683. [10] Maloney, 
Laurence. Computational Approachesto Color Con­stancy. PhD dissertation, Stanford University, 1985. [11] 
Mardia, K., Kent, J., and Bibby, J. Multivariate Analysis. Academic, London, 1979. [12] Marimont, David 
and Wandell, Brian. Linear models of sur­face and illuminant spectra. J. Opt. Soc. Am. A 9,11 (1992), 
1905-1913. [13] McCamy, C., Marcus, H., and Davidson, J. A Color Rendition Chart. J. Appl. Photographic 
Engrg. 11,3 (1976), 95-99. [14] Meyer, Gary. Wavelength Selection for Synthetic Image Gen­eration. Computer 
Vision, Graphics, and Image Processing 41 (1988), 57-79. [15] Parkkinen, J., Hallikainen, J., and Jaaskelainen, 
T. Character­istic Spectra of Munsell Colors. J. Opt. Soc. Am. A 6,2 (1989), 318-322. [16] Raso, Maria, 
and Fournier, Alain. A Piecewise Polynomial Approach to Shading Using Spectral Distributions. Proceed­ings 
of Graphics Interface 91. (Calgary, Alberta, June 3-7, 1991), 40-46. [17] Smith, Brent, Spiekermann, 
Charles, and Sember, Robert. Nu­merical Methods for Colorimetric Calculations: A Compari­son of Integration 
Methods. COLOR Research and Application 17,6 (1992), 384-393. [18] Smith, Brent, Spiekermann, Charles, 
and Sember, Robert. Numerical Methods for Colorimetric Calculations: Sampling Density Requirements. COLOR 
Research and Application 17,6 (1992), 394-401. [19] Wallis, Robert. Fast computation of tristimulus values 
by use of Gaussian quadrature. J. Opt. Soc. Am. 65,1 (1975), 91-94. [20] Wandell, Brian. The Synthesis 
and Analysis of Color Images. IEEE Trans. on Pattern Analysis and Machine Intelligence, PAMI-9,1 (1987), 
2-13. [21] Wyszecki, Gunter and Stiles, W.S. Color Science: Concepts and Methods, Quantitative Data and 
Formulae. John Wiley and Sons, 1982.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166143</article_id>
		<sort_key>199</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>26</seq_no>
		<title><![CDATA[Combining hierarchical radiosity and discontinuity meshing]]></title>
		<page_from>199</page_from>
		<page_to>208</page_to>
		<doi_number>10.1145/166117.166143</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166143</url>
		<keywords>
			<kw><![CDATA[Mach bands]]></kw>
			<kw><![CDATA[diffuse reflector]]></kw>
			<kw><![CDATA[discontinuity meshing]]></kw>
			<kw><![CDATA[global illumination]]></kw>
			<kw><![CDATA[hierarchical radiosity]]></kw>
			<kw><![CDATA[photorealism]]></kw>
			<kw><![CDATA[quadratic interpolation]]></kw>
			<kw><![CDATA[radiance function]]></kw>
			<kw><![CDATA[radiosity]]></kw>
			<kw><![CDATA[reconstruction]]></kw>
			<kw><![CDATA[shadows]]></kw>
			<kw><![CDATA[view-independence]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14153841</person_id>
				<author_profile_id><![CDATA[81311486606]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dani]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lischinski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P84396</person_id>
				<author_profile_id><![CDATA[81100016947]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Filippo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tampieri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P68459</person_id>
				<author_profile_id><![CDATA[81100196982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Donald]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Greenberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>122724</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baum, Daniel R., Stephen Mann, Kevin E Smith, and James M. Winget. "Making Radiosity Usable: Automatic Preprocessing and Meshing Techniques for the Generation of Accurate Radiosity Solutions," Computer Graphics, 25(4), July 1991, pages 51-60.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bern, Marshall and David Eppstein. "Mesh Generation and Optimal Triangulation," in Hwang, F.K. and D.-Z. Du, editors, Computing in Euclidian Geometry, World Scientific, 1992.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>144175</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Campbell, III, A. T. Modeling Global Diffuse Illumination for Image Synthesis, PhD dissertation, U. of Texas at Austin, Texas, December 1991.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122737</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Chen, Shenchang Eric, Holly E. Rushmeier, Gavin Miller, and Douglass Turner. "A Progressive Multi-Pass Method for Global Illumination," Computer Graphics, 25(4), July 1991, pages 165-174.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Chew, L. Paul. "Constrained Delaunay Triangulations,"Algorithmica, 4, 1989, pages 97-108.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147159</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Chin, Norman and Steven Feiner. "Fast Object-Precision Shadow Generation for Area Light Sources Using BSP Trees," in Proceedings of 1992 Symposium on Interactive 3D Graphics, March 1992.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325171</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Cohen, Michael F. and Donald E Greenberg. "The Hemi-Cube: A Radiosity Solution for Complex Environments," Computer Graphics, 19(3), July 1985, pages 31-40.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Cohen, Michael F., Donald E Greenberg, and David S. Immel. "An Efficient Radiosity Approach for Realistic Image Synthesis," IEEE Computer Graphics and Applications, 6(2), March 1986, pages 26-35.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>80992</ref_obj_id>
				<ref_obj_pid>80983</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Gigus, Ziv and Jitendra Malik. "Computing the Aspect Graph for Line Drawings of Polyhedral Objects," IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(2), February 1990, pages 113-122.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808601</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Goral, Cindy M., Kenneth E. Torrance, Donald E Greenberg, and Bennett Battaile. "Modeling the Interaction of Light Between Diffuse Surfaces," Computer Graphics, 18(3), July 1984, pages 213-222.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>282923</ref_obj_id>
				<ref_obj_pid>282918</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Guibas, Leonidas and Jorge Stolfi. "Primitives for the Manipulation of General Subdivisions and the Computation of Voronoi Diagrams," ACM Transactions on Graphics, 4(2), April 1985, pages 74-123.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Haines, Eric A. "Ronchamp: A Case Study for Radiosity," SIG- GRAPH' 91 Frontiers in Rendering Course Notes, July 1991.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Haines, Eric A. and John R. Wallace. "Shaft Culling for Efficient Ray- Traced Radiosity," in Proceedings of the Second Eurographics Workshop on Rendering, May 1991.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122740</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Hanrahan, Pat, David Salzman, and Larry Aupperle. "A Rapid Hierarchical Radiosity Algorithm," Computer Graphics, 25(4), July 1991, pages 197-206.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Heckbert, Paul S. "Discontinuity Meshing for Radiosity," in Proceedings of the Third Eurographics Workshop on Rendering, May 1992, pages 203-216.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>144727</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Heckbert, Paul S. Simulating Global Illumination Using Adaptive Meshing, PhD dissertation, UC Berkeley, California, June 1991.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Kok, Arjan J. F. and Frederik Jansen. "Source Selection for the Direct Lighting Computation in Global Illumination," in Proceedings of the Second Eurographics Workshop on Rendering, May 1991.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>142453</ref_obj_id>
				<ref_obj_pid>142443</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Lischinski, Dani, Filippo Tampieri, and Donald E Greenberg. "Discontinuity Meshing for Accurate Radiosity,"IEEE Computer Graphics and Applications, 12(6), November 1992, pages 25-39.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325169</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Nishita, Tomoyuki and Eihachiro Nakamae. "Continuous Tone Representation of Three-Dimensional Objects Taking Account of Shadows and Interreflections," Computer Graphics, 19(3), July 1985, pages 23- 30.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Reichert, Mark C. A Two-Pass Radiosity Method Driven by Lights and Viewer Position, Master's thesis, Cornell University, Ithaca, New York, January 1992.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Salesin, David, Dani Lischinski, and Tony DeRose. "Reconstructing Illumination Functions with Selected Discontinuities," in Proceedings of the Third Eurographics Workshop on Rendering, May 1992, pages 99-112.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134080</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Smits, Brian E., James R. Arvo, and David H. Salesin. "An Importance-Driven Radiosity Algorithm," Computer Graphics, 26(4), July 1992, pages 273-282.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Sparrow, Ephraim M. "On the Calculation of Radiant Interchange between Surfaces," in Ibele, Warren E., editor, Modern Developements in Heat Transfer, Academic Press, New York, 1963.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>193685</ref_obj_id>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Tampieri, Filippo. Discontinuity Meshing for Radiosity Image Synthesis, PhD dissertation, Cornell University, Ithaca, New York, May 1993.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134029</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Teller, Seth J. "Computing the Antipenumbra of an Area Light Source," Computer Graphics, 26(4), July 1992, pages 139-148.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Teller, Seth and Pat Hanrahan. "Global Visibility Algorithms for Illumination Computations," Computer Graphics, 27(4), August 1993.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Zienkiewicz, O. C. and R. L. Taylor. The Finite Element Method, pages 128-132, Vol. 1, McGraw-Hill, London, 4th edition, 1989.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Combining Hierarchical Radiosity and Discontinuity Meshing Dani Lischinski Filippo Tampieri Donald P. 
Greenberg Program of Computer Graphics Cornell University Ithaca, NY 14853 ABSTRACT We introduce a new 
approach for the computation of view­independent solutions to the diffuse global illumination problem 
in polyhedral environments. The approach combines ideas from hier­archical radiosity and discontinuity 
meshing to yield solutions that are accurate both numerically and visually. First, we describe a modi.ed 
hierarchical radiosity algorithm that uses a discontinuity­driven subdivision strategy to achieve better 
numerical accuracy and faster convergence. Second, we present a new algorithm based on discontinuity 
meshing that uses the hierarchical solution to recon­struct an object-space approximation to the radiance 
function that is visually accurate. Our results show signi.cant improvements over both hierarchical radiosity 
and discontinuity meshing algorithms. CR Categories and Subject Descriptors: I.3.3 [Computer Graphics]: 
Picture/Image Generation; I.3.7 [Computer Graph­ics]: Three-Dimensional Graphics and Realism. Additional 
Key Words and Phrases: diffuse re.ector, discon­tinuity meshing, global illumination, hierarchical radiosity, 
Mach bands, photorealism, quadratic interpolation, radiance function, ra­diosity, reconstruction, shadows, 
view-independence. 1 INTRODUCTION Computing solutions to the global illumination problem is an essen­tial 
part of photorealistic image synthesis. In this paper, we are in­terested in computing view-independent 
(or object-space) solutions for global illumination. Such solutions provide an approximation to the radiance 
function across each surface in the environment. Once a solution is computed, images from any viewpoint 
can be rendered with a relatively small additional effort. These methods are particu­larly attractive 
for applications such as architectural design, interior design, lighting design, illumination engineering, 
and virtual real­ity, in which the need for multiple views or walk-throughs of static environments arises. 
So far, most view-independent methods have been derived from the radiosity method that was originally 
developed to solve radia­tive heat transfer problems [23]. Computer graphics researchers adopted this 
method to compute the global illumination of diffuse Permission to copy without fee all or part of this 
material is granted Permission to copy without fee all or part of this material is granted provided 
that the copies are not made or distributed for direct provided that the copies are not made or distributed 
for direct commercial advantage, the ACM copyright notice and the title of the commercial advantage, 
the ACM copyright notice and the title of the publication and its date appear, and notice is given that 
copying is by publication and its date appear, and notice is given that copying is by permission of 
the Association for Computing Machinery. To copy permission of the Association for Computing Machinery. 
To copy otherwise, or to republish, requires a fee and/or specific permission. otherwise, or to republish, 
requires a fee and/or specific permission. &#38;#169;1993 ACM-0-89791-601-8/93/008 $1.50 &#38;#169;1993 
ACM-0-89791-601-8/93/008/0015 $1.50 polyhedral environments [10, 7, 19]. Radiosity has been extended 
and improved dramatically since, but there is still much to be done before the method can become a useful 
tool for its intended users. The goal of our research is to develop an ef.cient radiosity system that 
satis.es the following requirements: Objective (numerical) accuracy: Solutions produced by the sys­tem 
should converge rapidly to the exact solution. This requirement may seem obvious, however, in the computer 
graphics community results of simulations are too often judged solely by their visual ap­pearance. Subjective 
(visual) accuracy: While visual appearance should not be used to judge the objective accuracy of the 
simulation, it is still very important, since the image is the .nal product. Clearly, ac­curate visual 
appearance can be achieved through numerically ac­curate simulation (if the underlying model is physically 
accurate.) Unfortunately, experience has shown that the human visual system is extremely sensitive to 
small perceptual errors that are dif.cult to quantify. The simulated environments can be very complex 
and, therefore, the computation of ultra-accurate solutions is generally impractical. Thus, we must have 
means of producing visually ac­ceptable images even from coarse solutions. Ease of control: (i) The system 
should be controllable by users who are not necessarily familiar with its inner workings. Therefore, 
the control parameters should be intuitive and small in number. (ii) In many cases (such as early design 
stages) the user is interested in a quick solution, even if not exceedingly accurate. At other times, 
one might be willing to wait overnight for a reliable solution. Therefore, the system should provide 
the user with the option to trade speed for accuracy. Most radiosity systems do not satisfy any of these 
requirements. There are no error bounds on the solutions, because approximations are often used without 
justi.cations regarding their impact on the accuracy of the results. The resulting images typically exhibit 
many visual artifacts such as Mach bands, light and shadow leaks, jagged shadow boundaries, and missing 
shadows. Radiosity systems are seldom user-friendly and require massive user intervention: typi­cally, 
a time consuming trial-and-error process is required to pro­duce an image that looks right. Baum et al. 
[1] and Haines [12] provide good discussions of the various pitfalls of radiosity. In this paper we present 
a new radiosity method, which comes closer to satisfying our goals. The new method combines two re­cently 
developed approaches: hierarchical radiosity [14] and dis­continuity meshing [15, 18]. First, we present 
an improved hierar­chical radiosity algorithm that uses a discontinuity-driven subdivi­sion strategy 
to achieve better numerical accuracy and faster conver­gence. Second, we describe a new algorithm based 
on discontinuity meshing that uses the hierarchical solution to reconstruct a visually accurate approximation 
to the radiance function. Thus, results of high visual quality can be obtained even from coarse global 
illumi­nation simulations. Previous attempts to improve the visual quality of radiosity solutions were 
described by Nishita and Nakamae [19], Kok and Jansen [17], Chen et al. [4], and Reichert [20]. In all 
of these cases, however, the improvement takes place in image space, after the view and the resolution 
have been speci.ed. Our method, instead, operates entirely in object space, and the improved solution 
is view-independent.  2 HIERARCHICAL RADIOSITY The traditional radiosity approach [10, 7] discretizes 
the environ­ment into n elements and solves a linear system of n equations, where the radiosities of 
the elements are the unknowns. The most serious drawback of this approach is the need to compute the 
O(n2) coef.cients of the linear system, corresponding to the interactions (transfers of light energy) 
between pairs of elements. In addition to the overwhelming computational complexity, most of these compu­tations 
are performed to unnecessarily high accuracy, while some are not suf.ciently accurate. Hierarchical radiosity 
(HR) [14] overcomes these problems by decomposing the matrix of interactions into O(n) blocks, for a 
given accuracy. These blocks correspond to interactions of roughly equal magnitude, and the same computational 
effort is required for com­puting each block. HR operates by constructing a hierarchical sub­division 
of each input surface. Each node in the hierarchy repre­sents some area on the surface. Two nodes are 
linked together if the interaction between their corresponding areas can be computed within the required 
accuracy; otherwise, the algorithm attempts to link their children with each other. Each link corresponds 
to a block in the interaction matrix. HR has several important advantages: it is fast, the errors in 
its approximations are bounded, and it is controlled by only two param­eters: the error tolerance and 
the minimum node area. The smaller the values of these parameters, the more accurate (and expensive) 
the solution becomes. Thus, HR satis.es our goals of objective ac­curacy and ease of control. However, 
the HR algorithm still suffers from shadow leaks and jagged shadow boundaries. This occurs because surfaces 
are sub­divided regularly, not taking into account the geometry of the shad­ows. HR uses point sampling 
to classify the inter-visibility between two surfaces, so it is prone to missing small shadows altogether. 
Of course, as the user-speci.ed tolerance becomes smaller, the solution becomes more accurate, and the 
visual artifacts decrease. Never­theless, images of high visual quality can require solutions of pro­hibitively 
high accuracy. The number of links created by HR is O(n + m2) where n is the .nal number of nodes and 
m is the number of input surfaces. As the complexity of the environment increases, the m2 term eventu­ally 
becomes dominant, drastically reducing the ef.ciency of the algorithm. As pointed out by Smits et al. 
[22], this problem could be solved by grouping the input surfaces into higher level clusters. This is 
an interesting research topic by itself, and it will not be pur­sued in this paper.  3 DISCONTINUITY 
MESHING Radiosity methods typically attempt to approximate the radiance function with constant elements 
and use linear interpolation to dis­play the result. The actual radiance function, however, is neither 
piecewise constant nor piecewise linear. It is usually smooth, ex­cept along certain curves across which 
discontinuities in value or in derivatives of various order may occur. Discontinuities in radiance functions 
are discussed in detail elsewhere [16, 15, 18]; what fol­lows is a brief summary of the various types 
of discontinuity and their causes. The most signi.cant discontinuities are discontinuities in the ra­diance 
function itself (denoted D0). They occur along curves of con­tact or intersection between surfaces. Discontinuities 
in the .rst and the second derivatives (D1 and D2, respectively) occur along curves of intersection between 
surfaces in the environment and critical sur­faces corresponding to qualitative changes in visibility, 
or visual events. Visual events in polyhedral environments can be classi.ed into two types [9]: EV events 
de.ned by the interaction of an edge and a vertex, where the critical surface is a planar wedge; and 
EEE events de.ned by the interaction of three edges, where the critical surface is a part of a quadric. 
Discontinuities of higher than second order are also possible [16]. Discontinuities are very important 
both numerically and visually: all the boundaries separating unoccluded, penumbra, and umbra re­gions 
correspond to various discontinuities. When a discontinuity curve crosses a mesh element, the approximation 
to the radiance function over that element becomes less accurate. The resulting errors usually correspond 
to the most visually distracting artifacts in radiosity images. The traditional radiosity approach uses 
adap­tive subdivision [8] to reduce these errors, however there are several problems with this approach. 
First, the user must specify an initial mesh that is suf.ciently dense, or features will be lost. Second, 
the shape of the mesh is determined by the geometry of the surface be­ing meshed, and the discontinuities 
are not resolved exactly. As a result, many small elements are created as the method attempts to converge 
to shadow boundaries. Furthermore, although the result­ing solution may be of adequate visual quality 
for some views, arti­facts may become visible as the view changes (e.g., when we zoom in on a surface.) 
Discontinuity meshing (DM) algorithms compute the location of certain discontinuities and represent them 
explicitly, as bound­aries, in the mesh. This leads to solutions which are both numeri­cally and visually 
more accurate. Another advantage is that higher order elements can be used much more effectively in conjunction 
with discontinuity meshes [16]. Several algorithms have been de­scribed that use the idea of discontinuity 
meshing to various extents [1, 3, 6, 15]. Recently, a progressive radiosity DM algorithm was described 
by the authors [18]. The meshing in this algorithm is automatic. Using analytical visibility and form 
factor computations followed by quadratic interpolation it has produced radiosity solutions of im­pressive 
visual accuracy. This algorithm was also shown to be nu­merically accurate [24]. However, this method 
is too expensive for computing converged solutions of complex environment and only offers limited user 
con­trol in trading off speed for accuracy. The main reason for this is that all energy transfers are 
computed very accurately, regardless of their magnitude. 4 A COMBINED APPROACH Hierarchical radiosity 
and discontinuity meshing seem to comple­ment each other in their strengths and weaknesses: HR is fast, 
but the visual appearance of the results can be disappointing; DM, on the other hand, has produced visually 
accurate results, but so far it has been too expensive for simulation of complex environments. This observation 
motivated us to look for ways of merging the two methods. Our investigation resulted in the following 
two-pass ap­proach: The global pass uses a modi.ed HR algorithm to compute a ra­diosity solution within 
a prespeci.ed tolerance. Instead of regular quadtree subdivision, the modi.ed algorithm subdivides surfaces 
along discontinuity segments. This improves the numerical accu­racy and results in faster convergence. 
 initial linking discont. location Figure 1: The structure of the new radiosity system The local pass 
uses DM and quadratic interpolation to re.ne the approximation to the radiance function locally on each 
surface in the environment. Thus, the solution computed by the global pass is transformed into a more 
visually accurate form. When the computation is arranged in this way the simulation be­comes more ef.cient. 
The global pass need not be concerned with visual accuracy. This eliminates the need to maintain a topolog­ically 
connected mesh, to prevent T-vertices, or to use extremely .ne subdivision around shadow boundaries, 
since this has little ef­fect on the global distribution of light in the environment. The local pass, 
on the other hand, can create as many elements as necessary for a high quality reconstruction of the 
radiance function, without overburdening the global illumination simulation. As a result, it is possible 
to produce images of high visual accuracy even from quick simulations. To test our approach we have implemented 
a new radiosity sys­tem whose overall structure is shown in Figure 1. The global and the local passes 
are discussed in detail in the next two sections. In the rest of this section we brie.y describe the 
remaining parts. The initial linking stage creates for each input polygon a list of links to all the 
polygons that are visible from it. For each link it is determined whether the two polygons are completely 
or partially visible to each other. This creates a starting point for the global pass, which proceeds 
to re.ne these links as needed. We test visibility between two polygons using a combination of shaft-culling 
[13] and the ray-tracing algorithm that Hanrahan et al. [14] used. The discontinuity location stage computes 
the location of all the D0 discontinuities, since these are typically responsible for the most severe 
errors (both numerically and visually.) In most environments the direct illumination by primary light 
sources is responsible for the most perceptible illumination details. Therefore, all of the D1 and D2 
discontinuities caused by EV events involving the primary light sources are computed as well. The computed 
discontinuities are henceforth collectively referred to as primary discontinuities. EEE events are more 
dif.cult to handle because their correspond­ing critical surfaces are curved, rather than planar. However, 
the resulting discontinuities always lie within penumbra regions, and never de.ne the outer boundaries 
of a shadow. For these reasons, we excluded EEE events from our current implementation. We described 
the discontinuity location algorithm in a previous paper [18]. Tampieri [24] provides a more detailed 
description of this algorithm. Heckbert [15] and Teller [25] describe alternative algorithms for locating 
discontinuities. Teller s algorithm is the only one capable of handling EEE events.  5 THE GLOBAL PASS 
In order to understand how the accuracy of HR can be improved, we must examine its sources of error. 
Consider two nodes s and r linked together by the HR algorithm. Let Brs(x) denote the actual radiosity 
due to node s at point x on node r. The algorithm approximates this radiosity by a constant function 
Brs(x) B rs = prBsFrsVrs where pr is the re.ectivity of node r; Bs is the average radiosity of node s; 
Frs is the form factor from r to s; and Vrs is the inter-visibility factor between r and s (the visible 
fraction of the area of s, averaged over r). We are interested in bounding the error between the computed 
and the actual radiosities Ers = sup Brs(x) 0 B rs(1) xEr To that end, we de.ne the following upper 
and lower bounds: Bmin Bmax s = infxEs Bs(x) s = supxEs Bs(x) Fmin Fmax rs = infxEr Fxs = supFxs rs xEr 
Vmin Vmax rs = infxEr Vxs rs = supVxs xEr where Bs(x) is the radiosity at point x on s; Fxs is the form 
factor from point x to s; and Vxs is the fraction of the area of s visible from x. Clearly, both Brs(x) 
and B rs lie in the interval prBminFminVmin ,prBmaxFmaxVmax s rs rs s rs rs Therefore, the error Ers 
is bounded by the width of the interval () BmaxFmaxVmax BminFminVmin Ers < pr rs 0 rs (2) s rs s rs Three 
main factors affect the magnitude of the error: 1. the variation of the radiosity on the source node 
s 2. the variation of the form factor across the receiver node r 3. the variation in the visibility 
of the source from the receiver  Therefore, if we .nd the potential error in the transfer of light energy 
from s to r too large, we can try to reduce the error by reducing any of these factors. For instance, 
subdividing the receiving node will reduce the variation of the form factor. Subdividing the source will 
reduce the variation of the radiosity on the source. Subdividing either of the two may reduce the variation 
in the visibility. Unfortunately, errors due to visibility are more dif.cult to han­dle than errors 
of the other two types. If the two nodes are com­pletely visible to each other, the error usually decreases 
rapidly as the nodes are subdivided. When the two nodes are completely oc­cluded from each other no light 
energy transfer occurs, and the error is zero. Partial visibility, on the other hand, often results in 
very .ne subdivisions, primarily because of loose bounds on the variation in visibility between two .nite 
areas. In HR, visibility is estimated by casting a number of rays between the two nodes. Thus, if partial 
visibility is detected, all we know is that the actual visibility is in the interval (0,1). Clearly, 
it would be to our advantage to use a subdivision strategy that would result in as many totally visible 
or totally occluded pairs, as quickly as possible. Since discontinuity lines on the receiver cor­respond 
to abrupt changes in the visibility of the sources [16, 18], subdividing the receiver along these lines 
should quickly resolve partial occlusion. We have modi.ed the HR algorithm to perform discontinuity­driven 
subdivision instead of regular subdivision. There are two main changes in the data structures used by 
the new algorithm: .rst, we store with each node a list of all the discontinuity segments on the corresponding 
polygon; second, we use a 2D binary space partition­ing (BSP) tree [3] instead of a quadtree to represent 
the hierarchical subdivision of each initial polygon, since BSP trees allow for subdi­vision of polygons 
along arbitrarily oriented lines. Pseudocode for subdividing a node is given in Figure 2. When a node 
is subdivided we choose one of its discontinuity segments and split the node using the corresponding 
line equation. The segment is chosen such that the split is as balanced as possible. Boolean Subdivide(node) 
if not IsLeaf(node) then 0.1 return TRUE end if if node.area .minNodeArea then return FALSE end if if 
node.DSegments . = NIL then 0.01 DSegment s + ChooseBestSegment(node) (left,right) + SplitNode(node,s) 
(leftList,rightList) + SplitSegmentList(node,s) Number of Elements else (left,right) + SplitEqual(node) 
(leftList,rightList) + (NIL,NIL) end if node.left + CreateNode(left,leftList) node.right + CreateNode(right,rightList) 
return TRUE Figure 2: Pseudocode for the Subdivide routine Priority is given to D0 discontinuities over 
higher order ones, since the former typically bound areas totally occluded from the rest of the environment. 
The subdivision is completed by splitting the list of segments into two new lists, one for each child. 
If no segments are stored with the node, we split the node by connecting the midpoint of the longest 
edge to a vertex or another midpoint chosen so that the resulting children have roughly equal areas. 
5.1 Results Figure 4 demonstrates the improved hierarchical algorithm using a simple environment illuminated 
by two small triangular light sources. A 3D view of the environment is shown in image a1. The radiance 
function on the .oor polygon is shown in image a2. Im­age a3 shows the discontinuity segments on the 
.oor. D0 discontinu­ities are drawn in red; D1 and D2 discontinuities in yellow. In rows b and c, we 
compare the subdivision produced by the discontinuity­driven algorithm to the one produced by regular 
subdivision. The level of subdivision shown increases from left to right: the leftmost pair shows the 
subdivision at level 2, then level 4, 6, and 8. The new algorithm is much quicker to correctly separate 
regions corresponding to complete occlusion, partial visibility, and com­plete visibility. Already at 
subdivision level 4 (image b2), most of the nodes can be classi.ed as either totally visible or totally 
oc­cluded with respect to each of the light sources. For these areas there are no more visibility errors. 
At subdivision level 6 (image b3) all of the discontinuities have been used, and the partially visible 
nodes are now con.ned exactly to the areas of penumbra. In order to compare the rates of convergence 
of the two strategies we computed a set of approximations to the direct illumination on the .oor using 
a successively larger number of elements. Figure 3 shows the RMS and the maximum absolute errors versus 
the num­ber of elements for the two strategies. These errors were computed with respect to an analytical 
solution at the vertices of a 400 by 400 grid on the .oor. All the values were scaled to set the maximum 
brightness on the .oor to 1. Our algorithm converges faster in both error metrics. Note that the convergence 
of the regular subdivision is particularly poor in the maximum absolute error metric. The reason is that 
there are D0 segments on the .oor that are not aligned with the subdivision axes. Thus, there are always 
elements that are partially covered by the pyramid while the remaining part is brightly illuminated by 
the Figure 3: A comparison of errors between the two subdivision strategies using log-log plots light 
sources. The algorithm assigns a single constant value to each such element, and this results in a large 
error there. Our algorithm, on the other hand, resolves D0 discontinuities and therefore does not suffer 
from this problem. In the RMS error metric regular subdivision does converge, be­cause the elements that 
contain the errors become progressively smaller, and this is accounted for by the metric; however, the 
con­vergence is slower.  6 THE LOCAL PASS The global pass results in a hierarchical solution that is 
essentially a piecewise constant approximation to the radiance function on each polygon in the environment. 
Often, this approximation is quite coarse. Now our goal is to convert this solution into a form more 
suited for producing visually accurate images. To that end, we need to locally re.ne the radiance approximation 
on each polygon. Our experience with discontinuity meshing [18] has shown that reproducing the discontinuities 
in the radiance function, while main­taining a smooth approximation elsewhere is key to achieving visual 
accuracy, especially when multiple views of the same solution are to be rendered. Therefore, we construct 
a discontinuity mesh con­taining the precomputed primary discontinuities for each polygon. Mesh nodes 
are assigned radiance values using the hierarchical so­lution. This mesh is then used for the shaded 
display of the en­vironment. Thus, the local pass essentially performs an additional light gathering 
operation over the environment. However, instead of gathering to the nodes in the hierarchy, we gather 
to the elements of the discontinuity mesh. The discontinuity mesh is constructed using constrained Delau­nay 
triangulation (CDT) [5]. The Delaunay triangulation (DT) of a point set maximizes the minimum angle over 
all possible triangula­ a b c  Figure 4: Discontinuity-driven vs. regular subdivision tions of that 
set and has a number of other desirable properties [2]. These properties are important because they result 
in well-shaped elements that yield more accurate approximations and reduce vi­sual artifacts during display 
[1]. CDT takes as input a point set and a set of edges connecting some of the points, and creates a triangu­lation 
of the points that is constrained to include all the input edges. CDT preserves the properties of DT 
over all the constrained triangu­lations. We have implemented an incremental CDT algorithm that is a 
simple extension of the incremental DT algorithm described by Guibas and Stol. [11]. An alternative easy-to-implement 
algorithm is described in the excellent survey by Bern and Eppstein [2]. For each input polygon we provide 
the CDT routine with all of its boundary edges and discontinuity segments. The corners of all the leaf 
nodes in the corresponding hierarchy are given as well. Thus, the resulting mesh is dense enough to adequately 
sample the solu­tion computed by the global pass. As a result of the properties of the CDT, most of the 
triangles are well shaped unless the hierarchy is very coarse. The radiance across each triangle is approximated 
using a stan­dard quadratic element commonly used in .nite element meth­ods [27]. Six radiance values 
are computed for each element: three at the vertices, and three at the edge midpoints. Except for D0 
edges, these values are shared between adjacent faces (our CDT algorithm constructs a topological data 
structure suitable for such information sharing [11].) The six values are then interpolated by a quadratic 
bivariate polynomial. This scheme yields a C0 piecewise quadratic interpolant to the radiance on each 
polygon. This interpolant was found to provide approximations that look smoother and are less prone to 
Mach bands than the traditional piecewise linear interpola­tion [18]. Salesin et al. [21] describe a 
piecewise cubic interpolant that can be used instead, if C1 interpolation is desired. To obtain a radiance 
value at a point x we use the information available to us from the hierarchical solution. Below we describe 
four different methods that we have experimented with. Pseudocode for the last three methods is given 
in Figure 5. Method A. The simplest approach is to use the radiance value stored in the hierarchy leaf 
that contains x. If x is on the boundary between two or more leaves, their values are averaged to yield 
the radiance at x. This method has no overhead other than locating the containing leaves. The accuracy 
of the resulting value depends on the accuracy of the global pass solution. Consider the path from the 
root of the hier­archy to the leaf containing the point x. Every node along this pass has zero or more 
links to other nodes, representing areas on primary or secondary sources that illuminate x. The error 
at x due to one such link between a containing node r and an illuminating node s is bounded by equation 
(2). The total error at x is the sum of the errors over all the contributing links. Method B. Each contributing 
link stores the unoccluded form factor from the center of its node to the corresponding source, as well 
as the visibility factor. To obtain a more accurate radiance value for x we can recompute the unoccluded 
form factor to each source at point x. Each form factor is multiplied by the visibility stored with the 
link and by the radiosity of the source. This results in a smaller bound on the error due to a link between 
r and s () BmaxVmax BminVmin Ers(x) < prFxs s rs 0 s rs (3) Method C. The next logical step is to recompute 
both the form fac­tor and the visibility of each source as seen from x. In order to obtain an accurate 
visibility value the visible parts of the source are com­puted analytically [18]. As a result, the error 
bound shrinks further: Bmax Bmin Ers(x) < prFxsVxs (s 0 s )(4) However, the computation becomes more 
expensive. Method D. To reduce the cost, we can recompute the visibility for links to primary light sources 
only. This is justi.ed by the fact that primary sources are typically responsible for the most noticeable 
shadows. Moreover, these are precisely the sources for which dis­continuities have been computed and 
inserted into the mesh. Thus, we obtain the same accuracy as in method C for links to primary sources, 
while the error due to other links remains the same as in method B. Spectrum Shade(node,x) rad + 0 foreach 
l E node.links do ff + FormFactor(x,l.source) v + Visibility(x,l) rad + rad + ff 3 v 3 l.source.radiosity 
end for if IsInterior(node) then if Contains(node.left,x) then rad + rad + Shade(node.left,x) else if 
Contains(node.right,x) then rad + rad + Shade(node.right,x) else rad + rad +0.5 3 (Shade(node.left,x) 
+Shade(node.right,x)) end if end if return rad Real Visibility(x,link) case ShadingMethod in B:v + link.visibility 
c:v + RecomputeVisibility(x,link.source) D:if IsPrimary(link.source) then v + RecomputeVisibility(x,link.source) 
 else v + link.visibility end if end case return v Figure 5: Pseudocode for the Shade routine 6.1 Results 
We compared methods A, B, C, and D using a simple model of a square exhibit room displaying a modern 
sculpture illuminated by two small square light sources. Three global pass solutions of the exhibit room 
are shown at the top row of Figure 6, in order of increasing accuracy starting from the left. For each 
solution, the elements (leaf nodes) of the hierar­chical subdivision are shown as .at shaded, outlined 
polygons. The bottom row of the same .gure shows the corresponding local pass meshes. Table 1 reports 
statistics for both passes. The results of the global pass were fed to the local pass four times, once 
for each of the methods A, B, C, and D, yielding a total of twelve radiosity solutions shown in Figure 
7. Columns 1, 2, and 3 were computed respectively from the low, medium, and high accu­racy global pass 
solutions shown in Figure 6. Each row corresponds to a different shading strategy starting with method 
A for the top row. As demonstrated in the top row, method A is prone to visual ar­tifacts: the shading 
on walls is .at or not suf.ciently smooth; some shadows are entirely missing (image A1), while others 
have incor­rect boundaries. These artifacts are the result of interpolating ra­diance values obtained 
by sampling the piecewise constant global pass solution. Method B reduces some of these artifacts. The 
appearance of unoccluded areas is greatly improved, since accurate form factor are recomputed at every 
interpolated point in the mesh. However, the penumbra regions of the shadows cast by the sculpture are 
still Table 1: Statistics for images in Figures 6 and 7. Timings are in seconds for execution on an HP 
9000/720 workstation. Solution Accuracy low medium high input polygons 47 47 47 disc. segments 559 559 
559 initial links 652 652 652 total links 720 1316 21805 total nodes 147 803 6041 total leaf nodes 97 
425 3044 CDT elements 1538 2384 8177 shading calls 3799 5674 17984 initial linking 6 6 6 discontinuity 
comp. 1 1 1 hierarchical sol. 1 4 60 triangulation 0.53 0.81 2.78 method A 1 2 13 method B 6 10 80 method 
C 405 624 2633 method D 20 26 110 incorrect and shadows are still missing from the coarse solution (im­age 
B1.) The reason is that method B still uses node-to-node visi­bility factors to approximate node-to-point 
visibility. As shown in row C, method C correctly reconstructs all of the shadows. In particular, note 
the appearance of the shadows in the coarse solution (image C1.) This method results in the best visual 
accuracy we were able to obtain, given a global solution. Method D yields results that are almost indistinguishable 
from those given by method C. However, as can be seen from the tim­ings reported in Table 1, method D 
takes only a fraction of the time required by method C. In fact, it is not much more expensive than method 
B. When using methods C or D, little difference can be seen be­tween the medium and high accuracy solutions 
(columns 2 and 3). Although the latter solution is objectively more accurate, from a vi­sual standpoint, 
the former solution is almost as good. If fact, it is apparent that even very low accuracy global pass 
solutions can yield results of reasonable visual quality when followed by a local pass using method D 
(image D1.) When comparing the computation times reported in Table 1, it can be seen that the local pass 
is in most cases costlier than the global pass. It may be argued that the time used by the local pass 
could be better spent in further re.nement of the subdivision hierarchy in the global pass. One might 
expect that if the hierarchy were suf.ciently re.ned, even a very simple shading strategy would have 
suf.ced for visually accurate results. Figure 7, however, demonstrates that this is not the case. Image 
D2, computed from the medium accuracy global pass followed by method D for the local pass, is visually 
more accurate than images A3 and B3; yet, it took considerably less time to compute (38 versus 83 and 
150 seconds, respectively.) Another set of comparisons was made to illustrate the importance of including 
discontinuity segments in the mesh for the local pass. Figure 8 shows a view of the .oor of the exhibit 
room. The top row shows the mesh in wireframe with D0 discontinuities in red and D1 and D2 discontinuities 
in yellow. The bottom row shows the shaded .oor as reconstructed by the local pass. All images were computed 
from the medium accuracy global pass solution shown in image a2 of Figure 6 and all of them used method 
D in the local pass. As can be seen from the top row of Figure 8, no discontinu­ity segments were included 
in the left mesh, only D0 discontinuities were included in the middle mesh, and all the discontinuity 
seg­ments were included in the right mesh. When comparing the corresponding images in the bottom row, 
the higher quality of the right image stands out. Image b1 presents a b  Figure 6: Exhibit Room. Global 
pass solutions (top row) and the corresponding local pass meshes (bottom row). The accuracy of the solutions 
increases from left to right. many of the visual artifacts typical of conventional radiosity meth­ods: 
shadow and light leaks, fuzzy shadow boundaries, and incor­rectly shaped shadows. Image b2 shows how 
including D0 discon­tinuities greatly reduces shadow and light leaks, but still has prob­lems reproducing 
shadow boundaries and penumbra areas. Finally, image b3, correctly captures all shadow boundaries. We 
conclude, therefore, that it is necessary to represent discontinuities explicitly in the local pass mesh, 
even though some or all of them may have been resolved by the subdivision in the global pass. Discontinuities 
in the Mesh none D0 D0 D1 D2 triangulation 0.39 0.39 0.81 shading 9 10 26 disc. segments 0 36 559 CDT 
elements 1170 1190 2384 shading calls 2739 3027 5674 Table 2: Statistics for the comparison of meshing 
strategies shown in Figure 8. Timings are in seconds for execution on an HP 9000/720 workstation. As 
the statistics reported in Table 2 show, building a mesh that incorporates discontinuity segments takes 
longer than building one without discontinuities. Furthermore, including the discontinuities generally 
results in a larger number of elements and consequently shading the mesh takes longer. We believe, however, 
that the in­creased computation time is well justi.ed.  7 A FINAL COMPARISON In this section we demonstrate 
the performance of our combined approach on an environment of moderate complexity (1,688 input polygons.) 
Figure 9 shows a rendered view of the scene. There are two primary light sources: a small distant polygonal 
source outside the room simulates sunlight, and another polygonal source close to the ceiling provides 
the arti.cial illumination. The .gure shows two images of the same environment. The left image (HDMR) 
was generated using primary discontinuity seg­ments in both passes with shading method D in the local 
pass. To generate the right image (HR) we modi.ed our algorithm to essen­tially emulate regular HR: discontinuities 
were not used in either pass, the vertices of the triangles were shaded using method A, and linear interpolation 
was used for display. As can be expected in a complex environment, the initial linking stage results 
in a very large number of initial links, most of which represent interactions of very small magnitude. 
For ef.ciency, we use a simple culling strategy: we ignore all the initial links that do not involve 
a primary light source and whose form factor falls below a user speci.ed threshold. We found that by 
using a small threshold it is possible to eliminate most of the initial links, without any no­ticeable 
change in the resulting images. As was mentioned in Sec­tion 2, clustering of input surfaces in the initial 
linking stage should provide a more comprehensive solution to this problem. Table 3 reports various statistics 
for the two solutions from which the images in Figure 9 were rendered. The two solutions have roughly 
the same number of .nal triangles, yet the HDMR solu­tion looks dramatically better than the HR solution; 
while the latter exhibits many of typical problems of radiosity images, HDMR pro­duces sharp shadow boundaries 
and correct penumbrae, eliminates shadow and light leaks, and captures some small features that are en­tirely 
missed by HR. Furthermore, the total computation time was almost twice as long for the HR solution. We 
attempted to perform a similar comparison with our progres­sive DM algorithm [18]. However, we were not 
able to obtain a converged solution for this environment: after four hours of com­putation the DM algorithm 
was still in its fourth iteration. 7.1 Complexity of Discontinuity Meshing A legitimate concern regarding 
discontinuity meshing is that, in the­ory, l light source edges and m polygon edges can result in O(lm) 
distinct EV visual events. In the worst case, each event intersects O(m) polygons, resulting in a total 
number of O(lm2) discontinu­ity segments. In such a case each polygon has O(lm) discontinuity segments, 
which can result in as many as O(l2m2) elements in the discontinuity mesh for that polygon. We have found 
that this worst case analysis is too pessimistic in practice. Consider, for example, the environment 
shown in A B C D  Figure 7: Exhibit Room. A comparison of shading strategies. Columns 1, 2, and 3 were 
computed respectively from the low, medium, and high accuracy global pass solutions shown in Figure 6. 
Each row corresponds to a different shading strategy; starting from the top: method A, method B, method 
C, and method D. Figure 9. In this environment l is 8, and m is 6,744. The worst case upper bound on 
the number of discontinuity segments on a single polygon is 215,808. In practice, there were 18,664 discontinuity 
segments in the entire environment, an average of roughly 11 seg­ments per polygon. The highest number 
of segments on a single polygon (the .oor) is 2,175, resulting in only 7,627 triangles in the .oor s 
discontinuity mesh.  8 CONCLUSIONS By combining hierarchical radiosity with discontinuity meshing we 
have created a new radiosity method that is superior to both of its an­cestors: it is more accurate than 
the HR algorithm, both numerically and visually, and it is faster and more .exible than DM algorithms. 
The new algorithm is capable of producing high quality images even from quick simulations. Hierarchical 
radiosity has been recently extended to deal with very complex environments by introducing the notion 
of importance into the solution process [22]. This improvement is readily appli­cable to our algorithm 
as well: the global pass would simultane­ously solve for radiosity and for importance as described by 
Smits et al. [22]; the local pass would only reconstruct the radiance on surfaces which are direct receivers 
(or emitters) of importance. There are several aspects of our algorithm that can be substan­tially improved: 
Visibility computations. Our implementation uses shaft cull­ing [13] to reliably determine complete visibility 
between polygons, but point sampling is used to determine whether two polygons are entirely occluded 
from each other. Our method could be improved by using the accurate and reliable visibility algorithms 
described by  123 Figure 8: Exhibit Room Floor. A comparison of meshing strategies. Mesh (top row) and 
computed radiance (bottom row) on the .oor using simple CDT (left), CDT with D0 discontinuity segments 
(middle), and CDT with D0, D1, and D2 discontinuity segments (right). Figure 9: A comparison of Hierarchical 
Discontinuity Meshing Radiosity (left) vs. Hierarchical Radiosity (right) Radiosity Algorithm HDMR HR 
initial linking 2 : 16 : 27 2 : 16 : 27 discontinuity computations 0 : 09 : 06 0 : 00 : 00 hierarchical 
solution 0 : 16 : 42 3 : 58 : 01 triangulation 0 : 00 : 21 0 : 00 : 16 shading computations 0 : 33 : 
49 0 : 00 : 51 total time (hr:min:sec) 3 : 16 : 56 6 : 15 : 35 input polygons 1,688 1,688 discontinuity 
segments 18,664 0 initial links 165,814 165,814 links after culling 27,002 27,002 total links 39,056 
161,668 total nodes 5,778 35,454 total leaf nodes 3,733 18,571 avg. depth of hierarchy 1.31 2.04 CDT 
elements 41,090 41,284 shading calls 109,885 101,208 recomputed form factors 3,609,941 0 recomputed visibility 
terms 128,705 0 Table 3: Statistics for the comparison of hierarchical discontinuity meshing radiosity 
(HDMR) vs. hierarchical radiosity (HR) shown in Figure 9. All timings are for execution on an HP 9000/720 
work­station. Teller and Hanrahan [26]. We need to be able to compute tight bounds on the visibility 
be­tween two partially occluded polygons. This would improve the ef­.ciency of the global pass by eliminating 
unnecessary subdivision in penumbral areas. Choice of sources. Our algorithm is particularly effective 
for envi­ronments with a few primary light sources that are responsible for the most noticeable shadows. 
In general, however, primary light sources do not dominate the illumination on all the surfaces in an 
environment. Our algorithm should be extended to compute a set of the most dominant sources, primary 
or secondary, with respect to each receiving surface. This set should be used both for com­puting the 
discontinuities on that surface and for determining when visibility should be recomputed in the local 
pass. Choice of discontinuities. Not all the discontinuities are equally signi.cant. In the global pass, 
for example, we should choose dis­continuities that would resolve partial visibility most effectively, 
rather than ones that split the node most evenly. In the local pass we need to identify the discontinuities 
that are visually signi.cant and insert only these discontinuities into the mesh. ACKNOWLEDGEMENTS 
We would like to thank Brian Smits, Jim Arvo, and Kevin Novins for helpful discussions and for reviewing 
the manuscript. Ben Trumbore assembled and submitted the review draft when the authors were away on vacation. 
Suzanne Smits modeled the sculpture used in the examples in Section 6, and Matt Hyatt modeled the room 
used in the .nal comparison. This work was sup­ported by the NSF grant, Interactive Computer Graphics 
Input and Display Techniques (CCR-8617880), by the NSF/DARPA Science and Technology Center for Computer 
Graphics and Scienti.c Visualization (ASC-8920219), and by generous donations of equipment from Hewlett-Packard. 
 REFERENCES [1] Baum, Daniel R., Stephen Mann, Kevin P. Smith, and James M. Winget. Making Radiosity 
Usable: Automatic Preprocessing and Meshing Techniques for the Generation of Accurate Radiosity Solu­tions, 
Computer Graphics, 25(4), July 1991, pages 51 60. [2] Bern, Marshall and David Eppstein. Mesh Generation 
and Optimal Triangulation, in Hwang, F.K. and D.-Z. Du, editors, Computing in Euclidian Geometry, World 
Scienti.c, 1992. [3] Campbell, III, A. T. Modeling Global Diffuse Illumination for Image Synthesis, PhD 
dissertation, U. of Texas at Austin, Texas, December 1991. [4] Chen, Shenchang Eric, Holly E. Rushmeier, 
Gavin Miller, and Dou­glass Turner. A Progressive Multi-Pass Method for Global Illumina­tion, Computer 
Graphics, 25(4), July 1991, pages 165 174. [5] Chew, L. Paul. Constrained Delaunay Triangulations, Algorithmica, 
4, 1989, pages 97 108. [6] Chin, Norman and Steven Feiner. Fast Object-Precision Shadow Gen­eration for 
Area Light Sources Using BSP Trees, in Proceedings of 1992 Symposium on Interactive 3D Graphics, March 
1992. [7] Cohen, Michael F. and Donald P. Greenberg. The Hemi-Cube: A Radiosity Solution for Complex 
Environments, Computer Graphics, 19(3), July 1985, pages 31 40. [8] Cohen, Michael F., Donald P. Greenberg, 
and David S. Immel. An Ef­.cient Radiosity Approach for Realistic Image Synthesis, IEEE Com­puter Graphics 
and Applications, 6(2), March 1986, pages 26 35. [9] Gigus, Ziv and Jitendra Malik. Computing the Aspect 
Graph for Line Drawings of Polyhedral Objects, IEEE Transactions on Pattern Anal­ysis and Machine Intelligence, 
12(2), February 1990, pages 113 122. [10] Goral, Cindy M., Kenneth E. Torrance, Donald P. Greenberg, 
and Ben­nett Battaile. Modeling the Interaction of Light Between Diffuse Sur­faces, Computer Graphics, 
18(3), July 1984, pages 213 222. [11] Guibas, Leonidas and Jorge Stol.. Primitives for the Manipulation 
of General Subdivisions and the Computation of Voronoi Diagrams, ACM Transactions on Graphics, 4(2), 
April 1985, pages 74 123. [12] Haines, Eric A. Ronchamp: A Case Study for Radiosity, SIG-GRAPH 91 Frontiers 
in Rendering Course Notes, July 1991. [13] Haines, Eric A. and John R. Wallace. Shaft Culling for Ef.cient 
Ray-Traced Radiosity, in Proceedings of the Second Eurographics Work­shop on Rendering, May 1991. [14] 
Hanrahan, Pat, David Salzman, and Larry Aupperle. A Rapid Hier­archical Radiosity Algorithm, Computer 
Graphics, 25(4), July 1991, pages 197 206. [15] Heckbert, Paul S. Discontinuity Meshing for Radiosity, 
in Proceed­ings of the Third Eurographics Workshop on Rendering, May 1992, pages 203 216. [16] Heckbert, 
Paul S. Simulating Global Illumination Using Adaptive Meshing, PhD dissertation, UC Berkeley, California, 
June 1991. [17] Kok, Arjan J. F. and Frederik Jansen. Source Selection for the Direct Lighting Computation 
in Global Illumination, in Proceedings of the Second Eurographics Workshop on Rendering, May 1991. [18] 
Lischinski, Dani, Filippo Tampieri, and Donald P. Greenberg. Dis­continuity Meshing for Accurate Radiosity, 
IEEE Computer Graphics and Applications, 12(6), November 1992, pages 25 39. [19] Nishita, Tomoyuki and 
Eihachiro Nakamae. Continuous Tone Repre­sentation of Three-Dimensional Objects Taking Account of Shadows 
and Interre.ections, Computer Graphics, 19(3), July 1985, pages 23 30. [20] Reichert, Mark C. A Two-Pass 
Radiosity Method Driven by Lights and Viewer Position, Master s thesis, Cornell University, Ithaca, New 
York, January 1992. [21] Salesin, David, Dani Lischinski, and Tony DeRose. Reconstruct­ing Illumination 
Functions with Selected Discontinuities, in Proceed­ings of the Third Eurographics Workshop on Rendering, 
May 1992, pages 99 112. [22] Smits, Brian E., James R. Arvo, and David H. Salesin. An Importance-Driven 
Radiosity Algorithm, Computer Graphics, 26(4), July 1992, pages 273 282. [23] Sparrow, Ephraim M. On 
the Calculation of Radiant Interchange be­tween Surfaces, in Ibele, Warren E., editor, Modern Developements 
in Heat Transfer, Academic Press, New York, 1963. [24] Tampieri, Filippo. Discontinuity Meshing for Radiosity 
Image Syn­thesis, PhD dissertation, Cornell University, Ithaca, New York, May 1993. [25] Teller, Seth 
J. Computing the Antipenumbra of an Area Light Source, Computer Graphics, 26(4), July 1992, pages 139 
148. [26] Teller, Seth and Pat Hanrahan. Global Visibility Algorithms for Illu­mination Computations, 
Computer Graphics, 27(4), August 1993. [27] Zienkiewicz, O. C. and R. L. Taylor. The Finite Element Method, 
pages 128 132, Vol. 1, McGraw-Hill, London, 4th edition, 1989.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166144</article_id>
		<sort_key>209</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>27</seq_no>
		<title><![CDATA[Radiosity algorithms using higher order finite element methods]]></title>
		<page_from>209</page_from>
		<page_to>212</page_to>
		<doi_number>10.1145/166117.166144</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166144</url>
		<keywords>
			<kw><![CDATA[finite elements]]></kw>
			<kw><![CDATA[form-factor]]></kw>
			<kw><![CDATA[global illumination]]></kw>
			<kw><![CDATA[radiosity]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.1</cat_node>
				<descriptor>Computations on matrices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor>Spline and piecewise polynomial approximation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.8</cat_node>
				<descriptor>Finite element methods</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003714.10003736.10003737</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Functional analysis->Approximation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003718</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations in finite fields</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003719</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on matrices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003720</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on polynomials</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P249972</person_id>
				<author_profile_id><![CDATA[81100568054]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Roy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Troutman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15033556</person_id>
				<author_profile_id><![CDATA[81100480335]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Nelson]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Max]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>74367</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baum, Daniel R., Holly E. Rushmeier, James M. Winget, Improving Radiosity Solutions Through the Use of Analytically Determined Form-Factors, Computer Graphics 23(3), July 1989]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Burnett, David S., Finite Element Analysis, Addison Wesley Publishing Co., Reading, Massachusetts, May 1988]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325171</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Cohen, Michael F., Donald E Greenberg, The Hemicube: A Radiosity Solution for Complex Environments, Computer Graphics 19(3), July 1985]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83600</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Farin, Gerald, Curves and Sulfaces for Computer Aided Geometric Design, Academic Press, 1990.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808601</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Goral, Cindy M., Kenneth E. Torrance, Donald E Greenberg, Bennett Battaile, Modeling the Interaction of Light Between Diffuse Sulfaces, Computer Graphics 18(3), July 1984]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Hallquist, John O., MAZE - An Input Generator for DYNA2D and NIKE2D, LLNL Tech. Report, UCID-19029, Rev. 2.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>90787</ref_obj_id>
				<ref_obj_pid>90767</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Heckbert, Paul S., Generic Convex Polygon Scan Conversion and Clipping, Graphics Gems, Academic Press, 1990]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>894053</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Heckbert, Paul S., James M. Winget, Finite Element Methods for Global Illumination,U.C. Berkeley, Jan. 1991]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Heckbert, Paul S.,Discontinuity Meshing for Radiosity, Third Eurographics Workshop on Photorealism, Consolidation Express, Bristol, England, May 1992]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166143</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Lischinski, Dani, Filippo Tampieri, Donald E Greenberg, Combining Hierarchical Radiosity and Discontinuity Meshing, Computer Graphics, Annual Conf. Series, Aug. 1993]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>130799</ref_obj_id>
				<ref_obj_pid>130745</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Max, Nelson L., Michael J. Allison, Linear Radiosity Approximations using Vertex-to-Vertex Form Factors, Graphics Gems III, Academic Press, 1992]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Salesin, David, Dani Lischinski, Tony DeRose, Reconstructing Illumination Functions with Selected Discontinuities, Third Eurographics Workshop on Photorealism, Consolidation Express, Bristol, England, May 1992]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Siegel, Robert, John R. Howell, Thermal Radiation Heat Transfer, McGraw-Hill Book Co., N.Y., 1972]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Troutman, Roy, Parallel Radiosity Algorithms using Higher Order Finite Elements, Master's thesis, U.C. Davis, Dec. 1992]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74366</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Wallace, John R., Kells A. Elmquist, Eric A. Haines, A Ray Tracing Algorithm For Progressive Radiosity, Computer Graphics 23(3), July 1989]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166145</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Zats, Harold R., Galerkin Radios#y: Higher Order Global Illumination, Computer Graphics, Annual Conf. Series, Aug. 1993]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Radiosity Algorithms Using Higher Order Finite Element Methods Roy Troutman, Nelson L. Max Lawrence 
Livermore National Laboratory Abstract Many of the current radiosity algorithms create a piecewise con­stant 
approximation to the actual radiosity. Through interpolation and extrapolation, a continuous solution 
is obtained. An accurate solution is found by increasing the number of patches which describe the scene. 
This has the effect of increasing the computation time as well as the memory requirements. By using techniques 
found in the .nite element method, we can incorporate an interpolation function directly into our form 
factor computation. We can then use less ele­ments to achieve a more accurate solution. Two algorithms, 
derived from the .nite element method, are described and analyzed. CR Categories and Subject Descriptors: 
1.3.3 [Computer Graph­ics]: Picture/Image Generation - Display Algorithms. 1.3.7 [Com­puter Graphics]: 
Three-Dimensional Graphics and Realism. Additional Key Words and Phrases: .nite elements, form-factor, 
global illumination, radiosity. 1 Introduction The traditional radiosity algorithm computes the form 
factors at a collection of points [5]. There have been several techniques used to enhance and speed up 
the algorithm. Cohen described an algorithm which enabled more complex environments to be rendered by 
plac­ing a half cube or hemicube at each evaluation point and sampling through pixels on the hemicube 
surface [3]. We can improve the accu­racy of the solution by increasing the resolution of the hemicube 
or by analytically determining the form factors [1], [13]. Further improvements can be made by producing 
a mesh which follows the discontinuities introduced by shadow boundaries and surface inter­sections [9], 
[10]. Rather than assuming the radiosity arrives from piecewise con­stant patches, Max and Allison introduced 
an algorithm which assumed a piecewise linear approximation [11]. This algorithm works by placing an 
interpolation function directly into the form fac­tor computation. Using this technique, a more accurate 
solution can be obtained with less patches [9]. An extension to this algorithm is to increase the order 
of the interpolation function to quadratic, cubic or even higher [16]. These interpolation functions 
are what the .nite element method refers to as basis functions [2]. 2 Basis Functions The details of 
the basis functions, elements and nodes can be found in [2]. We will only give a brief overview to establish 
our terminol­ogy. 2.1 Approximation Function The .nite element method associates a basis function for 
each of the local nodes in a representative element. The basis function for a Address: P.O. Box 808, 
Livermore, Ca. 94550 email: roy@nersc.gov, max2@llnl.gov Permission to copy without fee all or part of 
this material is granted provided that the copies are not made or distributed for direct provided that 
the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and 
the title of the commercial advantage, the ACM copyright notice and the title of the publication and 
its date appear, and notice is given that copying is by publication and its date appear, and notice is 
given that copying is by permission of the Association for Computing Machinery. To copy permission of 
the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific 
permission. otherwise, or to republish, requires a fee and/or specific permission. &#38;#169;1993 ACM-0-89791-601-8/93/008 
$1.50 &#38;#169;1993 ACM-0-89791-601-8/93/008/0015 $1.50 global node becomes a combination of basis functions 
de.ned on the local nodes of all elements which contain the global node [2], [9], [14]. In this paper 
a node is contained or in an element if it is on the boundary or interior of the element. Using the basis 
function fi and radiosity Bi associated with each node i , we can approximate the radiosity at a point 
x in our environment as a linear combination of the radiosities of each node or n B (x)=. () (EQ 1) Bi 
fixi = 1 2.2 Element Construction Due to its compatibility with triangulation and orientation inde­pendence 
using Gouraud shading, we have chosen the triangle as our element. We also need to concern ourselves 
with the connectivity of this element. Our solution will be much more accurate if we align our mesh to 
the D0 and D1 discontinuities as described by [9] and [10]. We can obtain elements with C0 continuity 
by using the same nodes on the boundary of adjacent elements [2], [4]. By de.nition, the C0 elements 
can accurately model D1 discontinuities. A D0 dis­continuity would result from surface intersection, 
discrete changes in emissivity or discrete changes in re.ectivity. These can be mod­eled by aligning 
our edges to the discontinuities and duplicating the nodes along the edge [9]. Higher order discontinuities 
could be mod­eled by selectively enforcing higher derivative continuity across the common edges between 
adjacent elements, but this is quite complex [12] so we approximate them by using smaller elements. 
 3 Finite Element Methods This section will give a very brief introduction to .nite element mathematics 
to provide us with modi.cations needed for the radi­osity algorithm to incorporate higher order elements 
and the previ­ously discussed basis functions. 3.1 Residual Error We start by reiterating an equation 
from [8] which describes the radiosity for all points in the environment Bx= Ex+. xds (x, s) B (s) (EQ 
2) () () () .. O where cos .cos..(x, s)= V (x, s) ij (EQ 3) 2 pr Exact solutions to (EQ 2) are known 
only in the simplest of geom­etries [8]. The exact solution can be approximated using the linear combination 
in (EQ 1). Traditional radiosity methods can be thought of as having a constant basis function of fi 
x= 1 for all points () x inside patch i. These constant basis radiosity algorithms will not be reiterated. 
The method introduced in [11] uses linear basis functions centered on the vertices. We will be presenting 
algorithms for extend­ing polynomial basis radiosity to higher order polynomials. The traditional radiosity 
method assigns an emissivity and re.ec­tivity to each patch. We can enhance our radiosity algorithms 
by claiming that the exact emissivity and re.ectivity are also de.ned by an approximation function similar 
to (EQ 1). This would allow us to describe variations in emissivity and re.ectivity up to the degree 
of the basis function. For the sake of brevity, we will assume that the emissivity ek and re.ectivity 
. are constant across each individual k surface, k. We can replace B() s in (EQ 2) by our approximation 
function to obtain an approximate solution for Bx. If our approximation is () good, then the approximate 
solution for Bx and the value obtained () by applying (EQ 2) at point x should be close. If our approximate 
solution is exact, the difference between these two approximations will be zero. This gives us a measure 
of the accuracy of our approx­imation and is de.ned as the residual error. More speci.cally, it is expressed 
as rx= Ex+. x.ds .(x, s) B s- B (x) (EQ 4) () () () () O  3.2 Method of Weighted Residuals A general 
approximation technique is the method of weighted residuals. This technique requires the residual error 
to be orthogonal to a set {wi x} of weighting functions over the domain () O. It was shown in [8] that 
the resulting equations could be expressed in matrix form as [M - K] B = E (EQ 5) where B is a column 
vector containing the coef.cients to our approx­imation and Mij .dxwi x(x) = () fj O .dx wi x.ds .(x, 
s) fj (s) (EQ 6) = () Kij k . OO Ei = ek dx wi (x) . O where k is the index of the surface supporting 
weight function wi . 4 Higher Order Algorithms We have presented a set of interpolation functions in 
section 2 and combined them with our radiosity integral using the .nite element method in section 3. 
This gave us a matrix equation where each com­ponent of the matrix contained a weighting function. By 
replacing the weights with different functions we obtain the point collocation and Galerkin methods [8]. 
4.1 Point Collocation Method The traditional gathering algorithm as well as the linear vertex radiosity 
method introduced in [11] are examples of the point col­location method. This method replaces the weighting 
function in (EQ 6) with the dirac delta [8]. This simpli.es the M in (EQ 5) to be the identity matrix 
and E to be a column vector containing the emis­sivities at each node. The elements of K have the value 
Kij =.k .ds .(xi, s) fj (s) (EQ 7) O The contents of the integral describes a differential area to weighted 
area form factor where the area is de.ned by the domain of the basis associated with node j . We will 
call this a differential area to basis form factor. This integral can be solved using the approach speci.ed 
in [11]. The pseudocode is as follows Initialize F to 0 For each pixel h in hemicube k = index of patch 
at h Q = point on surface of k For each node j in patch k = + fj( ). Q Fij Fij h EndFor EndFor  4.2 
Galerkin Method The Galerkin method replaces the weighting functions with the basis functions giving 
us the following de.nitions for the matrices of (EQ 5). Mij = () fj dx fi x(x) . O Kij .dxfi x.ds .(x, 
s) fj (s) (EQ 8) = () k . OO Ei = ek dxfi (x) . O We ll start by looking at the equation for . We only 
need to Mij concern ourselves with the area where fi and fj are both non-zero. This will only occur if 
an element can be found which has the nodes i and j on the boundary or interior. Clearly this occurs 
if i equals j. We can easily compute by considering only the elements which Mij contain node i and looking 
through that small set of elements for the elements which also contain node j. We then integrate across 
these elements individually and sum the results. The formula for 2-D change of variables from the global 
triangle to the representative ele­ment gives us the Jacobian determinant which is the area of the global 
element. Therefore, the integral across an element is the same as the integral across the representative 
element multiplied by the area of the element. The .nal result is a constant multiplied by the area of 
the global element. The constant is dependent upon the relative posi­tions of the local nodes corresponding 
to i and j . We can store these constants in a matrix M. This matrix is symmetrical, which is what c 
we would expect by looking at the equation. The local node numbers for i and j correspond to the row 
and column of a location this matrix. Solving for Ei follows a similar path. In this case, we must inte­grate 
across the domain of the basis function. We can form a vector E which contains the integral of all of 
the local nodes across the c representative element. To compute Ei , we look at each element which has 
node i, use the local node number as an index into E , c multiply that array element by the area and 
then add it to the current value of Ei . After we visit each element, we multiply our result by the node 
emission ei . Computing is slightly more involved. We know from (EQ 7) Kij that the inner integral is 
a differential area to weighted area form fac­tor. In the Galerkin case, the weighted area still corresponds 
to the domain of a basis function, but the differential area corresponds to some point x in the domain 
of fi . We will express this differential area to basis form factor as F. Our equation simpli.es to xj 
.dx fix(EQ 9) = ()F Kij k . xj O The contents of the integral describe a basis to basis form factor. 
This integral is in a form that is appropriate for Gaussian quadrature [2], [16]. The problem of computing 
is now reduced to comput- Kij ing a set of form factors, adding the results multiplied by the appro­priate 
weight and multiplying by the re.ectivity. To compute the basis to basis form factor with gaussian quadrature 
we start by specifying the degree of precision [2]. This provides us with a collection of gauss points 
on each element. We compute an array of differential area to basis form factors FF (computed by the algorithm 
in section 4.1) at each of the gauss points. This hemicube will affect the basis to basis form factors 
associated with each node in the element. After we have completed computing the entire matrix of form 
factors, we multiply each row by the re.ectivity to obtain K. This gives us the following algorithm Initialize 
F to 0 For each patch p ap = area of patch p For each gauss point l Ql = global coordinate of point l 
wl = weight assigned to point l FF = array of form factors computed at Ql For each node i in p For j 
= 1 to total number of nodes =+ a× wl × fi(Ql)× FF j() Fij Fij p Endfor Endfor Endfor Endfor The matrix 
M is very sparse. To avoid using an excessive amount of memory due to random access, the matrix is computed 
one row at a time. This requires us to visit a node and .nd the patches which share this node. The winged 
edge data structure allows us to easily determine adjacent elements. The algorithm for computing M and 
E is as follows. Initialize M and E to 0 For each node i P = set of patches containing i For each patch 
p . P ap = area of patch p J = set of nodes in p li = local node number of node i For each j . J lj = 
local node number of node j =+ a× M(li, lj) Mij Mij p c Endfor =+ a× E() EiEi p cli Endfor Endfor To 
solve for B, we multiply each row of the form factor matrix by the re.ectivities and solve the matrix 
using the Gauss-Seidel iter­ation method. 5 Analysis A quantitative measurement of the accuracy of our 
algorithms are obtained by applying an error metric. We will apply this metric to images generated by 
our collocation and Galerkin algorithms. 5.1 Error Metric We determine the RMS radiosity reconstruction 
error by rendering each surface individually at the same distance. Our reference image was obtained by 
using quadratic basis elements on a discontinuity meshed version of our scene and applying a hemicube 
with a reso­lution of 314 x 314 x 157. Since 157 is a prime number, the chance of a correlation with 
a lower resolution hemicube is reduced. The .oating point radiosity of each pixel is compared to the 
same pixel in each rendered surface of the reference. The error is measured using n 2 ri - ei .... ri 
. i = 1 (EQ 10) n where ri is the radiosity value at pixel i in the reference scene, ei is the radiosity 
at pixel i in a test scene and n is the total number of pixels occupied by a rendered image of every 
surface. Computing the error in this manner reduces the chance of bias since it is improb­able that any 
particular node in our scene will be on a pixel center. 5.2 Collocation Results The collocation algorithm 
was implemented on a Cray YMP/C90. Timing information was obtained using the Unix times function. The 
computation time was considered to be the time spent executing the code added to the time spent completing 
system calls. The total time includes the time spent generating and solving the matrix. We do not include 
I/O times. We could not include the time spent generating the mesh since some of the following meshes 
were generated by hand. Because of its .exibility, Heckbert s software z-buffer [7] was used to project 
the environment onto the hemicube. Although the pro­gram computed the radiosity for the red, green and 
blue components, only the blue component was used to determine the error. We start by analyzing the source 
of error in Figure 1. Most of the error in this image is due to the shadow edges on the .oor. By ren­dering 
the radiosity of the .oor as a 3-D shaded surface, we enhance the radiosity discontinuities that would 
not be visible when the scene is rendered as an image. Figure 1 also shows the radiosity of the .oor 
of the reference scene rendered in this manner. This gives us more information about where errors occur 
as well as how close our approximation is to a converged reference. The reference image appeared to be 
extremely smooth. However, when we looked at the .oor rendered as a shaded surface, slight dis­continuities 
due to hemicube aliasing were detected. These artifacts are referred to as plaid patterns in [1] and 
[15]. When the edge of a light source is parallel or at a 45 degree angle to the edge of the hemic­ube, 
the amount of aliasing is greatly enhanced. In some scenes it may be possible to determine an ideal rotation 
for the hemicube in order to reduce aliasing, but when we introduce occlusion, the appar­ent edge of 
a source changes. In general, we can reduce the chance of a poor alignment by introducing a random rotation 
to the hemic­ube. This gives a slight improvement in terms of numerical error and a big improvement in 
terms of visually perceptible error. To improve our reference even further, we solved for the radiosity 
several times and averaged the results. We applied a uniform and discontinuity mesh to Figure 1. By increasing 
the resolution of a mesh, the amount of computation time increases as the error decreases. We did not 
have access to triangu­lation software that would easily allow us to create a variable sized discontinuity 
mesh. An interactive mesh generator called Maze [6] was used to produce a set of quadrilaterals which 
were then split into triangles. One of the goals used in producing this mesh was to limit the number 
of slices or poorly formed elements. Once the mesh was created, we were easily able to further subdivide 
the resulting trian­gles to improve the accuracy of our solution. Figure 2 shows the results of the algorithm 
when applied to the scene shown in Figure 1 with a 100 x 100 x 50 hemicube. A log error of less than 
-1.3 generated an image which was very dif.cult to dis­tinguish from the reference. A log error of less 
than -1.6 generated an image which could not be distinguished from the reference even with high quality 
display devices. A log error of approximately -2 was mostly due to hemicube aliasing. Note that the linear 
and con­stant uniform elements did not obtain these error levels in the time frame shown. The effect 
on the form factors due to visibility changes is basically quadratic, so we did not expect or see a great 
deal of improvement in the cubic element over the quadratic element. Dis­continuity meshing showed the 
most impressive results. The linear discontinuity elements produced the same error as the best uniform 
elements in half the time. The quadratic and cubic discontinuity ele­ments produced an error level so 
small that further reduction could only be obtained by increasing the resolution of the hemicube. The 
collocation algorithm was applied to other simple scenes. In some cases, even the higher order C0 elements 
did not conform well to the radiosity solution along edges of high variance. These edges can be found 
near dimly lit corners of a closed room. 5.3 Galerkin Results The Galerkin method was also implemented 
on the Cray YMP/ C90. This method required only minor modi.cations to the existing collocation algorithm. 
The program was implemented so that the user could specify the number of degrees of precision. It was 
shown in [14] that in the case of the uniform mesh, the optimal degrees of precision for the constant, 
linear, quadratic and cubic basis were one, one, four and four, respectively. At these levels the algorithm 
pro­duced less error in less time. We also found that the discontinuity mesh produced these same optimal 
degrees of precision. The con­stant basis Galerkin algorithm is identical to the constant basis col­location 
algorithm [14], so we don t present these results. Figure 2 shows the results of applying the Galerkin 
method using a uniform and discontinuity mesh for the scene in Figure 1. We used the same 100 x 100 x 
50 resolution hemicube for these tests. The qua­dratic and cubic basis achieved a lower error than the 
linear for both meshes. The quadratic and cubic discontinuity mesh again produced very small error levels 
immediately. In comparison to the collocation method, the Galerkin method as we implemented it took consider­ably 
more time to achieve the same error level. In general, the number of patches for a scene with triangular 
ele­ments is larger than the number of vertices. Since the number of hemicubes used in Gaussian quadrature 
depends on the number of patches, our Galerkin implementation required many more hemic­ubes to compute 
the matrices for the same scene. Figure 1 was com­puted using the collocation method with a quadratic 
basis. This required 2282 hemicubes. Our Galerkin implementation would require 3042 hemicubes if we used 
just 2 degrees of precision. 6 Conclusions and Future Work We have presented two radiosity algorithms 
which use the .nite element method and higher order basis functions to produce a more accurate solution 
in less time. The collocation method proved to be easier to implement and converged faster than the Galerkin 
method for the scenes presented. By applying discontinuity meshing, our algorithms computed an accurate 
solution in only a fraction of the time used by traditional methods. Applying adaptive meshing to these 
algorithms could present a challenging problem. In a more traditional approach, we would sub­divide our 
patches if the gradient became too large. The higher order elements can model these large gradients. 
The criteria for subdivision would require some investigation [10]. We restricted our elements to planar 
C0 triangles. We could use square or even curved elements. The curved element may require ray tracing 
for computing the form factors, but it would allow us to use exact geometries. An additional improvement 
to the accuracy might be possible by using C1 reconstruction technique [12]. Modi.cations to the algorithms 
for progressive radiosity is pos­sible. The collocation method could be used with a ray tracing algo­rithm 
similar to [15]. In this case we would have to sample the entire basis domain rather than a single patch. 
Creating an ef.cient algo­rithm would be another puzzle.    Acknowledgments This work was performed 
under the auspices of the U.S. Depart­ment of Energy by Lawrence Livermore National Laboratory under 
contract No. W-7405-Eng-48. References [1] Baum, Daniel R., Holly E. Rushmeier, James M. Winget, Improving 
Radiosity Solutions Through the Use of Analytically Determined Form-Factors, Computer Graphics 23(3), 
July 1989 [2] Burnett, David S., Finite Element Analysis, Addison Wesley Publishing Co., Reading, Massachusetts, 
May 1988 [3] Cohen, Michael F., Donald P. Greenberg, The Hemicube: A Radiosity Solution for Complex Environments, 
Computer Graphics 19(3), July 1985 [4] Farin, Gerald, Curves and Surfaces for Computer Aided Geo­metric 
Design, Academic Press, 1990. [5] Goral, Cindy M., Kenneth E. Torrance, Donald P. Greenberg, Bennett 
Battaile, Modeling the Interaction of Light Between Diffuse Surfaces, Computer Graphics 18(3), July 1984 
 Figure 1 Reference scene used for convergence test. On the left is the rendered scene. On the right 
is a height .eld showing the radiosity of the .oor. Collocation Galerkin 0 0 [6] Hallquist, John O., 
MAZE - An Input Generator for DYNA2D and NIKE2D, LLNL Tech. Report, UCID-19029, Rev. 2. [7] Heckbert, 
Paul S., Generic Convex Polygon Scan Conversion and Clipping, Graphics Gems, Academic Press, 1990 [8] 
Heckbert, Paul S., James M. Winget, Finite Element Methods for Global Illumination,U.C. Berkeley, Jan. 
1991 [9] Heckbert, Paul S., Discontinuity Meshing for Radiosity, Third Eurographics Workshop on Photorealism, 
Consolidation Express, Bristol, England, May 1992 [10] Lischinski, Dani, Filippo Tampieri, Donald P. 
Greenberg, Combining Hierarchical Radiosity and Discontinuity Mesh­ing, Computer Graphics, Annual Conf. 
Series, Aug. 1993 [11] Max, Nelson L., Michael J. Allison, Linear Radiosity Approx­imations using Vertex-to-Vertex 
Form Factors, Graphics Gems III, Academic Press, 1992  [12] Salesin, David, Dani Lischinski, Tony DeRose, 
Reconstruct­ ing Illumination Functions with Selected Discontinuities, Third Eurographics Workshop on 
Photorealism, Consolida­tion Express, Bristol, England, May 1992 [13] Siegel, Robert, John R. Howell, 
Thermal Radiation Heat Transfer, McGraw-Hill Book Co., N.Y., 1972 [14] Troutman, Roy, Parallel Radiosity 
Algorithms using Higher Order Finite Elements, Master s thesis, U.C. Davis, Dec. 1992 [15] Wallace, John 
R., Kells A. Elmquist, Eric A. Haines, A Ray Tracing Algorithm For Progressive Radiosity, Computer Graphics 
23(3), July 1989 [16] Zats, Harold R., Galerkin Radiosity: Higher Order Global Illumination, Computer 
Graphics, Annual Conf. Series, Aug. 1993 -0.5 Log Error Log Error -1.5 -2 -1 Time (Seconds) Time 
(Seconds) Uniform Discontinuity Figure 2 Error plots. The solid lines are the results from the uniform 
mesh. The dashed lines and single points are from the discontinuity mesh. The numbers indicate the degree 
of the basis functions. The error is reduced by increasing the number of elements.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166145</article_id>
		<sort_key>213</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>28</seq_no>
		<title><![CDATA[Galerkin radiosity]]></title>
		<subtitle><![CDATA[a higher order solution method  for global illumination]]></subtitle>
		<page_from>213</page_from>
		<page_to>220</page_to>
		<doi_number>10.1145/166117.166145</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166145</url>
		<keywords>
			<kw><![CDATA[Galerkin methods]]></kw>
			<kw><![CDATA[curved surfaces]]></kw>
			<kw><![CDATA[global illumination]]></kw>
			<kw><![CDATA[integral equations]]></kw>
			<kw><![CDATA[progressive refinement]]></kw>
			<kw><![CDATA[radiosity]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.1</cat_node>
				<descriptor>Computations on matrices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>F.2.1</cat_node>
				<descriptor>Computations on polynomials</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003719</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on matrices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003738</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Integral equations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003720</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on polynomials</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P107764</person_id>
				<author_profile_id><![CDATA[81100509533]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Harold]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Zatz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>74367</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Daniel Baum, Holly Rushmeier, and James Winget, "Improved Radiosity Solutions Through the Use of Analytically Determined Form- Factors", Computer Graphics, 23(3), pp. 325-334, 1989.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97896</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[A.T. Campbell, III and Donald Fussell, "Adaptive Mesh Generation for Global Diffuse Illumination", Computer Graphics, 24(4), pp. 155-164, 1990.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325171</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Michael Cohen and Donald Greenberg, "The Hemi-Cube: A Radiosity Solution For Complex Environments", Computer Graphics, 19(3), 1985, pp. 31-40.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378487</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Michael Cohen, Shenchang Chen, John Wallace, Donald Greenberg, "A Progressive Refinement Approach to Fast Radiosity Image Generation", Computer Graphics, 22(4), 1988, pp. 75-84.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Philip Davis, Interpolation and Approximation Blaisdell, New York, 1963.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5592</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[L.M. Delves and J. L. Mohamed, Computational Methods for Integral Equations, Cambridge University Press, New York, 1985.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808601</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Cindy Goral, Kenneth Torrance, Donald Greenberg, and Bennett Battaile, "Modeling the Interaction of Light Between Diffuse Surfaces", Computer Graphics, 18(3), July 1984, pp. 213-222.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[I. S. Gradshteyn and I. M. Ryzhik, Table of Integrals, Series, and Products, 4th edition, Academic Press, Inc., New York, 1965.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122740</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Pat Hanrahan, David Salzman, and Larry Aupperle, "A Rapid Hierarchical Radiosity Algorithm", Computer Graphics, 25(4), pp. 197-206, 1991.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>894332</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Paul Heckbert, Simulating Global Illumination Using Adaptive Meshing, Report No. UCB/CSD 91/636, University of California, Berkeley, 1991.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>894053</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Paul Heckbert and James Winget, Finite Element Methods for Global Illumination, Report No. UCB/CSD 91/643, University of California, Berkeley, 1991.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Paul Heckbert, "Discontinuity Meshing for Radiosity", Third Eurographics Worshop on Rendering Bristol, UK, May 1992.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>142453</ref_obj_id>
				<ref_obj_pid>142443</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Dani Lischinski, Filippo Tampieri, and Donald Greenberg, "Discontinuity Meshing for Accurate Radiosity", IEEE CG&amp;A, 12(6), Nov. 1992.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>130799</ref_obj_id>
				<ref_obj_pid>130745</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Nelson Max and Michael Allison, "Linear Radiosity Approximations using Vertex-to-Vertex Form Factors", Graphics Gems III, Academic Press, 1992, p. 319]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325169</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Tomoyuki Nishita and Eihachiro Nakamae, "Continuous Tone Representation of Three-Dimensional Objects Taking Account of Shadows and Interreflection", Computer Graphics, 19(3), 1985, pp. 23-30.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Starbase Radiosity and Ray Tracing Programmer's Manual Hewlett Packard Co., USA, 1990.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[J. Stoer and R. Bulirsch, Introduction to Numerical Analysis Springer- Verlag, New York, 1980.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[E.M. Sparrow, "Application of Variational Methods to Radiation Heat- Transfer Calculations", Journal of Heat Transfer, November 1960, pp. 375-380.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[E.M. Sparrow and R. D. Cess, Radiation Heat Transfer--Augmented Edition, Hemisphere Publishing Corp., Washington, 1978.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Filippo Tampieri and Dani Lischinski, "The Constant Radiosity Assumption Syndrome", in the Proceedings of the Second Eurographics Workshop on Rendering, Barcelona, 1991.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74366</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[John Wallace, Kells Elmquist, Eric Haines, "A Ray Tracing Algorithm for Progressive Radiosity", Computer Graphics, 23(3), 1989, pp. 315- 324.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Harold Zatz, Galerkin Radiosity: A Higher Order Solution Method for Global Illumination, Master's Thesis, Cornell University, Ithaca, New York, 1992.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Galerkin Radiosity: A Higher Order Solution Method for Global Illumination Harold R. Zatz1 Cornell Program 
of Computer Graphics Abstract This paper presents an alternative radiosity formulation using piecewise 
smooth radiance functions that incorporates curved surfaces directly. Us­ing the Galerkin integral equation 
technique as a mathematical foundation, surface radiance functions are approximated by polynomials. This 
model eliminates the need for a posteriori rendering interpolation, and allows the direct use of non-planar 
parametric surfaces. Convergence problems due to singularities in the radiosity kernel are analyzed and 
recti.ed, and sources of approximation error are examined. The incorporation of a shadow mask­ing technique 
vastly reduces the need for meshing and associated storage space accurate radiosity calculations can 
often be made with no meshing. The technique is demonstrated on traditional radiosity scenes, as well 
as environments with untessellated curved surfaces. CR Categories and Subject Descriptors: I.3.7 [Computer 
Graphics]: Three-Dimensional Graphics and Realism; I.3.3 [Computer Graphics]: Picture/Image Generation. 
Additional Keywords and Phrases: global illumination, radiosity, inte­gral equations, Galerkin methods, 
curved surfaces, progressive re.nement. 1 Introduction The behavior of light interacting with a macroscopic 
environment is ex­tremely complex. Despite considerable effort spent searching for a closed­form solution 
to global illumination problems [10, 22], it seems unlikely that such an approach will be found. To produce 
computer-generated pictures in a reasonable amount of time, approximations must be used. Typical approx­imation 
techniques include the use of direct lighting only, tessellation of the simulated environment into polygonal 
surfaces, constant or linear shading of surfaces, and sampling the intensity distribution at a limited 
number of points. Goral et al. [7] introduced the conventional radiosity approximations to computer graphics, 
assuming surfaces have purely diffuse re.ectance distributions, and that .nite regions on these surfaces 
have locally constant radiosity values. Intensity variations across a surface are accounted for by meshing 
it into a large number of smaller pieces. Although these assumptions are effective, recent research has 
demon­strated their limitations. Conventional radiosity techniques generally require that objects be 
.at or polygonal [1, 7, 3], even though Wallace has demon­strated [21] that radiosity transfers can be 
computed between non-planar surfaces. Generating images with accurately placed shadows involves a lengthy 
meshing process, whether surfaces are divided along arbitrary lines [15, 2, 9] or along actual lines 
of shadow discontinuity [13, 12]. In .nite element analysis, it is often possible to trade off a large 
num­ber of lower-order elements for a smaller number of higher-order elements. Sparrow [18] and Heckbert 
[10, 11] have successfully applied higher-order radiosity techniques to special-case geometries. Max 
and Allison [14] ex­plored some of the dif.culties of using a linear elements in more general 1now at 
Rhythm and Hues Studios, Inc., 910 North Sycamore Ave., Hollywood, CA 90038. E-mail: hzatz@rhythm.comor 
hzatz@alumni.caltech.edu Permission to copy without fee all or part of this material is granted provided 
that the copies are not made or distributed for direct provided that the copies are not made or distributed 
for direct commercial advantage, the ACM copyright notice and the title of the commercial advantage, 
the ACM copyright notice and the title of the publication and its date appear, and notice is given that 
copying is by publication and its date appear, and notice is given that copying is by permission of the 
Association for Computing Machinery. To copy permission of the Association for Computing Machinery. 
otherwise, or to republish, requires a fee and/or specific permission. otherwise, or to republish, requires 
a fee and/or specific permission. radiosity meshes. In this paper we reformulate the radiosity equations 
with the goal of applying higher-orderGalerkin techniques to more general environments, paying particular 
attention to the dif.culties caused by sin­gularities and shadow discontinuities. Bene.ts of this approach 
include the direct incorporation of curved surfaces into the solution technique, as well as a signi.cant 
memory savings due to a drastic reduction of mesh size. The Galerkin method does have its disadvantages; 
dealing with shadows and extremely bright light sources can be tricky, and computationally ex­pensive 
singularities can appear in many places in a complex environment. However, the use of higher-order functions 
to replace meshing provides a different perspective on the dif.culties of the global illumination problem, 
avoiding some of the dif.culties of conventional methods. 2 Background The radiosity model of global 
illumination is based on the principle of energy conservation. All light energy emitted within an enclosure 
is tracked as it re.ects off surfaces within that environment, until it dissipates into heat. Conventional 
radiosity methods [1, 2, 3, 4, 7, 9, 15, 21] generally simplify the solution procedure by using the Constant 
Radiosity Assumption [20] the primary assumption that radiosity values are constant over .nite regions, 
and subsidiary assumptions that emittance, re.ectivity, and surface normals are also constant over .nite 
regions. Unfortunately, this constant, polygonal approach to the radiosity problem limits the solution 
accuracy. Conventional radiosity methods attempt to compensate by increasing the mesh density, assuming 
that the environment can be accurately approximated if enough polygons. However, the number of polygons 
needed often exceeds the memory and computational resources available. Tampieri and Lischinski [20] further 
explain that the Constant Radios­ity Assumption leads to fundamental errors in radiosity computations. 
A solution computed on a tessellated surface can only be as accurate as the tes­sellation. The Constant 
Radiosity Assumption also presents inconsistency between its illumination and rendering phases. During 
the energy transfer phase, radiosity is assumed constant across each polygon. However, radios­ity renderings 
are made by sampling each polygon at a few points and then interpolating brightness values between these 
points. Basic signal process­ing shows that while interpolating a solution may make an image look more 
accurate, all such interpolation can do is mask error by blurring the image. A consistent radiosity solution 
must incorporate the interpolation into the energy transfer calculations. 2.1 The Radiosity Integral 
Equation In order to apply the appropriate mathematical tools to the solution of radios­ity problems, 
it is convenient to express the radiosity equation in parametric form. Parametrically, the key radiosity 
variables (radiosity, emittance, re­.ectivity, etc.) are represented as functions of two variables, (st) 
or (uv), over each surface i or j. By abstracting all the complexity of surface inter­action into a single 
kernel functionKij(stuv), the radiosity equation can be written as an integral equation,   XZZ Bi(st)= 
Ei(st)+ Kij(stuv)Bj(uv)du dv(1) j where the kernel function Kij(stuv) is the product of the double­ 
differential form factor Fij(stuv), re.ectivityi(st), area Ai(st), and visibility VISij(stuv)  Kij(stuv)=i(st)Fij(stuv)VISij(stuv)Aj(uv)(2) 
&#38;#169;1993 ACM -0-89791 -601 -8/93/008 $1.50 &#38;#169;1993 -0---8/93/008/0015 $1.50 The form factor 
and area functions can be further expanded in terms of the functions describing surface geometryxi(st) 
and normals ni(st): .. r ubc k . cb . a. c . bcccb k r c .. ni(st)nj(uv)xj(uv)xi(st)Fi j(stuv)= xi(st)xj(uv)4 
xj(uv) xj(uv) Aj(uv) = (3) 3 Mathematical Background The Galerkin method provides a method for solving 
integral equations in terms of a basis set of non-constant functions across each surface. This sec­tion 
provides the mathematical background necessary to apply the Galerkin method to the radiosity equation. 
3.1 Basis Set Projection To approximate the radiosity distribution by a combination of functions, we 
.rst need formal tools to manipulate an appropriate two-dimensional basis fT u j v g set. We denote this 
basis set k(st)k =0 1 ,where s and t are the parametric variables across a surface, andk speci.es a particular 
function in the set. Just as geometric vectors have a dot product that projects one onto the other, the 
inner product of two functionsf (st) and g(st) can be de.ned, 11 fg= f (st)g(st)(st)ds dt (4)Whji W ZZ 
W 11 (st) is some weighting function that describes the importance of different positions to the inner 
product. To apply the Galerkin method to radiosity, we use an orthonormal set of basis functions, k(st) 
a set designed so that for a particular inner product weight function(st),  8 hTjTi W fTWg kl kl=kl 
(5) Finding the combination of orthonormal basis functions closest to some particular function is relatively 
simple. Given that the radiosity function over surface i is Bi(st), we de.ne the coef.cients Bk i Bki 
= Bik(6) ph X jTi W T The original function can be approximated by the weighted sum, Bi(st) Bk k(st) 
(7) i k 3.2 Legendre and Jacobi Polynomials The Galerkin method is usually solved using an orthonormal 
polynomial basis set, de.ned on the interval [1 1]. Legendre and Jacobi polynomials are one-dimensional, 
orthonormal polynomials which can be combined into a two-dimensional basis set by multiplying two polynomials 
in different variables. We limit our analysis in the next two sections to polynomials of one variable. 
When the inner product has a weight function equal to one, the polyno­mials formed are the Legendre polynomials. 
The unnormalized Legendre polynomials are generated by a recursion rule [8], P0(x)=1 P1(x)= x (n + 1)Pn+1(x) 
= (2n + 1)xPn(x)nPn 1(x) (8) The normalized Legendre polynomials are x r 1 Pn(x)= n + Pn(x) (9) 2 Polynomial 
sets can also be created with non-constant inner product weight functions (x). Later in this paper (section 
4.2), a set of polynomials will be needed with a weight function that has a multiple zero at its endpoints. 
WW  The Jacobi polynomialsPi () have such behavior, with the weight function, (x) = (1x)(1+ x)(10) 
whereandare the degree of multiplicity. The unnormalized Jacobi polynomials have a more complex recursion 
rule than the Legendre polynomials [5]: P()(x)=1 P()(x)=+2++x 0 122 P(n+1) )x )) n (x)= A(C(B(n (11) 
n where 2 A()=(2n +++1)(2+2n +++2)  ra n )P() (2n ++(x) n B()=2(n +)(n +)(2n +++2)P()(x) nn 1 C()=2(n 
+ 1)(n +++ 1)(2n ++) (12) n These polynomials can be normalized by the factor [8]: (n +1)(++1+ n)(++1+2n) 
 W (13) (+1+ n)+1+ n)2 3.3 Quadrature Rules An informative explanation of one-dimensional quadrature 
rules has been compiled by Delves and Mohamed [6]. A condensed version is presented here. A quadrature 
rule is a method for approximating the integral of a function by a weighted sum of function samples at 
particular points. Quadrature rules can be used to approximate inner product integrals, like that in 
(4). Given a .xed function (x) and another function f (x), we can choose pointsi and weights wi such 
that: b Z W (p X + +1 f (x)(x)dx wif (i) (14) i Quadrature rules can be designed to be exact for a certain 
class of func­tions. The Gaussian quadrature rules, by computing optimal positions for the N sample pointsi, 
are exact for polynomials up to order 2N1. The Gauss quadrature rule with weight function (x) is closely 
tied to the set of orthogonal polynomials with the same weight function. To develop an N-point Gauss 
quadrature rule for the integral N 1 (x) f (x) dxwif (i) (15) Z Wp X start by choosing a set of orthogonal 
polynomialsi(x) with the same weight function (x), and expressed in terms of recursion rules [17] so 
that: 1(x)0 0(x)1 W TT a 2 f f T TW Tf 3 T i+1(x)(xi+1)i(x)i2+1i 1(x) (16) Take thesei andi coef.cients, 
and construct a tridiagonal symmetric matrix: . . 22 (17) 6 1 i=1 7 .. 4 5 ..N The eigenvalues of 
this matrix, which are also the roots of the polynomial T N (x), are the quadrature rule s positionsi. 
The square of the .rst coef­.cient of the ith eigenvector is the quadrature weight wi. The eigenvectors 
and eigenvalues for tridiagonal symmetric matrices can be found using QR factorization [17]. To create 
the Gauss-Legendre rule of order N, exact for polynomials up to degree 2N1, thei andi coef.cients are 
[22]: i2 i+1 =0i+1= 1) (18) (2i + 1)(2i and for general Jacobi polynomials P(): i  r 0 1 .2 r  
.. N 0N (+)() i+1= (2i +++ 2)(2i ++) 4(i +)(i +)i(++ i) i+1 = (++2i)2(++2i +1)(++2i1) (19)  When using 
these quadrature rules to project a function into a basis set using (6), it is important to use a suf.ciently 
accurate quadrature rule. If a one-dimensional polynomial basis set includes terms up to ordern, the 
projection integral (6) must be accurate up to order 2n since the function is represented as a polynomial 
of ordern, the projection integrand will be a polynomial of order 2n. Therefore, a one-dimensional Gaussian 
quadrature rule must have at least N + 1 sample points to integrate accurately [6]. = S  Figure 1: 
Conventional radiosity methods approximate a surface s radiosity by meshing it into a large number of 
constant intensity patches. Radiosity is represented by height above the surface. 4 Non-Constant Radiosity 
Consider the effect of meshing a single surface into constant radiosity patches (Figure 1). Although 
the radiosity is smooth on individual patches, combi­nations describe a discontinuous, stair-step radiosity 
function. To produce a smooth, consistent solution, we need to formulate radiosity in terms of smooth 
functions across an entire surface, instead of disjoint patches on parts of a surface. Figure 2 shows 
a hypothetical decomposition of a radiosity function. Constant, linear, and higher-order functions are 
combined to produce a smooth approximation to the radiosity function. If the radiosity of every surface 
were represented by a combination of these functions, the radiosity problem would reduce to .nding their 
relative weights. To properly compute these proportions, we use a radiosity formulation based on a linear 
combination of orthonormal basis functions l(st). T X TfgfTg Blcontribution of each function l(st). 
The full radiosity distribution on a surface becomes the function Instead of radiosity values, we use 
radiosity coef.cients the relative Btotal(st)= Bll(st) (20) l Functions on different surfaces must interact 
in a manner analogous to the way conventional patches interact through form factors. Just as conventional 
radiosity uses form factors to describe the interaction between patches, here the kernel function Kij(stuv) 
from (1) details how energy is transferred between functions on different surfaces. When two constant 
functions on different surfaces interact, the kernel function interaction is equivalent to a classical 
form factor. Other kernel functions describe higher­order interactions. 4.1 The Galerkin Method Given 
an orthonormal basis set, the Galerkin technique .nds a good [6] .t to the integral equation s solution 
within that set. Heckbert [10, 11] suggested that the Galerkin method and meshing could be used to solve 
the radiosity integral equation in a plane. This and subsequent sections demonstrate how it can be applied 
to three-dimensional radiosity. Starting with the parametric radiosity equation (1), XZZ Bi(st)= Ei(st)+ 
Kij(stuv)Bj(uv)du dv (21) expand the Bj(uv) term inside the integral in terms of the basis setl(uv) XZZ 
TfTg using (7). The Blj coef.cient can be moved outside of the integral, and the summations over j and 
l can be combined to produce the equation BlBi(st)= Ei(st)+ Kij(stuv)l(uv)du dv (22)j jl Now, take the 
inner product of both sides with the kth basis set function k(st). Using bilinearity and the relation 
described in (6), T X(ZZ j T 7 T u W Bki = Eik + Bl Kij(stuv)l(uv)dudvk(st) (23) j jl = S  Figure 
2: Higher-order radiosity approximates a surface s radiosity by di­viding it into several different smooth 
functions. These smooth functions are scaled and combined to approximate the original radiosity distribution. 
fT The inner product now depends only on known information; the kernel function Kij is a function of 
the environment, andl(uv)is a precomputed g basis set. The result of that inner product is denotedKijkl 
, the kernel matrix. Evaluating this inner product is the most dif.cult part of a radiosity solution, 
requiring four integrations two explicit, and two in the inner product. However, once the kernel matrix 
has been computed for each value ofi, j, k, and l, the radiosity equation can be written as a matrix 
equation, X jKkl ii ij BkEk Bl = (24) jl Just as a conventional form factor matrix relates constant 
radiosities on different elements, the kernel matrix relates radiosity functions across different surfaces. 
The Kkl i and Eik values are analogous to classical form ij , Bk factors, patch radiosities, and emittances, 
respectively. However, each of these coef.cients refers to some function representing part of the distribution 
of radiosity across a surface, as opposed to a constant value across a surface. Note also that even though 
(24) is written in terms of four indices, since the surface indices ij and function indices kl are independent 
of each other, (24) is still a two-dimensional matrix equation. This equation can be solved using any 
standard matrix technique, such as Gaussian elimination, or progressive re.nement techniques [4]. Cohen 
et al s progressive re.nement technique requires slight modi.cation with Galerkin radiosity, because 
the radiosity coef.cientsBlj may have negative values. These negative values do not indicate negative 
energies; they are a weight applied to the basis function. The shooting order should be based on unshot 
magnitude: Ml = Bl l(uv)dAj(uv)du dv (25)  kk ZZ jTj 4.2 Edge Singularities Near the common edge of 
two non-coplanar surfaces, the double-differential form factor approaches in.nity as a pole of order 
two[22]. Although the function still has a .nite integral, the singularity can cause serious conver­gence 
problems. If the singularity is ignored, Galerkin solution methods converge extremely slowly for a mediocre 
basis set, and may fail entirely for a bad basis set. To insure reasonable convergence, the basis set 
must compensate for the singularity. In (23), the singularity appears inside the quadruple integral that 
generates Kijkl . This integral also includes the inner product weight function W jjW (st). If the weight 
function is chosen with zeroes of suf.ciently high multiplicity where the kernel function Kij goes to 
in.nity, the two features can cancel and the integral will converge. Since the kernel singularity grows 
as a pole of order two, the weight function should have zeroes of (02) and (20) multiplicity two at its 
edges. The Jacobi polynomial sets PP (see section 3.2), have appropriate weight functions. By using a 
hybrid Galerkin method, the edge singularities are cancelled. For non-singular light transfers between 
surfaces that do not touch, a Leg­endre basis set is used. For the few transfers that are singular, a 
basis set (0 2) of Jacobi polynomials is used, either or (2 0) depending on the sin- PP gularity s location. 
After computing theKijkl coef.cients and the associated radiosity transferred in a singular shot, project 
this polynomial function ins and t is back into a Legendre basis set for storage. An empty box computed 
with this hybrid method is shown in Figure 6. cannot be expressed in terms of a few polynomials. Attempting 
to model such edges with a small polynomial basis set produces a fuzzy shadow with ripples around it 
the Gibbs behavior visible in Figure 7. Shadow edges come from discontinuities in the radiosity function 
[10]. One way to remove these discontinuities is to mesh the environment along curves of discontinuity 
[13, 12], a process which eliminates the occlusion dif­.culties of Galerkin radiosity. Unfortunately, 
discontinuity meshing meth­ods magnify the number of surfaces in the scene, vastly increasing computa­tion 
time. Even though shadows are primarily an interaction between a light source and a receiving surface, 
subdividing the receiving surface to produce accurate shadows complicates interactions with the rest 
of the environment. 5.1 Shadow Masking To smooth the shadow discontinuities out of the radiosity distribution 
seen by the Galerkin method, we propose using a shadow mask approximation. For the majority of emitter-receiver 
pairs, where shadows do not have a high-frequency effect on the solution, traditional visibility calculations 
can be used. However, for a select group of emitter-receiver pairs, we move the visibility term VISij(st 
uv) out of the kernel function and integral in equations (2) and (1), and replace it with a normalized 
shadow mask function Mij(st),  ee RRRR e VISij(stuv)du dv Mij(st) = (28) du dv This function approximates 
the fraction of the light originating from emitter j that arrives at a particular location on receiving 
surfacei. The shadow mask is one where the emitter is fully visible, zero where the emitter is fully 
occluded, and takes on intermediate values when the light is partially occluded. It is essentially a 
texture map for painting the shadow onto the receiving surface. During the radiosity pass, if the energy 
transfer from emitterj to receiver i involves a shadow mask, the radiosity is accumulated without visibility 
calculations in the special coef.cients Bkij instead of Bki . When light is re-emitted from surface i 
s basis functions, the kernel samples are multi­plied by the shadow mask across surface i, restoring 
some of the occlusion information. The radiosity across a surface, Bi(st), becomes the combina­tion of 
ordinary Galerkin basis functions and shadow mask-weighted basis functions. If h represents all light 
sources casting a shadow on surfacei, X T X ee T Bi(st)= Bik k(st)+ Mih(st)Bik hk(st) (29) k hk By using 
coef.cients Bki, radiosity in the shadow mask is maintained sep­ e h arately from radiosity coming from 
other parts of the environment. When a receiving surface has shadow masks associated with it, every surface 
inter­acts either with a shadow mask, or with the standard surface description not both. In this implementation, 
shadow masks were computed from equation (28) using multiple point-to-point visibility samples regularly 
spaced in the parametric dimensions. Values of Mij(st) were computed by linear in­ e terpolation between 
these sample points. Shadow mask samples could con­ceivably be taken along lines of discontinuity, or 
in some more complicated non-regular structure to improve ef.ciency or accuracy. In all environments 
tested, even accounting for the time spent constructing shadow masks, the time required to compute a 
radiosity solution using shadow masks was signi.cantly smaller than that for a full discontinuity mesh. 
For the simple environment in Figure 8, the shadow mask was a regular 40 by 40 grid of sample points 
on the .oor. Without Gibbs phenomena to transfer energy into higher order basis functions as in Figure 
7, the radiosity pass actually required fewer shots and less time to converge than the non­shadow masked 
version. Since a shadow mask only adds one surface to the rows (but not the columns) of the radiosity 
matrix for each associated emitter-receiver pair in the environment, shadow masks add relatively little 
to radiosity solution time compared to discontinuity meshing methods. Shadow masks can be precomputed 
for portions of the environment where shadow details are ex­pected to be signi.cant. Furthermore, since 
shadow masks are de.ned in parametric space, a single implementation can cast shadows to and from any 
type of surface. Unfortunately, shadow masks also have signi.cant disadvantages. By moving the visibility 
term out of the radiosity equation s integral, any corre­lation between the emitter s light distribution 
and the shape of the occluding  Figure Description CPU time Shots 6 Empty box 5.4s 7 7 Box with single 
occluder 59.7 s 33 8 Shadow masked box 42.8 s 22 12 Clay teapot 6.71 h 53 Table 1: Timings for the shadow 
generation and radiosity pass combined for various pictures computed with this algorithm. All timings 
are for an HP 9000/720 workstation. order increases; the regions where the solution is least accurate 
tend to be near singular edges. In this particular test case, the method of [13] took about the same 
amount of time as the highest-order Galerkin solution. However, the Galerkin method only required 6.5 
Megabytes of memory, compared to 75 Megabytes for a more conventional, meshing approach. For all environments 
tested in this paper, Galerkin and conventional radiosity methods tend to take about the same amount 
of time to produce equivalent pictures. However, the Galerkin radiosity technique s lower memory usage 
is maintained in more complex environments. 7 Results The radiosity solution computed by this method 
is a list of basis set expansion coef.cients Bki for each surface i and basis function k. The actual 
radiance at a given point (st) on surface i is recovered from these coef.cients using (7). If shadow 
masks were used, the additional coef.cientsBk are incorporated ih with (29). In this implementation, 
environments are rendered by a simple ray­tracing/scanline technique. When a ray intersects a surface, 
that intersection point is projected back into the surface s parametric space, and the result is used 
to compute a radiosity value for the appropriate pixel. 7.1 Curved Surfaces Curved surfaces can be easily 
incorporated into Galerkin radiosity; the kernel term s form factor as expressed in (3), includes surface 
normals explicitly. To implement curved surfaces, replace the traditional constant surface normal value 
with a function, computable at any parametric location. Sample pictures are shown with bicubic patches 
(Figure 12) and other curved surfaces (Figure 13). The Galerkin radiosity method was applied directly 
to these environments; the curved surfaces werenot tiled. For comparison purposes, the teapot environment 
was also computed using a commercially-available radiosity package [16]. This package uses the point-sampling 
algorithm of Wallaceet al. [21] to compute form factors, but does not perform adaptive meshing. Since 
this radiosity package cannot use bicubic patches directly, each of the teapot s patches were tessellated 
with a 20 by 20 grid. The radiosity solution took 6.2 hours, and over 54 megabytes of memory to compute; 
this simple forty-patch scene became a relatively complex, eight thousand polygon environment. In contrast, 
the Galerkin computation took 6.7 hours, but only required 3.9 megabytes of memory during the radiosity 
pass. Over 90% of this computation time was spent computing visibility samples. The signi.cant point 
of this comparison is that given approximately equivalent amounts of time to produce a solution, conventional 
and Galerkin methods produced similar results. But since Galerkin methods needn t maintain the detailed 
geometric structure of a mesh, they use signi.cantly less memory. 7.2 Parallelization Galerkin radiosity 
environments are not meshed into large, complicated data structures, so it is relatively easy to maintain 
copies of the environment in memory on multiple hosts. Since each individual light transfer between two 
surfaces depends only on the geometry and shadow masks, they can be computed on independent machines. 
Such a parallelization scheme was implemented, running concurrently on DECstations, HP 700 s and 800 
s, and on multiple processors of an Apollo DN10000. The image of Figure 13 was computed in parallel on 
.ve DECstations and .ve HP 700 s as a background process over two days.  8 Conclusions Using the Galerkin 
method, this paper has presented an alternative method for producing radiosity simulations. Through special 
treatment of the ra­diosity equation s singularities and discontinuities, the Galerkin technique s dependency 
on smooth kernels can be overcome. Although the resulting pictures are similar to those produced by conventional 
radiosity methods, the method used to generate them is fundamentally different: . The radiosity across 
a surface is represented as a smoothly varying function. Pictures are rendered directly from the radiosity 
solution, without an additional blurring step. . Adequately sampled curved surfaces can be used directly. 
Since curved surfaces don t need to be tessellated, they can be incorporated into a scene cheaply. Issues 
of approximating a surface s geometry and approximating a surface s radiosity are separated. . Energy 
transfer error analysis shows that meshing is only essential when two surfaces are extremely close to 
each other relative to their size. Meshing is not needed to model variations in intensity across a surface. 
By using shadow masks, the local details of shadow edge generation are separated from the global issues 
of energy balance. 9 De.ciencies of the Method As with any rendering algorithm, Galerkin radiosity has 
its own particular disadvantages. Problems with the treatment of shadows are the most signif­icant; if 
important shadows are missed, a solution will contain signi.cant Gibbs ringing behavior. It may not always 
be easy to determine ahead of time where detailed shadow masking or meshing will be necessary, pos­sibly 
requiring multiple solution attempts before all shadows are properly accounted for. Shadow masking is 
only a rough approximation to the true occlusion behavior; it eliminates any correlation between variations 
in light source intensity and the intensity of the shadow, virtually returning to the Constant Radiosity 
Assumption for a shadow s light source. Furthermore, the distri­bution of the shadow mask sample points 
can have a signi.cant impact on the accuracy of the shadow they generate. Higher order methods also have 
the potential to be computationally ex­pensive. Because of the (N + 1)4 samples required to transfer 
radiosity between surfaces of order N, radiosity calculations can become extremely expensive if too high 
a solution order is used. In general, an order of 4 or 5 is suf.cient, but self-intersecting or highly 
curved surfaces may require a higher-order solution. The method does not mathematically guarantee radiosity 
continuity be­tween adjacent coplanar surfaces. However, such surfaces appear much less frequently in 
a shadow masked environment than in a meshed environment. If such continuity is needed, it can be generated 
by using a high enough order on the adjacent surfaces that the error on each surface is reduced until 
their radiosity values along their common boundaries match visibly usually 8 or 9 in our tests. Finding 
all the singularities in a system can also be dif.cult. Environ­ments usually have a large number of 
T-intersections (see Figure 3), each of which could require a separate meshing step. Although T-intersections 
can often be ignored, there s always a risk that the ignored singularity will cause the solution to fail 
to converge, requiring recomputation. 10 Future Work Shadow masks are currently implemented using bilinear 
interpolation on a simple grid of sample points. Many more ef.cient sampling schemes are possible, such 
as adaptive quadtrees, or some method that directly computes the location of shadow discontinuities. 
Additionally, some method should be developed for automatically determining where shadow masks are needed. 
Some generalization of shadow masks is needed to account for variations in light source intensity. A 
means for enforcing continuity between adjacent surfaces, possibly by using some sort of modi.ed patch/element 
method could lower the re­quired solution order, and signi.cantly accelerate the algorithm when such 
surfaces are present. A method combining adaptive meshing and a low or­der Galerkin solution might produce 
reasonable images rapidly. Extending Figure 6: An empty box computed with up to fourth order polynomials, 
or 15 basis functions across each surface. On an HP 9000/720, the radiosity pass took 5.4 CPU seconds. 
 Hanrahan s hierarchical multigridding technique [9] to higher order func­tions could produce a means 
to do this. Some method must also be found to automatically determine an appropriate solution order for 
each surface, instead of the current area-based heuristic. The method of this paper uses a Legendre basis 
set for non-singular energy transfers. Galerkin methods frequently use a Chebyshev basis; by examining 
the relative accuracy of different basis sets, it may be possible to .nd a better basis set for the radiosity 
problem. This paper is only a .rst attempt at applying higher order solution methods to the radiosity 
problem. Much work remains to fully integrate this approach into the general framework of global illumination 
and radiosity.   Acknowledgements The Program of Computer Graphics at Cornell is one of .ve sites 
of the NSF/DARPA Science and Technology Center for Computer Graphics and Scienti.c Visualization (Grant 
# ASC-8920219). Thanks to HP/Apollo and DEC for equipment donations. This research was partially conducted 
under a National Science Foundation grant entitled Interactive Input and Display Techniques. The author 
was funded by a National Science Foundation Graduate Student Fellowship, a Dabnicorp Computer Graphics 
award, and by a Cornell University Sage Fellowship. I would like to thank Don Green­berg for providing 
insightful comments as to what was needed to write up this research, Dani Lischinski and Filippo Tampieri 
for providing the reference solution of Section 6, and especially Jim Arvo for his advice in developing 
the research and this text. I d also like to thank everyone at the Cornell Program of Computer Graphics, 
Rhythm and Hues, Inc., and the Caltech Graphics Lab for their support and encouragement.  References 
[1] Daniel Baum, Holly Rushmeier, and James Winget, Improved Ra­diosity Solutions Through the Use of 
Analytically Determined Form­Factors ,Computer Graphics, 23(3), pp. 325-334, 1989. [2] A. T. Campbell, 
III and Donald Fussell, Adaptive Mesh Generation for Global Diffuse Illumination ,Computer Graphics, 
24(4), pp. 155-164, 1990. [3] Michael Cohen and Donald Greenberg, The Hemi-Cube: A Radios­ity Solution 
For Complex Environments ,Computer Graphics, 19(3), 1985, pp. 31-40. [4] Michael Cohen, Shenchang Chen, 
John Wallace, Donald Greenberg, A Progressive Re.nement Approach to Fast Radiosity Image Gener­ation 
,Computer Graphics, 22(4), 1988, pp. 75-84. [5] Philip Davis, Interpolation and Approximation, Blaisdell, 
New York, 1963. [6] L. M. Delves and J. L. Mohamed, Computational Methods for Integral Equations, Cambridge 
University Press, New York, 1985. Figure 7: A box with an occluding rectangle computed with a fourth 
order basis on all surfaces except the .oor, which has an eighth order basis. The ripples on the .oor 
of the box appear because shadow discontinuities cannot be accurately described by a low frequency Galerkin 
basis set. [7] Cindy Goral, Kenneth Torrance, Donald Greenberg, and Bennett Bat­taile, Modeling the Interaction 
of Light Between Diffuse Surfaces , Computer Graphics, 18(3), July 1984, pp. 213-222. [8] I. S. Gradshteyn 
and I. M. Ryzhik, Table of Integrals, Series, and Products, 4th edition, Academic Press, Inc., New York, 
1965. [9] Pat Hanrahan, David Salzman, and Larry Aupperle, A Rapid Hierar­chical Radiosity Algorithm 
,Computer Graphics, 25(4), pp. 197-206, 1991. [10] Paul Heckbert, Simulating Global Illumination Using 
Adaptive Mesh­ing, Report No. UCB/CSD 91/636, University of California, Berkeley, 1991. [11] Paul Heckbert 
and James Winget, Finite Element Methods for Global Illumination, Report No. UCB/CSD 91/643, University 
of California, Berkeley, 1991. [12] Paul Heckbert, Discontinuity Meshing for Radiosity ,Third Euro­graphics 
Worshop on Rendering, Bristol, UK, May 1992. [13] Dani Lischinski, Filippo Tampieri, and Donald Greenberg, 
Discon­tinuity Meshing for Accurate Radiosity , IEEE CG&#38;A, 12(6), Nov. 1992. [14] Nelson Max and 
Michael Allison, Linear Radiosity Approximations using Vertex-to-Vertex Form Factors ,Graphics Gems III, 
Academic Press, 1992, p. 319 [15] Tomoyuki Nishita and Eihachiro Nakamae, Continuous Tone Repre­sentation 
of Three-Dimensional Objects Taking Account of Shadows and Interre.ection ,Computer Graphics, 19(3), 
1985, pp. 23-30. [16] Starbase Radiosity and Ray Tracing Programmer s Manual, Hewlett Packard Co., USA, 
1990. [17] J. Stoer and R. Bulirsch, Introduction to Numerical Analysis, Springer-Verlag, New York, 1980. 
[18] E. M. Sparrow, Application of Variational Methods to Radiation Heat-Transfer Calculations ,Journal 
of Heat Transfer, November 1960, pp. 375-380. [19] E. M. Sparrow and R. D. Cess, Radiation Heat Transfer 
Augmented Edition, Hemisphere Publishing Corp., Washington, 1978. [20] Filippo Tampieri and Dani Lischinski, 
The Constant Radiosity As­sumption Syndrome , in the Proceedings of the Second Eurographics Workshop 
on Rendering, Barcelona, 1991. [21] John Wallace, Kells Elmquist, Eric Haines, A Ray Tracing Algorithm 
for Progressive Radiosity ,Computer Graphics, 23(3), 1989, pp. 315­ 324. [22] Harold Zatz, Galerkin Radiosity: 
A Higher Order Solution Method for Global Illumination, Master s Thesis, Cornell University, Ithaca, 
New York, 1992.  Figure 8: A box with the transfer from light source to .oor shadow masked, reference 
solution, with solution orders zero, one, three, and seven. computed to fourth order on all surfaces 
except the .oor and light source, which are computed to eighth order.    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166146</article_id>
		<sort_key>221</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>29</seq_no>
		<title><![CDATA[Wavelet radiosity]]></title>
		<page_from>221</page_from>
		<page_to>230</page_to>
		<doi_number>10.1145/166117.166146</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166146</url>
		<keywords>
			<kw><![CDATA[global illumination]]></kw>
			<kw><![CDATA[hierarchical radiosity]]></kw>
			<kw><![CDATA[wavelets]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.9</cat_node>
				<descriptor>Fredholm equations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003714.10003738</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Integral equations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010148.10010149.10010158</concept_id>
				<concept_desc>CCS->Computing methodologies->Symbolic and algebraic manipulation->Symbolic and algebraic algorithms->Linear algebra algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003719</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on matrices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Experimentation</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P269895</person_id>
				<author_profile_id><![CDATA[81100259454]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Gortler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40023875</person_id>
				<author_profile_id><![CDATA[81100117380]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schr&#246;der]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15024219</person_id>
				<author_profile_id><![CDATA[81406592138]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Cohen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15033698</person_id>
				<author_profile_id><![CDATA[81100482576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Pat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanrahan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>154090</ref_obj_id>
				<ref_obj_pid>154074</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ALPERT, B. A Class of Bases in L2 for the Sparse Representation of Integral Operators. SIAM Journal on Mathematical Analysis 24, 1 (Jan 1993).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>154803</ref_obj_id>
				<ref_obj_pid>154793</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ALPERT, B., BEYLKIN, Q., COLEMAN, R., AND ROKHLIN, V. Wavelet-like Bases for the Fast Solution of Second-kind Integral Equations. SIAM Journal on Scientific Computing 14, 1 (Jan 1993).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BEYLKIN, Q., COIFMAN, R., AND ROKHLIN, V. Fast Wavelet Transforms and Numerical Algorithms I. Communications on Pure and Applied Mathematics 44 (1991), 141-183.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>163196</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[CHUI, C. I~. An Introduction to Wavelets, vol. 1 of Wavelet Analysis and its Applications. Academic Press Inc., 1992.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378487</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[COHEN, M., CHEN, S. E., WALLACE, J. R., AND GREENBERG, D. P. A Progressive Refinement Approach to Fast Radiosity Image Generation. Computer Graphics 22, 4 (August 1988), 75-84.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325171</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[COHEN, M. F., AND GREENBERG, D. P. The Hemi-Cube: A Radiosity Solution for Complex Environments. Computer Graphics 19, 3 (July 1985), 31-40.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>130655</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[DAUBECHIES, I. Ten Lectures on Wavelets, vol. 61 of CBMS-NSF Regional Conference Series in Applied Mathematics. SIAM, 1992.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5592</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[DELVES, L. M., AND MOHAMED, J. L. Computational Methods for Integral Equations. Cambridge University Press, 1985.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808601</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[GORAL, C. M., TORRANCE, K. E., GREENBERG, D. P., AND BATTAILE, B. Modelling the Interaction of Light between Diffuse Surfaces. Computer Graphics 18, 3 (July 1984), 212-222.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[GORTLER, S. J., COHEN, M. F., AND SLUSALLEK, P. Radiosity and Relaxation Methods; Progressive Refinement is Southwell Relaxation. Tech. Rep. CS-TR-408-93, Department of Computer Science, Princeton University, February 1993.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122740</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[HANRAHAN, P., SALZMAN, D., AND AUPPERLE, L. A Rapid Hierarchical Radiosity Algorithm. Computer Graphics 25, 4 (July 1991), 197-206.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>144727</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[HECKBERT, P. S. Simulating Global Illumination Using Adaptive Meshing. PhD thesis, University of California at Berkeley, January 1991.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[HECKBERT, P. S. Radiosity in Flafland. Computer Graphics Forum 2, 3 (1992), 181-192.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[KAJIYA, J. T. The Rendering Equation. Computer Graphics 20, 4 (1986), 143-150.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>142453</ref_obj_id>
				<ref_obj_pid>142443</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[LISCHINSKI, D., TAMPIERI, F., AND GREENBERG, D. P. A Discontinuity Meshing Algorithm for Accurate Radiosity. IEEE CG&amp;A 12, 4 (July 1992).]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>67254</ref_obj_id>
				<ref_obj_pid>67253</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[MALLAT, S. G. A Theory for Multiresolution Signal Decomposition: The Wavelet Representation. IEEE Transactions on Pattern Analysis and Machine Intelligence 11 (July 1989), 674-693.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>148286</ref_obj_id>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[PRESS, W., TEUKOLSKI, S., VETTERLING, W., AND FLAN- NERY, B. Numerical Recipies in C, The Art of Scientific Computing, 2 ed. Cambridge University Press, 1992.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[SALESlN, D., LISCHINSKI, D., AND DERosE, T. Reconstructing Illumination Functions with Selected Discontinuities. Third Eurographics Workshop on Rendering (1992), 99-112.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[SCHR/SDER, P. Numerical Integration for Radiosity in the Presence of Singularities. In Fourth Eurographics Workshop on Rendering (1993).]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[SCHRODER, P., GORTLER, S. J., COHEN, M. F., AND HANRA- nAN, P. Wavelet Projections For Radiosity. In Fourth Eurographics Workshop on Rendering (June 1993).]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166138</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[SCHRCIDER, P., AND HANRAHAN, P. On The Form Factor Between Two Polygons. In Computer Graphics, Annual Conference Series, 1003 (August 1993), Siggraph.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[STOER, J., AND BULIRSCH, R. Introduction to NumericalAnalysis. Springer Verlag, New York, 1980.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>80963</ref_obj_id>
				<ref_obj_pid>80960</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[SZELISKI, R. Fast Surface Interpolation Using Hierarchical Basis Functions. IEEE Trans. PAM112, 6 (June 1990), 513-439.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>11169</ref_obj_id>
				<ref_obj_pid>11166</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[YSERENTANT, H. On the Multi-level Splitting of Finite Element Spaces. Numerische Mathematik 49 (1986), 379-412.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166145</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[ZATZ, H. R. Galerkin Radiosity: A Higher-order Solution Method for Global Illumination. In Computer Graphics, Annual Conference Series, 1003 (August 1993), Siggraph.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Wavelet Radiosity Steven J. Gortler Peter Schröder Michael F. Cohen Pat Hanrahan Department of Computer 
Science Princeton University Abstract Radiosity methods have been shown to be an effective means to 
solve the global illumination problem in Lambertian diffuse environments. These methods approximate the 
radiosity integral equation by projecting the unknown radiosity function into a set of basis functions 
with limited support resulting in a set of n linear equations where n is the number of discrete elements 
in the scene. Classical radiosity methods required the evaluation of n 2 interaction coef.cients. Efforts 
to reduce the number of required coef.cients without compromising error bounds have focused on raising 
the order of the basis functions, meshing, accounting for discontinuities, and on developing hierarchical 
approaches, which have been shown to reduce the required interactions to O(n). In this paper we show 
that the hierarchical radiosity formulation is an instance of a more general set of methods based on 
wavelet theory. This general framework offers a uni.ed view of both higher order element approaches to 
radiosity and the hierarchical radiosity methods. After a discussion of the relevant theory, we discuss 
a new set of linear time hierarchical algorithms based on wavelets such as the multiwavelet family and 
a .atlet basis which we introduce. Initial results of experimentation with these basis sets are demonstrated 
and discussed. CR Categories and Subject Descriptors: I.3.7 [Computer Graphics]: Three-Dimensional Graphics 
and Realism Radiosity; G.1.9 [Numerical Analysis]: Integral Equations Fredholm equations. Additional 
Key Words and Phrases: global illumination, wavelets, hi­erarchical radiosity.  Introduction In computer 
graphics, radiosity methods have been used to solve the global illumination problem in environments consisting 
en­tirely of Lambertian (diffuse) re.ectors and emitters. The solution is a radiosity function over the 
domain of the surfaces in the scene. Classical radiosity [9, 6] (CR), derived from the radia­tive heat 
transfer literature, approximates the radiosity function as piecewise constant. An energy balance argument 
gives rise to a linear system. This system has n 2 coef.cients called form factors. Here n is the number 
of discrete areas, or elements, over which the radiosity function has been assumed to be constant. The 
form factor describes the fraction of the energy leaving one element and arriving at another. Typically, 
an iterative algorithm such as Gauss-Seidel iteration [22] or progressive radiosity [5, 10] is used to 
solve the system of linear equations for the radiosities. An integral equation called the rendering equation 
was proposed by Kajiya to model the global illumination problem [14]. He Permission to copy without 
fee all or part of this material is granted provided that the copies are not made or distributed for 
direct provided that the copies are not made or distributed for direct commercial advantage, the ACM 
copyright notice and the title of the commercial advantage, the ACM copyright notice and the title of 
the publication and its date appear, and notice is given that copying is by publication and its date 
appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
 permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. otherwise, or to republish, requires a fee and/or specific permission. 
&#38;#169;1993 ----8/93/008/0015 $1.50 &#38;#169;1993 ACM -0 -89791 -601 -8/93/008 $1.50 0123 Order 
of basis functions Figure 1: The space of projection methods for radiosity. showed that CR is a particular 
approximation to this equation. By casting the problem in this form, techniques developed for the solution 
of integral equations [8] can be exploited to solve the radiosity equation. In particular, Heckbert [12, 
13] has demonstrated that the lin­ear system in radiosity can be derived by projecting the radiosity 
integral into a .nite dimensional function space. The CR algo­rithm results from using the space of piecewise 
constant functions, (i.e., projecting the function into a set of constant (or box ) basis functions). 
In general, a function can be projected into any .nite dimensional function space. A desirable .nite 
dimensional space is one that can represent the function accurately with as few terms as possible. In 
his studies, Heckbert considered radiosity functions that are piecewise linear. Zatz [25] has used Legendre 
polynomi­als to arrive at solutions that are piecewise polynomial of higher order. Other researchers 
have explored the use of higher order bases in the mesh construction and reconstruction phases of the 
algorithm [18] as well as discontinuity meshing [15, 13]. The use of higher order bases, which we will 
refer to as galerkin radios­ity (GR), has been shown to lower the number of basis functions needed to 
obtain a particular level of accuracy, albeit at a higher cost per basis. A second avenue of research 
has attempted to lower the com­putational complexity of solving the linear system which arises in CR. 
Hanrahan et al. [11] presented a hierarchical radiosity method (HR) modeled after recent advances in 
n-body algorithms. HR ex­ploits the fact that neighboring patches in the environment often have similar 
form factors to distant patches. This reasoning is extended to form a hierarchy of patches, (i.e., a 
hierarchy of basis functions) in a straightforward manner. While the methods using higher order bases 
try to exploit co­herence in the illumination function, HR tries to exploit the co­herence in the form 
factor itself, more precisely, in the kernel of the radiosity integral. In particular, HR is based on 
approximating the kernel as a constant function over intervals of varying sizes. In places that the kernel 
varies slowly, large intervals are used. Where the kernel varies quickly, smaller intervals are needed. 
Recently Beylkin et al. [3] made the observation that integral operators satisfying very general smoothness 
conditions can be approximated to any .nite precision with only O(n) coef.cients when projected into 
a wavelet basis instead of the usual O(n 2). This remarkable result means that, in practice, integral 
equations governed by smooth kernels lead to sparse matrices that can be solved in linear time. Since 
the radiosity kernel is, in general, a smooth function of the type required by this theorem, wavelet 
methods can be used to obtain O(n) complexity radiosity algo­rithms. We call this wavelet radiosity. 
Hierarchical basis functions have been used before with .nite­element methods [24] and applied to problems 
such as surface interpolation [23]. In those instances, hierarchical basis functions were used to improve 
the condition number of the matrix. In our context, the hierarchical basis functions (wavelets) are used 
because many of the resulting matrix coef.cients are small enough to be ignored while still allowing 
for an accurate answer. In some sense we are regarding the matrix as an image on which we are able to 
perform lossy compression. Coef.cients are negligible because over many regions the kernel can be well 
approximated by a low order polynomial. The mathematical tools of wavelet analysis provide a general 
framework offering a uni.ed view of both higher order element approaches to radiosity, and the hierarchical 
radiosity methods. Figure 1 places earlier algorithms plus the new methods we inves­tigate here into 
a matrix relating hierarchy versus the order of the underlying basis. CR uses zero order polynomials, 
while GR uses higher order polynomials (indicated by the arrow). The vertical axis represents the sparseness 
obtained by exploiting smoothness of some order in the kernel. HR exploits constant smoothness in the 
kernel. Within this context, we recognize HR as a .rst order wavelet. Higher order wavelets can be used 
that result in an even sparser matrix. One such family of higher order wavelets is the multiwavelet family 
of [1] (M2,3 in Figure 1). We will also introduce a new family of wavelets, which we have dubbed .atlets 
(F2,3 in Figure 1) that require only low order quadrature methods while maintaining most of the bene.ts 
of other wavelet sets. This paper proceeds with a review of projection methods for solving integral equations 
followed by a discussion of re­cent advances concerning the solution of integral equations using wavelets. 
Finally we discuss our implementation and report ex­perimental .ndings. Some of the more technical details 
of wavelet projections, as well as a detailed analysis of the underlying math­ematical framework, are 
described in [20].  The Radiosity Integral Equation If all surfaces and emitters are Lambertian diffuse, 
the rendering equation can be written as, B(s1,s2) = cos .s cos .t E(s1,s2) + .(s1,s2)dt1dt2 VstB(t1,t2) 
pr2 st (1) where B(s1,s2) gives the radiosity at a point speci.ed by the surface parameters s1,s2, E 
the emission, and . the re.ectivity1. The kernel of the integral, cos .s cos .t k(s1,s2 ,t1,t2) = .(s1,s2) 
Vst pr2 st is a function describing the geometric and visibility relationship between two points in the 
domain; .s and .t are the angles be­tween the surface normals and the line between s and t; rst is the 
1The re.ectivity, ., is actually a function of wavelength. Without loss of gener­ality, we will consider 
only a monochromatic world for the remainder of this paper. distance between the two points; Vst is 1 
if point s is visible to point t and 0 otherwise. Over many large intervals, where r is large relative 
to the size of the patches, the kernel is well represented by a low order poly­nomial. Notable exceptions 
include the corners of the environment where r 2 goes to 0 and the kernel is singular, and shadow discon­tinuities 
where the visibility switches abruptly from 0 to 1. 3 Projections After a short review of function projections 
we will show how projections can be used to .nd approximate solutions to integral equations such as the 
radiosity equation. The ideas presented here can be found in greater detail in [12, 25]. We begin by 
writing the approximation of a function B(s) in a .nite dimensional function space where all functions 
B (s) can be expressed as a linear combination of n basis functions Ni(s) n B(s) B (s)= BiNi(s) i=1 
where the Bi are scalar coef.cients with respect to the chosen bases. For example, the space of piecewise 
constant functions is spanned by a basis of translated box functions, and the space of piecewise linear 
functions is spanned by a basis of translated hat functions. To complete the approximation, we must .nd 
a way to derive the coef.cients. For this, we de.ne an inner product of two func­ f tions f (s) and g(s) 
as (f, g) = ds f (s)g(s). Two functions are orthogonal iff (f, g) = 0. We then say that a function B 
(s) is the orthogonal projection of B(s) into the .nite dimensional function space if (B - B, Ni) = 0 
for all basis functions Ni(s). If the original basis functions are orthonormal we can .nd the coef.cients 
of a function B(s) with respect to the basis {Ni} by performing inner products B (s)=BiNi(s)=(B, Ni)Ni(s) 
ii In the case of bases which are not orthonormal we must use in­ner products with the dual basis functions 
(see [20]) to .nd the coef.cients. Using projection methods, instead of solving the integral equa­tion 
(1), we solve the related integral equation2 B (s)= E (s)+dt k(s, t)B (t),Ni(s)Ni(s) (2) i In words, 
we operate on (integrate against the kernel) the pro­jected function B (t). After having been operated 
on, the resulting function generally no longer lies in the .nite dimensional function space, so the function 
is reprojected against the Ni(s). B can be obtained by solving the linear system Bi = Ei +Bj Kij j 
Kij =dsdtk(s, t)Nj (t)Ni(s) (3) To compute the integrals Kij some form of numerical quadrature or closed 
form solution [21] must be employed. If the basis func­tions are piecewise constant, these integrals 
are related to the well known form factors. 2In order to simplify the presentation we will write the 
radiosity function as having one variable, and the kernel function as having two variables. In the text 
we will explain what needs to be done for a 3D radiosity implementation. It is important to remember 
that the projected equation is only an approximation to the original integral equation. Projections into 
different .nite dimensional spaces will result in different ap­proximations with differing amounts of 
error and different types of error. In general the projection error is O(hp+1) where h is the res­olution 
of the grid, and p the degree of the polynomial used which favors higher order basis functions. Higher 
order basis functions also result in smoother reconstructed radiosity solutions leading to fewer visual 
artifacts. However, higher order basis functions require more work to evaluate the associated inner products, 
pos­sibly offsetting potential savings. One set of choices for basis functions is given by the family 
of functions called wavelets. Wavelets Wavelet theory is a rapidly developing .eld that has its roots 
in pure mathematics [7] and signal processing [16]. Good introduc­tions to the topic can be found in 
[17, 4]. In this section we review some wavelet theory focusing on the relevant issues for radiosity. 
Wavelets form hierarchical bases which can offer alternative bases for familiar .nite dimensional function 
spaces. The simplest wavelet construction is the Haar construction shown in Figure 2. In the upper left 
is a set of basis functions which span all piece­wise constant functions at resolution 8 on the interval. 
Using the operators g (pairwise differencing) and h (pairwise averaging) we can construct another basis 
for the same space (upper right). Four of these functions are just like the original basis, only wider, 
thus we can repeat the construction (middle right). Repeating once more we .nally have a basis for the 
original space of functions consisting of the overall average f0 and the difference functions .i,j from 
all the lower levels. The last set of functions is known as the Haar wavelet basis. This construction 
is very similar to an image pyramid that one might use for texture mapping. In such a pyramid the image 
(function in our case) is represented at dif­ferent levels of resolution by successive averaging steps. 
In the Haar pyramid we only remember the overall average and all the differences between successive levels 
of the pyramid. The Haar basis is only the simplest example of an in.nite family of such constructions, 
however the basic principles are the same for all wavelet bases. More formally we start with two functions 
.(s) (sometimes called the detail function) and f(s) (the smooth function) de.ned on the unit interval 
s . [0, 1]. Scales (or levels) i and translates j of f(s) and .(s) are expressed as 2i/2 fi,j(s)= f(2i 
s - j) 2i/2 .i,j(s)= .(2i s - j) with j =0,..., 2i - 1. According to this indexing, the function fi,j 
is just like the function fi-1,j except that fi-1,j is twice v as wide, and 1/ 2 times as tall (the wider 
functions are shorter so that (fi,j,fi,k) remains constant independent of i). Similarly, fi,j is just 
like the function fi,j+1 except it is translated. To create an n =2L dimensional function space we construct 
an L level hierarchy of functions that are scales and translates of f and . (Figure 2 illustrates L = 
3). We obtain the wavelet basis for the hierarchy by choosing only the detail shapes on all levels plus 
the smooth shape on the top level, .i,j, i =0,...,L - 1 and f0. Between levels there is the so called 
two-scale relationship fi-1,j = hk-2jfi,k k .i-1,j = gk-2jfi,k k In words the f functions at a given 
level can be linearly combined to yield f and . functions at the next coarser level. This combi­ box 
 basis  g, h g, h f 3, j g, h g, h    g, h f 2, j g, h . 2, j  f g, h 0, j f 1, j . 0, j . 
1, j . 1, j . 2, j wavelet basis Figure 2: Transformation of a piecewise constant basis into the Haar 
wavelet basis. nation can be expressed as a convolution with some sequences h and g with the result subsampled 
by 2 (expressed by the factor 2 in the index k - 2j of h and g). The sequences h and g can be thought 
of as a low pass .lter and high pass .lter respectively. The projection of an arbitrary function B(s) 
into a wavelet3 basis can be formally written as B (s)= (B, f0)f0(s)+ (B, .i,j ).i,j (s) (4) ij Instead 
of computing all the above inner products, we can .nd the coef.cients ef.ciently by exploiting the two-scale 
relationship. Given the projection of some arbitrary function B(s) with respect to the lowest level basis 
fL,j the wavelet coef.cients can be found using a pyramid algorithm [16]. Each stage of this algorithm 
takes a vector of coef.cients and convolves it with the .lters h and g, returning the smooth and detail 
coef.cients one level up 3To simplify the discussion we are assuming that we have an orthonormal wavelet 
basis. We discuss the non orthonormal case in [20]. f1 f1 i, 2j i-1, j f2 f2 i-1, ji, 2j g, h . 1 
i, 2j+1  f1 i-1, j f2 . 2 i-1, ji, 2j+1 Figure 3: The M2 wavelet construction whose smooth shapes are 
the .rst two Legendre polynomials. Both of the detail shapes (lower right) have two vanishing moments. 
XformUp( vector Bf,int i) for( j =0;j< 2i/2;j ++) Bfup .k hk-2jBf[k]; [j] = Bup [j] = .k gk-2jBf[k]; 
( Bup Bup return f , . ); The entire one dimensional pyramid transform is then stated as PyramidUp( vector 
) BfL,k for( i = L;i> 0;i -- ) , ) = XformUp( Bfi,k ,i ); (Bfi-1,k B.i-1,k return ,i =0,...,L - 1); 
( Bf0 , B.i,k If the h and g convolutions have constant width (with respect to i) then each call to XformUp 
has cost linear in the length of the array passed in. Since each successive call in PyramidUp works on 
only the smooth half left by the previous call the overall runtime to build the pyramid is O(n + n 2+ 
n 4+ ... +1) = O(n). A similar algorithm PyramidDownreverses this process using XformDown for successive 
calls XformDown( vector Bf, vector B.,int i ) for( j =0;j< 2 * 2i; j ++ ) Bdown f [j] = k hj-2k Bf[k]+ 
k gj-2kB.[k]; Bdown return f ; PyramidDown( Bf0 , ,i =0,...,L - 1 ) B.i,k for( i = 0; i<L; i ++ ) = XformDown( 
Bfi,k , ,i); Bfi+1,k B.i,k return ; BfL,k A key property of wavelets essential to this work is that 
a suf­.ciently smooth function B(s), when expressed in a wavelet ba­sis (Equation 4) will have many small 
coef.cients. By ignoring these negligible coef.cients we are left with a sparse, approximate representation. 
The negligible coef.cients occur because wavelet functions have vanishing moments. We say that a function 
.(s) has M vanishing moments if ds .(s)s i =0,i =0,...,M - 1 The Haar wavelet (Figure 2) has one vanishing 
moment, thus the projection of a nearly constant function into the Haar basis will have wavelet coef.cients 
near 0. Similarly, if a wavelet basis function has two vanishing moments, the projection of a linear 
function will vanish. Figures 3 and 4 show examples of wavelets, ., with two vanishing moments. 5 Wavelets 
In Higher Dimensions Wavelet bases for functions of two or more variables are required for radiosity. 
Our goal is to project the kernel, which is a four dimensional function, into a basis set in which it 
has a sparse representation. f1 i, 2j  f1 i-1, j f2 f2 i-1, j i, 2j g, h . 1 f1 i-1, j i, 2j+1 f2 
. 2 i-1, ji, 2j+1 Figure 4: The F2 wavelet construction. F2 bases have two dif­ferent detail shapes. 
Both of the detail shapes have two vanishing moments. kfi,fi recurse kfi-1,.i-1 kfi-1,fi-1 kfi,.i-1 kfi,fi-1 
k.i-1 ,.i-1 k.i-1,fi-1 Figure 5: The 2D Pyramid Algorithm is applied to form factors taken from the .atland 
radiosity environment consisting of two parallel line segments. (Flatland [13] is radiosity in a plane). 
The dot size indicates the magnitude of a given entry in the matrix. An arbitrary function k(s, t) of 
two variables on a .nite two dimensional interval can be approximated by some function k (s, t) that 
lies in a two variable .nite dimensional function space. Given a particular one dimensional wavelet, 
a 2D wavelet basis4 is made up of the functions f0(s)f0(t) .i,j (s).i,k(t) .i,j (s)fi,k(t) fi,j (s).i,k(t) 
where we only couple functions on the same scale i, where i = 0,...,L - 1 and j, k =0,..., 2i - 1. The 
2D wavelet coef.cients may be obtained from the .nest resolution coef.cients BfL,j,fL,k using a 2D PyramidUp 
algo­rithm. This algorithm begins with the BfL,j ,fL,k written in a 2D matrix tableau. It then applies 
XformUp once to each row, fol­lowed by an application of XformUp to each resulting column. This procedure 
is applied recursively to the BfL-1,j,fL-1,k quar­ 4Another 2D wavelet basis could be constructed from 
the tensor product of a 1D wavelet basis. The different forms of multidimensional wavelet bases are discussed 
in [3, 20]. Figure 6: To illustrate the sparseness of the kernel matrix we transform the .atland radiosity 
matrix from Figure 5 into the 2D Haar basis. Many of the coef.cients are small in magnitude (small dots). 
Figure 7: We transform the same matrix into the F2 basis. Notice that even more of the coef.cients are 
negligible now. ter (Figure 5). The construction of a 2D PyramidDown follows analogously from the one 
dimensional PyramidDown. This construction can be extended to functions of four variables such as the 
kernel in 3D radiosity k(s1,t1,s2,t2). For this case, there are sixteen combinations of f and . functions 
in four vari­ables. The basis is made up of all .fteen combinations on the same scale i which involve 
. functions. The corresponding pyra­mid transformation functions are constructed as in the two dimen­sional 
case by applying XformUp and XformDown respectively to each dimension in turn. For this type of multidimensional 
wavelet basis Beylkin et al. [3] show that for a given error tolerance, only O(n) coef.cients need to 
be used to attain the prescribed error tolerance in the re­sults of our computations. Figures 6 and 7 
visualize the sparseness of a .atland radiosity kernel when written in two wavelet bases with one and 
two vanishing moments respectively.  6 Radiosity with Wavelets To obtain an ef.cient radiosity algorithm, 
we project the kernel by taking inner products with the wavelet basis functions. The coef.cients of the 
kernel with respect to the basis are given by kf = kf0,f0=dtdsk(s, t)f0(s)f0(t) ka =dtdsk(s, t).i,j 
(s).i,k(t) ijk = k.i,j,.i,k kß =dtdsk(s, t)fi,j (s).i,k(t) ijk = kfi,j ,.i,k k. =dtdsk(s, t).i,j (s)fi,k(t) 
ijk = k.i,j,fi,k Because of the vanishing moment properties of the wavelets and the smoothness properties 
of the kernel, many of these terms are nearly zero. A projected version of the integral operator can 
now be derived by projecting the kernel itself. This derivation which we only sketch here is described 
in greater detail in Beylkin et al. [3]. The ka , kß and k. coef.cients are used to represent the kernel 
which has been approximated with respect to the wavelet basis. Given this projection, after performing 
the necessary algebra, the approximate operator can be written as dt k (s, t)B(t)= ffaa Bkf0(s)+(Bikkijk 
).i,j (s) ij k ßß .. +(Bk)fi,j (s)+(Bk).i,j (s) ikijk ikijk ijk ijk (5) where Ba = B. =dt .i,k(t)B(t) 
ik ik = B.i,k Bß =dt fi,k(t)B(t) ik = Bfi,k Bf = Bf0=dt f0(t)B(t) 6.1 The Basic Algorithm Equation 
5 suggests the following three phase algorithm to ap­ proximate the kernel operating on a radiosity function. 
Step 1 Pull: Obtain the n (n = number of bases of the radiosity function) coef.cients Ba and the n coef.cients 
Bß of the radiosity function. If we are initially given the coef.cients BfL,j , the 2n needed coef.cients 
can be obtained by calling a procedure Pull which is just like PyramidUp except it returns both the f 
and . coef.cients. This step transforms n coef.cients into 2n coef.cients. A 1D Pull would then be Pull( 
vector ) BfL,k for( i = L;i> 0;i -- ) , ) = XformUp( Bfi,k ,i ); (Bfi-1,k B.i-1,k return , ,i =0,...,L 
- 1) ; ( Bfi,k B.i,k Step 2 Gather: Let the projected kernel operate on the projected radiosity function. 
This means that we sum over the index k, and is equivalent to a matrix multiply. Because of the vanishing 
moments 2 of the wavelet functions most of the n kernel coef.cients will be near zero and may be ignored 
if the action of the kernel is desired to .nite precision. The procedure Gather results in 2n coef.cients 
Gfi,j and G.i,j that represent the resultant radiosity function as a combination of fi,j(s) and .i,j 
(s). Step 3 Push: Reconstruction of the radiosity function using the 2n functions fi,j(s) and .i,j (s) 
is done with the procedure Push which is similar to PyramidDown but takes as arguments both the f and 
. coef.cients. A 1D Push would then be Push( Bfi,k , ,i =0,...,L - 1) B.i,k for( i = 0; i<L; i ++ ) += 
XformDown( Bfi,k , ,i); Bfi+1,k B.i,k return ; BfL,k Wrapping this projected operator within a Jacobi 
iteration loop results in the following algorithm (ka ,kß ,k. ) = ProjectKernel(); =; BfL,k EfL,k while( 
!converged ) G = 0; , ) = Pull( BfL,k );  (Bfi,k B.i,k (Gfi,j , G.i,j ) = Gather( Bfi,k , B.i,k , ka 
, kß , k. ); = Push( Gfi,j , ); GfL,k G.i,j =; BfL,k GfL,k + EfL,k Display(); The push and pull can 
be done in O(n) (linear in the number of elements) steps. The gather step (this is a complete gather 
sweep which updates all of the entries) can be done in O(m) time where m is the number of terms in the 
kernel expansion (matrix) that are signi.cant. We want m to be as small as possible. Wavelet bases will 
lead to m = O(n) where the constant factor in O(n) decreases with the number of vanishing moments. What 
remains is to project the kernel into the wavelet basis, which may be done as follows ProjectKernel() 
= Quadrature( k, fL,j , fL,k ); kfL,j,fL,k (ka , kß , k. ) = PyramidUp( kfL,j,fL,k ); where( (ka , kß 
, k. ) <. ) ProjectKernel( i, patch p, patch q ) smooth = AskOracle( p, q ); if( smooth ) return; else 
 (ka kß k. ) i,j(p),k(q) , i,j(p),k(q), i,j(p),k(q) = Quadrature( k, p, q ); if( i == L-1 ) return; 
 else ProjectKernel( i+1, left(p), left(q) ); ProjectKernel( i+1, left(p), right(q) ); ProjectKernel( 
i+1, right(p), left(q) ); ProjectKernel( i+1, right(p), right(q) ); If the oracle .nds the region under 
consideration suf.ciently smooth no more recursive calls need be executed, since the coef­.cients at 
lower levels will be insigni.cant by assumption. The function Quadrature() computes the projection of 
the kernel function onto the basis functions at the given level.  6.3 3D Radiosity In 3D radiosity B 
is a function of two variables so in the main program we use a 2D Pull and a 2D Push respectively. k 
is a function of four variables so in the bottom up ProjectKernel we use a 4D PyramidUp function. In 
the top down approach to ProjectKernel there are .fteen not three quadratures and sixteen recursive calls 
for all combinations of four children of p and q.  7 Implementation The top down algorithm described 
above has been implemented by extending the implementation of hierarchical radiosity described in Hanrahan 
et al. [11]. 7.1 Choice of Basis Two families of wavelets have been explored, multiwavelets [1] and 
a family of wavelets that we call .atlets. Each of these fam­ilies have members with any number of vanishing 
moments. The construction of MM (multiwavelet with M vanishing mo­ments) begins with M smooth functions 
which are the .rst M Legendre Polynomials, fm(s)= Lm(s), and M detail functions .m(s) that are piecewise 
polynomials of degree M - 1, and have M vanishing moments. A hierarchy is then constructed from these 
shapes. M1 is the Haar basis, however, for M greater than 1, MM is technically speaking not a true wavelet 
since it begins with a collection of f and . functions instead of a single pair. Multiwavelets form an 
orthonormal basis. Figure 3 shows the basis functions for the M2 hierarchy. The two-scale relationship 
for M2 is expressed concisely as . ... . . f1 f1 i,2ji-1,j 2 020 (ka , kß , k. )=0 ; 1 v 6.2 The Top 
Down Approach 8 ..... - ..... ..... ..... ..... ..... vv 31 31 f2 i,2j f2 i-1,j = f1 .1 i-1,j 0 -20 
2 i,2j+1 vv 13 -13 f2 .2 i,2j+1 i-1,j Unfortunately, this bottom up ProjectKernel is an expensive implementation 
requiring quadratic time and space. The costs can be dramatically cut by using an oracle which predicts 
which m of the n 2 coef.cients of the projected kernel are signi.cant. Then, these m values are computed 
directly by quadrature or symbolic integration. Assuming that the oracle can estimate the smoothness 
of the kernel for a given region (vis-a-vis a given number of van­ishing moments), an ef.cient top down 
recursive version of ProjectKernel can be written as follows Using this relationship the push and pull 
operations can be com­puted using a binary tree, instead of as a subsampled vector convo­lution. A node 
stores the four coef.cients of the functions fi1 -1,j , f2 i-1,j , .i1 -1,j, .i2 -1,j . During a pull, 
a node computes the val­ues of its coef.cients as a linear combination of the f1 i,2j i,2j, f2 coef.cients 
obtained from its left child, and the f1 i,2j+1 co­ i,2j+1, f2 ef.cients obtained from its right child. 
To represent the radiosity function over a patch we need a 2D M2 basis for which we use a quad-tree where 
each node stores sixteen coef.cients. During a pull, a node computes its coef.cients as a linear combination 
of the sixteen ff coef.cients from its children (four from each child). The .atlet basis FM is made up 
entirely of piecewise constant functions. The fm are M adjacent box functions, and the .m are M piecewise 
constant functions that have M vanishing moments. Figure 4 shows the F2 hierarchy. For F2, the two-scale 
relationship is given by . ... . .  7.2 Pull, Push and Gather Both multiwavelets and .atlets are instances 
of tree wavelets. A tree wavelet has the property that the convolution sequences h and g for two neighboring 
elements do not overlap. This property allows us to organize all computations along a tree which does 
not need to have uniform depth. Tree wavelets also allow for an­other simpli.cation. Since all necessary 
coef.cients reside in the immediate children of a node we can use the two-scale relation­ship to store 
only the ff coef.cients and need not represent the f., .f, and .. coef.cients explicitly. With this simpli.cation 
11 0 0 f1 f1 i,2ji-1,j ProjectKernel is implemented as follows ProjectKernel( i, patch p, patch q ) 
ParentLevelsmooth = AskOracle( p, q ); ..... ..... ..... ..... ..... ..... (6) f2 i,2j f1 i,2j+1 f2 
i-1,j .1 i-1,j 00 1 1 -13 -31 v = 2 if( ParentLevelsmooth || i == L ) kf,f = Quadrature( k, p, q ); 
-11 1 -1 f2 .2 i,2j+1 i-1,j The top two rows of the matrix in the above equation are chosen to give us 
box functions twice as wide. The bottom two rows are chosen to be orthogonal to constant and linear variation, 
(the vectors [1, 1, 1, 1], [0, 1, 2, 3])5. For a discussion of a similar con­struction see [2]. Both 
.atlets and multiwavelets can be constructed to have any number of vanishing moments to increase the 
sparseness of the integral operator representation. For both bases, the case M =1 reduces to the Haar 
basis. For M> 1 multiwavelets offer the bene.ts of projecting into a higher order space, resulting in 
in­creased convergence rates and smoother basis functions to repre­sent the answer. These bene.ts come 
at the expense of higher order quadratures necessary for the inner products. Flatlets for M> 1 also offer 
accelerated convergence while the quadratures remain equivalent to form factor computations for which 
there ex­its a large body of literature and code, and for which some closed form solutions are known. 
The .nal answer is still represented as a piecewise constant function, albeit at the .nest resolution 
fL,j . Since the degree of the basis functions does not go up in the .atlet case the width of support 
needs to be increased as M increases. With multiwavelets and .atlets there is also a cost incurred by 
increasing the number of vanishing moments. Larger M will result in h and g .lters with wider support. 
Thus any non-smoothness in k(s, t), such as a shadow discontinuity, will fall under the support of more 
basis functions. This increases the number of signi.cant terms in the integral operator. 5A technical 
detail concerns the fact that .atlets for M> 1 are not orthonormal and thus require the dual basis functions 
to compute PyramidUp (see [20]). CreateLink( kf,f,p,q ); else ProjectKernel( i+1, left(p), left(q) 
); ProjectKernel( i+1, left(p), right(q) ); ProjectKernel( i+1, right(p), left(q) ); ProjectKernel( 
i+1, right(p), right(q) ); In our implementation of radiosity using the MM and FM bases, f the radiosity 
function over each polygon is represented by Bff coef.cients that are stored in a quad-tree. Each node 
holds M2 Bff coef.cients. Pulling and pushing are done in the quad-tree as in [11] except that for different 
bases, we use different two-scale relationships. The kernel is represented by its kffff coef.cients that 
are stored on links created between nodes of different poly­gons quad-trees. Each such link caries M4 
interaction terms. For the FM bases the interaction terms are still form factors, but for MM the coef.cients 
on the links represent higher order inter­actions which require quadrature computations of the appropriate 
order. Gathering is done by moving B values across the links, weighted by the k values on the link. In 
this context, HR can be viewed as wavelet radiosity using the Haar basis.  7.3 Oracle The oracle must 
decide whether the kernel is suf.ciently smooth over two patches in the environment i.e., resembles a 
polynomial of degree M - 1 or less. If the kernel is smooth, all . terms will (suf.ciently) vanish and 
thus any work to evaluate the lower interaction terms can be avoided. The most accurate approach to measure 
the kernel smoothness is to directly evaluate the integrals of the kernel against the . on this and all 
lower levels and verify that they are below the required threshold. This is computationally too expensive 
and we approximate this computation in the following way. The kernel is sampled at the points required 
by a Gauss-Legendre quadra­ture rule of the appropriate order and an interpolating polynomial of degree 
M - 1 is constructed using Neville s algorithm [22]. Given this interpolating polynomial kP we compute 
the L1 error |kP - k| with a quadrature rule which places sample points in­between the previously chosen 
points. If the value of this integral Figure 8: Two different oracles and the interaction patterns they 
generate. is small we conclude that our current level of (smooth) approx­imation matches the kernel function 
well and the AskOracle function returns True. Note that the sample points for the inter­polating polynomial 
are chosen so that they can be used directly in the computation of the interaction link values. If the 
AskOracle function returns Falsethese samples are discarded. A less costly approach could use geometric 
information, such as the size, ori­entation, and distance between two patches. In effect this was done 
in the original HR implementation. However for the FM and MM , M> 1 bases it is not immediately clear 
what the corresponding geometric reasoning would be. It is important to realize that any such implementation 
of an oracle will introduce errors due to its approximate nature. If the oracle is not stringent enough, 
and necessary terms are neglected, artifacts will appear in the image. Figure 8 shows two differ­ent 
oracles and the interactions they force. Two successive levels of interactions are shown (top to bottom). 
On the left is an or­acle allowing patches close to the singularity (where the kernel varies rapidly) 
to be linked (meaning no further subdivision will be done). For this oracle the interaction patterns 
separate on the lower level. On the right is a more stringent oracle which does not allow singular interactions 
until patches have become very small. As a result we do not see the separation. As in [11] we use brightness 
re.nement which means that the stringency of the oracle is weighted by the brightness of the in­volved 
patches. Also as in [11] a fast partial visibility test is performed by using a constant number of jittered 
rays. If two patches are partially occluded and there is suf.cient energy being transferred between the 
two patches the oracle returns False.  7.4 Quadrature If the oracle returns True, numerical integrations 
must be per­formed to compute the kffff terms associated with the link to be created. Our implementation 
uses Gauss-Legendre quadrature [22] for this purpose. A Gauss-Legendre quadrature rule provides an accurate 
integration for polynomials up to order 2p - 1, where p is the number of sample points. The order of 
the quadrature and the related number of sample points required depends on the sum of the order of the 
wavelet bases, and the assumed order of the kernel itself. For the projection of the kernel against a 
.atlet basis, a two point rule is used for each constant section of the basis function. In the case of 
multiwavelets MM , M> 1, M points are chosen along each coordinate axis since we need to have a high 
enough order of integration to account for the polynomial variance in the kernel and the polynomial basis 
functions themselves. For example, for M = 3 we compute coef.cients when the kernel varies approximately 
up to 2nd by projecting onto basis functions Figure 9: Relative L1 error as a function of the number 
of interac­ 111 1 tion links for the haar basis with h =4 , 8 , (top to bottom). 16 , 32 The test con.guration 
is depicted in the upper right corner. 1.00000 0.10000 relative error 0.01000 upto 2nd order. Thus the 
integrand is approximately 4th order, and we can use a three point Gauss rule. The number of integrals 
which need to be computed for a link is M 4, however for all these integrals only a total of M4 samples 
of the kernel function are required. Using precomputed weights, these samples are combined to give all 
the desired integrals. We treat visibility following [11] by casting a constant number of jittered rays 
between two patches to estimate the fraction of visibility. This is then used to attenuate the quantity 
returned by the Gauss-Legendre quadrature. This technique relies on the fact that we always subdivide 
in the vicinity of a shadow discontinuity limiting errors due to the non-smooth nature of the kernel 
to a small region. When the two patches that are linked up are close to the singu­larity in k, quadratures 
will encounter numerical dif.culties if they are not properly adapted to the singularity. In particular 
a Gauss-Legendre rule will produce large errors and an adapted quadrature rule is required. This phenomenon 
is not unique to wavelet ra­diosity but applies to all GR methods. Special Gauss rules can be designed 
for the particular singularity found in the radiosity ker­nel. Zatz [25] uses such custom rules and notes 
the need for an automatic decision procedure as to when to switch the type of in­tegration. In our implementation 
of .atlets, we use a closed form solution for the form factor [21] whenever the patches border on the 
singularity. While this computation is expensive, it only needs to be invoked in a small fraction of 
interaction computations and contributes little to overall runtime. For multiwavelets we have no such 
closed form available. In this case the oracle forces subdivi­ 0.00100 0.00010 0.00001 Figure 10: Relative 
L1 error as a function of the number of interactions for the wavelet bases M1, M2, M3, and M4 (top to 
bottom) using the same test con.guration as in Figure 9. Here 1 h = 32 . sion to small enough patches 
at the singularity that the resulting errors contribute very little to the overall error. Alternative 
con­structions for singular transports are discussed in [19].  8 Experimental Results In this section 
we present .ndings that compare how radiosity behaves using different wavelet bases. We give results 
from the analysis of a simple 3D con.guration, for which we have an an­alytic solution against which 
to check our results. We .nish with an image of a full environment. One test case used the con.guration 
depicted in the inset in relative error 1.00000 0.10000 0.01000 0.00100 0.00010 0.00001 1000 10000 100000 
1000000 10000000 kernel evaluations Figure 11: Relative L1 error as a function of work. Figure 12: Computed 
image of perpendicular emitter and receiver. for the Haar basis (left), and F2 basis (right) using same 
amount of work. Note that we have not performed any post processing such as Gouraud shading. Figure 9. 
A pure emitter of side length 1 is placed 0.1 units above a pure receiver of side length two. For this 
particular con.guration the radiosity on the receiver is given by the differential area to .nite area 
form factor at every point. Figure 9 shows the behavior of the relative L1 error for the Haar basis as 
a function of the number of interactions for various grid sizes h. The far point on each of the lines 
corresponds to a full matrix solution. Note in particular that the .nal accuracy is reached well before 
all matrix elements are computed. Plots for higher order basis functions exhibit the same overall shape 
but with steeper slopes and overall lesser error. Figure 10 shows the behavior of the MM bases for M 
=1,..., 4 and h = 1 The ratio of successive slopes (as 32 . .tted to the points) is almost precisely 
1 : 2 : 3 : 4, as one would expect from the order of basis functions employed. For both plots we have 
depicted error as a function of number of interactions. However a user experiences error as a function 
of work which is more accurately measured by the number of kernel evaluations. Since the amount of work 
increases for higher order methods it is not clear a priori whether a higher order method will always 
yield better results in a shorter time. Figure 11 shows error as a function of kernel evaluations for 
the same data as that used in Figure 10. The plot for M = 1 is translated with respect to all others 
since we always use at least a two point quadrature rule even if the basis functions are constant. The 
plot shows that if Exact answer Error: Haar basis 11 0.050.05 0.50.5 0 0 -0.05-0.05 0.050.05 0.050.05 
0 0 -0.05-0.05 -0.05-0.05 Error: F2 basis Error: F3 basis Figure 13: Height.eld error plots for perpendicular 
emitter and receiver. Figure 14: Architectural scene computed with the M2 basis and rendered directly 
from the basis functions. suf.cient accuracy is required higher order basis functions achieve lower error 
for the same amount of work. We have also examined the behavior of our methods near the singularity of 
an environment consisting of perpendicular poly­gons (Figure 12). The emitter was chosen to be half as 
wide as the receiver to create more variation in the radiosity function. The grid size was set to 1 The 
upper left plot in Figure 13 shows the 32 . exact solution plotted as a height .eld over the receiver. 
On the top right is a plot of the difference between exact solution and the computed solution for the 
Haar basis. On the bottom are similar error surfaces for the F2 and F3 bases (left and right respectively). 
The amount of work was approximately constant (8000 interac­tions) for all three solutions. The graphs 
show clearly the lesser and smoother error for the F2 and F3 bases demonstrating the effectiveness of 
bases with more vanishing moments. This is also illustrated by the rendered images in Figure 12. The 
algorithm has also been run on a more complex environ­ment (Figure 14). This picture, as well as Figure 
12, does not use any postprocessing such as Gouraud shading. Instead the sur­face brightness is computed 
directly from the basis functions and associated coef.cients.   Conclusion and Future Work In this 
paper we have presented the basic theory of projections of integral operators into hierarchical bases, 
and laid out the theoreti­cal foundation of a new set of techniques involving wavelets. With this in 
hand, we introduced a new set of linear time algorithms we have called wavelet radiosity, and shown that 
the hierarchical radiosity described by Hanrahan et al. was an instance of a .rst order wavelet approach. 
We have introduced a new family of wavelets, dubbed .atlets and also experimented with a second family 
of wavelets, multi­wavelets. Both lead to ef.cient algorithms. Future work includes examining various 
wavelet bases which may have better proper­ties than the multiwavelets and .atlets. For example the Coi.et 
functions of [7, 3] allow for fast one point quadrature methods. The tree wavelets that we implemented 
do not enforce any kind of continuity at element boundaries, possibly leading to blocky artifacts. Spline 
wavelets [4] might provide a basis which would alleviate this. While our initial implementation was limited 
to quadrilateral polygons there is nothing in the underlying algorithms that pre­vents the use of any 
surface whose parameter domain is rectilinear, such as for example bicubic patches. The only change involves 
the reparameterization (change of variable) in the coupling integrals. It would be very desirable to 
design bases which work with trian­gular domains since triangles are a common primitive in meshing algorithms. 
There are still fundamental questions that have yet to be ad­dressed. We would like to gain a better 
understanding of how wavelet expansions interact with the visibility term in the kernel. It is also important 
to .nd methods that remain ef.cient when the environment consists of a large number of small polygons. 
 Acknowledgements The research reported here was partially supported by Apple, Sili­con Graphics Computer 
Systems, and the National Science Foun­dation (CCR 9207966). We would like to thank S. V. Krishnan for 
his useful comments. We would also like to thank Ju-sung Lee, Jonathan McAllister, and Michael Neufeld 
for creating the model of the room. References [1] Alpert, B. A Class of Bases in L2 for the Sparse 
Representation of Integral Operators. SIAM Journal on Mathematical Analysis 24,1 (Jan 1993). [2] Alpert, 
B., Beylkin, G., Coifman, R., and Rokhlin, V. Wavelet-like Bases for the Fast Solution of Second-kind 
Integral Equations. SIAM Journal on Scienti.c Computing 14, 1 (Jan 1993). [3] Beylkin, G., Coifman, R., 
and Rokhlin, V. Fast Wavelet Transforms and Numerical Algorithms I. Communications on Pure and Applied 
Mathematics 44 (1991), 141 183. [4] Chui, C. K. An Introduction to Wavelets, vol. 1 of Wavelet Analysis 
and its Applications. Academic Press Inc., 1992. [5] Cohen, M., Chen, S. E., Wallace, J. R., and Greenberg, 
D. P. A Progressive Re.nement Approach to Fast Radiosity Image Generation. Computer Graphics 22, 4 (August 
1988), 75 84. [6] Cohen, M. F., and Greenberg, D. P. The Hemi-Cube: A Radiosity Solution for Complex 
Environments. Computer Graphics 19, 3 (July 1985), 31 40. [7] Daubechies, I. Ten Lectures on Wavelets, 
vol. 61 of CBMS-NSF Regional Conference Series in Applied Mathematics. SIAM, 1992. [8] Delves, L. M., 
and Mohamed, J. L. Computational Methods for Integral Equations. Cambridge University Press, 1985. [9] 
Goral, C. M., Torrance, K. E., Greenberg, D. P., and Battaile, B. Modelling the Interaction of Light 
between Diffuse Surfaces. Computer Graphics 18, 3 (July 1984), 212 222. [10] Gortler, S. J., Cohen, M. 
F., and Slusallek, P. Radios­ity and Relaxation Methods; Progressive Re.nement is Southwell Relaxation. 
Tech. Rep. CS-TR-408-93, Department of Computer Science, Princeton University, February 1993. [11] Hanrahan, 
P., Salzman, D., and Aupperle, L. A Rapid Hierarchical Radiosity Algorithm. Computer Graphics 25, 4 (July 
1991), 197 206. [12] Heckbert, P. S. Simulating Global Illumination Using Adaptive Meshing. PhD thesis, 
University of California at Berkeley, January 1991. [13] Heckbert, P. S. Radiosity in Flatland. Computer 
Graphics Forum 2, 3 (1992), 181 192. [14] Kajiya, J. T. The Rendering Equation. Computer Graphics 20,4 
(1986), 143 150. [15] Lischinski, D., Tampieri, F., and Greenberg, D. P. A Dis­continuity Meshing Algorithm 
for Accurate Radiosity. IEEE CG&#38;A 12, 4 (July 1992). [16] Mallat, S. G. A Theory for Multiresolution 
Signal Decompo­sition: The Wavelet Representation. IEEE Transactions on Pattern Analysis and Machine 
Intelligence 11 (July 1989), 674 693. [17] Press, W., Teukolski, S., Vetterling, W., and Flan­nery, B. 
Numerical Recipies in C, The Art of Scienti.c Computing, 2 ed. Cambridge University Press, 1992. [18] 
Salesin, D., Lischinski, D., and DeRose, T. Reconstructing Illumination Functions with Selected Discontinuities. 
Third Euro­graphics Workshop on Rendering (1992), 99 112. [19] Schr¨oder, P. Numerical Integration for 
Radiosity in the Presence of Singularities. In Fourth Eurographics Workshop on Rendering (1993). [20] 
Schr¨oder, P., Gortler, S. J., Cohen, M. F., and Hanra­han, P. Wavelet Projections For Radiosity. In 
Fourth Eurographics Workshop on Rendering (June 1993). [21] Schr¨oder, P., and Hanrahan, P. On The Form 
Factor Be­tween Two Polygons. In Computer Graphics, Annual Conference Series, 1003 (August 1993), Siggraph. 
[22] Stoer, J., and Bulirsch, R. Introduction to Numerical Analy­sis. Springer Verlag, New York, 1980. 
[23] Szeliski, R. Fast Surface Interpolation Using Hierarchical Basis Functions. IEEE Trans. PAMI 12, 
6 (June 1990), 513 439. [24] Yserentant, H. On the Multi-level Splitting of Finite Element Spaces. Numerische 
Mathematik 49 (1986), 379 412. [25] Zatz, H. R. Galerkin Radiosity: A Higher-order Solution Method for 
Global Illumination. In Computer Graphics, Annual Conference Series, 1003 (August 1993), Siggraph.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166147</article_id>
		<sort_key>231</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>30</seq_no>
		<title><![CDATA[Hierarchical Z-buffer visibility]]></title>
		<page_from>231</page_from>
		<page_to>238</page_to>
		<doi_number>10.1145/166117.166147</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166147</url>
		<keywords>
			<kw><![CDATA[Z buffer]]></kw>
			<kw><![CDATA[octree]]></kw>
			<kw><![CDATA[pyramid]]></kw>
			<kw><![CDATA[spatial coherence]]></kw>
			<kw><![CDATA[temporal coherence]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Hidden line/surface removal</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Parallel processing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP43125610</person_id>
				<author_profile_id><![CDATA[81339502048]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ned]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Greene]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39074126</person_id>
				<author_profile_id><![CDATA[81100215003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kass]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39076011</person_id>
				<author_profile_id><![CDATA[81332515728]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gavin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807479</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[S.M. Rubin and T. Whitted. A 3-dimensional representation for fast rendering of complex scenes. Computer Graphics, 14(3):110-1 16, July 1980.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[A. Glassner. Space subdivision for fast ray tracing. IEEE CG&amp;A, 4(10):15-22, Oct. 1984.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[D. Jevans and B. Wyvill. Adaptive voxel subdivision for ray tracing. Proc. Graphics Interface '89, 164-172, June 1989.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15916</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[T. Kay and J. Kajiya. Ray tracing complex surfaces. Computer Graphics, 20(4):269-278, Aug. 1986.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[M. Kaplan. The use of spatial coherence in ray tracing. In Techniques for Computer Graphics, etc., D. Rogers and R. A. Earnshaw, Springer-Verlag, New York, 1987.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357302</ref_obj_id>
				<ref_obj_pid>357299</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[H. Hubschman and S. W. Zucker. Frame to frame coherence and the hidden surface computation: constraints for a convex world. ACM TOG, 1(2):129-162, April 1982.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>155315</ref_obj_id>
				<ref_obj_pid>155294</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[D. Jevans. Object space temporal coherence for ray tracing. Proc. Graphics Interface '92, Vancouver, B.C., 176- 183, May 11-15, 1992.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617407</ref_obj_id>
				<ref_obj_pid>615999</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[A. Glassner. Spacetime ray tracing for animation. IEEE CG&amp;A, 8(3):60-70, March 1988.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>93315</ref_obj_id>
				<ref_obj_pid>93267</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[J. Chapman, T. W. Calvert, and J. Dill. Spatio-temporal coherence in ray tracing. Proceedings of Graphics Interface '90, 196-204, 1990.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[S. Badt, Jr. Two algorithms for taking advantage of temporal coherence in ray tracing The Visual Computer, 4:123-132, 1988.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[B. Gaflick, D. Baum, and J. Winget. Interactive viewing of large geometric databases using multiprocessor graphics workstations. SIGGRAPH '90 Course Notes: Parallel Algorithms and Architectures for 3D Image Generation, 1990.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[J. Airey. Increasing update rates in the building walkthrough system with automatic model-space subdivision. Technical Report TR90-027, The University of North Carolina at Chapel Hill, Department of Computer Science, 1990.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91416</ref_obj_id>
				<ref_obj_pid>91394</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[J. Airey, J. Rohlf, and F. Brooks. Towards image realism with interactive update rates in complex virtual building environments. ACM SIGGRAPH Special Issue on 1990 Symposium on Interactive 3D Graphics, 24(2):41-50, 1990.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122725</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[S. Teller and C. Sequin. Visibility preprocessing for interactive walkthroughs. Computer Graphics, 25(4):61-69, 1991.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>894565</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[S. Teller and C. Sequin. Visibility computations in polyhedral three-dimensional environments. U.C. Berkeley Report No. UCB/CSD 92/680, April 1992.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[D. Meagher. Efficient synthetic image generation of arbitrary 3-D objects. Proc. IEEE Conf. on Pattern Recognition and Image Processing, 473-478, June 1982.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Hierarchical Z-Buffer Visibility Ned Greene* Michael Kass Gavin Miller Abstract An ideal visibility 
algorithm should a) quickly reject most of the hidden geometry in a model and b) exploit the spatial 
and perhaps temporal coherence of the images being generated. Ray casting with spatial subdivision does 
well on criterion (a), but poorly on criterion (b). Traditional Z-buffer scan conversion does well on 
criterion (b), but poorly on criterion (a). Here we present a hi­erarchical Z-buffer scan-conversion 
algorithm that does well on both criteria. The method uses two hierarchical data structures, an object-space 
octree and an image-space Z pyramid, to accelerate scan conversion. The two hierarchical data structures 
make it pos­sible to reject hidden geometry very rapidly while rendering visible geometry with the speed 
of scan conversion. For animation, the algorithm is also able to exploit temporal coherence. The method 
is well suited to models with high depth complexity, achieving orders of magnitude acceleration in some 
cases compared to ordi­nary Z-buffer scan conversion. CR Categories and Subject Descriptors: I.3.7 [Computer 
Graphics]: Three-Dimensional Graphics and Realism -Hid­den line/surface removal; J.6 [Computer-Aided 
Engineering]: Computer-Aided I.3.1 [Computer Graphics]: Hardware Architec­ture -Graphics Processors Additional 
Key Words and Phrases: Octree, Pyramid, Temporal Coherence, Spatial Coherence, Z Buffer. 1 Introduction 
Extremely complex geometric databases offer interesting chal­lenges for visibility algorithms. Consider, 
for example, an interac­tive walk-through of a detailed geometric database describing an entire city, 
complete with vegetation, buildings, furniture inside the buildings and the contents of the furniture. 
Traditional visi­bility algorithms running on currently available hardware cannot come close to rendering 
scenes of this complexity at interactive rates and it will be a long time before faster hardware alone 
will suffice. In order to get the most out of available hardware, we need faster algorithms that exploit 
properties of the visibility computa­tion itself. There are at least three types of coherence inherent 
in the visi­ *Apple Computer, U.C. Santa Cruz Apple Computer Permission to copy without fee all or part 
of this material is granted provided that the copies are not made or distributed for direct provided 
that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice 
and the title of the commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy permission 
of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or 
specific permission. otherwise, or to republish, requires a fee and/or specific permission. &#38;#169;1993 
ACM-0-89791-601-8/93/008/0015 $1.50 &#38;#169;1993 ACM-0-89791-601-8/93/008 $1.50 bility computation 
which can be exploited to accelerate a visibility algorithm. The first is object-space coherence: in 
many cases a single computation can resolve the visibility of a collection of objects which are near 
each other in space. The second is image­space coherence: in many cases a single computation can resolve 
the visibility of an object covering a collection of pixels. The third is temporal coherence: visibility 
information from one frame can often be used to accelerate visibility computation for the next frame. 
Here we present a visibility algorithm which exploits all three of these types of coherence and sometimes 
achieves orders of magnitude acceleration compared with traditional techniques. The dominant algorithms 
in use today for visibility computa­tions are Z-buffer scan conversion and ray-tracing. Since Z buffers 
do not handle partially transparent surfaces well, we will restrict the discussion to models consisting 
entirely of opaque surfaces. For these models, only rays from the eye to the first surface are relevant 
for visibility, so the choice is between Z buffering and ray-casting (ray-tracing with no secondary rays). 
Traditional Z buffering makes reasonably good use of image­space coherence in the course of scan conversion. 
Implementa­tions usually do a set-up computation for each polygon and then an incremental update for 
each pixel in the polygon. Since the incre­mental update is typically much less computation than the 
set-up, the savings from image-space coherence can be substantial. The problem with the traditional Z-buffer 
approach is that it makes no use at all of object-space or temporal coherence. Each polygon is rendered 
independently, and no information is saved from prior frames. For extremely complex environments like 
a model of a city, this is very inefficient. A traditional Z-buffer algorithm, for example, will have 
to take the time to render every polygon of ev­ery object in every drawer of every desk in a building 
even if the whole building cannot be seen, because the traditional algorithm can resolve visibility only 
at the pixel level. Traditional ray-tracing or ray-casting methods, on the other hand, make use of object-space 
coherence by organizing the ob­jects in some type of spatial subdivision. Rays from the eye are propagated 
through the spatial subdivision until they hit the first visible surface. Once a ray hits a visible surface, 
there is no need to consider any of the surfaces in the spatial subdivisions further down along the ray, 
so large portions of the geometry may never have to be considered during rendering. This is an important 
im­provement on Z buffering, but it makes no use of temporal or image-space coherence. While ray-casting 
algorithms that exploit temporal coherence have been explored, it seems extremely dif­ficult to exploit 
image-space coherence in traditional ray casting algorithms. Here we present a visibility algorithm which 
combines the strengths of both ray-casting and Z buffering. To exploit object­space coherence, we use 
an octree spatial subdivision of the type commonly used to accelerate ray tracing. To exploit image-space 
coherence, we augment traditional Z-buffer scan conversion with an image-space Z pyramid that allows 
us to reject hidden geom­etry very quickly. Finally, to exploit temporal coherence, we use the geometry 
that was visible in the previous frame to construct a starting point for the algorithm. The result is 
an algorithm which is orders of magnitude faster than traditional ray-casting or Z buffer­ing for some 
models we have tried. The algorithm is not difficult to implement and works for arbitrary polygonal databases. 
In section II, we survey the most relevant prior work on accel­erating ray casting and scan conversion. 
In section III, we develop the data structures used to exploit object-space, image-space and temporal 
coherence. In section IV, we describe the implementation and show results for some complex models containing 
hundreds of millions of polygons. Prior Work There have been many attempts to accelerate traditional 
ray-tracing and Z buffering techniques. Each of these attempts exploits some aspect of the coherence 
inherent in the visibility computation it­self. None of them, however, simultaneously exploits object­space, 
image-space and temporal coherence. The ray-tracing literature abounds with references to object­space 
coherence. A variety of spatial subdivisions have been used to exploit this coherence and they seem to 
work quite well (e.g. [1, 2, 3, 4, 5]). Temporal coherence is much less commonly ex­ploited in practice, 
but various techniques exist for special cases. If all the objects are convex and remain stationary while 
the cam­era moves, then there are constraints on the way visibility can change[6] which a ray tracer 
might exploit. On the other hand, if the camera is stationary, then rays which are unaffected by the 
motion of objects can be detected and used from the previous frame[7]. When interactivity is not an issue 
and sufficient mem­ory is available, it can be feasible to render an entire animation sequence at once 
using spacetime bounding boxes[8, 9]. While these techniques make good use of object-space coherence 
and sometimes exploit temporal coherence effectively, they unfortu­nately make little or no use of image-space 
coherence since each pixel is traced independently from its neighbors. There are heuris­tic methods which 
construct estimates of the results of ray-tracing a pixel from the results at nearby pixels (e.g. [10]), 
but there seems to be no guaranteed algorithm which makes good use of image-space coherence in ray tracing. 
With Z-buffer methods (and scan conversion methods in gen­eral) the problems are very different. Ordinary 
Z-buffer rendering is usually implemented with an initial set-up computation for each primitive followed 
by a scan-conversion phase in which the af­fected pixels are incrementally updated. This already makes 
very good use of image-space coherence, so the remaining challenge with Z-buffer methods is to exploit 
object-space and temporal co­herence effectively. A simple method of using object-space coherence in 
Z-buffer rendering is to use a spatial subdivision to cull the model to the viewing frustum [11]. While 
this can provide substantial accelera­tion, it exploits only a small portion of the object-space coherence 
in models with high depth complexity. In architectural models, for example, a great deal of geometry 
hidden behind walls may lie within the viewing frustum. In order to make use of more of the object-space 
coherence in architectural models, Airey et. al. [12, 13] and subsequently Teller and Sequin[15] proposed 
dividing models up into a set of disjoint cells and precomputing the potentially visible set (PVS) of 
polygons from each cell. In order to render an image from any viewpoint within a cell, only the polygons 
in the PVS need be considered. These PVS schemes are the closest in spirit to the visibility algorithm 
presented here since they attempt to make good use of both object-space and image-space coherence. Nonetheless, 
they suffer from some important limitations. Before they can be used at all, they require an expensive 
precomputation step to de­termine the PVS and a great deal of memory to store it. Teller and Sequin, 
for example, report over 6 hours of precomputation time on a 50 MIP machine to calculate 58Mb of PVS 
data needed for a model of 250,000 polygons[15]. Perhaps more importantly, the way these methods make 
use of cells may limit their appro­priateness to architectural models. In order to achieve maximum acceleration, 
the cells must be 3D regions of space which are al­most entirely enclosed by occluding surfaces, so that 
most cells are hidden from most other cells. For architectural models, this often works well since the 
cells can be rooms, but for outdoor scenes and more general settings, it is unclear whether or not PVS 
methods are effective. In addition, the currently implemented al­gorithms make very special use of axially-aligned 
polygons such as flat walls in rectilinear architectural models. While the methods can in principle be 
extended to use general 3D polygons for oc­clusion, the necessary algorithms have much worse computational 
complexity[15]. Finally, although the implementations prefetch PVS data for nearby cells to avoid long 
latencies due to paging, they cannot be said to exploit temporal coherence in the visibility computation 
very effectively. The algorithm presented here shares a great deal with the work of Meagher[16] who used 
object-space octrees with image-space quadtrees for rendering purposes. Meagher tried to display the 
octree itself rather than using it to cull a polygonal database, so his method is directly applicable 
to volume, rather than surface models. Nonetheless his algorithm is one of the few to make use of both 
object-space and image-space coherence. The algorithm does not exploit temporal coherence. 3 Hierarchical 
Visibility The hierarchical Z-buffer visibility algorithm uses an octree spa­tial subdivision to exploit 
object-space coherence, a Z pyramid to exploit image-space coherence, and a list of previously visible 
oc­tree nodes to exploit temporal coherence. While the full value of the algorithm is achieved by using 
all three of these together, the object-space octree and the image-space Z pyramid can also be used separately. 
Whether used separately or together, these data structures make it possible to compute the same result 
as ordinary Z buffering at less computational expense. 3.1 Object-space octree Octrees have been used 
previously to accelerate ray tracing[5] and rendering of volume data sets[16] with great effectiveness. 
With some important modification, many of the principles of these previous efforts can be applied to 
Z-buffer scan conversion. The result is an algorithm which can accelerate Z buffering by orders of magnitude 
for models with sufficient depth complexity. In order to be precise about the octree algorithm, let us 
begin with some simple definitions. We will say that a polygon is hidden with respect to a Z buffer if 
no pixel of the polygon is closer to the observer than the Z value already in the Z buffer. Similarly, 
we will say that a cube is hidden with respect to a Z buffer if all of its faces are hidden polygons. 
Finally, we will call a node of the octree hidden if its associated cube is hidden. Note that these definitions 
depend on the sampling of the Z buffer. A polygon which is hidden at one Z-buffer resolution may not 
be hidden at another. With these definitions, we can state the basic observation that makes it possible 
to combine Z buffering with an octree spatial subdivision: If a cube is hidden with respect to a Z buffer, 
then all polygons fully contained in the cube are also hidden. What this means is the following: if we 
scan convert the faces of an octree cube and find that each pixel of the cube is behind the current surface 
in the Z buffer, we can safely ignore all the geometry contained in that cube. From this observation, 
the basic algorithm is easy to construct. We begin by placing the geometry into an octree, associating 
each primitive with the smallest enclosing octree cube. Then we start at the root node of the octree 
and render it using the following recursive steps: First, we check to see if the octree cube intersects 
the viewing frustum. If not, we are done. If the cube does intersect the viewing frustum, we scan convert 
the faces of the cube to determine whether or not the whole cube is hidden. If the cube is hidden, we 
are done. Otherwise, we scan convert any geometry associated with the cube and then recursively render 
its children in front-to-back order. We can construct the octree with a simple recursive procedure. Beginning 
with a root cube large enough to enclose the entire model and the complete list of geometric primitives, 
we recur­sively perform the following steps: If the number of primitives is sufficiently small, we associate 
all of the primitives with the cube and exit. Otherwise, we associate with the cube any primi­tive which 
intersects at least one of three axis-aligned planes that bisect the cube. We then subdivide the octree 
cube and call the procedure recursively with each of the eight child cubes and the portion of the geometry 
that fits entirely in that cube. The basic rendering algorithm has some very interesting prop­erties. 
First of all, it only renders geometry contained in octree nodes which are not hidden. Some of the rendered 
polygons may be hidden, but all of them are nearly visible in the following sense: there is some place 
we could move the polygon where it would be visible which is no further away than the length of the diagonal 
of its containing octree cube. This is a big improvement over merely culling to the viewing frustum. 
In addition, the algo­rithm does not waste time on irrelevant portions of the octree since it only visits 
octree nodes whose parents are not hidden. Finally, the algorithm never visits an octree node more than 
once during rendering. This stands in marked contrast to ray-tracing through an octree where the root 
node is visited by every pixel and other nodes may be visited tens of thousands of times. As a result 
of these properties, the basic algorithm culls hidden geometry very efficiently. A weakness of the basic 
algorithm is that it associates some small geometric primitives with very large cubes if the primitives 
happen to intersect the planes which separate the cube's children. A small triangle which crosses the 
center of the root cube, for example, will have to be rendered anytime the entire model is not hidden. 
To avoid this behavior, there are two basic choices. One alternative is to clip the problematic small 
polygons so they fit in much smaller octree cells. This has the disadvantage of increasing the number 
of primitives in the database. The other alternative is to place some primitives in multiple octree cells. 
This is the one we have chosen to implement. To do this, we modify the recursive construction of the 
octree as follows. If we find that a primitive intersects a cube's dividing planes, but is small compared 
to the cube, then we no longer associate the primitive with the whole cube. Instead we associate it with 
all of the cube's children that the primitive intersects. Since some primitives are associated with more 
than one octree node, we can encounter them more than once during rendering. The first time we render 
them, we mark them as rendered, so we can avoid rendering them more than once in a given frame. 3.2 
Image-space Z pyramid The object-space octree allows us to cull large portions of the model at the cost 
of scan-converting the faces of the octree cubes. Since the cubes may occupy a large number of pixels 
in the im­age, this scan conversion can be very expensive. To reduce the cost of determining cube visibility, 
we use an image-space Z pyra­mid. In many cases, the Z pyramid makes it possible to conclude very quickly 
a large polygon is hidden, making it unnecessary to examine the polygon pixel by pixel. The basic idea 
of the Z pyramid is to use the original Z buffer as the finest level in the pyramid and then combine 
four Z values at each level into one Z value at the next coarser level by choosing the farthest Z from 
the observer. Every entry in the pyramid therefore represents the farthest Z for a square area of the 
Z buffer. At the coarsest level of the pyramid there is a single Z value which is the farthest Z from 
the observer in the whole image. Maintaining the Z pyramid is an easy matter. Every time we modify the 
Z buffer, we propagate the new Z value through to coarser levels of the pyramid. As soon as we reach 
a level where the entry in the pyramid is already as far away as the new Z value, we can stop. In order 
to use the Z pyramid to test the visibility of a polygon, we find the finest-level sample of the pyramid 
whose correspond­ing image region covers the screen-space bounding box of the polygon. If the nearest 
Z value of the polygon is farther away than this sample in the Z pyramid, we know immediately that the 
polygon is hidden. We use this basic test to determine the visi­bility of octree cubes by testing their 
polygonal faces, and also to test the visibility of model polygons. While the basic Z-pyramid test can 
reject a substantial number of polygons, it suffers from a similar difficulty to the basic octree method. 
Because of the structure of the pyramid regions, a small polygon covering the center of the image will 
be compared to the Z value at the coarsest level of the pyramid. While the test is still accurate in 
this case, it is not particularly powerful. A definitive visibility test can be constructed by applying 
the basic test recursively through the pyramid. When the basic test fails to show that a polygon is hidden, 
we go to the next finer level in the pyramid where the previous pyramid region is divided into four quadrants. 
Here we attempt to prove that the polygon is hidden in each of the quadrants it intersects. For each 
of these quadrants, we compare the closest Z value of the polygon in the quadrant to the value in the 
Z pyramid. If the Z-pyramid value is closer, we know the polygon is hidden in the quadrant. If we fail 
to prove that the primitive is hidden in one of the quadrants, we go to the next finer level of the pyramid 
for that quadrant and try again. Ultimately, we either prove that the entire polygon is hidden, or we 
recurse down to the finest level of the pyramid and find a visible pixel. If we find all visible pixels 
this way, we are performing scan conversion hierarchically. A potential difficulty with the definitive 
visibility test is that it can be expensive to compute the closest Z value of the polygon in a quadrant. 
An alternative is to compare the value in the pyramid to the closest Z value of the entire polygon at 
each step of the recursion. With this modification, the test is faster and easier to implement, but no 
longer completely definitive. Ultimately, it will either prove that the entire polygon is hidden, or 
recurse down to the finest level of the pyramid and find a pixel it cannot prove is hidden. Our current 
implementation uses this technique. When the test fails to prove that a polygon is hidden, our implementa­tion 
reverts to ordinary scan conversion to establish the visibility definitively. 3.3 Temporal coherence 
list Frequently, when we render an image of a complex model using the object-space octree, only a small 
fraction of the octree cubes are visible. If we render the next frame in an animation, most of the cubes 
visible in the previous frame will probably still be visi­ble. Some of the cubes visible in the last 
frame will become hidden and some cubes hidden in the last frame will become visible, but frame-to-frame 
coherence in most animations ensures that there will be relatively few changes in cube visibility for 
most frames (except scene changes and camera cuts). We exploit this fact in a very simple way with the 
hierarchical visibility algorithm. We maintain a list of the visible cubes from the previous frame, the 
temporal coherence list, and simply render all of the geometry on the list, marking the listed cubes 
as rendered, before commencing the usual algorithm. We then take the resulting Z buffer and use it to 
form the initial Z pyramid. If there is sufficient frame-to-frame coherence, most of the visible geometry 
will already be rendered, so the Z-pyramid test will be much more effective than when we start from scratch. 
The Z-pyramid test will be able to prove with less recursion that octree cubes and model polygons are 
hidden. As we will see in section IV, this can accelerate the rendering pro­cess substantially. After 
rendering the new frame, we update the temporal coherence list by checking each of the cubes on the list 
for visibility using the Z-pyramid test. This prevents the temporal coherence list from growing too large 
over time. One way of thinking about the temporal coherence strategy is that we begin by guessing the 
final solution. If our guess is very close to the actual solution, the hierarchical visibility algorithm 
can use the Z pyramid to verify the portions of the guess which are correct much faster than it can construct 
them from scratch. Only the portions of the image that it cannot verify as being correct require further 
processing.  4 Implementation and Results Our initial implementation of the hierarchical visibility 
algorithm is based on general purpose, portable C code and software scan conversion. This implementation 
uses the object-space octree, the image-space Z pyramid and the temporal coherence list. Even for relatively 
simple models the pure software algorithm is faster than traditional software Z buffering, and for complex 
models the acceleration can be very large. In order to test the algorithm, we constructed an office module 
consisting of 15K polygons and then replicated the module in a three dimensional grid. Each module includes 
a stairway with a large open stairwell making it possible to see parts of the neigh­boring floors. None 
of the office walls extends to the ceiling, so from a high enough point in any of the cubicles, it is 
possible to see parts of most of the other cubicles on the same floor. For simple models with low depth 
complexity, the hierarchi­cal visibility method can be expected to take somewhat longer than traditional 
scan conversion due to the overhead of perform­ing visibility tests on octree cubes and the cost of maintaining 
a Z pyramid. To measure the algorithm's overhead on simple models, we rendered a single office module 
consisting of 15K polygons at a viewpoint from which a high proportion of the model was visible. Rendering 
time for a 512 by 512 image was 1.52 seconds with the hierarchical visibility method and 1.30 seconds 
with tradi­tional scan conversion, indicating a performance penalty of 17%. When we rendered three instances 
of the model (45K polygons), the running time was 3.05 seconds for both methods indicating that this 
level of complexity was the breakeven point for this partic­ular model. Hierarchical visibility rendered 
nine instances of the same model (105K polygons) in 5.17 seconds, while traditional scan conversion took 
7.16 seconds. The chief value of the hierarchical visibility algorithm is, of course, for scenes of much 
higher complexity. To illustrate the point, we constructed a 33 by 33 by 33 replication of the of­fice 
module which consists of 538 million polygons. The model is shown rendered in figure 1. 59.7 million 
polygons lie in the viewing frustum from this viewpoint, about one tenth of the entire model. Using the 
hierarchical visibility method, the Z-pyramid test was invoked on 1746 octree cubes and culled about 
27% of the polygons in the viewing frustum. The bounding boxes of 687 cubes were scan converted which 
culled nearly 73% of the model polygons in the viewing frustum, leaving only 83.0K polygons of which 
41.2K were front facing (.000076 of the total model) to be scan converted in software. On an SGI Crimson 
Elan, the entire process took 6.45 seconds. Rendering this model using traditional Z buffering on the 
Crimson Elan hardware took approximately one hour and fifteen minutes. Rendering it in software on the 
Crimson would probably take days. The center left panel of figure 1 shows the depth complexity processed 
by the algorithm for the image in the upper left. The depth complexity displayed in this image is the 
number of times each pixel was accessed in a box visibility test or in Z-buffer polygon scan conversion. 
Note the bright regions corresponding to portions of the image where it is possible to see far into the 
model; these are regions where the algorithm has to do the most work. In this image, the average depth 
complexity due to box scans is 7.23, and due to polygon scan-conversion is 2.48 for a total of 9.71. 
The maximum depth complexity is 124. Dividing the number of times the Z pyramid is accessed by the number 
of pixels on the screen lets us assign a value of .43 for the depth complexity of the Z-pyramid tests. 
Thus, the total average depth complexity of Z-pyramid tests, box scans and polygon scans is 10.14. Note 
that this is not the depth complexity of the model itself, but only the depth complexity of the hierarchical 
visibility computation. Computing the true depth complexity of the scene would require scan converting 
the entire model of 538 million polygons in software, which we have not done. In the lower left of figure 
1, we show the viewing frustum and the octree subdivision. The two long strings of finely divided boxes 
correspond to the two brightest regions in the depth complexity image. Note that the algorithm is able 
to prove that large octree nodes in the distance are hidden. In the lower right, we show the Z pyramid 
for the scene. Even at fairly coarse resolutions, the Z pyramid contains a recognizeable representation 
of the major occluders in the scene. The office environment of figure 1 was chosen in part because it 
is a particularly difficult model for PVS methods. From every office cubicle in this environment, there 
are points from which almost every other cubicle on the same floor is visible. As a result, if the cubicles 
were used as cells in a PVS method, the potentially visible set for each cell would have to include nearly 
all the cells on its floor and many on other floors. Since each floor contains about 4 million polygons, 
the PVS methods would probably have to render many more polygons than the hierarchical method. In addition, 
the precomputation time for published PVS methods would be prohibitive for a model of this complexity. 
This model has 2000 times as many polygons as the model described by Teller and Sequin[15] which required 
6 hours of pre-processing. Admittedly, the replication of a single cell in the model means that it may 
not be a representative example, but it will be some time before people use models of this complexity 
without a great deal of instancing. The hierarchical visibility program we used for this example makes 
use of the replication in only two ways. First, the algorithm does not need to store half a billion polygons 
in main memory. Second, the algorithm only needs to consider a single cell in constructing the octree. 
These same simplifications would 2048 1024 512 256 128 64 32 16 8 4 2 1 128643216 8 4 21 Fig. 3: Total 
time in seconds to render all windows as a func­ tion of the number of pixels on the side of each window. 
apply to any complex model using a great deal of instancing. Figure 2 shows the hierarchical visibility 
method applied to an outdoor scene consisting of a terrain mesh with vegetation repli­cated on a two-dimensional 
grid. The model used for the lower left image consists of 53 million polygons, but only about 25K polygons 
are visible from this point of view. Most of the model is hidden by the hill or is outside the viewing 
frustum. The corre­sponding depth complexity image for hierarchical visibility com­putations is shown 
at the top left. The algorithm works hardest near the horizon where cube visibility is most difficult 
to establish. This frame took 7 seconds to render with software scan conversion on an SGI Crimson. In 
the lower right, we show a model consist­ing of 5 million polygons. Even though the model is simpler 
than the model in the lower left, the image is more complicated and took longer to render because a much 
larger fraction of the model is visible from this point of view. This image took 40 seconds to render 
with software scan conversion on an SGI Crimson. The average depth complexity for the scene is 7.27, 
but it reaches a peak of 85 in the bright areas of the depth complexity image in the upper right. These 
outdoor scenes have very different character­istics from the building interiors shown in figure 1 and 
are poorly suited to PVS methods because (a) very few of the polygons are axis-aligned and (b) the cell-to-cell 
visibility is not nearly as lim­ited as in an architectural interior. Nonetheless, the hierarchical visibility 
algorithm continues to work effectively. 4.1 Parallelizability and Image-space coherence We have made 
our hierarchical visibility implementation capable of dividing the image into a grid of smaller windows, 
rendering them individually and compositing them into a final image. The performance of the algorithm 
as the window size is varied tells us about the parallel performance of the algorithm and the extent 
to which it makes use of image-space coherence. If, like most ray tracers, the algorithm made no use 
of image-space coherence, we could render each pixel separately at no extra cost. Then it would be fully 
parallelizable. At the other extreme, if the algorithm made the best possible use of image-space coherence, 
it would render a sizeable region of pixels with only a small amount more computation than required to 
render a single pixel. Then it would be difficult to parallelize. Note that if we shrink the window size 
down to a single pixel, the hierarchical visibility algorithm becomes a ray caster using an octree subdivision. 
Figure 3 graphs the rendering time for a frame from a walk­through of the model shown in figure 1 as 
a function of the window size. For window sizes from 32 by 32 on up, the curve is rela­tively flat, indicating 
that the algorithm should parallelize fairly well. For window sizes below 32 by 32, however, the slope 
of the curve indicates that the time to render a window is almost independent of the window size. The 
algorithm can, for example, render a 32 by 32 region for only slightly more than four times the computational 
expense of ray-casting a single pixel with this algo­rithm. Comparing the single pixel window time to 
the time for the whole image, we find that image-space coherence is responsible for a factor of almost 
300 in running time for this example. 4.2 Use of graphics hardware In addition to the pure software 
implementation, we have at­tempted to modify the algorithm to make the best possible use of available 
commercial hardware graphics accelerators. This raises some difficult challenges because the hierarchical 
visibil­ity algorithm makes slightly different demands of scan-conversion hardware than traditional Z 
buffering. In particular, the use of octree object-space coherence depends on being able to determine 
quickly whether any pixel of a polygon would be visible if it were scan converted. Unfortunately, the 
commercial hardware graphics pipelines we have examined are either unable to answer this query at all, 
or take milliseconds to answer it. One would certainly expect some delay in getting information back 
from a graphics pipeline, but hardware designed with this type of query in mind should be able to return 
a result in microseconds rather than milliseconds. We have implemented the object-space octree on a Kubota 
Pa­cific Titan 3000 workstation with Denali GB graphics hardware. The Denali supports an unusual graphics 
library call which deter­mines whether or not any pixels in a set of polygons are visible given the current 
Z buffer. We use this Z query feature to determine the visibility of octree cubes. The cost of a Z query 
de­pends on the screen size of the cube, and it can take up to several milliseconds to determine whether 
or not a cube is visible. Our implementation makes no use of the Z pyramid because the cost of getting 
the required data to and from the Z buffer would exceed any possible savings. On a walk-through of a 
version of the office model with 1.9 million polygons, the Titan took an average of .54 seconds per frame 
to render 512 by 512 images. Because of the cost of doing the Z query, we only tested visibility of octree 
cubes containing at least eight hundred polygons. Even so, 36.5% of the running time was taken up by 
Z queries. If Z query were faster, we could use it effectively on octree cubes containing many fewer 
polygons and achieve substantial further acceleration. The Titan implementation has not been fully optimized 
for the De­nali hardware and makes no use of temporal coherence, so these performance figures should 
be considered only suggestive of the machine's capabilities. The other implementation we have that makes 
use of graphics hardware runs on SGI workstations. On these workstations, there is no way to inquire 
whether or not a polygon is visible without rendering it, so we use a hybrid hardware/software strategy. 
We do the first frame of a sequence entirely with software. On the second frame, we render everything 
on the temporal coherence list with the hardware pipeline. Then we read the image and the Z buffer from 
the hardware, form a Z pyramid and continue on in software. With this implementation, on the models we 
have tried, temporal coherence typically reduces the running time by a factor of between 1.5 and 2. In 
the course of a walk-through of our office model, we rendered the frame in the upper left of figure 1 
without temporal coherence, and then the next frame shown in the upper right of figure 1 using temporal 
coherence. The new polygons rendered in software are shown in magenta for illustration. For the most 
part, these are polygons that came into view as a result of panning the camera. The center right shows 
the depth complexity of the hierarchical computation for this frame. The image is much darker in most 
regions because the algorithm has much less work to do given the previous frame as a starting point. 
This temporal coherence frame took 3.96 seconds to render on a Crimson Elan, as compared with 6.45 seconds 
to render the same frame without temporal coherence. Current graphics accelerators are not designed to 
support the rapid feedback from the pipeline needed to realize the full poten­tial of octree culling 
in the hierarchical visibility algorithm. Hard­ware designed to take full advantage of the algorithm, 
however, could make it possible to interact very effectively with extremely complex environments as long 
as only a manageable number of the polygons are visible from any point of view. The octree sub­division, 
the Z pyramid and the temporal coherence strategy are all suitable for hardware implementation.  5 
Conclusion As more and more complex models become commonplace in com­puter graphics, it becomes increasingly 
important to exploit the available coherence in the visibility computation. Here we present an algorithm 
which combines the ability to profit from image­space coherence of Z-buffer scan conversion with the 
ability of ray tracing to avoid considering hidden geometry. It appears to be the first practical algorithm 
which materially profits from object­space, image-space and temporal coherence simultaneously. The algorithm 
has been tested and shown to work effectively on indoor and outdoor scenes with up to half a billion 
polygons. The hierarchical visibility algorithm can make use of existing graphics accelerators without 
modification. Small changes in the design of graphics accelerators, however, would make a large dif­ference 
in the performance of the algorithm. We hope that the appeal of this algorithm will induce hardware designers 
to alter future graphics hardware to facilitate hierarchical visibility com­putations.  Acknowledgements 
We thank Frank Crow and the Advanced Technology Group at Apple Computer for supporting this research. 
We also thank Mike Toelle, Avi Bleiweiss, Helga Thorvaldsdottir and Mike Keller of Kubota Pacific Corporation 
for helping us test our algorithm on a Titan workstation.  References [1] S. M. Rubin and T. Whitted. 
A 3-dimensional representation for fast rendering of complex scenes. Computer Graphics, 14(3):110-1 16, 
July 1980. [2] A. Glassner. Space subdivision for fast ray tracing. IEEE CG&#38;A, 4(10):15-22, Oct. 
1984. [3] D. Jevans and B. Wyvill. Adaptive voxel subdivision for ray tracing. Proc. Graphics Interface 
'89 , 164-172, June 1989. [4] T. Kay and J. Kajiya. Ray tracing complex surfaces. Com­puter Graphics, 
20(4):269-278, Aug. 1986. [5] M. Kaplan. The use of spatial coherence in ray tracing. In Techniques for 
Computer Graphics, etc., D. Rogers and R. A. Earnshaw, Springer-Verlag, New York, 1987. [6] H. Hubschman 
and S. W. Zucker. Frame to frame coherence and the hidden surface computation: constraints for a convex 
world. ACM TOG, 1(2):129-162, April 1982. [7] D. Jevans. Object space temporal coherence for ray trac­ing. 
Proc. Graphics Interface '92 , Vancouver, B.C., 176­183, May 11-15, 1992. [8] A. Glassner. Spacetime 
ray tracing for animation. IEEE CG&#38;A, 8(3):60-70, March 1988. [9] J. Chapman, T. W. Calvert, and 
J. Dill. Spatio-temporal coherence in ray tracing. Proceedings of Graphics Interface '90 , 196-204, 1990. 
[10] S. Badt, Jr. Two algorithms for taking advantage of temporal coherence in ray tracing The Visual 
Computer, 4:123-132, 1988. [11] B. Garlick, D. Baum, and J. Winget. Interactive viewing of large geometric 
databases using multiprocessor graphics workstations. SIGGRAPH '90 Course Notes: Parallel Algo­rithms 
and Architectures for 3D Image Generation, 1990. [12] J. Airey. Increasing update rates in the building 
walkthrough system with automatic model-space subdivision. Techni­cal Report TR90-027, The University 
of North Carolina at Chapel Hill, Department of Computer Science, 1990. [13] J. Airey, J. Rohlf, and 
F. Brooks. Towards image realism with interactive update rates in complex virtual building environ­ments. 
ACM SIGGRAPH Special Issue on 1990 Symposium on Interactive 3D Graphics, 24(2):41-50, 1990. [14] S. Teller 
and C. Sequin. Visibility preprocessing for interac­tive walkthroughs. Computer Graphics, 25(4):61-69, 
1991. [15] S. Teller and C. Sequin. Visibility computations in polyhedral three-dimensional environments. 
U.C. Berkeley Report No. UCB/CSD 92/680, April 1992. [16] D. Meagher. Efficient synthetic image generation 
of arbitrary 3-D objects. Proc. IEEE Conf. on Pattern Recognition and Image Processing, 473-478, June 
1982. Figure Captions Figure 1: A 538 million polygon office environment rendered with hierarchical 
visibility. Upper left: Rendered image. Center left: Depth complexity of the hierarchical visibility 
computation. Lower Left: Viewing frustum and octree cubes examined while rendering the image in the upper 
left. Lower right: Z pyramid used to cull hidden geometry. Upper right: Image rendered with temporal 
coherence. Polygons not rendered in the previous frame are shown in magenta. Center right: Depth complexity 
of the hierarchical visibility computation for the frame rendered using temporal coherence. Figure 2: 
Lower left: Image of a 53 million polygon model (mostly hidden) rendered using hierarchical visibility. 
Upper left: Corre­sponding depth complexity for the hierarchical visibility computation. Lower right: 
Image of a 5 million polygon model. Upper right: Corresponding depth complexity for the hierarchical 
visibility computation. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166148</article_id>
		<sort_key>239</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>31</seq_no>
		<title><![CDATA[Global visibility algorithms for illumination computations]]></title>
		<page_from>239</page_from>
		<page_to>246</page_to>
		<doi_number>10.1145/166117.166148</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166148</url>
		<keywords>
			<kw><![CDATA[algorithmic triage]]></kw>
			<kw><![CDATA[global illumination]]></kw>
			<kw><![CDATA[hidden surface removal]]></kw>
			<kw><![CDATA[radiosity]]></kw>
			<kw><![CDATA[visibility space]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.2</cat_node>
				<descriptor>Engineering</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Hidden line/surface removal</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39084426</person_id>
				<author_profile_id><![CDATA[81100244355]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Seth]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Teller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15033698</person_id>
				<author_profile_id><![CDATA[81100482576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanrahan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>917685</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AIREY, J. M. Increasing Update Rates in the Building Walkthrough System with Automatic Model-Space Subdivision and Potentially Visible Set Calculations. PhD thesis, UNC Chapel Hill, 1990.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122724</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BAUM, D. R., MANN, S., SMITH, K. P., AND WINGET, J. M. Making Radiosity Usable: Automatic Preprocessing and Meshing Techniques for the Generation of Accurate Radiosity Solutions. Computer Graphics (Proc. SIGGRAPH '91) 25, 4 (1991), 51-60.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>899045</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[CAMPBELL III, A., AND FUSSELL, D. S. An Analytic Approach to Illumination with Area Light Sources. Tech. Rep. TR-91-25, Department of Computer Sciences, UT Austin, 1991.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147159</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[CHIN, N., AND EEINER, S. Fast Object-Precision Shadow Generation for Area Light Sources Using BSP Trees. In Proc. 1992 Symposium on Interactive 3D Graphics (1992), pp. 21-30.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378487</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[COHEN, M. F., CHEN, S. E., WALLACE, J. R., AND GREEN- BERG, D. P. A Progressive Refinement Approach to Fast Radiosity Image Generation. Computer Graphics (Proc. SIGGRAPH '88) 22,4 (1988),75-84.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325171</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[COHEN, M. F., AND GREENBERG, D. P. The Hemi-Cube: A Radiosity Solution for Complex Environments. Computer Graphics (Proc. SIGGRAPH '85) 19, 3 (1985), 31-40.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807481</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[FUCHS, H., KEDEM, Z., AND NAYLOR, B. On visible surface generation by a priori tree structures. Computer Graphics (Proc. SIG- GRAPH '80) 14, 3 (1980), 124-133.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147158</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[EUNKHOUSER, T. A., S~,QUIN, C. H., AND TELLER, S. Management of Large Amounts of Data in Interactive Building Walkthroughs. In Proc. 1992 Workshop on Interactive 3D Graphics (1992), pp. 11 - 20.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>116704</ref_obj_id>
				<ref_obj_pid>116700</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Q~IGUS, Z., CANNY, J., AND SEIDEL, R. EfficienflyComputing and Representing Aspect Graphs of Polyhedral Objects. IEEE Transactions on Pattern Analysis and Machine Intelligence 13,6 (1991), 542-551.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[HAINES, E. A., AND WALLACE, J. R. Shaft Culling for Efficient Ray-Traced Radiosity. In Proc. 2~d Eurographics Workshop on Rendering (May 1991).]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122740</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[HANRAHAN, P., SALZMAN, D., AND AUPPERLE, L. A Rapid Hierarchical Radiosity Algorithm. Computer Graphics (Proc. SIG- GRAPH '91) 25, 4 ( 1991 ), 197 - 206.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>144727</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[HECKBERT, P. S. Simulating Globallllumination Using Adaptive Meshing. PhD thesis, Computer Sciences Department, UC Berkeley, June 1991.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>142453</ref_obj_id>
				<ref_obj_pid>142443</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[LISCHINSKI, D., TAMPIERI, F., AND GREENBERG, D. P. Discontinuity Meshing for Accurate Radiosity. IEEE Computer Graphics andApplications 12,6 (1992),25-39.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166143</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[LISCHINSKI, D., TAMPIERI, F., AND GREENBERG, D. P. Combining Hierarchical Radiosity and Discontinuity Meshing. Computer Graphics (Proc. SIGGRAPH '93) 27 (1993).]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>322418</ref_obj_id>
				<ref_obj_pid>2422</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[MEGIDDO, N. Linear programming in linear time when the dimension is fixed. Journalofthe ACM 31 (1984), 114-127.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[NISHITA, T., AND NAKAMAE, E. Half-Tone Representation of 3-D Objects Illuminated by Area Sources or Polyhedron Sources. In Proc. IEEE COMPSAC, 1983 (1983), pp. 237-242.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[PLANTINGA, H. An algorithm for finding the weakly visible faces from a polygon in 3D. Tech. Rep. 92 - 11, U of Pittsburgh, 1992.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[PLANTINGA, W., AND DYER, C. An algorithm for constructing the aspect graph. In Proc. 27th Annual IEEE Symposium on Foundations of Computer Science (1986), pp. 123-131.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>120888</ref_obj_id>
				<ref_obj_pid>120885</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[SEIDEL, R. Small-dimensional linear programming and convex hulls made easy. Discrete and Computational Geometry (1991), 423 -434.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[SOMMERVILLE, D. Analytical Geometry of Three Dimensions. Cambridge University Press, 1959.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134029</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[TELLER, S. Computing the Antipenumbra Cast by an Area Light Source. Computer Graphics (Proc. SIGGRAPH '92) 26, 2 (1992), 139-148.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>171029</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[TELLER, S. Visibility Computations in Densely Occluded Polyhedral Environments. PhD thesis, CS Dept., UC Berkeley, 1992.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>894547</ref_obj_id>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[TELLER, S., AND HOHMEYER, M. E. Computing the Lines Piercing Four Lines. Tech. Rep. UCB/CSD 91/665, Computer Science Department, UC Berkeley, 1991.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122725</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[TELLER, S., AND SEQUIN, C. H. Visibility Preprocessing for Interactive Walkthroughs. Computer Graphics (Proc. SIGGRAPH '91) 25, 4 (1991), 61-69.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[ZHAO, J., AND DOBKIN, D. Personal communication, 1992.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Global Visibility Algorithms for Illumination Computations Seth Teller Pat Hanrahan Institute of Computer 
Science Department of Computer Science Hebrew University of Jerusalem Princeton University Abstract The 
most expensive geometric operation in image synthesis is visibility determination. Classically this is 
solved with hidden surface removal algorithms that render only the parts of the scene visible from a 
point. Global illumination calculations, however, may require information between any two points in the 
scene. This paper describes global visibility algorithms that preprocess polygon databases in order to 
accelerate visibility determination during illumination calculations. These algorithms are sensitive 
to the output complexity in visibility space; that is, how many pairs of objects are mutually visible. 
Furthermore, the algorithms are incremental so that they work well with progressive refinement and hierarchical 
methods of image synthesis. The algorithms are conservative, but exact; that is, when they return visibility 
predicates they can be proved true. However sometimes they do not return either totally visible or totally 
invisible, but partially visible, even though in the same situation a better algorithm might return the 
exact answer. In this paper we describe the algorithms and their implementation, and show that, in a 
scene with low average visual complexity, they can dramatically accelerate conventional radiosity programs. 
CR Categories and Subject Descriptors: I.3.5 [Computa­tional Geometry and Object Modeling]: Geometric 
Algorithms, Languages, and Systems ; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism 
Radiosity; J.2 [Physical w Sciences and Engineering]: Engineering. Additional Key Words: Hidden surface 
removal, visibility space, radiosity, global illumination, algorithmic triage. 1 Introduction In the 
early days of image synthesis a central geometric problem was hidden surface removal. With the advent 
of -buffering, z modern workstations can display pictures of 3D scenes containing millions of polygons 
in real-time. However, such workstations have limited shading capabilities because they make the assumption 
that all light sources illuminate every object. One major thrust of current research in image synthesis 
is to remove this restriction so that the shading correctly accounts for the illumination incident on 
every object. To do this every surface element must assess what light sources, or more generally, what 
surfaces reflecting light towards it, are visible to it. This type of illumination calculation is termed 
global, in contrast to local, because the entire scene must be analyzed to determine if there are any 
occluders interfering with the transfer of light between objects. Collating such visibility information 
is more difficult than determining merely what is visible from a single vantage point, as is done in 
hidden surface removal. For example, the fastest algorithm currently known for computing a complete description 
of the interocclusion due to a polyhedral object of vertices can take 6 lg time [9]. n O p nn m This 
paper describes global visibility algorithms that analyze the entire visibility space, and are applicable 
to a range of illumination problems. Here, we apply them to a hierarchical radiosity algorithm. We have 
implemented several practical algorithms, and show that they allow efficient global visibility calculations 
for scenes of low visual complexity. The algorithms are based on three simple ideas: Permission to copy 
without fee all or part of this material is granted provided that the copies are not made or distributed 
for directprovided that the copies are not made or distributed for direct commercial advantage, the ACM 
copyright notice and the title of thecommercial advantage, the ACM copyright notice and the title of 
thepublication and its date appear, and notice is given that copying is bypublication and its date appear, 
and notice is given that copying is bypermission of the Association for Computing Machinery. To copy 
permission of the Association for Computing Machinery.otherwise, or to republish, requires a fee and/or 
specific permission.otherwise, or to republish, requires a fee and/or specific permission. &#38;#169;1993 
ACM -0 -89791 -601 -8/93/008 $1.50&#38;#169;1993 ----8/93/008/0015 $1.50 Visibility preprocessing. To 
compute what is visible from all points on the surfaces of the objects being shaded, we preprocess the 
scene to speed future visibility tests. For the purposes of global illumination we need only consider 
all pairwise interactions between objects. Preprocessing removes totally invisible pairs from consideration, 
and accelerateslater queries regarding visibility between points on partially visible pairs. Incremental 
visibility maintenance. The most efficient global illumination algorithms operate iteratively based on 
error criteria. Examples are hierarchical radiosity, where surfaces are subdivided with respect to each 
other according to potential light transfers between them [11], and progressive refinement methods where 
light is transferred among surfaces in order of brightness [5]. Thus, the visibility algorithms should 
be lazy and sensitive to required precision. They should also allow refinement so that more precise determinations 
can be made as needed. Conservative triage. Both the preprocessing and maintenance methods use conservative 
triage to avoid the combinatorial com­plexity of exact visibility determination. We classify visibility 
into three categories: totally , totally , and (partially visible). The classification is conservative 
in that  tialinvisible visiblevisible invisibleparptialara all interactions classified as or are correct; 
however, it is acceptable for the classification to return when the correct result is either or . This 
allows us to forego complex analysis or punt if such analysis will take too long to determine the exact 
answer. Of course, for this to work we need either another visibility algorithm to complete the analysis, 
or we must expect the situation to simplify eventually (e.g., through subdivision). The visibility algorithms 
presented here generalize previous work on preprocessing environments for interactive walkthroughs. In 
[24], an algorithm was given to preprocess a 2D environment of axial line segments, such as floorplans. 
This was extended to 3D axial rectangles in [22]. This paper treats the case of convex polygons in general 
position. The global visibility algorithms described here have been imple­mented with a global illumination 
system that computes radiosity values for polygonal scenes [11]. The algorithm maintains a hierar­chy 
of interactions between subdivided polygons at different levels of detail. A key feature of the algorithm 
is that only 2 nk O p k i n m interactions are ever examined (with the number of input poly­gons, and 
the number of elements created by subdividing those polygons). The hierarchical radiosity algorithm, 
as originally de­signed, used pairwise visibility information between polygons. In the original implementation, 
however, this visibility information was inexact. Visibility status was determined by shooting a constant 
number of rays between two polygons. If all of the rays reached from one polygon to the other, the polygons 
were considered totally visible, whereas if none of the rays reached, the polygons were considered totally 
invisible. The conservative algorithms described in this paper, in contrast, are provably more precise. 
2 Overview We present novel algorithms that subdivide space, construct a conservative visibility graph 
over the polygons in a geometric model, then maintain the correctness of the graph under recursive subdivision 
of the polygons. In the context of the hierarchical radiosity computation, this conservative visibility 
graph guarantees that throughout the computation, all polygons that potentially interact (e.g., exchange 
energy) will be known. The construction and maintenance of the graph occurs in four stages. 1. Spatial 
Subdivision. The geometric model is first spatially subdivided into convex polyhedral cells, linked across 
shared boundaries only when some portal, or transparent region, exists on the boundary. For a large class 
of models, and particularly for architectural models, the subdivision proves a natural way of hierarchically 
capturing the geometric and occlusive characteristics of the model. 2. Visibility Propagation. Each 
cell of the spatial subdivision encloses some portion of the geometric model. Clearly, only when two 
cells are mutually visible can their contents (i.e., model polygons) interact. Consequently, we hierarchically 
enumerate all visibility between portions of the model by first establishing inter­cell visibility, then 
establishing inter-polygon visibility only where cells are mutually visible. This is accomplished by 
propagating incremental visibility information through the cells of the spatial subdivision; as each 
cell sees into increasingly distant cells, the visibility graph is augmented to record any previously 
unknown interactions. (Portal enumeration is simply the first and crudest record of visibility propagation.) 
 Visibility propagation provably discovers all partially or totally visible cell (polygon) pairs, at 
the cost of occasionally misclassi­fying an invisible cell (polygon) pair as visible. The alternative, 
misclassification of some mutually visible interaction as invisible, is plainly unacceptable, since it 
may omit from consideration an interaction later to prove important. 3. Blocker Detection. In an exacting 
illumination computation such as global illumination, it is not sufficient to determine simply that two 
polygons are partially visible; some estimation must be made of the extent to which they are visible, 
as well as how much error might be incurred by the estimation. Therefore, once potential visibility between 
a pair of polygons is established, a set of interfering polygons or blockers is determined that may occlude 
part of one polygon as seen from some point on the other. This interference computation is again conservative; 
a non­interfering polygon may occasionally be classified as a blocker, but a blocker will never be classified 
as non-interfering. In the visibility graph, blocker lists augment existing links between mutually visible 
polygons; total visibility is established whenever the blocker list is empty. Perhaps surprisingly, we 
show that these conservative overestimated blocker lists are generally smaller than those maintained 
by existing algorithms. 4. Blocker Maintenance. In a hierarchical radiosity algorithm, polygons (patches, 
in radiosity parlance) are allowed to exchange radiant energy only when the interaction satisfies some 
specified global error bound [11]. Otherwise, the patches are subdivided, and interaction is recommenced 
among the child patches. It is natural to consider how the conservative visibility graph among the patches 
can be incrementally maintained under subdivision. Each child patch may be partially or totally visible, 
or completely invisible, to its child counterparts on the other polygon. We show how, given the parent 
interaction, conservative blocker lists for the children can be determined incrementally. We present 
a novel blocker maintenance technique involving linespace, a five-dimensional representation of 3D lines 
(i.e., light rays).  The algorithms we present are of interest in several ways. First, they comprise 
a practical treatment of visibility issues for unrestricted (i.e., non-axial) three-dimensional environments, 
in contrast to previous work [1, 8, 24]. Second, the conservative visibility description we compute identification 
of all mutually ww visible pairs, and the blocker set for each pair is a natural, output-sensitive way 
of characterizing visibility among polygons or more general objects, for any algorithms that require 
information about occlusion and/or illumination. Finally, we show that the use of these algorithms dramatically 
improves the time and space efficiency of an existing radiosity computation [11]. 3 Spatial Subdivision 
The geometric model is specified as a set of convex polygons P (Figure1). Thespaceembeddingthegeometricmodelissubdivided 
into convex polyhedral cells, typically separated by polygons (Figure 2). The construction is based on 
BSP trees [7], but the visibility algorithms we subsequently present are provably correct for any spatial 
subdivision satisfying a few geometric criteria [22]. . While polygons of sufficient size are present, 
a polygon is chosen whose support plane is to partition the remainder of the set. The choice is made 
using a simple heuristic that determines the polygon whose cross-section in the current cell is largest, 
when expressed as a fraction of the cell s areal intersection with the polygon s support plane; if any 
polygons separate the cell into mutually invisible parts, one such polygon is chosen. This heuristic 
tends to yield effective splitting trees in practice.  Figure 2: Subdivision of the model into cells 
and portals. Next, the portals, or transparent portions, of each cell boundary are explicitly constructed. 
Since cell boundaries are induced on the support planes of polygons, these boundaries are typically partially 
or completely obscured. Each boundary stores a list of coaffine and incident polygons. The portals on 
each boundarycomprise a convex decomposition of the set difference of the cell boundary with the union 
of these polygons. Each portal stores identifiers for the cells which it connects. The spatial subdivision 
therefore comprises an adjacency graph over the cells, since two cells are adjacent in this graph iff 
they share a boundary that is not completely opaque.  4 Visibility Propagation Once a conforming spatial 
subdivision is built, visibility propa­gation commences. The propagation algorithm operates in object 
space, and performs a constrained traversal of the adjacency graph outward from each source cell. Whenever 
a cell is reached by this traversal, its associated polygons are examined pairwise with those in the 
source for mutual visibility; unreached entities are definitely invisible from the source. A given cell 
can see into its neighbors only through portals, and into more distant cells only through portal sequences; 
i.e., ordered lists of portals such that each consecutive pair of portals lead into and out of the same 
cell. The cell adjacency graph is searched by determining cells between which an unobstructed sightline 
exists. models. Consequently, we have developed a simpler algorithm A sightline must be disjoint from 
any occluders and thus must that computes a polyhedral volume guaranteed to enclose the exact intersect, 
or stab, a portal in order to pass from one cell to the next. To establish inter-cell visibility, it 
is sufficient to find a stabbing line through a particular portal sequence; since if some point in the 
interior of one cell can see a point in the interior of another, a sightline must exist between the boundaries 
of the source cell and the reached cell. Thus, the problem of finding sightlines between cell interiors 
reduces to finding sightlines through portal sequences of increasing length. Consequently, the primitive 
visibility operation in a con­forming spatial subdivision is the determination of a stabbing line, given 
a portal sequence, or the determination that no such stabbing line exists. The portal sequences are generated 
incrementally by a depth-first search (DFS) emanating from a particular cell boundary; when a sequence 
no longer admits a sightline, the active branch of the DFS terminates. 4.1 Inter-Cell Visibility Sightline 
determination is an existence predicate, in that it merely establishes visibility between two points 
on different cells. Suppose two cells are mutually visible through a portal sequence. In general, only 
a portion of each cell is visible to the other, due to occlusion by opaque material abutting the edges 
of intervening portals (Figures 3, 4). Whenever inter-cell visibility is established, mutually visible 
volumes are constructed for the cell pair; these volumes and the reaching portal sequence are then used 
to determine inter-polygon visibility among the cells associated polygons. Figure 3:Visibilitypropagationfromasourcecell 
1. S Figure 4:Visibilitypropagationfromasourcecell 2. S The volume visible to a polygon in the presence 
of polygonal occluders is, in general, bounded by quadratic surfaces [18]. An algorithm for computing 
this volume was implemented and de­scribed in [21], but is not yet sufficiently robust for use on complex 
visible region. The algorithm is a straightforward construction that, using separating tangent planes, 
performs a kind of internal pivoting over the edges and vertices occurring along the portal sequence. 
We treat the algorithm briefly here; details can be found in [22]. Figure 5 : A pivoting step on edge 
, from 1 to . The algorithm exploits the fact that for each portal edge, at most two separating planes 
can contribute a face to polyhedral bounds on the illuminated volume (Figure 5), since at most one vertex 
from each halfspace of the associated portal can span a relevant plane with the edge. Consider some edge 
on a portal 2, and the portals occurring before 2. Each of these portals has at most one extremal vertex 
that spans a separating plane with (in the n ReR vSe n n eRv S figure, 1 has extremal vertex 1 and has 
extremal vertex ). Together with , only one of these (at most 3 ) extremal vertices can span a plane 
that contains all the other extremal vertices in the same halfspace as the portal 2. This single plane 
is the only one of the candidate planes that can contribute faces to the boundary of the illuminated 
volume. Therefore, for any portal edges there are at most 2boundary planes, each of which can be identified 
in time by pivoting over the vertices of the other portals. The total time to identify the 2relevant 
planes is therefore 2. Moreover, the set of planes can be updated incrementally O p n m n O p n m n whenever 
a new portal is encountered, simply by updating the existing halfspaces with respect to the new portal 
vertices, and introducing planes tight on the new portal edges. The positive halfspaces of the planes 
are inspected for an intersection with the c OO pp n v n S im cO mp n m BSP halfspaces bounding the reached 
cell in time with a linear programming algorithm [15, 19]. If no such intersection exists then the reached 
cell can not be visible to the source through the active portal sequence. 4.2 Inter-Polygon Visibility 
Whenever a cell is reached by the graph propagation, an active set of halfspaces bounds the volume in 
the reached cell visible to the source. The orientations of each of these halfspaces are reversed to 
bound the volume illuminated by the reached cell in the source. Only polygons in these respective volumes 
can be mutually visible. Each incident source polygon is prefixed to the front of the active portal sequence 
(Figure 6). The visible volume in the reached cell due to the augmented sequence is then tested for incidence 
with the appropriate subset of polygons stored in the reached cell. (The notion of conservative inter-polygon 
visibility can be simply extended to treat visibility between general objects [22].) Figure 6 depicts 
this mechanism for an analogous 2D situation, in which polygons and portals are line segments. A 
source cell (Figure 6-i) establishes inter-cell visibility to a cell via some portal sequence. The polygon 
in can have no interaction with s interior, and it is not considered further. Polygon is incident on 
the inter-cell visibility volume, and therefore potentially visible from some point in . However, when 
the portal sequence is augmented with the constraints due to (Figure 6-ii), polygonCD is found to be 
invisible from . Finally, is found to intersect  ASRRSAADBSARC s visible region in , and and are established 
to be mutually Figure 6: Establishing visibility of 2D polygons and . D visible. The convexity of 
spatial subdivision cells allows an important optimization. Any two polygons entirely incident on the 
boundaries of the same cell can have blockers only in the relative interior of that cell. When the cell 
interior is empty (as it typically will be), the polygons can be immediately classified as entirely mutually 
visible. Thus, the spatial subdivision quickly identifies many instances of complete mutual visibility 
between nearby polygons. Figures 7 and 8 depict the output of the inter-polygon visibility computation 
in three dimensions, for two polygons incident on different source cells. Display of the spatial subdivision 
has been suppressed for clarity. Figure 7: Visibility propagation from a polygon in cell 1. Figure 8: 
Visibility propagation from a polygon in cell 2. 5 Blocker Detection When a pairof polygons is found 
to be mutually visible, we record a visibility interaction , and proceed to identify S BR R I S R the 
blocker list of the pair. One could simply compute the set of blockers as those polygons incident on 
a convex volume containing and (as in [10]). However, the visibility graph and reaching portal sequence 
generally yield a better (i.e., smaller) blocker list. Denote the convex hull of all vertices of andRconv 
SS R RBS as . Clearly any blocker must be incident on convS S R CS SCBC S R RRSR to contribute to 
. Moreover, observe that only polygons visible to along a sequence reaching , or to along a sequence 
reaching need be considered as blockers of and . For, if some polygon is not visible to , then every 
ray leaving (including those rays to any point on ) must stab some polygon other than before stabbing 
. The polygons and do not generally see the same set of blockers (Figures 7 and 8). Therefore, is augmented 
whenever a search from () to () discovers a previously unknown blocker. Figure 9 depicts the result of 
the blocker computation, where all polygons except , , and have SRSRRSS BR S R B S R been removed. Note 
that, of the polygons from the large central room, neither the large blue interior wall panels nor the 
thin blue doorjambs (cf. Figure 1) are classified as blockers. Thus the purely spatial (shaft) cull produces 
a blocker list of size 12 or more, whereas the blocker detection algorithm presented here computes a 
list of 6 blockers. Figure 9: The final blocker list of and . Finally, the blocker criterion presented 
above is conservative, since may include polygons that are visible to or toBBR but do not affect occlusion 
between them. The exact determination of the blocker list is computationally involved; a polygon is a 
blocker of and only if some ray from to exists whose only front-facing polygon intersection, aside from 
that with , is with . (The asymmetry of the definition arises from the fact that, in a manifold polyhedral 
environment of oriented polygons, only front faces can be visible to front faces.) 6 Blocker Maintenance 
Given a set of polygons , the visibility preprocessing scheme produces, for every polygon , a set of 
visible polygons . For each polygon , the blocker list enumerates partial SBR 2 RPB2 n S VVf RS P S 
 SRRg2 PI S BR S R visible V S all polygons that potentially impede visibility between and . points 
only to top-level patches; this makes sense, since blockers should be as large as possible to cause maximal 
occlusion. For each interaction , we store a tube data structure, which associates an interacting patch 
pair, a blocker list, the visibility status of the interaction (i.e., or ), and some additional geometric 
information used for incremental visibility tests. In the hierarchical radiosity algorithm, when the 
energetic in­teraction between two patches can not be characterized to within the global error bound, 
one of the patches of the interaction is symmetrically subdivided, and its children are allowed to interact 
with the other patch [11]. Clearly, interactions between either tube data structure exploits this coherence 
to perform efficient and accurate visibility reclassification after subdivision. Each child interaction 
s blocker list is necessarily a subset of the parent s blocker list; we wish to efficiently, and incrementally, 
determine the child tube s blockers. We say that a blocker impinges on if it occludes from , and that 
is disjoint from if can not cause occlusion. Whenever a blocker list is discovered to be empty (i.e., 
to contain no impinging blockers), complete visibility between the interacting patches will be established, 
and no further visibility computations need be done for any children of this interaction. Conversely, 
whenever the blocker list is discoveredto be completely occluding, there can be no energy transport between 
and , and the interaction is discarded SR (alternatively, the culprit blocker(s) can be retained as 
proof that the patches cannot interact). Finally, when neither complete visibility nor complete occlusion 
can be quickly determined, the status of the child interaction remains partially visible. 6.1 Linespace 
The tube structure efficiently encodes the set of all lines between  SRa n a b t b and , using a five-dimensional 
line representation known as Plucker coordinates [20], or simply linespace. Lines in three dimensions 
correspond to hyperplanes and points in linespace. Any two 3D rays and can be oriented by considering 
their linespace counterparts , a 5D hyperplane, and , a 5D point (details of the mapping can be found 
in [21]). .a .a .b a aa side (a, b)< 0 side (a, b)= 0 side (a, b)> 0 The signed distance of from determines 
the sense in t b n a which the lines go around each other in 3D; if lies on t b n a the lines and are 
coplanar. This sidedness property can be b used to represent the set of lines through a collection 
of convex polygons. In practice, there is one caveat to using the linespace representation [23]. The 
only portion of linespace corresponding to 3D lines with real coefficients are those linespace points 
lying on a 4D manifold known as the Plucker quadric [20]; all other linespace points correspond to 3D 
lines with complex coefficients. Fortunately, the algorithms used in this paper need never consider the 
Plucker quadric, since they manipulate only lines known a priori to have real coefficients. Consider 
two convex polygons and , comprised of sets of oriented edges and , respectively. For there to exist 
some line L aS k .h a k R k S .b RS t L n R S k n .R b k that stabs the interiors of and , must lie in 
the appropriate signed halfspaces of the hyperplanesand. patch and the children of its counterpart are 
highly coherent. The Thus, the set of all lines through and corresponds to the interior of a five-dimensional 
convex polytope[21]. Rather than attempt to compute this polytope directly, we can manipulate the vertices 
of its intersection with the Plucker quadric, which are comparatively easy to generate. Each such vertex 
corresponds to a collection of four support lines from and , since four 5D hyperplanes must intersect 
with the Plucker quadric to generate each such vertex. These vertices must correspond to stabbing lines 
tight on four edges of and in 3D; i.e., lines through a vertex of SR and a vertex of (note that these 
lines necessarily have real 3D coefficients). In our implementation, there are at most sixteen such lines, 
since all patches are quadrilaterals. There are several advantages to performing blocker analysis in 
linespace. The data structure for a single blocker is constant size, and for a single patch interaction 
is linear in the number of blockers. The linespace analysis obviates complicated 3D topological and numerical 
computations. The only operations required by the linespace representation are mapping from 3D lines 
to 5D points and hyperplanes, and computing inner products between points and hyperplanes. 6.2 Incremental 
Blocker Maintenance The tube data structure, and incremental visibility maintenance, can now be fully 
described. Suppose patch is subdivided against patch into child elements . The tube for and each stores 
, , and a constant number of linespaceC R 2 S t CS RRSSCV R S CC R CR R R convRR t S RSBIBS S C R points 
whose convex hull includes the set of all lines through and . Finally, each blocker in is reclassified 
with respect to the child tube to produce , and the visibility status of each interaction is determined. 
As before, many instances of total invisibility, partial visibility, and total visibility are discovered 
quickly. Other situations are considered too complex to analyze completely, and we punt and classify 
the interaction as partially visible (perhaps causing further subdivision [11]). SR INVISIBLE PARTIAL 
VISIBLE PUNT Figure 10: Performing 3D triage in 5D linespace. Consider an interaction and a single potential 
blocker (Figure 10). We wish to determine, without extensive analysis, whether all, none, or some of 
the lines through and stab the S R blocker . Respectively, this is equivalent to determining whether 
convlies entirely inside, is disjoint from, or has some intersection with, the set of lines through the 
blocker (Figure 11). We exploit the fact that, in linespace, both sets of lines are convex. The points 
in are first classified with respect to the blocker hyperplanes . If all of the points lie inside the 
n k B t BS R c S onvt k S t f n R k R k S p B BR m S BR Rn k B k n V f V kk S p BS B R m VS pinvisiblear 
R tialvisible B , then , by convexity. is therefore completely occluding and is . If some of the points 
lie inside the , and some lie outside, some lines through and stab , and is . If all of the points lie 
outside some single , is . Finally, the complex case occurs when all of the points lie outside all of 
the . This does not guarantee total visibility, since V convS R t S p n ar R k tial BVS punt RV par VS 
tial S RR visibleinvisiblevisiblepartial k f k p B B m may still have some intersection with(this case 
is labeled in Figures 10 and 11); accordingly, is classified as . The logic for multiple blockers is 
straightforward; any single blocker can cause to be , but all blockers must be disjoint in order for 
to be . Otherwise, any impinging blocker causes to become .  6.3 Evolution Figure 12 depicts an example 
of blocker list evolution and incre­mental reclassification of child interactions. White lines connecting 
quadrilateral centroids represent interactions; green lines represent interactions, and red lines represent 
the tube Figure 12: Reclassification of child interactions after subdivision.   The linespace algorithms 
guarantee conservative visibility, in that blockers are only discarded from interactions if they are 
defi­nitely known to be disjoint. Existing algorithms use point-sampling [2, 6, 11, 16] or point-to-area 
visibility [3, 4] techniques and therefore do not guarantee correct inter-area visibility determina­tion. 
In contrast, we establish exact visibility information where possible, and adaptively subdivide until 
the uncertainty of visibility estimation in the remaining cases is so small as to be unimportant. The 
linespace blocker maintenance algorithms are simple and fast, although they sometimes overestimate occlusion 
by classifying disjoint blockers as impinging, and may not identify interactions as early as might a 
more sophisticated algorithm. Establishment of improved algorithms for the determination of An exact 
algorithm was The . . . 403 402 2 patch The inter-polygon 1 2 34 5 67 891011121314 portal sequence length 
Figure 13: Stabbing successes and failures, by sequence length. The inter-cell visibility determination 
uses a depth-first-search through the cell adjacency graph, applying an incremental stabbing predicate 
and visible volume computation at each step. The incremental operation expends linear time in the number 
of portals currently in the sequence, assuming a constant number of edges per portal, and so requires 
2time to stab a sequence of portals. p n m n In practice, this seems not to prohibit use of the algorithms 
on real data sets, since most portal sequences are short (less than ten portals), and the algorithmic 
constants are therefore more important than the asymptotic complexity measure.   Input patches. Input 
Initial refinement. Resulting Second refinement. Resulting Figure 14: Refinement of the input patches 
(left), 7.3 Blocker Detection There were 5 blockers between partially visible patches, on average, reached 
through portal sequences of average length five. The subdivision heuristic was effective; the BSP tree 
did not suffer from excessive free-space splitting, or regions in which subdivision planes were induced 
due to far away polygons. The visibility analysis communicated its results to the radiosity computation 
via an file. Each file line recorded an interaction between two polygons, the length of the associated 
blocker list , and the blockers themselves. A zero-length blocker list   IS BRS R invisiblevisibleascii 
S par R tialpvisiblepar V tial S R . pvisiblevisibleartial implied total visibility between and , i.e., 
; otherwise the visibility status was . 7.4 Blocker Maintenance The model input to the radiosity computation 
is shown at the upper left of Figure 14. The input patches form the radiosity program s initial mesh. 
The 4,391 initial and links are shown, respectively, in white (middle column) and green (right column). 
Two iterations of patch-patch refinement were performed. The resulting model mesh, and interactions are 
displayed in the second and third rows of Figure 14. The number of links drastically increases after 
the first iteration. Their increased density naturally indicates unoccluded regions of the model. Similarly, 
the green links indicate occlusion. links are not shown, as they were discarded by the radiosity program 
upon detection. Using the results of the visibility preprocessing, the initial refine took only 11 seconds, 
performing 145,846 interactions. The second refinement stage required 50 seconds, and performed 186,703 
links. Input links. links. Resulting links. links. Resulting links. links (middle), and links (right). 
 p p visible interactions. About 90% of the refined interactions were , thus requiring no sampling for 
form-factor estimation. Table 1 charts the evolution of each link type, the number of elements, and the 
number of interactions at each refine. Input Links Refine Links Refine Links V V 176,474 V 225,763 VV 
PP P 16,889 PP 22,157 I 1,096 ( 0.6%) I 1,339 ( 0.5%) 4,391 194,459 193,363 249,259 247,920 403 patches 
15,039 patches 17,347 patches 4,391 interactions 145,846 interactions 186,703 interactions Table 1: Link 
evolution by type, with patch and interaction counts. Since the time complexity of the radiosity algorithm 
is propor­tional to the number of interactions, the visibility preprocessing significantly decreased 
the computation done by the radiosity al­gorithm. Moreover, the modified radiosity algorithm was more 
accurate, since no partially visible interactions were missed due to sampling errors (as in [11]).  
7.5 Blocker Visualization All of the algorithms described in this paper were implemented using visualization 
tools that allowed interactive inspection of complex data structures. Figures 15 through 17 depict the 
use of this tool to investigate some interesting interactions. Again, partialvisiblepartial the white 
and green line segments represent and interactions, respectively; for a particular partial interaction 
in each figure, the tube is shown (in red), and the blockers for the interaction  Figure 15: The tube 
data structure (red, or-Figure 16: Spatially incident polygons that Figure 17: A interaction that ange) 
for an ordinary interaction. have not been classified as blockers. could be classified as . are highlighted 
in orange. Figure 15 depicts an ordinary interaction. Figure 16 depicts spatially incident polygons that 
(correctly) have not been classified as blockers. Figure 17 depicts a interaction for which no single 
blocker occludes the p source and receiver; a more sophisticated algorithm could classify this interaction 
as .  8 Summary and Conclusion We have presented several novel algorithms that represent an effective 
application of global visibility analysis to radiosity com­putations, an important problem in image synthesis. 
Given the complexity of both the visibility and radiosity approaches used, it was surprisingly easy to 
couple the two processes. We did so using an abstraction in which interactions between polygons were 
maintained along with all potentially blocking polygons. We argue that, for an interesting class of large 
models, inter-polygon visibil­ity has roughly constant complexity throughout the interior of the model. 
After construction of a spatial subdivision for the model, the visibility algorithms we present are output 
sensitive; they expend work proportional to the amount of inter-polygon visibility present. None of the 
visibility algorithms attempt to compute exact vis­ibility information. However, they achieve precision 
in a different sense, by reporting all visibilities conservatively; potentially visible interactions 
are always reported. Only blockers can occlude a specified source from a specified emitter. Thus, the 
blocker list formulation is applicable to the problem of discontinuity meshing in the presence of area 
light sources [12, 13, 14, 21], as well as to the construction of an oracle to decide which, if any, 
among a collection of discontinuities should be meshed upon earliest. We showed that the visibility analysis 
significantly accelerated a radiosity computation in a polygonal environment. Finally, we demonstrated 
the successful application of some elegant concepts such as linespace and algorithmic triage to the concrete 
problem of construction and incremental maintenance of blocker lists. Acknowledgments The authors are 
grateful to David Laur for his assistance with the geo­metric model and the color plates, and to Dani 
Lischinski for his valuable comments. This work was begun during a visit to the Science and Technology 
Center for the Visualization of Geometric Structures, in Min­neapolis, and partially supported by Apple, 
Silicon Graphics Computer Systems, and the National Science Foundation ( 9207966). References Airey 
J.M. ccrnsf [1] IncreasingUpdate Rates in the Building Walkthrough System with Automatic Model-Space 
Subdivision and Potentially Visible Set Calculations. PhD thesis, UNC Chapel Hill, 1990. [2] Baum M.D.R. 
Mann S. Smith K.P. and J. Winget Making Radiosity Usable: Automatic Preprocessing and Meshing Techniques 
for the Generation of Accurate Radiosity So­lutions. Computer Graphics (Proc. SIGGRAPH 91) 25, 4 (1991), 
5160. Campbell A III A. andFussell D.S. [3] An Analytic Approach to Illumination with Area Light Sources. 
Tech. Rep. TR-91-25, Department of Computer Sciences, UT Austin, 1991. [4] Fast Object-Precision Shadow 
Gen- Chin N. andFeiner S. A eration for Area Light Sources Using BSP Trees. In Proc. 1992 Symposium 
on Interactive 3D Graphics (1992), pp. 2130. [5] A Progressive Refinement Approach to Fast Radiosity 
Image Generation. Computer Graphics (Proc. SIGGRAPH 88) 22,4 (1988), 7584. [6] The Hemi-Cube: A Radiosity 
Solution for Complex Environments. Computer Graphics (Proc. SIGGRAPH 85) 19, 3 (1985), 3140. [7] On visible 
surface generation by a priori tree structures. Computer Graphics (Proc. SIG-GRAPH 80) 14, 3 (1980), 
124133. agement of Large Amounts of Data in Interactive Building Walk­throughs. In Proc. 1992 Workshop 
on Interactive 3D Graphics (1992), pp. 1120. Gigus A Z. Canny A J. equin andSeidel R. [9] Efficiently 
Computing and Representing Aspect Graphs of Polyhedral Objects. IEEE Trans­actions on Pattern Analysis 
and Machine Intelligence 13 , 6 (1991), 542551. [10] Shaft Culling for Efficient Ray-Traced Radiosity. 
In Proc. 2Eurographics Workshop on Rendering (May 1991). [11] A Rapid Haines Hanrahan E.A. P. andSalzman 
WallaD. ce nd andJ.R.Aupperle L. Hierarchical Radiosity Algorithm. Computer Graphics (Proc. SIG-GRAPH 
91) 25, 4 (1991), 197206. Heckbert P.S. A [12] Simulating Global Illumination Using Adaptive Meshing. 
PhD thesis, Computer Sciences Department, UC Berkeley, June 1991. [13] Discontinuity Meshing for Accurate 
Radiosity. IEEE Computer Graphics and Applications 12 , 6 (1992), 2539. [14] Lischinski D. T ampieri 
F. and A Greenber g D. P. Combining Hierarchical Radiosity and Discontinuity Meshing. Com­puter Graphics 
(Proc. SIGGRAPH 93) 27 (1993). [15] Linear programming in linear time when the dimen­sion is fixed. Journal 
of the ACM 31 (1984), 114127. NishitT. Nakamae A [16] Half-Tone Representation of 3-D Objects Illuminated 
by Area Sources or Polyhedron Sources. In Proc. IEEE COMPSAC, 1983 (1983), pp. 237242. [17] An algorithm 
for finding the weakly visible faces from a polygon in 3D. Tech. Rep. 9211, U of Pittsburgh, 1992. Megiddo 
Plantinga a N.W. H.andandDyer th C.E. AA Man­ [18] An algorithm for constructing the aspect graph. In 
Proc. 27Annual IEEE Symposium on Foundations of Computer Science (1986), pp. 123131. Seidel R.ville 
A [19] Small-dimensional linear programming and convex hulls made easy. Discrete and Computational Geometry 
(1991), 423434. SommerD. A [20] Analytical Geometry of Three Dimensions. Cambridge University Press, 
1959. [21] Computing the Antipenumbra Cast by an Area Light Source. Computer Graphics (Proc. SIGGRAPH 
92) 26, 2 (1992), 139148. [22] Visibility Computations in Densely Occluded Polyhe­dral Environments. 
PhD thesis, CS Dept., UC Berkeley, 1992. [23] Computing the Lines Piercing Four Lines. Tech. Rep. UCB/CSD 
91/665, Computer Science Department, UC Berkeley, 1991. Teller A S. and SHohmeyer equin dC.H.M.E. [24] 
Visibility Preprocessing for Interactive Walkthroughs. Computer Graphics (Proc. SIGGRAPH 91) 25, 4 (1991), 
6169. [25] Personal communication, 1992.Zhao J. andDobkin A D.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166149</article_id>
		<sort_key>247</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>32</seq_no>
		<title><![CDATA[Adaptive display algorithm for interactive frame rates during visualization of complex virtual environments]]></title>
		<page_from>247</page_from>
		<page_to>254</page_to>
		<doi_number>10.1145/166117.166149</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166149</url>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.6</cat_node>
				<descriptor>Constrained optimization</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Hidden line/surface removal</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003716</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003716</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P280650</person_id>
				<author_profile_id><![CDATA[81100182132]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Funkhouser]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P41115</person_id>
				<author_profile_id><![CDATA[81100058395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Carlo]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[S&#233;quin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>91416</ref_obj_id>
				<ref_obj_pid>91394</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Airey, John M., Rohlf, John H., and Brooks, Jr., Frederick P. Towards Image Realism with Interactive Update Rates in Complex Virtual Building Environments. ACM SIGGRAPH Special Issue on 1990 Symposium on Interactive 3D Graphics, 24, 2 (1990), 41-50.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Blake, Edwin H. A Metric for Computing Adaptive Detail in Animated Scenes using Object-Oriented Programming. Eurographics '87. G. Marechal (Ed.), Elsivier Science Publishers, B.V. (North-Holland), 1987.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>319122</ref_obj_id>
				<ref_obj_pid>319120</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Brooks, Jr., Frederick P. Walkthrough - A Dynamic Graphics System for Simulating Virtual Buildings. Proceedings of the 1986 Workshop on Interactive 3D Graphics.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360354</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Clark, James H. Hierarchical Geometric Models for Visible Surface Algorithms. Communications of the ACM, 19, 10 (October 1976), 547-554.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147158</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Funkhouser, Thomas A., Saquin, Carlo H., and Teller, Seth J. Management of Large Amounts of Data in Interactive Building Walkthroughs. ACM SIGGRAPH Special Issue on 1992 Symposium on Interactive 3D Graphics, 11-20.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578533</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Garey, Michael R., and Johnson, David S. Computers and Intractibility: A Guide to the Theory of NP-Completeness. W.H. Freeman and Company, New York, 1979.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Ibaraki, T., Hasegawa, T., Teranaka, K., and Iwase J. The Multiple Choice Knapsack Problem. J. Oper. Res. Soc. Japan 21, 1978, 59-94.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Rossignac, Jarek, and Bowel, Paul. Multi-resolution 3D approximations for rendering complex scenes. IFIP TC 5.WG 5.10 H Conference on Geometric Modeling in Computer Graphics, Genova, Italy, 1993. Also available as IBM Research Report RC 17697, Yorktown Heights, NY 10598.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>538586</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Schachter, Bruce J. (Ed.). Computer Image Generation. John Wiley and Sons, New York, NY, 1983.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Graphics Library Programming Tools and Techniques, Document #007-1489-01, Silicon Graphics, Inc., 1992.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122725</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Teller, Seth J., and S6quin, Carlo H. Visibility Preprocessing for Interactive Walkthroughs. Proceedings of SIGGRAPH '91. In Computer Graphics 25, 4 (August 1991), 61-69.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>171029</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Teller, Seth J. Visibility Computations in Densely Occluded Polyhedral Environments. Ph.D. thesis, Computer Science Division (EECS), University of California, Berkeley, 1992. Also available as UC Berkeley technical report UCB/CSD-92-708.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Zyda, Michael J. Course Notes, Book Number 10, Graphics Video Laboratory, Department of Computer Science, Naval Postgraduate School, Monterey, California, November 1991.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Adaptive Display Algorithm for Interactive Frame Rates During Visualization of Complex Virtual Environments 
 Thomas A. Funkhouser and Carlo H. S´equin z University of California at Berkeley Abstract We describe 
an adaptive display algorithm for interactive frame rates during visualization of very complex virtual 
environments. The algorithm relies upon a hierarchical model representation in which objects are described 
at multiple levels of detail and can be drawn with various rendering algorithms. The idea behind the 
algorithm is to adjust image quality adaptively to maintain a uni­form, user-speci.ed target frame rate. 
We perform a constrained optimization to choose a level of detail and rendering algorithm for each potentially 
visible object in order to generate the best image possible within the target frame time. Tests show 
that the algorithm generates more uniform frame rates than other previously described detail elision 
algorithms with little noticeable difference in image quality during visualization of complex models. 
CR Categories and Subject Descriptors: [Computer Graphics]: I.3.3 Picture/Image Generation viewing algorithms; 
I.3.5 Computational Geometry and Object Modeling geometric algorithms, object hierarchies ; I.3.7 Three-Dimensional 
Graphics and Realism virtual reality. 1 Introduction Interactive computer graphics systems for visualization 
of realistic­looking, three-dimensional models are useful for evaluation, design and training in virtual 
environments, such as those found in archi­tectural and mechanical CAD, .ight simulation, and virtual 
reality. These visualization systems display images of a three-dimensional model on the screen of a computer 
workstation as seen from a sim­ulated observer s viewpoint under interactive control by a user. If images 
are rendered smoothly and quickly enough, an illusion of real-time exploration of a virtual environment 
can be achieved as the simulated observer moves through the model. z Computer Science Division, Berkeley, 
CA 94720 Permission to copy without fee all or part of this material is granted Permission to copy without 
fee all or part of this material is granted provided that the copies are not made or distributed for 
direct provided that the copies are not made or distributed for direct commercial advantage, the ACM 
copyright notice and the title of the commercial advantage, the ACM copyright notice and the title of 
the publication and its date appear, and notice is given that copying is by publication and its date 
appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. otherwise, or to republish, requires a fee and/or specific permission. 
&#38;#169;1993 ACM-0-89791-601-8/93/008 $1.50 &#38;#169;1993 ACM-0-89791-601-8/93/008/0015 $1.50 It is 
important for a visualization system to maintain an interactive frame rate (e.g., a constant ten frames 
per second). If frame rates are too slow, or too jerky, the interactive feel of the system is greatly 
diminished [3]. However, realistic-looking models may contain millions of polygons far more than currently 
available workstations can render at interactive frame rates. Furthermore, the complexity of the portion 
of the model visible to the observer can be highly variable. Tens of thousands of polygons might be simultaneously 
visible from some observer viewpoints, whereas just a few can be seen from others. Programs that simply 
render all potentially visible polygons with some predetermined quality may generate frames at highly 
variable rates, with no guaranteed upper bound on any single frame time. Using the UC Berkeley Building 
Walkthrough System [5] and a model of Soda Hall, the future Computer Science Building at UC Berkeley, 
as a test case, we have developed an adaptive algorithm for interactive visualization that guarantees 
a user-speci.ed target frame rate. The idea behind the algorithm is to trade image quality for interactivity 
in situations where the environment is too complex to be rendered in full detail at the target frame 
rate. We perform a constrainedoptimization that selects a level of detail and a rendering algorithm with 
which to render each potentially visible object to produce the best image possible within a user-speci.ed 
target frametime. Incontrasttopreviouscullingtechniques,thisalgorithm guaranteesauniform, boundedframerate, 
evenduringvisualization of very large, complex models.  2 Previous Work 2.1 Visibility Determination 
In previous work, visibility algorithms have been described that compute the portion of a model potentially 
visible from a given observer viewpoint [1, 11]. These algorithms cull away large por­tions of a model 
that are occluded from the observer s viewpoint, and thereby improve frame rates signi.cantly. However, 
in very detailed models, often more polygons are visible from certain ob­server viewpoints than can be 
rendered in an interactive frame time. Certainly, there is no upper bound on the complexity of the scene 
visible from an observer s viewpoint. For instance, consider walk­ing through a very detailed model of 
a fully stocked department store, or viewing an assembly of a complete airplane engine. In our model 
of Soda Hall, there are some viewpoints from which an observer can see more than eighty thousand polygons. 
Clearly, vis­ibility processing alone is not suf.cient to guarantee an interactive frame rate. 2.2 Detail 
Elision To reduce the number of polygons rendered in each frame, an in­teractive visualization system 
can use detail elision. If a model can be described by a hierarchical structure of objects, each of which 
is represented at multiple levels of detail (LODs), as shown in Figure 1, simpler representations of 
an object can be used to improve frame rates and memory utilization during interactive vi­sualization. 
This technique was .rst described by Clark [4], and has been used by numerous commercial visualization 
systems [9]. If different representations for the same object have similar appear­ances and are blended 
smoothly, using transparency blending or three-dimensional interpolation, transitions between levels 
of detail are barely noticeable during visualization. 241 Polygons 44 Polygons Figure 1: Two levels 
of detail for a chair. Previously described techniques for choosing a level of detail at which to render 
each visible object use static heuristics, most often based on a threshold regarding the size or distance 
of an object to the observer [2, 8, 9, 13], or the number of pixels covered by an average polygon [5]. 
These simple heuristics can be very effective at improving frame rates in cases where most visible objects 
are far away from the observer and map to very few pixels on the workstation screen. In these cases, 
simpler representations of some objects can be displayed, reducing the number of polygons rendered without 
noticeably reducing image quality. Although static heuristics for visibility determination and LOD selection 
improve frame rates in many cases, they do not generally produce a uniform frame rate. Since LODs are 
computed indepen­dently for each object, the number of polygons rendered during each frame time depends 
on the size and complexity of the objects visible to the observer. The frame rate may vary dramatically 
from frame to frame as many complex objects become visible or invisible, and larger or smaller. Furthermore, 
static heuristics for visibility determination and LOD selection do not even guarantee a bounded frame 
rate. The frame rate can become arbitrarily slow, as the scene visible to the observer can be arbitrarily 
complex. In many cases, the frame rate may become so slow that the system is no longer interactive. Instead, 
a LOD selection algorithm should adapt to overall scene complexity in order to produce uniform, bounded 
frame rates. 2.3 Adaptive Detail Elision In an effort to maintain a speci.ed target frame rate, some 
com­mercial .ight simulators use an adaptive algorithm that adjusts the size threshold for LOD selection 
based on feedback regarding the time required to render previous frames [9]. If the previous frame took 
longer than the target frame time, the size threshold for LOD selection is increased so that future frames 
can be rendered more quickly. This adaptive technique works reasonably well for .ight sim­ulators, in 
which there is a large amount of coherence in scene complexity from frame to frame. However, during visualization 
of more discontinuous virtual environments, scene complexity can vary radically between successive frames. 
For instance, in a build­ing walkthrough, the observer may turn around a corner into a large atrium, 
or step from an open corridor into a small, enclosed of.ce. In these situations, the number and complexity 
of the objects visible to the observer changes suddenly. Thus the size threshold chosen based on the 
time required to render previous frames is inappropri­ate, and can result in very poor performance until 
the system reacts. Overshoot and oscillation can occur as the feedback control system attempts to adjust 
the size threshold more quickly to achieve the target frame rate. In order to guarantee a bounded frame 
rate during visualization of discontinuous virtual environments, an adaptive algorithm for LOD selection 
should be predictive, based on the complexity of the scene to be rendered in the current frame, rather 
than reactive, based only on the time required to render previous frames. A predictive algorithm might 
estimate the time required to render every object at every level of detail, and then compute the largest 
size threshold that allows the current frame to be rendered within the target frame time. Unfortunately, 
implementing a predictive algorithm is non­trivial, since no closed-form solution exists for the appropriate 
size threshold.  3 Overview of Approach Our approach is a generalization of the predictive approach. 
Con­ceptually, every potentially visible object can be rendered at any level of detail, and with any 
rendering algorithm (e.g., .at-shaded, Gouraud-shaded, texture mapped, etc.). Every combination of ob­jects 
rendered with certain levels of detail and rendering algorithms takes a certain amount of time, and produces 
a certain image. We aim to .nd the combination of levels of detail and rendering al­gorithms for all 
potentially visible objects that produces the best image possible within the target frame time. More 
formally, we de.ne an object tuple, ,tobeanin­stanceofobject ,renderedatlevelofdetail ,withrenderingalgo­rithm 
. We de.ne two heuristics for object tuples:   Benest RS a OdOLdR t L a OdLdR Cost ta OdLdR t and 
. The Cost heuristic estimates the time re­quired to render an object tuple; and the Bene.t heuristic 
estimates the contribution to model perception of a rendered object tuple. We de.ne to be the set of 
object tuples rendered in each frame. Using these formalisms, our approach for choosing a level of detail 
and rendering algorithm for each potentially visible object can be stated: Maximize : Subject to : (1) 
P S Cost a P Od S Ld Benest R t p a T O ar dLd getF R t rameTime This formulation captures the essence 
of image generation with real-time constraints: do as well as possible in a given amount of time. As 
such, it can be applied to a wide variety of problems that require images to be displayed in a .xed amount 
of time, including adaptive ray tracing (i.e., given a .xed number of rays, cast those that contribute 
most to the image), and adaptive radiosity (i.e., given a .xed number of form-factor computations, compute 
those that contribute most to the solution). If levels of detail representing no polygons at all are 
allowed, this approach handles cases where the target frame time is not long enough to render all potentially 
visible objectsevenatthelowestlevelofdetail. Insuchcases,onlythemost valuable objects are rendered so 
that the frame time constraint is not violated. Using this approach, it is possible to generate images 
in a short, .xed amount of time, rather than waiting much longer for images of the highest quality attainable. 
For this approach to be successful, we need to .nd Cost and Bene.t heuristics that can be computed quickly 
and accurately. Un- We model the time taken by the Per Primitive stage as a linear combination of the 
number of polygons and vertices in an object tuple, with coef.cients that depend on the rendering algorithm 
and machine used. Likewise, we assume that the time taken by the Per Pixel stage is proportional to the 
number of pixels an object covers. Our model for the time required to render an object tuple is: CostO 
a OdLdRC ti CmaxL . CC Poly a OCdL t Pix .a COR V t ert a OdL t . 12 3 where is the object, is the level 
of detail, is the rendering algorithm, and 1, 2 and 3 are constant coef.cients speci.c to afortunately, 
Cost and Bene.t heuristics for a speci.c object tuple cannot be predicted with perfect accuracy, and 
may depend on other object tuples rendered in the same image. A perfect Cost heuristic may depend on 
the model and features of the graphics workstation, the state of the graphics system, the state of the 
operating system, and the state of other programs running on the machine. A per­fect Bene.t heuristic 
would consider occlusion and color of other object tuples, human perception, and human understanding. 
We cannot hope to quantify all of these complex factors in heuristics that can be computed ef.ciently. 
However, using several simplify­ing assumptions, we have developed approximate Cost and Bene.t heuristics 
that are both ef.cient to compute and accurate enough to be useful. 4 Cost Heuristic The  R Cost a OdLdOR 
t L heuristic is an estimate of the time required to render object with level of detail and rendering 
algorithm . Of course, the actual rendering time for a set of polygons depends on a number of complex 
factors, including the type and features of the graphics workstation. However, using a model of a generalized 
rendering system and several simplifying assumptions, it is possible to develop an ef.cient, approximate 
Cost heuristic that can be applied to a wide variety of workstations. Our model, which is derived from 
the GraphicsLibrary ProgrammingToolsand Techniques document from Silicon Graphics, Inc. [10], represents 
the rendering system as a pipeline with the two functional stages shown in Figure 2: Per Primitive: coordinate 
transformations, lighting calcula­ a rendering algorithm and machine. For a particular rendering algorithm 
and machine, useful values for these coef.cients can be determined experimentally by rendering sample 
objects with a wide variety of sizes and LODs, and graphing measured rendering times versus the number 
of polygons, vertices and pixels drawn. Figure 3a shows measured times for rendering four different LODs 
of the chair shown in Figure 1 rendered with .at-shading. The slope of the best .tting line through the 
data points represents the time required per polygon during this test. Using this technique, we have 
derived cost model coef.cients for our Silicon Graphics VGX 320 that are accurate within 10% at the 95% 
con.dence level. A comparison of actual and predicted rendering times for a sample set of frames during 
an interactive building walkthrough is shown in Figure 3b. 0.01 0.2  Bene.t Heuristic to model perception 
of rendering object R Polygons t 0 900 walkthrough. a tions, clipping, etc. 5 Per Pixel: rasterization, 
z-buffering, alpha blending, texture mapping, etc. The Display Figure 2: Two-stage model of the rendering 
pipeline. Since separate stages of the pipeline run in parallel, and must wait only if a subsequent stage 
is backed up, the throughput of the pipeline is determined by the speed of the slowest stage i.e., the 
bottleneck. If we assume that the host is able to send primitives to the graphics subsystem faster than 
they can be rendered, and no other operations are executing that affect the speed of any stage of the 
graphics subsystem, we can model the time required to render an object tuple as the maximum of the times 
taken by any of the stages. and rendering algorithm . Ideally, it predicts the amount and ac­ curacy 
of information conveyed to a user due to rendering an object tuple. Of course, it is extremely dif.cult 
to accurately model hu­man perception and understanding, so we have developed a simple, easy-to-compute 
heuristic based on intuitive principles. Our Bene.t heuristic depends primarily on the size of an object 
tuple in the .nal image. Intuitively, objects that appear larger to the observer contribute more to the 
image (see Figure 4). Therefore, the base value for our Bene.t heuristic is simply an estimate of the 
number of pixels covered by the object. Our Bene.t heuristic also depends on the accuracy of an object 
tuple rendering. Intuitively, using a more detailed representation or a more realistic rendering algorithm 
for an object generates a higher quality image, and therefore conveys more accurate information to the 
user. Conceptually, we evaluate the accuracy of an object tuple rendering by comparison to an ideal image 
generated with an        Observer  View Plane Figure 4: Objects that appear larger contribute 
more to the image. ideal camera. For instance, consider generating a gray-level image of a scene containing 
only a cylinder with a diffusely re.ecting Lambert surface illuminated by a single directional light 
source in orthonormal projection. Figure 5a shows an intensity plot of a sample scan-line of an ideal 
image generated for the cylinder. First, consider approximating this ideal image with an image gen­erated 
using a .at-shaded, polygonal representation for the cylinder. Since a single color is assigned to all 
pixels covered by the same polygon, a plot of pixel intensities across a scan-line of such an image is 
a stair-function. If an 8-sided prism is used to represent the cylinder, at most 4 distinct colors can 
appear in the image (one for each front-facing polygon), so the resulting image does not ap­proximate 
the ideal image very well at all, as shown in Figure 5b. By comparison, if a 16-sided prism is used to 
represent the cylinder, as many as 8 distinct colors can appear in the image, generating a more closely. 
Based on this intuition, we assume that the error, i.e., the difference from the ideal image, decreases 
with the number of samples (e.g., rays/vertices/polygons) used to render an object tu­ple, and is dependent 
on the type of interpolation method used (e.g., Gouraud/.at). We capture these effects in the Bene.t 
heuristic by multiplying by an accuracy factor:  Accuracy a OdLdR ti Error i SamplesBaseErr a Ld or 
R t m a 11where Samples(L, R) is #pixels for ray tracing, or #vertices for Gouraud shading, or #polygons 
for .at-shading (but never more than #pixels); and is an exponent dependent on the interpolation m method 
used (.at = 1, Gouraud = 2). The BaseError is arbitrarily set to 0.5 to give a strong error for a curved 
surface represented by a single .at polygon, but still account for a signi.cantly higher bene.t than 
not rendering the surface at all. In addition to the size and accuracy of an object tuple rendering, 
our Bene.t heuristic depends on on several other, more qualitative, factors, some of which apply to a 
static image, while others apply to sequences of images: Semantics: Some types of object may have inherent 
im­portance. For instance, walls might be more important than pencils to the user of a building walkthrough; 
and enemy robots might be most important to the user of a video game. We adjust the Bene.t of each object 
tuple by an amount proportional to the inherent importance of its object type. a Focus: Objects that 
appear in the portion of the screen at which closer approximation to the ideal image, as shown in Figure 
5c. the user is looking might contribute more to the image than ones in the periphery of the user s view. 
Since we currently do not Prism track the user s eye position, we simply assume that objects Intensity 
Intensity Intensity Ideal Prism Ideal Pixels Pixels a) Ideal image. b) Flat-shaded 8-sided prism. 
Prism Ideal Intensity Pixels Pixels c) Flat-shaded 16-sided prism. d) Gouraud-shaded 16-sided prism. 
Figure 5: Plots of pixel intensity across a sample scan-line of images generated using different representations 
and rendering algorithms for a simple cylinder. Next, consider using Gouraud shading for a polygonal 
represen­ tation. In Gouraud shading, intensities are interpolated between vertices of polygons, so a 
plot of pixel intensities is a continuous, piecewise-linear function. Figure 5d shows a plot of pixel 
intensities across a scan line for a Gouraud shaded 16-sided prism. Compared to the plot for the .at-shaded 
image (Figure 5b), the Gouraud shaded image approximates the ideal image much more closely. More complex 
representations (e.g., parametric or implicit sur­ faces) and rendering techniques (e.g., Phong shading, 
antialiasing or ray tracing) could be used to approximate the ideal image even a a appearing near the 
middle of the screen are more important than ones near the side. We reduce the Bene.t of each object 
tuple by an amount proportional to its distance from the middle of the screen. Motion Blur: Since objects 
that are moving quickly across the screen appear blurred or can be seen for only a short amount of time, 
the user may not be able to see them clearly. So we reduce the Bene.t of each object tuple by an amount 
proportional to the ratio of the object s apparent speed to the size of an average polygon.  ImportancBenest 
O Hysteresis: Rendering an object with different levels of detail in successive frames may be bothersome 
to the user and may reduce the quality of an image sequence. Therefore, we reduce the Bene.t of each 
object tuple by an amount proportional to the difference in level of detail or rendering algorithm from 
the ones used for the same object in the previous frame. Each of these qualitative factors is represented 
by a multiplier between 0.0 and 1.0 re.ecting a possible reduction in object tuple bene.t. The overall 
Bene.t heuristic is a product of all the afore­mentioned factors: at O q d F Ld ocus R tai O Size t q 
Motion a O t q A a O ccur t q acyHyster a Od esis LdR at O q dLdR t This Bene.t heuristic is a simple 
experimental estimate of an ob­ject tuple s contribution to model perception. GreaterBene.t is assigned 
to object tuples that are larger (i.e., cover more pixels in the image), more realistic-looking (i.e., 
rendered with higher lev­els of detail, or better rendering algorithms), more important (i.e., e a semantically, 
or closer to the middle of the screen), and more apt to blend with other images in a sequence (i.e., 
hysteresis). In our implementation, the user can manipulate the relative weighting of these factors interactively 
using sliders on a control panel, and ob­serve their effects in a real-time walkthrough. Therefore, although 
our current Bene.t heuristic is rather ad hoc, it is useful for exper­imentation until we are able to 
encode more accurate models for human visual perception and understanding. 6 Optimization Algorithm 
We use the Cost and Bene.t heuristics described in the previous sections to choose a set of object tuples 
to render each frame by solving equation 1 in Section 3. Unfortunately, this constrained optimization 
problem is NP­complete. It is the Continuous Multiple Choice Knapsack Problem [6, 7], a version of the 
well-known Knapsack Problem in which el­ements are partitioned into candidate sets, and at most one element 
from each candidate set may be placed in the knapsack at once. In this case, the set S of object tuples 
rendered is the knapsack, the object tuples are the elements to be placed into the knapsack, the target 
frame time is the size of the knapsack, the sets of object tuples representing the same object are the 
candidate sets, and the Cost and Bene.t functions specify the size and pro.t of each element, respectively. 
The problem is to select the object tuples that have maximum cumulative bene.t, but whose cumulative 
cost .ts in the target frame time, subject to the constraint that only one object tuple representing 
each object may be selected. We have implemented a simple, greedy approximation algorithm for this problem 
that selects object tuples with the highest Value (). Logically, we add object tu-   Benest a OdLdR 
t . Cost a OdLdR t ples to S in descending order of Value until the maximum cost is competely claimed. 
However, if an object tuple is added to S which represents the same object as another object tuple already 
in S, only the object tuple with the maximum bene.t of the two is retained. The merit of this approach 
can be explained intuitively by noting that each subsequent portion of the frame time is used to render 
the object tuple with the best available bang for the buck. It is easy to show that a simple implementation 
of this greedy approach runs in log time for potentially visible objects, and produces a O a nn t n 
solution that is at least half as good as the optimal solution [6]. Rather than computing and sorting 
the Bene.t, Cost, and Value for all possible object tuples during every frame, as would be required by 
a naive implementation, we have implemented an incremental optimization algorithm that takes advantage 
of the fact that there is typically a large amount of coherence between successive frames. The algorithm 
works as follows: At the start of the algorithm, an object tuple is addedto S for each potentially visible 
object. Initially, each object is assigned the LOD and rendering algorithm chosen in the previous frame, 
or the lowest LOD and rendering algorithm if the object is newly visible. In each iteration of the optimization, 
the algorithm .rst increments the accuracy attribute (LOD or rendering algorithm) of the object that 
has the highest subsequent Value. It then decrements the accuracy attributes of the object tuples with 
the lowest current Value until the cumulative cost of all object tuples in S is less than the target 
frame time. The algorithm terminates when the same accuracy attribute of the same object tuple is both 
incremented and decremented in the same iteration. This incremental implementation .nds an approximate 
solution that is the same as found by the naive implementation if Values of object tuples decrease monotonically 
as tuples are rendered with greater accuracy (i.e., there are diminishing returns with more com­plex 
renderings). In any case, the worst-case running time for the algorithm is log . However, since the initial 
guess for the O a nn t LOD and rendering algorithm for each object is generated from the previous frame, 
and there is often a large amount of coherence from frame to frame, the algorithm completes in just a 
few iterations on average. Moreover, computations are done in parallel with the display of the previous 
frame on a separate processor in a pipelined architecture; they do not increase the effective frame rate 
as long as the time required for computation is not greater than the time required for display. 7 Test 
Methods To test whether this new cost/bene.t optimization algorithm pro­duces more uniform frame rates 
than previous LOD selection algo­rithms, we ran a set of tests with our building walkthrough applica­tion 
using four different LOD selection algorithms: a) No Detail Elision: Each object is rendered at the highest 
LOD. b) Static: Each object is rendered at the highest LOD for which an average polygon covers at least 
1024 pixels on the screen. c) Feedback: Similar to Static test, except the size threshold for LOD selection 
is updated in each frame by a feedback loop, based on the difference between the time required to render 
the previous frame and the target frame time of one-tenth of a second. d) Optimization: Each object is 
rendered at the LOD chosen by the cost/bene.t optimization algorithm described in Sections 3 and 6 in 
order to meet the target frame time of one-tenth of a second. For comparison sake, the Bene.t heuristic 
is limited to consideration of object size in this test, i.e., all other Bene.t factors are set to 1.0. 
 All tests were performed on a Silicon Graphics VGX 320 work­station with two 33MHz MIPS R3000 processors 
and 64MB of memory. We used an eye-to-object visibility algorithm described in [12] to determine a set 
of potentially visible objects to be rendered in each frame. The application was con.gured as a two-stage 
pipeline with one processor for visibility and LOD selection computations and another separate processor 
for rendering. Timing statistics were gathered using a 16timer. .s In each test, we used the sample observer 
path shown in Figure 6 through a model of an auditorium on the third .oor of Soda Hall. The model was 
chosen because it is complex enough to differenti­ate the characteristics of various LOD selection algorithms 
(87,565 polygons), yet small enough to reside entirely in main memory so as to eliminate the effects 
of memory management in our tests. The test path was chosen because it represents typical behavior of 
real users of a building walkthrough system, and highlights the differ­ences between various LOD selection 
algorithms. For instance, at the observer viewpoint marked A , many complex objects are si­multaneously 
visible, some of which are close and appear large to the observer; at the viewpoint marked B , there 
are very few ob­jects visible to the observer, most of which appear small; and at the viewpoint marked 
C , numerous complex objects become visible suddenly as the observer spins around quickly. We refer to 
these marked observer viewpoints in the analysis, as they are the view­points at which the differences 
between the various LOD selection algorithms are most pronounced. LOD Selection Compute Time Frame Time 
Algorithm Mean Max Mean Max StdDev None Static Feedback Optimization 0.00 0.00 0.00 0.01 0.00 0.01 0.01 
0.03 0.43 0.11 0.10 0.10 0.99 0.20 0.16 0.13 0.305 0.048 0.026 0.008 Figure 6: Test observer path through 
a model of an auditorium.  8 Results and Discussion Figure 7 shows plots of the frame time (seconds 
per frame) for each observer viewpoint along the test path for the four LOD selection algorithms tested. 
Table 1 shows cumulative compute time (i.e., time required for execution of the LOD selection algorithm) 
and frame time statistics for all observer viewpoints along the test path. 0.2  0.0  0 Table 1: Cumulative 
statistics for test observer path (in seconds). depicts the LOD selected for each object in the frame 
for observer viewpoint A higher LODs are represented by darker shades of gray. On the other hand, the 
frame time is very short in the frame at the observer viewpoint marked B (0.03 seconds). Since all visible 
objects appear relatively small to the observer, they are rendered at a lower LOD even though more detail 
could have been rendered within the target frame time. In general, it is impossible to choose a single 
size threshold for LOD selection that generates uniform frame times for all observer viewpoints. 0 Frames 
250 0 Frames 250 a) No detail elision. b) Static algorithm. 0.2 0.2  0  0  a) Static algorithm b) 
Optimization algorithm Figure 8: Images depicting the LODs selected for each object at the observer 
viewpoints marked A using the Static and Optimization algorithms. Darker shades of gray represent higher 
LODs. 0 Frames 250 0 Frames 250 c) Feedback algorithm. d) Optimization algorithm. Figure 7: Plots of 
frame time for every observer viewpoint along test observer path using a) no detail elision, b) static 
algorithm, c) feedback algorithm, and d) optimization algorithm. Note: the Frame Time axis in plot (a) 
is .ve-times larger than the others. If no detail elision is used, and all potentially visible objects 
are rendered at the highest LOD, the time required for each frame is generally long and non-uniform, 
since it depends directly on the number and complexity of the objects visible to the observer (see Figure 
7a). In our test model, far too many polygons are visible from most observer viewpoints to generate frames 
at interactive rates without detail elision. For instance, at the observer viewpoint marked A in Figure 
6, 72K polygons are simultaneously visible, and the frame time is 0.98 seconds. Overall, the mean frame 
time for all observer viewpoints on the test path is 0.43 seconds per frame. If the Static LOD selection 
algorithm is used, objects whose average polygon is smaller than a size threshold .xed at 1024 pixels 
per polygon are rendered with lower LODs. Even though the frame rate is much faster than without detail 
elision, there is still a large amount of variability in the frame time, since it depends on the size 
and complexity of the objects visible from the observer s viewpoint (see Figure 7b). For instance, at 
the observer viewpoint marked A , the frame time is quite long (0.19 seconds) because many visible objects 
are complex and appear large to the observer. A high LOD is chosen for each of these objects independently, 
resulting in a long overall frame time. This result can seen clearly in Figure 8a which The Feedback 
algorithm adjusts the size threshold for LOD se­lection adaptively based on the time taken to render 
previous frames in an effort to maintain a uniform frame rate. This algorithm gen­erates a fairly uniform 
frame rate in situations of smoothly varying scene complexity, as evidenced by the relatively .at portions 
of the frame time curve shown in Figure 7c (frames 1 125). However, in situations where the complexity 
of the scene visible to the observer changes suddenly, peaks and valleys appear in the curve. Some­times 
the frame time generated using the Feedback algorithm can be even longer than the one generated using 
the Static algorithm, as the Feedback algorithm is lured into a inappropriately low size threshold during 
times of low scene complexity. For instance, just before the viewpoint marked C , the observer is looking 
at a relatively simple scene containing just a few objects on the stage, so frame times are very short, 
and the size threshold for LOD selection is reduced to zero. However, at the viewpoint marked C , many 
chairs become visible suddenly as the observer spins around quickly. Since the adaptive size threshold 
is set very low, inappropriately high LODs are chosen for most objects (see Figure 9a), resulting in 
a frame time of 0.16 seconds. Although the size threshold can often adapt quickly after such discontinuities 
in scene complexity, some effects related to this feedback control (i.e., oscillation, overshoot, and 
a few very slow frames) can be quite disturbing to the user. In contrast, the Optimization algorithm 
predicts the complexity of the model visible from the current observer viewpoint, and chooses an appropriate 
LOD and rendering algorithm for each object to meet the target frame time. As a result, the frame time 
generated using the Optimization algorithm is much more uniform than using any of a) Feedback algorithm 
b) Optimization algorithm Figure 9: Images depicting the LODs selected for each object at the observer 
viewpoints marked C using the Feedback and Optimiza­tion algorithms. Darker shades of gray represent 
higher LODs. the other LOD selection algorithms (see Figure 7d). For all observer viewpoints along the 
test path, the standard deviation in the frame time is 0.008 seconds, less than one third of any of the 
other three algorithms tested. The longest frame time is 0.13 seconds, and the shortest is 0.075 seconds. 
As the Optimization algorithm adjusts image quality to maintain a uniform, interactive frame rate, it 
attempts to render the best image possible within the target frame time for each observer view­point. 
As a result, there is usually little noticeable difference be­tween images generated using the Optimization 
algorithm and ones generated with no detail elision at all. A comparison of images for observer viewpoint 
A generated using a) no detail elision, and b) using the Optimization algorithm to meet a target frame 
time of one tenth of a second are shown in Figure 10. Figure 10a has 72,570 polygons and took 0.98 seconds 
to render, whereas Figure 10b has 5,300 polygons and took 0.10 seconds. Even though there are less than 
a tenth as many polygons in Figure 10b, the difference in image qualityisbarelynoticeable. Forreference,theLODchosenforeach 
object in Figure 10b is shown in Figure 8b. Note that reduction in rendering time does not map to a linear 
reduction in polygon count since polygons representing lower levels of detail tend to be bigger on average. 
The Optimization algorithm is more general than other detail eli­sion algorithms in that it also adjusts 
the rendering algorithm (and possibly other attributes in the future) for each object independently. 
Examine Figure 11, which shows three images of a small library on the sixth .oor of Soda Hall containing 
several textured surfaces. Figure 111, shows an image generated using no detail elision it contains 
19,821 polygons and took 0.60 seconds to render. Fig­ bacbc ures 111 and 111 show images generated for 
the same observer viewpoint using the Optimization algorithm with target frame times of b) 0.15 seconds 
(4,217 polygons), and c) 0.10 seconds (1,389 polygons). Although the Optimization algorithm uses lower 
levels of detail for many objects (see Figures 111 and 111), and gener­ates images that are quite different 
than the one generated with no detail elision (see Figures 112 and 112), all three images look very bbcc 
similar. Notice the reduced tessellation of chairs further from the observer, and the omission of texture 
on the bookshelves in Figure 111. Similarly, notice the .at-shaded chairs, and the omission of books 
on bookshelves and texture on doors in Figure 11 1. Having experimented with several LOD selection algorithms 
in an interactive visualization application, we are optimistic that vari­ation in image quality is less 
disturbing to a user than variation in the frame times, as long as different representations for each 
ob­ject appear similar, and transitions between representations are not very noticeable. Further experimentation 
is required to determine which types of rendering attributes can be blended smoothly during interactive 
visualization. 9 Conclusion We have described an adaptive display algorithm for fast, uniform frame 
rates during interactive visualization of large, complex virtual environments. The algorithm adjusts 
image quality dynamically in order to maintain a user-speci.ed frame rate, selecting a level of detail 
and an algorithm with which to render each potentially visible object to produce the best image possible 
within the target frame time. Our tests show that the Optimization algorithm generates more uniform frame 
rates than other previously described detail elision algorithms with little noticeable difference in 
image quality dur­ing visualization of complex models. Interesting topics for fur­ther study include 
algorithms for automatic generation of multi­resolution models, and experiments to develop measures of 
image quality and image differences. 10 Acknowledgements We are grateful to Thurman Brown, Delnaz Khorramabadi, 
Priscilla Shih and Maryann Simmons for their efforts constructing the build­ing model. Silicon Graphics, 
Inc. has been very generous, allowing us to use equipment, and donating a VGX 320 workstation to this 
project as part of a grant from the Microelectronics Innovation and Computer Research Opportunities (MICRO 
1991) program of the State of California. We appreciate the assistance of Greg Ward, Sharon Fischler, 
and Henry Moreton who helped generate the color prints for this paper. Finally, we thank Seth Teller 
for his spatial sub­divisions, visibility algorithms, and other important contributions to this project. 
 References [1] Airey, John M., Rohlf, John H., and Brooks, Jr., Frederick P. Towards Image Realism 
with Interactive Update Rates in Complex Virtual Building Environments. ACM SIGGRAPH Special Issue on 
1990 Symposium on Interactive 3D Graphics , 24, 2 (1990), 41-50. [2] Blake, Edwin H. A Metric for Computing 
Adaptive Detail in Animated Scenes using Object-Oriented Programming. Euro­graphics 87. G. Marechal (Ed.), 
Elsivier Science Publishers, B.V. (North-Holland), 1987. [3] Brooks, Jr., Frederick P. Walkthrough -A 
Dynamic Graphics System for Simulating Virtual Buildings. Proceedings of the 1986 Workshop on Interactive 
3D Graphics . [4] Clark, James H. Hierarchical Geometric Models for Visible Surface Algorithms. Communications 
of the ACM, 19, 10 (Oc­tober 1976), 547-554. [5] Funkhouser, Thomas A., S´equin, Carlo H., and Teller, 
Seth J. Management of Large Amounts of Data in Interactive Build­ing Walkthroughs. ACM SIGGRAPH Special 
Issue on 1992 Symposium on Interactive 3D Graphics , 11-20. [6] Garey, Michael R., and Johnson, David 
S. Computers and Intractibility: A Guide to the Theory of NP-Completeness. W.H. Freeman and Company, 
New York, 1979. [7] Ibaraki, T., Hasegawa, T., Teranaka, K., and Iwase J. The Multiple Choice Knapsack 
Problem. J. Oper. Res. Soc. Japan 21, 1978, 59-94. [8] Rossignac, Jarek, and Borrel, Paul. Multi-resolution 
3D ap­proximations for rendering complex scenes. IFIP TC 5.WG 5.10 II Conference on Geometric Modeling 
in Computer Graphics, Genova, Italy, 1993. Also available as IBM Re­search Report RC 17697, Yorktown 
Heights, NY 10598. [9] Schachter, Bruce J. (Ed.). Computer Image Generation. John Wiley and Sons, New 
York, NY, 1983. [10] Graphics Library Programming Tools and Techniques , Docu­ment #007-1489-01, Silicon 
Graphics, Inc., 1992. [11] Teller, Seth J., and S´equin, Carlo H. Visibility Preprocessing for Interactive 
Walkthroughs. Proceedingsof SIGGRAPH 91. In Computer Graphics 25, 4 (August 1991), 61-69. [12] Teller, 
Seth J. Visibility Computations in Densely Occluded Polyhedral Environments . Ph.D. thesis, Computer 
Science Di­vision (EECS), University of California, Berkeley, 1992. Also available as UC Berkeley technical 
report UCB/CSD-92-708. [13] Zyda, Michael J. Course Notes, Book Number 10, Graphics Video Laboratory, 
Department of Computer Science, Naval Postgraduate School, Monterey, California, November 1991. a) No 
detail elision b) Optimization algorithm (0.10 seconds) a b 1 a) No detail elision b 1  b  b) Optimization 
algorithm (0.15 seconds) 1 2 3 c) Optimization algorithm (0.10 seconds) Figure 11: Images of library 
generated using a) no detail elision (19,821 polygons), and the Optimization detail elision algorithm 
with target frame times of b) 0.15 seconds (4,217 polygons), and c) 0.10 seconds (1,389 polygons). LODs 
chosen for objects in 1 and 1 are shown in 2 and darker shades of gray represent higher  ccc 2 bbcc 
c c 3 a ab t abs a acb t LODs. Pixel-by-pixel differences 1 1and 1 1are shown in 3 and 3 brighter colors 
represent greater difference. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166150</article_id>
		<sort_key>255</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>33</seq_no>
		<title><![CDATA[Discrete groups and visualization of three-dimensional manifolds]]></title>
		<page_from>255</page_from>
		<page_to>262</page_to>
		<doi_number>10.1145/166117.166150</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166150</url>
		<keywords>
			<kw><![CDATA[curvature]]></kw>
			<kw><![CDATA[discrete group]]></kw>
			<kw><![CDATA[geodesic]]></kw>
			<kw><![CDATA[hyperbolic geometry]]></kw>
			<kw><![CDATA[projective geometry]]></kw>
			<kw><![CDATA[quotient space]]></kw>
			<kw><![CDATA[spherical geometry]]></kw>
			<kw><![CDATA[tessellation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Hierarchy and geometric transformations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010244</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Hierarchical representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31078312</person_id>
				<author_profile_id><![CDATA[81332502305]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Charlie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gunn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Alan E Beardon. The Geometry of Discrete Groups. Springer-Verlag, 1983.]]></ref_text>
				<ref_id>Bea83</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Carl B. Boyer. A History of Mathematics. Princeton University Press, 1968.]]></ref_text>
				<ref_id>Boy68</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Manfredo P. Do Carmo. Differential Geometry of Curves and Surfaces. Prentice-Hall, 1976.]]></ref_text>
				<ref_id>Car76</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[A. Cayley. A sixth memoir upon quantics. Philosophical Transactions of the Royal Society of London, 149:61-90, 1859.]]></ref_text>
				<ref_id>Cay59</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[H.M.S. Coxeter. Non-Euclidean Geometry. University of Toronto Press, 1965.]]></ref_text>
				<ref_id>Cox65</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[H.M.S. Coxeter. Regular Polytopes. Dover, 1973.]]></ref_text>
				<ref_id>Cox73</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[H.M.S. Coxeter. Projective Geometry. Springer Verlag, 1987.]]></ref_text>
				<ref_id>Cox87</ref_id>
			</ref>
			<ref>
				<ref_obj_id>573874</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[D. B. A. Epstein, Jim Cannon, Derek Holt, Silvio Levy, Mike Patterson, and William Thurston. Word Processing in Groups. Jones and Bartlett, 1991.]]></ref_text>
				<ref_id>ECH+91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134031</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Helaman Ferguson, Alyn Rockwood, and Jordan Cox. Topological design of sculptural surfaces. Computer Graphics, 26:149-156, July, 1992. Proceedings of SIGGRAPH 1992.]]></ref_text>
				<ref_id>FRC92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[James Foley, Andries van Dam, Steven Feiner, and John Hughes. Computer Graphics: Principles and Practice. Addison-Wesley, 1990.]]></ref_text>
				<ref_id>FvDFH90</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[S. Gabriel and J. Kajiya. Spline interpolation in curved space. In State of the Art Image Synthesis, 1985. Course notes for SIGGRAPH 1985.]]></ref_text>
				<ref_id>GK85</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Charlie Gunn and Delle Maxwell. Not Knot. Jones and Bartlett, 1991.]]></ref_text>
				<ref_id>GM91</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Chaflie Gunn. A computer implementation of the twodimensional euclidean crystallographic groups. Master's thesis, UNC, Chapel Hill, 1983.]]></ref_text>
				<ref_id>Gun83</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Chaflie Gunn. Visualizing hyperbolic geometry. In Computer Graphics and Mathematics, pages 299-313. Eurographics, Springer Veflag, 1992.]]></ref_text>
				<ref_id>Gun92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>76331</ref_obj_id>
				<ref_obj_pid>76263</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Ping-Kang Hsiung and Robert H.E Dunn. Visualizing relativistic effects in spacetime. In Supercomputing 89. IEEE/ACM, Nov, 1989.]]></ref_text>
				<ref_id>HD89</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Silvio Levy. Automatic generation of hyperbolic tilings. Leonardo, 35:349-354, 1992.]]></ref_text>
				<ref_id>Lev92</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[E.H. Lockwood and R. H. Macmillan. Geometric symmetry. Cambridge University Press, 1978.]]></ref_text>
				<ref_id>LM78</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Tamara Munzner, Stuart Levy, Mark Phillips, Nathaniel Thurston, and Celeste Fowler. Geomview an interactive viewing program for sgi workstations, ftp@geom.umn.edu.]]></ref_text>
				<ref_id>MLP+</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[James Munkres. Topology: A First Course, chapter 8. Prentice-Hall, 1975.]]></ref_text>
				<ref_id>Mun75</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147206</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Mark Phillips and Charlie Gunn. Visualizing hyperbolic space: Unusual uses of 4x4 matrices. In 1992 Symposium on Interactive 3D Graphics, pages 209- 214. ACM SIGGRAPH, ACM, 1992.]]></ref_text>
				<ref_id>PG92</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[R.L.E. Schwarzenberger. N-Dimensional Crystallography. Pitman Publishing, 1980. chapters 13-16.]]></ref_text>
				<ref_id>Sch80</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[William Thurston. Three dimensional manifolds, kleinian groups and hyperbolic geometry. BAMS, 19:417-431, 1982.]]></ref_text>
				<ref_id>Thu82</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Steve Upstill. The Renderman Companion. Addison- Wesley, 1989. chapters 13-16.]]></ref_text>
				<ref_id>Ups89</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Jeff Weeks. snappea- a macintosh application for computing 3-manifolds. ftp@geom.umn.edu.]]></ref_text>
				<ref_id>Wee</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[JeffWeeks. The Shape of Space. MarcelDekker, 1985.]]></ref_text>
				<ref_id>Wee85</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Frederick Woods. Higher Geometry. Dover, 1961 (1922).]]></ref_text>
				<ref_id>Woo22</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 space, these groups act on the curved geometries of hyperbolic and spherical space. We construct easily 
computable models of our geometric spaces based on projective geometry; and establish algorithms for 
visualization of three-dimensional manifolds based upon the close connection between discrete groups 
and manifolds. We describe an object-oriented implementation of these concepts, and several novel visualization 
applications. As a visualization tool, this software breaks new ground in two directions: interactive 
exploration of curved spaces, and of topological manifolds modeled on these spaces. It establishes a 
generalization of the application of projective geometry to computer graphics, and lays the groundwork 
for visualization of spaces of non-constant curvature. CR Categories and Subject Descriptors : I.3.3 
[Picture/Image Generation] display algorithms I.3.5 [Computational Geometry and Object Modeling Graphics]: 
geometric algorithms, hierarchy and geometric transformations, I.3.7 [Three dimensional Graphics and 
Realism] color, shading, shadowing, and texture Additional Key Words and Phrases: discrete group, tessel­lation, 
quotient space, projective geometry, hyperbolic geometry, spherical geometry, curvature, geodesic. 1 
Discrete Groups Symmetry, broadly speaking, implies a redundant supply of infor­mation. A mirror image 
contains the same information as the scene that it mirrors. The theory of discrete groups has been developed 
over the past 100 years as a formalization of the process of extract­ing a single copy of the information 
present in symmetric con.g­urations. The discrete groups which we study here are groups of motions which 
act on a geometric space, such as Euclidean space, to produce tessellations by congruent non-overlapping 
cells. Fa­miliar examples include wallpaper patterns, and the interlocking designs of M. C. Escher. We 
consider two simple examples before introducing mathematical de.nitions. r provided that the copies are 
not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the 
publication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 
&#38;#169;1993 ACM-0-89791-601-8/93/008/0015 $1.50 Permission to copy without fee all or part of this 
material is granted provided that the copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the publication and its date appear, and notice is given that 
copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, 
requires a fee and/or specific permission. &#38;#169;1993 ACM-0-89791-601-8/93/008 $1.50 Currentaddress: 
SFB288,MA8-5,TechnischeUniversitlat,Strassedes 17 Juni 136, 1 Berlin 12, Germany, gunn@sfb288.math.tu-berlin.de 
 Discrete Groups and Visualization of Three-Dimensional Manifolds Charlie Gunn The Geometry Center, The 
University of Minnesota 1.1 The circle and the line Abstract When we evaluate the expression sin2we 
are only interested in  xR x. x R x  x mod 1, since sin is a periodic function: sin2sin2We describe 
a software implementation for interactive visualization , where is an integer. The set of all motions 
of the real line k k of a wide class of discrete groups. In addition to familiar Euclidean by integer 
amounts forms a group, which leaves invariant the function sin2 . We can form the quotient , which is 
the set of equivalence classes with respect to this group. This quotient can be represented by the closed 
interval, with the understanding that we identify the two endpoints. But identifying the two endpoints 
yields a circle. Once we know the values of sin2on the circle, we can compute it for any other value 
, simply by subtracting or adding integers to until the result lies in the range. In this example the 
discrete groupis the set of transformations of given by all translations , where is an integer.  RR 
y Sx Rx 0 yk 1 kx 0 S 1 is discrete since no non-trivial sequence inconverges to the the identity 
element. The quotient of under this action is 1, the unit circle. We write . S 1 -1 a-1 0 a 1 a+1 Figure 
1: The circle is the quotient of by the integers. I 0 R 1 is a fundamental domain for this group action. 
We can recover from the fundamental domain and: the union g 2 gI R covers without overlap. We move into 
two dimensions to bring out other features of the concepts introduced in this example. 1.2 The torus 
and the plane Instead of we now work with 2. Letbe the group of trans­  x y R R R 1 x yR x R 1 y T 
x y lations of 2 generated by1and 1, that is, unit translations in the coordinate directions. What is 
the quotient 2? Instead of the unit interval with its endpoints identi.ed, we are led to a unit square 
that has its edges identi.ed in pairs. If we imagine the square is made of rubber and that we can perform 
the identi.cations by bending the square and gluing,we.ndthattheresultingsurfaceisthetorus 2. SeeFigure 
2. P l P P l P P m m m P l P Figure 2: Making a torus from a square 1.3 Algebra and geometry: the fundamental 
group A key element of this approach is the interplay of algebraic and geometric viewpoints. To clarify 
this, we introduce the fundamental group of a space, formed by taking all the closed paths based at some 
point in the space. We get a group structure on this set: we can add paths by following one and then 
the other, and subtract by going around the second path in the reverse order. The zero-length path is 
the identity element. If one path can be moved or deformed to another path, the two paths correspond 
to the same group element. It is easy to check that different s yield isomorphic groups. We P say a space 
is simply connected if every closed path can be smoothly shrunk to a point, like a lasso, without leaving 
the space. [Mun75] The fundamental group of a simply connected space consists of just the identity element. 
In the above example 2 is simply connected; while 2, the quotient, isn t. When is the quotient of a simply 
connected YYXRTX space , we say that is the universal covering space of . The importance of simply connected 
spaces in the study of discrete groups is due to a basic result of topology that (subject to technical 
constraints which we will consider satis.ed) every space has a unique universal covering space [Mun75]. 
So in considering group actions, we need only consider actions on simply connected spaces. The interplay 
of algebra and geometry reveals itself in the fact that the fundamental group of the quotient, a purely 
topological object, is isomorphic to the group of symmetries , which arises in a purely geometric context. 
 1.4 Inside versus Outside Views In the cases we will consider, the universal covering space is a X geometric 
space, that is, it comes equipped with a metric that deter­mines distance between points and angles between 
tangent vectors. In this case we sometimes refer to as a model geometry. This metric allows us to compute 
geodesics, or shortest paths, between points in the space [Car76]. The quotient space inherits this metric.RRXTTR 
2 is the universal covering space of 2: if we unroll 2 onto 2, the copies of the torus will cover the 
plane completely, without over­lap. We say these copies tessellate the plane. For some purposes the rolled-up 
torus sitting in 3 is useful, but to gain the experience of what it is like to live inside the surface, 
we are better served by examining the tessellation of the universal covering space produced by the group. 
For example, if we want to make pictures of what an inhabitant 2 2: of sees, we will make them in Light 
follows geodesics, which appear to be very complicated on the rolled-up torus, but in 2 are just ordinary 
straight lines. A complicated closed path based at which wraps around the torus several times unrolls 
in the universal cover to be an ordinary straight line connecting and forsome . SeeFigure3. Animmediateconsequenceofthisis 
that an observer on the torus based at sees many copies of himself, one for every closed geodesic on 
the surface passing through . For example, if he looks to the left he sees his right shoulder; if he 
looks straight ahead he sees his back. See [Wee85] for a complete and elementary description of this 
phenomenon. We say the rolled-up torus represents the outsider s view; while the unrolled view we term 
the insider s view, since it shows what someone living inside the space would see. The importance of 
the insider s view becomes more telling in three dimensional spaces, since to roll up our fundamental 
domains requires four or more dimensions. In this case the insider s view becomes a practical necessity. 
Figure 3: Outside and inside views of a complicated torus path When we try to perform the analogous construction 
for the two­holed torus, instead of a square in the Euclidean plane 2, we are HR led to a regular octagon 
in the hyperbolic plane 2[FRC92]. We describe hyperbolic geometry in more detail below. 1.5 De.nition 
of discrete group A discrete group is a subgroup of a continuous group such that there is a neighborhood 
of the identity in with , the identity element. In the example of the torus above, the group acts on 
2. Such an action on a topological space is called properly discontinuous if for every closed and bounded 
subset of , the set of K i K 6 dUXX KGXU i GR I 2 such thatis .nite. In the cases to be discussed here, 
is discrete if and only if the action of is properly discontinuous. If in addition the quotient space 
is compact, we say that is a crystallographic, or crystal, group. The group of the torus discussed in 
1.2 above is a crystallo­graphic group, the simplest so-called wallpaper group. There are exactly 17 
wallpaper groups of the Euclidean plane. See [Gun83] for a full discussion of this case and the details 
of a computer im­plementation. 1.6 Dirichlet domains Given a discrete group, there is a technique for 
constructing a funda­mental domain, known as a Dirichlet domain. We de.ne it now for future reference. 
Given a discrete group acting on a space and a point , the orbit of under is . Then the  P 2 XPQ 2 OO 
PPP O P S g 2 gPXX Dirichlet domain with respect to is the set of points in which are closer to than 
to any other point of . We can be more precise. For each , construct the perpendicular bisector Figure 
4: (235), (236) and (237) triangle groups tessellate 22, and 2. of the segment . Denote by the half-space 
containing P of which the other geometries arise. 1 Q Q P bounded by . Then the Dirichlet domain determined 
by The projective plane 2 is gotten from the ordinary plane by and is adjoining a line at in.nity. Projective 
space can be constructed in every dimension by adjoining an 1 dimensional hyperplane at in.nity. We assume 
the reader is familiar with homogeneous co- In practice, for many of the groups the intersection can 
be assumed ordinates for projective space [Cox65]. The group of self-mappings to involve only .nitely 
many s. The resulting polyhedron is of projective space can then be represented via homogeneous convex. 
If a face is determined by , then 1will coordinates as elements of the matrix group 1 , the FF 0 Q H 
Q D QP 2 PFnP n n r nn 1 . PPggg be a congruent face determined by This face pairing is used in the sequel. 
Note that, since depends upon , there are potentially many different shapes for the Dirichlet domain 
for a given group. [Bea83] Computational geometers may recognize that a Dirichlet domain with respect 
to is a Voronoi cell with respect to the orbit of . 2 Non-Euclidean Geometries In the examples above, 
the model geometry was Euclidean. There are two other simply connected two-dimensional spaces in addition 
to 2 which can serve as our model geometries: the sphere 2 and the hyperbolic plane 2. They have geometries 
(to be described in more detail below) which satisfy all the postulates of Euclidean geometry except 
for the Parallel Postulate: Given a line and a  RPL nLHnLPMPLHS point not on , there is a unique line 
passing through which is parallel to . The sphere has no parallel lines; while 2 has in.nitely many for 
a given and . See [Cox65] for an account of the discovery and development of these non-Euclidean geometries. 
An equivalent characterization of Euclidean, spherical, and hy­perbolic geometry is that the sum of the 
angles of a triangle is, respectively, equal to, greater than, or less than, . Figure 4 shows tessellations 
of these three spaces by triangles with angles 2 3, where 5 6 7 yields spherical, Euclidean, and hyperbolic 
space. We now turn to demonstrating models for these three geometries which share a common root in projective 
geometry. This will lead directly to techniques for visualizing discrete groups which act on these spaces. 
2.1 Projective geometry Projective geometry is the geometry of lines without regard to dis­tance or measure. 
It was discovered at roughly the same time as the the non-Euclidean geometries discussed above; we show 
in the sequel how it can be considered to be the fundamental geometry out projective general linear group. 
This group consists of all invertible matrices of dimension 1 1 , where two matrices are equivalent if 
one is a scalar multiple of the other [Cox87]. Much of the success of the approach described in this 
paper is due to the cir­cumstance that many computer graphics rendering transformation pipelines support 
4 .  PGLR 2.2 From projective to metric geometry Projective geometry does not include a notion of distance 
or angle measure. However, every projective transformation preserves a quantity known as the cross ratio 
. The cross ratio is a function of four collinear points: nABCDAB C BA D Here the points are represented 
by a homogeneous coordinate sys­tem on their common line; for convienience we can assume this is ordinary 
Euclidean measure on the line. This invariant has been used by Cayley to construct metric geometries 
on the foundation of projective geometry [Cay59]. First choose a homogeneous conic which is to be invariant. 
The conic is known as the Absolute for the associated geometry. The projective transformations preserving 
form a subgroup of the full projective group. Two given points 0 and 1 determine a line, which intersects 
the conic in a pair of points 0 and 1, whose coordinates may be complex numbers. Then de.ne a distance 
functionTdPKPKnQTQTQPPPLPLTHQ 0 1 log 01 01 (1) where the constant is determined according to the nature 
of in order to make the distance function real. Since the cross ratio is a multiplicative function, use 
of the log function yields an additive function. Measurement of angles between lines 0 and 1 proceeds 
1See Appendix A.1 in like manner, by determining the two tangent lines to which lie in the pencil of 
lines determined by 0 and 1. This yields models for spherical, hyperbolic, and Euclidean geometry which 
share the same straight lines; what is different is how distance along them and between them is measured. 
The subgroup becomes the isometry group for the metric geometry. We will for simplicity s sake work in 
two dimensions, that is, Hx yLwLQ with homogeneous coordinates , and consider only distance measurement, 
not angle measurement. All our results generalize directly to arbitrary higher dimension. Since the cases 
of spherical and hyperbolic geometry are more straightforward, we begin with them. 2.2.1 Spherical geometry 
For the spherical case, we choose to be the totally imaginary conic 2 2 2 0. The proper choice for is 
2. We can derive from an inner product between pairs of points: if  PyyxwxwyydwPQwPPx p yQPw P PP PP 
PKi xx 0 00 0and1 11 1then01 01 01 01. Then (1) reduces to: 0 1 arccos 00 11 This is the familiar measurement 
between points on the unit sphere. Projective transformations which preserve Q constitute the special 
orthogonal group SO(3), the group of rotations of three-dimensional Euclidean space. Although it is tempting 
to consider the familiar picture of 2 sitting isometrically in 3, it is more appropriate to KSPQwRPSwx 
 think of the model presented purely in terms of 2. In this model, to each point of 2 we assign two antipodal 
points of 2. 2.2.2 Hyperbolic geometry Forthehyperboliccase,wechoose tobethetotallyrealconic 2 2 2 0, 
a cone aligned with the -axis. The correct choice for is 12 . The derived inner product of two points 
0 0 0 0  yPwP PwdxP yPwHHPQ p PP P xHPx PPyHyxyw and 1 11 1isthen 01 01 01 01, sometimes called the 
Minkowski inner product. Our model for hyperbolic geometry will consist of the interior of this cone, 
where P P 0 P 1 0. Then (1) reduces to: 01 0 1 arccosh 00 11 where 0 and 1 lie in the interior of the 
cone. The isometry group is SO(2,1), the so-called Minkowski group. Consider the hyperboloid of two sheets 
, de.ned by the con­dition 1. Just as the unit sphere is a model for spherical geometry, the upper sheet 
of is a model for hyperbolic geometry. The most convenient model for 2 is hidden within . Consider the 
plane 1. It intersects in a circle that bounds a diskDHDD . We can project our hyperboloid onto from 
the origin. This projection respects the distance function de.ned above (it is, after all, a projective 
invariant). Then is a model of hyperbolic geometry, the so-called Klein or projective model. It is shown 
in the right-most .gure in Figure 4. In three dimensions, this yields 33 a model of as the interior of 
the unit ball in . There are HR several other commonly used models of hyperbolic geometry, most notably 
the Poincarre or conformal model [Bea83]. Our choice of the projective model here was determined by the 
fact that it yields the correct results for visualizing the insider s view. 2.2.3 Euclidean geometry 
Euclidean, or parabolic, geometry arises when apply a limiting process to the conic 2 2 2 0. As 0, the 
expression for distance reduces to 22  RPdPEPP.x p yxwxy.SyO 01 0101 where 0 and 1 have been dehomogenized. 
The isometry group of this geometry 2 is the semi-direct product of 2 , the circle, and 2 , the two-dimensional 
Euclidean translation group. 2.3 Comments This development in terms of projective geometry is given fully 
in [Woo22] and is due to Cayley and Klein. For a treatment de­rived from the modern differential geometric 
viewpoint see [Car76]; for an implementation description following this viewpoint see [Gun92]. To justify 
the use of the names spherical and hyperbolic it is worthwhile to verify that the geometries induced 
by the indicated metrics on the indicated subspaces in fact yield geometries which behave correctly with 
respect to parallel lines and sums of angles of triangles. For a detailed discussion of how to construct 
isometries of hy­perbolic 3-space in the projective model discussed here see [PG92]. 2 The above results, 
stated for the two-dimensional case, can be extended to arbitrary dimension. 3 Manifolds and Discrete 
Groups An -dimensional manifold, or -manifold, is a topological space such that is locally homeomorphic 
to , that is, every point  X XnXR n MnMR n X of has a neighborhood that can be mapped 1-1 and continuously 
onto a small ball in . If in addition we can realize as the quotient of a geometric space by a discrete 
group, we say that has a geometric structure modeled on . A related concept to that of manifold is orbifold. 
An orbifold is like a manifold, but it may have singular points where it is locally homeomorphic not 
to R n R n but rather to the quotient of by a .nite group. Orbifolds arise, generally speaking, when 
the elements of the discrete group have .xed points, such as rotations or re.ections. Initial work on 
the connection of discrete groups and theory of manifolds was done by Henri Poincarre in the 1880 s. 
To this day much research in this .eld is driven by the Poincarre Conjec­ture, which asserts that a closed, 
connected, simply connected 3­dimensional manifold is homeomorphic to the 3-dimensional sphereS 3. This 
conjecture is closely related to the classi.cation problem: making a list of all 3-manifolds. For example, 
in dimension 2, there is a uniformization theorem which says that any closed 2­dimensional manifold has 
a geometric structure modeled on one ofSRH 2, 2,or 2. RecentworkbyThurstonandothershasshownthat many 
(possibly all) 3-manifolds have essentially unique geometric structures. That is, there are good reasons 
to believe that to every 3-manifold there corresponds an essentially unique discrete group [Thu82]. The 
geometric structures for 3-manifolds come from eight model geometries: 3, 3, and 3 plus .ve additional 
simply RSH connected spaces. The additional .ve are not as nice as the .rst 2See Appendix A.2. three, 
since they are not isotropic: not all directions in space are the same. In any case, the most prevalent 
geometric structure is hyperbolic. The current software implementation does not support these .ve additional 
geometries. In the discussion that follows, we will concentrate on the in­sider s, rather than the outsider 
s, view of three dimensional orb­ifolds. That is, we will look at the tessellations of the simply connected 
space (Euclidean, hyperbolic, or spherical) induced by discrete groups. 4 Software Implementation 4.1 
OOGL In order to visualize the spaces under consideration, we have de­veloped an implementation within 
an object-oriented graphics li­brary, OOGL. The generic OOGL class is Geom. Subclasses in­clude include 
geometric primitives such as PolyList, Vect, Bezier, and Mesh; and organizational objects such as List 
and Inst(for instancing geometry). Methods with which Geoms come equipped include: Bound, Create, Copy, 
Delete, Save, Load, Pick, and Draw . . An interactive viewer, Geomview [MLP], has been con­structed based 
upon OOGL. It supports viewing in the three geome­tries discussed above: Euclidean, hyperbolic, and spherical. 
This is possible since as noted above isometries in the three geometries can be expressed as elements 
of 4 . The underlying low-level graphics libraries (in the case of OOGL, GL or Renderman 3 ) sup­port 
the use of elements of 4 for modeling and viewing P GL R PGLR transformations. This is a result of the 
fact that 4 is the smallest group which contains both the Euclidean isometries and the perspective transformation. 
The visualization task is also made easier by the fact that OOGL supports 4-dimensional vertices within 
all primitives. This provides a base for creating geometric models in hyperbolic and spherical space 
using homogeneous coordinates.  4.2 Shading We have established how it is possible to implement non-Euclidean 
isometries using standard projective transformations. We have not addressed the question of correct lighting 
and shading of surfaces in these spaces. Indeed, the standard shading algorithms (in contrast to the 
standard transformations) are implicitly Euclidean. In or­der to model the behavior of light correctly 
in these non-Euclidean spaces, it is necessary to provide customized shaders which replace the default 
ones. This has been successfully achieved within the Renderman shading language [Ups89],[Gun92]. Figure 
5 shows a view inside hyperbolic space from the movie Not Knot . Interac­tive software shaders for OOGL 
for hyperbolic and spherical space have also been written. These custom shaders use the expressionsfor 
distance and angle described in 2.2 to replace the Euclidean ones. Additionally, the decay of light intensity 
as a function of distance depends on the formula for the surface area of a sphere in each space. That 
is, the amount of light falling on an area element at distance from a light source will be inversely 
proportional to the total area of the sphere with radius . For example, in hyperbolic space light decays 
dkdr exponentially: the area of a sphere of radius r is given by sinh Figure 5: A view of the tessellation 
of hyperbolic space by regular right-angled dodecahedra, as in the movie Not Knot . This image was rendered 
using Renderman. and sinh exp for large . The shaders used to create .gures rrr 6 and 9 also involve 
a term to model fog. 4 5 The DiscreteGroupclass The DiscreteGroup class is a subclass of Geom. The min­imal 
data includes a set of generating isometries represented by elements of 4 and some geometric data, represented 
by PGLR other OOGL objects. The DiscreteGroup class supports the standard methods listed above, and other 
methods of its own. Because of the close connection to manifolds outlined in Section 3, it can also be 
thought of as a Manifold class. Many design decisions were made to support visualization of the insider 
s view of a manifold. From this point of view, every element of the scene description belongs to the 
manifold and hence should be tessellated by the group in the process of creating the insider s view. 
We have departed from this philosophy in one important respect: we do not tessellate the lights contained 
in the scene description. To do so would have sacri.ced interactivity for a questionable increase in 
authenticity. Points of interest among DiscreteGroupmethods include: 5.1 File format There is an ascii 
.le format for loading and saving discrete groups. 5 This format supports the three geometries described 
above, and includes lists of generators and group elements and also geometric objects for display within 
the tessellation. 5.2 DiscreteGroupDraw Each DiscreteGroupinstance includes a list of group elements 
and a collection of other Geoms. The general algorithm transforms each Geomby each group element and 
then draws it. There are some subtleties. Most of these groups are in.nite, but we only compute and store 
a .nite list of elements at any time. One of the dif.culties 4See Appendix A.3. 3GLisatrademarkofSiliconGraphics,Inc.;andRenderman,ofPixar. 
5SeeAppendixA.4. Figure 6: A view inside the Euclidean orbifold from Not Knot with the camera as a paper 
airplane. of navigating in the tessellations produced by discrete groups is that normal .ight tends to 
wander to the edge of the computed tessellation. To solve this problem, the DiscreteGroupobject is provided 
with an automatic centering mechanism. It detects when the camera leaves the Dirichlet domain de.ned 
by the group, and moves the camera by an isometry (determined by the face-pairings), to stay within this 
central region. Note that since lighting is not tessellated, lights must be de.ned within the camera 
coordinate system in order that lighting is invariant under this movement. Another added feature is that 
there is a separate associated Geom which represents the camera, or observer. Before being tessellated 
it is moved to the location of the camera, which as described above is constrained to stay within the 
Dirichlet domain. The observer then becomes aware of his own movement in the space. This is an important 
feature especially for detecting the singular locus of orbifolds. For example, when the camera approaches 
a axis of symmetry of order in an orbifold, this fact is made clear by the nn approach of 1 other copies 
of the camera to the same axis, a symmetry which the geometry of the Dirichlet domain alone may not reveal. 
 5.3 DiscreteGroupEnum(int constraint() ) is a method for enumerating lists of group elements given 
the gen­erators. One such list is used by the draw routine: it de.nes which copies of the fundamental 
domain to draw. The constraint func­tion accepts a single group element and returns 0 or 1 according 
to whether it satis.es its criteria. For example, a matrix may be rejected if it moves the origin far, 
its determinant is small, or its expression as a word in the generating elements is long. This enu­meration 
software uses software acceleration provided by the theory . of automatic groups [ECH91], [Lev92] if 
an automatic structure Figure 7: A typical session of Maniviewshowing some of its panels but hiding Geomviewpanels. 
 5.4 DiscreteGroupDirDom creates a fundamental domain using the Dirichlet domain algorithm described 
above. This is useful for exploring groups for which no other geometry has been provided. For display 
purposes, both a wire-frame of the full polyhedron and a possibly scaled version with faces colored to 
re.ect the face-pairing identities are drawn. See Figure 9. The user can deduce features of the group 
by examining the face-pairing patterns, or by moving the distinguished point . P   6 Example applications 
A variety of applications have been developed based on the DiscreteGroupsoftware class. Maniview is short 
for Manifold Viewer. In the paradigm of object-oriented software tools, it is essentially an Inspector 
for the class DiscreteGroup . Maniview communicates with Geomviewvia a two-way pipe. Geomviewreads the 
description of the discrete group output by Maniview and displays it. The user typically loads a discrete 
group into Maniview, and then manipulates the discrete group via a set of control panels. These panels 
are grouped into: display settings, enumeration of group elements,choiceoffundamentaltile, andsavingandloadingvarious 
elements. A typical snapshot of a Maniview session is shown in Figure 7. One of the milestones in the 
theory of discrete groups was the enumeration of the 230 crystal groups in three dimensional Euclidean 
space at the end of the nineteenth century. For a sur­vey see [LM78],[Sch80]. eucsyms, an interactive 
application which allows the exploration of these groups has been developed by Olaf Holt at the Geometry 
Center, and adapted to use the DiscreteGroup software. eucsyms is connected by a two­way pipe with Maniview. 
Euclidean crystal groups can be written as a semi-direct product of a translation subgroup and a .nite 
sub­ has been provided for the discrete group. group of SO(3), a point group. The structure of eucsymsre.ects 
 Figure 8: A snapshot of a session using eucsyms, an appli­cation for exploring the 230 Euclidean crystal 
groups. This .gure shows an earlier version of eucsyms than that de­scribed in the paper. See the paper 
version for an up-to-date .gure. this decomposition. Choice of group proceeds by .rst choosing the lattice 
which the translation subgroup leaves invariant (one of seven lattice types), then choosing the point 
group, .nally choosing the particular semi-direct product of the two. Figure 8 shows a view inside the 
symmetry group r3. We have also hooked up Maniviewto a powerful program for computing hyperbolic structures 
on three dimensional manifolds, snappea by Jeff Weeks [Wee]. This is a popular tool used by research 
topologists to construct and examine three dimensional manifolds. Geomview, Maniview, eucsyms, and snappea 
are all available via anonymous ftp from geom.umn.edu [128.101.25.35]. Some of the computation of the 
groups and geometrical models shown in the .gures have been computed using a Mathematica 6 package developed 
at the Geometry Center, also available via anonymous ftp from the same site.  7 Example spaces 7.1 Not 
Knot The mathematical animation Not Knot [GM91] pioneered the vi­sualization of the insider s view of 
hyperbolic space. It features one Euclidean orbifold (see Figure 6) and a series of hyperbolic orb­ifolds 
converging to a hyperbolic manifold that is the complement of the three linked circles known as the Borromean 
rings. Figure 5 shows one of these orbifolds, which tessellates 3 with right-angled H dodecahedra. One 
of the six generators is a rotation of2 around  Figure 9: A view of the 120-cell. We reduce the size 
of the shaded polyhedra but draw the original edges. the large red axis. As a matrix this generator is: 
1 618033 1 618033 0 2 058171 1 618033 0 0 1 272019 0 0 10 2 058171 1 272019 0 2 618033 Note that all 
the non-zero entries are powers of the golden ratio. This is an example of an arithmetic group and is 
of particular math­ematical interest. The discrete groups underlying Not Knot have been converted into 
the DiscreteGroup format. Now, viewers interested in exploring the spaces depicted in Not Knot can do 
so.  7.2 The Poincare homology sphere Possibly the most famous three dimensional spherical manifold 
is the so-called Poincarre homology sphere. It arises abstractly by identifying the opposite faces of 
a regular dodecahedron with a twist of 5. 7 The tessellation of 3 corresponding to this manifold consists 
S of 120 regular dodecahedra, which meet 3 around each edge, and is known as the 120-cell or dodecahedral 
honeycomb [Cox73]. In contrast to the right-angled dodecahedronof hyperbolic space, these dodecahedra 
have dihedral angles of 23 . An inside view of this manifold appears in Figure 9. Note that the largest 
dodecahedron, which completely .lls the view as if it surrounds the viewer, is also the farthest away. 
This is a typical feature of life in spherical space; as objects move away they decrease in size until 
they reach a maximum distance of 2, then they begin to increase in size until they reach the antipodal 
point of the viewer at a distance of , where they expand to .ll completely the .eld of view, since every 
geodesic leaving the observer also passes through the antipodal point. Stereo viewing in spherical space 
would place great strain on Euclidean trained eyes: when an object is exactly at the equator, the lines 
of sight from an observer s eyes are parallel;  6MathematicaisatrademarkofWolframResearch,Inc 7SeeAppendixA.5. 
as an object moves beyond the equator, the observer must look "anti-crosseyed" at it.  8 Directions 
for further work Common ancestry in projective geometry means that some impor­tant procedures can be 
shared with traditional Euclidean systems. However, there remain a host of computer graphics issues related 
to modeling and animation in non-Euclidean spaces to be addressed. Many geometric constructions are very 
different. For example, consider a equidistant curve, that is, the set of points equidistant from a line. 
In the Euclidean plane an equidistant curve is a par­allel line. But equidistant curves in spherical 
and hyperbolic space are not straight lines. What, then, is the proper generalization of a cylinder in 
these spaces? Also, neither space allows similarity transformations: changing the size of an object changes 
its shape! Other questions arise. What sort of harmonic analysis is available to synthesize fractal terrains 
and textures in these spaces? If we hope to do physically-based modeling in these spaces, we need to 
expand our understanding of the laws of physics beyond the behavior of light described above in relation 
to shading. Finally, the theory of splines in non-Euclidean spaces was explored in [GK85]. In the area 
of topological content, one obvious goal is to im­plement the .ve non-isotropic three dimensional model 
geometries. Also, there are many sorts of discrete groups, particularly those that create fractal patterns, 
which do not .t neatly into the current framework. In the direction of mathematical research and user 
interface, the efforts described here suggest various techniques for exploring 3­manifolds. Connecting 
this software with virtual reality technology would allow the researcher to perform a variety of explorations 
of the space. The use of sound also promises to yield useful evidence. Looking at the wider world of 
Riemannian geometry, this work is one step in the direction of visualizing arbitrary curved spaces, the 
Riemannian manifolds that .gure centrally in relativity and cosmology. For related work see [HD89]. Finally, 
this work opens a new domain for artistic creativity, three dimensional analogues of M. C. Escher s dramatic 
interlocking planar tessellations. 9 Conclusions Approaching metric geometries via their common ancestry 
in pro­jective geometry yields simple models which can be directly imple­mented in existing rendering 
systems. The resulting systems allow interactive navigation of curved spaces for the .rst time. Custom 
shaders provide realistic rendering of the insider s view. Methods for manipulating and displaying discrete 
groups allow interactive exploration of a wide class of topological manifolds modeled on these spaces, 
that have never been visualized before. The resulting system provides a unique tool for mathematicians, 
educators, and scientists and artists whose work is related to spatial symmetry. Acknowledgements I would 
like to acknowledgevaluable ideas, comments, suggestions, and assistance from Bill Thurston, Jeff Weeks, 
Silvio Levy, Stuart Levy, Mark Phillips, John Sullivan, David Banks, and the reviewers. A Appendices 
for CD-ROM The following sections were deleted from the printed version of the paper due to space limitations 
that are not present in this CD-ROM version. A.1 More on projective geometry The main contributors to 
projective geometry include Poncelet, von Staudt, Cayley, and Klein [Boy68]. Projective geometry is distinguished 
by a perfect duality be­tween point and line; every two points determine a line; but equally surely, 
every two lines determine a point, the point of intersection. "Parallel" lines also intersect in a point, 
a point at in.nity. Conse­quently, every theorem has a dual version in which point and line are reversed. 
Projective space can be constructed in every dimen­sion by adjoining an 1 dimensional hyperplane at in.nity. 
After we adjoin these points, they have no special status. A full account can be found in [Cox65]. Coordinates 
in the projective plane arise from the choice of a triangle of reference. Then every point can be written 
as a weighted sum where X,Y,W are the vertices of the tri­angle of reference, and its coordinates are 
the vector . By   x ywnxXnx yYnywnwWnP n x ywx ynwn de.nition, for any non-zero . If the triangle 
of reference is chosen to include the Cartesian origin as one vertex, the line at in.nity as the opposite 
side, and the other two sides are perpendicular to each other, we arrive at homogeneous coordinates for 
the Euclidean plane, in which we can choose so that 1 except for points on the line at in.nity. The resulting 
coordinates are the familiar Cartesian coordinates [Woo22]. A.2 Constructing Isometries Note that it 
is possible to express the Absolute as a symmetric matrix. For example, for the hyperbolic plane 10 0 
01 0 00 1  LH n NQP . P P . PQPQ t P t Then the Minkowski inner product 01 01 , where 1 is the column 
vector form of 1. All isometries in these geometries may be constructed as prod­ucts of re.ections [Cox65]. 
We consider a re.ection in a hyperplane in . This hyperplane determines a 1-dimensional orthogonal subspace 
(orthogonal with respect to the Minkowski inner prod­uct!). After normalizing , the re.ection in acting 
on a point can be expressed as 2 . We can write this as a matrix, x 0 xAA ij xI ij Nx NN i QN ii N j 
LQR n x with 2where is the matrix form of the Absolute. Similar expressions apply for re.ections in 
andS n . A.3 Spherical space caveats There are some details to the implementation of visualization in 
3 which deserve mention. They re.ect the fact that each point of corresponds to a pair of antipodal points 
on . The behavior of the rendering system in this situation depends on how the conversion from homogeneous 
coordinates into 3D is done. In some systems, wwSSw n . PPS n points with 0 are negated to force 0, then 
clipping is performed. This essentially collapses 3 onto 3. The other alternative is that the region 
0 is treated separately from  w . wS 0; typically, clipping is done so that line segments are clipped 
to lie within the latter region. This is preferable for visualizing 3. The result is that half the sphere, 
where 0, is invisible at any moment. This can be arranged to be the hemisphere that is behind the camera. 
See [FvDFH90] for a discussion of homogeneous clipping. A.4 Sample data .le The following is a sample 
DiscreteGroup data .le borrom2.dgp, which describes the Euclidean orbifold shown in Figure 6. # Comments 
are delimited by # s. DISCGRP # Class Identifier (group borrom2 ) # Group name (comment " Order 2 Borromean 
orbifold. " ) # Arbitrary comment (attribute Euclidean ) # { Euclidean | hyperbolic | spherical } (enumdepth 
4 ) # Length of words in generators to compute (enumdist 10.0 ) # No group element that moves origin 
> 10 (dimn 3 ) # Dimension of the underlying space (ngens 6 ) # Number of generators (gens # List of 
generators with symbolic names # The generators are 6 180 degree rotations around axes lying on centers 
# of the sides of a cube of side-length 1.0. # a is rotation around line parallel to z-axis, passing 
through (.5, 0, 0). a -1 0 0 0 0 -1 0 0 0010 1001 # ditto for d, through (-.5, 0, 0) d -1 0 0 0 0 -1 
0 0 0010 -1 0 0 1 # b rotates a line parallel to x-axis, through point (0, .5, 0) b 1000 0 -1 0 0 0 0 
-1 0 0101 # ditto for e, through (0, -.5, 0) e 1000 0 -1 0 0 0 0 -1 0 0 -1 0 1 # etc. c -1 0 0 0 0100 
0 0 -1 0 0011 f -1 0 0 0 0100 0 0 -1 0 0 0 -1 1 ) # distinguished point for Dirichlet domain computation 
(cpoint 0.000000 0.000000 0.000000 1.000000 ) # geometry to use to represent the camera: a paper airplane 
(camgeom { = OFF # see man 5 OOGL for this file format 525 000 -0.1 0 0.5 0.1 0 0.5 0 -0.1 0.5 0 0.1 
0.5 3 012 2002000.8 3 034 0200200.8 } ) # end of DiscreteGroup format file  A.5 More on the 120-cell 
Each of its six generators moves a face of one dodecahedra so it lines up with its opposite face; a typical 
generator looks like: 0 1 0 809017 0 5 0 0 309017 0 5 0 809017 0 309017 0 B C 9 A 0 0 309017 0 809017 
0 5 0 309017 0 0 5 0 809017 Its .rst homology group is trivial, hence the name. (The .rst ho­mology group 
is a commutative version of the fundamental group.) Originally Poincarre had stated his famous conjecture 
in terms of the .rst homology groups, and this manifold had provided the im­portant counterexample which 
led to the revised conjecture into its current form. References [Bea83] Alan F. Beardon. The Geometry 
of Discrete Groups . Springer-Verlag, 1983. [Boy68] Carl B. Boyer. A History of Mathematics. Princeton 
University Press, 1968. [Car76] Manfredo P. Do Carmo. Differential Geometry of Curves and Surfaces . 
Prentice-Hall, 1976. [Cay59] A. Cayley. A sixth memoir upon quantics. Philo­sophical Transactions of 
the Royal Society of London , 149:61 90, 1859. [Cox65] H.M.S. Coxeter. Non-Euclidean Geometry. University 
of Toronto Press, 1965. [Cox73] H.M.S. Coxeter. Regular Polytopes. Dover, 1973. [Cox87] H.M.S. Coxeter. 
Projective Geometry. Springer Ver­lag, 1987. . [ECH91] D. B. A. Epstein, Jim Cannon, Derek Holt, Silvio 
Levy, Mike Patterson, and William Thurston. Word Processing in Groups . Jones and Bartlett, 1991. [FRC92] 
Helaman Ferguson, Alyn Rockwood, and Jordan Cox. Topological design of sculptural surfaces. Computer 
Graphics, 26:149 156, July, 1992. Proceedings of SIGGRAPH 1992. [FvDFH90] James Foley, Andries van Dam, 
Steven Feiner, and John Hughes. Computer Graphics: Principles and Practice. Addison-Wesley, 1990. [GK85] 
S. Gabriel and J. Kajiya. Spline interpolation in curved space. In State of the Art Image Synthesis, 
1985. Course notes for SIGGRAPH 1985. [GM91] Charlie Gunn and Delle Maxwell. Not Knot. Jones and Bartlett, 
1991. [Gun83] Charlie Gunn. A computer implementation of the two­dimensional euclidean crystallographic 
groups. Mas­ter s thesis, UNC, Chapel Hill, 1983. [Gun92] Charlie Gunn. Visualizing hyperbolic geometry. 
In Computer Graphics and Mathematics ,pages 299 313. Eurographics, Springer Verlag, 1992. [HD89] Ping-Kang 
Hsiung and Robert H.P. Dunn. Visualizing relativistic effects in spacetime. In Supercomputing 89. IEEE/ACM, 
Nov, 1989. [Lev92] Silvio Levy. Automatic generation of hyperbolic tilings. Leonardo, 35:349 354, 1992. 
[LM78] E. H. Lockwood and R. H. Macmillan. Geometric symmetry. Cambridge University Press, 1978. . [MLP] 
Tamara Munzner, Stuart Levy, Mark Phillips, Nathaniel Thurston, and Celeste Fowler. Geomview an interactive 
viewing program for sgi worksta­tions. ftp@geom.umn.edu. [Mun75] James Munkres. Topology: A First Course, 
chapter 8. Prentice-Hall, 1975. [PG92] Mark Phillips and Charlie Gunn. Visualizing hyper­ bolic space: 
Unusual uses of 4x4 matrices. In 1992 Symposium on Interactive 3D Graphics , pages 209 214. ACM SIGGRAPH, 
ACM, 1992. [Sch80] R.L.E. Schwarzenberger. N-Dimensional Crystallog­ raphy. Pitman Publishing, 1980. 
chapters 13-16. [Thu82] William Thurston. Three dimensional manifolds, kleinian groups and hyperbolic 
geometry. BAMS, 19:417 431, 1982. [Ups89] Steve Upstill. The Renderman Companion . Addison- Wesley, 1989. 
chapters 13-16. [Wee] Jeff Weeks. snappea a macintosh application for computing 3-manifolds. ftp@geom.umn.edu. 
[Wee85] Jeff Weeks. The Shape of Space . Marcel Dekker, 1985. [Woo22] Frederick Woods. Higher Geometry. 
Dover, 1961 (1922).  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166151</article_id>
		<sort_key>263</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>34</seq_no>
		<title><![CDATA[Imaging vector fields using line integral convolution]]></title>
		<page_from>263</page_from>
		<page_to>270</page_to>
		<doi_number>10.1145/166117.166151</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166151</url>
		<keywords>
			<kw><![CDATA[convolution]]></kw>
			<kw><![CDATA[filtering]]></kw>
			<kw><![CDATA[flow fields]]></kw>
			<kw><![CDATA[periodic motion filtering]]></kw>
			<kw><![CDATA[rendering]]></kw>
			<kw><![CDATA[special effects]]></kw>
			<kw><![CDATA[texture synthesis]]></kw>
			<kw><![CDATA[visualization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Filtering</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P32489</person_id>
				<author_profile_id><![CDATA[81100575702]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cabral]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P170336</person_id>
				<author_profile_id><![CDATA[81100344095]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Leith]]></first_name>
				<middle_name><![CDATA[Casey]]></middle_name>
				<last_name><![CDATA[Leedom]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bresenham, J. Algorithm for Computer Control of a Digital Plotter. In IBM Systems Journal 4, 1 (1965), 25-30.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>262376</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bronstein, I. and Semendyayev, K. Handbook of Mathematics. Van Norstrand Reinholt (1985), 291-293.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Chang, S. Fundamentals Handbook of Electrical Engineering and Computer Engineering. John Wiley &amp; Sons, Inc. (1982), 264-266.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>163196</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Chui, K. An Intlvduction to Wavelets. Academic Press, Inc. (1992), 49-60.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147150</ref_obj_id>
				<ref_obj_pid>147130</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Crawfis, R. and Max, M. Direct Volume Visualization of Three-Dimensional Vector Fields. Proceedings of the Workshop on Volume Visualization, Kaufman and Lorensen Eds (1992).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378484</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Drebin, R., Carpenter, L. and Hanaran, E Volume Rendering. Computer Graphics 22, 4 (August 1988), 65-74.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Dumortier, F., Roussarie, R., Sotomayor, J. and Zoladek, H., Study of Field Bifurcations. Lecture Notes in Mathematics, Springer-Verlag (1991).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122721</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Freeman, W., Adelson, E. and Heeger, D. Motion without Movement. Computer Graphics 25, 4 (July 1991), 27-30.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97902</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Haeberli, E Paint By Numbers: Abstract Image Representation. Computer Graphics 24, 4 (August 1990), 207-214.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15921</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Heckbert, E Filtering by Repeated Integration. Computer Graphics 20, 4 (August 1986), 315-321.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74361</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Kajiya, J. and Kay, T. Rendering Fur with Three Dimensional Textures. Computer Graphics 23, 3 (July 1989), 271-280.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949700</ref_obj_id>
				<ref_obj_pid>949685</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Kenwright, D. and Mallinson, G. A 3-D Streamline Tracking Algorithm Using Dual Stream Functions. IEEE Visualization '92 Conference Proceedings (October 1992), 62-68.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Max, Nelson. Personal Communication (1992).]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Perlin, K. An Image Synthesizer. Computer Graphics 19, 3 (August 1985), 287-296.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74359</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Perlin, K. Hypertexture. Computer Graphics 23, 3 (July 1989), 253-262.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>130597</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Pratt, W. Digital Image Processing. 2nd ed. John Wiley &amp; Sons, Inc. (1991), 243-245.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122752</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Sims, K. Artificial Evolution for Computer Graphics. Computer Graphics 25, 4 (August 1991), 319-328.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Sims, K. Choreographed Image Flow. The Journal of Visualization and Computer Animation 3, 1 (January-March 1992), 31-43.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>33404</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Tufte, E. The Visual Display of Quantitative Information. Chesire, CT: Graphics Press (1983).]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122749</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Turk, G. Generating Textures on Arbitrary Surfaces Using Reaction-Diffusion Textures. Computer Graphics 25, 4 (July 1991), 289-298.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378482</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Upson, C. and Keeler, M. V-Buffer: Visible Volume Rendering. Computer Graphics 22, 4 (August 1988), 59-64.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147149</ref_obj_id>
				<ref_obj_pid>147130</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Van Gelder, A. and Wilhelms, J. Interactive Animated Visualization of Flow Fields. Proceedings of the Workshop on Volume Visualization, Kaufman and Lorensen Eds. (1992).]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122751</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Van Wijk, J. Spot Noise Texture Synthesis for Data Visualization. Computer Graphics 25, 4 (July 1991), 309-318.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122750</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Witkin, A. and Kass, M. Reaction-Diffusion Textures. Computer Graphics 25, 4 (July 1991), 299-308.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Imaging Vector Fields Using Line Integral Convolution Brian Cabral Leith (Casey) Leedom* Lawrence 
Livermore National Laboratory ABSTRACT Imaging vector .elds has applications in science, art, image pro­cessing 
and special effects. An effective new approach is to use linear and curvilinear .ltering techniques to 
locally blur textures along a vector .eld. This approach builds on several previous tex­ture generation 
and .ltering techniques[8, 9, 11, 14, 15, 17, 23]. It is, however, unique because it is local, one-dimensional 
and inde­pendent of any prede.ned geometry or texture. The technique is general and capable of imaging 
arbitrary two- and three-dimen­sional vector .elds. The local one-dimensional nature of the algo­rithm 
lends itself to highly parallel and ef.cient implementations. Furthermore, the curvilinear .lter is capable 
of rendering detail on very intricate vector .elds. Combining this technique with other rendering and 
image processing techniques like periodic motion .ltering results in richly informative and striking 
images. The technique can also produce novel special effects. CR categories and subject descriptors: 
I.3.3 [Computer Graphics]: Picture/Image generation; I.3.7 [Computer Graphics]: Three-Dimensional Graphics 
and Realism; I.4.3 [Image Process­ing]: Enhancement. Keywords: convolution, .ltering, rendering, visualization, 
tex­ture synthesis, .ow .elds, special effects, periodic motion .ltering. 1. INTRODUCTION Upon .rst inspection, 
imaging vector .elds appears to have lim­ited application con.ned primarily to scienti.c visualization. 
However, much of the form and shape in our environment is a function of not only image intensity and 
color, but also of direc­tional information such as edges. Painters, sculptors, photogra­phers, image 
processors[16] and computer graphics researchers[9] have recognized the importance of direction in the 
process of image creation and form. Hence, algorithms that can image such directional information have 
wide application across both scien­ti.c and artistic domains. Such algorithms should possess a number 
of desirable and sometimes con.icting properties including: accuracy, locality of calculation, simplicity, 
controllability and generality. Line Integral Convolution (LIC) is a new technique that possesses many 
of these properties. Its generality allows for the introduction of a com­ * Authors current e-mail addresses 
are: cabral@llnl.gov and casey@gauss.llnl.gov. Permission to copy without fee all or part of this material 
is granted Permission to copy without fee all or part of this material is granted provided that the copies 
are not made or distributed for direct provided that the copies are not made or distributed for direct 
commercial advantage, the ACM copyright notice and the title of the commercial advantage, the ACM copyright 
notice and the title of the publication and its date appear, and notice is given that copying is by publication 
and its date appear, and notice is given that copying is by permission of the Association for Computing 
Machinery. To copy permission of the Association for Computing Machinery. To copy otherwise, or to republish, 
requires a fee and/or specific permission. otherwise, or to republish, requires a fee and/or specific 
permission. &#38;#169;1993 ACM-0-89791-601-8/93/008 $1.50 &#38;#169;1993 ACM-0-89791-601-8/93/008/0015 
$1.50 pletely new family of periodic motion .lters which have wide application (see section 4.1). It 
represents a con.uence of signal and image processing and a variety of previous work done in com­puter 
graphics and scienti.c visualization. 2. BACKGROUND There are currently few techniques which image vector 
.elds in a general manner. These techniques can be quite effective for visu­alizing vector data. However, 
they break down when operating on very dense .elds and do not generalize to other applications. In particular, 
large vector .elds (512x512 or greater) strain existing algorithms. Most vector visualization algorithms 
use spatial resolution to represent the vector .eld. These include sampling the .eld, such as with stream 
lines[12] or particle traces, and using icons[19] at every vector .eld coordinate. Stream lines and particle 
tracing techniques depend critically on the placement of the streamers or the particle sources. Depending 
on their placement, eddies or cur­rents in the data .eld can be missed. Icons, on the other hand, do 
not miss data, but use up a considerable amount of spatial resolu­tion limiting their usefulness to small 
vector .elds. Another general approach is to generate textures via a vector .eld. Van Wijk s spot noise 
algorithm[23] uses a vector .eld to control the generation of bandlimited noise. The time complexity 
of the two types of implementation techniques presented by Van Wijk are relatively high. Furthermore 
the technique, by de.nition, depends heavily on the form of the texture (spot noise) itself. Spe­ci.cally, 
it does not easily generalize to other forms of textures that might be better suited to a particular 
class of vector data (such as .uid .ow versus electromagnetic). Reaction diffusion techniques[20, 24] 
also provide an avenue for visualizing vector .elds since the controlling differential equa­tions are 
inherently vector in nature. It is possible to map vector data onto these differential equations to come 
up with a vector visualization technique. Here too however, the time complexity of these algorithms limit 
their general usefulness. Three-dimensional vector .elds can be visualized by three­dimensional texture 
generation techniques such as texels and hypertextures described in [11, 15]. Both techniques take a 
texture on a geometrically de.ned surface and project the texture out some distance from the surface. 
By de.nition these techniques are bound to the surface and do not compute an image for the entire .eld 
as is done by Van Wijk[23]. This is limiting in that it requires a priori knowledge to place the surface. 
Like particle streams and vector streamers these visualization techniques are critically dependent on 
the placement of the sampling surface. The technique presented by Haeberli[9] for algorithmicly gener­ating 
paintings via vector-like brush strokes can also be thought of as a vector visualization technique. Craw.s 
and Max[5] describe a three-dimensional variation on this in which blurred cyl­inders represent three-dimensional 
brush strokes whose directions and colors are controlled by a three-dimensional vector .eld. Both techniques 
represent a conceptual extension of traditional icon placement, where the icons are more sophisticated 
shapes. How­ever, these techniques break down as the density of the .eld increases since they require 
spatial resolution to work. What is needed is a technique that can image dense vector .elds, is independent 
of both prede.ned sampling placement constraints and texture generation techniques and can work in two 
and three dimensions. Such a technique would be very general and have wide application. 3. DDA CONVOLUTION 
One approach is a generalization of traditional DDA line draw­ing techniques[1] and the spatial convolution 
algorithms described by Van Wijk[23] and Perlin[14]. Each vector in a .eld is used to de.ne a long, narrow, 
DDA generated .lter kernel tangential to the vector and going in the positive and negative vector direction 
some .xed distance, L. A texture is then mapped one-to-one onto the vector .eld. The input texture pixels 
under the .lter kernel are summed, normalized by the length of the .lter kernel, 2L, and placed in an 
output pixel image for the vector position. Figure 1, illustrates this operation for a single vector 
in a .eld. This effectively .lters the underlying texture as a function of the vector .eld. The images 
in .gure 2 are rendered using the DDA convolution algorithm. On the left is a simple circular vector 
.eld; to its right is the result of a computational .uid dynamics code. The input texture image in these 
examples is white noise. Although the description above implies a box .lter, any arbitrary .lter shape 
can be used for the .lter convolution kernel. It is important to note that this algorithm is very sensitive 
to symmetry of the DDA algorithm and .lter. If the algorithm weights the forward direction more than 
the backward direction, the circular .eld in .gure 2 appears to spi­ral inward implying a vortical behavior 
that is not present in the vector .eld. 3.1 LOCAL FIELD BEHAVIOR The DDA approach, while ef.cient, is 
inherently inaccurate. It assumes that the local vector .eld can be approximated by a Vector field DDA 
line Input texture Output image Figure 1: The mapping of a vector onto a DDA line and input pixel .eld 
generating a single output pixel. Figure 2: Circular and turbulent .uid dynamics vector .elds imaged 
using DDA convolution over white noise. straight line. For points in vector .elds where the local radius 
of curvature is large, this assumption is valid. However, where there are complex structures smaller 
than the length of the DDA line, the local radius of curvature is small and is not well approximated 
by a straight line. In a sense, DDA convolution renders the vector .eld unevenly, treating linear portions 
of the vector .eld more accu­rately than small scale vortices. While this graceful degradation may be 
.ne or even desirable for special effects applications, it is problematic for visualizing vector .elds 
such as the ones in .gure 2, since detail in the small scale structures is lost. Van Wijk s spot noise 
algorithm[23] also suffers from this prob­lem since the spots are elliptically stretched along a line 
in the direction of the local .eld. If the ellipse major axis exceeds the local length scale of the vector 
.eld, the spot noise will inaccu­rately represent the vector .eld. An accurate measure of local .eld 
behavior would require a global analysis of the .eld. Such tech­niques currently do not exist for arbitrary 
vector .elds, would most likely be expensive to calculate[13] and are an area of active research[7]. 
 4. LINE INTEGRAL CONVOLUTION The local behavior of the vector .eld can be approximated by computing 
a local stream line that starts at the center of pixel (x, y) and moves out in the positive and negative 
directions.1 The for­ward coordinate advection is given by equation (1). P0 = x 0.5 y + 0.5 ,+( ) Pi 
= Pi - 1 + V ( V ( Pi - 1 Pi - 1 ) ) .si - 1 (1) V ( P ) = the vector from the input vector field at 
lattice point ( Px , Py ) se = 8.if. V.||. e................. 0.if. Pc Pc- Vc < 0... Pc Pc- Vc .otherwise 
. . . . . . . for e, c( ) . top y,( ) bottom, y( ) left, x( ) right, x( ). . . . . . . . . . (2) . si 
= min stop,sbottom,sleft,sright ( ) 1. Vector .eld lattice and image coordinates are usually speci.ed 
in a left­handed coordinate system while vector components are usually speci.ed in a right-handed coordinate 
system. In this case, the y-component of the lat­tice coordinate in equation (1) must be re.ected about 
the vertical center of the lattice to operate in a consistent coordinate system. This re.ection has been 
omitted to preserve simplicity of presentation. Figure 3: A two-dimensional vector .eld showing the 
local stream line starting in cell (x, y). The vector .eld is the upper left corner of the .uid dynamics 
.eld in .gures 2 and 4. Only the directional component of the vector .eld is used in this advection. 
The magnitude of the vector .eld can be used later in post processing steps as explained in section 4.3.1. 
.si is the posi­tive parametric distance along a line parallel to the vector .eld from Pi to the nearest 
cell edge. As with the DDA algorithm, it is important to maintain symme­try about a cell. Hence, the 
local stream line is also advected back­wards by the negative of the vector .eld as shown in equation 
(3). P' = P0 0 ) (3) V ( P' i - 1 P' = P' - .s' ii - 1 i - 1 V ( ) P' i - 1 Primed variables represent 
the negative direction counterparts to the positive direction variables and are not repeated in subsequent 
de.nitions. As above .s i, is always positive. The calculation of .si in the stream line advection is 
sensitive to round off errors. .si must produce advected coordinates that lie within the i+1th cell, 
taking the stream line segment out of the cur­rent cell. In the implementation of the algorithm a small 
round off term is added to each .si to insure that entry into the adjacent cell occurs. This local stream 
line calculation is illustrated in .gure 3. Each cell is assumed to be a unit square. All spatial quantities 
(e.g., .si) are relative to this measurement. However, the cells need not be square or even rectangular 
(see section 6) for this approxima­tion to work. So, without loss of generality, descriptions are given 
relative to a cubic lattice with unit spacing. Continuous sections of the local stream line i.e. the 
straight line segments in .gure 3 can be thought of as parameterized space curves in s and the input 
texture pixel mapped to a cell can be treated as a continuous scalar function of x and y.2 It is then 
possible to integrate over this scalar .eld along each parameterized space curve. Such integrals can 
be summed in a piecewise C1 fash­ion and are known as line integrals of the .rst kind (LIFK)[2]. The 
convolution concept used in the DDA algorithm can now be com­ 2. Bilinear, cubic or Bezier splines are 
viable alternatives to straight line segments. However, these higher order curves are more expensive 
to com­pute. bined with LIFK to form a Line Integral Convolution (LIC). This results in a variation of 
the DDA approach that locally follows the vector .eld and captures small radius of curvature features. 
For each continuous segment, i, an exact integral of a convolution ker­nel k(w) is computed and used 
as a weight in the LIC as shown in equation (4). si +.si hi = k (w) dw (4) . si where s0 = 0 si = si 
- 1 +.si - 1 The entire LIC for output pixel F (x, y) is given by equation (5). ll' . F ( ) hi +. F ( 
) h' P' i Pi i i = 0 i = 0 F' (x, y)= (5) ll' hi +. h' . i i = 0 i = 0 where F ( P ) is.the.input.pixel.corresponding.to 
the.vector.at.position ( P , P ) x y l = i.such.that. si = L < si + 1 (6) The numerator of equation (5) 
represents the line integral of the .l­ter kernel times the input pixel .eld, F. The denominator is the 
line integral of the convolution kernel and is used to normalize the out­put pixel weight (see section 
4.2). The length of the local stream line, 2L, is given in unit pixels. Depending on the input pixel 
.eld, F, if L is too large, all the resulting LICs will return values very close together for all coordi­nates 
(x, y). On the other hand, if L is too small then an insuf.cient amount of .ltering occurs. Since the 
value of L dramatically affects the performance of the algorithm, the smallest effective value is desired. 
For most of the .gures, a value of 10 was used. Singularities in the vector .eld occur when vectors in 
two adja­cent local stream line cells geometrically point at a shared cell edge. This results in .si 
values equal to zero leaving l in equation (6) unde.ned. This situation can easily be detected and the 
advec­tion algorithm terminated. If the vector .eld goes to zero at any point, the LIC algorithm is terminated 
as in the case of a .eld sin­gularity. Both of these cases generate truncated stream lines. If a zero 
.eld vector lies in the starting cell of the LIC, the input pixel value for that cell, a constant or 
any other arbitrary value can be returned as the value of the LIC depending on the visual effect desired 
for null vectors. Using adjacent stream line vectors to detect singularities can however result in false 
singularities. False singularities occur when the vector .eld is nearly parallel to an edge, but causes 
the LIC to cross over that edge. Similarly, the cell just entered also has a near parallel vector which 
points to this same shared edge. This artifact can be remedied by adjusting the parallel vector/edge 
test found in equation (2), to test the angle formed between the vector and the edge against some small 
angle theta, instead of zero. Any vector which forms an angle less than theta with some edge is deemed 
to be parallel to that edge. Using a value of 3° for theta removes these artifacts. The images in .gure 
4 were rendered using LIC and correspond to the same two vector .elds rendered in .gure 2. Note the increased 
amount of detail present in these images versus their DDA counterparts. In particular the image of the 
.uid dynamics vector .eld in .gure 4 shows detail incorrectly rendered or absent in .gure 2. Figure 
4: Circular and turbulent .uid dynamics vector .elds imaged using LIC over white noise. The images in 
.gure 5 show the effect of varying L. The input texture is a photograph of .owers. The input vector .eld 
was cre­ated by taking the gradient of a bandlimited noise image and rotat­ing each of the gradient vectors 
by 90°, producing vectors which follow the contours of the soft hills and valleys of the bandlimited 
noise. With L equal to 0, the input image is passed through unchanged. As the value of L increases, the 
input image is blurred to a greater extent, giving an impressionistic result. Here, a biased ramp .lter[10] 
is used to roughly simulate a brush stroke. Figures 2, 4, 8, 9 and 11 were generated using white noise 
input images. Aliasing can be a serious problem when using LIC with a high frequency source image such 
as white noise. The aliasing is caused by the one-dimensional point sampling of the in.nitely thin LIC 
.lter. This aliasing can be removed by either creating a thick LIC .lter with a low-pass .lter cross 
section or by low-pass .lter­ing the input image. This second alternative is preferable since it comes 
at no additional cost to the LIC algorithm. The images in .gure 6 show the effect of running LIC over 
256x256 white noise which has been low-pass .ltered using a fourth order Butterworth .lter with cutoff 
frequencies of 128, 84, 64, and 32. It is worth noting that Van Wijk s spot noise algorithm[23] can be 
adapted to use the local stream line approximation to more accurately represent the behavior of a vector 
.eld. Instead of straight line elliptical stretching, each spot could be warped so that the major axis 
follows the local stream line. Furthermore, the minor axis could either be perpendicular to the warped 
major axis or itself could be warped along transverse .eld lines. However, an algorithm to perform this 
task for an arbitrary local stream line would be inherently more expensive and complex than the LIC algorithm. 
Sims[18] describes an alternative technique which produces results similar to LIC. This alternative approach 
warps or advects texture coordinates as a function of a vector .eld. The similarity between the two techniques 
is predictable even though the tech­niques are quite different. The dilation and contraction of the tex­ture 
coordinate system warping has the visual effect of blurring and sharpening the warped image. This is 
due to the resampling and reconstruction process necessary when warping from one coordinate system to 
another. Thus, for regions where the source image is stretched along the vector .eld an apparent blurring 
will occur similar to those seen with LIC. However, the techniques are completely different in two fundamental 
ways. First, LIC is a local operator, meaning no information outside of a .xed area of interest is needed. 
Warping even when done locally requires maintaining global consistency to avoid tearing holes in the 
warped image. This increases the complexity of the warping operation when com­pared to LIC. Second, LIC 
is a spatially varying .ltering operation and does not warp or transform any texture coordinates. 4.1 
PERIODIC MOTION FILTERS The LIC algorithm visualizes local vector .eld tangents, but not their direction. 
Freeman, et al[8] describe a technique which simu­lates motion by use of special convolutions. A similar 
technique is used by Van Gelder and Wilhelms[22] to show vector .eld .ow. This technique can be extended 
and used to represent the local vec­tor .eld direction via animation of successive LIC imaged vector 
.elds using varying phase shifted periodic .lter kernels. The success of this technique depends on the 
shape of the .lter. In the previous examples (.gures 2 and 4), a constant or box .lter is used. If the 
.lter is periodic like the .lters used in [8], by chang­ing the phase of such .lters as a function of 
time, apparent motion  Figure 5: Photograph of .owers processed using LIC with L equal to 0, 5, 10 and 
20 (left to right, top to bottom). Figure 6: The upper left hand quarter of the circular vector .eld 
is convolved using LIC over Butterworth low-pass .ltered white noise with cutoff frequencies of 128, 
86, 64, and 32 (left to right, top to bottom). in the direction of the vector .eld is created. However, 
the .lters used in [8] were, by design, high-pass Laplacian edge enhancing .lters. Using this .lter over 
a bandlimited noise texture produces very incoherent images since the high frequency components of the 
noise are accentuated. Instead, it is possible, and desirable, to create periodic low-pass .lters to 
blur the underlying texture in the direction of the vector .eld. A Hanning .lter, 1/2(1 + cos(w+ß)), 
has this property. It has low band-pass .lter characteristics, it is periodic by de.nition and has a 
simple analytic form. This func­tion will be referred to as the ripple .lter function. Since the LIC 
algorithm is by de.nition a local operation, any .lter used must be windowed. That is, it must be made 
local even if it has in.nite extent. In the previous section we used a constant .lter implicitly windowed 
by a box of height one. Using this same box window on a phase shifted Hanning .lter we get a .lter with 
abrupt cutoffs, as illustrated in the top row of .gure 7. This abrupt cutoff is noticeable as spatio-temporal 
artifacts in animations that vary the phase as a function of time. One solution to this problem is to 
use a Gaussian window as suggested by Gabor[4].3 By multiplying, or windowing, the Hanning function by 
a Gaussian, these cutoffs are smoothly attenuated to zero. How­ever, a Gaussian windowed Hanning function 
does not have a sim­ple closed form integral. An alternative is to .nd a windowing function with windowing 
properties similar to a Gaussian and which has a simple closed form integral. Interestingly, the Hanning 
function itself meets these two criteria. In the bottom row of .gure 7, the .ve phase shifted Hanning 
.lter functions in the top row are multiplied by the Hanning window function in the middle row. The general 
form of this function is shown in equation (7). In this equa­ 1 + cos (cw) 1 + cos (dw + ß) k (w)= × 
(7) 22 .= 1 (1 + cos (cw)+ cos (dw + ß) .+ cos (cw) cos (dw + ß)) 4 tion c and d represent the dilation 
constants of the Hanning win­dow and ripple functions respectively. ß is the ripple function phase shift 
given in radians. The integral of k(w) from a to b used in equation (4) is shown in equation (8). b k 
(w) dw (8) . a sin (bc)- sin (ac) . b - a +. . c . . sin (bd +ß) - sin (ad +ß) . . . +. 1 d ..= 4 . sin 
(b (c - d) -ß) - sin (a (c - d) -ß). . + .. 2 (c - d) .. . sin (b (c + d) +ß) - sin (a (c + d) +ß). . 
+ .. 2 (c + d) As mentioned above, both the Hanning window and the Han­ning ripple .lter function can 
be independently dilated by adjust­ing c and d to have speci.c local support and periodicity. The window 
function has a .xed period of 2p. Choosing the periodicity of the ripple function represents mak­ing 
a design trade-off between maintaining a nearly constant fre­quency response as a function of phase shift 
and the quality of the 3. D. Gabor in 1946 created a localized form of the Fourier transform known as 
the Gabor transform. This transform is the Fourier transform of an input signal multiplied by a Gaussian 
window translated along the sig­nal as a function of time. The net result is a signal which is spatially 
and frequency localized. Wavelet theory is based on a generalization of this type of spatial and frequency 
localization. Figure 7: Phase shifted Hanning ripple functions(top), a Han­ning windowing function(middle), 
and Hanning ripple func­tions multiplied by the Hanning window function(bottom). apparent motion[3]. 
A low frequency ripple function results in a windowed .lter whose frequency response noticeably changes 
as a function of phase. This appears as a periodic blurring and sharpen­ing of the image as the phase 
changes. Higher frequency ripple functions produce windowed .lters with a nearly constant fre­quency 
response since the general shape of the .lter doesn t radi­cally change. However, the feature size picked 
up by the ripple .lter is smaller and the result is less apparent motion. If the ripple frequency exceeds 
the Nyquist limit of the pixel spacing the appar­ent motion disappears. Experimentation shows that a 
ripple func­tion frequency between 2 and 4 cycles per window period is reasonable. One can always achieve 
both good frequency response and good feature motion by increasing the spatial resolution. This comes, 
of course, at a cost of increased computation[16]. 4.2 NORMALIZATION A normalization to the convolution 
integral is performed in equation (5) to insure that the apparent brightness and contrast of the resultant 
image is well behaved as a function of kernel shape, phase and length. The numerator in equation (5) 
is divided by the integral of the convolution kernel. This insures that the normalized area under the 
convolution kernel is always unity resulting in a constant overall brightness for the image independent 
of the .lter shape and LIC length. Because the actual length of the LIC may vary from pixel to pixel, 
the denominator can not be precomputed. However, an inter­esting effect is observed if a .xed normalization 
is used. Truncated stream lines are attenuated which highlights singularities. The images in .gure 8 
a show another section of the .uid dynamics vector .eld imaged with variable and constant kernel normaliza­tion. 
The implementation of the LIC algorithm uses precomputed sum tables for the integral to avoid costly 
arithmetic in the inner­most loop. A second normalization may be done to insure the output image retains 
the input image s contrast properties. The LIC algorithm reduces the overall image contrast as a function 
of L. In fact, in the case of the box .lter, as L goes to in.nity the entire output image goes to the 
average of the input image. This can be ameliorated by amplifying the input or contrast stretching the 
output image as a function of L. Clearly as L goes to in.nity the ampli.cation or con­trast stretching 
must go to in.nity as well. The images in all the .gures are contrast stretched.  4.3 IMPLEMENTATION 
AND APPLICATION The LIC algorithm is designed as a function which maps an input vector .eld and texture 
to a .ltered version of the input tex­ture. The dimension of the output texture is that of the vector 
.eld. If the input texture is smaller than the vector .eld the implementa­tion of the algorithm wraps 
the texture using a toroidal topology. That is, the right and left edges wrap as do the top and bottom 
edges. If the texture is too large it is cropped to the vector .eld dimensions. Careful attention must 
be paid to the size of the input texture relative to that of the vector .eld. If too small a texture 
is used, the periodicity induced by the texture tiling will be visible. For scienti.c applications this 
is unacceptable. One must insure Figure 9: White noise convolved with checkerboard vector .eld using 
.xed normalization (left), and then gradient shaded (right) to give the appearance of a rough woven surface 
tex­ture. that the input texture is large enough so that the periodicity induced by the coordinate wrapping 
is not apparent. The algorithm can be used as a data operator in conjunction with other operators much 
like those of Sims[17] and Perlin[14]. Spe­ci.cally, both the texture and the vector .eld can be preprocessed 
and combined with post processing on the output image. The LIC implementation is a module in a data .ow 
system like that found in a number of public domain and commercial products. This imple­mentation allows 
for rapid exploration of various combinations of operators. 4.3.1 POST PROCESSING The output of the LIC 
algorithm can be operated on in a variety of ways. In this section several standard techniques are used 
in combination with LIC to produce novel results. An interesting example of constant kernel normalization 
is shown in .gure 9. A simple basket weave pattern is generated by alternating vector directions in a 
checkerboard fashion. Each checker is surrounded by null vectors. This vector .eld is then used to convolve 
white noise. The LIC is truncated as it nears the edges of the checkers which results in a gradual attenuation. 
When that output is gradient shaded, the basket weave becomes very realistic. While other techniques 
could be used to generate such a texture, the simplicity of the source data illustrates the versatility 
of LIC. A surface wind velocity .eld is imaged in .gure 10 using LIC to blur 1/f noise. The resulting 
image is composed over an image of North America to present scale and location. The LIC algorithm is 
slightly modi.ed to image vector magnitude by varying the length of the line integral, 2L, as a function 
of the vector .eld magnitude. In .gure 10 this effect is seen as clumpiness in 1/f cloud-like struc­tures 
where the wind velocity .eld is small.  Another method to add vector magnitude information is seen 
in .gure 11. The [.xed normalization] .uid dynamics .eld of .gure 8 is multiplied by a color image of 
the vector magnitude. The advan­tage of this approach over variable length LIC is that the .ne grained 
detail generated by .xed length LIC is retained even in low magnitude areas. The LIC algorithm can be 
used to process an image using a vec­tor .eld generated from the image itself. In .gure 12, a vector 
.eld is generated from the input image by low-pass .ltering the image, taking the gradient of the resulting 
image and rotating the vectors by 90°. The LIC algorithm can also be used to post process images to generate 
motion blur. A rendering algorithm or paint system can easily specify a pixel by pixel velocity .eld 
for objects. By using a biased triangle .lter[10] and variable length LIC the input image can be motion 
blurred in the direction of apparent motion. This has precisely the desired results for motion blurring 
as seen in .gure 13.  4.4 THREE-DIMENSIONAL LIC The LIC algorithm easily generalizes to higher dimensions. 
Equations (1), (3) and (5) trivially extend to three dimensions. In the three-dimensional case, cell 
edges are replaced with cell faces. Both the input vector .eld and input texture must be three-dimen­sional. 
The output of the three-dimensional LIC algorithm is a three-dimensional image or scalar .eld. This .eld 
is rendered using volume rendering techniques such as those found in [21] and [6]. Figure 14 is a three-dimensional 
rendering of an electrostatic .eld with two point charges placed a .xed distance apart from one another. 
In this volumetric rendering, the magnitude of the vector .eld is used to control the opacity transfer 
functions. Great ef.­ciency gains can be achieved if the LIC algorithm exploits this by avoiding rendering 
for vector .eld cells whose magnitude is out­side of the volume renderer s min/max threshold window. 
 5. PERFORMANCE There is a distinct performance and quality trade-off between the DDA convolution algorithm 
and LIC. LIC is roughly an order of magnitude slower than the DDA method. Both algorithms were timed 
using cells processed per second (CPS) as the .gure of merit. The tests were run on an unloaded IBM 550 
RISC 6000. The DDA algorithm averages about 30,000 CPS while LIC averages about 3,000 CPS. The three-dimensional 
algorithm only marginally degrades in performance with the increase in dimensionality, processing some 
1,200 CPS. Since the algorithm remains one-dimensional in nature, the cost per cell only increases by 
a factor of three as a function of dimension. Using the thresholding described above, the performance 
of the three-dimensional LIC algorithm has exceeded 30,000 CPS.  6. FUTURE WORK A number of research 
directions relating to LIC remain out­standing. Currently no methods exist for determining the accuracy 
of a vector .eld representation, such as those created by LIC or any other method. These accuracy metrics 
would necessarily be related to the differential topology of the entire vector .eld. As mentioned ures 
2, 4, 8 and 11 and for using the algorithm in their work. Dean above, much work in theoretical and applied 
mathematics has been Williams and Jerry Potter provided the North America wind veloc­done in this area. 
This work needs to be studied and applied to ef.-ity data. Lastly, thanks to John Zych who helped with 
the rendering cient vector .eld imaging algorithms. of the North America image.  LIC is conceptually 
independent of the advection algorithm used to de.ne the parametric support used by the convolution operation. 
The method described here might be best characterized as a variable step Euler s method. Other techniques 
such as a fourth order Runge-Kutta could produce differing or improved results. A thorough investigation 
into this issue is beyond the scope of this paper. It does, however, represent an area deserving special 
attention. Visualizing the orthogonal complement of a two-dimensional vector .eld is accomplished by 
rotating the individual vectors 90°. However, in three-dimensional vector .elds the orthogonal com­plement 
of a vector is a plane. This suggests that a generalization of the one-dimensional LIC .lter would be 
a two-dimensional sur­face .lter. This .lter would have as its geometric support a differ­ential surface 
whose normals would be de.ned by the vector .eld, thus creating a Surface Integral Convolution (SIC). 
As with the LIC, an arbitrary two-dimensional .lter could then be used to .lter the three-dimensional 
input image. Another direction for generalization is to develop versions of the algorithm which operate 
directly on curvilinear and arbitrarily grided vector .elds without resampling the input data. The LIC 
algorithm could easily be modi.ed to handle arbitrary line inter­sections and topologies of both type 
of grids. As with the rectilin­ear LIC, it would have an analogous three-dimensional generalization. 
Two additional problems remain however: generat­ing curvilinear and arbitrarily girded textures and output 
resam­pling. One possible image processing application of LIC is the deblur­ring of motion blurred images. 
Images acquired with a moving CCD camera often exhibit such blurring. If the CCD frequency response curves 
and the camera motion are known, one-dimen­sional deconvolution techniques could be used in conjunction 
with LIC to deblur the images. The local nature of the LIC algorithm suggests a parallel imple­mentation. 
Such an implementation could, in principle, compute all pixels simultaneously. This would allow for interactive 
genera­tion of periodic motion animations and special effects.  7. SUMMARY Line integral convolution 
represents a new and general method for imaging two- and three-dimensional vector .elds. The algo­rithm 
.lters an input image along local stream lines de.ned by an input vector .eld and generates an output 
image. The one-dimen­sional .lter shape is independent of either input and can be arbi­trary. To indicate 
directional .ow of the vector .eld, a whole family of continuous motion .lters has been introduced. These 
.l­ters give apparent motion in the direction of the vector .eld. The technique can also be used to create 
special effects. Additionally, the local nature of the algorithm lends itself to ef.cient and simple 
implementations. 8. ACKNOWLEDGMENTS This work was performed under the auspices of the U.S. Depart­ment 
of Energy by Lawrence Livermore National Laboratory under contract W-7405-ENG-48. The SIGGRAPH 93 reviewers 
provided many helpful comments and suggestions. Thanks to Nel­son Max who suggested using higher order 
functions within a cell and who provided critical assessment all along the way. Roger Craw.s deserves 
special thanks for various conversations over the past couple of years on the topic of vector visualization. 
Chuck Grant provided helpful suggestions clarifying the language used to discuss periodic motion .lters. 
John Bell and Jeff Greenough pro­vided the turbulent computational .uid dynamics data used in .g-  
REFERENCES 1. Bresenham, J. Algorithm for Computer Control of a Digital Plotter. In IBM Systems Journal 
4, 1 (1965), 25-30. 2. Bronstein, I. and Semendyayev, K. Handbook of Mathemat­ics. Van Norstrand Reinholt 
(1985), 291-293. 3. Chang, S. Fundamentals Handbook of Electrical Engineering and Computer Engineering. 
John Wiley &#38; Sons, Inc. (1982), 264-266. 4. Chui, K. An Introduction to Wavelets. Academic Press, 
Inc. (1992), 49-60. 5. Craw.s, R. and Max, M. Direct Volume Visualization of Three-Dimensional Vector 
Fields. Proceedings of the Work­shop on Volume Visualization, Kaufman and Lorensen Eds (1992). 6. Drebin, 
R., Carpenter, L. and Hanaran, P. Volume Rendering. Computer Graphics 22, 4 (August 1988), 65-74. 7. 
Dumortier, F., Roussarie, R., Sotomayor, J. and Zoladek, H., Study of Field Bifurcations. Lecture Notes 
in Mathematics, Springer-Verlag (1991). 8. Freeman, W., Adelson, E. and Heeger, D. Motion without Movement. 
Computer Graphics 25, 4 (July 1991), 27-30. 9. Haeberli, P. Paint By Numbers: Abstract Image Representa­tion. 
Computer Graphics 24, 4 (August 1990), 207-214. 10. Heckbert, P. Filtering by Repeated Integration. 
Computer Graphics 20, 4 (August 1986), 315-321. 11. Kajiya, J. and Kay, T. Rendering Fur with Three 
Dimensional Textures. Computer Graphics 23, 3 (July 1989), 271-280. 12. Kenwright, D. and Mallinson, 
G. A 3-D Streamline Tracking Algorithm Using Dual Stream Functions. IEEE Visualization 92 Conference 
Proceedings (October 1992), 62-68. 13. Max, Nelson. Personal Communication (1992). 14. Perlin, K. An 
Image Synthesizer. Computer Graphics 19, 3 (August 1985), 287-296. 15. Perlin, K. Hypertexture. Computer 
Graphics 23, 3 (July 1989), 253-262. 16. Pratt, W. Digital Image Processing. 2nd ed. John Wiley &#38; 
Sons, Inc. (1991), 243-245. 17. Sims, K. Arti.cial Evolution for Computer Graphics. Com­puter Graphics 
25, 4 (August 1991), 319-328. 18. Sims, K. Choreographed Image Flow. The Journal of Visual­ization and 
Computer Animation 3, 1 (January-March 1992), 31-43. 19. Tufte, E. The Visual Display of Quantitative 
Information. Chesire, CT: Graphics Press (1983). 20. Turk, G. Generating Textures on Arbitrary Surfaces 
Using Reaction-Diffusion Textures. Computer Graphics 25, 4 (July 1991), 289-298. 21. Upson, C. and Keeler, 
M. V-Buffer: Visible Volume Render­ing. Computer Graphics 22, 4 (August 1988), 59-64. 22. Van Gelder, 
A. and Wilhelms, J. Interactive Animated Visual­ization of Flow Fields. Proceedings of the Workshop on 
Vol­ume Visualization, Kaufman and Lorensen Eds. (1992). 23. Van Wijk, J. Spot Noise Texture Synthesis 
for Data Visualiza­tion. Computer Graphics 25, 4 (July 1991), 309-318. 24. Witkin, A. and Kass, M. Reaction-Diffusion 
Textures. Com­puter Graphics 25, 4 (July 1991), 299-308.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166152</article_id>
		<sort_key>271</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>35</seq_no>
		<title><![CDATA[Frequency domain volume rendering]]></title>
		<page_from>271</page_from>
		<page_to>278</page_to>
		<doi_number>10.1145/166117.166152</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166152</url>
		<keywords>
			<kw><![CDATA[Fourier transform]]></kw>
			<kw><![CDATA[digital signal processing]]></kw>
			<kw><![CDATA[medical imaging]]></kw>
			<kw><![CDATA[scientific visualization]]></kw>
			<kw><![CDATA[shading models]]></kw>
			<kw><![CDATA[volume rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.1</cat_node>
				<descriptor>Computations on matrices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003719</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on matrices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31099446</person_id>
				<author_profile_id><![CDATA[81332532185]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Totsuka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15037068</person_id>
				<author_profile_id><![CDATA[81100593780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Levoy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bracewell, Ronald, The Fourier Transform and its Applications, revised second edition, McGraw-Hill, 1986.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5536</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bracewell, Ronald, The Hartley Transform, Oxford University Press, 1986.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325171</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Cohen, Michael and Greenberg, Donald, "The Hemicube: A Radiosity Solution for Complex Environments", Computer Graphics, Vol. 19, No.3, pp.31-40, 1985.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378484</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Drebin, Robert, Carpenter, Loren, and Hanrahan, Pat, "Volume Rendering", Computer Graphics, Vol.22, No.4, pp.65- 74, 1988.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Dunne, Shane, Napel, Sandy, and Rutt, Brian, "Fast Reprojection of Volume Data", Proceedings of the First Conference on Visualization in Biochemical Computing, IEEE Computer Society Press, pp. 11-18, 1990.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Hottel, Hoyt, and Sarofim, Adel, "Radiative Transfer", McGraw-Hill, 1967.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>44652</ref_obj_id>
				<ref_obj_pid>44650</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Levoy, Marc, "Display of Surfaces from Volume Data", IEEE Computer Graphics andApplications , Vol.8, No.3, pp.29-37, 1988.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>78965</ref_obj_id>
				<ref_obj_pid>78964</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Levoy, Marc, "Efficient Ray Tracing of Volume Data", ACM Transactions on Graphics, Vol.9, No.3, pp.245-261, 1990.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>155302</ref_obj_id>
				<ref_obj_pid>155294</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Levoy, Marc, "Volume Rendering using the Fourier Projection-Slice Theorem", Proceedings of Graphics Interface '92, Canadian Information Processing Society, pp.61- 69, 1992.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>169705</ref_obj_id>
				<ref_obj_pid>169711</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Malzbender, Tom, "Fourier Volume Rendering",ACM Transactions on Graphics, Vo1.12, No.3, July 1993.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Napel, Sandy, Dunne, Shane, and Rutt, Brian, "Fast Fourier Projection for MR Angiography", Magnetic Resonance in Medicine, Vo1.19, pp.393-405, 1991.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15900</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Nishita, Tomoyuki and Nakamae, Eihachiro, "Continuous Tone Representation of Three-Dimensional Objects", Computer Graphics, Vol.20, No.4, pp.125-132, 1986.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83876</ref_obj_id>
				<ref_obj_pid>83867</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Pentland, Alex, "Linear Shape from Shading", International Journal of Computer Vision, Vol.4, pp.153-162, 1990.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949556</ref_obj_id>
				<ref_obj_pid>949531</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Subramanian, K.R. and Fussel, Donald, "Applying space subdivision techniques to volume rendering", Proceedings of the First IEEE Conference on Visualization. (Visualization ' 90), IEEE Computer Society Press, pp. 150-159, 1990.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97919</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Westover, Lee, "Footprint Evaluation for Volume Rendering", Computer Graphics, Vol.24, No.4, pp.367-376, 1990.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Zuiderveld, Karel, Koning, Anton, and Viergever, Max, "Acceleration of ray-casting using 3D distance transforms", Proceedings of the SPIE- Visualization in Biomedical Computing 1992, Vo1.1808, pp.324-335, 1992.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Frequency Domain Volume Rendering y..y Takashi TotsukaMarc Levoy SONY Corporation Computer Science 
Department, Stanford University Spatial Domain Frequency Domain  Abstract The Fourier projection-slice 
theorem allows projections of volume data to be generated in 2 log time for a volume of size 3. O nn 
n The method operates by extracting and inverse Fourier transforming 2D slices from a 3D frequency domain 
representation of the volume. Unfortunately, these projections do not exhibit the occlusion that is characteristic 
of conventional volume renderings. We present a new frequency domain volume rendering algorithm that 
replaces much slice extraction of the missing depth and shape cues by performing shading calcula­tions 
in the frequency domain during slice extraction. In particular, we demonstrate frequency domain methods 
for computing linear or nonlinear depth cueing and directional diffuse re.ection. The resulting images 
can be generated an order of magnitude faster than volume renderings and may be more useful for many 
applications. CR Categories: I.3.7 [Computer Graphics]: Three-dimensional Figure 1: Volume rendering 
using Fourier projection slice Graphics and Realism.; I.3.3 [Computer Graphics]: Picture/Image theorem 
Generation; Display Algorithms.  Additional Keywords: Volume rendering, Fourier transform, Shading models, 
Scienti.c visualization, Medical imaging, Digi-For a 3D volume, the theorem states that the following 
two are a tal signal processing. Fourier transform pair:  The 2D image obtained by taking line integrals 
of the volume along rays perpendicular to the image plane. 1 Introduction The 2D spectrum obtained by 
extracting a slice from the Volume rendering is an important tool for visualizing 3D scalar Fourier transform 
of the volume along a plane which includes .elds. Most existing algorithms operate in the spatial domain. 
the origin and is parallel to the image plane. They can be classi.ed as either image space algorithms 
(e.g. [7]) Using this theorem, once a volume data is Fourier transformed,or object space algorithms (e.g. 
[4], [15]) depending on the order an (orthographic) image for any viewing direction can be obtainedin 
which the data is traversed: along each ray cast from the image by extracting a 2D slice of the 3D spectrum 
at the appropriateplane or along X, Y, and Z axis of the volume data. The complexity of these algorithms 
is 3orientation and then inverse Fourier transforming it (.gure 1). The since all voxels must be visited 
to cost of this approach is dominated by the 2D inverse fast Fourierrender an image. This high cost limits 
the use of these algorithms transform (IFFT) which is 2 log . Hence, the overall costin interactive environments. 
Although ef.cient algorithms exist for O n O nn O nnn sparse data sets [8], [14],[16], such optimization 
is data dependent. is also 2 log . Since log grows slowly, the advantage of this approach over spatial 
domain algorithms is greater at large dataIn an effort to drastically reduce rendering costs, frequency 
do­ sizes. main algorithms based on the Fourier projection slice theorem have Despite their theoretical 
speed advantage, frequency domain vol­been proposed [5], [10]. It is well known that the integral of 
a 1D ume rendering algorithms suffer from several well-known problems: signal is equal to the value of 
its spectrum at the origin. The Fourier projection slice theorem extends this notion to higher dimensions. 
 High interpolation cost: Because the sample points of the 3D spectrum and those of the 2D slice do not 
coincide except * Sony Corporation. 6-7-35 Kitashinagawa, Shinagawa at the origin, the 3D spectrum must 
be interpolated and then Tokyo 141, Japan (totsuka@av.crl.sony.co.jp) resampled in order to extract a 
2D slice. Since this interpo- Center for Integrated Systems, Stanford University lation is imperfect, 
replicas of the volume data are not fully Stanford, CA 94305 (levoy@cs.stanford.edu) suppressed, causing 
ghosts to appear on the projection image. Because any .lter that provides a sharp cutoff in the spatial 
domain also has wide support, high-quality interpolation is y O n Permission to copy without fee all 
or part of this material is granted provided that the copies are not made or distributed for directprovided 
that the copies are not made or distributed for direct expensive. As the interpolation is 2, the FFT 
is still commercial advantage, the ACM copyright notice and the title of the commercial advantage, the 
ACM copyright notice and the title of the asymptotically dominant. However, due to a large constant publication 
and its date appear, and notice is given that copying is by publication and its date appear, and notice 
is given that copying is by factor associated with the interpolation, current implementa­ permission 
of the Association for Computing Machinery. To copy permission of the Association for Computing Machinery. 
To copy tions spend the majority of their running time in interpolation, otherwise, or to republish, 
requires a fee and/or specific permission. otherwise, or to republish, requires a fee and/or specific 
permission. &#38;#169;1993 ACM-0-89791-601-8/93/008 $1.50 &#38;#169;1993 ACM-0-89791-601-8/93/008/0015 
$1.50 making the algorithm not attractive for practical data sizes (1283 or 2563). Memory cost: Due to 
the wide dynamic range and complex arith­metic associated with Fourier transforms, a pair of .oating 
point numbers is required for each voxel. Assuming a 64­bit double precision representation, 16 bytes 
are required per voxel. Bycontrast,only1bytepervoxelisnecessaryinspatial domain algorithms. Lack of depth 
information: The projection obtained by the Fourier projection slice theorem is a line integral normal 
to the direction of view. Voxels on a viewing ray contribute equally to the image regardless of their 
distance from the eye. The image therefore lacks occlusion, an important visual cue. While some users 
(diagnostic radiologists in particular) prefer integral projections since nothing is hidden from view, 
this characteristic would be considered a drawback in most appli­cations. The .rst two problems listed 
above are technical in nature, and several promising solutions are proposed later in this paper. The 
lack of occlusion is fundamental, however, in so far as no projection­slice theorem is known that mimics 
the integro-differential equation ([6]) approximated by volume rendering algorithms. Fortunately, occlusion 
is only one of many cues employed by the human visual system to determine the shape and spatial relationships 
of objects. Other available cues include perspective, shading, texture, shadows, atmospheric attenuation, 
stereopsis, ocular accommodation, head motion parallax, and the kinetic depth effect. It is possible, 
of course, to apply any shading technique in the spatial domain before the volume is Fourier transformed. 
However, such a naive approach would require recomputation of the volume followed by an expensive 3D 
forward FFT each time the view or the lighting condition is changed. In an earlier paper [9], we instead 
showed that for a limited class of shading models, the dependence on viewing direction and lighting direction 
could be factored out of the projection integral, yielding equations of the form  I a w i X i p n w 
i oZ l1 t 1 f i p fx i teytezt t dt h h 1 0 Here, effects of viewing and lighting direction are solely 
expressed by weights while the volumes are independent of them. The indicated integration can be evaluated 
ef.ciently using the projection slice theorem. For example, linear depth cueing can be computed as the 
weighted sum of projections through three volumes that are depth cued before 3D forward FFT along X, 
Y, and Z directions, respectively. The obvious disadvantage of this hybrid spatial-frequency do­main 
approach is that it requires multiple copies of the volume. While still asymptotically faster than conventional 
spatial domain volume rendering, implementation considerations (problems one and two above) make it barely 
superior in practice. In the present paper, we describe methods for rendering vol­umes with depth cueing 
and directional shading that operate entirely within the frequency domain. They are based on two well-known 
properties of the Fourier transform. Multiplication by a linear ramp in the spatial domain is equiv­alent 
to differentiation in the Fourier domain. Differentiation in the spatial domain is equivalent to multipli­cation 
by a linear ramp in the Fourier domain. Using these properties, depth cueing implemented in [9] as spatial 
domain multiplication, is implemented in the present paper using frequency domain differentiation. Similarly, 
directional shading, Spatial Domain Frequency Domain F(s) Pm(s) * h(x) f(x) Figure 2: Premultiplication 
of the volume data implemented in [9] using spatial domain differentiation, is imple­mented in the present 
paper using frequency domain multiplication. The remainder of the paper is organized as follows. Section 
2 reviews the previous works. Section 3 presents our new frequency domain shape cueing techniques. Sections 
4 and 5 refer to solutions to the interpolation and the memory cost problems, respectively. Section 6 
shows results from our implementation, and section 7 gives conclusions and possible future directions. 
 2 Base Algorithm We begin by brie.y reviewing current frequency domain volume rendering algorithms. 
In the following discussion, small letters ( . . . ) represent data in the spatial domain and capital 
letters (fFe gGee . . . ) represent data in the frequency domain. We also assume that the transform between 
the two domains is the Fourier transform which is denoted by . Let be a volume and be its Fourier transform. 
ands ffH x s FF s F s F sx are 3D vectors in the spatial and frequency domain, respectively. Given , 
the algorithm .rst transforms it into the frequency domain to yield . This is done only once. For each 
view, the discrete spectrum is interpolated along the extraction plane (parallel to the image plane and 
passing through the origin) using a .lter . The interpolated spectrum is resampled to obtain a 2D spectrum 
which is then inverse transformed to obtain a spatial domain projection. By the convolution theorem, 
interpolation corre­ f x e h x H s HFF s Pp m n m Hh x f xs a x h i x a sponds to in the spatial domain. 
Here, is the response of the .lter. Unless is an ideal lowpass .lter, its response has a smooth shoulder. 
Thus, the periphery of the volume and consequently the periphery of the projected image is attenuated. 
To cope with this vignetting problem, the volume datacan 1 be premultiplied by the reciprocal of the 
response, before its forward transformation [10]. As and cancel during interpolation, we obtain a correct 
slice of (.gure 2). We have implemented this method using .lters obtained from Malzbender and have obtained 
excellent results, as documented in section 4 and 6. 3 Shape Cueing Techniques 3.1 Depth Cueing Depth 
cueing is obtained by weighting voxels according to their distance from the observer. Let be the weighting 
function or depth cueing function for a given eye position. Then, a depth-cued volume is expressed as 
. By transforming it to the frequency domain and extracting a slice, we obtain a depth cued projection. 
As stated earlier, this straightforward approach requires an expensive 3D FFT (3 log ) for each view. 
There is, however, an elegant and inexpensive equivalent operation in frequency domain. Including the 
compensation for the .lter response, spatialp m x gn H s nf x np e m d x f x e d F x f f e p x m x e 
d x e domain depth cueing can be expressed as . By transforming and interpolating, this corresponds 
to at sample points on the slice in the frequency domain. Using the convolution theorem, this expression 
can be rewritten as follows: DH s 0 s H a 0 Hd x F s F f F f n s D f x f n s DPdp x m xs e xs pp m n 
m g P x m n x g H s g 0 n s n HH n HD ss H s H 0 (2) where . Thus, merely by replacing the interpolation 
.lter with , we have obtained depth cueing. Note that the above expression operates entirely in the frequency 
domain, and moreover is evaluated only on the plane of the slice being extracted. Hence, it is a 2D operation. 
Note also that because is independent of the eye position, the 3D forward transform is performed only 
once. Although must be computed for each view, the cost of recom­putation is small because the support 
of .lter is small (3353) and is usually a simple expression. In practice, the recompu­tation is negligible 
compared with the cost of interpolation itself. This frequency domain depth cueing method applies to 
any depth cueing function . Indeed, the method can be designed to highlight the middle portion of the 
volume while attenuating the front and back portions. By way of example, we .rst consider simple linear 
depth cueing, . Let the view vector be . The signed depth measured from the origin of the volume is thus 
given by , and can be written as where is the strength of the depth cueing effect and is a constant (see 
.gure 3). Taking Fourier transforms, we obtain d e lx x n L CfHH cue a 00 s he D xlx f aa e s d e ly 
a x e u H e u Ci a z s CC r cue i cue C n C cue D VV l VVs ere Le H xs V CC e avav x gg CF av s g Hd 
l sx C avg 42 where is the differential operator of convolution (). Substituting the interpolation .lter 
with depth cueing () yields (5)2 The .rst term exhibits the depth cueing effect. Since can r HH 0 be 
precomputed and stored in a table, computation of is of insigni.cant cost. An example of frequency domain 
linear depth cueing and projection is shown in .gure 6(b). As a reference, the same volume rendered without 
depth cueing is shown in .gure 6(a). Although any function can be used for , .nding one that has DH 0 
H 3 0 a simple form reduces the cost of computing . The size of is also a consideration, since it directly 
impacts rendering time. To illustrate this important issue, let us employ a half period of Figure 3: 
Linear depth cueing a sine wave as . Since the transform of a sine function is two impulses, can be computed 
by shifting and adding three copies1 with complex weights. Note that this considerably increases the 
size of the .lter kernel. By adjusting the origin, amplitude, and period such that the value is zero 
at the farthest voxel and unity at the closest voxel, we eliminate the need for a DC term. now has the 
form 1 2 where 1 and 2 are s w CF s u s w CF s s w CDC complex constants determined by the amplitude 
and the shift of the wave and is determined by the period of the wave. The period is typically made long 
enough so that the depth cueing appears almost linear. We can further remove one of the impulses by doubling 
the weight of the remaining impulse. By removing one of the impulses, the projection image is no longer 
a real2. However, the real part of the result still contains the correct projection image. With this 
technique, depth cueing is implemented by an interpolation with a shifted , which is practically free. 
The notion of a shifted gives us an alternative way to look at the process. Extracting a slice from a 
spectrum at a position translated from the origin by a distance in a direction corresponds totH VV Hdd 
V e iadt phase-shifting the spatial domain projection by 2at distance in the same direction . The real 
part of such a phase-shifted projection appears to fade in and out as a function of position in direction 
and, for appropriate values of , the visual effect is that of depth cueing. 3.2 Directional Shading 
In a scene composed of surfaces, directional shading using the well­known Lambertian re.ection model 
is given by MAX06  N C amb C d a amb x m O L c L ambC dif C dif OO c L cdif p e N L e amb L t L dif 
 where and are constants de.ning the strength of ambient and directional shading terms, is an object 
color, and are constants de.ning the color of ambient and directional lights, and and are unit surface 
normal and light vectors, respectively. 1Two for the impulses of the sine wave term and one for the constant 
term of . 2The imaginary part is a cosine wave since we are using the analytic signal of the depth cueing 
function. See the discussion on the Hilbert transform in [1]. Figure 4: Hemispherical light source Ignoring 
the attenuation of light inside the volume, the ambient term can be approximated using 7 The diffuse 
term, however, must be handled carefully because the nonlinear function MAX does not have a simple frequency 
domain E i x j NL e LN j representation. Note that the frequently used alternative, , which shadessurfaces 
as if they are two-sided rather than the bound­ing surface of a solid,is also nonlinear and cannot be 
handled directly in the frequency domain. To avoid this problem, we employ a hemispherical light source 
[12], [9]. The irradiance on a surface having normal vector illuminated by a hemisphere whose pole points 
in direction as shown in .gure 4 is proportional by Nusselt s analog (as described in [3]) to the projection 
of the visible portion of the hemisphere down onto the plane containing the surface, or11 1cos1 8 E 
i a L dif C dif O c L dif o a p L dif N p e L t N e L t 22 With this shading model, the diffuse term 
in a surface model is expressed as 1 19 2 For volumes, we have2 1 r f x a CC dif LL dif jr p jr ff xx 
j o j p r f rjr f x f x e Lx e L tt j h (10) Since volume datasets do not have explicitly de.ned surfaces, 
is used as the normal vector at each location. The strength of directional shading in volume rendering 
algorithms is commonly made proportional to the gradient magnitude as a simulation of the surface-ness 
of the volume [4],[7]. Locales having high gradient magnitudes (i.e., steep jumps in density) re.ect 
more light. Equation (10) can be computed entirely in the frequency domain. By the derivative theorem, 
the gradient in one domain is the .rst moment in the other domain. Thus, the shading computation can 
be performed as a moment computation in the frequency domain. This useful property of linear shading 
can also be exploited in image understanding algorithms. For example, [13] uses the moment to estimate 
the orientation of surfaces assuming that the re.ectance function is linear with respect to the slope 
of the surfaces. Transforming equations (7) and (10) to the frequency domain and including compensation 
for the .lter response, we obtain F d C amb L amb f x 12 1 Ff f x p m x gFfjr f x j p m x g Spectra 
H(s) Extracted slice Figure 5: Shading computation in frequency domain. 1 (ambient term), 2 (shad­  
a dif s e L 1 ing term), 3 (constant term). 2 1 2 1 k a C amb a kL p a amb C amb CCC difdifdif L n amb 
L dif L p F difdif f f pp F iC x jr C jr fp difm x f x L x j dif gjn p r m H s f e xsLx t g t en L H 
tt s t 2 The .rst term corresponds to the ambient term and the part of equation (9) while the second 
term corresponds to the ac­companying constant 1. Once and f x Hf x p m x jr f x j N p m e Lx are Fourier 
transformed, the shading computation can be performed during slice extraction (.gure 5). Note that the 
interpolation .l­ter is applied .rst in order to reconstruct the pure spectrum of from the premultiplied 
volume. Then, the .rst moment of the spectrum is computed to apply the directional shading. Although 
computing a moment incurs a few additional .oating point operations per sample on the slice, the additional 
expense is small relative to the number of operations that are required to evaluate the convolution at 
the sample point. It should also be noted that equation (11) can be easily extended to multiple light 
sources. In this case, we only have to add the moment terms for additional light sources. The increase 
in the computation cost is minor. Figure 6(c) shows a projection shaded using this technique. As before, 
the method operates entirely in the frequency domain and requires computations only on the plane of the 
slice being extracted The major drawback of this shading model is that it requires a second spectrum, 
since there is no simple way to compute a gradient magnitude in the frequency domain. Hence, two slices 
must be extracted from two volumes. A linear Ffjr fC amb x j Lp mamb x f g x C dif C dif L dif r f (11) 
x shading equation such as that requires only one volume can be derived under an appropriate interpretation. 
However, the upper bound of is restricted in order not to generate negative values and consequently the 
shading effect is restricted. 3.3 Combining Depth Cueing and Shading It is possible to combine the depth 
cueing and directional shad­ing techniques described in the foregoing section. When the two techniques 
are used together, the shading must be applied .rst. Otherwise, distortion by the depth cueing would 
result in incorrect gradient vector by which the shading effect is computed. However, this order of operation 
requires two convolutions: one performed before the shading computation to recover by interpolation .l- 
HF ter and one performed after shading in order to apply the depth cueing function. This approach makes 
depth cueing no longer an inexpensive operation since we can t use the composite .lter . We can work 
around this problem by reversing the order of shad­ing and depth cueing and then adjusting the result 
to get the desired effect. Using this ordering, we employ the composite .lter to H 0 perform the interpolation 
and the depth cueing at once. As we will see, for practical settings, even this adjustment is not necessary. 
Here, we will examine the effect of reversed order operation in spatial domain. We focus on the gradient 
term of the shading equation (second term of equation (10)) since other terms are not affected by the 
order. Applying depth cueing function to equation (10), we obtain the shaded and depth cued term. Omitting 
the coef.cient 12 , the gradient term is . Reversing the order of computation, we get d xV d D a t L 
p C r a dif f h fL xx f dif e x d LxV d r 0 d D r e h e d x p LV D tp e Vx f t e xxV t r re r d L V f 
xx e L e L d x d x (12) The second term is the difference from the correct value. Since 2  nn On is 
a function of depth , the difference can be rewritten as where this If is ilbeccuei e x C cue C dif 1is 
a 1D deptshading effect, is usually common special case is is non-zero, w1 omes zero and the adjustng, 
the difference term i L dif 1set luminated from the side). h cumenshoe nenclu f 1In perpt is wn ied adin 
x V eing function. this cnot nen adjug all th e L To mendicular to (i.n .gure 6(d). ase, the difcessary. 
An stment. For (13) aximize the e., the scene e coef.cients is 14 ference term example of linear depth 
 which we can compute during slice extraction without convolution. For a more complex depth cueing function, 
a convolution is neces­sary. 4 Reducing Rendering Time Although the interpolation required in order to 
extract an arbitrarily oriented slice from the 3D spectrum is 2 , it consumes most of the running time. 
As might be expected, the cost of this interpolation step is almost entirely determined by the size of 
the .lter. For the 3 3 3 .lter we employ, 27 input samples contribute to each output sample. If we instead 
employed a 1 1 1 .lter, only one nn input sample would contribute to each output sample, a great saving 
in time. Because a smaller .lter has less sharp cut off in spatial domain, the resulting image would 
contain strong ghosts if it were used uniformly over the entire interpolation process. However, by adaptively 
changing the .lter size, we can reduce rendering time while maintaining high image quality. Most of the 
energy in a spectrum usually resides in a small number of low frequency components, while the vast majority 
of high frequency components are nearly zero. We have observed that usually 99% of the energy is contained 
by about 10% of the frequency components. This property makes an adaptive scheme which selects an inex­pensive 
.lter for weak frequency components very attractive. For simplicity, let us consider interpolation of 
a 1D spectrum by two .lters; a larger .lter 1 and a smaller .lter 2. Each input sample component is .ltered 
or scattered by either 1 or 2 according to its strength. Let 1 be the set of those samples that are .ltered 
by  HFFHH FHF a FF 1 and 2 be those .ltered by 2. Obviously, 1 2 . The correct result we want is 1 or 
in the spatial domain, 1. The adaptive scheme can thus be written as follows: 1 11 22 1 Lfh j a h uu. 
a h F fhh l j L f FHF Z h f n d l1 n t n H max H h 1 ju f Z h l1 F t h 1 nunj fFH h jjgu f j dxdsdxHh 
g dmax fh 1 221 1 22 1 (15) The term 2 2 1 denotes the difference between the adaptively .ltered image 
and the correct image. The mean square error is given by integrating the power of this error term. Using 
Rayleigh s theo­rem, its upper bound is given in the frequency domain as follows. 12 22 1 12 2 -2 2 12-2 
(16) where is the length of the non-zero region of and -is the maximum of 2 1. This upper bound allows 
us to select input samples to be .ltered by 2 such that the mean square error of the rendered image is 
below a user de.ned tolerance. Similar analysis provides an upper bound for the mean square error when 
more than 2 .lters are employed. The idea extends straightforwardly to 3D discrete signals. This adaptive 
scheme is incorporated to the slice extraction as follows. First, each sample in the 3D spectrum is examined, 
and those whose magnitude is small enough to satisfy equation (16) are marked. This process is done only 
once after a volume data is transformed to the frequency domain. During slice extraction, each sample 
point on the slice plane is visited. If for a given sample point all of the 3D spectrum voxels that fall 
within the support of the larger .lter are marked, the smaller .lter is employed instead. It is possible 
to improve this scheme further. To avoid testing all voxels falling within the support of the larger 
.lter, we modify the preprocess to mark only those voxels that themselves satisfy equation (16) and for 
which all neighboring voxels lying within a distance from them equal to one-half of the support of the 
larger .lter satisfy the equation. Given this more conservative marking, it is suf.cient during slice 
extraction to test the spectrum voxel closest to the slice sample position. If that voxel is marked, 
we know without visiting any other voxels that it is safe to employ the smaller .lter. 5 Reducing Memory 
Cost Because the 3D spectrum is complex and requires a .oating point representation due to its large 
dynamic range, a straightforward implementation using a double precision format consumes 16 times more 
memory than a spatial domain algorithm3. This explosion in memory cost can be controlled by using the 
Hartley transform [10] and a shorter number representation. The Hartley transform is a direct relative 
of the Fourier transform [2]. The transform is de.ned as follows:  Hf fx g a F H s a Z l1 t 1 fxCsxdx 
 cas217 3Assuming each voxel is represented by one byte in the spatial domain algorithm. With shading, 
spatial domain algorithms require more memory. (a) (b) (c) (d) Figure 6: Examples of frequency domain 
depth cueing and shading. (a) projection without depth cueing, (b) linear depth cueing, (c) directional 
shading without depth cueing, (d) directional shading with depth cueing. where cas2cos 2sin 2. Since 
the kernel is a real  F H sCsx a CsxCsxfx  function, this transform maps a real function to a real 
spectrum . Use of the Hartley transform, therefore, eliminates the need for a complex number. Since the 
Fourier spectrum of a real signal is hermitian4, the same amount of memory saving is possible with the 
Fourier transform by dropping half of the spectrum (e.g., store S x However, such only the positive 
coef.cients along the axis). implementation would unnecessarily complicate the slice extraction process. 
 Due to wide dynamic range of spectra, a .oating point format is necessary. Considering the necessity 
of premultiplying the volume before transforming, a 64-bit double precision format is a safe choice to 
represent a spectrum of a 2563 volume. However, even using the Hartley transform, this occupies 8 times 
more memory than the original volume. This problem can be minimized by using a shorter .oating point 
format. We have de.ned and used a 16-bit .oating point format which reduces the memory cost factor to 
two. 6 Results Figures 7-9 show images rendered using the algorithms we have described. The shading, 
depth cueing, adaptive .ltering, the Hart­ley transform, and the 16-bit .oating point format are all 
used in rendering these three images. Figure 7 shows a human skull mounted in a lucite head cast. The 
data was acquired using computed tomography (CT). Zeros are padded to the original data (1063) and resulting 
1283 volume data was rendered. The volume is shaded by a hemispherical light source located to the right 
and is also linearly depth cued with respect to the observer s position. The use of multiple light sources 
is shown in .gure 8. A polyg­onalization of the Utah teapot has been 3D scan-converted into a 2563 volume 
data which is then shaded by a red, a green, and a blue light located perpendicular to the observer and 
120 degrees apart. The resulting color on the surface provides some intuition for the orientation of 
the gradient vector. Figures 9 and 10 compare the frequency domain rendering tech­nique with a conventional 
spatial domain volume rendering. These images were generated using identical shading and depth cueing. 
There is no visible difference between the two images. The adaptive .ltering scheme described in section 
4 was imple­mented using a 3 3 3 and a 1 1 1 .lter with the maximum 4A signal whose real part is even 
and whose imaginary part is odd, i.e. f a x m. f . a . x m nnnn . Figure 7: Human head. Frequency domain 
volume render­ing. Data courtesy of North Carolina Memorial Hospital. difference in response set to (-) 
0.3. Figures 7-9 were gener­ max ated using this scheme. As shown in table 1, the scheme reduced the 
cost of interpolation to about 15% of the non-adaptive case. Relative error was always below 40dB, a 
level at which image differences are not visible. Table 1 also shows rendering times to generate .gures 
7-9. Ren­dering times by a spatial domain renderer are also shown for com­parison. These times include 
all necessary operations to create a 2D projection. For the frequency domain rendering technique, it 
consists of slice extraction (interpolation and resampling), inverse Harteley transform, and format conversion 
to and from the 16-bit .oating point format and the machine s native format. Times were measured on an 
IRIS Crimson with a 50Mhz R4000 processor us­ing non-optimized code. As the table shows, the running 
time of the frequency domain method grows much slower than the spatial domain method, which grows at 
3 . On The effect of round off error caused by the 16-bit .oating format was very small. Relative difference 
from images generated using a 64-bit double precision representation were below 50dB. Figures 7­9 were 
generated using this format.  7 Conclusions The use of the Fourier projection slice theorem allows us 
to re­place the 3 spatial domain projection computation that arises On Volume data Size Adaptive .ltering 
Rendering time Non adaptive Adaptive Num. ops. Num. ops.(Ratio) Freq. domain Spatial domain Head 1283 
5 92 105 1 01 105 (17.1%) 0.54 sec 3.15 sec Teapot 2563 1 81 106 2 33 105 (12.9%) 1.77 24.29 Turbine 
2563 185 106 300 105 (16.2%) 2.03 24.38 y Figure 8: Utah teapot. Frequency domain volume rendering. 
The pot is lit by a red light (right), a green light (upper left), and a blue light (lower left). in 
volume rendering with an 2 log frequency domain com- OOnnOnn putation, although the frequency domain 
projection operator is non­occluding, resulting in a loss of realism. In this paper, we have shown that 
other 3 spatial domain rendering computations that arise in volume rendering (i.e., shading and depth 
cueing) can be replaced with 2 frequency domain methods, and we propose that a judicious selection of 
these methods can restore much of the realism lost by using a non-occluding projection. The speed advantage 
of our algorithm over volume rendering is considerable. As our experiments show, a 1283 volume can be 
rendered in a fraction of a second on a conventional workstation. Further optimization of the code should 
achieve interactive render­ing without specialized hardware. Besides its speed advantage, the frequency 
domain approach lends itself to simple and elegant speed-accuracy tradeoffs. By extracting only the central 
portion of the 3D spectrum present on a slice, a renderer could provide a low resolution image quickly 
while the user is rotating the volume, to be replaced with a higher quality image when the mouse button 
or joystick is released. Since the core computations of the algorithm are convolution and the FFT, an 
implementation using digital signal processors (DSPs) obviously suggests itself. With the growth of multimedia 
appli­cations involving video and sound encoding and decoding, such processors are becoming a standard 
part of most graphics worksta­tions. It should also be noted that these computations exhibit high data 
level parallelism and can be parallelized in any one of several ways. With regard to limitations and 
improvements, further effort should be made to relax the limitations imposed by the linear nature of 
the Fourier/Hartley transform. The algorithm currently does not  h n y h n y A .ltering operation 
consists of a .lter table look up, a reference to a voxel, a multiplication, and an addition. Table 1: 
Effect of adaptive .ltering  Figure 9: Turbine blade. Frequency domain volume ren­dering. The blade 
is lit by a green light (top), a blue light (bottom), and a dim red light (right). Data courtesy of Gen­eral 
Electric. allow non-linear attenuation.  Acknowledgements The authors wish to thank Tom Malzbender for 
helpful suggestions and his interpolation .lter coef.cients and Ronald Bracewell for useful hints on 
the use of the Hartley transform. The notion that shading could be factored with respect to digital compositing, 
an idea that inspired the present work, was suggested by Brice Tebbs. Discussions with Adam Levinthal 
were useful in the early stages of this project. Hide Hirase s volume modeling toolkit helped us creating 
test datasets. This research was supported by the National Science Founda­tion (NSF), the National Aeronautics 
and Space Administration (NASA), and the sponsoring companies of the Stanford Center for Integrated Systems 
(CIS).  References [1] Bracewell, Ronald, The Fourier Transform and its Applica­tions, revised second 
edition, McGraw-Hill, 1986. [2] Bracewell, Ronald, The Hartley Transform, Oxford Univer­sity Press, 1986. 
[3] Cohen, Michael and Greenberg, Donald, The Hemicube: A Radiosity Solution for Complex Environments 
,Computer Graphics, Vol.19, No.3, pp.31-40, 1985. [4] Drebin, Robert, Carpenter, Loren, and Hanrahan, 
Pat, Vol­ume Rendering ,Computer Graphics, Vol.22, No.4, pp.65­74, 1988. [5] Dunne, Shane, Napel, Sandy, 
and Rutt, Brian, Fast Repro­jection of Volume Data ,Proceedingsof the First Conference on Visualization 
in Biochemical Computing, IEEE Computer Society Press, pp.11-18, 1990. [6] Hottel, Hoyt, and Saro.m, 
Adel, Radiative Transfer , McGraw-Hill, 1967. [7] Levoy, Marc, Display of Surfaces from Volume Data ,IEEE 
Computer Graphics and Applications, Vol.8, No.3, pp.29-37, 1988. [8] Levoy, Marc, Ef.cient Ray Tracing 
of Volume Data ,ACM Transactions on Graphics, Vol.9, No.3, pp.245-261, 1990. [9] Levoy, Marc, Volume 
Rendering using the Fourier Projection-Slice Theorem ,Proceedings of Graphics Inter­face 92, Canadian 
Information Processing Society, pp.61­69, 1992. [10] Malzbender, Tom, Fourier Volume Rendering ,ACM Trans­actions 
on Graphics, Vol.12, No.3, July 1993. [11] Napel, Sandy, Dunne, Shane, and Rutt, Brian, Fast Fourier 
Projection for MR Angiography ,Magnetic Resonance in Medicine, Vol.19, pp.393-405, 1991. [12] Nishita, 
Tomoyuki and Nakamae, Eihachiro, Continuous Tone Representation of Three-Dimensional Objects ,Com­puter 
Graphics, Vol.20, No.4, pp.125-132, 1986. [13] Pentland, Alex, Linear Shape from Shading ,International 
Journal of Computer Vision, Vol.4, pp.l53-162, 1990. [14] Subramanian, K.R. and Fussel, Donald, Applying 
space subdivision techniques to volume rendering ,Proceedings of the First IEEE Conference on Visualization. 
(Visualization 90), IEEE Computer Society Press, pp.150-159, 1990. [15] Westover, Lee, Footprint Evaluation 
for Volume Render­ing ,Computer Graphics, Vol.24, No.4, pp.367-376, 1990. [16] Zuiderveld, Karel, Koning, 
Anton, and Viergever, Max, Ac­celeration of ray-casting using 3D distance transforms ,Pro­ceedings of 
the SPIE Visualization in Biomedical Comput­ing 1992, Vol.1808, pp.324-335, 1992.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166153</article_id>
		<sort_key>279</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>36</seq_no>
		<title><![CDATA[View interpolation for image synthesis]]></title>
		<page_from>279</page_from>
		<page_to>288</page_to>
		<doi_number>10.1145/166117.166153</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166153</url>
		<keywords>
			<kw><![CDATA[image morphing]]></kw>
			<kw><![CDATA[incremental rendering]]></kw>
			<kw><![CDATA[interpolation]]></kw>
			<kw><![CDATA[motion blur]]></kw>
			<kw><![CDATA[motion compensation]]></kw>
			<kw><![CDATA[real-time display]]></kw>
			<kw><![CDATA[shadow]]></kw>
			<kw><![CDATA[virtual holography]]></kw>
			<kw><![CDATA[virtual reality]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Range data</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14048866</person_id>
				<author_profile_id><![CDATA[81451598765]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shenchang]]></first_name>
				<middle_name><![CDATA[Eric]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31101694</person_id>
				<author_profile_id><![CDATA[81100005753]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Lance]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Williams]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>91416</ref_obj_id>
				<ref_obj_pid>91385</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Airey, J., J. Rohlf and F. Brooks. Towards Image Realism with Interactive Update Rates in Complex Building Environments. ACM SIGGRAPH Special Issue on 1990 Symposium on Interactive 3D Graphics, 41-50.]]></ref_text>
				<ref_id>AIRE91</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Apple Human Interface Group. Object Maker.]]></ref_text>
				<ref_id>APPL92</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[In Interactive Experience, CHI'92, Monterey CA.]]></ref_text>
				<ref_id>exhibit</ref_id>
			</ref>
			<ref>
				<ref_obj_id>47454</ref_obj_id>
				<ref_obj_pid>47450</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Besl, P.J. Active Optical Range Imaging Sensors. Machine Vision and Applications Vol. 1, 1988, 127-152.]]></ref_text>
				<ref_id>BESL88</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Beier, T. and S. Neely. Feature-Based Image Metamorphosis. SIGGRAPH'92 Proceedings, 35-42.]]></ref_text>
				<ref_id>BEIE92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97894</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Chen, S. E. Incremental Radiosity: An Extension of Progressive Radiosity to an Interactive Image Synthesis System. SIGGRAPH'90 Proceedings, 135-144.]]></ref_text>
				<ref_id>CHEN90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378487</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Cohen, M. F., S. E. Chen, J. R. Wallace and D. P. Greenberg. A Progressive Refinement Approach to Fast Radiosity Image Generation. SIGGRAPH'88 Proceedings, 75- 84.]]></ref_text>
				<ref_id>COHE88</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134039</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Deering, M. High Resolution Virtual Reality. SIG- GRAPH'92 Proceedings, 195-202, 1992.]]></ref_text>
				<ref_id>DEER92</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Goshtasby, A. Stereo Correspondence by Selective Search. Proc. Japan Computer Vision Conf., 1-10, July, 1989.]]></ref_text>
				<ref_id>GOSH89</ref_id>
			</ref>
			<ref>
				<ref_obj_id>13023</ref_obj_id>
				<ref_obj_pid>13021</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Greene, N. Environment Mapping and Other Applications of World Projections. IEEE CG&amp;A, Vol. 6, No. 11, November, 1986.]]></ref_text>
				<ref_id>GREE86</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Greene, N. and M. Kass. Approximating Visibility with Environment Maps. Technical Report 41, 1993, Apple Computer, Inc.]]></ref_text>
				<ref_id>GREE93</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Hofman, G. R. The Calculus of the Non-Exact Perspective Projection. Eurographics'88 Proceedings, 429-442]]></ref_text>
				<ref_id>HOFM88</ref_id>
			</ref>
			<ref>
				<ref_obj_id>155315</ref_obj_id>
				<ref_obj_pid>155294</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Jevans, D. Object Space Temporal Coherence for Ray Tracing. Graphics Interface'92 Proceedings, 176-183, 1992.]]></ref_text>
				<ref_id>JEVA92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807465</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Lippman, A. Movie Maps: An Application of the Optical Videodisc to Computer Graphics. SIGGRAPH'80 Proceedings, 32-43.]]></ref_text>
				<ref_id>LIPP80</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Miller, G., E. Hoffert, S. E. Chen, E. Patterson, D. Blacketter, S. Rubin, S. A. Applin, D. Yim and J. Hanan. The Virtual Museum: Interactive 3D Navigation of a Multimedia Database. The Journal of Visualization and Computer Animation, Vol. 3, No. 3, 183-198, 1992.]]></ref_text>
				<ref_id>MILL92</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Miller, G.and S. E. Chen. Real-Time Display of Surroundings Using Environment Maps. Technical Report 42, 1993, Apple Computer, Inc.]]></ref_text>
				<ref_id>MILL93</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[MPEG Video Committee Draft, December, 1990.]]></ref_text>
				<ref_id>MPEG90</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Nagel, H.-H. Image Sequences - Ten (octal) Years from Phenomenology to a Theoretical Foundation. Proc. 8th ICPR, Paris 1986, 1174-1185.]]></ref_text>
				<ref_id>NAGE86</ref_id>
			</ref>
			<ref>
				<ref_obj_id>889041</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Poggio, T. and R. Brunelli. A Novel Approach to Graphics. MIT A.I. Memo No. 1354, C.B.I.P. Paper No. 71, February, 1992.]]></ref_text>
				<ref_id>POGG91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37435</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Reeves, W. T., D. H. Salesin and R. L. Cook. Rendering Antialiased Shadows with Depth Maps. SIGGRAPH'87 Proceedings, 283-291.]]></ref_text>
				<ref_id>REEV87</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Schumacker, R., B. Brand, M. Gilliland, and W. Sharp. Study for Applying Computer-Generated Images to Visual Simulation, Technical Report AFHRL-TR-69-14, NTIS AD700375, U.S. Air Force Human Resources Lab., Air Force Systems Command, Brooks AFB, TX, September, 1969.]]></ref_text>
				<ref_id>SCHU69</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Shapiro, B. L., N. I. Badler. Generating Soft Shadows with a Depth Buffer Algorithm. IEEE CG&amp;A, Vol. 4, No. 10, 5-38, 1984.]]></ref_text>
				<ref_id>SHAP84</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122725</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Teller, S and C. Sequin. Visibility Preprocessing for Interactive Walkthroughs. SIGGRAPH'91 Proceedings, pp.61-69, 1991.]]></ref_text>
				<ref_id>TELL92</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Venolia, D. and L. Williams. Virtual Integral Holography. Proc. SPIE-Extracting Meaning from Complex Data: Processing, Display, Interaction (Santa Clara, CA, February, 1990), 99-105.]]></ref_text>
				<ref_id>VENO90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Williams, L. Casting Curved Shadows on Curved Surfaces. SIGGRAPH'78 Proceedings, 270-274.]]></ref_text>
				<ref_id>WILL78</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91450</ref_obj_id>
				<ref_obj_pid>91385</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Williams, L. 3D Paint. ACM SIGGRAPH Special Issue on 1990 Symposium on Interactive 3D Graphics, 225- 233.]]></ref_text>
				<ref_id>WILL90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74371</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Wolberg, G. and T. E. Boult. Separable Image Warping with Spatial Lookup Tables. SIGGRAPH'89 Proceedings, 369-377.]]></ref_text>
				<ref_id>WOLB89</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Wolf, P. R. Elements of Photogrammetry, McGraw- Hill, New York, 1983.]]></ref_text>
				<ref_id>WOLF83</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 View Interpolation for Image Synthesis Shenchang Eric Chen, Lance Williams Apple Computer, Inc. ABSTRACT 
Image-space simplifications have been used to accelerate the calculation of computer graphic images since 
the dawn of visual simulation. Texture mapping has been used to provide a means by which images may themselves 
be used as display primitives. The work reported by this paper endeavors to carry this concept to its 
logical extreme by using interpolated im­ages to portray three-dimensional scenes. The special-effects 
technique of morphing, which combines interpolation of tex­ture maps and their shape, is applied to computing 
arbitrary in­termediate frames from an array of prestored images. If the im­ages are a structured set 
of views of a 3D object or scene, inter­mediate frames derived by morphing can be used to approximate 
intermediate 3D transformations of the object or scene. Using the view interpolation approach to synthesize 
3D scenes has two main advantages. First, the 3D representation of the scene may be replaced with images. 
Second, the image synthesis time is independent of the scene complexity. The correspondence between images, 
required for the morphing method, can be pre­determined automatically using the range data associated 
with the images. The method is further accelerated by a quadtree de­composition and a view-independent 
visible priority. Our ex­periments have shown that the morphing can be performed at interactive rates 
on today s high-end personal computers. Po­tential applications of the method include virtual holograms, 
a walkthrough in a virtual environment, image-based primitives and incremental rendering. The method 
also can be used to greatly accelerate the computation of motion blur and soft shadows cast by area light 
sources. CR Categories and Subject Descriptors: I.3.3 [Computer Graphics]: Picture/Image Generation; 
I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Real­ism. Additional Keywords: image morphing, 
interpolation, virtual reality, motion blur, shadow, incremental rendering, real-time display, virtual 
holography, motion compensation. INTRODUCTION Generating a large number of images of an environment 
from closely spaced viewpoints is a very useful capability. A traditional application is a flight in 
the cabin of an aircraft simulator, whereas the contemporary model is perhaps a walk through a virtual 
environment; in both cases the same scene is Permission to copy without fee all or part of this material 
is granted provided that the copies are not made or distributed for direct provided that the copies 
are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of 
the commercial advantage, the ACM copyright notice and the title of the publication and its date appear, 
and notice is given that copying is by publication and its date appear, and notice is given that copying 
is by permission of the Association for Computing Machinery. To copy permission of the Association for 
Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. otherwise, 
or to republish, requires a fee and/or specific permission. &#38;#169;1993 ACM-0-89791-601-8/93/008 $1.50 
&#38;#169;1993 ACM-0-89791-601-8/93/008/0015 $1.50 displayed from the view of a virtual camera controlled 
by the user. The computation of global illumination effects, such as shadows, diffuse and specular inter-reflections, 
also requires a large number of visibility calculations. A typical approach to this problem is to rely 
on the computer to repetitively render the scene from different viewpoints. This approach has two ma­jor 
drawbacks. First, real-time rendering of complex scenes is computationally expensive and usually requires 
specialized graphics hardware. Second, the rendering time is usually not constant and is dependent on 
the scene complexity. This prob­lem is particularly critical in simulation and virtual reality ap­plications 
because of the demand for real-time feedback. Since scene complexity is potentially unbounded, the second 
prob­lem will always exist regardless of the processing power of the computer. A number of approaches 
have been proposed to address this problem. Most of these approaches use a preprocess to compute a subset 
of the scene visible from a specified viewing re-gion[AIRE91, TELL92]. Only the potentially visible objects 
are processed in the walkthrough time. This approach does not completely solve the problem because there 
may be viewing regions from which all objects are visible. Greene and Kass[GREE93] developed a method 
to approximate the visibil­ity at a location from adjacent environment maps. The envi­ronment maps are 
Z-buffered images rendered from a set of dis­crete viewpoints in 3D space. Each environment map shows 
a complete view of the scene from a point. An environment map can take the form of a cubic map, computed 
by rendering a cube of 90° views radiating from that point [GREE86]. The environ­ment maps are pre-computed 
and stored with viewpoints ar­ranged in a structured way, such as a 3D lattice. An image from a new viewpoint 
can be generated by re-sampling the environ­ment maps stored in adjacent locations. The re-sampling pro­cess 
involves rendering the pixels in the environment maps as 3D polygons from the new viewpoint. The advantage 
of this approach is that the rendering time is proportional to the envi­ronment map resolutions and is 
independent of the scene com­plexity. However, this method requires Z-buffer hardware to render a relatively 
large number of polygons interactively, a feature still not available on most low-end computers. This 
paper presents a fast method for generating intermedi­ate images from images stored at nearby viewpoints. 
The method has advantages similar to those of Greene and Kass method. The generation of a new image is 
independent of the scene complexity. However, instead of drawing every pixel as a 3D polygon, our method 
uses techniques similar to those used in image morphing[BEIE92]. Adjacent images are morphed to create 
a new image for an in-between viewpoint. The morphing makes use of pre-computed correspondence maps and, 
therefore, is very efficient. Our experiments with the new method have shown that it can be performed 
at interactive rates on inexpen­sive personal computers without specialized hardware. The new method 
is based on the observation that a sequence of images from closely spaced viewpoints is highly coherent. 
Most of the adjacent images in the sequence depict the same ob­jects from slightly different viewpoints. 
Our method uses the camera s position and orientation and the range data of the im­ages to determine 
a pixel-by-pixel correspondence between im­ages automatically. The pairwise correspondence between two 
successive images can be pre-computed and stored as a pair of morph maps. Using these maps, corresponding 
pixels are in­terpolated interactively under the user s control to create in-be­tween images. Pixel correspondence 
can be established if range data and the camera transformation are available. For synthetic images, range 
data and the camera transformation are easily obtainable. For natural images, range data can be acquired 
from a ranging camera [BESL88], computed by photogrammetry [WOLF83], or modeled by a human artist [WILL90]. 
The camera transforma­tion can be found if the relative positions and orientations of the camera are 
known. The idea of using images to represent a virtual environment has been presented previously. An 
earlier approach uses com­puter controlled videodiscs to perform surrogate travel [LIPP80]. A more recent 
approach uses digital movie technolo­gies to construct a virtual museum [MILL92]. In both systems, a 
user navigates a finite set of routes and directions that have been pre-determined. Our method allows 
greater flexibility in the navigation because the stored frames can be interpolated smoothly to synthesize 
arbitrary intermediate points of view. A static subject or environment portrayed by a restricted set 
of images indexed by the user's point of view supports a form of "desktop virtual reality" termed "virtual 
integral holography" [VENO90]. In this context also, our method permits smooth interpolation of the images 
to present a continuous display se­quence, rather than quantizing the user's point of view and jumping 
to the closest prestored image. The morphing method can be used to interpolate a number of different 
parameters, such as camera position, viewing an­gle, direction of view and hierarchical object transformation. 
The modeling and viewing transformations can be concatenated to compute the correspondence mapping between 
two images. Generally, the images can be arranged in an arbitrary graph structure. The nodes of the graph 
are the images. Each arc in the graph represents a correspondence mapping, which is bi-direc­tional, 
and two maps are associated with each arc. The number of interpolation parameters determines the dimensionality 
of the graph. For instance, the graph for a virtual camera moving with two degrees of freedom (the latitudes 
and longitudes of a sphere bounding an object at a central "look-at" point, for ex­ample) is a simple 
polyhedron (rendering of objects rather than environments will be discussed in more detail in Section 
4.4, Image-based Primitives.) The camera s location coordinates in­dex a point on a face of the polyhedron, 
and the desired view is synthesized by interpolating the images and mappings stored with the vertices 
and edges of the face. Note that if each image is of the form of an environment map, view angle and direction 
also can be interpolated by re-projecting the environment map to the desired view orientation [MILL93] 
without increasing the dimensionality of the graph. Similarly, a camera moving in 3D is supported by 
a graph which takes the form of a 3D space lat­tice. The barycentric coordinates of the view location 
can be used to interpolate among the images attached to the vertices of the enclosing tetrahedron in 
a lattice of tetrahedra. For the representation of scenes with objects moving or changes other than those 
consequent to a change in viewpoint, the graph becomes a general polytope. Generally, arbitrary dis­tortions 
of surfaces are accommodated by the mapping, as are hierarchical motions of linkages or the limbs of 
animated char­acters1. To index such an elaborate set of mappings by the var­ious parameters can be an 
arbitrarily complex process, requir­ing multivariate interpolation of a multidimensional graph. Without 
loss of generality, this paper will concentrate on the interpolation of the camera position in 1D and 
2D space (accommodating "virtual holograms" of objects as well as re­stricted navigation in 3D scenes). 
The scene is assumed to be static, and all the image changes are as a result of camera movement. Although 
the method can be applied to natural im­ages, only synthetic ones have been attempted in the work de­scribed 
here. Interpolation of images accurately supports only view-independent shading. Reflection mapping or 
Phong specular reflection could be performed with separate maps for reflection map coordinates or normal 
components, but only dif­fuse reflection and texture mapping have been presented here. Section 2 introduces 
the basic algorithms of the method as well as its limitations and optimizations. Section 3 gives im­plementation 
details and shows some examples. Section 4 shows applications of the method to virtual reality, temporal 
anti-aliasing, generating shadows from area lights, image­based display primitives and incremental rendering 
("progressive refinement"). Conclusions and future directions are discussed in the last section. 2 VISIBILITY 
MORPHING Image morphing is the simultaneous interpolation of shape and texture. The technique generally 
involves two steps. The first step establishes the correspondence between two images and is the most 
difficult part of most morphing methods. The correspondence is usually established by a human animator. 
The user might, for example, define a set of corresponding points or line segments within a pair or set 
of images. An algo­rithm is then employed to determine the correspondence (mapping) for the remainder 
of the images[BEIE92]. The sec­ond step in the process is to use the mapping to interpolate the shape 
of each image toward the other, according to the particu­lar intermediate image to be synthesized, and 
to blend the pixel values of the two warped images by the same respective coeffi­cients, completing the 
morph. Our method uses the camera transformation and image range data to automatically determine the 
correspondence between two or more images. The correspondence is in the form of a forward mapping. The 
mapping describes the pixel-by-pixel correspondence from the source to the destination image. The mapping 
is also bi-directional since each of the two images can act as the source and the destination. In the 
basic method, the corresponding pixels 3D screen coordinates are interpolated and the pixels from the 
source image are moved to their interpo­lated locations to create an interpolated image. For pixels which 
map to the same pixel in the interpolated image, their Z­coordinates are compared to resolve visibility. 
Cross-dissolv­ing the overlapping pixels colors may be necessary if the im­age colors are not view-independent. 
This process is repeated for each of the source images. This method is made more efficient by the following 
two properties. First, since neighboring pixels tend to move to­gether in the mapping, a quadtree block 
compression is em­ployed to exploit this coherence. Adjacent pixels which move in a similar manner are 
grouped in blocks and moved at the same time. This compression is particularly advantageous since a view-independent 
visible priority among the pixel blocks can be established. The pixel blocks are sorted once by their 
Z-co­ 1Establishing such elaborate mappings is straightforward for synthetic images, a classic vision 
problem for natural ones. ordinates, when the maps are created, and subsequently dis­played from back 
to front to eliminate the overhead of a Z­buffer for visibility determination. We will describe our method 
in terms of the morphing be­tween two images first. Generalization of the method to more images is straightforward 
and will be discussed later. 2.1 Establishing Pixel Correspondence As a camera moves, objects in its 
field of view move in the opposite direction. The speed of each object s apparent move­ment is dependent 
on the object s location relative to the cam­era. Since each pixel s screen coordinates (x, y and z) 
and the camera s relative location are known, a 4x4 matrix transforma­tion establishes a correspondence 
between the pixels in each pair of images. The transformations can be pre-computed and reduced to a 3D 
spatial offset vector for each of the pixels. The offset vector indicates the amount each of the pixels 
moves in its screen space as a result of the camera s movement. The off­set vectors are stored in a morph 
map, which represents the forward mapping from one image to another. This map is simi­lar in concept 
to a disparity map computed from a stereo pair[GOSH89], the field of offset vectors computed for optical 
flow analysis[NAGE86], or motion compensation in video compression and format conversion[MPEG90]. For 
a computed image or range image, an exact pixel-by-pixel map can be cre­ated. The mapping is many-to-one 
because many pixels from the first image may move to the same pixel in the second im­age. Therefore, 
the morph map is directional and two morph maps are needed for a pair of images. The use of a pre-computed 
spatial look-up table for image warping has been presented in [WOLB89]. Wolberg used the look-up table 
to implement arbitrary forward mapping func­tions for image warping. Wolberg's maps contained absolute 
coordinates rather than offset vectors. In a typical image morph, as described in the beginning of this 
section, a sparse correspondence provided by a human op­erator is used to perform strictly two-dimensional 
shape inter­polation. Such a morph can also be used to interpolate stored images in order to represent 
3D scenes or objects, as suggested in [POGG91]. The advantages of our method are that the corre­spondence 
is dense (every pixel has an explicitly computed map coordinate), the correspondence is automatic (rather 
than relying on human effort), and the explicit prestored maps per­mit the image deformations to be generated 
very quickly. 2.2 Interpolating Correspondences To generate an in-between view of a pair of images, 
the off­set vectors are interpolated linearly and the pixels in the source image are moved by the interpolated 
vector to their destina­tions. Figure 1 shows the offset vectors, sampled at twenty­pixel intervals, 
for the camera motion sequence in Figure 3. The interpolation is an approximation to the transforma­tion 
of the pixel coordinates by a perspective viewing matrix. A method which approximates the perspective 
changes with lo­cal frame shifting and scaling is presented in [HOFM88]. Per­spective transformation 
requires multiplication of the pixel co­ordinates by a 4x4 matrix and division by the homogeneous coordinates, 
a rather computationally taxing process, although bounded by image resolution rather than scene complexity. 
Linear interpolation of pixel coordinates using the morph maps, on the other hand, is very efficient 
and can be performed incrementally using forward differencing. If the viewpoint offset is small, the 
interpolation is very close to the exact solution. Moreover, quadratic or cubic inter­polation, though 
slightly more expensive to perform, can be used to improve the accuracy of the approximation. When the 
viewpoint moves parallel to the viewing plane, the linear in­terpolation produces an exact solution. 
This case is demon­strated in Figure 2a, which traces the paths of mapped pixels in the interpolated 
image as the viewpoint traverses the four cor­ners of a square parallel to the viewing plane. The squares 
in the figure are the extents of the pixel movement. Because the squares are parallel to the viewing 
plane, the linear interpola­tion of the square corners produces the same result as perspec­tive transformation. 
Another special case is when the view­point moves perpendicular to the viewing plane along a square parallel 
to the ground(Figure 2b). The resulting pixel locations form trapezoids, which are the projections of 
squares parallel to the ground. The trapezoids can be interpolated linearly in the horizontal direction. 
The vertical direction requires perspective divisions. The divisions can be avoided if a look-up table 
in­dexed by the vertical offset is pre-computed for each possible integer height of the trapezoids. The 
second case can be gener­alized to include the case when the squares are perpendicular to both the ground 
and the viewing plane. If the viewpoints are aligned with a 3D lattice, the result will always fall into 
one of the above two cases, which allows us to use linear interpolation to generate an exact solution. 
 2.3 Compositing Images The key problem with forward mapping is that overlaps and holes may occur in 
the interpolated image. 2.3.1 Overlaps One reason overlaps occur is due to local image contraction. Local 
image contraction occurs when several samples in a local neighborhood of the source image move to the 
same pixel in the interpolated image. A typical example of this case is when our view of a plane moves 
from perpendicular to oblique. Per­spective projection causes the image to contract as the plane moves 
away from the point of view. In the mapping, the sam­ples on the far side of the plane contract while 
the samples on the near side expand. Contraction causes the samples to overlap in the target pixels. 
Multiple layers of pixel depths also will cause the samples to overlap, as in the case of the foreground 
sculpture in Figure 3. Resolving this case is really a hidden surface problem. One way of solving this 
problem is to use the Z-buffer algorithm to determine the frontmost pixel. A more efficient way of deter­mining 
the nearest pixel is presented in the Optimization Sec­tion. 2.3.2 Holes Holes between samples in the 
interpolated image may arise from local image expansion when mapping the source image to the destination 
image. This case is shown in Figure 3 where a source image is viewed from viewpoints rotated to the right. 
The cyan regions indicate holes. Generally, a square pixel in the source image will map to a quadrilateral 
in the destination image. If we interpolate the four corners of the square instead of the pixel s center, 
the holes can be eliminated by filling and filtering the pixels in the destination quadrilateral. A more 
efficient, though less accurate, method to fill the holes is to interpolate the adjacent pixels colors 
or offset vec­tors. The holes are identified by filling the interpolated image with a reserved "background" 
color first. For those pixels which still retain the background color after the source to target mapping, 
new colors are computed by interpolating the colors of adjacent non-background pixels. Alternatively, 
we can in­terpolate the offset vectors of the adjacent pixels. The interpo­lated offset is used to index 
back to the source image to obtain the new sample color. Note that using a distinguished back­ground 
color may not identify all the holes. Some of the holes may be created by a foreground object and are 
filled by a back­ground object behind it (e.g., the holes in the sculpture in the rightmost image in 
Figure 3). This problem is alleviated, though not completely eliminated, when more source images are 
added as described below (e.g. Figure 5d). Holes may also arise from sample locations invisible in each 
of the source images but visible in the interpolated image. The hole region, as shown in Figure 4, is 
the intersection of the umbra regions cast by viewpoints A and B and the visible region from point M. 
The small circle in the hole region is completely missed by the two source images from points A and B. 
One way of solving this problem is to use multiple source images to minimize the umbra region. Figure 
5a shows the holes (cyan pixels) created by rotating one source image. Fig­ure 5b shows that the number 
of holes is significantly less when two sources images are used. The number of holes can be reduced further 
if we place the two source viewpoints closer (Figure 5c). The remaining holes can be filled by interpolating 
the adjacent pixels(Figure 5d). If the images are computer-gen­erated, a ray-tracing type of rendering 
can be used to render only those missing pixels. Penumbra Umbra Hole AM B Fig. 4 Penumbra, umbra and 
hole regions  2.4 Optimization The basic method is made more efficient by the following two steps. 2.4.1 
Block Compression Since adjacent pixels tend to move together in the map­ping, a block compression scheme 
such as a quadtree can be ap­plied to compress the morph map. The compression serves two purposes. First, 
it reduces the size of the morph map. Second, it allows us to interpolate offsets for entire blocks instead 
of pixel-by-pixel. The second aspect greatly accelerates the inter­polation process as the main cost 
in the process is the interpo­lation of the offset vectors. The compression ratio is related to the image 
depth com­plexity and the viewpoint movement. For images with high depth complexity, the compression 
ratio is usually low. The ra­tio is also lower if the viewpoint s movement results in greater pixel depth 
change. Figure 6 shows the quadtree decomposition of the morph map for the image sequence in Figure 3. 
The max­imal offset threshold within a block is one pixel in Figure 6a and two pixels in Figure 6c, which 
means the offset vector co­ordinates within a block do not differ more than one or two pixel units. The 
compression ratio in Figure 6a is 15 to 1 and in Figure 6b is 29 to 1 (i.e., the number of blocks vs. 
the number of pixels). The threshold provides a smooth quality degradation path for increased performance. 
Large threshold factors result in fewer quadtree blocks and, therefore, reduce the interpolation time. 
The performance gain is at the expense of increasing blockiness in the interpolated image. The interpolation 
times in Figure 6b and 6d are accelerated by a factor of 6 and 7 respec­tively. Note that the speedup 
factor does not grow linearly with the compression ratio because the same number of pixels still need 
to be moved. 2.4.2 View-Independent Visible Priority In the basic method, the Z-buffer algorithm is 
used to re­solve visibility. However, as shown in Figure 7, the A-closer­than-B priority established 
in View1 is still valid in View2, since Point A and Point B do not overlap in View2. The priority is 
incorrect in View3 when A and B overlap. As long as the an­ gle . in the figure is less than 90 degrees, 
the A-B priority does not need to be changed when the viewpoint is moved. This ob­servation allows us 
to establish a view-independent visible priority for every source pixel for a viewing range. The pixels 
are ordered from back to front based on their original Z-coordi­nates when the morph maps are created, 
and are subsequently drawn in a back-to-front order in the interpolation process. This ordering of the 
samples, or sample blocks, eliminates the need for interpolating the Z-coordinates of every pixel and 
up­dating a Z-buffer in the interpolation process. View3 Fig. 7 View-independent visible priority Note 
that the priority established here is for image pixels rather than for the underlying objects, unlike 
list-priority algo­rithms for hidden-surface removal[SCHU69]. This method applies to multiple source 
images as well. The source images' pixel Z-coordinates are transformed to a single coordinate system 
for establishing the Z-priority. All the pix­els in the source images are sorted into the same priority 
list. The priority can be assigned to every quadtree pixel block. With static objects and a moving camera, 
pixel offsets are di­rectly related to Z-coordinates. Since the pixels within a block have similar offsets, 
they also have similar Z-coordinates. The Z-coordinates within a block are filtered to determine a Z 
value for the priority sort. The result is a sorted list of pixel blocks valid for the entire range between 
views.  3 IMPLEMENTATIONS The method presented above can be summarized as follows. 3.1 Preprocessing 
The preprocessing stage establishes the correspondence be­tween each pair of source and destination images. 
As mentioned in Section 1, the source images are connected to form a graph structure. Each node of the 
graph contains a source image, its range data and camera parameters (i.e., camera s position, ori­entation). 
For each set of adjacent nodes in the graph, a sorted list of quadtree blocks is created (e.g., a block 
list is created for every triangle in a 2D lattice structure). Each block in the list contains a pointer 
to a pixel block in a source image, the size, the screen coordinates and the offset vectors of the block. 
The block list is created in the following steps: Step 1. Get input data: a source node (image, range 
data and camera parameters), a destination node (only the camera param­eters are needed) and a threshold 
factor for the quadtree decom­position. Step 2. Create a morph map from the source to the destina­tion 
(Section 2.1). Step 3. Decompose the morph map into quadtree blocks and add the blocks to a block list 
(Section 2.4.1). Step 4. Repeat Step 1 to 3 for each directional arc connect­ing the set of nodes. 5. 
Sort the block list from back to front by the blocks Z­coordinates. 3.2 Interactive Interpolation In 
the interactive interpolation stage, the block list corre­sponding to a new viewing location is retrieved. 
The parametric coordinates of the location with respect to the adjacent nodes are used as interpolation 
parameters. An interpolated image for the new location is generated in the following steps: Step 1. Get 
input data: interpolation parameters and a sorted block list. Step 2. Fill the interpolated image with 
a distinguished background color. Step 3. For every block in the list in back-to-front order, compute 
its new location from the offset vectors and the inter­polation parameters. Copy the pixel block from 
the source im­age to its new location in the interpolated image (Section 2.2). Step 4. For every pixel 
in the interpolated image that still retains the background color, compute its color by filtering the 
colors of the adjacent non-background pixels (Section 2.3.2). 3.3 Examples Figure 8 shows a sequence 
of images generated by moving the viewpoint to the right. The images were rendered at 256x256 resolution 
using progressive radiosity [COHE88] from a model created for the Virtual Museum project[MILL92]. Figure 
9 shows two intermediate images created by morph­ing the leftmost and rightmost images. Each image took 
0.17 second to generate (excluding the preprocessing time) on a Macintosh Quadra 950. Note that for the 
interpolation to work properly, the source image cannot be anti-aliased. Anti-aliasing is view-dependent. 
It blends silhouette pixel colors from a particular viewpoint. Since the Z-buffer cannot be anti-aliased 
in the same way, the anti-aliased silhouette pixels may attach to either the fore­ground or the background 
objects depending on the quantiza­tion of the Z-buffer. This problem can be solved by morphing high-resolution 
unfiltered source images and then filtering the interpolated image. The method can be applied to interpolating 
more than two source images. Figure 10 shows a sequence of images interpo­lated from the four source 
images in the corners. The view­points of the source images form a square parallel to the view­ing plane. 
Therefore, as discussed before, linear interpolation is an exact solution to the perspective transformation. 
New im­ages are computed from the nearest three corner images. The barycentric coordinates of the new 
viewpoint are used to inter­polate the three images. Dividing the lattice into simplices minimizes the 
cost of interpolation. APPLICATIONS The morphing method can be used in a wide variety of ap­plications 
which require fast visibility computations of a prede­fined static scene. Simulation and virtual reality 
applications typically require a scene to be displayed interactively from dif­ferent viewpoints. Temporal 
anti-aliasing, or motion blur, can be accelerated by using morph maps to integrate image samples over 
time. The image samples are interpolated from key images using the morphing method. We also present an 
application of morph mapping to compute shadows from area lights using the shadow buffer method [WILL78]. 
The morphing method makes it possible to define a new class of graphic display primitives based on images. 
This approach is also useful in incremental rendering as it provides a way to reuse the pixels computed 
for previous images. 4.1 Virtual Reality Instead of representing a virtual environment as a list of 3D 
geometric entities, the morphing method uses images (environment maps). To perform a walkthrough, the 
images ad­jacent to the viewpoint are interpolated to create the desired view. In addition to supporting 
walkthroughs in virtual environ­ments, the method can be used to create virtual holograms, where the 
display on the screen will change with respect to the user s viewpoint to provide 3D motion parallax. 
One existing approach uses 3D rendering to display the scene from the view­point obtained by a head location 
sensor[DEER92]. Another approach uses a finite set of pre-rendered frames, each corre­sponding to a particular 
viewing location[VENO90]. With the morphing method, only a few key images are required. The in­terpolation 
can generate the in-between frames. Figure 10 shows a sequence of images with vertical and horizontal 
motion parallax. The image-based morphing method is inexpensive compu­tationally and provides a smooth 
quality-speed tradeoff. Al­though the total storage requirement may be large, the amount of data needed 
to compute a frame is relatively small and can be read from secondary storage as needed. This approach 
is very appropriate for CD-ROM based devices because of their large storage capability. As the complexity 
of geometrical models increases, the advantage of image-based approaches will be more significant because 
of their bounded overhead. Another advantage of using the image-based approach is that a real environment 
can be digitized by photographic means. Using a camera to capture the environment usually is much easier 
than modeling it geometrically. Although our method relies on range data to establish the correspondence 
be­tween images, range data should be easier to obtain than the complete 3D geometry of the environment. 
 4.2 Motion Blur If an image in a motion sequence is a sample at an instant of time instead of over a 
time interval, the motion will appear to be jerky and the image is said to be aliased in the temporal 
do­main. One way to perform temporal anti-aliasing is super-sam­pling. The motion is sampled at a higher 
rate in the temporal domain and then the samples are filtered to the displayed rate. Super-sampling requires 
the computation of many more sam­ples. For images which are expensive to render, this technique is very 
inefficient. The morphing method allows additional temporal samples to be created by interpolation. The 
interpolation time is con­stant regardless of the rendering time for each frame. The sam­pling rate is 
determined by the largest offset vector from the morph map in order to perform proper anti-aliasing. 
Figure 11a is a motion blurred image computed from 32 source images for the camera motion in Figure 8. 
The images were first rendered at 512x512 resolution and then filtered down to 256x256 resolu­tion before 
temporal anti-aliasing was performed. The tempo­ral samples were anti-aliased with a box filter. Each 
image took around 5 seconds to render on a high-end workstation with 3D graphics hardware support. Figure 
11b was computed from the same number of images interpolated from three of the source images. Each interpolated 
image took 0.6 second to compute on a Macintosh Quadra950. The only minor visible difference between 
the two images is the top of the inside loop of the foreground sculpture, due to the holes created from 
the interpo­lation as discussed previously. The super-sampling approach requires the sampling rate to 
be determined based on the worst case. For images with fast moving objects and slowly moving backgrounds, 
this method is not very efficient. One way to solve this problem is to seg­ment the images based on object 
movement and use different sampling rates for each segment. For instance, the foreground sculpture in 
this figure needs to be sampled at the highest rate while the wall behind it needs only a few samples. 
In the case of motion caused by viewpoint changes as in this figure, the seg­ments can be sorted in order 
of depth as discussed in Section 2.4.2. Each segment is filtered independently and a temporal coverage 
value for each pixel is kept to indicate the ratio of background samples vs. all samples. The multiple 
segment lay­ers are then composited in front-to-back order with each seg­ment s pixel colors attenuated 
by the coverage value from the previous segment. 4.3 Shadows A very general and efficient way of rendering 
shadows is the shadow buffer algorithm [WILL78]. The algorithm computes a Z-buffer (i.e., shadow map) 
from the point of view of the light source. To compute shadows, a surface point s coordinates are transformed 
to the light source s space and its Z-coordinate is compared to the corresponding Z-coordinate in the 
shadow map. If the point is further away then it is in shadow. The algorithm only works for point light 
sources. To ap­proximate a linear or an area source, many point lights may be needed [SHAP84]. The cost 
of computing the shadows is pro­portional to the number of point sources used. light2 light3 eye light 
source The morphing method can be used to significantly reduce the cost of computing the shadow map for 
each of the point sources. Figure 12 illustrates the process of using the method to compute shadows from 
a linear light source. A shadow map is computed first for each of the two end points of the source (i.e., 
light1 and light2) using the conventional rendering method. A morph map from the viewpoint to each of 
the two end points is also computed to transform the screen coordinates to each point source s coordinate 
space (i.e., map1 and map2). The shadow map for an in-between point (e.g., light3) on the linear source 
is interpolated from the corner shadow maps using the morph­ing method. The same interpolation factor 
is used to interpolate the two morph maps (map1 and map2) to create a morph map from the viewpoint to 
the in-between light source point (map3). The standard shadow buffer algorithm is then used to compute 
shadows for the in-between point source. The process is repeated for all the in-between points at a desired 
interval. The resulting shadow images are composited to create the soft shadow of the linear source. 
This method can be generalized to any area or volume light source. Figure 13 shows the result after 
compositing 100 in-be­tween shadow images generated by randomly distributed points on a rectangular light 
source above the triangle. Four source shadow maps located at the corners of the rectangle were created 
for the interpolation. The shadow maps were rendered at 512x512 resolution and the shadow image resolution 
is 256x256. Percentage closer filtering [REEV87] was used to anti-alias the shadows for each image. Each 
shadow image took 1.5 seconds to compute. Shading for the illuminated pixels was computed by Lambert's 
Law weighted by the projected size of the rectangle source over the pixel. 4.4 Image-Based Primitives 
A 3D object is perceived on a flat display screen through a series of 2D images. As long as we can generate 
the images from any viewpoint, it does not matter if a 3D description of the object is available. The 
morphing method permits any view of an object to be generated by interpolation from some key images. 
Therefore, a new class of primitives based on images can be defined. These image-based primitives are 
particularly useful for defining objects of very high complexity since the interpolation time is independent 
of the object complexity. Figure 14 shows a sequence of images of a rotating teapot generated by the 
morphing method. The middle images were generated by interpolating the two key images at the extreme 
left and right. The key images were rendered with viewpoints rotated 22.5 degrees around the center of 
the teapot. A larger angular increment of the key images may result in holes and distortions as a result 
of the linear interpolation. Figure 15 is the same source images extrapolated to show the pixel blocks 
which compose the teapot. Rendering an object using the morphing method is really not different from 
rendering a complete scene as described pre­viously. The image-based object or scene can be treated as 
a sprite that can be composited with images generated by other means. 4.5 Incremental Rendering Adjacent 
images in an animation sequence usually are highly coherent. Therefore, it s desirable to perform the 
render­ing incrementally. Ideally, the rendering should be limited to only the pixels which are different 
from the previous frame. However, searching for the pixels that change is not always trivial. Some incremental 
rendering approaches which make use of frame-to-frame coherence were presented in [CHEN90], [JEVA92]. 
The morphing method provides a natural way of making use of frame coherence. For an animation sequence 
where the mo­tion of every frame is known in advance, the frames can be ren­dered initially at a coarse 
temporal sampling rate. The remain­ing frames can then be computed by the morphing method. The missing 
samples or view-dependent shading, such as high­lights, of the interpolated frames can be computed by 
additional rendering. If accuracy rather than speed is the main concern, the map-based interpolation 
or extrapolation of pixel coordinates can be replaced by perspective transformation. 5 CONCLUSIONS AND 
FUTURE DIRECTIONS The interactive speed which the image-based display has achieved on modest computing 
platforms has fulfilled our pri­mary goal in pursuing this research. In addition to this primary objective, 
we have demonstrated effective application of the view interpolation approach to computing some of the 
more complex rendering effects. Image-based computer graphics promises to be a productive area of research 
for some time. A number of intriguing research problems suggest themselves: An automatic camera has been 
developed to record an array of images of an object from viewpoints surrounding it [APPL92]. What are 
the prospects for automatic camera loca­tion selection to minimize the number of holes in the interpo­lated 
images? Similarly, what are good algorithmic criteria for dispensing with as many recorded images as 
possible, or select­ing the best subset of images to represent the object? By modeling the 3D transformation 
from one image to the next by a field of straight-line offsets, we introduce an approx­imation analogous 
to polygonization (except in the restricted cases mentioned in Section 2.2). Higher-dimensional, rather 
than linear, interpolation might be expected to better approxi­mate the arcs traversed by objects rotating 
between views. Curved motion blur is another possible benefit of higher-order interpolation. View-dependent 
shading such as specular reflection would extend the useful range of morphing as a display technique. 
One possibility mentioned previously is to define additional maps for specular surfaces, which specify 
normal components or reflection map coordinates. Special-purpose image compression might profit greatly 
from morph-mapping algorithms. The resemblance of the morph maps to motion-compensation vectors commonly 
used in video sequence compression has been mentioned. These vec­tors, used in format conversion to address 
the interlace prob­lem, and in compression to squeeze a little more redundancy out of the signal, also 
find application in optical flow algorithms for tracking objects in the visual field. The redundancy 
removed from the video sequence by motion compensation is limited, as it applies only between successive 
frames. In a morph mapping encoder, objects which appear and disappear repeatedly could be encoded with 
a small set of maps. The decoder, a hybrid of an image warper and a graphics pipeline, would use them 
as "sprites" from a catalog of maps. The representation of objects and surfaces as sets of images and 
maps, possibly pyramidal maps, suggests the application of morph mapping to more general global illumination 
models. The approach of determining visibility to an area light source to compute soft shadows can be 
extended to treating all surfaces as sources of radiosity. For many global illumination prob­lems, a 
few images and morph maps can serve to represent hun­dreds or thousands of computed images. 6. ACKNOWLEDGMENTS 
Thanks to the Virtual Museum team for the museum model and images. Dan Venolia anticipated the use of 
range images as display primitives (without interpolation) in his virtual holog­raphy work. Ken Turkowski 
contributed the teapot images. Ned Greene, Nelson Max and members of the Advanced Technology Computer 
Graphics Group have offered useful ideas and criti­cism. Frank Crow and Apple Computer s continuous support 
of this research is highly appreciated.    REFERENCES [AIRE91] Airey, J., J. Rohlf and F. Brooks. 
Towards Image Re­ alism with Interactive Update Rates in Complex Building Environments. ACM SIGGRAPH 
Special Issue on 1990 Symposium on Interactive 3D Graphics, 41-50. [APPL92] Apple Human Interface Group. 
Object Maker. [exhibit] In Interactive Experience, CHI 92, Monterey CA. [BESL88] Besl, P.J. Active Optical 
Range Imaging Sensors. Machine Vision and Applications Vol. 1, 1988, 127-152. [BEIE92] Beier, T. and 
S. Neely. Feature-Based Image Meta­ morphosis. SIGGRAPH 92 Proceedings, 35-42. [CHEN90] Chen, S. E. Incremental 
Radiosity: An Extension of Progressive Radiosity to an Interactive Image Synthesis System. SIGGRAPH 90 
Proceedings, 135-144. [COHE88] Cohen, M. F., S. E. Chen, J. R. Wallace and D. P. Greenberg. A Progressive 
Refinement Approach to Fast Ra­diosity Image Generation. SIGGRAPH 88 Proceedings, 75­ 84. [DEER92] Deering, 
M. High Resolution Virtual Reality. SIG-GRAPH 92 Proceedings, 195-202, 1992. [GOSH89] Goshtasby, A. Stereo 
Correspondence by Selective Search. Proc. Japan Computer Vision Conf., 1-10, July, 1989. [GREE86] Greene, 
N. Environment Mapping and Other Appli­cations of World Projections. IEEE CG&#38;A, Vol. 6, No. 11, November, 
1986. [GREE93] Greene, N. and M. Kass. Approximating Visibility with Environment Maps. Technical Report 
41, 1993, Apple Computer, Inc. [HOFM88] Hofman, G. R. The Calculus of the Non-Exact Per­spective Projection. 
Eurographics 88 Proceedings, 429-442 [JEVA92] Jevans, D. Object Space Temporal Coherence for Ray Tracing. 
Graphics Interface 92 Proceedings, 176-183, 1992. [LIPP80] Lippman, A. Movie Maps: An Application of 
the Op­tical Videodisc to Computer Graphics. SIGGRAPH 80 Pro­ceedings, 32-43. [MILL92] Miller, G., E. 
Hoffert, S. E. Chen, E. Patterson, D. Blacketter, S. Rubin, S. A. Applin, D. Yim and J. Hanan. The Virtual 
Museum: Interactive 3D Navigation of a Multi­media Database. The Journal of Visualization and Computer 
Animation, Vol. 3, No. 3, 183-198, 1992. [MILL93] Miller, G.and S. E. Chen. Real-Time Display of Sur­roundings 
Using Environment Maps. Technical Report 42, 1993, Apple Computer, Inc. [MPEG90] MPEG Video Committee 
Draft, December, 1990. [NAGE86] Nagel, H.-H. Image Sequences - Ten (octal) Years from Phenomenology to 
a Theoretical Foundation. Proc. 8th ICPR, Paris 1986, 1174-1185. [POGG91] Poggio, T. and R. Brunelli. 
A Novel Approach to Graphics. MIT A.I. Memo No. 1354, C.B.I.P. Paper No. 71, February, 1992. [REEV87] 
Reeves, W. T., D. H. Salesin and R. L. Cook. Render­ing Antialiased Shadows with Depth Maps. SIGGRAPH 
87 Proceedings, 283-291. [SCHU69] Schumacker, R., B. Brand, M. Gilliland, and W. Sharp. Study for Applying 
Computer-Generated Images to Visual Simulation, Technical Report AFHRL-TR-69-14, NTIS AD700375, U.S. 
Air Force Human Resources Lab., Air Force Systems Command, Brooks AFB, TX, September, 1969. [SHAP84] 
Shapiro, B. L., N. I. Badler. Generating Soft Shad­ows with a Depth Buffer Algorithm. IEEE CG&#38;A, 
Vol. 4, No. 10, 5-38, 1984. [TELL92] Teller, S and C. Sequin. Visibility Preprocessing for Interactive 
Walkthroughs. SIGGRAPH 91 Proceedings, pp.61-69, 1991. [VENO90] Venolia, D. and L. Williams. Virtual 
Integral Holog­raphy. Proc. SPIE-Extracting Meaning from Complex Data: Processing, Display, Interaction 
(Santa Clara, CA, Febru­ary, 1990), 99-105. [WILL78] Williams, L. Casting Curved Shadows on Curved Sur­faces. 
SIGGRAPH 78 Proceedings, 270-274. [WILL90] Williams, L. 3D Paint. ACM SIGGRAPH Special Is­sue on 1990 
Symposium on Interactive 3D Graphics, 225­ 233. [WOLB89] Wolberg, G. and T. E. Boult. Separable Image 
Warp­ing with Spatial Lookup Tables. SIGGRAPH 89 Proceed­ings, 369-377. [WOLF83] Wolf, P. R. Elements 
of Photogrammetry, McGraw-Hill, New York, 1983.    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166154</article_id>
		<sort_key>289</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>37</seq_no>
		<title><![CDATA[Spatial anti-aliasing for animation sequences with spatio-temporal filtering]]></title>
		<page_from>289</page_from>
		<page_to>296</page_to>
		<doi_number>10.1145/166117.166154</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166154</url>
		<keywords>
			<kw><![CDATA[anti-aliasing]]></kw>
			<kw><![CDATA[computer animation]]></kw>
			<kw><![CDATA[spatio-temporal filtering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.1</cat_node>
				<descriptor>Computations on matrices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Antialiasing**</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003719</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on matrices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010386</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Antialiasing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Experimentation</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP36042753</person_id>
				<author_profile_id><![CDATA[81100495695]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mikio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shinya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>808585</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Loren Carpenter, 'The A-buffer, An Antialiased Hidden Surface Method,' Computer Graphics 18, No.3, pp.103- 108, 1984.]]></ref_text>
				<ref_id>CARPENTER</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808586</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Edwin Catmull, 'An Analytic Visible Surface Algorithm for Independent Pixel Processing,' Computer Graphics 18, No.3, pp.109-115, 1984.]]></ref_text>
				<ref_id>CATMULL84</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808590</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[R. L. Cook, T. Porter, L. Carpenter, 'Distributed Ray Tracing,' Computer Graphics 18, No.3, pp.137-145, 1984.]]></ref_text>
				<ref_id>COOK84</ref_id>
			</ref>
			<ref>
				<ref_obj_id>8927</ref_obj_id>
				<ref_obj_pid>7529</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[R. L. Cook, 'Stochastic Sampling in Computer Graphics,'ACM Trans. Graphics, 5, No.l, pp.51-57, 1986.]]></ref_text>
				<ref_id>COOK86</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325182</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[M. A. Dipp6, 'Anti-aliasing through Stochastic Sampling,' Computer Graphics 19, No.3, pp.69-78, 1985.]]></ref_text>
				<ref_id>DIPPE</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[James D. Foley, Andies van Dam, Steven K. Feiner, John F. Hughes, 'Computer Graphics Principal and Practice,' Addison-Wesley, 1990.]]></ref_text>
				<ref_id>FOLEY</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325184</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Charles W. Grant, 'Integrated Analytic Spatial and Temporal Anti-Aliasing for Polyhedra in 4-Space,' Computer Graphics 19, No.3, pp.79-84, 1985.]]></ref_text>
				<ref_id>GRANT</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97913</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[P. Haeberli, K. Akeley, 'The Accumulation Buffer: Hardware Support for High-Quality Rendering,' Computer Graphics, 24, No.4, pp.309-318, 1990.]]></ref_text>
				<ref_id>HAEBERLI</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808588</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[P. S. Heckbert, P. Hanrahan, 'Beam Tracing Polygonal Objects,' Computer Graphics, 18, No.3, pp.119-128, 1984.]]></ref_text>
				<ref_id>HECKBERT</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325179</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Mark E. Lee, Richard A. Redner, and Samuel P. Uselton, 'Statistically Optimized Sampling for Distributed Ray Tracing,' Computer Graphics 19, No.3, pp.61-67, 1985.]]></ref_text>
				<ref_id>LEE</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122736</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[D. Mitchell, 'Spectrally Optimal Sampling for Distributed Ray Tracing,' Computer Graphics 25, No.4, pp.157- 164, 1991.]]></ref_text>
				<ref_id>MITCHELL</ref_id>
			</ref>
			<ref>
				<ref_obj_id>576199</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[A. N. Netravali and B. G. Haskell, 'Digital Pictures - Representation and Compression,' Prenum Press, 1988.]]></ref_text>
				<ref_id>NETRA</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97901</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Takafumi Saito and Toki Takahashi, 'Comprehensible Rendering of 3-D Shapes,' Computer Graphics 24, No.4, pp. 197- 206, 1990.]]></ref_text>
				<ref_id>SAITO</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37408</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[M. Shinya, T. Takahashi, and S. Naito, 'Principles and Applications of Pencil Tracing,' Computer Graphics,21, No.4, pp. 45-54, 1987.]]></ref_text>
				<ref_id>SHINYA</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Spatial Anti-aliasing for Animation Sequences with Spatio-temporal Filtering Mikio Shinya NTT Human 
Interface Laboratories 3-9-11 Midori-cho, Musashino-shi Tokyo 180, Japan email: shinya@nttarm.ntt.jp 
tel: +81 422 59 2648 Abstract Anti-aliasing is generally an expensive process because it requires super-sampling 
or sophisticated rendering. This paper presents a new type of anti-aliasing filter for animation sequences, 
the pixel­tracing filter, that does not require any additional sample nor addi­tional calculation in 
the rendering phase. The filter uses animation information to calculate correlation among the images, 
and sub-pixel information is extracted from the sequence based on the correlation. Theoretical studies 
prove that the filter becomes an ideal anti­aliasing filter when the filter size is infinite. The algorithm 
is simple image processing implemented as post­filtering. The computational cost is independent of the 
complexity of the scene. Experiments demonstrate the efficiency of the filter. Almost complete anti-aliasing 
was achieved at the rate of about 30 seconds per frame for very complex scenes at a resolution of 256x256 
pixels. The pixel tracing filter provides effective anti­aliasing for animation sequences at a very modest 
computational cost. CR Categories and Subject Descriptors: I.3.3 [Computer Graph­ics]: Picture/Image 
Generation; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Additional Keywords and 
Phrases: Anti-aliasing, Spatio-tempo­ral filtering, Computer Animation 1 Introduction Aliasing artifacts 
have been troublesome in the field of graphics for a long time. These problems are particularly bad in 
animation sequences, since flickering thin objects and traveling jaggies are very noticeable. Permission 
to copy without fee all or part of this material is granted Permission to copy without fee all or part 
of this material is granted provided that the copies are not made or distributed for direct provided 
that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice 
and the title of the commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy permission 
of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or 
specific permission. otherwise, or to republish, requires a fee and/or specific permission. &#38;#169;1993 
ACM-0-89791-601-8/93/008 $1.50 &#38;#169;1993 ACM-0-89791-601-8/93/008/0015 $1.50 For viewers in general, 
these spatio-temporal artifacts are more noticeable than the purely spatial ones in still images. To 
detect spatial aliasing, the true images (e.g., continuous lines or checker board patterns) should be 
inferred from the sampled image by intelligent, high-level visual processing. On the other hand, spatio­temporal 
aliasing can be detected by low-level vision processes (e.g., flicker detection and optical flow segmentation) 
without deep knowledge. This may seem rather negative, but it also implies a positive aspect: there may 
be easier ways to detect and remove aliasing in animation sequences. Usually, there is strong correlation 
among the successive frames of motion pictures. This correlation allows efficient image compres­sion 
in video codecs (coder/decoder) [NETRA]. This motivates us to extract sub-pixel information from image 
sequences, which could reduce aliasing artifacts. This paper mathematically analyzes spatio-temporal 
character­istics of motion image sequences, and clarifies the useful features of their spectrum. Based 
on the analysis, a new type of anti-aliasing algorithm is proposed. In the algorithm, the image sequences 
are filtered with a linear shift-variant spatio-temporal filter called the pixel-tracing filter. Through 
the image sequence, the filtering pro­cess traces the pixels corresponding to the same object point, 
and the weighted sum of their colors is calculated. Theoretical studies prove that the filter acts as 
an ideal anti-aliasing filter when the filter size is infinite. Unlike most anti-aliasing algorithms, 
this algorithm is achieved by post-filtering. The advantages are: fast execution independent of the 
scene complexity (e.g., number of polygons),  simplicity of implementation,  no dependence on the rendering 
process.  Experiments showed that the algorithm was efficient in terms of computational cost and provided 
effective image improvement.  2 Related Work There are too many studies of anti-aliasing to review exhaustively, 
so only spatio-temporal approaches are briefly mentioned here. There are two major methods of spatio-temporal 
anti-aliasing: super­sampling and analytic calculation. In the super-sampling scheme, distributed ray 
tracing [COOK84] and alpha-blending [HAEBERLI] with stochastic sampling [DIPPE, COOK86] are the most 
successful and commonly used. Their advantages are simplicity and generality, but the disadvantage is 
a computational cost that is proportional to the rate of super-sampling. Although adaptive sampling [LEE] 
and optimal sampling patterns [MITCHELL] have been investigated, image improvement by super-sampling 
is generally computationally expensive. The analytic approach, on the other hand, is attractive because 
an exact solution can be calculated in relatively modest computation time. However, algorithms usually 
involve rather com­plicated processes, such as three-dimensional scan-conversion [GRANT] and analytic 
filtering of polygons [CATMULL84], and are only applicable to particular object primitives (typically 
poly­gons). In short, both approaches directly calculate sub-pixel or sub­frame information and then 
apply local filters. Our approach differs from the above methods in three ways. First, our approach does 
not require any additional sample or additional calculation in the rendering phase. Second, it evaluates 
sub-pixel information from the image sequences themselves, taking the advan­tage of global spatio-temporal 
correlation. Third, our method uses a temporally global filter to removes spatial aliasing while other 
methods attempt to produce motion-blur by local temporal filtering.  3 Fourier Analysis Temporal variation 
in animation sequences is usually due to the motion of the camera and objects. In this section, we mathematically 
analyze the spatio-temporal spectra of image sequences of moving objects. The velocity on the image plane 
is first assumed to be constant in time and space; analyses with spatial and temporal variation follow. 
The analyses provide an ideal anti-aliasing filter with infinite integral under certain conditions. Throughout 
this section, a one-dimensional space (image) is assumed for simplicity, but extension to two-dimensional 
images is mathematically straight­forward. 3.1 Preparation Let x be the image coordinate in pixels and 
t be the time in frames. Let a real function f 0(x ) be the image at t =t0 , and f ( x;t ) be the image 
sequence. The spatial Fourier transform of f is defined by F0( .) =.f0(x )exp(..x)dx, F0(. ; x) = f (x;t 
)exp(..x)dx, . where . denotes the spatial angular frequency (rad/pixel), and . is the imaginary unit, 
.2 =-1 . Similarly, the temporal Fourier trans­form is defined by Figure 1: Aliasing in the Fourier 
Domain. . F(. , .) = F(. ;t )exp(..t) dt where . is the temporal angular frequency in rad/frame. The 
sampled image sequence fs (x, t ) is represented by fs (x; t )= f ( x;t ).d (x -2pk / .)d (t -2pl / O), 
k ,l where .and Oare the sampling frequencies in space and time. When one point per pixel per frame is 
sampled, .=2 p , and O=2p . The Fourier transform of fs is F s (. ,. ) =.F n , m (. ,. ), (1) n, m where 
F n ,m (. ,. ) = F (.+n.,.+mO). Equation 1 indicates that replicas of F appear, centered at the grid 
points (-n., -mO) , as illustrated in Figure 1. When F(. , .) .0 outside the Nyquist frequencies (±. 
/2,±O/ 2) , some replicas intrude on the reconstruction range, causing aliasing artifacts. In other words, 
anti-aliasing can be achieved if replicas Fn ,m can be filtered out. Therefore, anti-aliasing can be 
regarded as a process which calculates filtered images from the sampled images, and consequently, our 
objective is to find some mapping .f0( x)w( x0 -x )dxfs for any x0 . Here, w(x) denotes some desirable 
spatial anti-aliasing filter. The notation defined here is listed in Table 1. An introduction to sampling 
theory and aliasing can be found in [FOLEY]. Table 1: Symbols and notation x position on the image (pixel) 
t time (frame) . spatial angular frequency (rad/pixel) . temporal angular frequency (rad/frame) f 0(x 
) image at t = t0 f ( x;t ) image at t fs (x; t ) sampled image sequence F0( .) the spatial spectrum 
of f 0  F(. , .) the spatio-temporal spectrum of f Fs (. ,. ) The spatio-temporal spectrum of fs . spatial 
sampling frequency O temporal sampling frequency Fn ,m the replica of F centered at (-n., -mO) w (x 
) spatial anti-aliasing filter g(x, t) shift variant spatio-temporal filter G(., . ) the spatio-temporal 
spectrum of w  3.2 Constant Velocity Motion First, let us consider the simplest motion, constant velocity 
motion. In this case, the image at t can be represented by f ( x;t ) = f 0(x + v0(t 0 - t)), (2) where 
v0 is the velocity of the pattern. Its spatio-temporal spectrum is F(. , .) = exp(..t )dt f 0(x +v0(t0 
-t ))exp(..t) dx .. = F0(. )exp(..v0(t -t0 ))exp(..t)dt . (3) = 2p F0(. )exp(-..v0 t0)d (v0 . +. ), where 
d is Dirac s delta function and we used the equality .exp(.uv) dv =2 pd (u). Equation 3 clarifies a 
very important fact: the spectrum F can be separated from the replicas even though the spatial spectrum 
F0( .) ranges beyond the Nyquist frequency. Figure 2 illustrates this situation. The replicas can be 
filtered out as shown in the figure if velocity v0 is known. Fortunately, the velocity can be easily 
calcu­lated from animation data in graphics applications. Thus, an ideal anti-aliasing filter in this 
case looks like (4) Gv(. ,. )= 2pd (v0. +. ). (Strictly speaking, the filter Gv involves a convergence 
problem because infinite animation sequences are assumed here. This will be solved in the next section.) 
 The linear filtering in the Fourier domain GvFs is equivalent to convolution in real time-space, that 
is, fs (x, t)d (( x0 -x )- (t0 -t )v0) dxdt. (5) .. This motivates us to study more general cases.  
Figure 2: Spatio-temporal spectrum of constant velocity motion.  3.3 General Motion Let us consider 
general motion. When the image point x0 at t0 moves to x1 at t1 , we denote the motion by x1 =.(t1; x0,t0 
). (6) For example, the flow . v for constant motion is: . v (t; x0,t0) = x0 +v(t -t0). Note that the 
reciprocity generally holds from the definition x =. (t; .(t0; x,t ),t 0). To avoid convergence problems, 
a finite animation sequence should be considered. With the flow . the sequence can be described as: .f 
0(.(t0; x, t )) if T .[-T /2,T /2] f ( x,t ) =. . 0 otherwise, where T is the length of the animation. 
The sampled image sequence and its spectrum are represented by fs (x, t ) = f ( x,t ) .d (x - 2 pk / 
.)d (t - 2pl / O), k ,l T /28 Fs (.,. ) = dt f 0(. (t0;x ,t)) ..-T /2 .-8 n, m exp(.(.+ n.)x +. (.+ mO) 
t)dx . = Fn , m. n,m Next, let us consider the anti-aliasing filter g. Filtering for constant motion, 
Eq. 5, can be rewritten as fsd (x0 -.v (t 0; x,t ))dxdt. ..(x, t)By analogy, we set our filter kernel 
g as g(x, t) = (1/ T )w (x0 -.(t0;x, t))(../ .x )t0,t = (1/ T )w (x0 -.(t0; x,t )) D. (t 0; x,t ) (7) 
for space-variant filtering at (x0, t0) : h (x0,t0) = fs ( x, t)g(x ,t)dxdt. (8) .. Here, w (x ) represents 
some appropriate anti-aliasing filter, such as a sinc-function, Gauss function, box function, and so 
on. The factor 1/T is the normalization constant, and D.=( .. / . x ) compensates for image magnification 
variation due to spatially non-uniform motion. Now, we prove that the filtering defined by Eq. 8 becomes 
an ideal anti-aliasing filter in the limit that T .8. From the Parseval Identity, Eq. 8 can be rewritten 
as (. ,.) h(x0,t0) = (1/2p)2 Fs G *( ., . )d.d. .. (. ,. ) = (1/2p )2 Fn, mG *( ., .)d. d. ... . n 
,m = hn ,m , n ,m where G * denotes the complex conjugate of G . The function G is the spatio-temporal 
spectrum of g, calculated by G (., . ) = (1/ T) w (x0 -. (t 0; x,t ))(.. / . ) .. x exp(.(. x +.t)) dtdx 
= (1/T ) w (x0 - u) .. exp( .. (t ; x, t0). )exp(..t)dudt, where u =. (t0;x ,t) . Then, the integral 
hn, m can be evaluated as hn, m = 1/(2p )2 (1/T ) T /2exp(.( .+mO)t1)dt1 .-T/2 . f 0(. (t 0; x1, t1))exp(. 
(.+ n.) x1)dx1 w( x0 -u)exp(-.. (t2;u,t0). ) .. exp(-..t2)dudt2 d. d. .. .T /2 = (1/ T ) exp(.mOt1) dt1 
-T /2 . f 0(. (t 0; x1, t1 ))exp(.n.x1)dx1 w( x0 -u)d (t1 - t2)d ( x1 -. (t 2;u,t0 )) dudt2 .. = w( 
x0 -u) f 0(u)du . . T /2exp(.n..(t1;u,t0 )exp(.mOt1)dt1/ T , -T/2 where we used the reciprocity . (t0;. 
(t1;u,t0 ),t1) =u . Conse­quently, lim hn,m = w (x0 - y) f 0( u)du( lim Kn (mO;u)/ T ), T .8 . T .8 where 
Kn (.; u) is the Fourier transform of the function kn , kn(t ;u) = exp(.n.. (t ;u, t0 )). Obviously, 
h0,0 =.w (x0 - u) f0( u)du. On the other hand, when Kn is not singular at .=mO , the aliasing pattern 
tends to 0, as lim hn,m = 0. T .8 This completes the proof. Note that Kn (mO, u) can be singular when, 
for example, motion is periodic with a frequency of (mO / n ) , or constant motion with a velocity of 
(mO / n.). 3.4 Discrete Filtering The filtering Eq. 8 can also be represented in a discrete form. By 
setting .=2 p and O=2p (1 sample/pixel/frame sampling), we have h(x0,t0) = f ( x; t)g (x ,t)dxdt s .. 
T /2X /2= (1/ T ) dt f ( x;t )g(x, t ) .. (9) -T/2 -X /2 .d( x - k)d(t -l)dtdx k,l X /2 T /2 = (1/ T) 
f (k;l )w(x0 -.( t0; k, l)) .. k =- X /2 l =-T /2 D. (t0; k,l) for T-frame image sequences at the X 
pixel image resolution. Since Eq. 9 is a finite weighted sum of the sampled images, it can be directly 
computed. The magnification factor D.=( .. / . x ) compensates for image distortion due to non-uniformity 
of motion flow. For spatially uniform motion (more generally, incompressible flow), D.=1. Furthermore, 
since D. (t0;t, x). 1 as t .t0 , we can assume D..1, when the filter size T is small. If non-uniformity 
is not negligible, we have to evaluate D. point by point. Analytic formulae for D. are given in the Appendix. 
For practical implementation, we slightly modify the filtering equation Eq. 9. By assuming local uniformity 
of motion flows, we have h(x0,t0) = (1/ T ).f (k; l)w (.( l;x 0,t 0) -k ), (10) k,l where we used the 
uniformity x - y =. (t; x,t ')-.(t;y ,t'). The advantage of Eq. 10 over Eq. 9 is that only one flow . 
(l; x0,t0) should be traced for (x0, t0) rather than all flows . (t0;l, k). The normalization factor 
(1/T) relies on T /2 lim (1/ T) w( .(l; x0, t0) -k ) =1, T .8.. l =-T /2 k and would cause a normalization 
problem for finite T. Thus, it is better to adopt explicit normalization such as h(x0,t0) =.f (k;l )w 
(. (l ;x0,t0) -k)/.w(. (l; x0,t0)-k). k,lk, l (11)  4 Algorithm This section shows a simple algorithm 
for applying the anti-aliasing filter. When only one velocity field occupies the image, the only problem 
is to calculate the flow . (t; x0,t0 ) . However, when more than two velocity fields overlap, the filter 
should be separately applied because the theories rely on the uniqueness of the field. This happens when 
the projections of differently moving objects overlap (Figure 3). Thus, the keys to the implementation 
are how to evaluate the motion flow .and how to separate fields of different velocity. To deal with multiple 
flows, we adopt the filtering equation Eq. 11, assuming local uniformity of flows. Data From animation 
models, we receive animation data for the sequence, such as transformation of objects and camera parameters. 
From the rendering process, RGB values, z-values, and object-id values are provided for each pixel at 
each frame, for example, in the form of G-buffers [SAITO]. Here, the object-id s are only used to identify 
object motion, and can be omitted for walk-through scenes. Let us denote these values for the pixel (kx 
,ky ) at the frame l by rgb[kx ][ky ][ l], z[ kx ][ky][l], and id [kx][ky ][l ] , respectively. As the 
work space to capture multiple flows, we have a list structure of rgb, z, and afor each pixel, denoted 
by rgb flow [ix ][iy], zflow [ix][iy] , and a flow[ix ][ iy]. Pixel Tracing We now treat two-dimensional 
images. Let us denote two-dimensional vectors and their x,y-components by using the arrow and suffix 
notation, such as k =( kx, ky ). The motion flow, . (l ;k0,l0) =( .x, .y ) , corresponding to the sample 
point k 0 =(k0x , k0 y ) at t =l0 , can be easily calculated from the animation information, the object-id, 
A, and the z-value (Figure 3). Let the transformation from the object coordinate of Object A to the screen 
space at t be TA (t) . Then, the corresponding object point pA =(xA ,yA ,zA ,wA) is given by pA =(k0 
x ,k0 y, z[k0 x ][k 0y ][ l0 ],1)TA -1(l0). At t =l , the object point pA is projected by TA (l) , and 
thus, the flow . and the corresponding depth . (l; k0,l0 ) can be calculated as . (l ;k0;l0) = ( x / 
w, y / w ) . (l ;k0,l0) = z / w (x, y, z, w) = pAT A (l0 ) (12) = (k0x ,k0 y, z[k0x ][k0 y ][l 0],1) 
- TA 1(l0)TA( l) Figure 3: Pixel-tracing. When the sample point misses an object, the filtering can 
be applied in the following way. In the example in the Figure 3, Object B fails to hit the sample point 
at k 0, t =l 0 , but pixel tracing from k 2 =(k2x , k2 y),t =l2 reveals that Object B should exist in 
the reconstruction area of k 0, t =l 0 because the traced point . (l0;k2, l2) lies in the area. Therefore, 
we trace the flow for k 0 by   . miss (l;k 0,l0) =. (l; k2,l2)+. where .=k 0 -. (l0; k2,l2) Separation 
and Summation To separate different velocity fields, we adopt a simple rule, that is, when the difference 
between two flows is smaller than some threshold, we regard them as the same flow. This can be described 
as follows. If both of the inequalities, , (14) k 0 -. (l0;k1, l1) <dth and (15) k 1 -. (l1;k0, l0) <dth 
, hold, the two sampled data are judged to belong to the same flow. Here, . denotes some norm on the 
image plane. The threshold d th can be determined, for example, according to the diameter of the support 
of the filter kernel w (x ). In the example in Figure 3, Inequality 15 is not true, so the two samples 
are processed as different flows. Note that the projection points of the same object do not necessarily 
belong to the same flow because of perspective. With this criteria function, same_flow(), which returns 
1 when two samples are judged as being in the same flow and 0 otherwise, the actual filtering for each 
velocity flow becomes rgb flow = .. same _ flow(k ,l ;k0,l0)w( . (l; k0,l 0)- k ) l rgb [kx ][ ky ][l]/ 
. same _ flow()w() (16) a flow = same_ flow()w ()/ w (), .. zflow = min(z flow ,. (l ;k 0,l 0)) Here, 
a represents the coverage of this flow. Note that the summa­tion with k can be calculated only in a neighborhood 
of the flow . when the filter w (x ) is compactly supported.  At each pixel, we store the calculated 
RGB values, a -values, and z­values for all flows as a list, like the A-buffer structure [CARPEN-TER]. 
After the filtering, we sort the list with respect to the z-values at each pixel, and the final RGB values 
are determined by simplea ­blending in the order of the sorted list, from near to far, = rgb final a 
flow1rgb flow1 + (1-a flow1)a flow2 rgbflow2 + (17) Procedure The procedure for filtering the frame l0 
can be summa­rized in the following way. 1) For each sample, k 0 , at frame l - l0 , do the following 
for all frames l within the filter. i) Calculate . (l ;k0,l0) according to Eq. 12. ii) Calculate the 
weighted sum according to Eq. 16. 2) For l . l0 within the filter, do the following for all the samples, 
 k . i) Calculate the flow . (l0;k ,l ) according to Eq. 12. ii) For all samples k0 ' at l0 such that 
w (k0 ' -. ( l0;k ,l )). 0, do the following. a) If k ,l belongs to any flow listed for k0 ' ,l0 , skip 
b) and c).  b) Calculate the flow . miss (l ';k0 ' ,l 0) according to Eq. 13, and calculate the weighted 
sum according to Eq. 16. c) Append the result to the list of k0 ' ,l0. 3) For each pixel at l0 , sort 
the resulting list with respect to the z­values and apply alpha-blending according to Eq. 17. As this 
filter involves a pixel tracing process, we call it the pixel­tracing filter. 5 Experiments and Discussion 
The first experiment shows the influence of filter size, T The test pattern is an almost horizontal (1 
degree from the horizon) thin rectangle with the width of 1/8 pixel, constantly moving in the vertical 
direction at a speed of 0.22 = 11/50 pixel/frame. Note that limT .8 (h50,11 / T ). 0. Figures 4-a, -b, 
and -c show the original image, and the results of applying filters of various sizes. As the filter kernel 
w(x), we used the box function with one pixel area. Figures 4-d and -e show the analytic solution and 
the root-mean-square error of the filtered results with respect to the filter size. As shown in the figure, 
effective anti-aliasing was achieved with a filter size 128. The remaining error comes from the replica 
F50,11 The second experiment shows an example of non-uniformly accelerated motion. The test pattern is 
rotating radial thin rectangles. As shown in Figure 5, aliasing was mostly removed with the 32­frame 
filter. The final experiment shows application to a more practical image sequence taken from a walk-through 
scene in Také Tera. In the sequence, only the camera moves and everything else is fixed in space. The 
original images were synthesized by using the GL library on the IRIS workstations at a resolution of 
256x256. The scene consists of about 4M polygons. Figure 6 demonstrates the efficiency of the algorithm, 
where the severe aliasing artifacts seen in the original image were largely removed. The filter size 
was 16 frames. The execution time is about 30 CPU seconds per frame on an IRIS Crimson R4000-50 at 256x256 
resolution. The computation cost is directly proportional to nx × ny × nz , where nx × ny is the image 
resolution, and nz is the filter size. Considering that frame-by­frame recording onto a VCR takes about 
thirty seconds per frame, this powerful anti-aliasing is almost free! Furthermore, as the filter­ing 
is simple image processing with pixel-level independence, it might be possible to design parallel hardware 
to execute it in real­time, which would be attractive for visual simulators and virtual reality applications. 
Future work includes application to reflected/refracted images, and coupling with stochastic sampling 
techniques. The algorithm relies on transformation between the screen space and the object space. Although 
conventional ray tracers cannot provide the trans­formation, the beam tracing/pencil tracing approach 
[HECKBERT, SHINYA] can calculate it in the form of system matrices and thus may be applicable to the 
filtering. Since stochastic sampling techniques are powerful tools for anti-aliasing, it is an attractive 
idea to combine the two approaches. If we jitter the sample point of each pixel at each frame, the pixel­tracing 
filter acts exactly as a purely spatial filter for objects that are steady on the image plane. This means 
that spatial stochastic super­sampling can be performed by the pixel-tracing filter with only one point 
per pixel per frame sampling. This could also reduce the problems with constant velocity motion in the 
case of n .v0 = mO , which we observed in Figure 4. 6 Conclusion A new type of efficient anti-aliasing 
filter, the pixel-tracing filter, was proposed for animation sequences. The filter sums sub-pixel information 
using the correlation among images calculated from animation information. Theoretical studies prove the 
ability of the filter, and experimental results demonstrate the efficiency. The algorithm is simple image 
processing implemented as post-filtering. The computational complexity is of constant order with regard 
to the complexity of scenes (e.g., number of polygons). With the pixel-tracing filter, effective anti-aliasing 
can be completed for animation sequences with a very modest computational cost.  Acknowledgments The 
author would like to thank the Siggraph reviewers, whose comments greatly contributed to improving the 
theoretical part. He also wishes to thank Takahiko Kamae, Rikuo Takano, and Kazuyoshi Tateishi for their 
administrative support and encouragement, Atsushi Kajiyama for his technical support, and Toki Takahashi, 
Taka Saito, and Toshi Tanaka for helpful discussion. References [CARPENTER] Loren Carpenter, The A-buffer, 
An Antialiased Hidden Surface Method, Computer Graphics 18, No.3, pp.103­108, 1984. [CATMULL84] Edwin 
Catmull, An Analytic Visible Surface Al­gorithm for Independent Pixel Processing, Computer Graphics 18, 
No.3, pp.109-115, 1984. [COOK84] R. L. Cook, T. Porter, L. Carpenter, Distributed Ray Tracing, Computer 
Graphics 18, No.3, pp.137-145, 1984. [COOK86] R. L. Cook, Stochastic Sampling in Computer Graph­ ics, 
ACM Trans. Graphics, 5, No.1, pp.51-57, 1986. [DIPPE] M. A. Dippé, Anti-aliasing through Stochastic Sampling, 
Computer Graphics 19, No.3, pp.69-78, 1985. [FOLEY] James D. Foley, Andies van Dam, Steven K. Feiner, 
John F. Hughes, Computer Graphics Principal and Practice, Addison-Wesley, 1990. [GRANT] Charles W. Grant, 
Integrated Analytic Spatial and Tem­poral Anti-Aliasing for Polyhedra in 4-Space, Computer Graph­ics 
19, No.3, pp.79-84, 1985. [HAEBERLI] P. Haeberli, K. Akeley, The Accumulation Buffer: Hardware Support 
for High-Quality Rendering, Computer Graphics, 24, No.4, pp.309-318, 1990. [HECKBERT] P. S. Heckbert, 
P. Hanrahan, Beam Tracing Polygo­nal Objects, Computer Graphics, 18, No.3, pp.119-128, 1984. [LEE] Mark 
E. Lee, Richard A. Redner, and Samuel P. Uselton, Statistically Optimized Sampling for Distributed Ray 
Trac­ing, Computer Graphics 19, No.3, pp.61-67, 1985. [MITCHELL] D. Mitchell, Spectrally Optimal Sampling 
for Dis­tributed Ray Tracing, Computer Graphics 25, No.4, pp.157­164, 1991. [NETRA] A. N. Netravali and 
B. G. Haskell, Digital Pictures -Representation and Compression, Prenum Press, 1988. [SAITO] Takafumi 
Saito and Toki Takahashi, Comprehensible Rendering of 3-D Shapes, Computer Graphics 24, No.4, pp.197­206, 
1990. [SHINYA] M. Shinya, T. Takahashi, and S. Naito, Principles and Applications of Pencil Tracing, 
Computer Graphics, 21, No.4, pp. 45-54, 1987. Appendix: Calculation of D. Here, we derive D. in Eq. 9 
for two-dimensional images. We assume motion flows . (t ;x0, y0,t 0) represented by Eq. 12. In the two­dimensional 
case, D. becomes the Jacobian of . , (.. / .x0)(../ .y0) xy0 xx 0 D.= (..y / .x0) y0(..y / .y0)x 0 = 
(..x / .x 0)y 0( ..y / .y0) x0 -(.. x / .y0)x0(..y / .x0) y0. By setting the translation matrix TA -1(t0)TA 
(t) ={ } , tijthe partial deviation (.. x / . x0) y0 , etc., can be calculated as (.. x / .x0) y0 = (1/ 
w)/(.x / .x0) y0 -( x / w 2 )(.w / .x 0) y0 = (1/ w)(t11 -( nx / nz )t31) - ( x / w 2) (t14 -( nx / nz 
)t 34), where n = ( nx, ny ,nz ) is the normal vector of the object surface at pA , and we used (.x / 
.x0)y 0 = (. x / .x 0)y0,z 0 + (. z0/ .x0) y0(. x / . z0)x0,y0 = t11 - (nx / nz )t 31 and so on. Figure 
5: Rotating thin rectangles.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166155</article_id>
		<sort_key>297</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>38</seq_no>
		<title><![CDATA[Motion compensated compression of computer animation frames]]></title>
		<page_from>297</page_from>
		<page_to>304</page_to>
		<doi_number>10.1145/166117.166155</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166155</url>
		<keywords>
			<kw><![CDATA[compression]]></kw>
			<kw><![CDATA[computer animation]]></kw>
			<kw><![CDATA[computer graphics]]></kw>
			<kw><![CDATA[motion prediction]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.2</cat_node>
				<descriptor>Exact coding**</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P32645</person_id>
				<author_profile_id><![CDATA[81100130209]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Guenter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P108549</person_id>
				<author_profile_id><![CDATA[81100541234]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hee]]></first_name>
				<middle_name><![CDATA[Cheol]]></middle_name>
				<last_name><![CDATA[Yun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39063104</person_id>
				<author_profile_id><![CDATA[81100434457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Russell]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Mersereau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>15888</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Denber, Michael J. and Turner, Paul M. A Differential Compiler for Computer Animation. Proceedings of SIGGRAPH '86 (Dallas, Taxas, August 18-22,1986). In Computer Graphics 20,4 (August 1986), 21-27.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325174</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Duff, Tom. Compositing 3-D Rendered Images. Proceedings of SIGGRAPH '85 (San Francisco, California, July 22-26,1985). In Computer Graphics 19, 3(July 1985), 41-44.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>59921</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Jain, Anil K. Fundamentals of Digital Image Processing. Prentice-Hall, Englewood Cliffs, New Jersey, 1989.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>574886</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Jayant, Nuggehally S. and Noll, Peter. Digital Coding of Waveforms. Prentice-Hall, Englewood Cliffs, New Jersey, 1984.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Jones, Stephen C. and Moorhead II, Robert J. Hardware-specific Image Compression Techniques for the Animation of CFD data. Proceedings of SPIE - International Society for Optical Engineering, 1668 (San Jose, California, February 10-11,1992), 141-146.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>130247</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Lim, Jae S. Two-Dimensional Signal and Image Processing., Prentice-Hall, Englewood Cliffs, New Jersey, 1990.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Martucci, Stephen A. Reversible Compression of HDTV Images Using Median Adaptive Prediction and Arithmetic Coding. Proceedings of IEEE ISCAS, 2(New Orleans, Louisiana, May 1-3,1990),1310-1313.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Melnychuck, Paul W. and Rabbani, Majid. Survey of Lossless Image Coding Techniques. Proceedings of SPIE - International Society for Optical Engineering, 1075 (Los Angeles, California, January 17-20, 1989), 92-100.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15903</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Sederberg, Thomas W. and Parry, Scott R. Free-Form Deformation of Solid Geometric Models. Proceedings of SIGGRAPH '86 (Dallas, Taxas,August 18-22,1986). In Computer Graphics 20,4 (August 1986), 151-160.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>214771</ref_obj_id>
				<ref_obj_pid>214762</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Witten, Ian H., Neal, Radford M., and Cleary, John G. Arithmetic Coding for Data Compression. In Communications of the ACM 30,6 (June 1987), 520-540.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Motion Compensated Compression of Computer Animation Frames yz . Brian K. Guenter , Hee Cheol Yun, and 
Russell M. Mersereau Abstract devices. Animation image .les are typically stored off-line on removable 
media. This paper presents a new lossless compression algorithm for An alternative to using off-line 
storage is to compress the computer animation image sequences. The algorithm uses image data and store 
it on-line. This is very desirable for transformation information available in the animation script sequence 
editing and image manipulation, for example. For and .oating point depth and object number information 
stored high image quality only lossless compression is acceptable; at each pixel to perform highly accurate 
motion prediction with very low computation. The geometric data, i.e., the depth and object number, is 
very ef.ciently compressed us­ing motion prediction and a new technique called direction coding, typically 
to 1 to 2 bits per pixel. The geometric data is also useful in z-buffer image compositing and this new 
compression algorithm offers a very low storage overhead method for saving the information needed for 
z-buffer image compositing. The overall compression ratio of the new al­gorithm, including the geometric 
data overhead, is compared to conventional spatial linear prediction compression and is shown to be consistently 
better, by a factor of 1.4 or more, even with large frame-to-frame motion. images can then be exactly 
reconstructed from their com­pressed representation. Errors do not accumulate if images are combined 
or manipulated and run through the compres­sion decompression cycle several times. Muchmore informationisavailabletoa 
compressionalgo­rithm for computer animation than is the case for live action video. However, surprisingly 
little work has been done on exploiting the information in a computer animation script to improve image 
compression ef.ciency. Previous work such as that described in [1] and [5] is actually image based com­pression 
although the application is to computer animation. The lossless compression algorithm for computer anima­tion 
to be described in this paper combines elements of both motion prediction and spatial linear prediction 
compression techniques, using each when most appropriate. The new compression algorithm uses transformation 
information in the animation script to perform essentially perfect image space motion prediction with 
very low computation. This is a major advantage of the new algorithm because motion prediction with subpixel 
accuracy based only the information present in the image sequence is computationally expensive [6][3]. 
Poor quality motion prediction increases the mo­tion prediction error which reduces the maximum achievable 
compression ratio. One of the most effective lossless image compression tech­niques is DPCM followed 
by entropy coding [4][8]. For typ­ical live action video sequences the best compression achiev­able using 
this method is usually less than 2 to 1 [7]. The best computer generated images are nearly indistinguishable 
from real images so we can expect that good synthetic images will not compress any better than live action 
video. For scenes with fairly rapid camera and object motion the compression ratio we have achieved with 
our new motion prediction compression is approximately 1.5 times that of spatial linear prediction compression 
techniques -about 3 to 1 compression with the new technique as opposed to 2 to 1 compression with DPCM. 
As camera and object motion decrease the compression ratio of the new technique steadily increases while 
spatial prediction compression remains con­stant at roughly 2 to 1. Extra geometric information, the 
object number and the CR Categories: I.4.2[compression(coding)]exact coding. Additional keywords: compression,computer 
anima­tion,computer graphics, motion prediction 1 Introduction With the increasing popularity and falling 
cost of computer animation comes a new problem: storing the enormous data .les which even short computer 
animation sequences require. Five minutes of NTSC resolution computer animation takes up approximately 
8.5 gigabytes of storage; .lm resolution takes many times more. This amount of data cannot be economically 
stored on line in high speed secondary storage iy This work was supported by the National Science Foundation 
under grant MIP-9205853 Computer Animation Laboratory, GVU center, College of Com­ puting, Georgia Institute 
of Technology, Atlanta GA 30332, E-mail: brian.guenter@cc.gatech.edu z Digital Signal Processing Labratory, 
School of Electrical Engi­neering, Georgia Institute of Technology, Atlanta GA 30332, E-mail: yun@eedsp.gatech.edu, 
rmm@eedsp.gatech.edu Permission to copy without fee all or part of this material is granted provided 
that the copies are not made or distributed for direct  provided that the copies are not made or distributed 
for direct commercial advantage, the ACM copyright notice and the title of the commercial advantage, 
the ACM copyright notice and the title of the publication and its date appear, and notice is given that 
copying is by publication and its date appear, and notice is given that copying is by permission of 
the Association for Computing Machinery. To copy permission of the Association for Computing Machinery. 
To copy otherwise, or to republish, requires a fee and/or specific permission. otherwise, or to republish, 
requires a fee and/or specific permission. &#38;#169;1993 ACM-0-89791-601-8/93/008 $1.50 &#38;#169;1993 
ACM-0-89791-601-8/93/008/0015 $1.50 depth at each pixel, is stored in each frame to perform mo­tion prediction. 
The geometric information is compressed very ef.ciently in our new algorithm, typically to 1 or 2 bits 
per pixel. For z-buffer compositing applications [2] this is another advantage of the new algorithm, 
because the depth information needed for z-buffer compositing is stored in very little space. We assume 
the animation script contains a homogeneous matrix transformation for every object in every frame. The 
matrix transforms the object from the model space coordinate frame into the screen space coordinate frame. 
The transfor­mation matrices are stored in an auxiliary .le along with the compressed image data and 
constitute part of the overhead of the new compression algorithm. This limits the current implementation 
to rigid body motion but this is not an in­trinsic limitation of the algorithm. Non-rigid body motion 
can be accommodated by storing appropriate transformation information, such as free form deformation 
mesh points for example [9], in the animation script. The current implementation assumes that objects 
are rep­resented as polygonal surfaces. Algorithms exist for convert­ing many different surface representations 
to approximating polygonal surfaces. Many commercial image synthesis pro­grams perform this conversion 
internally so the limitation to a polygonal representation is not unduly restrictive. The geometric data 
has special properties we exploit to im­prove compression. As a consequence the coder is split into two 
parts: a geometrical data coder and a color data coder. General notation used throughout the paper is 
presented in Section 2. Section 3 of the paper presents block diagrams of the algo­rithm. Section 4 describes 
the geometrical data coding algo­rithm. Section 5 describes the color data coding algorithm. Animation 
test results are presented in section 6 and conclu­sions and suggestions for further research are presented 
in section 7.  2 Notations and Data structure In this paper the frame number, which is used to identify 
the speci.c frame, is expressed as a superscript. A subscript represents the object number when it is 
expressed as a single value and the spatial location when it is expressed as a pair of values. If the 
subscripts are omitted, that symbol represents the whole set of the corresponding data for that frame. 
The data structure of a frame is divided into two parts. The .rst part is the set of 44 homogeneous matrices 
for all the objects 01 by which the point in the model object space is transformed to the screen space. 
The other part is the 2-dimensional array of the data  ppN x m en f h TN ji Ne x ppNj m ni h y m ppNN 
y u o m 4 meZ m ni n g m NP om ni CP m nim ni em h 0101 , where represents the number of objects and 
and represent the number of pixels in each direction. Each pixel datum is composed of the object number 
, depth and colors of the pixel at the spatial location . For example, the point in the i-th frame is 
transformed to the point 4 x j ey j ez j m4 mejthneZ i m T in the -frame as follows: 1 Tk h T N kjm 
ni 4 T ki m . 0.B Zm m n i n 1AC h NN i . i .B 0 Z i xyz j C i 1AC ithZ i 4m 1 11 where . As mentioned 
above, the symbols without the subscripts represent the whole set of data for the frame. For example 
stands for the set of matrices and , , represent the whole two dimensional array, also called a .eld, 
containing the object number, depth and color values of the -frame, respectively. The object number and 
the depth are collectively called the geometrical data .eld. The color .eldC i represents the R,G,B 
color .elds, but sometimes can be used for one speci.c color .eld. The object number and color values 
are represented as integers, but the depth is a real number. In our implemen­tation, each of the RGB 
color values is usually represented by 8 bits/pixel (256 levels), and the depth is double preci­sion 
.oating point. Since the compression ef.ciency of the geometrical data is highly dependent on the accuracy 
of the calculation, the double precision representation is preferred. The required number of bits for 
the object number depends on the total number of objects.  3 System Block diagrams The heart of the 
coding scheme uses a linear predictive cod­ing algorithm (DPCM) [4]. Since there exists substantial correlation 
between successive frames in computer anima­tion as well as in real-life video, good compression gain 
can be achieved by these predictive schemes. Since both the object number and depth .elds are needed 
to compute the motion trajectory of each pixel and these are encoded to­gether into one data stream , 
the whole system is divided into a geometrical data coding block for and a color CN iii G i NC ii eZ 
i data coding block for as shown in Figs 1. The object number and color data are coded loss­lessly, but 
the depth is allowed to contain error within a speci.ed limit to achieve a high compression gain, becauserequires 
a relatively larger number of bits (64 bits/pixelZ i ZN i eC i for a double precision representation) 
than . The DPCM system requires storage for several frames determined by the order of the predictor. 
The geometrical data coding block stores the object number .elds and the _depth .elds of previous frames. 
Since only the decoded values are available for the depth .eld in the decoder, the _ geometrical data 
encoder uses the decoded depth .eld instead of the original depth .eld for correct reconstruction _ 
Z i Z i N i N i eZZ ii from the encoded data. The stored geometrical data are provided to the color data 
coding block to predict the color data in other frames by motion prediction. Ti  Ti Ni Gi Zi Ii Figure1: 
-Order Frame Encoder and Decoder  4 Geometrical Data Coding Figs 2 show the block diagrams of the encoder 
and decoder for geometrical data. The principle behind the geometrical data coding is that the geometrical 
data of the current frame is predicted from several previous frames which are compared to the current 
original frame pixel-by-pixel. Each pixel is classi.ed as matched if , of the current frame N m ni Z 
m n N im ni Z m ni P m ni are the same as , of the predicted frame, and unmatched otherwise. Since the 
object number and depth for the matched pixels can be recovered from the predicted frame in the decoder, 
the only information that needs to be M i Ci transmitted are the matching status .eld which records whether 
or not each pixel is matched, and the complete ge­ometrical data for the unmatched pixels. Unmatched 
pixels occur mainly in recently uncovered regions which cannot be predicted from previous frames, or 
from highly curved regions that are dif.cult to predict. For the unmatched regions the geometrical data 
can be coded effectively by exploiting the spatial correlations be­tween pixels, because pixels which 
belong to the same planar polygonsatisfythesameplaneequation. Analgorithmcalled direction coding is proposed 
and described later. With direc­tion coding, the unmatched pixels are classi.ed into direction matched 
pixels and totally unmatched pixels. This matching information replaces at the unmatched pixel and this 
 M m ni M i ¯ modi.ed matching status .eld is . After the direction cod­ing, since the majority of the 
frame is matched, the entropy of ¯is very small. Thus ¯can be compressed effectively by entropy coding 
or run-length coding and the original ge­ometrical data , are transmitted in uncompressed  M i N m ni 
Z m ni M i M m ni form only for the totally unmatched pixels. ¯ At the receive decoder, the matching 
status .eld , and  Ti-n Ni-n Z . i-n . . Ti-1 Ni-1 . i-1 TiNiZ . i Ti i G . . Ti-n Ni-n Z. i-n . . 
Ti-1 Ni-1 Z. i-1 TiNi Zi Figure 2: Geometrical Data Encoder and Decoder the geometrical data ,for totally 
unmatched pixels are obtained. For the matched pixels which can be identi.ed ¯ by the , the object number 
and the depth are copied from the predicted frame. Then for the unmatched ´ pixels the , are recovered 
by the direction decoder N m ni Z m ni Z and the complete recovered frame data is fed into the frame 
predictor for the next frame prediction. 4.1 Frame Predictor The set of four pixels , 1, 11, 1is P m 
ni S m ni P m nim n P mi n P mim n NTZ n iii de.ned as a pixel square . Each pixel square can be 
classi.ed into one of three categories. The .rst is the plane pixel square where all four corner pixels 
come from the same planar polygon and make a planar square. The second is the adjacent polygon pixel 
square where four pixels are from the two adjacent polygons that share an edge that intersects the pixel 
square. The third is a non-adjacent polygon pixel square where the polygon boundary is across the pixel 
square but the polygons do not share an edge. These three cases are illustrated in Fig. 3. The planar 
pixel square can be easily transformed by Eq. 1 and rendered into other frames. For the adjacent polygon 
pixel square, if the plane equations of the two polygons can be obtained by exploiting the neighboring 
pixel squares, then the pixel square can be partitioned into two polygons and each polygon can be transformed 
and rendered onto other frames in the same way. There is not enough information to make correct partitions 
for the non-adjacent polygon pixel squares, then these cannot be used to predict other frames. The pixel 
square can be de.ned as a planar pixel square S m ni NON-ADJACENT POLYGON PIXEL SQUARE PLANAR PIXEL 
SQUARE i - th FRAME j - th FRAME Figure 3: Transform of Pixel Squares between Frames if the four pixels 
satisfy the following plane conditions. 1 11 1 j ZN m nim ni ih Z mN i m ni n h m NZ m nimi n m Z h 
mi N mi n j n 44mm 3 11 11 In Eq. 3 the inequality is used to deal with the error due to the limited 
precision of computation and the small numberis determined as the allowable error for the depth of the 
pixels which are from the same planar polygon. In some cases, the above two conditions are not enough 
to determine whether the pixel square is planar or not. For example, the pixels lying across the boundary 
of two separate polygons which are parallel to each other might satisfy these two conditions. There are 
several ways of reducing the pos­sibility of an incorrect classi.cation of a planar pixel square. One 
way is to add the following conditions, which test the relations between the depths of the surrounding 
pixels. If the following plane conditions are satis.ed with regard to at least one corner of the pixel 
square, can be considered to be a plane pixel square. 2 11 24 11  j lZ h l ki me m mZ ll k i i k m Z 
k l kli S h m ni kne j n i 4 2m for 1and 1. For the non-planar pixel square which does not satisfy the 
above plane conditions, if some two pixel squares around it are planar and if the intersection of those 
two planes are found to be across the pixel square by solving the plane equations of those two planes, 
then this pixel square is an adjacent polygon pixel square that can be divided into two polygons and 
transformed into other frames. One way to .nd the two plane pixel squares is to test the above plane 
conditions for each pair of pixel squares which are on the opposite sides of the current pixel square. 
The geometrical data for most of a frame can be computed by transforming all the planar and adjacent 
polygon pixel squares of the previous frame into the current frame and rendering the transformed polygons 
on the frame buffer of geometrical data using the z-buffer algorithm. Since the current frame does not 
change much from the previous frame, the transformed polygon of one pixel square is small and covers 
only a few pixels. Under this assumption, there are several effective techniques for geometrical data 
rendering. One simple method is to .nd the bounding box of the transformed pixel square and test whether 
each pixel point insidetheboundingboxisinsidethepolygonornot. Foreach inside pixel point, the depth of 
that point can be computed from the plane equation of the transformed polygon for the z­buffer rendering 
process. The back-face removal step might be applied before rendering. There is one special case where 
the viewpoint and object are not moving. In this case the transform matrices of the current and previous 
frames are the same and the whole 1 transform matrix will be the identity matrix in T kj 4 T ki m Eq. 
1. For these pixels, the above complicated steps are not necessary and the only thing to do is simply 
to apply the depth of the previous frame to the z-buffer algorithm at the same pixel location. All these 
pixels are classi.ed as matched pixels. Due to occlusion some parts of the current frame may not be predictable 
from the previous frame. The percentage of predictable pixels can be increased by using higher order 
prediction. In the n-th order case, the previous n frames are transformed and rendered on the same frame 
buffers of object number and depth. The frame predictor used in both the encoder and decoder have identical 
frame buffers for a object number and depth for higher order prediction. The encoder and decoder should 
store the same frame data in both predictors so that the pre­dicted frames in both blocks will be the 
same. 4.2 Frame Comparator The frame comparator compares the input frame data to the predicted frame 
data on a pixel-by-pixel basis and records the result in the matching status .eld . If a pixel of the 
current frame and of the predicted frame satisfy the following conditions, then the pixel is considered 
to be predictable from the previous frames and said to be a matched pixel. 5  P m ni j ZN m nim n 
P i M m ni m i h Z m ni N m ni j MM im ni P m ni 4 m 6 In Eq. 6, an inequality is used for the same 
reason as in Eq. 3. If a pixel is found to be matched, is set to 1 and otherwise set to 0. Because of 
the similarity between the adjacent frames, most of will be 1. For matched pixels, the depth of the input 
frame is replaced by the predicted depth to guarantee the consistency of data between the M i Z m ni 
Z m ni Z m ni NZ im ni Z i encoder and decoder, because only will be available in the decoder. By Eq. 
6 the accuracy of the new depth is guaranteed to be within. This modi.ed depth .eld is ¯and will be given 
to the direction coder along with and . D 7 D D  6 P ( M , N , Z ) 3 333 D D P ( M , N , Z ) 1 5 2 222 
D P ( M , N , Z ) D 4 1 111 2 D 3 DEPTH MATCHING TEST MATCHING DIRECTIONS Figure 4: Determination of 
Direction Matching 4.3 Direction Encoder Unmatched regions are usually from recently uncovered re­gions 
or highly curved regions that cannot be predicted. Since the pixels in those regions might come from 
some plane polygons or might be on the extension of a plane from a surrounding matched regions, a spatial 
prediction technique exploiting the plane relationship between neighboring pixels can be used to code 
these pixels. One such method is to .nd the matching direction in which two neighbor pixels lying on 
a straight line match the object number and depth of the current pixel and record the direction value 
by overwriting the which was originally zero. Since only the recon-  M m ni structed data are available 
in the decoder, those two pixels should be pixels that have been already coded. Therefore it should be 
checked whether the matching status values of those two pixels is still zero. The matching conditions 
for a direction at 3 in Fig. 4 are described as follows : 10and 20 7 123 8 DDZPM m ni MZN j Z 6 hhh m 
NZZ j h M m NZ h 6 D i h i i P 44 m m where 3 is the spatially predicted depth of the pixel 3 in the 
speci.ed direction as in Eq. 10. ´ 3221 10These conditionsare testedforeightdirectionsfromdirection1 
to 8 as in Fig. 4. The .rst matched direction becomes the matching direction of the pixel and the corresponding 
direction value, which is de.ned as 1 in Fig. 4, is assigned to which was originally zero. The depth 
of the current pixel is replaced by the predicted value in the matched direction as in Eq. 10 for consistency 
of the data in theencoderanddecoder. Ifthereisnomatchingdirection,the number 10 is assigned, corresponding 
to a totally unmatched pixel. After direction coding, the frame data will be: 1 matched ¯  M ´Z m nm 
nii hh .8 . Z 3 pp m ni ´39 29 direction matched (11) 10 totally unmatched matched _´ direction matched 
(12) totally unmatched  Figure 5: Example of Direction Encoding. The unmatched region inside the dashed 
polygon is coded by direction cod­ing. Coding is performed from left to right and from bottom to top. 
remains unchanged because the object number is loss­lessly coded. Fig. 5 shows an example of direction 
coding where the region inside the polygon is originally unmatched. The en­coding is performed from left 
to right and from bottom to top and the arrows represent the direction of matching. This illustrates 
that the number of totally unmatched pixels is very small and the matching directions are mostly 2 because 
direc­tion 2 is the .rst test direction in Fig. 4. Since the geometrical data for the matched and direction 
matched pixels are pre­dictable, the data to be transmitted are the matching status .eld ¯which has very 
low entropy and the s M i f N m ni eZ m ni g  for a few totally unmatched pixels. 4.4 Frame Synthesizer 
Since the matched pixels can be identi.ed from the matching status .eld ¯decoded in the receive decoder, 
the frame syn­thesizer can recover the geometrical data for all the matched pixels by copying the data 
from the predicted frame. Then the geometrical data , for the matched pixels and to- M i N m ni Z m ni 
tally unmatched pixels are correctly recovered whereas those for direction matched pixels remain undetermined: 
¯ ¯ if 10 if¯ 1 (13) undetermined if ¯ 29  NZ m n ii hh .8 . ZN ZZ m nim n iii h ZN m n ii M M m n 
iii Z h h m ni ppppN m ni ¨if¯10 if¯ 1 (14) undetermined if ¯29 4.5 Direction Decoder For the direction 
matched pixels, the object number is recovered by copying the object number of the pixel which is _ located 
in the matching direction and the depth becomes ´ the predicted value from the two pixels located in 
the Ki-n ... Ki-1 Ki DECODER Figure 6: Color Data Encoder and Decoder, where _ matching direction as 
in Eq. 10. are recovered correctly  Z m ni N m ni Z i R i __for all the pixels and the depth .eld is 
the same as in Eq. 12. The accuracies of for the matched pixel and ´ for the direction matched pixel 
are guaranteed by Eq. 6 m ni m and Eq. 9, respectively. These decoded data are fed into the predictor 
and are the same for both the encoder and decoder to make the same predictions as in Fig. 2.  5 Color 
Data Coding Since the s from the geometrical data coding block 4 TeNeZ m can be used to compute the locations 
of a current frame pixel on the previous frames and the previous color data frames are stored, the color 
of the pixel can be predicted by estimating the color at the transformed locations in the previous frames. 
The errorimage betweentheoriginalandthepredictedframe, also called the residual image , has a relatively 
low entropy compared to the original. The pixels are classi.ed into two classes, matched and unmatched 
pixels, based on whether the locations in the old frames are traceable or not. Since this residual image 
still has some spatial correlation, spatial linear prediction coding (DPCM) can be applied to reduce 
the entropy. This DPCM coded residual image is called a differ­ential residual and entropy coding using 
Huffman coding or arithmetic coding, can compress the differential residual losslessly. In decoder after 
the residual is recovered by DPCM decoding, the original color data is obtained as the sum of the predicted 
value and the residual D i R im n C m ni C m ni CR m nii R m ni for the matched pixel. The color of 
the unmatched pixel is the same as residual . 5.1 Color Frame Predictor A pixel of one frame can be 
mapped to another frame by the transformation in Eq. 1 and if the transformed point satis.es the following 
condition, it is considered to be the same point as the current pixel and the pixel is said to be matched. 
If the pixel is transformed into the j-th frame and the transformed point is inside the pixel square 
, the matching condition is as follows: Z minj Z maxj ZP trjm nians Z minj S p qj . Z trjans . Z maxj 
S p qj 4m R i 15where the is the depth of the transformed point and , are the minimum and maximum depth 
of the four corner pixels of , respectively. Since several pre­vious frames of geometrical and color 
data are available to deal with the occlusion problem, the transformed point in the nearest frame which 
satis.es the above condition is used to predict the color data of the current pixel. L i records the 
matching status values for color data which are zeros for the unmatched and ones for the matched pixels. 
Since generally the transformed point is not the pixel point, an interpolationis necessary for the computationof 
the color. 5.2 Subtractor and DPCM Encoder Colors of matched regions in the residual are residual values 
generated by sutracting the predicticed colors from the original colors whereas the colors in the unmatched 
regions remain unchanged as follows:  C p qi ´C p qi C p q ifif C i 01 C m ni 16  R m ni h U C m ni 
m C m ni L m ni h 4m Unmatched regions are mostly from recently uncovered re­gions and the entropy of 
such unmatched regions can be reduced by spatial linear predictive coding (DPCM). The residual image 
in the matched region has low entropy caused by changes of illumination, by the movement of objects, 
viewpoints or light sources between frames. Since this kind of error has relatively slow spatial variation, 
DPCM can be effectively applied also to the matched regions of the residual image. These two steps, frame 
subtraction and DPCM, can be implemented with a combined operation as in Fig. 6. Since the matched and 
unmatched regions have different kind of data as explained above, each region should be coded in­dependently. 
Usually 2-D DPCM uses three left and lower ´ neighboring pixels to predict the current pixel . If ´ 
the current pixel is an unmatched pixel, should be the original color . If the current pixel is a matched 
pixel, ´ should be the residual value. Since the residual value is ´ not available for an unmatched 
pixel, the is set to zero when 0 as follows: if 0´17 if 1 _ The spatially predicted value is the integer 
part of a linear combination of those as follows: _´´´  C m ni CL p q ii hhh U 4 LCC i p q mi 4 C p 
q ni i m hCCC p qimim ni m n L i i m n CC p q i C h m ni m44mm int18 1111 frame 5 frame 15 frame 25 
20.88 20.78 20.58 11.35 11.23 10.59 0.32 0.67 1.06 7.20 0.93 6.19 0.99 8.26 7.12 Inthispaper,thepredictioncoef.cients 
, , areselected D i hC to be 0.75,-0.5,0.75 respectively. Then the differential residual image is obtained 
by sub­tracting the spatially predicted value from the residual value as follows: _ C i C i _if 0 19 
_ if 1 D m ni h U C m ni m C m ni D i m C m ni C m ni L m ni h 4m The differential residual , which 
has very small entropy, can be compressed losslessly by an entropy coding technique. 5.3 Adder and DPCM 
decoder The and in the predictor of the decoder are the L im n C m ni C i same as those in the encoder. 
The original color .eld is recovered by the combined step of DPCM decoding and frame addition as follows: 
_ if 0 20 _ if 1 C m ni h U D m ni i C m ni i C m ni L m ni h 4m _ where the spatial prediction is 
the same as in the encoder. The recovered color is fed back to the predictor for the prediction of the 
next frames. 6 Test Results 31 animation frames were generated to test the proposed compression algorithm. 
Each frame is composed of 7 objects and various kinds of textures were mapped by solid texture mapping. 
Through the whole 31 frames the ball bounces back and forth between the two wood blocks. In the .rst 
11 frames the viewpoint does not change and in the next 10 frames the view point moves approximately 
5 degrees/frame. During the last 10 frames zooming is performed. Plate 2,3, and 4 show the frames for 
above three cases. In the case of Plate 2, since most of the objects are not mov­ing and the view point 
is .xed, all regions except the rolling ballarematchedregions. Pixelpointsinthecurrentframeare transformed 
exactly to the same pixel points of other frames, then there are no errors due to bilinear color interpolation 
and since even the colors on the edges of stationary objects are predictable, the residual will have 
extremely small entropy which results in very high compression gain. Plate 3 is a more general case in 
which both an object and the viewpoint are moving.There are mainly three kinds of residual errors. First, 
the recently uncovered regions are the major error regions which can be compressed only by DPCM. Second, 
in matched regions the changes in illumina­tions on the object surfaces causes residual errors. Usually 
illumination changes are due to the changes of specular re­.ection which varies with the movement of 
the view point or the object itself. Generally these errors change slowly and can be lowered by DPCM. 
Third, since the object boundaries are often unmatched regions which do not satisfy Eq. 15, the  ODRIPCGINAL 
 N GERGBMOOTM ION Table 1: Entropies of original image and residuals by DPCM and Motion Prediction (unit 
: bits/pixel) residual errors are large there. And even in the case where the pixels on the object boundary 
are matched, relatively large errors due to the color interpolation occur because the color data of these 
pixels were generated by antialiasing. In Plate 4, since with zooming there are no recently uncovered 
re­gions and the spatial frequency is decreasing, the entropy of the residual signal will be smaller 
than that in Plate 3. The second order compression algorithm was implemented and tested on this test 
sequence. Fig. 7 shows the entropies of the original pictures and the differential residual images by 
2-D DPCM and motion prediction algorithm. Table 1 illustrates the entropies of several frames. Plates 
5 and 6 show the differential residual images of the RED component by linear predictive coding (DPCM) 
and the new motion pre­diction, respectively. As explained above, the major errors in Plate 6 are on 
recently uncovered regions and along the object boundaries which are often unmatched regions. The biggest 
differences between DPCM and motion prediction occurred on the wood texture which has relatively higher 
spatial frequency than any other regions. In these high spa­tial frequency regions, motion prediction 
shows much higher performance than spatial linear predictive coding. Through the whole 31 frames, the 
entropy of the original frame is around 20.6 bits/pixel. In the .rst 11 frames where the view­point is 
.xed, only about 0.9 bits/pixel is required for the motion prediction technique, except for the .rst 
frame which cannot be motion predicted and is coded only by DPCM. This contrasts with DPCM which needs 
around 11.5 bits/pixel. The necessary bits/pixel for the next 10 frames in which both the viewpoint and 
object are moving is around 8.3 bits/pixel which is about 3 bits/pixel gain over DPCM. In the next 10 
frames, all entropies are decreasing with zooming as expected above, but the motion prediction algorithm 
still outperforms DPCM by about 3.5 bits/pixel. 7 Conclusion The motion prediction compression algorithm 
for computer animation image sequences presented here consistently out­performs spatial linear prediction. 
This is true even though the new algorithm encodes double precision depth and integer object number information 
for a total of 96 bits/pixel includ­ing RGB data while the spatial compression algorithm does not. The 
depth and object number information are useful in Figure 7: Entropies of the Bouncing Ball Sequence. 
The data for motion prediction are the sum of compressed geometrical data and entropy of residual. their 
own right for performing z-buffer image compositing. The compression ratio achieved by the new algorithm 
was 1.4 times or more greater than that achieved with spatial lin­ear prediction even for scenes with 
rapid changes in camera view point and substantial changes in object occlusion rela­tionships. The overall 
compression ratio achieved with the new algorithm for image sequences with signi.cant object and viewpoint 
motion was approximately 3 to 1. There is still scope for improvement in the algorithm. We have noticed 
signi.cant remaining correlation in the residual images. Long strings of zero residual values are punctu­ated 
by short bursts of plus and minus one residual values. Additionally the geometric overhead can be signi.cantly 
re­duced by storing the polygonal surface data and re-rendering it with the z-buffer algorithm, a process 
which should be no more time consuming than re-rendering pixel squares as in the current implementation. 
This makes the auxiliary data .le more complex but for animation sequences more than a few seconds long 
the geometric data overhead should drop to .1 bit/pixel or less. We are currently implementing this extension. 
The current implementation of the algorithm is limited to objects represented as polygonal surfaces. 
As discussed in the introduction this is not an undue restriction since ef.cient algorithms exist for 
approximating many different surface types with polygonal surfaces. However it would be an in­teresting 
research project to extend the geometric coder to other surface types so that an exact, rather than an 
approxi­mating polygonal, surface representation could be used. This extension would require modifying 
both the motion predic­tion stage and the direction coding stage to compute surface equations directly 
from the depth information stored in the image.  8 Acknowledgments The authors acknowledge help from 
Ragnar Jonsson,Stephen A. Martucci, Wayne Wooten and Lonnie D. Harvel. References [1] Denber, Michael 
J. and Turner, Paul M. A Differen­tial Compiler for Computer Animation. Proceedings of SIGGRAPH 86 (Dallas, 
Taxas, August 18 22,1986). In Computer Graphics 20,4 (August 1986), 21 27. [2] Duff, Tom. Compositing 
3-D Rendered Images. Pro­ceedings of SIGGRAPH 85 (San Francisco, California, July 22 26,1985). In Computer 
Graphics 19, 3(July 1985), 41 44. [3] Jain, Anil K. Fundamentals of Digital Image Pro­cessing. Prentice-Hall, 
Englewood Cliffs, New Jersey, 1989. [4] Jayant, Nuggehally S. and Noll, Peter. Digital Coding of Waveforms. 
Prentice-Hall, Englewood Cliffs, New Jersey, 1984. [5] Jones, Stephen C. and Moorhead II, Robert J. Hardware-speci.c 
Image Compression Techniques for the Animation of CFD data. Proceedings of SPIE -In­ternational Society 
for Optical Engineering, 1668 (San Jose, California, February 10 11,1992), 141 146. [6] Lim, Jae S. Two-Dimensional 
Signal and Image Pro­cessing., Prentice-Hall, Englewood Cliffs, New Jersey, 1990. [7] Martucci, Stephen 
A. Reversible Compression of HDTV Images Using Median Adaptive Prediction and Arithmetic Coding. Proceedings 
of IEEE ISCAS, 2(New Orleans, Louisiana, May 1 3,1990),1310 1313. [8] Melnychuck, Paul W. and Rabbani, 
Majid. Survey of Lossless Image Coding Techniques. Proceedings of SPIE -International Society for Optical 
Engineering, 1075 (Los Angeles, California, January 17 20, 1989), 92 100. [9] Sederberg, Thomas W. and 
Parry, Scott R. Free-Form Deformation of Solid Geometric Models. Proceedings of SIGGRAPH 86 (Dallas, 
Taxas,August 18 22,1986). In Computer Graphics 20,4 (August 1986), 151 160. [10] Witten, Ian H., Neal, 
Radford M., and Cleary, John G. Arithmetic Coding for Data Compression. In Commu­nications of the ACM 
30,6 (June 1987), 520 540. Plate 3: Frame 10,19 Plate 1: Frame 5 of the test sequence Plate 4: Frame 
20,29 Plate 2: Frame 0,9   Plate 5: The differential image of frame 5 by DPCMPlate 6: The differential 
residual image of frame 5 by motion 512 512. compensation 512 512. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166156</article_id>
		<sort_key>305</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>39</seq_no>
		<title><![CDATA[Space diffusion]]></title>
		<subtitle><![CDATA[an improved parallel halftoning technique using space-filling curves]]></subtitle>
		<page_from>305</page_from>
		<page_to>312</page_to>
		<doi_number>10.1145/166117.166156</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166156</url>
		<keywords>
			<kw><![CDATA[digital halftoning]]></kw>
			<kw><![CDATA[dot diffusion]]></kw>
			<kw><![CDATA[error diffusion]]></kw>
			<kw><![CDATA[ordered dithering]]></kw>
			<kw><![CDATA[parallel algorithms]]></kw>
			<kw><![CDATA[space-filling curves]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Digitizing and scanning</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Grayscale manipulation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Experimentation</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14055077</person_id>
				<author_profile_id><![CDATA[81392603538]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuefeng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39085146</person_id>
				<author_profile_id><![CDATA[81544367056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Webber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cole, A. J. Halftoning Without Dither or Edge Enhancement. The Visual Computer 7 (1991), 232- 246.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Floyd, R. W. and Steinberg, L. An Adaptive Algorithm for Spatial Greyscale. Proceedings of the S. I. D. 17, 2 (Second Quarter 1976), 75- 77.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Jarvis, J. E, Judice, C. N., and Ninke, W. H. A Survey of Techniques for the Display of Continuous Tone Pictures on Bilevel Displays. Computer Graphics and Image Processing 5 (1976), 13- 17.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Judice, C. N., Jarvis, J. F. and Ninke, W. H. Using Ordered Dither to Display Continuous Tone Pictures on an AC Plasma Panel. Proceeding of the S.I.D. 15, 4 (Fourth Quarter, 1974), 161- 169.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Knowlton, K. and Harmon, L. Computer-Produced Grey Scales. Computer Graphics and Image Processing 1 (1972), 1 - 20.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>35040</ref_obj_id>
				<ref_obj_pid>35039</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Knuth, D. E. Digital Halftones by Dot Diffusion. ACM Transactions on Graphics 6, 4 (October 1987), 245- 273.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>27674</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Ulichney, R. Digital Halftoning. The MIT Press, Cambridge, Massachusetts, 1988.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122727</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Velho, L. and de M. Gomes, J. Digital Halftoning with Space Filling Curves. Proceedings of SIGGRAPH '91 (Las Vegas, 28 July-2 August 1991). In Computer Graphics 25, 4 (July 1991), 81 - 90.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Witten, I. H. and Neal, M. Using Peano Curves for Bilevel Display of Continuous Tone Images. IEEE Computer Graphics and Applications (May 1982), 47- 52.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Witten, I. H. and Wyvill, B. On the Generation and Use of Space-filling Curves. Software Practice and Experience 13 (1983), 519- 525.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>139894</ref_obj_id>
				<ref_obj_pid>139834</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Wyvill, G. and McNaughton, C. Three Plus Five Makes Eight: A Simplified Approach to Halftoning. Scientific Visualization of Physical Phenomena (Boston, 1991), Springer-Verlag, New York, 379- 392.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Space Diffusion: An Improved Parallel Halftoning Technique Using Space-Filling Curves Yuefeng Zhang 
and Robert. E. Webber Computer Science Department The University of Western Ontario London, Ontario, 
Canada N6A 5B7 ABSTRACT Dot diffusion has been proposed as a way of combining the strengths of ordered 
dithering and error diffusion to create a parallelizable halftoning technique. However, dot diffusion 
pays a price in image quality in order to achieve parallelizability. Space-.lling curves have been used 
to improve error diffusion. We show that by com­bining dot diffusion with a space-.lling curve traversal 
technique, a parallelizable halftoning technique results that does not pay a cost in image quality. This 
new technique we call space diffusion. KEYWORDS: Space-.lling curves, dot diffusion, error diffusion, 
ordered dithering, digital halftoning, parallel algorithms. 1 INTRODUCTION 1.1 OVERVIEW To print a 
gray scale image on a binary (e.g., black and white) de­vice such as a laser printer, the gray image 
has to be transformed into a binary image. The process of doing this while not signi.­cantly altering 
the appearance of the image is called digital halfton­ing. Although grey-scale display devices are becoming 
cheaper and cheaper, there seems to consistently be a trade-off between the spatial resolution of display 
devices and the range of usable intensi­ties at each spatial location. Thus it is reasonable to expect 
that for quite some time into the future, there will continue to be situations where the digital halftoning 
process will be necessary. As the spatial resolution of display devices increases, the paral­lelizability 
of digital halftoning algorithms will play a greater role in the evaluation of the usefulness of a particular 
method in the design of an effective display system. Many digital halftoning methods have been proposed 
in the literature. The ones we will consider in this paper are: ordered dithering, error diffusion, dot 
diffusion, error diffusion along space-.lling curves, and error diffusion along space-.lling curves combined 
with patterning. Other methods may be found elsewhere [3, 5, 7]. We will propose a new method, called 
space diffusion, which combines the best features of these methods while remaining parallelizable. In 
the following subsection, the above mentioned previously known halftoning methods are presented in more 
detail. After that, the space diffusion method is presented in Section 2. Then, Section 3 contains a 
discussion of experimental results aimed at evaluating Permission to copy without fee all or part of 
this material is granted provided that the copies are not made or distributed for direct provided that 
the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and 
the title of the commercial advantage, the ACM copyright notice and the title of the publication and 
its date appear, and notice is given that copying is by publication and its date appear, and notice is 
given that copying is by permission of the Association for Computing Machinery. To copy permission of 
the Association for Computing Machinery. otherwise, or to republish, requires a fee and/or specific permission. 
otherwise, or to republish, requires a fee and/or specific permission. &#38;#169;1993 ACM -0 -89791 -601 
-8/93/008 $1.50 &#38;#169;1993 ----8/93/008/0015 $1.50 the performance of this method. Final remarks 
are contained in Section 4. For convenience, in the following discussion, we assume that the gray image 
and its corresponding binary image have the same size.  1.2 BACKGROUND Of the previously known halftoning 
methods, ordered dithering [3] is one of the more widely used digital halftoning techniques. It is based 
on the idea of mapping grey values to black and white by comparing them to a threshold value that varies 
across the image. Suitable patterns of thresholds map regions of similar grey values into corresponding 
patterns of black and white values. Because the thresholds are independent of the image values and the 
mapping for each image value is independent of all the other image values, the method is very easily 
parallelized. The major drawback of this method is that it tends to produce regular patterning in large 
uniform regions as shown later in Section 3 where experimental results are presented. This drawback is 
avoided in Floyd and Steinberg s error diffu­sion technique [2]. The error diffusion method is based 
on using a single threshold that is the same across the whole image. Instead of ignoring the difference 
between the actual value and the result of the thresholding (as ordered dithering does), error diffusion 
takes this difference and passes it on to neighboring nodes. This process is performed left-to-right 
row-by-row, so the error is spread (dif­fused) over the values in front and below the value currently 
being processed. This method is inherently non-parallel as it is based on a very speci.c order of processing 
of the individual grey values. The error diffusion technique usually produces images that are much sharper 
than the images produced by the ordered dither meth­ods. One dif.culty with this method is that it sometimes 
generates features that do not appear in the original image. These features are the result of the distribution 
of error values across the image and hence do not occur in ordered dithering where the error values are 
not distributed at all. Knuth [6] presents a new technique, called dot diffusion, that inherits the main 
advantagesof ordered dithering and error diffusion techniques. At the same time, it avoids some of the 
dif.culties with these two methods. The strategy of dot diffusion is to replicate a .xed matrix over 
the image. This assignment of matrix values to image locations divides the pixels of the image into classes 
that map to the same matrix value. Then, a technique similar to error diffusion is used to halftone these 
classes simultaneously. Speci.cally, the following matrix 34 48 40 32 29 15 23 31 4258565321 5 7 10 5062614513 
1 2 18 384654372517 9 26 28 14 22 30 35 49 41 33 20 4 6 1143595752 12 0 3 1951636044 2416 8 2739475536 
is usually used to classify the pixels of the image into 64 classes g x 0B . modymod ng x.y n 1C A according 
to 8, 8, where is the location of a pixel. Starting with the pixels that map to the index value 0, at 
each stage, the thresholded value of pixels in the lowest numbered un­processed class is computed and 
the resulting error difference is distributed to the remaining neighboring pixels that are in unpro­cessed 
classes with higher matrix numbers. As with error diffusion, the threshold used is the same at each location. 
This method is related to error diffusion in that it also distributes the error term that correspond 
to the difference between the original grey value and the resulting thresholded binary value. It is related 
to ordered dithering in that ordered dithering is also generally imple­mented by way of a replicated 
matrix that establishes a positional bias in location of black and white values in the image. This ma­trix 
replication structure causes bother ordered dithering and dot diffusion to suffer from the problem of 
having regular patterning occur in large regions of uniform intensity as illustrated in Section 3. Fixing 
this problem with dot diffusion is one of the main aspects of this paper. Recently [1, 8, 9, 10, 11], 
it has been observed that error diffu­sion itself can be improved by replacing the left-to-right row-by-row 
order of processing of pixels by an order patterned after the discrete version of space .lling curves. 
Two commonly used curves are Peano curves [1] and Hilbert curves [8]. Our method is based on Hilbert 
curves and they are discussed in more detail in Section 2. The main idea of the halftoning techniques 
that are based on space-.lling curves is to use the space-.lling curves to establish the order in which 
the pixels of the image are visited. Error terms are then distributed along this path in a manner similar 
to error diffusion. One problem with using traditional space-.lling curves for traversal patterns is 
that they .t best on square images that have widths that are powers of two. However, two approaches exist 
in the literature to addressing this problem for halftoning algorithms. One approach is based on a generalization 
of Peano curves called Murray polygons [1]. Another approach exists based on decompos­ing rectangles 
into smaller rectangles from one of a few standard sizes [11]. However, these methods have two other 
drawbacks. One is that, like traditional error diffusion, they are inherently serial (as the algorithm 
is driven by a particular serial traversal of the pixels). The other is that they do not handle long 
narrow features in images as well as either dot diffusion or error diffusion do. On the other hand, this 
method does handle large uniform regions in images better than dot diffusion or ordered dithering. These 
aspects of the method are illustrated in Section 3. A further re.nement of the space-.lling traversal 
method is to use small standard patterns to replace segments of the halftoned image [8]. This can be 
used to compensate for some problems common to laser printing involving interaction between neighboring 
dots on a page of paper. This method was also investigated in Section 3. Its main drawback is that it 
blurs the image. In this paper, we propose a parallel mechanism for the halftoning techniques using space-.lling 
curves. Speci.cally, discrete Hilbert curves are used to subdivide the image into regions. Other space­.lling 
curves could be used as well. A strategy similar to dot diffusion is used to halftone the resulting regions 
simultaneously. This method avoids the dif.culties with both the dot diffusion and the above space-.lling 
curve halftoning techniques. 7 7 21 22 25 26 37 38 41 42 6 6 20 23 24 27 36 39 40 43 5 5 19 18 29 28 
35 34 45 44 4 4 16 17 30 31 32 33 46 47 3 3 15 12 11 10 53 52 51 48 2 2 14 13 8 9 54 55 50 49 1 11 2 
7 6 57 56 61 62 0 00 3 4 5 58 59 60 63 01234567 01234567 ( a ) ( b ) 9 5 9 10 7 15 11 15 12 11 64 7 
11 7 11 614 13 9 14 13 9 2 13 3 12 51 2 7 12 7 40 1 15 1 14 15 40 3 4 5 5 n n 357 1535 126 814 101012 
045 264 8133 1212 118 10106 0 3 48 10610 11 0 315 15 12 11 2 14 13 8 9 6 7 2 1 214 13 8 9 14 13 8 9 11 
2 7 6 9 8 13 14 11 2 7 6 12 7 6 00 3 4 5 10 11 15 00 m 12 n n g 34 m n 50 m3 4 m 5 01234567 01234567 
( c ) ( d ) Figure 1: A 8x8 Hilbert polygon and its corresponding subdivisions.  2 SPACE DIFFUSION In 
our method, the gray image is subdivided into small regions along a space-.lling curve. Then, the order 
of pixels along the segment that traverses a given region is used to assign the pixels of that region 
to their appropriate classes. Finally, a strategy similar to the dot diffusion technique is used to halftone 
these classes of pixels simultaneously. Figure 1a shows a discrete Hilbert curve covering an 88 square 
region. Such curves can be easily generated to cover any 22region. The particular curve in Figure 1a 
de.nes a traversal ordering of the 88 image as shown in Figure 1b. This ordering is derived by labeling 
the pixel at one end of the curve by 0 and then incrementally labelling each of the remaining pixels 
as one moves along the curve until the last pixel is reached which is labeled 63. This image can be subdivided 
into 44 regions by replacing each label by its value mod16 ( 44 ) as shown in Figure 1c. In general, 
we could break any 2 2image up into 22regions by .rst labeling each pixel of the region by its Hilbert 
curve ordering and then reducing these labels mod 22. Within each of these regions, the pixels are assigned 
classes whose numbers are integer values from 0 to 22. These class assignments can then be used as the 
basis of a dot diffusion process. However, it differs from the traditional dot diffusion setup in that 
the matrices are not all oriented in the same manner. Figure 1d shows what the class assignments might 
be if we had attempted to replicate the initial 44 matrix of Figure 1c, rather than allowing the Hilbert 
curve to  re-orient each of the matrices. The class assignments shown in Figure 1d can be viewed as 
the basis of a standard dot diffusion process using a slightly different base matrix. Indeed, if we were 
breaking an image into 88 regions, then we would end up with matrices similar to that of Figure 1b. If 
we compare that matrix of class assignments to those in the matrix given for dot diffusion in Section 
1, we see that matrix de.ned by the space .lling curve does not distribute the classes as well as the 
matrix given in the original presentation of the dot diffusion method [6]. However, the defects of this 
matrix are more than compensated for by the result of constantly re-orienting it along a space-.lling 
path as illustrated by the experimental results in Section 3. It is worthwhile noting that determining 
a good matrix when the orientation of the matrix is kept constantly the same was a particularly dif.cult 
aspect of the development of the original dot diffusion method. The method based on this restructuring 
of the matrix orientations we call space diffusion. Although the example in Figure 1 presents the method 
in terms of square matrices being re-oriented, unlike dot diffusion, there is no need to break the image 
up into square regions. We can make the class assignments by reducing the original Hilbert labels by 
mod for any value of rather than restricting k k ourselves to values of that are squares of powers of 
2. However, in using these odd shaped decompositions, we haven t observedany signi.cant differences in 
image quality from when we used squares of powers of two as the basis for class assignments. The details 
of error distribution are exactly the same as those presented in Knuth s paper [6]. As with dot diffusion, 
the error distribution process is inherently parallel since information does not pass between pixels 
in different regions. 3 EXPERIMENTAL RESULTS By combining space .lling traversals with diffusion of the 
error term, we hope to gain the sharper detail characteristic of methods such as error diffusion and 
dot diffusion while taken advantage of other space .lling methods superior handling of large uniform 
regions (i.e., areas lacking sharp details). However, to know if this goal is actually accomplished, 
it is necessary to evaluate the method on actual grey images. Figures 2 through 13 show a good image 
for comparing the various methods in that these .gures combine both .nely detailed regions with large 
uniform regions. The scene of these .gures is a couple of boats in front of a light house. The rigging 
of the boats provides the .ne detail and the hull of the boat provides the main example of a large uniform 
region although the sky is also of interest in this regard. The image is 256 256. Traditional edge enhancement 
has been applied to the original image since that improves the results of all methods in Figures 2 through 
7. Figures 8 through 13 illustrate the results on the un-enhanced version of the same images. In the 
un-enhanced images, most of the .ne detail is lost by all methods. Figures 2 through 7 illustrate each 
of the 6 digital halftoning techniques described in this paper. Figure 2 was done using the space diffusion 
method described in Section 2 where the number of distinct classes assigned was 36. The signi.cant .ne 
detail to observe occurs in two places: 1) the rigging to the immediate right of the lighthouse (which 
is itself midway down the left-hand side of the image) and 2) the curved rigging immediately above the 
cabin of the boat on the right-hand side of the image. Figure 3 was done using dot diffusion. Figure 
4 was done using error diffusion. Both of these images preserve this .ne detail. However, Figure 5, which 
was done using the traditional space .lling curve, can be seen to have lost the sharpness of the details 
in these areas. Figure 6 was done using the pattern based version of the space .lling method and shows 
signi.cant loss of .ne detail. Figure 7 was done by ordered dithering and seems to have a similar loss 
of .ne detail to the traditional space .lling method. Looking at the large uniform areas in each of these 
images, we observe that the ships hull in the dot diffusion image and in the ordered dither image show 
signi.cant regular patterning. In the error diffusion method, there appears to be a fairly regular grain 
to the patterning underlying the sky portion of the image. This patterning is not evident in the space 
diffusion image. This kind   Figure 6: Space-.lling curve method using patterns of up to 8 pixels 
Figure 4: Error diffusion method on enhanced image of boat scene on enhanced image of boat scene  Figure 
5: Space-.lling curve method using run summation on en- Figure 7: Ordered dithering method using 8x8 
patterns on enhanced hanced image of boat scene image of boat scene  Figure 8: Space diffusion method 
on un-enhanced image of boat scene scene  Figure9: Dotdiffusionmethodonun-enhancedimageofboatscene un-enhanced 
image of boat scene  of regular patterning is more evident in pictures with larger more uniform regions. 
Figure 14 shows such an image. Figure 14 presents the same six methods as used to create the images of 
Figure 2 through 7, but this time the underlying grey image is a simple rectangle with a constantly increasing 
intensity moving from right to left. The top image was produced using space diffusion. The image immediately 
below it is the result of dot diffusion and the regular patterning is quite evident. Below that is an 
image produced by error diffusion. As one moves across this image, one encounters many grey regions where 
there is a regular bias in the patterning. Below the error diffusion image is the traditional space-.lling 
curve method. On this image it produces very much the same result as the space diffusion method. Below 
the space-.lling image is an image produced by ordered dithering that produces strikingly regular patterning 
artifacts. The bottom image was produced using the patterning version of the space-.lling method. These 
small patterns produce very regular patterning at some of the grey intensity values (for example, near 
the middle of the rectangle). In summary, experimental results indicate that the space diffu­sion method 
is similar in quality to both the error diffusion method and the traditional space-.lling curve method. 
However, each of these methods as a slight weaknessin comparison to the space diffu­sion method (the 
error diffusion method is worse on large uniform regions and the space-.lling method is worse on .ne 
detail). In addition to this, both of these methods are inherently serial whereas the space diffusion 
method is easily parallelizable. 4 CONCLUSION In this paper, we presented a new digital halftoning technique 
that subdivides the image into regions by subdividing a space-.lling curve over the image into segments. 
The pixels of the image are then divided into classes by using coordinate translations on these segments. 
A strategy similar to dot diffusion is used to halftone these classes of pixels simultaneously. So far, 
all the published halftoning techniques based on space­.lling curves are sequential. This paper provides 
a parallel mecha­nism for these methods. The traditional dot diffusion method uses a .xed square matrix 
to divide the pixels of the image into classes. Our method, however, allows various subdivisions of the 
image by dividing the space-.lling curve over the image into segments in different ways or using different 
space-.lling curves. In this sense, our method may serve as a generalization of the traditional dot diffusion 
method. In comparing various halftoning methods, the key problems appear to be loss of .ne detail (or 
image sharpness) on the one hand and regular patterning in large uniform regions on the other hand. The 
space diffusion method appears to be better than other known methods in both these measures although 
the difference is not always large. In general, we believe that space diffusion is a signi.cantly better 
way of parallelizing error diffusion than is the dot diffusion method.   Author/Title Index [1] Cole, 
A. J. Halftoning Without Dither or Edge Enhancement. The Visual Computer 7 (1991), 232 246. [2] Floyd, 
R. W. and Steinberg, L. An Adaptive Algorithm for Spatial Greyscale. Proceedings of the S. I. D. 17, 
2 (Second Quarter 1976), 75 77. [3] Jarvis, J. F., Judice, C. N., and Ninke, W. H. A Survey of Techniques 
for the Display of Continuous Tone Pictures on  Bilevel Displays. Computer Graphics and Image Processing 
5 (1976), 13 17. [4] Judice, C. N., Jarvis, J. F. and Ninke, W. H. Using Ordered Dither to Display Continuous 
Tone Pictures on an AC Plasma Panel. Proceeding of the S.I.D. 15, 4 (Fourth Quarter, 1974), 161 169. 
[5] Knowlton, K. and Harmon, L. Computer-Produced Grey Scales. Computer Graphics and Image Processing 
1 (1972), 1 20. [6] Knuth, D. E. Digital Halftones by Dot Diffusion. ACM Trans­actions on Graphics 6, 
4 (October 1987), 245 273. [7] Ulichney, R. Digital Halftoning. The MIT Press, Cambridge, Massachusetts, 
1988. [8] Velho, L. and de M. Gomes, J. Digital Halftoning with Space Filling Curves. Proceedings of 
SIGGRAPH 91 (Las Vegas, 28 July-2 August 1991). In Computer Graphics 25, 4 (July 1991), 81 90. [9] Witten, 
I. H. and Neal, M. Using Peano Curves for Bilevel Display of Continuous Tone Images. IEEE Computer Graphics 
and Applications (May 1982), 47 52. [10] Witten, I. H. and Wyvill, B. On the Generation and Use of Space-.lling 
Curves. Software Practice and Experience 13 (1983), 519 525. [11] Wyvill, G. and McNaughton, C. Three 
Plus Five Makes Eight: A Simpli.ed Approach to Halftoning. Scienti.c Visualization of Physical Phenomena 
(Boston, 1991), Springer-Verlag, New York, 379 392.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166157</article_id>
		<sort_key>313</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>40</seq_no>
		<title><![CDATA[An implicit formulation for precise contact modeling between flexible solids]]></title>
		<page_from>313</page_from>
		<page_to>320</page_to>
		<doi_number>10.1145/166117.166157</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166157</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P189240</person_id>
				<author_profile_id><![CDATA[81100519117]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marie-Paule]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gascuel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>165682</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[David Baraff. Dynamic simulation of non-penetrating rigid bodies. PHD Thesis, Cornell University, May 1992.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134084</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[David Baraffand Andrew Witkin. Dynamic simulation of nonpenetrating flexible bodies. Computer Graphics, 26(2):303- -308, July 1992. Proceedings of SIGGRAPH'92 (Chicago, Illinois, July 1992).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122757</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Jules Bloomenthal and Ken Shoemake. Convolution surfaces. Computer Graphics, 25(4):251--256, July 1991. Proceedings of SIGGRAPH'91 (Las Vegas, Nevada, July 1991).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91427</ref_obj_id>
				<ref_obj_pid>91394</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Jules Bloomenthal and Brian Wyvill. Interactive techniques for implicit modeling. Computer Graphics, 24(2): 109-- 116, March 1990.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Marie-Paule Gascuel, Anne Verroust, and Claude Puech. A modeling system for complex deformable bodies suited to animation and collision processing. Journal of Visualization and Computer Animation, 2(3), August 1991. A shorter version of this paper appeared in Graphics Interface' 91.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74335</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Jean-Paul Gourret, Nadia Magnenat Thalmann, and Daniel Thalmann. Simulation of object and human skin deformations in a grasping task. Computer Graphics, 23(3):21--29, July 1989. Proceedings of SIGGRAPH'89 (Boston, MA, July 1989).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Annie Luciani, Stephane Jimenez, Olivier Raoult, Claude Cadoz, and Jean-Loup Florens. An unified view of multitude behaviour, flexibility, plasticity, and fractures: balls, bubbles and agglomerates. In IFIP WG 5.10 Working Conference, Tokyo, Japan, April 1991.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Dimitri Metaxas and Demetri Terzopoulos. Constrained deformable superquadrics and nonrigid motion tracking. In CVPR, pages 337--343. IEEE Computer Society Conference, June 1991. Lahaina, Maui, Hawaii.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378508</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Gavin Miller. The motion dynamics of snakes and worms. Computer Graphics, 22(4): 169-- 177, August 1988. Proceedings of SIGGRAPH'88 (Atlanta, August 1988).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378528</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Matthew Moore and Jane Wilhelms. Collision detection and response for computer animation. Computer Graphics, 22(4):289--298, August 1988. Proceedings of SIGGRAPH'88 (Atlanta, August 1988).]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74355</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Alex Pentland and John Williams. Good vibrations: Modal dynamics for graphics and animation. Computer Graphics, 23(3):215--222, July 1989. Proceedings of SIGGRAPH'89 (Boston, MA, July 1989).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37427</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Demetri Terzopoulos, John Platt, Alan Barr, and Kurt Fleischer. Elastically deformable models. Computer Graphics, 21(4):205--214, July 1987. Proceedings of SIGGRAPH'87 (Anaheim, California, July 1987).]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617444</ref_obj_id>
				<ref_obj_pid>616002</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Demetri Terzopoulos and Andrew Witkin. Physically based model with rigid and deformable components. IEEE Computer Graphics and Applications, pages 41--51, December 1988.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>565650</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Andrew Witkin and William Welch. Fast animation and control for non-rigid structures. Computer Graphics, 24(4):243- -252, August 1990. Proceedings of SIGGRAPH'90 (Dallas, Texas, August 1990).]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Implicit Formulation for Precise Contact Modeling between Flexible Solids . Marie-Paule Gascuel 
iMAGIS, LIENS, CNRS URA 1327 Ecole Normale Superieure, 45 rue d Ulm 75005 Paris, France Abstract This 
paper presents an implicit deformable model, based on iso­surfaces of potential fields generated by skeletons, 
that provides elegant and unified formulations for both geometric parameters such as shape or deformation 
and physical properties such as rigid­ity. The model is especially designed to improve collision and 
contact processing for non-rigid objects. In particular, it generates and maintains exact contact surfaces 
during interactions. Keywords: animation, simulation, deformation, implicit surface, collision detection, 
collision response. 1 Introduction Dynamic animation systems based on simplified physical laws have 
drawn a lot of attention during the past few years. One of the reasons why they seem so attractive is 
their ability to respond automatically to collisions. Nevertheless, contrary to rigid solid animation 
where complete analytical solutions have been found [1], modeling interactions between deformable objects 
still remains a challenge. In particular, none of the models proposed up to now generates an exact contact 
surface between interacting flexible solids. This paper presents a new, continuous model for deforma­ble 
material based on an implicit formulation which unifies the description of geometry and of physical properties 
of solids. Well adapted to the simulation of local deformations, the model is especially designed to 
improve collision and contact processing for non-rigid objects. In addition to an efficient collision 
detection mechanism, it generates and maintains exact contact surfaces during interactions. These surfaces 
are then used for the calculation of reaction forces. Compact, efficient, easy to implement and to control, 
our im­plicit deformable model would be a particularly convenient tool in character animation where locally 
deformable flesh must be simulated. r From september: iMAGIS, IMAG, BP53X 38041 Grenoble Cedex. Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct provided that the copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the commercial advantage, the ACM copyright notice and the 
title of the publication and its date appear, and notice is given that copying is by publication and 
its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy permission of the Association for Computing Machinery. otherwise, or to republish, requires 
a fee and/or specific permission. otherwise, or to republish, requires a fee and/or specific permission. 
&#38;#169;1993 ACM -0 -89791 -601 -8/93/008 $1.50 &#38;#169;1993 ----8/93/008/0015 $1.50 1.1 Previous 
approaches Flexible models in Computer Graphics result from either nodal approaches (which include finite 
elements [6], finite differences [12, 13], and systems using elementary masses [9, 7]) or global approaches 
[11, 14]. The latter optimize the animation by approxi­mating deformations by particular classesof global 
transformations. Well adapted to the animation of homogeneous blocks of elastic material, they would 
not, however, be convenient to use when simulating a material subject to local deformations (a sponge 
for in­stance), or when modeling non homogeneous complex objects like those used in character animation 
(typically, deformable coating over rigid skeletons). Collisions between flexible objects are a complex 
phenomenon. In particular, they are not instantaneous and do not conserve energy. Among the solutions 
used to cope with this problem in Computer Graphics, penalty methods [10] are probably the most widely 
spread. They don t generate any contact surface between interacting flexible solids but use instead the 
amount of local interpenetration to find a force that pushes the objects apart. A different solution 
consists in using the relative stiffnesses of solids tofindcorrectdeformedshapesincontactsituations. 
Here,response is computed by integrating deformation forces within the contact areas. But combined with 
a deformable model based on spline surfaces controlled by discrete spring systems [5], this method does 
not generate exact contact surfaces. A third approach [2] extends the analytical interaction processing 
used for rigid solids [1] to a global deformable model [14]. Contact surfaces are approximated by discrete 
sets of contact points which, as the authors emphasize, is somewhat unsatisfactory. In all these methods, 
the lack of a contact surface between in­teracting flexible solids generates local interpenetration and 
imper­fectly deformed shapes. The extent of these artifacts is exacerbated by the lasting quality of 
soft collisions, and forbids any correct evaluation of reaction forces. 1.2 Overview This paper presents 
a new deformable model which improves interaction processing for flexible solids. Our main point is the 
use of isopotential implicit surfaces generated by skeletons" to model the objects. Developed up to 
now as a tool for free form modeling [4, 3], this formalism has not been used as a way to 1 model physical 
properties. It leads to a concise formulation for both geometric parameters, such as shape and deformation, 
and physical properties, such as rigidity and elastic behavior. The deformable model is continuous and 
provides easy modeling of local deformations. The associated method for collision detection and response 
is an improved version of [5]. The inside/outside functions associated with implicit solids greatly reduces 
the computational cost of collision detection. The model generates and maintains exact contact surfaces 
between interacting objects. Opposite compression forces are respectively applied to the solids along 
contact surfaces, so a correct integration of response forces can be calculated. Section 2 describes 
the implicit deformable model, and explains how to design both homogeneous and non-homogeneous flexible 
solids. The processing of interactions is detailed in Section 3, including the particular cases of multiple 
collisions and of in­teractions with rigid solids. Section 4 discusses implementation. Section 5 focuses 
on the possibilities for future research opened by our method. 2 Implicit Deformable Solids Our aim is 
to simulate damped material where deformations due to collisions remain local. Rather than considering 
the general Lagrange equations of motion for non-rigid objects (as in [12]), we use the same approximation 
as in [13, 5]: the mass distribution of solids is considered to be constant, so motion is calculated 
using rigid body equations which lead to a more efficiently computed animation. More precisely, deformable 
solids are split into two layers: A rigid component which obeys the rigid body equations of motion. Its 
mass distribution corresponds to the object s rest shape. A deformable layer at rest relative to the 
rigid layer. This section presents a new model for the deformable layer based on implicitly defined isopotential 
surfaces generated by skeletons. We first review the definition of these surfaces. 2.1 Implicit surfaces 
Implicit surfaces such as distance surfaces" [4] and convolution surfaces" [3] allow the free form 
design of shapes through the manipulation of skeletons" that generate potential fields. Very simple 
to define and to control, they constitute a good alternative to traditional implicit surfaces defined 
by analytical equations. An implicit surface generated by a set of skeletons with associated field functions" 
is defined by: 1 d S f P 2s S 3 hf P f i 1 g f P S i X i is1 n f 1i n P where Flexible solids have been 
described by superquadrics [11, 8], another kind of implicit surface. Contrary to the approach developed 
here, the choice of an implicit geometric description of objects was not closely related to the way physical 
properties were modeled. Figure 1: Isopotential objects generated by skeletons. This surface surrounds 
the solid defined by , which can S i f i f P 1 have several disconnectedcomponents. Normal vectors are 
directed along the field s gradient. The skeletons can be any geometric primitive admitting a well defined 
distance function, such as: points, curves, parametric surfaces, or volumes. The field functions are 
monotonically de­creasing functions of the distance to the associated skeleton [4]. For convolution surfaces 
[3], they are given by integrals of exponential contributions from each point of the skeleton. In order 
to optimize the computations, these functions usually have a restricted scope of influence. Examples 
of isopotential surfaces are shown in Figure 1. An implicit surface can easily be deformed by introducing 
a deformation term in its implicit representation e.g.1 g f r P aa g r P af . In the remainder of this 
paper, only this type of deformations are considered. 2.2 Defining elastic material with potential fields 
The method is based on the following observation: the set of pointssatisfying (where is the field function) 
is sufficient   Pf P 1 ff to define a surface. This set of points being fixed, the variation of around 
the isosurface can be used to model physical properties. The next section explains how to express stiffness 
with field functions in a way which yields a very simple correspondence between applied forces and resulting 
deformations. Correspondence between forces and deformations A deformable model is defined by a correspondencebetween 
forces and deformations. In computer graphics, this correspondence has been given by both linear [13, 
6, 14] and non-linear [12] elasticity. In non-linear models, the stiffness is not only a function of 
the point you consider, but may also depend on its current location inside the solid. The applied force 
during a displacement of from X 0 f P r x 0 fy 0 fz 0 a RX Pr P a f Z r X x Xr 0 P P a f c k y r k P 
a Yfz r d P Y aa P to is: (1) To improve generality, implicit deformable solids should be capable of 
exhibiting both linear and non-linear behaviors. In practice,thecorrespondencebetweenforcesanddeformations 
will be used during the collision process, to integrate the reaction forces colinear to normal vectors 
along contact surfaces between solids. For this application, defining solids with exact elastic properties 
at each point along the principal deformation direction P (or radial direction") defined by the normal 
vector is N r P a sufficient. To express exact non-linear elasticity in radial directions, we let be 
a small radial force and the resulting small radial displacement. From equation (1) they must satisfy: 
. If we express deformations by variations in the field function, it yields:  dR r Y a dfYGradf YdY 
d Gr Y adf YdRk P k P Y r Y a dY f (2) As said previously, we want to use the way the field function 
varies inside the implicit solid to express physical properties. Let us directly model stiffness with 
the field s gradient: (3) This choice simplifies equation (2) which yields: where the normal vector remains 
constant during radial Z XX 0 P c dfY8 Y g Gr r g P Z Pad a X f X 0 fN P f r c YXP NN ar P PP aa k dR 
P R f r Y X PY0 a NY NPR RP deformations. Let be the deformation field term associated at equilibrium 
with the radial force . Here, the correspondence formula (4) becomes: (5) We use equation (5) to define 
the general correspondence between deformations and forces characterizing implicit deformable solids. 
Used with a field function satisfying (3), this correspondence gives exact elastic properties in radial 
directions, but it associates no deformation at all with forces lying in the local tangent plane. Again, 
this is not a problem since the formula will only be used for computing the radial component of compression 
forces due to collisions. Modeling stiffness with field functions Let be an object defined by a single 
skeleton and a point of this skeleton. The field function along the segment between and the closest point 
of the surface can be expressed as a function of the distance . From equation (3), the local stiffness 
at point satisfies: . f k r P r a S GrNad r P ra r f f fi P a f P 0 r rPr P r P k aa P a S Pf Gr a ld 
jj ad r PP r fr Pf f S P 0 a S r jj P f N r P a P S P (4) S But , so: The resulting geometric representation 
of stiffness (the opposite of the field function s slope) facilitates the control of the simulated material. 
The user does not need to be a specialist in mathematical physics to easily design linear and non-linear 
elastic models as those of Figure 2. The field functions currently implemented are given in Appendix 
A. Homogeneity of the solids When an object is generated by several skeletons, different field functions 
can be associated with each one allowing non­homogeneous objects to be readily designed. The object behaves 
f(P) f(P) (a) (b) 1 0 r0 R0r0 R Figure 2: Examples of field functions. (a) Linear elasticity: stiffness 
is constant during deformations. (b) Non-linear elasticity: stiffness increases during compressions. 
 according to the local stiffness in a zone influenced by a single skeleton. Otherwise, stiffness contributions 
from different skeletons blend together: kPNP Gradfk i P X Gradf i P When distance surfaces" are used,summing 
stiffness contributions in blending areas can be a problem. The stiffness may pass by a local extremum 
while varying between values associated with different skeletons. This problem corresponds to the bulge 
(or the narrowing) in shape which can appear when two fields superimpose [3]. jj P k 1 inj s1 Grad r 
f j fP a f jj i Indeed, there is no reason why should take intermediate values between the . In particular, 
homogeneous objects are not easy to model with distance surfaces. Giving the same field function to all 
the skeletons is far from sufficient. Convolution surfaces", for which field functions are integrals 
of field contributions from each point of the associated skeleton, solve this problem. With this model, 
if the same field functions are used for several neighboring skeletons there is no bulge in shape nor 
in the stiffness function, so complex homogeneous objects can be designed. More generally, stiffness 
smoothly assumes intermediate values in areas influenced by multiple skeletons as does the field s gradient. 
2.3 Animation of implicit deformable solids Implicit deformable solids are especially suitable for a 
precise mod­eling of interactions. While penalty methods directly use the degree of interpenetration 
between objects to evaluate response forces, our model completely suppresses interpenetrations by introducing 
an intermediate contact modeling" step between the detection and the response to collisions. The general 
animation algorithm is the following: At each time step, 1. Integrate the equations of motion for the 
rigid components of the solids by taking external forces and torques into account:   IA X FTmAI amaa 
F t I aam T where is the matrix of inertia of a solid computed from its rest shape. represents linear 
acceleration and angular acceleration. 2. Displace flexible components from their rest shapes. 3. Treat 
interactions between objects:  (a) Detect interpenetrations. (b) Model contact by deforming each solid 
in order to generate contact surfaces. (c) Integrate reaction and friction forces. Add them to the set 
of external actions to be applied to the rigid components at the next time step.  4. Display the objects 
with their new deformed shapes. This algorithm is used with an adaptive time step. As with penalty methods, 
overly deep interpenetrations generate overly large response forces (resulting, in the modeling contact 
phase, in excessive deformation of the objects). When this situation is detected, the system recomputes 
the objects positions using a smaller time interval. The next section details the three steps of the 
interaction pro­ cessing module and studies extensions to multiple collisions and to interactions with 
rigid implicit solids. 3 Interactions between Implicit Solids 3.1 Interpenetration detection We use axis-parallel 
bounding boxes to quickly cull most non­intersecting cases. Afterwards, we benefit from the implicit 
rep­resentation of the objects, as in [11]. For each pair of solids, sample points associated with one 
of them are tested against the inside/outside function of the other. This is done, of course, only for 
the sample points located inside the second solid s bounding box. As the list of solids interacting together 
is the only information needed for the modeling contact step, detection is stopped for a given pair of 
solids as soon as an interpenetration point is found. The method used for computing sample points at 
each time step is detailed in Section 4. As will be shown, the detection process can be optimized by 
starting detection in the neighborhood of points (if any) that most penetrated the other object during 
the last time step. 3.2 Modeling contact Once detected,an interpenetration must be suppressed by deforming 
each object according to the set of interacting solids. Deforming objects involves generating contact 
surfaces as well as modeling the transverse propagation of deformations (see Figure 3). Rather than simulating 
local interactions inside the objects, the system directly computesdeformed shapesat equilibrium using 
a model for damped propagation. Deformations outside a given propagation area", an offset of the interpenetration 
zone, are ignored. Interpenetration areas: Generating contact surfaces In addition to being a very natural 
model to express physical properties, the implicit surface formalism is also convenient for generating 
exact contact surfaces. See Figure 4. Suppose that two objects and interact locally. We are looking for 
new terms and to add to their respective field functions and in the interpenetration zone (represents 
the action of object on object ). After deformation, the objects will  f i jf j g ji iS i g ij S j g 
ji  Interpenetration zone fi = fj Propagation area for Si Solids after deformation Figure 3: Modeling 
contact consists in applying different defor­mation fields in the interpenetration zone and in the propagation 
area" associated with each solid (view in cross section). Figure 4: (left) Contact between two colliding 
objects. (right) View in cross section showing the exact contact modeling. be defined in this area by: 
The deformation fields and must be negative (they model local compression of the objects) and locally 
generate a contact surface, thus equations (6) and (7) must have common solutions. In order to give the 
new contact surface exactly the same bor­der as the interpenetration area, deformation fields must satisfy 
g ij r P af g ji r P af0 gg P gS f ji r P af f j r P af1 S i in points where . More­over, the contact 
surface generated must fit with the local rigidities of colliding objects. So opposite forces must be 
applied by the two compressed objects on each point of this surface. Adding extra skeletons to generate 
deformation field terms would be inconve­nient; Indeed, we prefer to directly use s skeleton to deform 
and vice versa. Consequently, the deformation field terms of an interpenetration area are defined by:g 
j P f j P (7) g iji 1 i (6) With this choice, all the properties needed are verified: deformation field 
terms are negative in the interpenetration zone, and generate a contact surface defined by: (8) Let be 
a point of the contact surface, be s unit normal vector, and . P 2 S i N j r P a f i f P N i r P f j 
a P N i r P a S i Let be the radial force applied by at . The correspondence (5) between forces and 
deformations yields : , so: R g i P j Pi R r P i ra PNg a ij r 1 i f P r P a jj a f R j f i r P P a R 
jj N i r ij P r P a P.N a i r P a 1 ff i jj P R i r P N a j P R i R r P i a PS f j . From equation (8), 
opposite forces are then applied by the objects along the contact surface: Deformations in propagation 
areas" We want to optimize the contact modeling process by directly com­puting deformed shapes in contact 
positions rather than simulating local interactions inside the flexible material. Designing a purely 
geometric layer is justified here: only deformations along contact surfaces will be used for computing 
response forces. The use of ge­ometric propagation will not affect the motion at all. Moreover, we wish 
to model damped material where deformations outside given propagation areas" can be neglected. Providing 
the user with a set of intuitive parameters, such as the thickness of the propagation areas around interpenetration 
zones or the way deformations are attenuated, offers a simple and efficient control of the simulated 
material. More precisely, the user controls s propagation field term p ji r P d a S i ew i Se ji p j 
S ii (due to the collision with ) through two additional param­eters in s description: A thickness value 
giving the size of the offset were deformations propagate around an interpenetration zone. Deformations 
will be neglected outside this area. An attenuation value" giving the ratio between the maximal value 
desired for and the current maximal compression term in the interpenetration area. Becauseoftheparameter,thesizeofthebulgeduetopropagation 
of deformations will first increase during a collision, while the solid is progressively compressed, 
and then decrease back to zero when the colliding objects move off. The propagation field must be positive 
within the propagation 2 p ji p ji g ji area in order to model a local expansion of the solid, compensating 
for the compression due to collision. To preserve the shape s first order continuityand its derivative 
must become zero at the exterior limit of the propagation area, and have the same value and gradient 
vector as the contact term in the border of the interpenetration zone. Let be a point within the propagation 
area, and the closest point of in s gradient direction (see Figure 3). To satisfy the conditions just 
listed, we define along the line by:a k2 a 0 w k r x P f Pe 0 Gr 0 f jj i P Gr ad a ad pp r j f ji p 
jij f P i P 0 P 0 a jj PGra k0 a ak0 ad a S 0 wj 0 w g i j i i dSdP j Pp 0 j P i PP 00 N 0 p ji where 
, is the maximal propagation value equal to times the maximal compression field value, and is the piecewise 
polynomial function shown in Fig­ure 5. An exact formula is given in Appendix B. With this choice, all 
the conditions on can be verified. Let us prove that . The method would be easy to extend to higher order 
continuity by considering constraints over higher order derivatives, and by using more complex attenuation 
functions. ak,a0,w (x) a0 x=d(P,P0) 0w Figure 5: Attenuation function defining the propagation field. 
With the value of s derivative in zero, we obtain: f i r P a a j r p a j ikj 0 w P i Gradf j P 0 Grp 
j i adPg ji P 0 e i Expansion of the objects in propagation areas must not produce new interpenetrations. 
To best avoid this situation, we insure that each deformed object does not cross the median surfaces 
of equation (see Figure 3). In other words, must be less than or equal to throughout the propagation 
area. If the problem does occur, the system truncates the propagation term and issues a warning that 
a smaller value should be chosen for . 3.3 Computation of response forces  Radial reaction forces The 
reaction forces directed along normal vectors are given by the correspondence (5) between forces and 
deformations. They are are numerically integrated along contact surfaces (this process is detailed in 
Section 4). Because of our choice for the contact surface, the principle that opposite reactions occur 
on two colliding objects is verified. Friction and damping forces To model both tangential friction 
in contact areas and damping due to the progressive compression of the solids, we include a friction 
coefficient in the description of each object. When a collision occurs, the friction and damping force 
at a point of the contact surface between and is expressed by:  V i r P c a i SF i SP i S j c V ij 
c r P j a V j SP j F i V i PPP (9) where (respectively ) is the speed of , a point on the surface of 
the solid (respectively ). Like radial reaction forces, friction forces are numerically integrated along 
contact surfaces. Figure 6 shows the action of response forces during a few steps of an animation. From 
collisions to lasting contacts The deformed shapes generated during the contact modeling step can be 
conveniently used for lasting contacts and equilibrium states, becauseopposite forces are applied to 
each side of a contact surface. In Equation (9), s tangential component represents friction due F i to 
the different tangential speeds of the solids at a contact point, while the normal component models the 
loss of energy due to the progressive deformation of the solids. The energy consumed over time enables 
colliding objects to settle into lasting contact situations, and then into resting stable states without 
unwanted oscillations. Figure 7 is an example of equilibrium state between four solids. Figure 6: Flexible 
clover falling on a quite rigid staircase. Figure 7: An equilibrium state between a rigid floor, a flexible 
vaulting horse, and two soft balls. 3.4 Multiple interactions An important benefit of our model is that, 
in multiple interaction situations such as in Figure 7, the resulting shapes and reaction forces are 
completely independent of the order in which objects, or pairs of objects, are considered. When an object 
interacts with several others, its compression field term (which produces the contact forces) is defined 
as a sum of terms due to the different collisions. No propagation term must be added in an interpenetration 
zone with another object, so, in practice, we always use a procedural method to compute field values. 
To evaluate the field generated by a deformed object at a given point : 1. Compute the initial field 
value .   PPS i f 0 1 PPSS 0 f i PPS 0 S i 2. For each object interacting with , if lies inside , 
add the contact deformation term . 3. If was not lying inside any of the , compute and sum all non-zero 
propagation terms at . Truncate this sum if  needed (as explained at the end of Section 3.2) before 
adding it to the field value. If an intersection area is detected between more than two solids as in 
Figure 8, several negative compression terms are simultaneously added in this area. This leaves a small 
space between the solids, whose shapes remain continuous (generating multiple contact points would produce 
singularities). S j S1 S3 Figure 8: Deformation of 3 intersecting solids (cross sections). 3.5 Interactions 
with rigid implicit solids Another important issue for our model is its ability to simulate interactions 
between flexible and rigid objects, like the vaulting horse and the floor in Figure 7. Suppose an interpenetration 
has been detected between a rigid solid and a flexible object . The deformation field term applied to 
in the contact area must make exactly fit s shape; i.e., f j r S P i af1 g ji r g P j a i f Pi r P aa 
1 g S ji r f P j a P f Sf 1 aii P 1 f 1 i S2 Pf i SP j 1 S i the solutions of must be points satisfying 
. Moreover, the deformation field term must be negative in the interpenetration zone given by . Then, 
we define by: The usual formula is used for the attenuation function in s propagation area. Simply, s 
gradient vector (used to define the f j fP 0 aa S ij S j 3 r Grad r f i a slope of the attenuation function) 
is now replaced by so that the bulge will exactly fit s normal vectors at the border of the contact surface. 
When this is done, response forces corresponding to s deformation are integrated along the contact surface, 
and opposite forces are applied to the rigid solidS j according to the principle that opposite reactions 
occur on two colliding objects. 4 Implementation Our modeling and animation system for implicit deformable 
solids is implemented in C++ on an SGI Indigo workstation. The cur­rent implementation uses distance 
surfaces which provide us with analytical expressions of normal vectors. 4.1 Optimizing the animation 
process One of the main problems raised by implicit isosurfaces is the search for efficient ways to discretize 
objects. The animation process uses discretizations three times by animation step: for collision detection, 
for integrating response forces,and for displaying objects. Despite recent improvements in adaptive octree 
techniques, spatial partitioning polygonizations remain quite expensive. Using this type of algorithm 
at each time step would prevent any interactive computation and display of the animation. 3 Analytical 
solutions such as those developed in [1] should be used for interactions between pairs of rigid objects. 
Fortunately, the solids only deform locally during animations and return to their rest shapes. Their 
topology never changes (otherwise, our hybrid model with its invariant matrix of inertia wouldbeinvalid). 
Consequently,theobjectsneednotbecompletely re-sampled at each time step. Before an animation is calculated, 
sample points and the associ­ated normal vectors are precomputed form the object s rest shape, and stored 
relative to the local coordinate system. Then, at each animation step: The sample points are positioned 
according to the current position and orientation of the solid s rigid component. They are used to detect 
collisions. To benefit from temporal coherence, tests for interpenetration are first performed in the 
neighborhood of points which penetrated most deeply  d P j 4 into the other object at the previous 
time step. Finally, the sample points in deformed areas are recomputed, before display, by using a linear 
search algorithm along the undeformed normal direction. The use of this direction insures that the points 
will come back to their initial positions after any deformation. To improve in efficiency, response forces 
in contact areas are integrated during this process. If a point is located in an interpenetration zone, 
the field function computes the deformation term, which is equal to the local reaction force. As soon 
as a point of the contact surface is found, this force (plus the friction force term), multiplied by 
the area of an elementary surface , is added to the sum of external  dsds actions applied to the object. 
In the current implementation,is approximated by an average value computed from the size of the discretization 
voxels. 4.2 Rendering The implicit formalism provides us with exact high level descrip­tions of deformed 
solids, even if only a few sample points are used during the computations. During animations, we directly 
save the parameters needed to compute the deformed field functions defin­ing the objects (including stiffness, 
scope of influence, attenuation parameters, and the current list of colliding objects). To give an idea 
of required disc space, the file describing the implicit objects of Figure 7 take less than 1 Kbyte, 
while the storage of the associated sample points with their normal vectors and the list of triangles 
takes more than 1200 Kbyte. Once the objects are stored, any method could be used for rendering, including 
computing polygonizations with an arbitrary precision. We currently use direct ray-tracing on implicit 
sur­faces, implemented as an extension to the public domain renderer Rayshade (by C. Kolb). If a ray 
intersects an implicit solid s bounding box, we first look for a seed point along the ray which is located 
inside the solid. If we find one, an intersection point is computed by binary search. Testing if a point 
is inside or outside is done by evaluating the solid s potential field. Normal vectors at the intersection 
points are analytically computed from the field gradient. 4 P j We use a continuation method for sampling, 
so starting detection a few points before in the list of sample points is sufficient. 5 Conclusion This 
paper presents a novel way to model deformable solids. The implicit formalism provides a compact and 
unified formulation for both geometric and physical properties, and enables to keep a continuous high 
level representation of the objects. The model offers simple and quite general control of the simulated 
material. One can experiment with field function curves to adjust stiffness variations when objects are 
compressed, or with different ways to propagate deformations due to collisions. All the parameters are 
easy to understand, even for a non-specialist. Well adapted to local deformations, the system is especially 
designed for a precise modeling of interactions. It generates exact contact surfaces between solids which 
facilitates a precise evalu­ation of reaction forces. The model applies to sudden collisions, lasting 
contacts, and equilibrium situations. It gives an elegant so­lution to the multiple collision problem, 
producing new deformed shapes and response forces which are independent of the order in which collisions 
are detected. The model can be generalized to treat interactions between flexible and rigid objects. 
During animations, discretized representations of the objects are displayed at interactive rates, while 
a compact storage of their implicit description is performed. This description, which still defines curved 
contact surfaces, is used for producing subsequent, high-quality images. Future work Implementing convolution 
surfaces [3] would facilitate the design of complex objects composed of homogeneous materials. To improve 
generality, these surfaces should be extended to non-exponential potential fields. At present, deformed 
shapes only depend on the set of external forces currently applied to the solids, so deformations disappear 
as soon as there is no longer contact between objects. Modeling visco-elastic or elasto-plastic behaviors 
in addition to pure elasticity would be a good extension. Moreover, some objects of the real world conserve 
their volume during deformations while others are partially compressible. Modeling the propagation of 
deformations according to a compressibility parameter would provide easier control. The implicit deformable 
model opens new directions for fu­ture research, particularly within the area of human simulation. Modeling 
complex objects such as deformable flesh covering rigid skeletons could not be done with previous global 
deformation tech­niques (the skeleton must not be deformed, nor the flesh on the opposite side of the 
skeleton). Nodal approaches can be used, but they demand complicated databases [6] and involve an expensive 
numerical simulation of deformations propagating in damped ma­terial. Our ability to model local deformations 
while preserving a compact continuous representation of objects, even when they are non-homogeneous, 
would be helpful. Moreover, the precise contact modeling presented here should allow human models to 
interact with the simulated world. Acknowledgements Many thanks to Jean-Dominique Gascuel for his constant 
support during this research and for implementing direct ray-tracing on implicit surfaces. Thanks to 
Philippe Limantour, Christophe Vedel, Frederic Asensio and Julien Signes for interesting early discussions, 
and to Fran0ois Sillion, Alain Chesnais, Dave Forsey, Phil Brock and Jules Bloomenthal for re-reading 
this paper. References [1] David Baraff. Dynamic simulation of non-penetrating rigid bodies. PHD Thesis, 
Cornell University, May 1992. [2] David Baraff and Andrew Witkin. Dynamic simulation of non­penetrating 
flexible bodies. Computer Graphics, 26(2):303­-308, July 1992. Proceedings of SIGGRAPH 92 (Chicago, Illinois, 
July 1992). [3] Jules Bloomenthal and Ken Shoemake. Convolution surfaces. Computer Graphics, 25(4):251--256, 
July 1991. Proceedings of SIGGRAPH 91 (Las Vegas, Nevada, July 1991). [4] Jules Bloomenthal and Brian 
Wyvill. Interactive techniques for implicit modeling. Computer Graphics, 24(2):109--116, March 1990. 
[5] Marie-Paule Gascuel, Anne Verroust, and Claude Puech. A modeling system for complex deformable bodies 
suited to animation and collision processing. Journal of Visualization and Computer Animation, 2(3), 
August 1991. A shorter version of this paper appeared in Graphics Interface 91. [6] Jean-Paul Gourret, 
Nadia Magnenat Thalmann, and Daniel Thalmann. Simulation of object and human skin deformations in a grasping 
task. Computer Graphics, 23(3):21--29, July 1989. Proceedings of SIGGRAPH 89 (Boston, MA, July 1989). 
[7] Annie Luciani, Stephane Jimenez, Olivier Raoult, Claude Cadoz, and Jean-Loup Florens. An unified 
view of multitude behaviour, flexibility, plasticity, and fractures: balls, bubbles and agglomerates. 
In IFIP WG 5.10 Working Conference, Tokyo, Japan, April 1991. [8] Dimitri Metaxas and Demetri Terzopoulos. 
Constrained de­formable superquadrics and nonrigid motion tracking. In CVPR, pages 337--343. IEEE Computer 
Society Conference, June 1991. Lahaina, Maui, Hawaii. [9] Gavin Miller. The motion dynamics of snakes 
and worms. Computer Graphics, 22(4):169--177, August 1988. Proceed­ings of SIGGRAPH 88 (Atlanta, August 
1988). [10] Matthew Moore and Jane Wilhelms. Collision detection and response for computer animation. 
Computer Graphics, 22(4):289--298,August1988. ProceedingsofSIGGRAPH 88 (Atlanta, August 1988). [11] Alex 
Pentland and John Williams. Good vibrations: Modal dynamics for graphics and animation. Computer Graphics, 
23(3):215--222, July 1989. Proceedings of SIGGRAPH 89 (Boston, MA, July 1989). [12] Demetri Terzopoulos, 
John Platt, Alan Barr, and Kurt Fleis­cher. Elastically deformable models. Computer Graphics, 21(4):205--214, 
July 1987. Proceedings of SIGGRAPH 87 (Anaheim, California, July 1987). [13] Demetri Terzopoulos and 
Andrew Witkin. Physically based model with rigid and deformable components. IEEE Computer Graphics and 
Applications, pages 41--51, December 1988. [14] Andrew Witkin and William Welch. Fast animation and con­trol 
for non-rigid structures. Computer Graphics, 24(4):243­-252, August 1990. Proceedings of SIGGRAPH 90 
(Dallas, Texas, August 1990). Appendix A. Equation for the field functions Field functions currently 
implemented are parameterized by a scope of influence , a thickness value , and a stiffness value : 
 R 2 r 0 k ifif elsewhere d p p n k n r f 0 i p P R q2 b n r 0 ar 0 p r R a 3 Rbr e 2 p a a n dr k c 
r a0 0 n e r b 0 p a R r q k 2 3 hr 0 0 2 r p r 0 c 0 r RR0 b b n k r 0 r 0 p a R2 k 1 3 ,. If linear 
elasticity is chosen, , and . If non-linear elasticity is selected, we use , ,  c 3 kr 0 h 2a1 (of course 
any other stiffness variation inside the object would be easy to implement). Appendix B. Equation for 
the attenuation function The equation we use for the attenuation function is: if if a k a 0 w cr 4 wkcr 
44 a 3 a 00 a hwdrx 23 a wwk 3 rd 2 4 x 4 3 ra 2w 0 0 w kh 2 rhw 22 wh 2 w Where and . 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166158</article_id>
		<sort_key>321</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>41</seq_no>
		<title><![CDATA[Interval methods for multi-point collisions between time-dependent curved surfaces]]></title>
		<page_from>321</page_from>
		<page_to>334</page_to>
		<doi_number>10.1145/166117.166158</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166158</url>
		<keywords>
			<kw><![CDATA[inclusion function]]></kw>
			<kw><![CDATA[interval Newton method]]></kw>
			<kw><![CDATA[interval linear equation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor>Chebyshev approximation and theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.4</cat_node>
				<descriptor>Reliability and robustness</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003636</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Approximation algorithms analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003705</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical software</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003736.10003737</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Functional analysis->Approximation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39079347</person_id>
				<author_profile_id><![CDATA[81100167784]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Snyder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P10764</person_id>
				<author_profile_id><![CDATA[81392607731]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Woodbury]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39076349</person_id>
				<author_profile_id><![CDATA[81332499016]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kurt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fleischer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P28732</person_id>
				<author_profile_id><![CDATA[81100456871]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Bena]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Currin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14034821</person_id>
				<author_profile_id><![CDATA[81100070192]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Barr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Alefeld, G., and J. Herzberger, Introduction to Interval Computations, Academic Press, New York, 1983.]]></ref_text>
				<ref_id>ALEF83</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74356</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Baraff, David, "Analytical Methods for Dynamic Simulation of Non-penetrating Rigid Bodies," Computer Graphics, 23(3), pp. 223-232, July 1989.]]></ref_text>
				<ref_id>BARA89</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97881</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Baraff, David, "Curved Surfaces and Coherence for Nonpenetrating Rigid Body Simulation," Computer Graphics, 24(4), pp. 19-28, August 1990.]]></ref_text>
				<ref_id>BARA90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122722</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Baraff, David, "Coping with Friction for Non-penetrating Rigid Body Simulation," Computer Graphics, 25(4), pp. 31-39, July 1991.]]></ref_text>
				<ref_id>BARA91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134084</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Baraff, David, and A. Witkin, "Dynamic Simulation of Non-penetrating Flexible Bodies," Computer Graphics, 26(2), pp. 303-308, July 1992.]]></ref_text>
				<ref_id>BARA92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134027</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Duff, Tom, "Interval Arithmetic and Recursive Subdivision for Implicit Functions and Constructive Solid Geometry," Computer Graphics, 26(2), July 1992, pp. 131-138.]]></ref_text>
				<ref_id>DUFF92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134085</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Metaxas, Dimitri, and D. Terzopoulos, "Dynamic Deformation of Solid Primitives with Constraints," Computer Graphics, 26(2), pp. 309-312, July 1992.]]></ref_text>
				<ref_id>META92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134082</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Mitchell, Don, and E Hanrahan, "Illumination from Curved Reflectors," Computer Graphics, 26(2), July 1992, pp. 283- 291.]]></ref_text>
				<ref_id>MITC92</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Moore, R.E., Interval Analysis, Prentice Hall, Englewood Cliffs, New Jersey, 1966.]]></ref_text>
				<ref_id>MOOR66</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1098639</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Moore, R.E., Methods and Applications of Interval Analysis, SIAM, Philadelphia.]]></ref_text>
				<ref_id>MOOR79</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Moore, R.E., "New Results on Nonlinear Systems," in Interval Mathematics 1980, Karl Nickel, ed., Academic Press, New York, 1980, pp. 165-180.]]></ref_text>
				<ref_id>MOOR80</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378528</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Moore, M. and Wilhelms, J., "Collision Detection and Response for Computer Animation," Computer Graphics, 22(4), pp. 289-298, August 1988.]]></ref_text>
				<ref_id>MOOR88</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Press, W. H., B. E Flannery, S. A. Teukolsky, and W. T. Vetterling, Numerical Recipes, Cambridge University Press, Cambridge, England, 1986.]]></ref_text>
				<ref_id>PRES86</ref_id>
			</ref>
			<ref>
				<ref_obj_id>52091</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Ratschek, H. and J. Rokne, New Computer Methods for Global Optimization, Ellis Horwood Limited, Chichester, England, 1988.]]></ref_text>
				<ref_id>RATS88</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122745</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Sclaroff, Stan, and A. Pentland, "Generalized Implicit Functions for Computer Graphics," Computer Graphics, 25(4), pp. 247-250, July 1991.]]></ref_text>
				<ref_id>SCLA91</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Six, H.W., and D. Wood, "Counting and Reporting Intersections of d-Ranges," IEEE Transactions on Computers, C-31 (3), March 1982, pp. 181-187.]]></ref_text>
				<ref_id>SIX82</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134094</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Snyder, John, and J. Kajiya, "Generative Modeling: A Symbolic System for Geometric Modeling," Computer Graphics, 26(2), pp. 369-378, July 1992.]]></ref_text>
				<ref_id>SNYD92a</ref_id>
			</ref>
			<ref>
				<ref_obj_id>130368</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Snyder, John, Generative Modeling for Computer Graphics and CAD: Symbolic Shape Design Using Interval Analysis, Academic Press, Cambridge, MA, July 1992.]]></ref_text>
				<ref_id>SNYD92b</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134024</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Snyder, John, "Interval Analysis for Computer Graphics," Computer Graphics, 26(2), pp. 121-130,July 1992.]]></ref_text>
				<ref_id>SNYD92c</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325233</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Toth, Daniel L., "On Ray Tracing Parametric Surfaces," Computer Graphics, 19(3), July 1985, pp. 171-179.]]></ref_text>
				<ref_id>TOTH85</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97883</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Von Herzen, B., A.H. Barr, and H.R. Zatz, "Geometric Collisions for Time-Dependent Parametric Surfaces," Computer Graphics, 24(4), August 1990, pp. 39-48.]]></ref_text>
				<ref_id>VONH90</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interval Methods for Multi-Point Collisions between Time-Dependent Curved Surfaces Abstract We present 
an ef.cient and robust algorithm for .nding points of collision between time-dependent parametric and 
implicit surfaces. The algorithm detects simultaneous collisions at multiple points of contact. When 
the regions of contact form curves or surfaces, it returns a .nite set of points uniformly distributed 
over each contact region. Collisions can be computed for a very general class of surfaces: those for 
which inclusion functions can be constructed. Included in this set are the familiar kinds of surfaces 
and time behaviors encountered in computer graphics. We use a new interval approach for constrained minimization 
to detect collisions, and a tangency condition to reduce the dimension­ality of the search space. These 
approaches make interval methods practical for multi-point collisions between complex surfaces. An interval 
Newton method based on the solution of the interval lin­ear equation is used to speed convergence to 
the collision time and location. This method is more ef.cient than the Krawczyk Moore iteration used 
previously in computer graphics. CR Categories: I.3.5 [Computer Graphics]: Computational Ge­ometry and 
Object Modeling; G.4 [Mathematical Software]: Relia­bility and Robustness General Terms: collision detection, 
parametric surface, con­strained minimization, interval analysis Additional Key Words: inclusion function, 
interval Newton method, interval linear equation 1 Introduction Detecting geometric collisions between 
curved, time-dependent (moving and deforming) objects is an important and dif.cult prob­lem in computer 
graphics. This paper discusses a practical and ro­bust algorithm for detecting collisions between objects 
represented as parametric or implicit surfaces. We ignore the problem of com­puting the physical response 
to collisions; much of this topic is treated in other work [BARA90,META92]. Instead, we concen­trate 
on the purely geometric problem of computing a solution set Permission to copy without fee all or part 
of this material is granted provided that the copies are not made or distributed for direct provided 
that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice 
and the title of the commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. permission of the 
Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific 
permission. otherwise, or to republish, requires a fee and/or specific permission. &#38;#169;1993 8/93/008/0015 
$1.50 &#38;#169;1993 ACM - 0 - 89791 - 601 - 8/93/008 $1.50  Figure 1: Problem Statement: Given a collection 
of time-dependent curved surfaces, .nd a set of collision points representing the contact regions. In 
this example, the dots show the points detected by the collision algorithm when a torus moves down over 
a cone, contacting it in a circle. of points where a set of time-dependent surfaces .rst contact (Fig­ure 
1). Previous work on geometric collision detection is fairly extensive, both in computer graphics and 
in other .elds such as CAD/CAM and robotics. Detection of collisions between polyhedral objects was studied 
in [MOOR88]. Baraff [BARA90] presented a method of computing collisions between parametric or implicit 
surfaces by computing extremal points using non-linear equation solvers. Sclaroff and Pentland [SCLA91] 
present a method for detecting collisions between implicit surfaces by plugging vertices of a polyhedral 
approximation of one surface into the inside-outside function of the other. Von Herzen, et. al., [VONH90] 
presented an algorithm for detecting collisions of parametric surfaces using Lip­schitz bounds. Duff 
[DUFF92] used interval methods to compute collisions between boolean combinations of implicit surfaces. 
To make collision detection practical, much of the previous work traded off accuracy and robustness for 
ef.ciency, or limited the kinds of shapes that could be handled. Polyhedral methods such as in [MOOR88], 
although fairly ef.cient, are not well suited to surfaces that deform in time. Exploiting coherence for 
rolling or sliding contact of polyhedral objects is dif.cult, and use of a .xed sampling mesh can cause 
severe approximation errors. Polyhedral methods also require many numerically dif.cult special cases 
which led [MOOR88] and [SCLA91] to neglect cases where tunneling may occur either between polygon edges 
or between small implicit surfaces passing entirely through a large polygon. Baraff [BARA90] chose to 
limit objects to the union of con­ take one or more steps in the ODE solver compute collisions in the 
resulting time interval .. if a collision occurs (at time t) in the interval compute a collision response 
reset ODE solver to t = t endif Figure 2: Computational Model for Collision Detection and Response vex 
polyhedra and strictly convex closed surfaces. This restriction simpli.ed his collision detection algorithm 
and allowed tracking of single contact points between curved objects. He did not treat non-convex surfaces 
(such as saddle shapes) and manifolds with boundary (such as half a sphere). We solve the problem for 
a more general class of surfaces with many points of contact, as shown in Figure 1. As noted in [VONH90], 
methods which depend solely on point­wise evaluations, including the above methods, cannot guarantee 
accurate collision detection. To solve this problem, Von Herzen bounded the output of functions over 
a region using a Lipschitz bound. Duff [DUFF92] used interval analysis to produce tighter bounds than 
Von Herzen s Lipschitz bound. Both of these methods used binary subdivision to search for collisions; 
we speed up the approach signi.cantly by combining binary subdivision with an interval Newton method. 
The technique we describe offers several fundamental improve­ments over previous techniques: 1. The most 
novel aspect of our technique is the ability to detect simultaneous collisions (multiple contacts at 
the same time), even when the collisions occur at a higher dimensional mani­fold of contact, rather 
than at a set of isolated points. In this case, the algorithm samples the region of contact with a .nite, 
uniformly-distributed set of points. The spatial sampling den­sity is a parameter to the algorithm. To 
our knowledge, no previous algorithm handles this situation. 2. Our technique works for both rigid and 
deforming objects, and for implicit or parametric objects. 3. Our technique is practical for computer 
graphics applications, and has been used in animations involving hundreds of objects. 4. Our technique 
includes a method (tangency constraints) to reduce the dimensionality of the space of possible solution 
points, as shown in Figure 3, dramatically speeding up the method. The tangency constraints also provide 
a square system of equations for the interval Newton method, helping us detect isolated point collisions. 
 5. Our technique uses a test for uniqueness of roots of a system of equations in a region. This test 
can be veri.ed in many cases, allowing the algorithm to terminate without further subdivision around 
collision points. 6. Our technique can be used both to compute collisions between formerly disjoint 
bodies which come into contact, or to com­pute additional points of contact between bodies as they roll 
or slide over each other (see Section 1.1).  1.1 Fitting Collision Detection into a Larger System Figure 
2 shows how collision detection .ts into a larger pro­gram for computing physical simulations of dynamic 
systems. The system is composed of three parts: the ODE (ordinary differential equation) solver module, 
the collision detection module, and the collision response computation module. The ODE solver computes 
the motions of objects over time, using equations governing the dy­namic behavior of bodies, and produces 
a functional representation of the motion.1 Motion is computed without considering collisions, so that 
the results are only valid until the next collision occurs. The collision detection module takes the 
functional representation pro­duced by the ODE solver and computes when and where the .rst collision 
occurs in the given time interval. If a collision occurs, a collision response is computed, which may 
discontinuously change the state of the system of bodies. The ODE solver continues forward in time from 
this computed collision time, discarding any state after it. Two modes of operation are required in collision 
detection: 1. compute any collisions for bodies that are initially not in con­tact 2. compute additional 
collisions for bodies that are already in continuous (rolling or sliding) contact  The algorithm described 
in this paper handles both situations. For greatest ef.ciency and modularity, we advocate handling coherence 
in the ODE solver. By coherence, we mean the tracking of contact points between bodies rolling or sliding 
over each other. In these situations, collision detection is required only to compute new points of contact 
not already tracked by the ODE solver (mode 2 above). The solver must therefore inform the collision 
detection module of the motion of the contact points it is tracking, so that these points may be excluded 
from consideration (see Eq. 7). The collision detection module must also compute the initial points of 
contact when the simulation is begun or when continuous contact begins between bodies (mode 1 above). 
 1.2 Overview The mathematics of the collision detection problem is treated in Section 2. Sections 3, 
4, and 5 discuss the constrained minimiza­tion algorithm, an interval Newton enhancement, and termination 
criteria, respectively. Section 6 presents a simple culling test which discards non-colliding surface 
pairs and tightens a bound on the collision time. The full collision algorithm, combining constrained 
minimization, the culling test, and other tools from computational geometry, is presented in Section 
7. Our technique, like all interval methods, requires inclusion functions, whose construction is sum­marized 
in Section 8. Finally, results and conclusions are described in Sections 9 and 10. Appendix A extends 
our approach to sur­faces that are piecewise smooth by adding conditions for face, edge, and vertex interactions 
(see Figure 11). Appendix B describes the construction of inclusion functions for Chebyshev polynomials. 
  2 The Collision Problem The equations that specify that two surfaces collide may be di­vided into 
two parts: a contact constraint, that speci.es that the two surfaces intersect, and a tangency constraint, 
that speci.es that the two surfaces are tangent at their point of intersection. The tan­gency constraint 
reduces the dimensionality of the space of possible collision points, as shown in Figure 3. It also allows 
faster conver­gence (using interval Newton, which we will describe in Section 4) 1In our rigid body simulations, 
the solver produces a time-varying quaternion and translation vector. Each component of the quaternion 
and vector is represented using univariate Chebyshev polynomials. contact with tangency contact without 
tangency Figure 3: Reducing the Dimensionality of the Space of Collision Points Using the Tangency Condition: 
The intersection of two bodies (like a sphere moving to the right with a stationary plane) typically 
forms a whole 2D manifoldofcontactthroughtime. Withthetangencyconstraint,thesolution space is often reduced 
to one or a few points by eliminating cases like that shown on the right. Reducing the solution space 
to an isolated space-time point is one of the ideas that makes this method practical. incoming collision 
outgoing collision Figure 4: Incoming and Outgoing Collisions: The unbroken circles repre­sent bodies 
later in time. A dot represents the collision point; the arrows represent the direction of movement. 
and robust testing of isolated collisions (using an interval solution uniqueness test described in Section 
5). We also distinguish between incoming collisions, in which the surfaces collide by moving closer to 
each other, and outgoing colli­sions, in which the surfaces are interpenetrating and become tangent as 
they move apart. These situations are compared in Figure 4. The distinction is necessary in the simulation 
of dynamic systems where each surface encloses a solid. Eliminating outgoing collisions al­lows the simulator 
to ignore collisions which were previously de­tected; i.e., collisions between surfaces already in contact 
which are moving away as a response to the collision. 2.1 Parametric Surfaces Let two deforming parametric 
surfaces be represented by the u twice-differentiable mappings S1(u1v1t) and S2(u2v2t), where Si: R3 
R3. At a particular instant of time, each of the surfaces is formed by the image of Si over a rectangle 
in (uivi) space.2 In this section, we consider the case of collisions between solids each bounded by 
a single, smooth, closed parametric surface. Ap­pendix A generalizes the discussion to parametric surfaces 
which are only piecewise smooth. Contact Constraint The contact constraint merely states that the two 
surfaces intersect (i.e., the vector difference of the two surfaces 2Using a rectangular domain for parametric 
surfaces does not limit the kinds of surfaces that can be collided. Parametric surfaces de.ned on non-rectangular 
domains can be handled by mapping a rectangle into the required non-rectangulardomain before mapping 
onto the surface [SNYD92b]. is the zero vector): S1(u1v1t)S2(u2v2t)=0(1) Tangency Constraint The tangency 
constraint implies that the instantaneous normal vectors on the two surfaces at their point of contact 
are anti-parallel. Stated another way, the (uv) tangent vectors on one surface must be perpendicular 
to the instantaneous normal vector on the other surface. We thus have the following system of two equations3 
(u1v1t) N2(u2v2t)   B  A u1 =0 (2)S1 0s  nn 1C (u1v1t)N2(u2v2t) where N1 and N2 are the outward 
normal vectors to the surfaces S1 and S2, respectively, given by SiSi N Ni(uivit)(uivit)(uivit) for 
i =12ui vi The algorithms that follow here assume that N1 and N2 are nowhere 0; that is, surfaces have 
a nonvanishing normal vector everywhere and for all relevant time.4 The whole collision equality constraint 
is given by a nonlinear system of 5 equations in 5 variables, three from Eq. 1 and two from Eq. 2. Incoming 
Constraint The incoming collision condition states that the relative velocity of the collision point 
must face the same way as the surface normal (the two vectors must form an acute angle),5 and the two 
normals must face in opposite directions (forming an obtuse angle). This condition yields two inequality 
constraints: S1S2 Sv 11  n n a a ((u1v1t)(u2v2t))N1(u1v1t)0 tt (3) andN1(u1v1t)N2(u2v2t)0 2.1.1 
Example: Rigid Parametric Surfaces The above constraints may be applied to the special case of rigid 
parametric surfaces. In this case, we have two time-independent surfaces s1(u1v1) and s2(u1v1). The time-varying 
version of these surfaces is given by  Si(uivit)Ri(t) si(u1v1) + Ti(t) for i =12 where Ri(t) is a time-varying 
rotation matrix and Ti(t) is a time­varying translation vector, specifying the trajectory of surface 
i s coordinate origin. Contact Constraint The contact constraint may be expressed as R1(t) s1(u1v1) 
+ T1(t)R2(t) s2(u2v2)T2(t)=0 3A similar, though functionally dependent, constraint may be derived by 
switching S1 and S2. 4If the calculated normal vector becomes zero, such as at the poles of a parametric 
sphere, the tangency constraint becomes trivially true. The algorithm will therefore rely on the contact 
constraint to detect a collision in this case. 5We assume here that the surfaces are parameterized so 
that the normals N1 and N2 face outward. Tangency Constraint Let n1(u1 v1) and n2(u2 v2) be the time­independent 
normals of the surfaces s1 and s2, given by si vi) N ni(ui vi)(ui si (ui vi)ui vi The time-varying surface 
normals can therefore be expressed as Ni(ui vit) Ri(t) ni(ui vi) since Ri(t) is a rotation matrix. The 
tangency constraint is then given by 0 n 1 (R1(t) s1(u1 v1)) N2(u2 v2 t) B n A e u1 =0 s C (R1(t) 
s1(u1 v1)) N2(u2 v2 t) u2 Incoming Constraint The incoming constraint is given by R1(t) s1(u1 v1) +T1(t)R2(t) 
s2(u2 v2) a U n n aa T2(t) N1(u1 v1 t)0 and N1(u1 v1 t) N2(u2 v2 t)0 whereRi andTi are the time derivatives 
of the rotation matrix and translation vector of the two surfaces. 2.2 Implicit Surfaces Let two time-varying 
implicit surfaces be represented using the scalar functions F1(xyzt) and F2(xyzt). Points on each surface 
are de.ned as the zero-sets of these functions. Contact Constraint The contact constraint is the system 
of two equations F1(xyzt) = 0 (4) F2(xyzt) Tangency Constraint Let the function Fi(xyzt) be the spatial 
gradient of the implicit functions (i.e., with respect to x, y, and z). The tangency constraint is then 
given by  rNrr F1(xyzt) F2(xyzt)=0 (5) This constraint, although a system of three equations, contains 
only two functionally dependent equations. The entire collision equality constraint for implicit surfaces 
is thus given by a system of .ve (four functionally independent) equations in the four variables x, y, 
z, and t (Eqs. 4 and 5).6 Incoming Constraint The incoming constraint is given by F1 (xyzt) F2(xyzt) 
t F2 (6) (xyzt) F1(xyzt)0 t rkrkrnr kkaa and F1(xyzt) F2(xyzt)0 6We note that detecting collisions 
between implicit and parametric surfaces is a simpler problem than colliding pairs of parametric or implicit 
surfaces. By substituting the output of the parametric surface as the input ( xyz) of the implicit surface, 
a system in 3 variables, (uvt), results, where u and v are the parametric surface coordinates. Figure 
5: Simultaneous Collisions The tori collide at two isolated points. We also note that CSG operations 
on implicit surfaces, as in [DUFF92], can be handled very ef.ciently using our techniques. Assume the 
surface F1 is represented as the boolean subtraction of two simple implicit surfaces Fa(xyzt) and Fb(xyzt). 
The .rst equation in the contact constraint (Eq. 4) then becomes (Fa = 0 and Fb 0) or (Fb = 0 and Fa 
0) a assuming implicit surface functions are positive outside the surface they represent. Similar restricted 
equality constraints can be derived for the tangency constraint. Constraints for rigid motion of implicit 
surfaces are easily de­rived by applying the above general equations to the rigidly moving implicit surface 
F(xyzt) f (w) where w RT (t)((xyz)TT(t)) where f : R3 R is the implicit equation of the time-independent 
u surface, RT (t) is the transpose of the time-varying rotation matrix, and T(t) is the time-varying 
translation vector. 2.3 Collision As a Constrained Minimization Problem The .nal collision constraint 
may be described as an equality con­straint involving a function C (for the contact and tangency con­straints) 
and a logical composition of inequality constraints in­volving a function D (for the incoming constraint). 
For colli­sions between parametric surfaces, C and D are vector functions of (u1 v1 u2 v2 t) (equations 
1 3); for implicit surfaces they are functions of (xyzt) (equations 4 6). We are interested only in the 
minimum t collision, since our representation for the time behavior of the surfaces may be invalid after 
this time. The desired collision time for parametric surfaces, t, can there­fore be expressed using the 
constrained minimization problem C(u1 v1 u2 v2 t)=0 and minimum t (u1 v1u2v2 t)X0 D(u1 v1 u2 v2 t)0 wwww 
2 c ja .. r A similar statement results for detection of collisions between im­plicit surfaces. We would 
like to compute tor detect that the constraint is satis.ed nowhere in the parameter space X0. We also 
need the location of the collision and the surface normal vectors there. There may be multiple points 
of contact at the time of collision, which we call simultaneous collisions, as shown in Figure 5. The 
points of contact may be a .nite number of isolated points as in Figure 5, or they may form a curve or 
surface, called the contact manifold. For example, if the falling torus from Figure 5 were in the same 
orientation as the stationary one at the bottom, the collision points would form a circle. To detect 
simultaneous collisions, we need to detect minimum t solutions which are simultaneous or simultaneous 
within some tolerance. Mathematically, we require the set of collision points (u1 v1 u2 v2) such that 
  ... o .... ............. a . C(u1 v1 u2 v2 t)=0 and D(u1 v1 u2 v2 t)0 For a given collision point, 
the location of the collision, p, is pS1(u1 v1 t) S2(u2 v2 t) The normal vectors at the collision may 
be de.ned similarly by evaluating N1 and N2 at the collision point. To compute a collision among a set 
of N time-varying parametric surfaces Si(ui vi t), we .rst use simple culling procedures to exclude pairs 
of surfaces which can t collide. Section 3.2 will discuss a method of solving these sets of constrained 
minimization problems which can compute simultaneous collisions and which does not . spend undue computation 
on collisions which occur after t. It returns the collision points when these occur at a .nite set of 
isolated points, or a .nite subset of the collision points uniformly distributed over the contact manifold. 
3 Interval Tools for Computing Collisions We now turn to a discussion of the interval tools necessary 
to solve the sets of constrained minimization problems that arise in colli­sions. 3.1 Review of Interval 
Analysis An interval, A =[ab], is a closed subset of R de.ned as [ab] x a x bxab R f lb jii2g The lower 
and upper bounds of an interval are written as[ab] a [ab] b A vector-valued interval of dimension n, 
A =(A1 A2 An), isa subset of Rn de.ned as  f midwub j2 wmid ... g ... A x xi Aii =1 2 n where each Ai 
is an interval. An interval Ai that is a component of a vector-valued interval is called a coordinate 
interval of A. The width of an interval, written ([ab]), is de.ned by ([ab]) ba The midpoint of an interval, 
written ([ab]), is de.ned by a + b ([ab]) 2 Similarly, the width and midpoint of a vector-valued interval 
of dimension n, A, are de.ned as n (Ai) (A) = max i=1 midwmidwmid ... mid (A)=((A1) (A2) (An)) Hereafter, 
we will use the term interval to refer to both intervals and vector-valued intervals; the distinction 
will be clear from the context. An inclusion function for a function f , written f , produces an 2 interval 
bound on the output of f over an interval representing its input domain. Mathematically, for all intervals 
X in the domain of f , if a point x is in the input interval X then f (x) is contained in the output 
interval f (X); i.e., xX f (x) f (X) for all xX 2 u2 2 2 Much more information about inclusion functions 
and their prop­erties can be found in the literature (see, for example, [MOOR79, ALEF83,RATS88]). Section 
8 and the Appendices discuss ways to create inclusion functions given the functions they are to bound. 
3.2 Constrained Minimization Algorithm The constrained minimization problem involves .nding the global 
minimizers7 of an objective function f : Rn R for all points that ufgu satisfy a constraint function 
F: Rn 01. For the caseof computing collisions between parametric surfaces, we have the following variables, 
objective function, and constraint function: x (u1 v1 u2 v2 t) f (x) t F(x)(C(u1 v1 u2 v2 t) = 0) and 
(D(u1 v1 u2 v2 t) 0) a A region in the minimization algorithm is a 5D interval vector of the form X (U1 
V1 U2 V2 T)  aUaUaUaUaU ( u1 lu1 uv1 lv1 uu2 l uu 2 v2 l vu 2 tl tu ) where the superscripts l and 
u denote lower and upper bounds. The relevant inclusion functions are8 l tu f (X) t 2 22 a U lb 2 ub 
2 i ub a 2 a 01if C(X)0 C(X)0 F(X)and D(X)00 0otherwise where C is an inclusion function for the collision 
equality con­straint C, and D is an inclusion function for the incoming inequal­ity constraint D. The 
algorithm in Figure 6 .nds solutions to the constrained min­imization problem in a speci.ed region X0. 
The algorithm uses a priority queue to order regions based on the upper bound of the objective function. 
Regions bounding the set of global minimizers 7The global minimizers are the domain points at which the 
global minimum of the objective function is achieved, subject to the constraints. 8The terminology C(X) 
0 denotes that Ci (x) 0 for each component 2 lb 2 h lb 2 h interval i of C(X) (and similarly for upper 
bounds). Minimize(f ,F,A,X0 ,d,,) 222 x . xx1. place initial region X0 on priority queue L initialize 
f s upper bound u + initialize solution setS initialize singular solution setS while L is nonempty get 
next region Y from L if lb f (Y)u +discard Y else if lb f (Y)uand there exists Si S such that d(Y)d(Si) 
 22  k 2 k then discard Y else if Y satis.es acceptance criteria A then add Y to solution list S if 
Y doesn t contain a unique feasible point add Y to S endif umin(uub f (Y)) delete from S and S all Si 
lb f (Si)u + else subdivide Y into regions Y1 and Y2 x2 2 f 2 YY 2 2 g Y 3 2 2 for Yi Y1Y2 evaluate 
F on Yi if F(Yi )=[00] discard Yi evaluate f on Yi if lb f (Yi )u +discard Yi insert Yi into L according 
to ubf (Yi ) endfor endif endwhile Figure 6: Global Constrained Minimization Algorithm: This algorithm 
.nds the global minimizers of an objective function f , with constraints F, acceptance criteria A, initial 
region X0, solution distance mapping function d, simultaneity threshold, and solution separation distance. 
are subdivided until they are rejected or satisfy the acceptance cri­teria, A, and are accepted as solutions. 
It halts with an empty list of solutions if there are no solutions to the constraint function in X0, 
or a list of regions, S, representing the set of global minimizers of the constrained minimization problem. 
The variable u is a progressively re.ned least upper bound for the global minimum of the objective function. 
If we were only looking for a single collision point, we could halt the algorithm immediately after .nding 
the .rst solution. To .nd collisions at multiple points of contact, the algorithm must be continued until 
the priority queue is empty. The variable u helps to prune the search after .nding the .rst solutions. 
Selecting Finite Sets of Points from Contact Manifolds The parameters ,, and d allow the algorithm to 
select a .nite set of o 2 o regions distributed uniformly within the set of global minimizers, when 
this set is not .nite. The parameter is the simultaneity threshold, which speci.es how close the value 
of the objective function must be for two points to be considered global minimizers. For collisions, 
speci.es how close in time two events must be in o ordertobeconsideredsimultaneous. Theparameteristhe 
solution separation distance, which speci.es how far apart two accepted regions must be to be accepted 
as separate solutions. The parameterd is an inclusion function for the mapping which takes points in 
parameter space to points in whatever space we desire distances to be compared. We call the function 
d the solution distance mapping function. As the algorithm progresses, it maintains two solution lists, 
S and S. S contains all accepted regions. We call S the singular solution set. The elements of S not 
in S are regions in which the existence if lb f (Y) u and there exists Si S such that d(Y) d(Si)then 
discard Y check that the region Y is not too close to regions already accu­2 . k 2 o lb 22 . ko 2 mulated 
onto S. Note that the test f (Y) u is critical to ensure that Y doesn t have an objective function value 
small enough to invalidate all the currently accepted regions.9 We use two lists, S and S, so that in 
the case that the global minimizers form a .nite set of points, the algorithm can .nd all such points 
without discarding some based on distance to those already found. The algorithm is therefore able to 
resolve multiple isolated collisions that happen in a small area, regardless of the 10 value of. Ordering 
Based on Upper Bounds Constrained minimization algorithms that have appeared before [RATS88,SNYD92c] 
order regions based on the lower bound of the objective function. We use the upper bound to make tractable 
computing solutions on a contact manifold. At any time, the union of all regions on the priority queue 
forms a bound on the set of global minimizers of the constrained minimiza­tion problem. As the algorithm 
progresses, regions are subdivided or rejected, so that the regions which remain on the priority queue 
become a tighter bound on this set. Because of the inclusion mono­tonicity property of inclusion functions,11 
as regions on the queue shrink, the computed lower bound on the objective function tends to increase 
and the upper bound tends to decrease. Assume the set of global minimizers forms a continuousmanifold 
rather than a .nite collection of isolated points, as shown Figure 1. If the priority queue is ordered 
using lower bounds, when a given region is subdivided, its children will generally have larger lower 
bounds for the objective function, and will be placed in the priority queue behind less highly subdivided 
regions. A breadth .rst traver­sal tends to result, with less highly subdivided regions examined .rst. 
If we have a whole manifold of global minimizers and strin­gent acceptance criteria, we will have to 
compute a huge number of tiny regions bounding the entire solution manifold before even the .rst region 
is accepted as a solution. By ordering based on the upper bound, more highly subdivided regions tend 
to be examined .rst because they tend to have smaller upper bounds. We quickly get to a region which 
is small enough to satisfy the acceptance criteria. This allows our upper bound u to be updated. It also 
allows regions to be accumulated onto our singular list S. Regions that are too close to any member of 
S can then be eliminated, making it possible to .nd a distribution of points on the contact manifold 
without undue computation. Acceptance Criteria The constraint inclusion function, F(X), 2 because it 
contains an equality constraint, returns either [0 0] (i.e., the constraint is satis.ed nowhere in X) 
or [0 1] (i.e., the constraint 9If a region can possibly have a feasible point with a value of f less 
than from the s value of f in regions on S, we should not reject it just because it is close with respect 
to the function d to these regions. The algorithm might then discard a global minimizer because of its 
closeness to regions which are possibly far from the global minimizer, in terms of bounds on the objective 
function. 10One problem with this technique is that if the collisions happened at a contact manifold 
and a .nite number of additional isolated points, the algorithm may discard some of the isolated points 
because of the closeness criterion. We consider this problem minorsincetheset ofglobalminimizersis in.niteandthealgorithmmustchoseasubset 
anyway. 2 ef 2 e 2 11An inclusion function, f , is inclusion monotonic if YXf (Y) f (X). In practice, 
the standard ways of constructing inclusion functions generate inclusion of a unique feasible point 
has been veri.ed. The statements monotonic inclusion functions. may be satis.ed in X). We must resort 
to other means to determine if the constraint is actually satis.ed. Section 5 discusses conditions which 
guarantee that a region contains a unique solution to the equality constraints. These conditions can 
therefore be used as acceptance criteria in the algorithm, which we call the isolated point acceptance 
criteria. They also allow the upper bound u to be updated via u min(uf (Y)) . ub 2 since Y is guaranteed 
to contain a feasible point. The algorithm also makes use of emergency acceptance criteria which do not 
guarantee a unique solution but are guaranteed to be satis.ed for regions of small enough size.12 The 
simplest such criterion is (X) ; a better one is (S(X)) where S is the w o w 2 o 2 parametric mapping 
of one of the colliding surfaces. Regions which are accepted via the emergency acceptance criteria are 
inserted both onto the list of solutions, S, and the singular solutions, S. Subdivision The simplest 
method of subdividing candidate in­tervals in the minimization algorithm is bisection, in which two intervals 
are created by subdividing one of the input dimensions at its midpoint. Many methods can be used to select 
which dimension to subdivide. For example, we can simply pick the dimension of greatest width. A better 
alternative is to scale the parametric width by some measure of its importance to the problem we are 
solving. For each variable in the collision problem xi, and a given candidate region X, we have used 
a scaling value si de.ned by m fj fj si max((X)(X)) X j lb 2 jj ub 2 k xi xi j=1 Here, f refers to 
the equality constraints (contact and tangency) of the collision problem. We then pick a dimension to 
subdivide, i, such that the scaled width si (Xi) is largest. Given a candidate interval, techniques also 
exist which allow us to compute a smaller interval which can possibly contain feasi­ble points of the 
constraint. These methods and how they can be added to our simple minimization algorithm are discussed 
in Sec­tion 4. Even more sophisticated subdivision methods exist, such as Hansen s method which involves 
accumulating gaps inside candi­date intervals by using in.nite interval division (see [RATS88] for a 
full description). Multiple Element Constrained Minimization The algorithm can easily be modi.ed to accept 
an array of sets of minimization pa­ 22 . w rameters (f FAX0)i . This allows simultaneous solution of 
sets of problems from different pairs of surfaces, or different tangency situations for the same pair 
of piecewise parametric surfaces. As a result, computation is not wasted on collisions which happen after 
the .rst collision, tt. We call this modi.ed constrained mini­mization algorithm the multiple element 
constrained minimization algorithm. Sets of minimization subproblems may be implemented by asso­ciating 
the array index of the appropriate minimization subproblem with each region inserted onto the priority 
queue, and using the ap­propriate indexed inclusion functions and acceptance criteria when processing 
the region. 12Although we cannot guarantee a region X contains a solution, we can guarantee that it is 
arbitrarily close, in the sense that w(C(X))where C is an inclusion 2 bs 2 function for the collision 
equality constraint function C. Avoiding Detection of Tracked Points We can add additional inequality 
constraints to the constraint function F in order to avoid detecting collisions which occur at contact 
points already being tracked. If p is such a tracked point on a surface S(uvt), the ODE solver computes 
a trajectory for p = S(u(t) v(t) t). We then dis­card all global minimizers to the constrained minimization 
problem which satisfy  l k ddd k d l dd S(uvt) S(u(t) v(t) t)(7) where is a constant chosen by the user. 
The functions u and v have known representations, as computed by the solver. A natural interval extension 
of this constraint involving an inclusion function for S is then included in the constraint inclusion 
F. An additional constraint is added for each tracked point.  4 Interval Newton Methods In order to 
more quickly re.ne our intervals towards the solutions of the collision equality constraint C = 0, we 
make use of an interval Newton method. Interval Newton methods are applicable to the general problem 
of .nding zeroes of a differentiable function f : Rn Rm in an interval X Rn. They allow us to .nd an 
interval bound on the set u T . f 0 t2 . tjg 2 X= x Xf (x)=0 Let Z(X) be such a bound (i.e., XZ(X)). 
We can reduce the size of our candidate region X by13 X= XZ(X) In particular, Z(X) X =implies that X 
contains no solutions. We call the operator Z(X) the interval Newton operator. Since X can only decrease 
in size after it is intersected with Z(X), this procedure can be applied iteratively to produce smaller 
and smaller regions, as in  Xi+1 =(XiZ(Xi)) Note however that a smaller region is not necessarily produced. 
Interval Newton methods should therefore be combined with bisec­tion. When interval Newton iteration 
is effective at reducing the size of X its use is continued. Otherwise, bisection subdivision is performed. 
The following sections present three methods for computing Z(X): use of the Krawczyk-Moore form (Section 
4.1) use of the interval inverse (Section 4.2.1) use of matrix iteration (Section 4.2.2) In each case, 
we modify the constrained minimization algorithm from Figure 6 by replacing the subdivide step with the 
code shown in Figure 7. 4.1 Fixed Point Methods: the Krawczyk Moore Form The familiar (point-wise) Newton 
s method is used to converge on the solution to a system of equations f (x) = 0 where f : Rn Rn . T u 
 13A B denotes the interval formed by the intersection of the intervals A and B. Newton(Y) compute interval 
Newton step onY if step succeeds then YYZ(Y)   0 x 0 T 00 if Y= , discard Y (proceed with next region) 
 00 else if Yis suf.ciently smaller than Y, insert Yinto L (proceed with next region) else subdivide 
Y(continue with Y replaced by Y) else subdivide Y (continue with Y) Figure 7: Interval Newton Modi.cation 
to the Constrained Minimization Algorithm: The above algorithm replaces the subdivide step in the algo­rithm 
of Figure 6. Themethodstartswith aninitial guess at asolutionx0 and iterates via xi+1 = p(xi) where p(x)= 
x Yf (x) Y is a nonsingular nn matrix which in straightforward Newton s N method is the inverse of the 
Jacobian matrix of f at x, i.e. YJ1(x) Under certain conditions, this iterative procedure converges 
to a .xed point x. If convergence is achieved, then the .xed point xis a solution since p(x )= xf (x 
)=0 because Y is nonsingular. An interval analog of this method may be developed. Let X be an interval 
in Rn in which zeroes of f are sought. We require a bound on X. But XxXf (x)=0 . 2 .. f 2 . j . xu g 
t . 2 . = x Xp(x)= x(p(X) X) where p(X) is an inclusion function for the Newton operator p(x). The Krawczyk 
Moore form, K(X cY), provides the necessary inclusion function for the Newton operator p(x). It is simply 
a mean value form for p (see [SNYD92c] for a discussion of the mean value form) given by K(XcY) c Yf 
(c)+(I YJ(X))(Xc) N 22 where I is the nn identity matrix, J(X) is an inclusion function for the Jacobian 
of f evaluated on X, and c is any point in X. Note that the vector addition and subtraction and the matrix/vector 
multiplication operations used in K must be computed using interval arithmetic. We therefore have X(XK(XcY)) 
for any cX and nonsingular matrix Y. Thus, K(XcY) can be used as an interval Newton operator. Fairly 
good results can be 2 mid . t achieved with c =(X) and Y = J1(c) [TOTH85,MITC92]. 4.2 Linear Interval 
Equation Methods . A second method for .nding an interval bound on Xinvolves solving the interval analog 
of a linear equation. Let the coordinates of x be x1 x2 xn. By the Mean Value Theorem, given a cX, for 
each xX, there exist n points, 12 n such that f (x)= f (c)+ J(1 n)(xc) where the jacobian matrix J is 
given by fi ee ... ee 2 22 ee c . 2 . j e . e . 2 . 2 .. 2 . 2 e 2 . e r Jij(1 2 n)= (i) xj and where 
each iX. Let J be an inclusion function for the Jacobian matrix of f , i.e., fi J(X) J Jij (X) xj If 
x is a zero of f , then there exists JJ(X) such that f (x)=0= f (c)+ J(xc) Therefore, if Q(X) is the 
set of solutions 0 ffjj 0 2 2 2g 2 g Q(X) xf (c)+ J(xc) = 0 for some JJ(X) then Q(X) contains all zeroes 
of f in X. To compute an interval bound, Z, on Q(X), let y = xc, and let Zbe an interval bound on the 
set y Jy = f (c) for some JJ(X) Then the interval Z de.ned using interval addition as ZZ+[cc] is an interval 
bound on Q(X). Thus, computing the interval Newton bound Z can be accomplished by solving an interval 
linear equation of the form Mx = b where MJ(X) is an nn interval matrix, and bf (c) is an interval vector.14 
Stated another way, we require a bound on the set 2 fj9MN2 2M g Q(Mb) x Mb such that x =The next two 
sections discuss two methods for solving these interval linear equations. 4.2.1 Solving the Interval 
Linear Equation with the Interval Inverse One method to bound the set of solutions to the interval linear 
equation involves computing the interval inverse. We seek a bound N on Q(Mb): the set of solutions, x, 
for Mx = b. If M is an nn interval matrix, an interval that bounds Q(Mb) is 1b ZM In our research, we 
have found a different method to be superior, described in the next section. 14In this case, the interval 
b has a lower bound equal to its upper bounds in each coordinate (called a point interval), neglecting 
inaccuracies in the computation of f . where M 1 is the interval inverse of the interval matrix. Assuming 
M contains no singular matrices, the interval inverse is an interval bound on the set T 1   fj2g m 
mM A simple way of computing the interval inverse is to use the interval analog of LU decomposition. 
That is, we take the LU decomposition algorithm [PRES86, pages 31 38] and replace all arithmetic operations 
with their corresponding interval arithmetic counterparts (see [MOOR79] for a discussion of interval 
arithmetic). If at any point in the iteration we attempt to divide by an interval which contains zero, 
then we cannot compute the interval inverse, and the interval Newton step fails (but see the next section 
for a way to reduce the size of candidate regions without using the interval inverse). After enough iterations 
of the constrained minimization algorithm, and assuming the conditions discussed in Section 5 hold, candidate 
regions are usually small enough to make this technique effective. 4.2.2 Solving the Interval Linear 
Equation with Matrix Itera­tion Another method to bound the set of solutions to the interval linear equation 
involves matrix iteration. The algorithm we present here requires an initial bound on the set of solutions; 
that is it .nds a bound on the set Q(Mb) X where X is a given interval. The algorithm is therefore effective 
at reducing the size of a candidate interval in which solutions to an equality constraint are sought, 
but cannot be used to verify solution existence using the theorems in Section 5.15 Figure 8 contains 
the code for Linear Solve, which .nds bounds on the solution to the linear interval equation. Linear 
Solve is based on the observation that the i-th equation of the linear system Mx = b: Mi1x1+ Mi2x2+ + 
Minxn = bi 6 P nnn 6 implies, for each j such that Mij = 0, that bi k=j Mikxk xj = Mij The interval 
analog of this equation may therefore be used for each interval matrix entry, Mij, which does not contain 
0 to .nd a bound on one of the variables xj. This bound is intersected with the old bound on xj yielding 
an interval which is possibly smaller but no larger than it was. Reducing the size of one interval may 
then further reduce the sizes of others as the iteration proceeds. Note that the algorithm does not halt 
when an interval element of M contains 0; it just proceeds to the next element which excludes 0. An important 
property of Linear Solve is that it can be ap­pliedtoanonsquarelinearequation,16 andisthereforeusefulinthe 
overconstrained equality constraint for implicit surfaces, and the vertex-to-edge and vertex-to-vertex 
tangency situations of piece­wise parametric surfaces (see Appendix A). Linear Solve can be applied in 
many situations where LU decomposition fails because of the singularity of the interval Jacobian matrix. 
Even when the Jacobian matrix is singular at the solution point, Linear Solve is usually effective at 
reducing the widths of some of the input vari­ 15This is because, unlike the technique presented in Sections 
4.2.1, this technique T Linear Solve(M,b,x) repeat loop through rows ofM (i =12m) loop through columns 
ofM (j =12n) 62 0 x 0 TP Y 6 Y ... YY.Y ... Y if 0 Mij then x(bi kMikxk)Mij j =j . xjxxj j if xj = return 
no solution endif endloop endloop while there is suf.cient improvement inx Figure 8: Interval Linear 
Equation Solution Algorithm: This algorithm computes the interval Newton step (.rst statement of the 
algorithm in Fig­ure 6). ables. These features are critical in making the singular situations described 
in Section 5 computationally tractable.17 The suf.cient improvement condition mentioned in the algo­rithm 
can be implemented as i+1) i)  w i . w (x (x where a typical value of the improvement factor, , is 
0.9. Here xi denotes the interval bound computed after i iterations of the repeat loop. Specifying a 
maximum number of repeat iterations also limits the amount of computation. 5 Termination Criteria Two 
theorems in interval analysis specify conditions under which a square system of equations contains a 
unique solution in a region. 18 Theorem 1 (Krawczyk Moore Existence) If K(XcY) X, 66k 2 ktt K(XcY)= ,and 
I YJ(X)1, then there is a unique root in X, and pointwise Newton s method will converge to it. Theorem 
2 (Linear Interval Equation Existence) If Q(X) X and Q(X) = , then there is a unique root in X. The conditions 
implied by these theorems thus lead to acceptance criteria, A, for the constrained minimization algorithm. 
Implemen­tation of Theorem 1 s conditions is clear from the discussion in Section 4.1. To verify the 
conditions of Theorem 2, we use the in­tervalinversemethoddiscussedinSection4.2.1. Wehavebeenable to 
verify solution uniqueness much earlier (i.e., in larger regions) using Theorem 2 s test. We note the 
conditions for Theorems 1 and 2 can only be veri.ed when the determinant of the Jacobian of the equality 
constraint function, C, is nonzero in some neighborhood of the solution. For collision detection, the 
Jacobian determinant is zero at a solution to the contact and tangency constraints in the following situations: 
the contacting surfaces become tangent but never interpene­trate. They can even stay tangent for an interval 
of time. 17We prefer the method of matrix iteration described here to a faster method (the interval analog 
of Gauss-Seidel iteration) which involves solving only for the diagonal matrix elements after a preconditioning 
step (see [RATS88]). This method requires a square system of equations, and will fail when the Jacobian 
matrix is singular at the solution. Interval computations in the preconditioning step also have the effect 
of increasing the size of the solution set Q(Mb) even before any iteration takes place. does not bound 
Q(Mb) directly, but instead bounds Q(Mb) X. 18See [TOTH85]for a proofsketch and referencesforTheorem1, 
[SNYD92b]for a 16Thatis, thenumberofequations, m, is unequal to the number of variables, n. proof of 
Theorem 2. Note that these bounding radii can be computed using a 2D un­constrained minimization problem, 
for which the algorithm of Sec­tion 3.2 is suitable. Using the constrained minimization algorithm, this 
time on a simple 1D problem, we then .nd a bound on the time of collision . via t0 minimum t [t0 t1] 
t T1(t)+ O1 T2(t) O2 R1+ R2 t1 minimum t[t0 t1] t T1(t)+ O1 T2(t) O2 R1+ R2  t f j k k u . gw 2 . 
 We can then replace the [t0 t1] interval in the full collision mini­mization problem for that pair of 
surfaces with [t0 t1 ], or cull the  T (t) pair of surfaces if no solutions to the 1D problem are found. 
1 T (t) 2 7 The Full Collision Algorithm Figure 9: Bounding Sphere Collision Time Bound the surfaces 
contact instantaneously on a curve or higher-The complete algorithm for detecting collisions can now 
be de­scribed. The following discussion pertains to a set of parametric both the surfaces contact at 
an in.nite set of points through surfaces; a similar algorithm can be developed for the case of im­plicit 
surfaces or parametric/implicit combinations. We are given a In these cases, we can not verify that a 
unique solution exists and set of solids de.ned by a parametric boundary representation, and must resort 
to heuristic criteria (the emergency criteria of the a time interval in which to detect collisions, [t0 
constrained minimization algorithm). That is, we consider surfaces steps summarize the .nal algorithm: 
to have collided when a bound on Cis suf.ciently small in a region.19 If a solution is on the boundary 
of a candidate region, we note that 1. Detect pairs of objects which can possibly collide. the conditions 
of either theorem will be dif.cult to verify since the step, we bound each time-varying surface by evaluating 
an set result (e.g., Q(X)) will always slightly spill out of the original inclusion function for its 
time-varying mapping over [t0 To solve this problem, we should slightly increase the More precisely, 
a bounding box through time on the surface S(uvt) is given by c +(X ) e 2 is a small constant (like 
.1). 0 0 where dimensional region. an interval of time. region X. region examined for acceptance, 
X, via e t1]. The following For this t1]. S([0 1] [0 1] [t0 t1]) X c)(1 +assuming S is evaluated on 
the unit square.20 We can thenWe then can perform the test whether any of the resulting bounding boxes 
intersect us­test on this bigger region X. Then, even if the solution is on the boundary of X, the theorem 
conditions will eventually be satis.ed if the Jacobian of C is nonsingular in a neighborhood of the solution. 
6 A Simple Bound for the Time of Collision We can save time in the collision algorithm by using a fast 
algorithm to reduce the time interval over which collisions are searched. This time bound may also tell 
us, with a minimum of computation, if the two surfaces fail to intersect, obviating the need for further 
computation. The test presented here uses 1D minimization (only over time) rather than minimization over 
the 4 or 5 dimensional space required to solve the full problem. As shown in Figure 9, we compute two 
bounding spheres around each of our parametric surfaces. In the case of rigid motion, this may be computed 
beforehand as a preprocessing step. We specify two points for each parametric surface O1 and O2 about 
which to compute a bounding sphere. These points should be chosen to minimize the size of the resulting 
bound; using the center of mass of the surface is a good choice. The bounding radii are: R1 max(u1 v1) 
s1(u1 v1) O1 R2 max(u2 v2) s2(u2 v2) O2 w k k ing highly ef.cient algorithms from computational geometry 
[SIX82]. All pairs of bounding boxes which do intersect must be processed further; the rest are culled. 
2. For rigid bodies, additional object pairs can be culled using the bounding sphere test of Section 
6. A variant of this test can also be used for deformable surfaces. 3. If any pairs of objects remain 
to be processed, we must invoke the full constrained minimization algorithm. Here, we distin­guish between 
free objects and objects already in continuous contact, whose contact points are being tracked with the 
ODE solver. For objects already in continuous contact, additional constraints are added (Section 3.2) 
to prevent re-detection of the tracked points. All such problems are placed on the initial priority queue 
of the multiple element constrained minimiza­tion algorithm. 4. Weuselocalmethods,suchasNewton smethod,toconvergeto 
the actual collision point in each solution region which contains an isolated collision (i.e., for which 
the interval existence and uniqueness test of Section 5 succeeded). We arbitrarily choose the midpoint 
as the collision point for the rest of the solution regions (termed singular solutions in Section 3). 
  20Note that for rigidsurfaces we can cache a boundingboxon the time-independent 19This implies, because 
of the contact constraint, that the surfaces come within a rigidsurfaceandcomputeonlyaboundovertimeontheresultingrotatedandtranslated 
speci.ed constant. bounding box. 8 Implementing Inclusion Functions The collision detection algorithm 
depends on inclusion functions for the time-varying surfaces and their various derivatives. Note that 
the equality constraint for the parametric surface case (Equa­tion 2) involves derivatives of the time-varying 
surface mappings Si(ui vi t). The interval Newton method then requires an additional derivative of the 
equality constraint with respect to each of the inde­pendent variables. Interval analysis provides the 
necessary theory for constructing inclusion functions for these functions. For simple polynomial surfaces 
(e.g., bicubic patches or algebraic surfaces) interval arithmetic suf.ces to provide an inclusion func­tion 
for the time-independent surface. Toth [TOTH85] has presented ef.cient inclusion functions for Bezier 
surfaces. Mitchell and Han­rahan have proposed a simple stack-based representation of surfaces which 
allows generation of inclusion functions for the surface and its derivatives [MITC92]. Inclusion functions 
for more complicated surfaces and their derivatives can also be constructed. We have used the system 
described in [SNYD92a,SNYD92b], which automates the construction of inclusion functions (and inclusion 
functions for the derivatives) of any functions formed by the composition of a quite powerful set of 
symbolic operators. For physical simulations, the ODE solver computes a represen­tation of the time behavior 
of the surfaces. The solver may directly compute a continuousrepresentation or it may be later reconstructed 
by point sampling the solver s results, typically producing a poly­nomial. Appendix B discusses a method 
to bound Chebyshev poly­nomials. 9 Results We have successfully tested this method on a series of collision 
detection examples, including both rigidly moving and deforming objects. For example, Figure 12 shows 
the results of a dif.cult collision detection run in which the contact manifold forms a series of disjoint 
2D regions. A collection of 59 points was generated in the contact region with a simultaneity threshold 
of 0.001 and solution separation distance of 0.04, using 28704 iterations and 88.81 CPU seconds.21 While 
the running time may seem large, the problem itself is suf.ciently dif.cult that its running time exceeded 
our threshold of 8 CPU hours without the use of every new technique presented in this paper: adding the 
tangency constraint (rather than using the contact constraint alone), sorting by upper bound in the constrained 
minimization algorithm (rather than by lower bound), and using Linear Solve for the interval Newton step 
(rather than the Krawczyk Moore operator). Figure 1, 5, and 12 16 show the results of the algorithm for 
several different time-varying shapes. The table in Figure 10 compares running times for a second example 
involving two rotating and translating bumpy parametric surfaces which collide at an isolated point. 
Several solution meth­ods are compared: LEQN (interval Newton using the linear equation solution techniques 
of Section 4.2), KM (interval Newton using the Krawczyk Moore operator), NIN (without interval Newton), 
and NTAN (without the tangency condition). Since the collision occurs at an isolated point, both the 
LEQ and KM methods were able to ac­cept a single solution region by verifying the solution existence 
and 21The term iteration refers to an evaluation of the inclusion functions f and F 22 (objective function 
and constraint function) in the constrained minimization algorithm. All CPU times are measured on a HP 
9000 Series 750 computer. Running Times Example Iterations CPU (secs) LEQN 6331 32.67 KM 10087 148.28 
NIN, =1e-3 17395 8.58 NIN, =1e-4 29921 15.46 NIN, =1e-5 40127 21.52 NIN,=1e-6 48187 23.25 NIN,NTAN, =1e-3 
52307 14.59 NIN,NTAN, =1e-4 587711 169.87 NIN,NTAN,=1e-5 3822605 1207.46 Figure 10: Table of Results 
for Various Methods: see Section 9   . . uniqueness test. The other methods required an accuracy 
parameter for acceptance; we used the simple criterion (X). w e Because we used a prototype system to 
gather the data, we em­phasize the importance of iteration count data over CPU time. Our system requires 
the traversal of a complicated data structure for each inclusion function evaluation which overwhelms 
the .oating point computation actually needed in the function. The interval Newton methods are sensitive 
to this bias, since their implementation re­quired many symbolic operators. We believe the iteration 
counts shown here to be a reasonable measure of expected running time, if the inclusion functions are 
hand-coded for the surfaces of interest. 10 Conclusions We have presented a robust interval algorithm 
that can detect col­lisions between complex curved surfaces. The algorithm handles a greater range of 
situations than previous algorithms. It detects both isolated collision points and collision points on 
contact manifolds. It can avoid detection of points close to a set of tracked points with speci.ed trajectories. 
It ef.ciently handles detection of simultane­ous collisions between sets of moving objects. The technique 
is practical for simulations involving large numbers of moving and deforming objects (see Figures 15 
and 16). Wedrawseveralconclusionsfromourexperimentalresults. First, interval methods, such as [VONH90] 
and [DUFF92], which do not make use of the interval Newton method or the tangency condition soon become 
impractical as we increase the accuracy parameter (refer to the NIN,NTAN lines of the table in Figure 
10). Interval Newton iteration combined with the tangency condition (especially using the interval linear 
equation approach) is very effective at re­ducing computation. Second, our method can solve the dif.cult 
problem of detecting collision points on a contact manifold. We have found the methods described here 
to be indispensable, includ­ing the idea of the tangency constraint, the constrained minimization algorithm 
discussed in Section 3, and the interval linear equation approach to interval Newton iteration. We note 
that many areas for improvement remain. Sorting by lower bound of the objective function rather than 
by upper bound is more ef.cient for isolated point collisions. We have noted an ef.ciency gain of a factor 
of from 1 to 10 in using the lower bound for such cases. On the other hand, sorting by lower bound is 
com­pletely impractical for detecting collisions on a contact manifold. If we know the nature of the 
collision solution set a priori, we can choose the appropriate method. Alternatively, combining the two 
approaches, perhaps by racing them in parallel on the same problem, may decrease the average running 
time. We are study­ing several ways to increase ef.ciency that involve more optimally choosing the next 
dimension to subdivide, and determining a sub­division location other than the midpoint.  Acknowledgments 
We wish to thank Mark Montague and Allen Corcorran for their production help. Thanks also go to the Siggraph 
reviewers for their careful reading, suggestions, and bug .xes. This work was sup­ported in part by grants 
from Apple, DEC, Hewlett Packard, and IBM. Additional support was provided by NSF (ASC-89-20219), as 
part of the NSF/DARPA STC for Computer Graphics and Scienti.c Visualization. All opinions, .ndings, conclusions, 
or recommenda­tions expressed in this document are those of the author and do not necessarily re.ect 
the views of the sponsoring agencies. References [ALEF83] Alefeld, G., and J. Herzberger, Introduction 
to Interval Com­putations, Academic Press, New York, 1983. [BARA89] Baraff, David, Analytical Methods 
for Dynamic Simulation of Non-penetrating Rigid Bodies, Computer Graphics, 23(3), pp. 223-232, July 1989. 
[BARA90] Baraff, David, Curved Surfaces and Coherence for Non­penetrating Rigid Body Simulation, Computer 
Graphics, 24(4), pp. 19-28, August 1990. [BARA91] Baraff, David, Coping with Friction for Non-penetrating 
Rigid Body Simulation, Computer Graphics, 25(4), pp. 31-39, July 1991. [BARA92] Baraff, David, and A. 
Witkin, Dynamic Simulation of Non-penetrating Flexible Bodies, Computer Graphics, 26(2), pp. 303-308, 
July 1992. [DUFF92] Duff, Tom, Interval Arithmetic and Recursive Subdivision for Implicit Functions and 
Constructive Solid Geometry, Com­puter Graphics, 26(2), July 1992, pp. 131-138. [META92] Metaxas, Dimitri, 
and D. Terzopoulos, Dynamic Deforma­tion of Solid Primitives with Constraints, Computer Graphics, 26(2), 
pp. 309-312, July 1992. [MITC92] Mitchell, Don, and P. Hanrahan, Illumination from Curved Re.ectors, 
Computer Graphics, 26(2), July 1992, pp. 283­ 291. [MOOR66] Moore, R.E., Interval Analysis, Prentice 
Hall, Englewood Cliffs, New Jersey, 1966. [MOOR79] Moore, R.E., Methods and Applications of Interval 
Analysis, SIAM, Philadelphia. [MOOR80] Moore, R.E., New Results on Nonlinear Systems, inInterval Mathematics 
1980, Karl Nickel, ed., Academic Press, New York, 1980, pp. 165-180. [MOOR88] Moore, M. and Wilhelms, 
J., Collision Detection and Re­sponse for Computer Animation, Computer Graphics, 22(4), pp. 289-298, 
August 1988. [PRES86] Press, W. H., B. P. Flannery, S. A. Teukolsky, and W. T. Vet­terling, Numerical 
Recipes, Cambridge University Press, Cam­bridge, England, 1986. [RATS88] Ratschek, H. and J. Rokne, New 
Computer Methods for Global Optimization, Ellis Horwood Limited, Chichester, England, 1988. [SCLA91] 
Sclaroff, Stan, and A. Pentland, Generalized Implicit Func­tions for Computer Graphics, Computer Graphics, 
25(4), pp. 247-250, July 1991. [SIX82] Six, H.W., and D. Wood, Counting and Reporting Intersec­ tions 
of d-Ranges, IEEE Transactions on Computers, C-31(3), March 1982, pp. 181-187. [SNYD92a] Snyder, John, 
and J. Kajiya, Generative Modeling: A Sym­ bolic System for Geometric Modeling, Computer Graphics, 26(2), 
pp. 369-378, July 1992. [SNYD92b] Snyder, John, Generative Modeling for Computer Graphics and CAD: Symbolic 
Shape Design Using Interval Analysis , Academic Press, Cambridge, MA, July 1992. [SNYD92c] Snyder, John, 
Interval Analysis for Computer Graphics, Computer Graphics, 26(2), pp. 121-130, July 1992. [TOTH85] Toth, 
Daniel L., On Ray Tracing Parametric Surfaces, Com­puter Graphics, 19(3), July 1985, pp. 171-179. [VONH90] 
Von Herzen, B., A.H. Barr, and H.R. Zatz, Geometric Col­ lisions for Time-Dependent Parametric Surfaces, 
Computer Graphics, 24(4), August 1990, pp. 39-48. A Collision Constraints for Piecewise Parametric Surfaces 
A piecewise surface is composedof a set of smooth faces,a set of edges where these faces meet, and a 
set of vertices or points where edges meet. Edges form the 1D boundaries over which the surface is not 
smooth; vertices are the 0D boundaries between smooth edge curves. A data structure containing the faces, 
edges, and vertices of a solid is called its boundaryrepresentation . For example, the boundary representation 
of a cylindrical solid contains three faces: one cylinder and two circular endcaps, two edges where the 
cylinder and endcap meet, but no vertices. To detect collisions between two piecewise surfaces, we must 
search for collisions between each pair of faces, between edges and faces, between vertices and faces, 
etc. The constraints governing collisions are different in each of these cases, which we call tangency 
situations. There are 6 types of tangency conditions in a collision between piecewise surfaces as shown 
in Figure11. Constraintsfortheface-to-facetangencysituationareidenticalto the constraints discussed in 
Section 2.1. The following paragraphs discuss the other tangency situations. We must combine all the 
constrained minimization problems for the variouspossibletypesoftangencysituations. Forexample,ifthesurfacesare 
a pair of cylindrical solids, we obtain 25 separate constrained minimization subproblems: 9 face-to-face 
problem, 12 edge-to-face problems, and 4 edge-to-edgeproblems. Twotorirequireonlyasingleface-to-faceproblem. 
Each problem is then solved simultaneously using the multiple element constrained minimization algorithm. 
Edge-to-Face For the edge-to-face case, we have an edge curve, C(st), which forms a boundary of a surface, 
Sa(uavat), and another surface, S(uvt). The edge curve is typically formed by evaluating a parametric 
surface Sa at a speci.c value for either the u or v parameter, e.g. .xed YY YY t Y YYYYY C(st) Sa(svt) 
where v.xed is a constant set at one of the extremes of the v interval over which Sa is evaluated. The 
contact constraint for edge-to-face collisions is C(st) S(uvt)=0 (8) and the tangency constraint by Y 
YY (st)N(uvt)=0 (9) s where N is the time-varying normal to the surface S. The edge-to-face equality 
constraint can be represented a system of 4 equations in 4 variables. To de.ne the incoming collision 
condition, we need to de.ne what out­wardness means on an edge curve. Assuming all surfaces form the 
valid boundaries of a closed solid, the edge curve C(st) is shared between two Y surfaces Sa and Sb. 
We can therefore de.ne two outward directions, given by the outward pointing normals to the shared surfaces 
Sa and Sb. For example, these outward directions may be de.ned as Coutward-1(st) .xed Na(svt) Coutward-2(s.xed 
  Y tt YYY t) Nb(ust) where Na and Nb are the outward normal vectors of the respective surfaces. The 
incoming constraint forces the relative velocity between the surface and the edge curve to be in the 
same direction (using a dot product test) as the surface s normal. The surface s normal must also face 
away from at least one of the edge curve s outward directions. The incoming constraint is: SC YYYYYY 
C Y YYYY e e , ((uvt)(st))N(uvt) 0 and tt Coutward-1(s(10) N(uvt)t) 0or Coutward-2(s N(uvt)t)0 Edge-to-Edge 
The edge-to-edge caseinvolvestwo edgecurves, C1(s1t) and C2(s2t). For this case, just a contact constraint 
is suf.cient, given by the following system of three equations in three variables: YYYY C1(s1t) C2(s2t) 
= 0 (11) To de.ne the incoming collision condition, we de.ne two outward direc­tions for each edge curve, 
as in the previous discussion for the edge-to-face case. The relative velocity between the edge curves 
must face in the same direction as at least one of the .rst curve s outward directions: C1C2Coutward-1 
((s1t)(s2t))1 (s1t) 0or YY YY YY ee tt (12)C1C2Coutward-2 ((s1t)(s2t))1 (s1t)0 tt Also, at least one 
of the outward directions on one curve must face away from one of the outward directions of the other 
curve. A logical combination of 6 inequalities is the result. Vertex-to-Face, Vertex-to-Edge, Vertex-to-Vertex 
The vertex-to­face case involves a vertex P(t) and a surface S(uvt). As in the edge-to-edge YY case, 
a contact constraint is suf.cient, of the form P(t) S(uvt) = 0 (13) where the point P is formed by evaluating 
a surface at a .xed point in its (uv) parameter space, e.g. .xed.xed Y t YYYY P(t) Sa(uvt) system of 
three equations in a single unknown for the vertex-to-vertex case. The incoming collision condition can 
be derived by de.ning a number of outward directions for the colliding vertex, corresponding to the normal 
vector of each surface containing that vertex. The normal to the surface S must face away from at least 
one of these outward directions, as in the  YY YY e edge-to-face case. The relative velocity between 
the surface and the vertex must face in the same direction as the surface s normal, via SP ((uvt)(t))N(uvt)0 
tt Similar systems of inequalities can be derived for situations where a vertex collides with an edge 
or another vertex. B Inclusion Functions for Chebyshev Polynomials Chebyshev polynomials are a good 
basis for a continuous representation of timebehavior. Theyallowsimplecontrolofapproximationerror,andcanbe 
differentiated using a simple method to produce a Chebyshev representation of the derivative (see [PRES86, 
pages 158 165] for a discussion of the advantages of Chebyshev polynomials, their properties, and algorithms 
for their manipulation). The basis functions for a Chebyshev polynomials are Tn(x) cos(n arccos(x)) t 
which expand to a series of polynomials of the form T0(x)=1 T1(x)= x T2(x)=2x21 . Tn+1(x)=2xTn(x) Tn 
1(x) n 1 The function Tn(x) has n + 1 extrema with values of1 at the locations i t nYY ... Y e xi cos( 
) i =01n n The i-th extremumof thebasisfunction Tn is either a minimum or maximum according to the rules 
1if(i + n) 1mod2Tn(xi)= +1if(i + n) 0mod2 A Chebyshev approximation of order N is given by specifying 
N coef.- YYY ... Y n YY X t cients cii =01N 1, which determine the polynomial N 1 c0 C(x)= ciTi(x)+ 2 
i=1 Given the order of the Chebyshev approximation function C(x), N, we can easily compute an inclusion 
function for C(x). Let the interval over which we are to bound C(x) be given by X =[x0x1]. As a preprocessing 
step, we .rst tabulate the locations of the extrema of the basis functions, up to some maximum order. 
(Note that the results can then be used for any approximating polynomial.) For each Chebyshev basis function, 
Ti(x)i = YY ... Y n .. Y YYYYY 2 YYY 01N 1, we .rst evaluate Ti(x0) and Ti (x1). We then determine whether 
any extrema of Ti(x) occur in [x0x1] using the tabulated locations of the extrema. A lower bound on the 
basis function over [ x0x1], b0 is i min(Ti (x0) Ti(x1)1)ifminof Ti(x)[x0x1]b0 i min(Ti (x0)Ti(x1))otherwise. 
Similarly, an upper bound is  t n 2 Y t X YYYY 2 Y max(Ti (x0) Ti(x1)1)ifmaxof Ti(x)[x0x1]b1 i max(Ti 
(x0)Ti(x1))otherwise. The .nal inclusion function is then N 1 C(X) ci[b0 bi 1] + c0 i 2 i=1 A system 
of three equations in three unknowns results. Similarly, a system ofthree equationsintwounknownsresultsfor 
thevertex-to-edgecase,anda where operations are computed with interval arithmetic. Scenes from test 
animations: In the following .gure pairs, the upper image is the scene immediately before the collision, 
while the bottom image is the scene at the collision time. Points of contact are shown as white dots, 
which are uniformly distributed over regions where there are line and surface contacts. At the time of 
collision, surfaces become transparent to make the dots visible. Scenes from Fruit Tracing :This animation 
shows the results of collision detection for a more complicated setting involving hundreds of colliding 
objects. In this animation, moving parametric surfaces representing fruit are collided with a static 
lobster shape, de.ned as an implicit surface. (Lobster data generated by David Laidlaw, Matthew Avalos, 
Caltech, and Jose Jimenez, Huntington MRI Center.)   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166159</article_id>
		<sort_key>335</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>42</seq_no>
		<title><![CDATA[Sensor-actuator networks]]></title>
		<page_from>335</page_from>
		<page_to>342</page_to>
		<doi_number>10.1145/166117.166159</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166159</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.3</cat_node>
				<descriptor>Probabilistic algorithms (including Monte Carlo)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.9</cat_node>
				<descriptor>Sensors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Visual</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010257</concept_id>
				<concept_desc>CCS->Computing methodologies->Machine learning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10010365</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Visual analytics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010553.10010559</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems->Sensors and actuators</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003677</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Markov-chain Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003682</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Sequential Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003671</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP40037196</person_id>
				<author_profile_id><![CDATA[81319502903]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michiel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[van de Panne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43128578</person_id>
				<author_profile_id><![CDATA[81100188679]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eugene]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fiume]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[[1] N. Badler, B. Barsky, and D. Zeltzer (Eds). <i>Making Them Move</i>. Morgan Kaufmann, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>80151</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[[2] R. Beer. <i>Intelligence as Adaptive Behavior</i>. Academic Press, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[[3] V. Braitenberg. <i>Vehicles: experiments in synthetic psychology </i>. MIT Press, 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[[4] R. A. Brooks. A Robust Layered Control System for a Mobile Robot. <i>IEEE Journal of Robotics and Automation</i>, 2, 1 (March 1986), 14-23.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378531</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[[5] L. S. Brotman and A. N. Netravali. Motion Interpolation by Optimal Control. Proceedings of SIGGRAPH '88. In <i>ACM Computer Graphics</i>, 22, 4 (August 1988), 309-315.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134083</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[[6] M. F. Cohen. Interactive Spacetime Control for Animation. Proceedings of SIGGRAPH '92. In <i>ACM Computer Graphics</i>, 26, 2 (July 1992), 293-302.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[[7] E. Fiume and M. Ouellette. On distributed, probabilistic algorithms for computer graphics. <i>Proceedings of Graphics Interface '89</i>, 211-218, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>111166</ref_obj_id>
				<ref_obj_pid>111154</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[[8] M. Girard. Constrained Optimization of Articulated Animal Movement in Computer Animation. In <i>Making Them Move</i>, Morgan Kaufmann, 1991, 209-232.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>155325</ref_obj_id>
				<ref_obj_pid>155294</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[[9] J. K. Hodgins, P. K. Sweeney, and D. G. Lawrence. Generating Natural-looking Motion for Computer Animation. <i>Proc. of Graphics Interface '92</i>, 265-272.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[[10] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization by simulated annealing. <i>Science</i>, 220, 13 (May 1983), 671-680.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[[11] P. Maes, R. Brooks. Learning to Coordinate Behaviors. <i>Proc. of AAAI '90</i>, 1990, 796-802.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[[12] T. McGeer. Passive Walking with Knees. <i>Proceedings of the IEEE International Conference on Robotics and Automoation </i>, 1640-1645, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97882</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[[13] M. McKenna and D. Zeltzer. Dynamic Simulation of Autonomous Legged Locomotion. Proceedings of SIGGRAPH '90. In <i>ACM Computer Graphics</i>, 24, 4 (August 1990), 29-38.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378508</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[[14] G. S. P. Miller. The Motion Dynamics of Snakes and Worms. Proceedings of SIGGRAPH '88. In <i>ACM Computer Graphics</i>, 22, 4 (August 1988), 169-178.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[[15] W.-Y. Ng. Perspectives on Search-Based Computer-Aided Control System Design. <i>IEEE Control Systems Magazine</i>, 13, 2 (April 1993), 65-72.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166160</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[[16] J. T. Ngo and J. Marks. Spacetime Constraints Revisited. Proceedings of SIGGRAPH '93. In <i>ACM Computer Graphics</i>, 27 (August 1993).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[[17] M. G. Pandy, F. E. Zajac, E. Sim, and W. S. Levine. An Optimal Control Model for Maximum-Height Human Jumping. <i>J. Biomechanics</i>, 23, 12, 1185-1198, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122755</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[[18] M. H. Raibert and J. K. Hodgins. Animation of Dynamic Legged Locomotion. Proceedings of SIGGRAPH '91. In <i>ACM Computer Graphics</i>, 25, 4 (July 1991), 349-358.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122752</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[[19] K. Sims. Artificial Evolution for Computer Graphics. Proceedings of SIGGRAPH '91. In <i>ACM Computer Graphics </i>, 25, 4 (July 1991), 319-328.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>155326</ref_obj_id>
				<ref_obj_pid>155294</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[[20] A. J. Stewart and J. F. Cremer. Beyond Keyframing: An Algorithmic Approach to Animation. <i>Proceedings of Graphics Interface '92</i>, 273-281, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[[21] M. van de Panne, E. Fiume, and Z. G. Vranesic. A Controller for the Dynamic Walk of a Biped Across Variable Terrain. <i>Proceedings of the 31st IEEE Conference on Decision and Control</i>, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97904</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[[22] M. van de Panne, E. Fiume, and Z. G. Vranesic. Reusable Motion Synthesis Using State-Space Controller. Proceedings of SIGGRAPH '90. In <i>ACM Computer Graphics</i>, 24, 4 (August 1990), 225-234.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[[23] J. Wilhelms and R. Skinner. An Interactive Approach to Behavioral Control. <i>Proceedings of Graphics Interface '89</i>, 1-8, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378507</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[[24] A. Witkin and M. Kass. Spacetime Constraints. Proceedings of SIGGRAPH '88. In <i>ACM Computer Graphics</i>, 22, 4 (August 1988), 159-168.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166160</article_id>
		<sort_key>343</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>43</seq_no>
		<title><![CDATA[Spacetime constraints revisited]]></title>
		<page_from>343</page_from>
		<page_to>350</page_to>
		<doi_number>10.1145/166117.166160</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166160</url>
		<keywords>
			<kw><![CDATA[evolutionary computation]]></kw>
			<kw><![CDATA[genetic algorithms]]></kw>
			<kw><![CDATA[massive parallelism]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.3</cat_node>
				<descriptor>Probabilistic algorithms (including Monte Carlo)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.6</cat_node>
				<descriptor>Parameter learning</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.8</cat_node>
				<descriptor>Heuristic methods</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Visual</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010257.10010293.10010316</concept_id>
				<concept_desc>CCS->Computing methodologies->Machine learning->Machine learning approaches->Markov decision processes</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10010365</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Visual analytics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010205.10010206</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Search methodologies->Heuristic function construction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010070.10010071.10010316</concept_id>
				<concept_desc>CCS->Theory of computation->Theory and algorithms for application domains->Machine learning theory->Markov decision processes</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003677</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Markov-chain Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010257.10010258</concept_id>
				<concept_desc>CCS->Computing methodologies->Machine learning->Learning paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003682</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Sequential Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003671</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP77035015</person_id>
				<author_profile_id><![CDATA[81414603567]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[Thomas]]></middle_name>
				<last_name><![CDATA[Ngo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP77033873</person_id>
				<author_profile_id><![CDATA[81100484340]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Joe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marks]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>122722</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[D. Baraff. Coping with friction for non-penetrating rigid body simulation. Computer Graphics, 25(4):31-40, July 1991.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378531</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[L. S. Brotman and A. N. Netravali. Motion interpolation by optimal control. Computer Graphics, 22(4):309-315, August 1988.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134083</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[M. F. Cohen. Interactive spacetime control for animation. Computer Graphics, 26(2):293-302, July 1992.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Y. Davidor. A genetic algorithm applied to robot trajectory generation. In L. Davis, editor, Handbook of Genetic Algorithms, chapter 12, pages 144-165. Van Nostrand Reinhold, New York, 1991.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[L. Davis. Handbook of Genetic Algorithms. Van Nostrand Reinhold, New York, 1991.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>102044</ref_obj_id>
				<ref_obj_pid>101883</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[H. de Garis. Genetic programming: Building artificial nervous systems using genetically programmed neural network modules. In Proceedings of the Seventh International Conference on Machine Learning, pages 132-139, Austin, Texas, June 1990.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907087</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[K. A. De Jong. An Analysis of the Behavior of a Class of Genetic Algorithms. PhD thesis, University of Michigan, 1975.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>534133</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[D. E. Goldberg. Genetic Algorithms in Search, Optimization, and Machine Learning. Addison-Wesley, Reading, Massachusetts, 1988.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378530</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[J. K. Hahn. Realistic animation of rigid bodies. Computer Graphics, 22(4):299-308, August 1988.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>129194</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[J. H. Holland. Adaptation in Natural and Artificial Systems. University of Michigan Press, 1975.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[J. Luh, M. Walker, and R. Paul. On-line computational scheme for mechanical manipulators. Trans. ASME, J. Dynamic Systems, Measurement, and Control, 102:69-76, 1980.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[P. Maes and R. A. Brooks. Learning to coordinate behaviors. In Proceedings of the Eighth National Conference on Artificial Intelligence, pages 796-802, Menlo Park, California, 1990. American Association for Artificial Intelligence.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[J. T. Ngo and J. Marks. Physically realistic trajectory planning in animation: A stimulus-response approach. Technical Report TR-21-92, Center for Research in Computing Technology, Harvard University, October 1992.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[B. F. Skinner. The Behavior of Organisms; An Experimental Analysis. The Century Psychology Series. D. Appleton- Century, New York, London, 1938.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>93133</ref_obj_id>
				<ref_obj_pid>93126</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[G. Syswerda. Uniform crossover in genetic algorithms. Proceedings of the Third International Conference on Genetic Algorithms, pages 2-9, 1989.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617556</ref_obj_id>
				<ref_obj_pid>616011</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[J. Wilhelms and R. Skinner. A "notion" for interactive behavioral animation control. IEEE Computer Graphics and Applications, 10(3):14-22, May 1990.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378507</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[A. Witkin and M. Kass. Spacetime constraints. Computer Graphics, 22(4): 159-168, August 1988.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Spacetime Constraints Revisited J. Thomas Ngo Graduate Biophysics Program . Harvard University Abstract 
The Spacetime Constraints (SC) paradigm, whereby the animator speci.es what an animated .gure should 
do but not how to do it, is a very appealing approach to animation. However, the algorithms available 
for realizing the SC approach are limited. Current tech­niques are local in nature: they all use some 
kind of perturbational analysis to re.ne an initial trajectory. We propose a global search algorithm 
that is capable of generating multiple novel trajectories for SC problems from scratch. The key elements 
of our search strategy are a method for encoding trajectories as behaviors, and a genetic search algorithm 
for choosing behavior parameters that is currently implemented on a massively parallel computer. We de­scribe 
the algorithm and show computed solutions to SC problems for 2D articulated .gures. CR Categories: I.2.6 
[Arti.cial Intelligence]: Learning parameter learning. I.2.6 [Arti.cial Intelligence]: Problem Solv­ing, 
Control Methods and Search heuristic methods. I.3.7 [Com­puter Graphics]: Three-Dimensional Graphics 
and Realism animation. I.6.3 [Simulation and Modeling]: Applications. Additional Key Words: Genetic algorithms, 
evolutionary compu­tation, massive parallelism. 1 Introduction The Spacetime Constraints (SC) formulation 
was proposed in 1988 by Witkin and Kass as a powerful paradigm for indirectly control­ling the physically 
realistic motion of articulated .gures ( crea­tures ) for animation purposes [17]. The essence of the 
SC ap­proach is to have the animator specify what the animated creature should do, and the computer determine 
how the creature should do it. Speci.cally, the animator de.nes: the physical structure of the creature; 
 the actuators that control the creature s internal con.guration; and criteria for evaluating the creature 
s motion. oy Present address: Interval Research Corporation, 1801-C Page Mill Road, Palo Alto, CA 94304. 
Email: ngo@interval.com. One Kendall Square, Building 700, Cambridge, MA 02139. Email: marks@crl.dec.com. 
Joe Marks Cambridge Research Lab y Digital Equipment Corporation The computer is left with the task 
of computing a physically realistic trajectory for the creature that is optimal according to the animator­supplied 
criteria. The computer s role in this paradigm is quite challenging. 1 Find­ing globally optimal solutions 
to SC problems is hard for two main reasons [13]: Multimodality even when the problem is suitably dis­cretized, 
there are an exponential number of possible tra­jectories that a creature can follow, many of which may 
be locally optimal or near optimal. Search-space discontinuities a small change in the behavior of a 
creature s actuators can lead to a large change in its trajectory. Current algorithmic techniques for 
SC problems sidestep the dif­.culties of global optimization entirely by settling for some form of local 
optimization: typically an initial trajectory is perturbed in some way until it is locally optimal [2, 
17, 3]. In this paper we propose a global search algorithm that ful.lls a complemen­tary need by generating 
multiple near-optimal trajectories for SC problems from scratch. The key elements of our search strategy 
are a method for encoding trajectories as sets of stimulus-response behavior rules [14, 16, 13], and 
a genetic algorithm (GA) [10, 8, 5] for choosing behavior parameters that is currently implemented on 
a massively parallel SIMD computer. 2 Algorithm Our algorithm can be described in summary as follows: 
x A dynamics module (2.1) simulates a physically correct vir­tual environment in which the effects of 
creature behaviors may be tested by trial and error. x A behavior module (2.2) generates such behaviors 
using a parameterized algorithm that is based on the concepts of stimulus and response [14]. x A search 
module (2.3) uses a genetic algorithm to choose values for the stimulus and response parameters that 
will generate near-optimal behaviors according to the evaluation criteria for the given SC problem. 1This 
is not to suggest that the animator s role has been trivialized completely far from it! Specifying appropriate 
evaluation criteria for a creature s motion is a challenging problem, as is the task of reducing a complex 
animation problem to a concatenated sequence of related SC problems [3]. 2.1 Physical simulator Like 
Hahn s [9], our physical simulator employs forward dynam­ics, but treats an articulated .gure as an autonomously 
deforming object without internal degrees of freedom. The deformations are produced kinematically by 
the creature s stimulus-response control algorithm (2.2). The principal advantage of this approach is 
dra­ x maticallyreducedCPUcost.2 We.ndthatinpracticethereislittle sacri.ce in terms of physical realism 
even though torques about the joints are never computed explicitly. If torque limits were to be important 
for a particular application, a penalty for excessive torques could be added to the evaluation criteria; 
the torques can be computed using inverse dynamics [11]. However, we have found that simply limiting 
the rate at which a creature s internal degrees of freedom can accelerate something that can be speci.ed 
triv­ially in our behavior-based representation is enough to produce visually reasonable behavior. Friction 
is taken to be static when one joint is in contact with the .oor, and slippage is proportional to contact 
force when two joints touch. Collision and contact forces are treated essentially 3 as in Baraff [1]. 
We specialized Baraff s elegant treatment to two dimensions, then extended it to accommodate autonomous 
deformations. For a rigid polygon in 2D interacting with a .at 1D .oor, the contact-force computation 
is much simpler than in the 3D case, because the number of contact points is limited to two. The extension 
to accommodate autonomous deformations is straightforward as well. The key to Baraff s technique is that 
the relative normal acceleration ( ¨) for a given contact point turns D i x Di i fy out to be a linear 
function of the vector of normal contact forces ( ). The relation remains linear after adding a contribution 
to ¨that takes into account the creature s deformation, so the procedure for satisfying the non-penetration 
and non-stickiness constraints ( ¨0 and 0) remains essentially unchanged. D i o f i o 2.2 Stimulus-response 
representation Trial behaviors to be tested in the simulator (2.1) are generated by the stimulus-response 
(SR) control algorithm. This algorithm neither learns nor plans; rather, it causes the creature to execute 
instinctive re.exes triggered by conditions sensed in its virtual environment. The conditions are called 
stimulus functions and the re.exes are called responses; and their parameters remain .xed for the duration 
of a trial behavior. The search module (2.3) .nds values for both the stimulus parameters and the response 
parameters essentially by trial and error. The use of these SR parameters in lieu of a more conventional 
time series of con.gurations or forces may be the most important factor in the success of our approach 
to SC problems. A response is a prescription for changing the creature s shape smoothly. It consists 
of a time constant and a complete set of target values for the creature s internal angles. Applying the 
response for one time step means iterating through one time step of the critically damped equation of 
motion 20 m mm o i a mo mi a o i h o i n x 20 2Physical simulation is the CPU-intensive portion of our 
approach. 3A modi.cation was necessitated by the SIMD architecture of our machine, be­cause of which 
we implemented a .xed-timestep integrator with analytic recomputa­tion after a mid-timeslice collision, 
rather than a more accurate variable-timestep inte­grator with instantaneous handling of the impulsive 
forces that arise from collisions. for each of the internal angles, where is the time constant, 0is the 
target value of the internal angle, and is its actual value. The effect of this equation of motion is 
to cause the real shape of the   f o o  o N gf o o  o N g mo i o i creature, given by 12, to approach 
the target shape 10 20 0smoothly, even when the target shape changes abruptly due to a switch from one 
response to another. To de.ne a stimulus function we must introduce the concept of a sense variable. 
A sense variable is some real-valued function of the physical environment. Our standard list includes: 
proprioceptive senses each joint angle; tactile senses the force exerted by each rod endpoint on the 
.oor (and vice versa); kinesthetic sense the vertical velocity of the center of mass; position sense 
the vertical position of the center of mass relative to the .oor. A stimulus function is a scalar function 
de.ned over sense space. If the sense variables are 12, then the expression that W n hf v V v r j v 
j v h V g v j of we use for a stimulus function is: 20 1max 1 where the 0andare parameters determined 
by the search module (2.3), the weight is: log min1 andminis the predetermined smallest permissible value 
of(2.3). (In practice all of these constants are normalized so that the sense variables fall between 
0 and 1.) The locus of points in sense space for which the stimulus-function expression is positive its 
sensitive region is a hyper-rectangle with dimen­ 00 0 x and f j x v j  j W n v j W jX jVV g c jjef 
v v  v V g j sions 21222centeredat 12. The set of SR parameters for a creature consists of an array 
of SR pairs (10 pairs in all the tests we have run to date). In the following pseudocode, which describes 
how an SR array is used to generate behavior, the state variables are the creature s physical state and 
a pointer to the active response: Initialize creature state from SC problem description Activate response 
0 for t=1 to T Determine deformation for time t from active response Simulate resulting dynamics for 
time t Measure sense variables from the environment Identify highest-valued stimulus function Activate 
corresponding response if stimulus positive end for The .nal step may or may not change which response 
is active; a change is made only if the highest-valued stimulus function is positive. Thus, a response 
is typically active for several consecutive time steps and produces coherent motion. Symbol Description 
Lo Hi Distribution 0 Time constant Target angles 2.min 1 4 max Logarithmic Uniform 02 Stimulus centers 
Stimulus extents -0.5 0.4 1.5 4 Uniform Logarithmic 2.3 Genetic algorithm In 2.1 and 2.2 we described 
what a creature would do given a xxx structural speci.cation, initial conditions, and values for its 
stimu­lusandresponseparameters. In 3wewillsupplyexamplesofhow the resulting behaviors might be evaluated 
quantitatively. Taken together, these ingredients de.ne a scalar function of the SR pa­rameters. It remains 
to show how to .nd near-optimal values of this function. Our search procedure is a parallel genetic algorithm 
(GA) writ­ten in C* on a Thinking Machines CM-2 with 4096 processors. In a GA, a population of candidate 
solutions is subjected to a procedure that simulates biological evolution. Each candidate solution each 
genome, in GA parlance has some probability of being mutated, recombining with another genome, and dying, 
based on its value. In our implementation, each processor is responsible for evaluat­ing one genome per 
generation. Our GA is described below in pseudocode: do parallel Randomize genome end do for generation 
= 1 to number of generations do parallel Evaluate genome Select mate from another processor Cross genome 
with mate Mutate genome  end do end for In this section we shall explain the component operations of 
this genetic algorithm: Randomize, Select mate, Cross, and Mutate. Although these operations have worked 
without modi.cation for all of the test cases shown here and elsewhere [13], they should not be considered 
optimal.  tvomT ijj o i too ii To i  Table 1: The parameters that comprise one SR pair in the genome, 
their typical ranges, and their probability distributions in the initial random population. The timestep 
size and total simulation time are minmax .and , respectively; and the quantities and are lower and 
upper bounds on the joint angles taken from the structural speci.cation of the creature. Randomization 
Values for all of the parameters in the genome are chosen at random in the initial population, with the 
probability distributions given in Table 1. We then employ a form of hill­climbing to enrich the initial 
gene pool: after evaluating this initial population, we mutate and re-evaluate each solution four times, 
and on each processor choose the best out of .ve. Because of our highly specialized mutation operator 
(see below), this has the effect of producing a fairly non-random population that is skewed in favor 
of multi-step behaviors.  Figure 1: Visualization of a GA population. The display consists of 6464 pixels, 
each one corresponding to a processor in the CM-2. Each pixel is colored according to the value of the 
solution stored at the corresponding processor: the colors range from dark blue for the worst solutions 
to bright yellow for the best ones. In this mature population two relatively homogeneous regions ( colonies 
) have emergedfrom abackgroundofmediocresolutions;thecornerareas form one region on the toroidal grid. 
Mate selection This is the only step in the algorithm that requires interprocessor communication, and 
it is also the only stage at which better solutions are given an advantage. In the simplest GA s, mating 
between any two individuals is possible. To prolong the diversity of the population this helps cope with 
multimodality it is common practice to impose a geographic distribution on the population and to permit 
only local mating. In our implementation, the processors are laid out on an imaginary 6464 toroidal grid; 
this scheme is especially easy to achieve on the CM-2. To choose a mate, each processor performs a random 
walk of (typically) 10 steps on the grid, and identi.es the best solution encountered. If the best solution 
encountered is itself, it does not mate. (Thus, a solution that is best in its neighborhood is protected 
from crossover and mutation; this is a form of elitism [7].) Otherwise, it fetches a copy of the selected 
mate s genome in preparation for crossover. This local mating scheme causes good genes (solution frag­ments) 
to diffuse slowly through the population, leading to the formation of colonies of similar solutions. 
Figure 1 depicts a population that arose in one of our experiments. As a colony spreads, more processors 
are put to work on variations of the supe­  Time Time AB AB  Leap Leap Time Time BC BC CD DE Figure 
2: Five-Rod Fred moves to the right by curling and leaping. Since his initial con.guration is symmetric, 
his curling action must be asymmetric in order to generate net rightward motion. Momen­tum is essential 
in permitting a forward leap. Inversion in the air is necessary to delay landing long enough to fall 
forward instead of backward. Note: In thisandsubsequent.gureswehavearbitrarily broken the motion into 
easily perceived phases. This is only for presentation purposes, and is not indicative of any aspect 
of the algorithm. Ticks along the horizontal time axis show which time slices are drawn, and in what 
linestyles. The .rst, intervening, and last frames in each phase are drawn with thick grey, thin black, 
and thick black lines, respectively. An italicized phrase at the lower right of each .gure describes 
the corresponding action. rior solution that spawned the colony. But because the spreading is not instantaneous, 
suboptimal solutions still get some processing time. When one colony dominates (a typical outcome when 
genetic algorithms of this type are left to run inde.nitely), the population is said to have converged. 
Crossover The crossover operators used in the GA as it was orig­inally described [10] are based on a 
relatively literal interpretation of the biological metaphor: the bit representations of the param­eters 
in the genomes are placed end-to-end in linear fashion, and crossover operations call for snipping two 
genomes at analogous Invert CD Time Curl DE EF Roll FG Figure 3: Fred rolls on landing. Because 
his curled con.guration is a hexagon and not a circle, Fred bounces as he rolls. locations along the 
bit string, and swapping one pair of ends. In the SR representation, a linear layout would not be very 
meaningful, and we know in advance that certain groups of pa­rameters should migrate together between 
genomes. Therefore our crossover operator is tailored to treat the genome in a more structured fashion. 
Given the probabilities that we use, a hybrid might be constructed from two genomes (of ten SR pairs 
each) as follows: six intact SR pairs are taken from the mate and two from Tip Time AB Fall Time BC Hop 
Time CD Land Time D C Figure 4: Mr. Star-Man canters on two side limbs. The C D C sequence is repeated 
cyclically. Were Mr. Star-Man to have human-like strength and structure he would not be able to walk 
this way for very long, since his arm would get tired. This is one situation in which penalties for excessive 
torques might in.uence the range of behaviors discovered. the original genome; one new SR pair is constructed 
by taking a stimulus and a response from each parent, respectively; and one new SR pair is constructed 
by uniform crossover [15], i.e., each number in the SR data structure is copied at random from one or 
the other parent. The precise probabilities that we use in performing the crossover are not critical; 
we have not tried to optimize them because the values we use currently are probably irrelevant for fu­ture 
implementations of our general approach (4). The key point x is that the crossover operation must be 
tailored to the problem to get good performance. Mutation Our mutation operator [13] is also tailored 
speci.cally for the SR representation. One SR pair is subjected to creep [5]; i.e., each of the parameters 
in that SR pair is changed by a small amount. Another SR pair is randomized from scratch, with one stipulation 
that turns out to be quite important in coping with a large number of sense variables: that at least 
one corner of the sensitive region (2.2) of the newly generated stimulus function x Twist Time AB Tip 
Time BC Roll Time CD Hop Time DE Shift Time EF Fall Time FG Figure 5: Mr. Star-Man begins to cartwheel, 
but when his time is nearly up, he falls to the .nish line. The subtle posture shifts in A B and E F 
are critical to the motion. be guaranteed to coincide with the original sense-space trajectory. Time 
AB  AB Time BC Time BC Time CD  Time   Time D B Figure 6: Mr. Star-Man shuf.es. The B C D B sequence 
is repeated cyclically. Star-Man s stride length is limited by how long his right foot can stay off the 
ground during the step marked Scissor. His arm helps prolong this step by swinging down, offsetting the 
upward momentum contribution by his leg. Without this restriction, freshly generated stimulus functions 
tend either to dominate the trajectory or not to modify it at all. 3 Results In an earlier suite of tests 
involving unbranched articulated .gures [13], we showed that the algorithm could cope with two-point 
boundary conditions as well as one-point boundary conditions with athletic evaluation functions (jump 
height and walking distance). The present test suite is a further exploration of the walking problem 
that involves bigger, more complex articulated .gures, most of which are branched. We present results 
for three selected runs. In the .rst two cases, the evaluation function was proportional to the net horizontal 
dis­tance covered by the creature s center of mass in the given time. C B Figure 7: Beryl Biped skips. 
The B C B sequence is repeated cyclically. Skipping differs from walking; the foot that starts out in 
the back never moves to the front. Balance is less of an issue in this type of skipping than in walking 
because intermediate postures are statically stable. However, this skipping is evidently not as fast 
as walking can be. The farther the creature s center of mass moved to the right, the better the trajectory 
regardless of how the movement was ac­complished. In the third case, the evaluation function had to be 
modi.ed slightly to obtain the desired behavior. In each case, ap­proximately 30 to 60 minutes of elapsed 
time on the CM-2 was required to compute each solution shown. 4 The .rst problem involves Five-Rod Fred, 
a creature comprised of .ve equal-length rods, linked consecutively. The middle rods are of equal mass, 
but the terminal rods are .ve times heavier. . Each joint allows its pair of connected rods to be at 
most 30 from collinear. We constructed this creature in the expectation that it would behave like an 
inchworm. To our surprise, 64 generations of evolution produced the solution depicted in Figure 2. After 
100 generations, this behavior was improved by the addition of a rolling phase at the end of the motion 
(Figure 3). Fred s .nal behavior is generated by just .ve of the ten available SR pairs, which is not 
unusual for the behaviors described here. Ini­4The quoted times are for 5 rods, 50 time steps, and 100 
generations. "Left" Time AB "Right" Time BC Gather Time CD "Right" Time DE "Left" Time EF Figure 8: 
Beryl walks, but one step (C D E) is really a skip. The GA/SR combination frequently produces hybrid 
gaits of this type. tially, two SR pairs produce the asymmetric curling motion shown "Left" Time AB "Right" 
Time BC "Left" TimeCD "Right" TimeDE Figure 9: Beryl walks. How to modify the evaluation function to 
reward grace as well as distance remains an open question. in the .rst panel of Figure 3. Two more SR 
pairs produce the leaping motion. Finally, a single SR pair causes Fred to adopt an inversely curled 
con.guration for the remainder of the trajectory. Mr. Star-Man is another .ve-rod creature, but one with 
a dif­ferent, branched topology. All of his rods are of equal length and mass. Joint-angle ranges are 
de.ned with respect to Star-Man s torso : each limb rod is con.ned to one of the quadrants de­.ned relative 
to his top torso rod. The trajectory in Figure 4 depicts a behavior that evolved after 20 generations: 
Star-Man tips over on his side, and then employs a sideways cantering motion. This motion is cyclic; 
the hop-land sequence (C D C ) is repeated several times. By generation 37 a new behavior had evolved 
the start of a cartwheel maneuver followed by a fall to the right (Figure 5). The best behavior after 
94 generations was yet a third strategy that involved sideways shuf.ing (Figure 6). Beryl Biped is a 
headless2D humanoid with a rigid torso, jointed legs, point feet, and rod masses of human proportion. 
Despite her limitations, Beryl learns human-like locomotion. To elicit these behaviors, we had to change 
the evaluation function. When center-of-mass translation was used as the measure of progress, Beryl adopted 
the short-term strategy of plunging headlong into the ground (not shown). When the reference point was 
changed from the center of mass to the midpoint between her two feet, she still tried falling forward, 
but in 100 generations she discovered the more tactically sound gaits depicted in Figures 7 9. Thus, 
like any optimization procedure, the genetic algorithm will fail to .nd good optima if the evaluation 
function is suf.ciently deceptive, but simple changes to the evaluation function can often restore acceptable 
behavior. 4 Conclusions and future work We have presented an effective algorithm for 2D SC problems 
involving articulated .gures. The trajectories computed by our algorithm differ qualitatively from those 
that would be produced by existing local-search techniques: they are complex, varied, multi-staged, and 
sometimes far from obvious. Our work differs from previous global-search and learning approaches to articulated­.gure 
motion control [6, 4, 12] in its adherence to physical law, the nature of the articulated .gures being 
considered,and the generality of the problem statement, respectively. Developing an algorithm of this 
type requires numerous deci­sions regarding the design of mutation and crossover operators and the assignment 
of parameter values. It is dif.cult to be sure that one has chosen optimally, since doing so is itself 
a combinatorial search problem! However, few of the choices are critical. In this paper and elsewhere 
[13] we have described what we believe are the important choices to get right. Our current work is directed 
towards gaining a better under­standing of how the algorithm works and making more use of its present 
capabilities. In particular we are investigating other stimulus-response representations, ways in which 
a user can in­.uence the GA interactively, alternative search algorithms, and editing techniques whereby 
composite trajectories can be formed by splicing together precomputed behaviors. In the future, we hope 
to extend our techniques to work with 3D articulated .gures. 5 Acknowledgments We thank Tom Cheatham 
for use of the CM-2 at Harvard. Carter Cornwall, Dave Davis, Steven Salzberg, Stuart Shieber, and the 
members of the Harvard Animation Group, the MIT Leg Lab, and the Cambridge Seminar on Natural and Arti.cial 
Computation all provided useful advice and suggestions. Adam Ginsburg wrote support software for the 
project. JTN is grateful for a Graduate Fellowship from the Fannie and John Hertz Foundation. This work 
was supported in part by an NSF grant to Martin Karplus. References [1] D. Baraff. Coping with friction 
for non-penetrating rigid body simulation. Computer Graphics, 25(4):31 40, July 1991. [2] L. S. Brotman 
and A. N. Netravali. Motion interpolation by optimal control. Computer Graphics, 22(4):309 315, August 
1988. [3] M. F. Cohen. Interactive spacetime control for animation. Computer Graphics, 26(2):293 302, 
July 1992. [4] Y. Davidor. A genetic algorithm applied to robot trajectory generation. In L. Davis, editor, 
Handbook of Genetic Algo­rithms, chapter 12, pages 144 165. Van Nostrand Reinhold, New York, 1991. [5] 
L. Davis. Handbook of Genetic Algorithms. Van Nostrand Reinhold, New York, 1991. [6] H. de Garis. Genetic 
programming: Building arti.cial ner­vous systems using genetically programmed neural network modules. 
In Proceedings of the Seventh International Con­ference on Machine Learning , pages 132 139, Austin, 
Texas, June 1990. [7] K. A. De Jong. An Analysis of the Behavior of a Class of Genetic Algorithms. PhD 
thesis, University of Michigan, 1975. [8] D. E. Goldberg. Genetic Algorithms in Search, Optimiza­tion, 
and Machine Learning. Addison-Wesley, Reading, Mas­sachusetts, 1988. [9] J. K. Hahn. Realistic animation 
of rigid bodies. Computer Graphics, 22(4):299 308, August 1988. [10] J. H. Holland. Adaptation in Natural 
and Arti.cial Systems . University of Michigan Press, 1975. [11] J. Luh, M. Walker, and R. Paul. On-line 
computational scheme for mechanical manipulators. Trans. ASME, J. Dy­namic Systems, Measurement,and Control 
, 102:69 76, 1980. [12] P. Maes and R. A. Brooks. Learning to coordinate behaviors. In Proceedings of 
the Eighth National Conference on Arti­.cial Intelligence, pages 796 802, Menlo Park, California, 1990. 
American Association for Arti.cial Intelligence. [13] J. T. Ngo and J. Marks. Physically realistic trajectory 
plan­ning in animation: A stimulus-response approach. Technical Report TR-21-92, Center for Research 
in Computing Tech­nology, Harvard University, October 1992. [14] B. F. Skinner. The Behavior of Organisms; 
An Experimen­tal Analysis. The Century Psychology Series. D. Appleton-Century, New York, London, 1938. 
[15] G. Syswerda. Uniform crossover in genetic algorithms. Pro­ceedings of the Third International Conference 
on Genetic Algorithms, pages 2 9, 1989. [16] J. Wilhelms and R. Skinner. A notion for interactive be­havioral 
animation control. IEEE Computer Graphics and Applications, 10(3):14 22, May 1990. [17] A. Witkin and 
M. Kass. Spacetime constraints. Computer Graphics, 22(4):159 168, August 1988. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166161</article_id>
		<sort_key>351</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>44</seq_no>
		<title><![CDATA[Animation of plant development]]></title>
		<page_from>351</page_from>
		<page_to>360</page_to>
		<doi_number>10.1145/166117.166161</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166161</url>
		<keywords>
			<kw><![CDATA[L-system]]></kw>
			<kw><![CDATA[animation through simulation]]></kw>
			<kw><![CDATA[combined discrete/continuous simulation]]></kw>
			<kw><![CDATA[modeling of plants]]></kw>
			<kw><![CDATA[piecewise-continuous differential equation]]></kw>
			<kw><![CDATA[realistic image synthesis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.4.2</cat_node>
				<descriptor>Parallel rewriting systems (e.g., developmental systems, L-systems)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Biology and genetics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Combined</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010444.10010087</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Computational biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010095</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Systems biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003766.10003767.10003769</concept_id>
				<concept_desc>CCS->Theory of computation->Formal languages and automata theory->Formalisms->Rewrite systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010935</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Genetics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14162862</person_id>
				<author_profile_id><![CDATA[81100465812]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Przemyslaw]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Prusinkiewicz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39072851</person_id>
				<author_profile_id><![CDATA[81100544780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Hammel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15021855</person_id>
				<author_profile_id><![CDATA[81100095011]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mjolsness]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M. Aono and T. L. Kunii. Botanical tree image generation. IEEE Computer Graphics andApplications , 4(5): 10-34, 1984.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>140548</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[R. Barzel. Physically-based modeling for computer graphics -- a structured approach. Academic Press, Boston, 1992.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[C. G. de Koster and A. Lindenmayer. Discrete and continuous models for heterocyst differentiation in growing filaments of blue-green bacteria. Acta Biotheoretica, 36:249-273, 1987.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378505</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[E de Reffye, C. Edelin, J. Frangon, M. Jaeger, and C. Puech. Plant models faithful to botanical structure and development. Proceedings of SIGGRAPH '88 (Atlanta, Georgia, August 1- 5, 1988), in Computer Graphics 22, 4 (August 1988), pages 151-158, ACM SIGGRAPH, New York, 1988.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1062427</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[L. Edelstein-Keshet. Mathematical models in biology. Random House, New York, 1988.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[D.A. Fahrland. Combined discrete event-continuous systems simulation. Simulation, 14(2):61-72, 1970.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[K. W. Fleischer and A. H. Barr. A simulation testbed for the study of multicellular development: Multiple mechanisms of morphogenesis. To appear in Artificial Life III, Addison- Wesley, Redwood City, 1993.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[J.D. Foley, A. van Dam, S. Feiner, and J. Hughes. Computer graphics: Principles and practice . Addison-Wesley, Reading, 1990.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>90691</ref_obj_id>
				<ref_obj_pid>90692</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[F. D. Fracchia, E Prusinkiewicz, and M. J. M. de Boer. Animation of the development of multicellular structures. In N. Magnenat-Thalmann and D. Thalmann, editors, Computer Animation '90, pages 3-18, Tokyo, 1990. Springer-Verlag.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[J. Frangon. Sur la mod61isation de l'architecture et du d6veloppement des v6g6taux. In C. Edelin, editor, L'Arbre. Biologie et D~veloppement. Naturalia Monspeliensia, 1991. No_ hors s6rie.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[N. Greene. Organic architecture. SIGGRAPH Video Review 38, segment 16, ACM SIGGRAPH, New York, 1988.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74351</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[N. Greene. Voxel space automata: Modeling with stochastic growth processes in voxel space. Proceedings of SIG- GRAPH '89 (Boston, Mass., July 31-August 4, 1989), in Computer Graphics 23, 4 (August 1989), pages 175-184, ACM SIGGRAPH, New York, 1989.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>920650</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[J.S. Hanan. Parametric L-systems and their application to the modelling and visualization of plants. PhD thesis, University of Regina, June 1992.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[J. M. Janssen and A. Lindenmayer. Models for the control of branch positions and flowering sequences of capitula in Mycelis muralis (L.) Dumont (Compositae). New Phytologist, 105:191-220, 1987.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>7547</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[W. Kreutzer. System simulation: Programming styles and languages. Addison-Wesley, Sydney, 1986.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[A. Lindenmayer. Mathematical models for cellular interaction in development, Parts I and II. Journal ofTheoreticalBiology, 18:280-315, 1968.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[A. Lindenmayer and H. Jtirgensen. Grammars of development: Discrete-state models for growth, differentiation and gene expression in modular organisms. In G. Rozenberg and A. Salomaa, editors, Lindenmayer systems: Impacts on theoretical computer science, computer graphics, and developmental biology, pages 3-21. Springer-Verlag, Berlin, 1992.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[B. B. Mandelbrot. The fractal geometry of nature. W. H. Freeman, San Francisco, 1982.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[G. S. E Miller. Natural phenomena: My first tree. Siggraph 1988 Film and Video Show.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[G. J. Mitchison and M. Wilcox. Rules governing cell division in Anabaena. Nature, 239:110-111, 1972.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[E. Mjolsness, D. H. Sharp, and J. Reinitz. A connectionist model of development. Journal of Theoretical Biology, 152(4):429-454, 1991.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>135545</ref_obj_id>
				<ref_obj_pid>129873</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[H. Noser, D. Thalmann, and R. Turner. Animation based on the interaction of L-systems with vector force fields. In T. L. Kunii, editor, Visual computing- integrating computer graphics with computervision, pages 747-761. Springer-Verlag, Tokyo, 1992.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>16608</ref_obj_id>
				<ref_obj_pid>16564</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[E Prusinkiewicz. Graphical applications of L-systems. In Proceedings of Graphics Intelface ' 86I Vision Intelface ' 86, pages 247-253, 1986.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[E Prusinkiewicz, M. Hammel, and J. Hanan. Lychnis coronaria. QuickTime movie included in the Virtual Museum CD- ROM, Apple Computer, Cupertino, 1992.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>103565</ref_obj_id>
				<ref_obj_pid>103356</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[E Prusinkiewicz and J. Hanan. Visualization of botanical structures and processes using parametric L-systems. In D. Thaimann, editor, Scientific Visualization and Graphics Simulation, pages 183-201. J. Wiley &amp; Sons, Chichester, 1990.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[E Prusinkiewicz and J. Hanan. L-systems: From formalism to programming languages. In G. Rozenberg and A. Salomaa, editors, Lindenmayer systems: Impacts on theoretical computer science, computer graphics, and developmental biology, pages 193-211. Springer-Verlag, Berlin, 1992.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83596</ref_obj_id>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[E Prusinkiewicz and A. Lindenmayer. The algorithmic beauty ofplants. Springer-Verlag, New York, 1990. With J. S. Hanan, F. D. Fracchia, D. R. Fowler, M. J. M. de Boer, and L. Mercer.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378503</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[E Prusinkiewicz, A. Lindenmayer, and J. Hanan. Developmental models of herbaceous plants for computer imagery purposes. Proceedings of SIGGRAPH '88 (Atlanta, Georgia, August 1-5, 1988), in Computer Graphics 22, 4 (August 1988), pages 141-150, ACM SIGGRAPH, New York, 1988.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>103149</ref_obj_id>
				<ref_obj_pid>103147</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[L. E Shampine, I. Gladwell, and R. W. Brankin. Reliable solution of special event location problems for ODEs. ACM Transactions on Mathematical Software, 17, No. 1:11-25, March 1991.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[K. Sims. Panspermia. SIGGRAPH Video Review, ACM SIGGRAPH, New York, 1990.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808571</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[A. R. Smith. Plants, fractals, and formal languages. Proceedings of SIGGRAPH '84 (Minneapolis, Minnesota, July 22-27, 1984) in Computer Graphics, 18, 3 (July 1984), pages 1-10, ACM SIGGRAPH, New York, 1984.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Animation of Plant Development Przemyslaw Prusinkiewicz Mark S. Hammel Eric Mjolsness Department of 
Computer Science Department of Computer Science Department of Computer Science University of Calgary 
University of Calgary Yale University Calgary, Alberta, Canada T2N 1N4 Calgary, Alberta, Canada T2N 1N4 
New Haven, CT 06520-2158 ABSTRACT intervals. This creates several problems if a smooth animation of development 
is sought [27, Chapter 6]:This paper introduces a combined discrete/continuous model of plant development 
that integrates L-system-style productions and  Although, in principle, the time interval can be arbitrarily 
differential equations. The model is suitable for animating simu­ small, once it has been chosen it becomes 
a part of the model lated developmental processes in a manner resembling time-lapse and cannot be easily 
changed. From the viewpoint of com­photography. The proposed technique is illustrated using several puter 
animation, it is preferable to specify this interval as developmental models, including the .owering 
plants Campanula an easy to control parameter, decoupled from the underlying rapunculoides, Lychnis coronaria 
, and Hieracium umbellatum. model. CR categories: F.4.2 [Mathematical Logic and Formal Lan­  The continuity 
criteria responsible for the smooth progression guages]: Grammars and Other Rewriting Systems: Parallel 
rewrit­ of shapes during animation can be speci.ed more easily in ing systems, I.3.7 [Computer Graphics]: 
Three-Dimensional the continuous time domain.Graphics and Realism: Animation, I.6.3 [Simulation and Mod­ 
eling]: Applications, J.3 [Life and Medical Sciences]: Biology It is conceptually elegant to separate 
the model of develop­ Keywords: animation through simulation, realistic image synthe­sis, modeling of 
plants, combined discrete/continuous simulation, L-system, piecewise-continuous differential equation. 
 1 INTRODUCTION Time-lapse photography reveals the enormous visual appeal of de­veloping plants, related 
to the extensive changes in topology and geometry during growth. Consequently, the animation of plant 
development represents an attractive and challenging problem for computer graphics. Its solution may 
enable us to retrace the growth of organs hidden from view by protective cell layers or tissues, illustrate 
processes that do not produce direct visual effects, and expose aspects of development obscured in nature 
by concurrent phenomena, such as the extensive daily motions of leaves and .ow­ers. Depending on the 
application, different degrees of realism may be sought, ranging from diagrammatic representations of 
develop­mental mechanisms to photorealistic recreations of nature s beauty. Known techniques for simulating 
plant development, such as L­systems [16, 27, 28, 31], their variants proposed by Aono and Ku­nii [1], 
and the AMAP software [4, 10], operate in discrete time, which means that the state of the model is known 
only at .xed time ment, de.ned in continuous time, from its observation, taking place in discrete intervals. 
Smooth animations of plant development have been created by Miller (a growing coniferous tree [19]), 
Sims (arti.cially evolved plant-like structures [30]), and Prusinkiewicz et. al. (a growing herbaceous 
plant Lychnis coronaria [24]), but the underlying tech­niques have not been documented in the literature. 
Greene proposed a model of branching structures [12] suitable for animating accre­tive growth [11], but 
this model does not capture the non-accretive developmental processes observed in real plants. This paper 
introduces a mathematical framework for modeling plants and simulating their development in a manner 
suitable for animation. The key concept is the integration of discrete and con­tinuous aspects of model 
behavior into a single formalism, called differential L-systems (dL-systems), where L-system-style produc­tions 
express qualitative changes to the model (for example, the initiation of a new branch), and differential 
equations capture con­tinuous processes, such as the gradual elongation of internodes. The proposed integration 
of continuous and discrete aspects of de­ velopment into a single model has several predecessors. Barzel 
[2] introduced piecewise-continuous ordinary differential equations (PODEs) as a framework for modeling 
processes de­scribed by differential equations with occasionally occurring discon­tinuities. PODEs lack 
a formal generative mechanism for specifying changes to system con.guration resulting from discrete events, 
and therefore cannot be directly applied to simulate the development of organisms consisting of hundreds 
or thousands of modules. Fleischer and Barr [7] addressed this limitation in a model of mor­phogenesis 
consisting of cells developing in a continuous medium. Thecon.gurationofthesystemisdeterminedimplicitlybyitsge-placesapex 
byastructureconsistingofanewapex ,aninternode  IABLA ometry. For example, in a simulated neural network, 
a synapse is formed when a growing dendrite of one cell reaches another cell. Mjolsness et. al. [21] 
pursued an alternative approach in a connec­tionist model of development. Differential equations describe 
the continuous aspects of cell behavior during interphase (time between cell divisions), while productions 
inspired by L-systems specify changes to the system con.guration resulting from cell division and death. 
The connectionist model makes it possible to consider networks with arbitrary topology (not limited to 
branching struc­tures), but requires productions that operate globally on the entire set of cells constituting 
the model. This puts a practical limit on the number of components in the system. Fracchia et. al. [9] 
(see also [27, Chapter 7]) animated the devel­opment of cellular layers using a physically-based model 
in which differential equations simulate cell growth during the interphase, and productions of a map 
L-system capture cell divisions. The pro­ductions operate locally on individual cells, making it possible 
to simulate the development of arbitrarily large layers using a .nite number of rules. Unfortunately, 
this technique does not seem to extend beyond the modeling of cellular layers. Timed L-systems [27, Chapter 
6] were introduced speci.cally as a formal framework for constructing models of branching structures 
developing in continuous time. They operate under the assumption that no information exchange between 
coexisting modules takes place. This is a severe limitation, as interactions between the mod­ules are 
known to play an important role in the development of many plant species [14, 27, 28]. A practical application 
of timed L-systems to animation is described by Noser et. al. [22]. The model of development proposed 
in this paper combines ele­ments of PODEs, the connectionist model, and L-systems. The necessary background 
in L-systems is presented in Section 2. Sec­tions 3 and 4 introduce the de.nition of differential L-systems 
and illustrate it using two simple examples. Section 5 applies combined discrete/continuous simulation 
techniques to evaluate dL-systems over time. Section 6 focuses on growth functions, which char­acterize 
continuous aspects of model development. Application of differential L-systems to the animation of the 
development of higher plants is presented in Section 7, using the models of a compound leaf and three 
herbaceous plants as examples. A summary of the results and a list of open problems conclude the paper. 
2 L-SYSTEMS An extensive exposition of L-systems applied to the modeling of plants is given in [27]. 
Below we summarize the main features of L-systems pertinent to the present paper. We view a plant as 
a linear or branching structure com- A posed of repeated units called modules. An L-system de- B B 
scribes the development of L this structure in terms of rewriting rules or produc- A I tions, each of 
which replaces the predecessor module by Figure 1: Example of a typical zero, one, or more succes­ L-system 
production sor modules. For example, the production in Figure 1 re­ , and two lateral apices supported 
by leaves . In general, productions can be context free and depend only on the replaced module, or context-sensitive 
and depend also on the neigh­borhood of this module. A developmental sequence is generated by repeatedly 
applying productions to the consecutively obtained structures. In each step, productions are applied 
in parallel to all parts of the structure obtained so far. The original formalism of L-systems [16] has 
a threefold discrete character [17]: the modeled structure is a .nite collection of mod­ules, each of 
these modules is in one of a .nite number of states, and the development is simulated in discrete derivation 
steps. An exten­sion called parametric L-systems [25, 27] increases the expressive power of L-systems 
by introducing a continuous characterization of the module states. Each module is represented by an identi.er 
de­noting the module type (one or more symbols starting with a letter) and a state vector of zero, one, 
or more numerical parameters. For instance, denotes a module of type with two Mw d 1 d A 55 f 9 m 5 
w 2 d9 m 5 MA d 5 f 9 m 5 parameters and , forming the vector w . The interpretation of parameters depends 
on the semantics of the module de.nition, and may vary from one module type to another. For example, 
parameters may quantify the shape of the module, its age, and the concentration of substances contained 
within it. In the formalism of L-systems, modeled structures are represented as strings of modules. Branching 
structures are cap­tured using bracketed strings, with the matching pairs of brackets [ and ] delim­iting 
branches. We visualize these struc­tures using a turtle interpretation of strings [23, 28], extended 
to strings of modules with parameters in [13, 25, 27]. A prede­.ned interpretation is assigned to a set 
of Figure 2: Turtle reserved modules. Some of them representinterpretation of physical parts of the modeled 
plant, for ex­ a sample string ample a leaf or an internode, while others represent local properties, 
such as the magnitude of a branching angle. Reserved modules frequently used in this paper are listed 
below: line segment of length ,  F Tx f 2 T Xxs r T orientation change of the following line by degrees 
with respect to the preceding line, X s a prede.ned surface scaled by the factor . The interpretation 
of a string of modules proceeds by scanning it from left to right and considering the reserved modules 
as commands that maneuver a LOGO-style turtle. For example, Figure 2 shows the turtle interpretation 
of a sample string:  F 1 nF 45 L 0 Lm 75 w FK 0 m 8 n 2 30 L 0 m 5 w F 0 m 6 K 1 f where symbolsanddenote 
prede.ned surfaces depicting a leaf and a .ower. 3 DEFINITION OF dL-SYSTEMS Differential L-systems extend 
parametric L-systems by introducing continuous time .ow in place of a sequence of discrete derivation 
steps. As long as the parameters w of a module wremain in the D A A domain of legal values , the module 
develops in a continuous way. Once the parameter values reach the boundary of the domain , a production 
replaces module w by its descendants D A C A k D A A 2 C t Ad in a discrete event. The form of this 
production may depend on which segment of the boundary of has been crossed. For example, modulein Figure 
3 is cre- M2 ated at time as one of two descendants ... of the initial module M3 . Itdevelopsinthe ... 
interval , and   t 1 n t d ft F M 4 ceases to exist at time , giving rise to two 0 ta tß time Figure 
3: Fragment of the lineage tree new modules andof a hypothetical modular structure . The instant is M 
5 Mt F 2 the time at which parameters of reach the boundary of its do­main of legal states . A hypothetical 
trajectory of module in D its parameter space is depicted in Figure 4. In order to formalize the above 
description, let us assume that the modeled structure consists of a sequence of modules (an extension 
to branching structures is straightforward if a proper de.nition of M5 t context is used [27, 28]). The 
state of the structure at time is represented as a string: The module wimmediately preceding a given 
module w in the string is called the left neighbor or left context of w, and the module wimmediately 
following wis called its right neighbor or right context. When it is inconvenient to list the indices, 
we use the symbols,, and/or subscripts , to specify the context of w , as in the expression: A i A ii 
M1 i lrA i d h 1 d A l A i h d 11 l 1 MA A 4 2 i e1 M .22 . . i e1 A r A nrF m n m www The continuous 
behavior of w is described by an ordinary dif­ferential equation that determines the rate of change wof 
pa­rameters w as a function of the current value of these parameters and those of the module s neighbors: 
wwww The above equation applies as long as the parameters w are in the domain characteristictothemoduletype 
. Weassumethat is an open set, and specify its boundary as the union of a .nite D m C AA t u F k 1 ddt 
lim wd Af A t D wAl 2 f C f A C k rA mmA C wA k kA d d 1 fedt 2 fmmmf D m A number of nonintersecting 
segments , . The time at which the trajectory of module w) reaches a segment of the boundary of satis.es 
the expression: w The replacement of module w by its descendants at time is described by a production: 
www www p A k BA ka B A 1 kl 1 k l1 k B 1 tkAB t2 k A 2 k 2k A 2 r B k kr B mk2 k m A k k mk m kk C 
m A k t F The module w is called the strict predecessor and the sequence of modules wwwis called the 
successor of this production. The index emphasizes that different productions can be associated with 
individual segments of the Figure 4: A hypothetical trajectory of module in its parameter 2 space boundary 
. The initial value of parameters assigned to a module wupon its creation is determined by a function 
which takes as its arguments the values of the parameters w, w, and wat the time immediately preceding 
production application: B rk jk j C A h k0 jk j dn ttf lim t t F h o A t k j w C2 l tftf D rB k j tAm 
l h A k j wwww The vector wmust belong to the domain . (A stronger condition is needed to insure that 
the number of productions applied in any .nite interval will be .nite.) In summary, a differential L-system 
is de.ned by the initial string of modules and the speci.cation of each module type under consideration. 
The speci.cation of a module type consists of four components: D A f C A ff A fP A f where: the open 
set is the domain of legal parameter values of modules of type , the set is the boundary of , consisting 
of nonintersecting segments , the function speci.es a system of differential equations that describe 
the continuous behavior of modules of type in their domain of legal parameter values , MMAp C AA k d 
2D f A P A C AA 1 C hmPm A A mkk j d Cf A p A m 1 fmAmm C f A p D 1 A f A C m m A m g mf C A m D p AA 
A k the set of productions captures the discrete behavior of modules of type . A production is applied 
when the parameters of a module of type reach segment of the boundary . At this time module disappears, 
and zero, one, or more descendant modules are created. The functions embedded in productions determine 
the initial values of parameters in the successor modules.    Fr(1) Fl(12) Fr(12)Fr( 22) Fl( 22) 
 Fl(12) Figure 5: Initial steps in the construction of a dragon curve 4 EXAMPLES OF dL-SYSTEMS We will 
illustrate the notion of a dL-system using two sample models suitable for animating the development of 
the dragon curve and the .lamentous alga Anabaena catenula. 4.1 A dL-system model of the dragon curve 
In the discrete case, consecutive iterations of the dragon curve (described, for example, in [27, Chapter 
1]) can be obtained by the following parametric L-system: sp 12 a 2 F rl 2 s F A F r 1 F 2 F r 2 ls p 
22 r 2 F 2 F F ll s p 222 r F 2 45 o Assuming that symbols and represent turns of , this L­system encodes 
a Koch construction [18, Chapter 6] that repeatedly substitutes sides of an isosceles right-angled triangle 
for its hy­potenuse (Figure 5). Subscripts and indicate that the triangle is formed respectively on the 
left or right side of the oriented predeces­sor segment. A corresponding dL-system that generates the 
dragon curve through the continuous progression of shapes indicated in Figure 6 is given below: initial 
string:F r xfsx a s 22 F r2 1 f s 1 p FF p 2 if solve if produce if solve F l xf ss d aa s dt F F T srf 
0 ds dt s d p 22 0 2 F h sf s 2 F l 0 f s p 22 F if produce if solve if produce The operation of this 
model starts with the replacement of the initial module with the string:F dxdthh 1 d p x 12 F 2 d rT 
T 1 0 1 2 f 1 F r 0 f dx dt p 2 ( d 2 F 2 h F T Ts Ff hdsdt 1 Ff d1 r 0 dxdt Fd F ll 2 0 f T 1 p 22 2 
f which has the same turtle interpretation: a line segment of unit length . Next, the horizontal line 
segment represented by module decreasesinlengthwiththespeed ,whilethediagonal segments represented by 
modules and elongate with the speed . The constant determines the lifetime of the modules: after time 
, the module reaches zero length and is removed from The turtle interprets the .rst parameter as the 
segment length, and ignores the second parameter. the string (replaced by the empty string ), while both 
modules F l p 2 ( 2 F r and reach their maximum length of and are replaced by their respective successors. 
These successors subsequently follow the same developmental pattern. It is not accidental that the predecessor 
and the successor of the productions for and have identical geometric in- F r xfsF l xfs terpretations. 
Since productions are assumed to be applied instan­taneously, any change of the model s geometry introduced 
by a production would appear as a discontinuity in the animation. In general, correctly speci.ed productions 
satisfy continuity criteria [27, Chapter 6], which means that they conserve physical entities such as 
shape, mass, and velocity of modules. 4.2 A dL-system model of Anabaena catenula The continuously developing 
dragon curve has been captured by a context-free dL-system, in which all productions and equations depend 
only on the strict predecessor module. A simple example of a context-sensitive model inspired by the 
development of the blue-green alga Anabaena catenula [3, 20, 27] is given below. Anabaena forms a nonbranching 
.lament consisting of two classes of cells: vegetative cells and heterocysts. A vegetative cell usu­ally 
divides into two descendant vegetative cells. However, in some cases a vegetative cell differentiates 
into a heterocyst. The spacing between heterocysts is relatively constant, in spite of the continuing 
growth of the .lament. Mathematical models explain this phenomenon using a biologically motivated hypothesis 
that the distribution of heterocysts is regulated by nitrogen compounds produced by the heterocysts, 
diffusing from cell to cell along the .lament, and decaying in the vegetative cells. If the compound 
concentration in a vegetative cell falls below a speci.c level, this cell differentiates into a heterocyst 
(additional factors are captured by more sophisticated models). A model operating in continuous time 
according to this description can be captured by the following dL-system: initial string:  Fx l fcx 
l x F max v F dxdth xf i d xc max rc xffF dcdt cc max x min d r fDcF rv a xc maxl F cf r c max 2 2 cF 
h 2 xhc max fc max if solve if produce F h xfcc d c mindxdt FFF v d hv rkxf x xxc maxmax fc 2 Fx vfF 
1 dcdth 2 d krx cmax c max fc 2 c if produce : solve Vegetative cells and heterocysts are characterized 
by their length and concentration of nitrogen compounds . The differ­ential equations for the vegetative 
cell indicate that while the cell length is below the maximum value and the compound concentration is 
above the threshold , the cell elongates ex­ponentially according to the equation , and the compound 
concentration changes according to the equation:   xxcdcdt d Dc l F c r 2 dxdt c 2 min Fc d v x 2 max 
rxhcmc The .rst term in this equation describes diffusion of the compounds through the cell walls. Following 
Fick s law [5, page 404], the Figure 6: Development of the dragon curve simulated using a dL­system, 
recorded in time intervals . Top left: Superim­posed stages , top right: stages , bottom row: stages 
16 2 24240 c 2 r 832 2 cc l 2 c o t 8d 2 hc 81 16 T and . rate of diffusion is proportional to the differences 
of compound concentrations, and , between the neighbor cells and the cell under consideration. The term 
describes exponential decay of the compounds in the cell. In addition to the differential equations, 
two productions describe the behavior of a vegetative cell. If the cell reaches maximum length while 
the concentration is still above the threshold , the cell divides into two vegetative cells of length 
andc 1 min 2 kxx max c min cc kx max , with the compound concentration inherited from their parent cell. 
Otherwise, if the concentration drops down to the threshold , the cell differentiates into a heterocyst. 
Both productions satisfy the continuity criteria by conserving total cell length and concentration of 
nitrogen compounds. The last line of the model speci.es the behavior of the heterocysts. Their length 
and compound concentration converge exponentially to the limit values of and . The heterocysts do not 
x max c max undergo any further transformations. Simulation results obtained using the above model are 
shown in Figure 7. The cells in the .lament are represented as horizontal line segments with the colors 
indicating the concentration of nitro­gen compounds. Consecutive developmental stages are drawn one under 
another. An approximately equal spacing between the hete­rocysts (shown in white) is maintained for any 
horizontal section, as postulated during model formulation. Note that for incorrectly chosen constants 
in the model, the spacing between heterocysts may be distorted; for example, groups of ad­jacent vegetative 
cells may almost simultaneously differentiate into heterocysts. 5 EVALUATION OF dL-SYSTEMS Although Figures 
6 and 7 were obtained using dL-systems, we have not yet discussed the techniques needed to evaluate them. 
This term denotes the calculation of the sequence of strings ,  h o t d h 1 fmmmfhn o t d h n o th 
0 d h 0 representing the states of the modeled structure at the desired intervals . We address the prob­lem 
of dL-system evaluation in the framework of the combined discrete/continuous paradigm for system simulation 
introduced by Fahrland [6] and presented in a tutorial manner by Kreutzer [15]. Figure 7: Diagrammatic 
representation of the development of An­abaena catenula, simulated using a dL-system with the constants 
set to the following values: , , , D d h d0 m o03 t d r od1 t 1 m 01 kx max d0 tm min 37d c min 1 r d 
x c max d2000 m d1 r 255 t cmax d cc 0 min d m 15 c 575 max d5 ,,,, .The development was recorded from 
to at the intervals . Developmental stages are shown as horizon­tal lines with the colors indicating 
the concentration of nitrogen compounds. Dark brown represents ; white represents . According to this 
paradigm, the evaluation can be viewed as a dy­namic process governed by a scheduler: a part of the simulation 
program that monitors the state of the model, advances time, and dispatches the activities to be performed. 
In the absence of dis­crete events (productions), the scheduler repeatedly advances time by the time 
slice . During each slice, the differential equations associated with the modules are integrated numerically 
(using an integration technique appropriate for the equations in hand), thus advancing the state of the 
structure from to . If the scheduler detects that a discrete event should occur (i.e., a produc­tion 
should be applied) at time within the interval , this t 0 n tfht 0 th n t n t 0 tf f F ttt F t 0 o 0 
o ttt interval is divided into two subintervals and . The differential equations are integrated in the 
interval and yield parameter values for the production application at time . The pro­duction determines 
the initial values for the differential equations associated with the newly created modules; these equations 
are in­tegrated in the remaining interval . Each of the intervalsn tft 0 n t 0 f n ttft F 0 t Fo t o 
t n t 0 n ftft F t Foo tt and is subdivided further if more discrete events occur during . Plant structures 
generated using dL-systems may consist of large numbers (thousands) of modules. If many modules are replaced 
at different times during the interval , the global advance­ment of time may require an excessive subdivision 
of this interval, leading to a slow evaluation of the model. This problem can be solved by detecting 
and processing events the interval individually for each module. The increase of simulation speed is 
obtained at the expense of accuracy, since the state of the context tt 00 2 tftt 0 t Fo t n tft Fo t 
of a module replaced at time must be approxi­mated, for example, by its state at time . No accuracy is 
lost in the context-free case. In the above description we assumed that the scheduler is capa­ble of 
detecting each instant at which a discrete event occurs. If the differential equations are suf.ciently 
simple, we can solve them analytically and determine time explicitly. In general, we need numerical techniques 
for special event location in piecewise­continuous ordinary differential equations, as described by Sham­pine 
et. al. [29], and Barzel [2, Appendix C]. x max x 0203 x 6 x max x 6 ..6 o tx 01 0.0 1.0 2.0 T Figure 
8: Examples of sigmoidal growth functions. a) A family of G. t x T r d3 min m 0 logistic functions plotted 
using for different initial values x 0 . b) A cubic function . 6 GROWTH FUNCTIONS Growth functions describe 
continuous processes such as the ex­pansion of individual cells, elongation of internodes, and gradual 
increase of branching angles over time. For example, the differ­ential equations included in the dL-system 
for the dragon curve (Section 4.1) describe linear elongation of segments and , and F h F r F l linear 
decrease in length of segments . The dL-system model of Anabaena (Section 4.2) assumes exponential elongation 
of cells. In higher plants, the growth functions are often of sigmoidal (S­shaped) type, which means 
that they initially increase in value slowly, then accelerate, and eventually level off at or near the 
maxi­mum value. A popular example of a sigmoidal function is Velhurst s logistic function (c.f. [5, page 
212]), de.ned by the equation: dxdt d r z 1 2 x 0 max x X xx 0 with a properly chosen initial value (Figure 
8a). Speci.cally, must be greater than zero, which means that neither the initial length nor the initial 
growth rate of a module described by the logistic function will be equal to zero. In order to obtain 
a continuous progression of forms, it is often convenient to use a growth function that has zero growth 
rates at both ends of an interval within which its value increases from (possibly zero) to . These x 
min x max T requirements can be satis.ed, for example, by a cubic function of time. Using the Hermite 
form of curve speci.cation [8, page 484], we obtain: o xdxdt d x d x max 2 t 6d 2 o T 3 2 xx min t 2 
2 x o T 0 F 3 x d6 t 3 o Tx F min 2 tx 2 t 3do T n0 2 x 6 fTt o T 2 w 2 x F z x 1 min 2 fTt X t where 
and . The equivalent differ­ential equation is: with the initial condition . In order to extend this 
curve to in.nity (Figure 8b), we de.ne: for dxdt d G x T t d X 60 T 2x 1 2 Tt tt 2 n0 GTffT Fw 1 m for 
Although the explicit dependence of the function on time is questionable from the biological point of 
view (a plant module does not have a means for measuring time directly), parametric cubic functions constitute 
a well understood computer graphics tool [8, Chapter 11.2] and can be conveniently used to approximate 
the observed changes of parameter values over time. Figure 9: Development of a compound leaf simulated 
using a dL- system. Parameter values are: , , , ,, ,,  0 m 5 r a d2 m 0 x amax d T 3 0 m 0d rn 2 i 
0 m d0d1 r d m 40d x 0 imax 1d01 T d0 max x 3 m th 0dd 0 602d0 0 k 05d , , ,, ,and r o s t d 20 mm 
001 s max d6 m 0 . The stages shown represent frames 50, 215, 300, 400, 500, 600, and 900 of an animated 
sequence. 7 MODELING OF HIGHER PLANTS In this section we present sample applications of dL-systems to 
the animation of the development of higher plants. 7.1 Pinnate Leaf A pinnate leaf provides a simple 
example of a monopodial branch­ing structure. Monopodial branching occurs when the apex of the main axis 
produces a succession of nodes bearing organs leaves or .owers which are separated by internodes. In 
the case of pinnate leaves with the lea.ets occurring in pairs (termed opposite arrangement), the essence 
of this process can be captured by the L-system production [27, page 71]: F a F a 2A F i nF F i L wn 
2 L w F a f , L , where denotes the apex, an internode, and a lea.et. The dL-system model given below 
extends this L-system with growth functions that control the expansion of all components and gradually 
increase branching angles over time. initial string: if solve if F a xfnx x dda x x th th F dxdxdta 
ii F d xnn ai 0 frkx d n ax 1 0 00 2 1 nF L 2 ksTxf x 00 amaxxx n 2 L 1 sxf 0dn wn dt 2 d T 0 Ls 0 w 
produce if produce solve Lsx dsdti 1 sd imax s solve F r i T aaa dddt d r sd 1 2 xs max d xT solve The 
apex has two parameters and which indicate its current   rF a xx amax n length and the remaining number 
of internodes to be produced. The apex elongates according to the logistic function with parameters (controlling 
growth rate) and (controlling the asymptotic apex length). Upon reaching the threshold length , the apex 
produces a pair of lea.ets and subdivides into an internode of length and a shorter apex of length. Once 
theF i kxn 0 L 1 2 kx th x prede.ned number of leaf pairs have been created, the apex transforms itself 
into an internode and produces the terminal lea.et. The length of internodes, the size of lea.ets, and 
the magnitude of the branching angles increase according to the logistic functions. Snapshots of the 
leaf development simulated by the above model are shown in Figure 9.  Figure 10: Development of the 
herbaceous plant Campanula ra­punculoides. The snapshots show every frame of a computer animation, starting 
with frame 175. 7.2 Campanula rapunculoides The in.orescence of Campanula rapunculoides (creeping bell­.ower) 
has a monopodial branching structure similar to that of a pinnate leaf; consequently, it is modeled by 
a similar dL-system: initial string: if solve if   F a xfnx x d a x x thth F dxa i F d xnn i 0 fvkx 
d nfx0 dndt 0 nF K d T 0 0 K w F a 1 2 kxfn 2 1 produce if produce solve F i Tx aa dxdddt d G x Td T 
12 t solve The apex is assumed to grow at a constant speed. Cubic growth functions describe the elongation 
of internodes and the gradual in­crease of branching angles. The combination of the linear growth of 
the apex with the cubic growth of the internodes results in .rst­order continuity of the entire plant 
height (except when apex is transformed into internode and terminal .ower ). F i KF a Figure 10 presents 
a se­ (4,2) (4,3) quence of snapshots from an animation of Campan­ula s development. It was obtained 
using the above dL-system augmented with rules that govern the de­velopment of .owers K (2,1) (2,4) from 
a bud to an open .ower to a fruit. The petals and sepals have been modeled as B´ezier patches, speci.ed 
by control pointsFigure 11: A B´ezier patch de­ placed at the ends of simple.ned by a branching structure 
branching structures (Fig­ure 11). Each structure is attached to the remainder of the model at point 
S. The lengths of the line segments and the magnitudes of the branching angles have been controlled by 
cubic growth functions, yielding the developmental sequence shown in Figure 12. When the .ower transforms 
into a fruit, productions instantaneously remove the petals from the model (it is assumed that the time 
over which a petal falls off is negligible compared to the time slice used for the animation of development). 
Manipulation of B´ezier patches using L-systems has been described in detail by Hanan [13]. 7.3 Lychnis 
coronaria The in.orescence of Lychnis coronaria (rose campion) is an ex­ample of a sympodial branching 
structure, characterized by large branches that carry the main thrust of development. As presented in 
[27, page 82] and [28], the apex of the main axis turns into a .ower shortly after the initiation of 
a pair of lateral branches. Their apices turn into .owers as well, and second-order branches take over. 
The lateral branches originating at a common node develop at the same rate, but the development of one 
side is delayed with respect to the other. This process repeats recursively, as indicated by the following 
L-system: s 121 A i 2 A 0 AF n e1 AA 4 wn A 4 F w FK r Ki 7 p p a 7 A i 0 0 Production shows that, at 
their creation time, the lateral apices have different states and . Consequently, the .rst apex re­quires 
eight derivation steps to produce .ower and initiate a new pair of branches, while the second requires 
only four steps. A corresponding dL-system using cubic growth functions to de­scribe the elongation of 
internodes is given below: initial string: if solve AFAx a A d A max AA maxdxdedt Fd G 1 0 n Ax T 0 wn 
tA e max 2 w F 0 K if produce solve For simplicity, we have omitted leaves and symbols controlling the 
relative orientation of branches in space. The operation of the model  Figure 13: Development of Lychnis 
coronaria. The snapshots show every frame of a computer animation, starting with frame 150. is governed 
by apices characterized by their age and assumed to have negligible size. Upon reaching the maximum age 
, an apex splits into two internodes , creates two lateral apices with different initial age values and 
, and initiates .ower . In order to satisfy continuity criteria, the initial length of internodes is 
assumed to be zero. 7.4 Hieracium umbellatum The compound leaf and the in.orescences of Campanula and 
Ly­chnis have been captured by context-free dL-systems, assuming no .ow of information between coexisting 
modules. Janssen and Lindenmayer [14] (see also [27, Chapter 3] and [28]) showed that context-free models 
are too weak to capture the whole spectrum of developmental sequences in plants. For example, the basipetal 
.owering sequence observed in many compound in.orescences re­quires the use of one or more signals that 
propagate through the developing structure and control the opening of buds. Such a se­quence is characterized 
by the .rst .ower opening at the top of the main axis and the .owering zone progressing downward towards 
the base of the plant. Figure 14 shows a synthetic image of Hieracium umbellatum, a sam­ple composite 
plant with a basipetal .owering sequence. Following model I postulated by Janssen and Lindenmayer, we 
assume that the opening of buds is controlled by a hormone generated at some point of time near the base 
of the plant and transported towards the apices. The hormone propagates faster in the main axis than 
in the lateral branches. As a result, it .rst reaches the bud of the main axis, then Figure 14: A model 
those of the lateral branches in the of Hieracium umbel­ basipetal sequence. The growth latum. of the 
main axis and of the lateral branches stops when the hormone attains their respective termi­nal buds. 
In addition, the hormone penetrating a node stops the Figure 15: Development of Hieracium umbellatum. 
The stages shown represent frames 170, 265, 360, 400, 470, 496, and 520 of an animated sequence. development 
of a leaf originating at this node. Snapshots from a diagrammatic animated developmental sequence illustrating 
this process are shown in Figure 15. The complete listing of the dL­ system capturing the develop­ ment 
of Hieracium is too long a x to be included in this paper, but a speci.cation of the activities of the 
main apex provides a good illustration of the context­ sensitive control mechanism in­ h volved. We conceptualize 
this apex as a growing and periodi­ cally dividing tube of length , Figure 16: A concep-which may be 
penetrated by the tual model of the apex. hormone to a height (Fig-  F a 2 hF r a 1 xx ure 16). The 
apex can assume three states: (not yet reached by the hormone), (being penetrated by the hormone), and 
(completely .lled with the hormone). The apical behavior is captured by the following rules: solve if 
F l x l fx h l l d xhh th F lladxdt ii 0 F x d xx i 0 G a kxf thth 0 n F a 0 0 w F a 0 1 2 kx produce 
if produce : if solve if produce if F a 1 xfx kx hx d hh h u i dxdt i kxxF d x ai a i 112 G d xk xfx 
x th xf th x d fhkxx n dhdtth F aa 0d 1 0 w v 1 F 2 a 0kxf 1 2 h 2 kxkx produce if produce The .rst three 
rules model the apex without the hormone. If the pre­ceding internode is not yet completely penetrated 
by the hormone and the length of the apex is below the threshold value , the apex elongates according 
to the growth function . xx thl if h l F a 1 x l F d l h l xF i 0 x d x th F l F a F 0 a 0 Gx Upon reaching 
the threshold length , the apex subdi­vides, producing an internode and a lateral apex . Finally, once 
the hormone penetrates the entire internode (as indicated by the condition ), it .ows into the apex, 
which then changes its state to . Figure 17: Development of a single .ower head of Hieracium umbellatum 
The continuous rule for describes the growth of the apex with rate and the propagation of the hormone 
with constant speed . The next two productions capture the alternate cases of the apex vGxF a 1 F a 2 
hkx subdivision, with the hormone level below or above the level at which the new internode splits from 
the apex. The last production is applied when the hormone reaches the tip of the apex, and changes its 
state to the .owering state . The complete model of Hieracium umbellatum contains additional rules that 
describe the elongation of internodes, the propagation of the hormone within and between the internodes, 
and the develop­ment of .ower heads. The heads undergo the sequence of trans­formations illustrated in 
Figure 17. The bracts (green parts of the .ower head) have been represented using B´ezier patches controlled 
by the dL-system, while the petals have been formed as extend­ing chains of .lled rectangles, with the 
angles between consecutive rectangles controlled by cubic growth functions. This technique allowed us 
to represent each petal with a relatively modest number of polygons (10). 8 CONCLUSIONS We have introduced 
differential L-systems as a combined dis­crete/continuous model suitable for computer simulation and 
an­imation of plant development. Continuous aspects of module be­havior are described by ordinary differential 
equations, and discon­tinuous qualitative changes are captured by productions. The link between L-systems 
and dL-systems makes it possible to use existing discrete developmental models as a starting point for 
constructing dL-systems suitable for animation. Differential L-systems have a wide spectrum of prospective 
appli­cations, ranging from modest projects, such as the diagrammatic animation of developmental mechanisms 
employed by plants, to ambitious ones, such as the realistic animation of the growth of ex­tinct plants. 
On the conceptual level, dL-systems expand piecewise­continuous differential equations with a formal 
speci.cation of dis­crete changes to system con.guration. The resulting formalism makes it possible to 
model developing branching structures with a theoretically unlimited number of modules. From a different 
perspective, dL-systems can be considered as the continuous-time extension of parametric L-systems. The 
following problems still require solutions: Combined differential-algebraic speci.cation of continu­ous 
processes. In some cases it is convenient to describe continuous aspects of model behavior using explicit 
func­tions of time instead of differential equations. For example, the expression of the cubic growth 
function using the differ­ential equation presented in Section 6 is somewhat arti.cial. In order to accommodate 
explicit function speci.cations, the de.nition of dL-systems should be extended to comprehend differential-algebraic 
equations. Incorporation of stochastic rules. Differential L-systems have been formulated in deterministic 
terms. Stochastic rules should be incorporated to capture the specimen-to-specimen variations in modeled 
plants, as has been done for L-systems. Development of the simulation software. The simulations discussed 
in this paper were carried out using a programming language based on parametric L-systems [13, 26]. In 
this environment, the user must explicitly specify the formulae for numerically solving the differential 
equations included in the models (the forward Euler method was used in all cases). From the user s perspective, 
it would be preferable to incorporate a differential equation solver into the simulator, and specify 
the models directly in terms of dL-systems. Improved realism of dL-system models. We have not ad­dressed 
many practical problems related to the construction of realistic models, such as the avoidance of intersections 
between modules, the improved modeling of growing plant organs (petals, leaves, and fruits), and the 
simulation of wilt­ing. The simulation and visualization of natural phenomena has the in­triguing charm 
of blurring the line dividing the synthesis of images from the re-creation of nature. The animation of 
plant development adds a new phenomenon to this (un)real world. Acknowledgements We would like to thank 
Jim Hanan for his essential work on the plant modeling software cpfg used in the simulations, and for 
valuable references and comments. At different points in time, Gavin Miller, Karl Sims and Alvy Ray Smith 
revealed to us the techniques used in their developmental animations. M. Raju and C. C. Chinnappa explained 
the details of the development of Lychnis coronaria and Hieracium umbellatum. We also gained many insights 
from illumi­natingdiscussionswithBill Remphrey,JohnReinitz,StanLetovsky, and Keith Ferguson. This research 
was sponsored by an operating grant and a graduate scholarship from the Natural Sciences and En­gineering 
Research Council of Canada, and a grant from the U.S. Air Force Of.ce of Scienti.c Research. References 
[1] M. Aono and T. L. Kunii. Botanical tree image generation. IEEE Computer Graphics and Applications,4(5):10 
34, 1984. [2] R. Barzel. Physically-based modeling for computer graphics a structured approach. Academic 
Press, Boston, 1992. [3] C. G. de Koster and A. Lindenmayer. Discrete and continuous models for heterocyst 
differentiation in growing .laments of blue-green bacteria. Acta Biotheoretica, 36:249 273, 1987. [4] 
P. de Reffye, C. Edelin, J. Franc¸on, M. Jaeger, and C. Puech. Plant models faithful to botanical structure 
and development. Proceedings of SIGGRAPH 88 (Atlanta, Georgia, August 1 5, 1988), in Computer Graphics 
22, 4 (August 1988), pages 151 158, ACM SIGGRAPH, New York, 1988. [5] L. Edelstein-Keshet. Mathematical 
models in biology. Ran­dom House, New York, 1988. [6] D. A. Fahrland. Combined discrete event continuous 
systems simulation. Simulation, 14(2):61 72, 1970. [7] K. W. Fleischer and A. H. Barr. A simulation testbed 
for the study of multicellular development: Multiple mechanisms of morphogenesis. To appear in Arti.cial 
Life III, Addison-Wesley, Redwood City, 1993. [8] J. D. Foley, A. van Dam, S. Feiner, and J. Hughes. 
Computer graphics: Principles and practice. Addison-Wesley, Reading, 1990. [9] F. D. Fracchia, P. Prusinkiewicz, 
and M. J. M. de Boer. An­imation of the development of multicellular structures. In N. Magnenat-Thalmann 
and D. Thalmann, editors, Computer Animation 90, pages 3 18, Tokyo, 1990. Springer-Verlag. [10] J. Franc¸elisation 
de l architecture et du d´on. Sur la mod´evelop­pement des v´eg´etaux. In C. Edelin, editor, L Arbre. 
Biologie et D´eveloppement. Naturalia Monspeliensia, 1991. Nohors ¯ s´erie. [11] N. Greene. Organic 
architecture. SIGGRAPH Video Review 38, segment 16, ACM SIGGRAPH, New York, 1988. [12] N. Greene. Voxel 
space automata: Modeling with stochas­tic growth processes in voxel space. Proceedings of SIG-GRAPH 89 
(Boston, Mass., July 31 August 4, 1989), in Computer Graphics 23, 4 (August 1989), pages 175 184, ACM 
SIGGRAPH, New York, 1989. [13] J. S. Hanan. Parametric L-systems and their application to the modelling 
and visualization of plants. PhD thesis, University of Regina, June 1992. [14] J. M. Janssen and A. Lindenmayer. 
Models for the control of branch positions and .owering sequences of capitula in Mycelis muralis (L.) 
Dumont (Compositae). New Phytologist, 105:191 220, 1987. [15] W. Kreutzer. System simulation: Programming 
styles and languages. Addison-Wesley, Sydney, 1986. [16] A. Lindenmayer. Mathematical models for cellular 
interaction in development, Parts I and II. Journal of Theoretical Biology, 18:280 315, 1968. [17] A. 
Lindenmayer and H. J¨urgensen. Grammars of develop­ment: Discrete-state models for growth, differentiation 
and gene expression in modular organisms. In G. Rozenberg and A. Salomaa, editors, Lindenmayer systems: 
Impacts on the­oretical computer science, computer graphics, and develop­mental biology, pages 3 21. 
Springer-Verlag, Berlin, 1992. [18] B. B. Mandelbrot. The fractal geometry of nature. W. H. Freeman, 
San Francisco, 1982. [19] G. S. P. Miller. Natural phenomena: My .rst tree. Siggraph 1988 Film and Video 
Show. [20] G. J. Mitchison and M. Wilcox. Rules governing cell division in Anabaena. Nature, 239:110 
111, 1972. [21] E. Mjolsness, D. H. Sharp, and J. Reinitz. A connection­ist model of development. Journal 
of Theoretical Biology, 152(4):429 454, 1991. [22] H. Noser, D. Thalmann, and R. Turner. Animation based 
on the interaction of L-systems with vector force .elds. In T. L. Ku­nii, editor, Visual computing integrating 
computer graphics with computervision, pages747 761. Springer-Verlag, Tokyo, 1992. [23] P. Prusinkiewicz. 
Graphical applications of L-systems. In Proceedingsof GraphicsInterface 86 VisionInterface 86 , pages 
247 253, 1986. [24] P. Prusinkiewicz, M. Hammel, and J. Hanan. Lychnis coro­naria. QuickTime movie included 
in the Virtual Museum CD-ROM, Apple Computer, Cupertino, 1992. [25] P. Prusinkiewicz and J. Hanan. Visualization 
of botanical struc­tures and processes using parametric L-systems. In D. Thal­mann, editor, Scienti.c 
Visualization and Graphics Simula­tion, pages 183 201. J. Wiley &#38; Sons, Chichester, 1990. [26] P. 
Prusinkiewicz and J. Hanan. L-systems: From formalism to programming languages. In G. Rozenberg and A. 
Salomaa, ed­itors, Lindenmayer systems: Impacts on theoretical computer science, computer graphics, and 
developmental biology ,pages 193 211. Springer-Verlag, Berlin, 1992. [27] P. Prusinkiewicz and A. Lindenmayer. 
The algorithmic beauty of plants. Springer-Verlag, New York, 1990. With J. S. Hanan, F. D. Fracchia, 
D. R. Fowler, M. J. M. de Boer, and L. Mercer. [28] P. Prusinkiewicz, A. Lindenmayer, and J. Hanan. 
Develop­mental models of herbaceous plants for computer imagery purposes. Proceedings of SIGGRAPH 88 
(Atlanta, Geor­gia, August 1 5, 1988), in Computer Graphics 22, 4 (August 1988), pages 141 150, ACM SIGGRAPH, 
New York, 1988. [29] L. F. Shampine, I. Gladwell, and R. W. Brankin. Reliable solu­tion of special event 
location problems for ODEs. ACM Trans­actions on Mathematical Software, 17, No. 1:11 25, March 1991. 
[30] K. Sims. Panspermia. SIGGRAPH Video Review, ACM SIGGRAPH, New York, 1990. [31] A. R. Smith. Plants, 
fractals, and formal languages. Proceed­ings of SIGGRAPH 84 (Minneapolis, Minnesota, July 22 27, 1984) 
in Computer Graphics, 18, 3 (July 1984), pages 1 10, ACM SIGGRAPH, New York, 1984. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166162</article_id>
		<sort_key>361</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>45</seq_no>
		<title><![CDATA[Modeling soil]]></title>
		<subtitle><![CDATA[realtime dynamic models for soil slippage and manipulation]]></subtitle>
		<page_from>361</page_from>
		<page_to>368</page_to>
		<doi_number>10.1145/166117.166162</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166162</url>
		<keywords>
			<kw><![CDATA[physically based modeling]]></kw>
			<kw><![CDATA[real-time simulation]]></kw>
			<kw><![CDATA[slippage]]></kw>
			<kw><![CDATA[soil dynamics]]></kw>
			<kw><![CDATA[soil manipulation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.2</cat_node>
				<descriptor>Earth and atmospheric sciences</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Visual</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10010365</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Visual analytics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010437</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Earth and atmospheric sciences</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Experimentation</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31084536</person_id>
				<author_profile_id><![CDATA[81392601469]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14106190</person_id>
				<author_profile_id><![CDATA[81100284557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[Michael]]></middle_name>
				<last_name><![CDATA[Moshell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Balovnev, V.I. New Methods for Calculating Resistance to Cutting of Soil. Translated from Russian, Published for the U.S. Department of Agriculture and the National Science Foundation. Washington, D.C., 1983.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bromhead, E. N. The Stability of Slopes. Surrey University Press, 1986.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Burg, Jeniffer, Moshell, J. Michael, et al., Behavioural Representation in Virtual Reality. Proceedings of Behavioral Representation Symposium. Institute for Simulation and Training. Orlando, FL, 1991.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Cernica, J. N., Geotechnical Engineering. Holt, Rinehart &amp; winston, 1982.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Chowdhury, R. N., Slope Analysis. Elsevier North-Holland Inc., 1978.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Das, Braja M. Principles of Geotechnical Engineering. Second Edition, PWS-KENT Publishing Company, 1990.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Huang, Y. H., Stability Analysis of Earth Slopes. Van Nostrand Reinhold Co., 1983.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97884</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Kass, Michael and Miller, Gavin. Rapid, Stable Fluid Dynamics for Computer Graphics. Proceedings of SIGGRAPH '90 (Dallas, Texas, August 6-10, 1990). In Computer Graphics 24, 4(August 1992).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Li, Xin. Physically-Based Soil Models of Dynamic Terrain in Virtual Environments. Technical Report. CS-TR-92-26, University of Central Florida. Nov. 1992.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Moshell, J. Michael. Li, Xin. et al. Nap-of-Earth Flight and the Realtime Simulation of Dynamic Terrain. Proceedings of International Society for Optical Engineering. Apr. 1990.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>30424</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Winston, P.H., Artificial Intelligence. Addison-Wesley, pp.75-78, 1984.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Modeling Soil: Realtime Dynamic Models for Soil Slippage and Manipulation Xin Li and J. Michael Moshell 
Institute for Simulation and Training University of Central Florida ABSTRACT A physically based model 
of an object is a mathematical representation of its behavior, which incorporates principles of Newtonian 
physics. Dynamic soil models are required in animations and realtime interactive simulations in which 
changes of natural terrain are involved. Analytic methods, based on soil properties and Newtonian physics, 
are presented in the paper to model soil slippage and soil manipulations. These methods can be used to 
calculate the evolution of a given soil configuration under the constraint of volume conservation and 
to simulate excavating activities such as digging, cutting, piling, carrying or dumping soil. Numerical 
algorithms with linear time and space complexities are also developed to meet the requirement of realtime 
computer simulation. CR Categories: I.3.7 [Computer Graphics]: Graphics and Realism; I.6.3 [Simulation 
and Modeling]: applications. Additional Keywords: physically based modeling, real­time simulation, soil 
dynamics, slippage, soil manipulation. 1. INTRODUCTION Physically-based modeling is a growing area of 
computer graphics research. A good deal of work has been done toward physically based models of objects 
such as rigid and nonrigid bodies, hydraulic surfaces or natural terrain. However, soil models that are 
both physically realistic and computationally efficient in realtime simulations have not been developed. 
Recently, substantial interest in dynamic soil models has been expressed by some developers of realtime 
simulations of Dynamic Terrain systems. Such systems provide the capability, within a realtime graphical 
simulation, of reconstructing landscape architecture or rearranging the terrain surface. These systems 
essentially involve allowing the simulation's user to conduct excavating activities in the terrain database 
at any freely chosen location. These activities may include digging ditches, piling up dirt, cutting 
the soil mass from the ground, carrying it for a distance, and dumping it at another location. To these 
deformations, the soil mass must behave in realistic manners under external stimuli. Moshell and Li developed 
a visually plausible kinematic soil model [10]. In their work, a bulldozer blade serves as a local force 
function used to change the heights of the terrain. Excess terrain volume which is "scaped off" by the 
moving 12424 Research Parkway, Suite 300, Orlando, Florida. Phone: (407) 658-5073, email: lix@ist.ucf.edu 
This work is sponsored by the U.S. Army Simulation, Training and Instrumentation Command (STRICOM). All 
opinions are, however, solely due to the authors. blade is added to the moving berm in front of the blade. 
The berm is then smoothed by a bidirectional Cardinal spline algorithm. The demonstration of the model 
appears realistic and runs in realtime. The simulation, however, is kinematic. No forces are computed. 
The soil does not slump when the bulldozer leaves. The volume of given soil is not conserved. Burg and 
Moshell focussed on the problem of piling up soil such that the soil spills down from the mounds in a 
realistic-looking way [3]. In their approach, the terrain is modeled by a 2-d grid of altitude posts. 
Constraint equations are defined to describe relationships among altitude posts and their neighbors. 
An iterative relaxation algorithm, suggested in [11], is used to simulate the falling soil. The constraints 
enforce an averaging or "smoothing" of each altitude post with its neighbors. The algorithm is volume-preserving 
under certain conditions. The model is purely kinematic. The physical properties of different types of 
soil are not modeled. Our research work is focused on dynamic models of soil slippage and soil manipulations. 
For the slippage model, we determine if a given soil configuration is in static equilibrium, calculate 
forces which drive a portion of the soil to slide if the configuration is not stable, and meanwhile preserve 
the volume conservation. For the soil manipulation models, we investigate interactions between soil and 
excavating machines, implement a bulldozer model and a scooploader model. These models are based on analytic 
methods and Newtonian physics. The computational times of the corresponding algorithms are fast enough 
to meet the requirement of realtime graphical simulations. For clarity, this paper mainly focuses on 
the 2-d case. Extensions to 3-d have been completed and are briefly discussed. 2. PRELIMINARIES The 
discussion of soil models needs some understanding of soil properties. In this section, we introduce 
some concepts which are borrowed directly from civil engineering. Interested readers are referred to 
[2], [4], [5] and [7] for more details. The shear strength of the soil is the resistance per unit area 
to deformation by continuous shear displacement of soil particles along surfaces of rupture. It may be 
attributed to three basic components: 1) frictional resistance to sliding among soil particles; 2) cohesion 
and adhesion among soil particles; and 3) interlocking of solid particles to resist deformation. (Cohesion 
is molecular attraction among like particles. Adhesion is a molecular attraction among unlike particles.) 
The shear stress, on the other hand, is the force per unit area experienced by a slope, which pushes 
the mass to move along the failure plane. The combined effects of gravity and water are the primary influences 
on the shear stress. It may also be influenced by some natural phenomena such as chemical actions, earthquakes, 
or wind. The shear strength force and stress force, denoted by s and t respectively, are defined as the 
shear strength and stress multiplied by the total area. The measure of s and t can be determined from 
the Mohr-Coulomb theory indicated in [5]: Fig. 1: The Failure Plane (2.1) s = c L + W cos(a) tan(f) 
(2.2) t = W sin(a) where L is the length of the failure plane, a is the degree of natural slope, and 
W=.. is the weight of soil above the failure plane (see Fig. 1). c, f and . describe properties of the 
soil, where c indicates the cohesion, f is the angle of internal friction (i.e. it is a measure of the 
friction among soil particles) and . is the unit weight. Some typical parameters and their units are 
listed in the table below [1]: SOIL TYPE c (t/m) f (degree) . (t/m2) dry sand 0 26-33 1.9-2.0 Sandy loam 
0-2.0 14-26 1.8-2.0 Loam 0.5-5.0 10-28 1.8-2.1 Soil is a very complex material. It may be influenced 
by changes in the moisture content, pore pressures, structural disturbance, fluctuation in the ground 
water table, underground water movements, stress history, time, chemical action or environmental conditions. 
Predicting the changes of complex configurations is either intractable or highly costly. However, for 
many interactive applications, speed and realistic appearance are more important than accuracy. Hence 
in this paper, we assume that only homogeneous and isotropic soil will be processed. Conditions such 
as seepage, pore pressure, existence of tension cracks and deformation resulting from permanent atomic 
dislocation will not be considered. 3. STATIC EQUILIBRIUM AND RESTORING FORCE In this section, we develop 
methods to determine whether or not a given configuration is stable, calculate the critical angle above 
which sliding occurs, and quantify the force which pushes the soil mass moving along the failure plane. 
3.1 STABILITY The stability of a given soil configuration is determined by the factor of safety, denoted 
by F, of a potential failure surface. From the Mohr-Coulomb theory, F is defined as a ratio between the 
strength force and the stress force [5]: c L + W cos(a) tan(f)(3.1) F = s = tW sin(a) When F is greater 
than 1, the configuration is said to be in a state of equilibrium. Otherwise, failure is imminent. To 
analyze the factor of safety, we divide the given soil mass into n slices with equal width .x: Y yi 
yi+1 yi-1  X .X .X Fig. 2: Dividing the given mass into small slices The calculation of the factor of 
safety of each slice can be done individually. The following free body diagram shows forces applied on 
slice i: yi yi-1 Pi Pi+1 P'i P'i+1 .x (a) (b) Fig. 3: Free body diagram for slice i In (a), the P s 
are forces exerted between slices. They are pairwise equal and in opposite directions and thus can be 
cancelled. At any time t, therefore, sliding can only happen in the top triangle area of a slice. (b) 
shows forces acting on this area, where strength and stress forces are given by (2.1) and (2.2) with 
L, W and a replaced by Li, Wi and ai respectively. To determine if there exists a failure angle ai (so 
that the soil mass above it will slide) and calculate the net force exerted on the failure plane if ai 
does exist, we start from (3.1). Note that Li and Wi can be expressed in terms of ai. Replacing Li and 
Wi in (3.1) with functions of ai, we obtain 2c+.tan(f)[hcos(ai)-.xsin(ai)]cos(ai)(3.2) F(ai) = .(hcos(ai)-.xsin(ai))sin(ai) 
where h=yi-yi-1 is the height of the triangle in Fig. 3-(b). For any angle ai>tan-1(h/.x), function F(ai) 
makes no physical sense. In the range of [0, tan-1(h/.x)], F(ai) reaches its minimum when the first derivative 
of F(ai), with respect to ai, is equal to 0. That is dF 1 (3.3) = [A cos(2ai) + B sin(2ai) + C] = 0 t 
2 da where A = .22 tan(f)(.x2-h2)-2.ch, B = .2h.xtan(f)+2.c.x, and .2 C = -2 tan(f)(.x2+h2). Solving 
(3.3) gives us four angles (see [9]). We can choose the one which satisfies 0=ai=tan-1(h/.x) in (3.2) 
to calculate the factor of safety F. The given configuration is statically stable if F>1. Otherwise sliding 
is inevitable. 3.2 CRITICAL SLOPE ANGLE Suppose that we have F<1 for a given configuration. In the range 
of [0, tan-1(h/.x)] there are at most two angles, say ß1 and ß2, such that F(ß1)=F(ß2)=1. The angle ß0=min(ß1, 
ß2) is said to be the critical-slope angle of the configuration. Above this angle impending slip occurs. 
ß1 and ß2 can be obtained by solving the equation (3.4) for a: 2c+.tan(f)[hcos(a)-.xsin(a)]cos(a) (3.4) 
F(a) = = 1 .[hcos(a)-.xsin(a)]sin(a) where all symbols are as explained earlier. The solution to (3.4) 
is derived in [9]. .x 3.3 RESTORING FORCE Let a configuration be given in Fig.4-(a) with ß0 as the 
critical-slope angle. The force that pushes the mass in the triangle along the edge gh0 can be computed 
as follows. First the line segment h0hn is divided into n small segments with equal length .h. Fig 4-(b) 
shows the free body diagram of the i-th dovetail indicated by the shaded area in (a). hi hn si' .h hi+1 
Ni' hi hi-1 hi-1 Ni ti Li h0 .x .x (a) (b) Fig. 4: Analyzing the restoring force Let's analyze forces 
exerted on the dovetail. The weight W i can be decomposed into two forces, namely Ni and ti, which are 
normal and parallel to the edge Li respectively. si is the strength force resisting the sliding motion, 
si the opponent force generated by strength force si+1, and Ni the force supporting the dovetail above 
it. The net force fi applied on dovetail-i is therefore given by a vectorial summation: (3.5) fi = Ni 
+ ti + si +si + Ni The total net force f acting on the whole triangle area is the summation of fi s, 
1=i=n, i.e. (3.6) f =Sn i=2 ti i=1(Ni+ti+si+si'+Ni ) =S n since t1=s1 (due to F(ß0 )=1), Nn =0, sn =0, 
Ni = Ni+1 and si = -si+1 for 1=i=n-1. Based on (3.6) and Fig.4, [9] gives a derivation of (3.7) by letting 
.h tend to zero. ..x2 hn2+.x2 (3.7) f = Ln ( 2+.x2 ) cos(ß0) + 4 h0 .. x (hn-h0-.x(ßn-ß0)) sin(ß0) 2where 
ßn=tan-1(hn/.x) and ß0=tan-1(h0/.x). (3.7) can be used to quantify the total force on the top triangle 
area of each slice.  4. VOLUME CONSERVATION The approach used in this section is strongly related to 
[8]. Recall that, in the previous discussion, a given configuration is divided into n slices. The i-th 
slice, 1=i=n, can be conveniently thought of as a container holding an amount of soil whose quantity 
is given by (yi+yi-1).x/2. Slice i-1 Slice i Slice i+1 .x Fig. 5: Considering slices as containers Let 
us consider a small change, denoted by .Wi , of the mass Wi in slicei Since Wi = (yi+yi-1)..x/2, we have 
(4.1) .Wi = (yi+.yi+yi-1+.yi-1)..x/2 - (yi+yi-1)..x/2 = (.yi+.yi-1)..x/2 On the other hand, let us assume 
that there is a force fi exerted on the triangle area Ai at the top of slicei, which is parallel to the 
edge Li. Due to fi, Ai tends to move along the direction of fi at a velocity vi. The rate of the "flow" 
of mass of Ai through slice i can be computed by .A ivi/. x. Thus, the "mass throughput" of slicei can 
be quantified by .Aivi.t/.x, where .t is a unit of time. Similarly, the mass throughput of slicei+1 is 
given by .Ai+1vi+1.t/.x. From the principle of volume conservation, the change of soil quantity in slicei 
is the amount of soil which goes out, minus the amount of soil which goes in. It can be expressed by 
.Ai .Ai+1 (4.2) .Wi = vi.t -.t .x .xvi+1 where Ai=(yi-hi).x/2. Putting (4.1) and (4.2) together and rearranging 
it, we have .yi .yi-1 1 (4.3) + = [(yi-hi))vi - (yi+1-hi+1))vi+1] .t.t.x Now let .t tend to 0. It follows 
that dyi 1 dyi-1 (4.4) + = [(yi-hi))vi - (yi+1-hi+1))vi+1] dtdt .xRecall that (3.7) gives us a formula 
to compute force fi. From Newton's second law, we have dvi ..x dvi (4.5) fi = .Ai = (yi-hi) dt2dt Rearranging, 
we obtain both dvi 2fi (4.6) = , and dt..x(yi-hi) 2 ( fi (4.7) vi = dt ..x ) yi-hi Now we take the second 
derivative of (4.4) with respect to t and plug (4.6) and (4.7) into the resulting formula. That yields 
d2yi d2yi-1 (4.8) + dt2dt2 2 [dyi-dhi( fi dyi+1-dhi+1( fi+1 = dt+fi - dt + fi+1] dt yi-hi dt ..x ) ) 
yi+1-hi+1 Note that we can denote hi and fi as functions of yi-1 and yi, i.e. hi=h(yi-1,yi) and fi=f(yi-1 
,yi), since they can be determined based only on yi-1 and yi if .x and other soil properties are fixed. 
Hence, (4.8) is an equation with three variables, namely yi-1, yi, yi+1. Let us suppose that we have 
divided the given configuration into n slices. Now we end up with n+1 unknowns, y0, y1, ..., y, and n+1 
ordinary n differential equations involving yi's, their time derivatives and integrals. Solving these 
equations, we will obtain the solution for the soil behavior which satisfies both the soil dynamics and 
the volume conservation. 5. NUMERICAL SOLUTION In this section we linearize equations (4.8) for both 
purposes of simplification and discretization. we start from (4.4). Assume that, at any instance of time 
t, velocity vi of mthe mass on the top of slicei is represented by vi(t), the value mof yi is represented 
by yi(t), the rate of the change of yi is mrepresented by yi'(t)=dyi(t)/dt. Then, at the very next time 
mminstance tm+1, the force fi=fi(yi-1(tm), yi(tm)) can be computed by (3.7) according to the value of 
yi-1 and yi from the previous step. If the Euler integration algorithm is used, the velocity vi at the 
time tm+1 can be computed by )yi(t)) fi(yi-1(tm, m (5.1) vi(tm+1) = vi(t) + .t m Wi where .t is the integration 
step size. Similarly vi+1(tm+1) is calculated. It follows that, from (4.5), we have (5.2) yi'(tm+1) + 
yi-1'(tm+1) = 1 [(yi(t)-h(yi-1(t),yi(t)))vi(tm+1) .xmmm -)-h(yi(t)))vi+1(tm+1)] (yi+1(tmm),yi+1(tm Since 
at the time instance tm+1, all items on the right hand side are knowns, either from the previous step 
of the simulation or from the calculations of vi(tm+1) and vi+1(tm+1), we may treat it as a constant, 
namely Ci. We now have n+1 equations in the following format: y0'(tm+1) = C0 (5.3) y1'(tm+1) + y0'(tm+1) 
= C1 ...... yn'(tm+1) + yn-1'(tm+1) = Cn Solving (5.3) for yi'(tm+1), i=0, 1, ... n, we will be able 
to use the Euler method again to determine the new values for each yi: (5.4) yi(tm+1) =yi(tm) + yi'(tm+1).t 
Algorithm 1 describes the procedure of the numerical solution, in which each step of the algorithm takes 
linear time to execute. Thus the time complexity of the algorithm is O(n) where n is the number of elevation 
posts in a given configuration. The space required to store forces, velocities and heights of posts is 
also proportional to n. Algorithm 1. At any time tm+1 of simulation, do the following: 1) for each post 
yi, calculate its mass velocity vi(tm+1) by using (5.1); 2) for yi, compute the right hand side of (5.2); 
3) use forward substitution to solve equations (5.3) for yi'(tm+1), i=0, 1, ... n; 4) use Euler integration 
to determine new value for each yi(tm+1). 6. EXTENSION TO 3-D In going to 3-d soil dynamics, we use 
some essential concepts and results from the discussion on 2-d. First, a given soil configuration is 
partitioned into small prisms. The values of elevation posts (i.e. vertices) of each prism are evolved 
by an approximation procedure as follows. Consider, in Fig. 6, the post z(i,j) chosen arbitrarily: x-plane 
y-plane d-plane Fig. 6: An approximation of the 3-d configuration z(i, j) is surrounded by six prisms. 
At any time instance t, those prisms are the only ones that affect the height of z(i,j). The effect caused 
by those prisms can be approximated by considering forces exerted on three planes, namely the x­plane, 
y-plane and d-plane. They are indicated by different types of shaded areas in Fig. 6. Thus the 3-d problem 
is reduced to a 2-d problem. The finer the partitioning is, the smaller the base triangles of prisms 
are, and the more accurate the approximation will be. Let's assume that, at any time tm, the height of 
post z(i,j) is represented by zij(tm), and the rate of change of z(i,j) is represented by zij'(tm). Since 
zij'(tm) is affected by forces from 3 planes, it can be expressed as a summation of three terms: (6.1) 
zij'(tm) = zxij'(tm) + zyij'(tm) + zdij'(tm) where zxij'(tm), zyij'(tm) and zdij'(tm), are rates of changes 
of zij'(tm) caused by forces exerted on the x-plane, y-plane and d-plane respectively. During a simulation, 
each time slice .t is divided into two substeps .t1 and .t2. In .t1, we first use (3.7) to compute forces 
exerted on three different planes. Then zxij'(tm+1), zyij'(tm+1) and zdij'(tm+1) can be obtained by solving 
equations (5.3). In step . t2, Euler integration is used to determine new values for each zij(tm+1): 
(6.2) zij(tm+1) = zij(tm) + [zxij'(tm) + zyij'(tm) + zdij'(tm)].t For .t1 and .t2 of each iteration in 
the simulation, we split our 2-d computational problem into 3 terms: x-plane scan, y-plane scan and d-plane 
scan. Each scan has two phases corresponding to two time substeps. A scan on any plane involves calculations 
of forces exerted on that plane, rates of changes of z(i, j) caused by the forces, new height of each 
post, etc. Computations for each scan in a time substep are independent of scans on the other planes 
in the same substep, and therefore can be performed either sequentially or in parallel. It is important 
to notice that, in the same time substep, scans in different orders (x-scan then y-scan then d­scan, 
or y-scan then x-scan then d-scan, etc.) will have the same effect. The reasons are discussed in [9]. 
The 3-d algorithm can be briefly described as follows: Each iteration of simulation is divided into two 
phases. Steps (1)-(3) of Algorithm 1 are performed first for each scan. Then step (4) is applied for 
each scan to calculate new values of posts. Both time and space complexity of the 3-d algorithm remain 
linear in the number of posts.  7. INTERACTION BETWEEN SOIL AND BLADE In this section, we analyze the 
interaction between the soil mass and a bulldozer's blade. Let's assume that the height of the blade 
is H. The shape of the blade can be modeled by an arc of a circle centered at the location <xc,yc> with 
radius R. We divide the arc into n segments, each of which has length R.ß. Furthermore, the soil mass 
in front of the blade is also partitioned into n slices by horizontal lines at each joint point of two 
arc segments as shown in Fig. 7. Y portion of the blade, it has a smaller magnitude and points in the 
negative y-direction. In the lower portion, it has a larger magnitude and points in the positive y-direction. 
Let fyi be the y component of the parallel force. It can be computed by: (7.4) fyi = (C1 - C2yi ) e2ai 
sin(ai) cos(ai) .L B where C1=Ae[.(H+y0)tan(f)+ c] and C2 =AeB.tan(f). Now we calculate the summation 
of all fyi's, represented by Fy, which gives us the total force pushing the soil mass in front of the 
blade upwards. 1 (7.5) Fy= 2 Sni=1 (C1 - C2yi ) e2ai sin(2ai) .L To get an accurate solution, we let 
.a approach 0. In this H case we have the following equations [9]:  (7.6) ai = a0+ i .a X (7.7) Lim 
.L = R .aFig. 7: Dividing the blade and soil mass .a.0 To calculate the force resisting cutting, we arbitrarily 
pick the i-th slice from the partitioning. The arc segment can be (7.8) Lim yi = yc - Rcos(a0+ i .a)approximated 
by a line segment from point <xi,yi> to point .a.0 <xi+1, yi+1>. Note that the length of the line segment, 
denoted Replacing ai ,.L and yi in (7.5) by right hand sides of above by .L, approaches the length of 
the arc when .ß approaches equations and making .a infinitesimal, we obtain: 0. The idea is explained 
in Fig. 8: an R( 2a Fy (7.9) = [C1-C2yc+C2Rcos(a)] esin(2a) da2 )a0 To simulate cases in which the blade 
are not fully loaded, we fix the lower bound angle of the definite integral and keep the upper bound 
angle changing from a0 to an. That will give us the following figure: 0.8 Fig. 8: Free body diagram for 
i-th slice If the cutting part of the bulldozer pushes the soil mass Height (m) 0.6 0.4 0.2 with enough 
force, the equilibrium will be destroyed. At this moment, the resistance parallel to blade motion at 
the point <xi,yi> can be calculated by the formula [1]: 2ai+B (7.1) Ti= Ae[.(H + y0 - yi) + c cot(f)] 
tan(f) where Ti is the localized shear stress and ai is the magnitude of the angle of inclination of 
.L to the horizon. The remaining symbols are as explained earlier. All angles are given in radians. Constants 
A and B are only related to f and d (d is the angle of external friction), of the given soil: sin(d) [ cos(d) + vsin2(f) - sin2(d) ] 
  .......... (7.2) A = 1 - sin(f ) sin(d) (7.3) B = d + sin-1( ) -p sin(f) Due to different cutting 
depths (given by H+y0-yi ) and different inclination angles ai, the magnitudes of Ti vary. The resistance 
force exerted on .L can be computed by fi = Ti .L. As shown in Fig. 8-(b), fi can be further decomposed 
into two components, one normal to .L and another parallel to .L. The normal force is cancelled by the 
opposite force contributed by .L. The parallel force has the following property: In the upper 0.0 Total 
Upward Force (t) Fig. 9: Total upward force along the blade In Fig 9, the vertical axis indicates y coordinates 
of points up to which the soil is loaded and the horizontal axis gives Fy under the given configuration. 
The data is recorded with a0=1.22, R=100cm, c=1.9, d=0.5, f=0.54 and .=2.0 (angles are measured in radius). 
For example, if the soil is loaded up to the middle point of the height of the blade, i.e. y=36.0 cm, 
the curve shows that at this point the total upward force reaches its maximum (about 20 metric tons). 
The analysis shows that the total force is always positive. That is, the soil mass being cut always moves 
upward along the blade. This phenomenon is also observed experimentally [1]. The sequence of events occurring 
during the process of interaction between the cutting blade and the excavated soil before the blade can 
be described by 3 steps. 1) the soil chip being cut from the main soil mass moves upward along the blade 
because of resistance to the soil. 2) the soil chip is broken up into individual lumps on the upper part 
of the blade. 3) These lumps move downward toward the soil layers being further cut and from the soil 
prism which is being dragged. This phenomenon is depicted by Fig. 10: Fig. 10: Pattern of soil movement 
ahead of the blade 8. SOIL IN A BUCKET In this section, we present a graphical model of a scooploader. 
For clarity, we assume that only buckets which can be represented by convex polygons will be processed. 
Again we first divide the soil configuration and the bucket into n slices with equal width .x. This is 
shown in Fig 11, where the thick line segments indicate the bucket: yi yi-1 yi+1 Y X .x .x Fig. 11: 
Dividing the soil mass in a bucket The motion of the soil mass in the bucket is a combination of two 
movements: 1) the movement of a portion of the given soil mass along a potential failure plane on the 
top; and 2) the whole mass along the bucket surface. We will refer to these motions as local movement 
and global movement respectively. In general, a local movement is caused by an unstable configuration 
of the given soil, while a global movement is due to the shear stress experienced by a surface of the 
soil mass in contact with the bucket. This can be seen more clearly through the free body diagram of 
slice-i arbitrarily picked from the partitioning (see Fig. 12), where fi is the force driving a local 
movement along the failure plane denoted by line segment <yi-1, hi>. This force can be quantified by 
(3.7). Let's now consider the global movement. The driving force, denoted by G, can be calculated by 
analyzing the free body diagram of each free body. As shown in Fig. 12, the weight Mi of slice-i can 
be decomposed into two elements: the shear stress force ti and the normal stress force Ni. Ni is canceled 
by the opposite force provided by the bucket surface <bi-1, bi>. ti is the force which pushes the mass 
to move along the bucket surface. The shear strength force si, on the other hand, resists the shear displacement 
of soil particles along the bucket surface. These forces can be determined from the Mohr-Coulomb theory 
as indicated by [5]: (8.1) ti = Mi sin(ai) (8.2) si = cLi + Mi cos(ai) tan(d) where c is the coefficient 
of cohesion, d is the angle of external friction, Li is the length of the line segment from bi-1 to bi, 
Mi is the weight of slice-i, and ai is the angle between the bucket surface and the horizontal. d indicates 
a measure of the friction between soil and the surface of the bucket. It is given in radians. For loamy 
clay and sand, the typical values of d are 18 and 30 respectively [1]. The units of these symbols are 
as explained earlier. For equilibrium consideration, we use a method similar to the one described in 
[6]. The stress force t and the strength force s can be expressed by vectorial summations: (8.3) t =Sni=1 
ti= Sni=1 Mi sin(ai) <cos(ai), sin(ai)> (8.4) s = Sni=1 si =Sni=1 Mi cos(ai) tan(d)<cos(ai), sin(ai)> 
Note that the term cLi is dropped from(8.4), since the cohesion coefficient c describes molecular attraction 
among like particles and is zero between soil and a bucket surface. Thus, the safety factor Fs can be 
defined as |s|(8.5) Fs = |t |  When Fs is less than one, sliding of the whole mass along the bucket surface 
is inevitable. In this case, the total driving force G of the global movement can be computed by .t-s, 
if t>0 (8.6) G = ..t+s, otherwise In order to simulate the movement of soil mass in a bucket, we decompose 
G to smaller components which are parallel to the bucket surface. These component forces are distributed 
to slices so that the dynamics of soil can be considered individually for each slice. After carefully 
analyzing the behavior of the soil mass, we know that the following constraints must be satisfied: 1) 
The summation of component forces should equal G; 2) All slices should have the same x-acceleration. 
The first constraint is obvious. The second one should be always true simply because: 1) a bucket always 
has a convex shape; and 2) some slices would fall apart and tension cracks or deformation would occur 
if the x components of their accelerations are different. Let Gx=Gcos( a ) and Gy=Gsin(a ) be the x and 
y components of G respectively, where a is the angle between the vector G and the horizontal. Let gi 
be a component force of G, which is experienced by the bucket surface of i-th slice. From the constraints 
we have (8.7) G cos(a) = Sn i=1 gi cos(ai) (8.8) G sin(a) = Sni=1 gi sin(ai) g1g2 gn (8.9) cos(a1) = 
cos(a2) = ..... = cos(an) M1M2Mn(8.7)~(8.9) give us n+1 equations with n+1 unknowns, namely g1, g2, ... 
gn and a. Other variables can be computed according to the geometry of the given configuration. Solving 
the equations we obtain Mi cos(a) (8.10) gi= G, for i = 1, 2, ..., n. M cos(ai) -11 where a = tan( M 
Sni=1 Mi tan(ai) ). Having fi and gi computed, we model the soil dynamics in a bucket by using Algorithm 
1 to evaluate simultaneous equations in (5.3). In order to do so, we simply replace fi by fi+gi when 
calculating the rate of changes of each post at the time tm+1. The rest of the algorithm remains unchanged. 
 9. IMPLEMENTATIONS 9.1 IMPLEMENTATION OF A BULLDOZER Recall that the terrain surface is represented 
by a regular tessellation model. An array, namely z, of size m×n is used to store the height of elevation 
posts. An element z(i,j) in the array represents the elevation at the location <i,j>. As mentioned in 
section 7, an excavating process of a bulldozer can be separated into three phases. These actions can 
be simulated by an algorithm with three corresponding stages: digging, piling and soil slipping. First, 
the algorithm keeps track of the motion of the blade. If the altitude value of the bottom of the blade 
is denoted by b(i,j) at the location <i,j>, then any elevation post z(i,j) passed through by b(i,j) are 
forced to have the same value. This procedure will create a ditch along the path of the bulldozer on 
the terrain surface. The second stage models the upward movement of the soil along the blade. Let P be 
a set of soil prisms which have been passed through by the blade in the last time step. Let zp(i,j), 
zq(i,j) and zr(i,j) be surrounding posts of a prism p(i,j). The amount of soil contributed by prisms 
in P to the soil chip moving upwards can be computed by: chips are broken into individual lumps depend 
on the cohesion property of the given soil, the procedure can be simulated by spreading the soil to a 
chunk shown below: uBlade direction The dimensions of the soil chunk are determined according to the 
following equation: (9.2) .z = . (1+ c) V u = w . (1+c) where V is the total volume calculated by (9.1), 
w the width of the blade, c the cohesion coefficient, and . a constant which determines how far forward 
the soil chip moves during one time step. In the implementation, . is chosen experimentally to make the 
simulation looks more realistic. After all this is done, .z is added to the elevations of corresponding 
posts, and the slippage model introduced in previous sections is used to simulate the free flow motion 
of broken lumps of soil. It should be mentioned that the soil being brought to the top of the berm arrives 
continuously in the real world. However, with a discrete time simulation process, the chunk is a reasonable 
representation of the amount and location of the soil that would really arrive during one time step. 
The slippage model smoothly integrates this chunk into the berm, resulting in a realistic appearance. 
Another important phenomena associated with physical properties of soil is swelling, which is due to 
a number of reasons: 1) the affinity of the soil for water; 2) the base exchange behavior and electrical 
repulsion; and 3) the expansion of entrapped air within the soil mass [4]. The model simulating the expansion 
of excavated soil is also discussed and implemented in [9].  9.2 IMPLEMENTATION OF A BUCKET In implementing 
a 3-d bucket, we first divide it into m×n cross sections in a way such that they are parallel to either 
the x-z plane or the y-z plane. We refer to these sections as x­sections and y-sections respectively. 
The result of the division is shown in Fig. 14, where an x-section and a y-section are emphasized by 
two shaded polygons. y-section Therefore, the 3-d soil dynamics in a bucket is reduced to m×n 2-d cases. 
For each individual cross section, we further partition a 2-d soil configuration into soil slices (see 
Fig.  11.). The soil dynamics of each slice is handled by means of the technique introduced in section 
8. A simulation procedure can be described as follows: Each iteration of a simulation can be accomplished 
by two steps. The first step computes forces for each soil slice of every bucket section according to 
(5.3) and (8.10). The second step uses the Euler integration method to determine new values for each 
elevation post (see Algorithm 1). These posts are intersections of x-sections and y-sections. The cutting 
and loading activities of a scooploader can be modeled by a method similar to the one presented in section 
7. The discussion, therefore, is omitted.  10. CONCLUSION AND FUTURE WORK Experimental realtime models 
of a bulldozer and a scooploader have been implemented in the c programming language. Both time and space 
costs of algorithms are linear in the size of the bulldozer's blade, the size of the bucket and the resolution 
of the ground mesh. The simulations were done on a Silicon Graphics 4D/240 GTX computer. When using 4 
processors, two bulldozers run at 6-8 frames/second. The scooploader model uses 2 processors, running 
at 10-15 frames per second. The number of elevation posts to model the ground for both models is 90×90. 
The simulations look very realistic. Future research work may include soil compressibility and moisture 
content. The compression of soil layers is due to deformation and relocation of soil particles and expulsion 
of air or water from the void spaces [6]. Fundamental principles for estimating settlements of soil under 
superimposed loading should be explored so it can be used to provide vehicle tracks or conduct trafficability 
studies. The moisture content of the soil affects its unit weight and cohesion and results in different 
behaviors. Those properties should be incorporated into analytical models to provide more realistic simulations. 
Fig. 15: Two bulldozers are at a work scene 11. Winston, P. H., Artificial Intelligence. Addison-Wesley, 
 Fig. 16: A scooploader is loading pp.75-78, 1984. Fig. 17: A scooploader is dumping  ACKNOWLEDGMENTS 
The authors are very grateful to Dr. Charles E. Hughes and Clay Johnson for revising the paper; to Lance 
Marrou and Jinxiong Chen for their support. REFERENCES 1. Balovnev, V.I. New Methods for Calculating 
Resistance to Cutting of Soil. Translated from Russian, Published for the U.S. Department of Agriculture 
and the National Science Foundation. Washington, D.C., 1983. 2. Bromhead, E. N. The Stability of Slopes. 
Surrey University Press, 1986. 3. Burg, Jeniffer, Moshell, J. Michael, et al., Behavioural Representation 
in Virtual Reality. Proceedings of Behavioral Representation Symposium. Institute for Simulation and 
Training. Orlando, FL, 1991. 4. Cernica, J. N., Geotechnical Engineering. Holt, Rinehart &#38; winston, 
1982. 5. Chowdhury, R. N., Slope Analysis. Elsevier North-Holland Inc., 1978. 6. Das, Braja M. Principles 
of Geotechnical Engineering. Second Edition, PWS-KENT Publishing Company, 1990. 7. Huang, Y. H., Stability 
Analysis of Earth Slopes. Van Nostrand Reinhold Co., 1983. 8. Kass, Michael and Miller, Gavin. Rapid, 
Stable Fluid Dynamics for Computer Graphics. Proceedings of SIGGRAPH '90 (Dallas, Texas, August 6-10, 
1990). In Computer Graphics 24, 4(August 1992). 9. Li, Xin. Physically-Based Soil Models of Dynamic 
Terrain in Virtual Environments. Technical Report. CS-TR-92-26, University of Central Florida. Nov. 1992. 
 10. Moshell, J. Michael. Li, Xin. et al. Nap-of-Earth Flight and the Realtime Simulation of Dynamic 
Terrain.  Proceedings of International Society for Optical Engineering. Apr. 1990.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166163</article_id>
		<sort_key>369</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>46</seq_no>
		<title><![CDATA[Turbulent wind fields for gaseous phenomena]]></title>
		<page_from>369</page_from>
		<page_to>376</page_to>
		<doi_number>10.1145/166117.166163</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166163</url>
		<keywords>
			<kw><![CDATA[Fourier synthesis]]></kw>
			<kw><![CDATA[advection-diffusion]]></kw>
			<kw><![CDATA[gaseous phenomena]]></kw>
			<kw><![CDATA[stochastic modeling]]></kw>
			<kw><![CDATA[transport model of illumination]]></kw>
			<kw><![CDATA[turbulent flow]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.3</cat_node>
				<descriptor>Probabilistic algorithms (including Monte Carlo)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003677</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Markov-chain Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003682</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Sequential Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003671</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31029451</person_id>
				<author_profile_id><![CDATA[81100148921]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39071740</person_id>
				<author_profile_id><![CDATA[81100188679]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eugene]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fiume]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>97918</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[D. S. Ebert and R. E. Parent. "Rendering and Animation of Gaseous Phenomena by Combining Fast Volume and Scanline A-buffer Techniques". ACM Computer Graphics (SIG- GRAPH '90), 24(4):357-366, August 1990.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358553</ref_obj_id>
				<ref_obj_pid>358523</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[A. F. Fournier, D. Fussell, and L. Carpenter. "Computer Rendering of Stochastic Models". Communications of the ACM, 25(6):371-384, June 1982.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[A. Ishimaru. VOLUME 1. Wave Propagation and Scattering in Random Media. Single Scattering and Transport Theory. Academic Press, New York, 1978.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97884</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[M. Kass and G. Miller. "Rapid, Stable Fluid Dynamics for Computer Graphics". A CM Computer Graphics (SIGGRAPH '90), 24(4):49-57, August 1990.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[M. Lesieur. Turbulence in Fluids: Stochastic and Numerical Modelling. Kluwer Academic Publisher, Dordrecht, The Netherlands, 1990.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>78965</ref_obj_id>
				<ref_obj_pid>78964</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[M. Levoy. "Efficient Ray Tracing of Volume Data". ACM Transactions on Computer Graphics, 9(3):245-261, July 1990.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>35069</ref_obj_id>
				<ref_obj_pid>35068</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[J. E Lewis. "Generalized Stochastic Subdivision". ACM Transaction on Graphics, 6(3):167-190, July 1987.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949719</ref_obj_id>
				<ref_obj_pid>949685</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[N. Max, R. Crawfis, and D. Williams. "Visualizing Wind Velocities by Advecting Cloud Textures". In Proceedings of Visualization '92, pages 179-183, Los Alamitos CA, October 1992. IEEE CS Press.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>42249</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[W. H. Press, B. R Flannery, S. A. Teukolsky, and W. T. Vetterling. Numerical Recipes in C. The Art of Scientific Computing. Cambridge University Press, Cambridge, 1988.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325250</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[W. T. Reeves and R. Blau. "Approximate and Probabilistic Algorithms for Shading and Rendering Structured Particle Systems". ACM Computer Graphics (SIGGRAPH '85), 19(3):313-322, July 1985.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[R. S. Rogallo and R Moin. "Numerical Simulation of Turbulent Flows". Annual Review of Fluid Mechanics, 16:99-137, 1984.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>157618</ref_obj_id>
				<ref_obj_pid>157615</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[G. Sakas. "Modeling and Animating Turbulent Gaseous Phenomena Using Spectral Synthesis". The Visual Computer, 9:200-212, 1993.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[M. Shinya and A. Fournier. "Stochastic Motion- Motion Under the Influence of Wind". In Proceedings of Eurographics '92, pages 119-128, September 1992.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97923</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[K. Sims. "Particle Animation and Rendering Using Data Parallel Computation". ACM Computer Graphics (SIGGRAPH '90), 24(4):405-413, August 1990.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[E. Vanmarcke. Random Fields. MIT Press, Cambridge, Massachussetts, 1983.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[R. R Voss. "Fractal Forgeries". In R. A. Earnshaw, editor, Fundamental Algorithms for Computer Graphics. Springer- Verlag, 1985.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122719</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[J. Wejchert and D. Haumann. "Animation Aerodynamics". ACM Computer Graphics (SIGGRAPH '91), 25(4):19-22, July 1991.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122750</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[A. Witkin and M. Kass. "Reaction-Diffusion Textures".ACM Computer Graphics (SIGGRAPH '91), 25(4):299-308, July 1991.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Turbulent Wind Fields for Gaseous Phenomena Jos Stam Eugene Fiume0 Department of Computer Science University 
of Toronto 10 King s College Circle Toronto, Canada, M5S 1A4 Abstract The realistic depiction of smoke, 
steam, mist and water reacting to a turbulent .eld such as wind is an attractive and challenging problem. 
Its solution requires interlocking models for turbulent .elds, gaseous .ow, and realistic illumination. 
We present a model for turbulent wind .ow having a deterministic component to spec­ify large-scale behaviour, 
and a stochastic component to model turbulent small-scale behaviour. The small-scale component is generated 
using space-time Fourier synthesis. Turbulent wind .elds can be superposed interactively to create subtle 
behaviour. An advection-diffusion model is used to animate particle-based gaseous phenomena embedded 
in a wind .eld, and we derive an ef.cientphysically-basedillumination modelforrenderingthesys­tem. Because 
the number of particles can be quite large, we present a clustering algorithm for ef.cient animation 
and rendering. CR Categories and Subject Descriptors: I.3.7 [Computer Graphics]: Three-Dimensional Graphics 
and Realism; I.3.3 [Computer Graphics]: Picture/Image Generation; G.3 [Proba­bility and Statistics]: 
Probabilistic algorithms. Additional keywords and phrases: turbulent .ow, stochastic mod­elling, Kolmogorov 
energy spectrum and cascade, transport model of illumination, Fourier synthesis, advection-diffusion, 
gaseous phenomena. 1 Introduction We have come to appreciate the central role that irregularity plays 
in modelling the shape of natural objects. The analogue for wind and .uids is turbulence, and its effects 
are no less essential to the realistic portrayal of gaseous natural phenomena: curling wisps of smoke, 
mist blowing across a .eld, car exhaust, an aerosol spray, steam rising from a coffee mug, clouds forming 
and moving across the sky, the fall of leaves, a swirl of dust in a room, a hurricane. These effects 
are caused by the interaction of objects with a wind velocity .eld. Modelling the effect of wind requires 
that we model both the wind .eld and this interaction. Both Sims [14] and We­jchert and Haumann [17] 
model a wind .eld as the superposition of deterministic .elds. Modelling a visually convincing turbulent 
wind .eld this way is painstaking. The greatest success in this 0The .nancial support of the Natural 
Sciences and Engineering Research Council ofCanadaandoftheInformationTechnologyResearchCentreofOntariois 
gratefully acknowledged. The helpful suggestions of the referees are greatly appreciated. direction was 
the particle-based Blowing in the Wind animation by Reeves and Blau [10]. Stochastic modelling is a natural 
alternative strategy. In [13], Shinya and Fournier describe an approach developed independently of ours 
but which has some similarities. They employ stochastic processes and Fourier synthesis to derive a wind 
.eld in spatiotem­poral frequency domain, and invert the result to get a periodic space-time wind .eld. 
We employ the same paradigm, but our model and application are quite different. Although both wind models 
can be applied to a wide range of phenomena, and [13] demonstrates this very well, their main concern 
is with coupling the wind model to macroscopic physical models of rigid or deformable objects, whereas 
we are mostly concerned with microscopic inter­actionwithgaseousand.uidphenomena. Consequently,ourmodel 
of turbulence is dissimilar: Shinya and Fournier assume a constant deterministic temporal evolution (Taylor 
Hypothesis), while for us temporal evolution is also a stochastic process. Our wind model also differs 
in that an animator has direct control over deterministic and stochastic components of a .eld. In this 
paper, turbulent wind .elds are modelled as stochastic processes. The model is empirically plausible[5]. 
A wind .eld is generated from large-scale motion and from the statistical charac­teristics of the small 
turbulent motion, both freely chosen by an animator. This is analogous to modelling rough terrain by 
pro­viding the global shape as given by a set of height samples, and the desired roughness of the terrain 
[2]. The large scale of the wind .eld will be modelled using simple wind .eld primitives [14, 17]. The 
small scale of the wind .eld will be modelled as a three-dimensional random vector .eld varying over 
space and time. This .eld is generated using inverse an FFT method[16] that we have generalized to a 
vector .eld. The resulting wind .eld has two desirable properties. First, it is periodic and is thus 
de.ned for any point in space-time. Second, it is generated on a discrete lattice and can be interactively 
calculated using four-linear interpolation. Gases have been modelled in several ways. Ebert models a 
gas as a solid texture. With some trial-and-error (and in our experience, signi.cant human effort), realistic 
animations were obtained[1]. Sakas models a gas as a 3-D random density .eld, generating it using spectral 
synthesis [12]. While spectral synthesis is useful in generating turbulent wind .elds, it is not ideal 
for directly generat­ing density .elds: visual artifacts appear due to the periodicity of the .eld and 
the entire density .eld must be computed at once. The temporal evolution of the density .eld is limited 
to simple transla­tions. Both of the above models are computationally expensive to visualize, and hence 
interactive modelling is not feasible. Using physically-based turbulence to animate density .elds is 
mathemat­ically nontrivial, but we shall show that this can be done ef.ciently. We model gases as density 
distributions of particles. The evolu­tion of a density distribution within our wind .eld is described 
by an advection-diffusion equation. We ef.ciently solve this equation by modelling the gas as a fuzzy 
blobby with time varying pa­rameters. A fast ray-tracing algorithm is used, based on a front to back 
single-scattering illumination model, to render such a density distribution. 2 A Multiple-Scale Wind 
Field Model Physically, wind .elds are the result of the variations of the velocityand the pressure of 
a .uid (including air) over space u x t F p x t and time. These variations are caused by various forces: 
external forces applied to the .uid, non-linear interactions between dif­ferent modes of the velocity 
.eld and viscous dissipation at a rate . By summing these forces and equating them to the acceleration 
of the .uid we obtain the Navier-Stokes equations: 12 bn f u u t W u cr r u Wc un f r p o b r u o F 
1 where is the density of the .uid. If the velocities of the .uid are much smaller than the speed of 
sound, we can assume that the .uid is incompressible [5], i.e., When proper initial conditions and boundary 
conditions are speci­.ed, Eqs. 1 and 2 are suf.cient to solve for the velocity .eld and the pressure 
of the .uid for any time instant. The above equations could be used to animate realistic wind .elds. 
One would .rst specify the physical properties of the .uid that make up the model, including an initial 
velocity .eld and boundary conditions. One would then control the .uid motion by applying external forces. 
Realistic wind .elds would be obtained by solving the Navier-Stokes equations as needed. This is entirely 
akin to the control problem for articulated .gures, and it shares the same dif.culties. First, a desired 
effect is hard to achieve by programming it using only external forces. Second, the non­linearities present 
in the Navier-Stokes equations make them hard to solve numerically, especially in the presence of turbulence 
(low viscosity). Linearizing the equations can improve stability and ef.ciency, which has been done by 
Kass and Miller to model the surface of water [4]. This results in highly viscous .uids that do not exhibit 
turbulence. We shall model a turbulent wind .eld by separating it into a large-scale component and a 
small scale component . The large-scale term is composed of simple wind .elds, resulting in very viscous 
.uids. The small-scale term is a random .eld. We shall make a useful but physically implausible assumption 
that the components are independent, that is, that large scales do not affect the small scales and vice-versa. 
Hence we will write u l 0u s 2 3 u x t u l x t o u s x t This assumption permits the real-time simulation 
and independent control of both large-scale and small-scale effects. The results, as we shall see, are 
quite convincing. We shall further discuss this assumption in our conclusions. 3 Small Scale Modelling 
3.1 Random Vector Fields In this section we will denote the small scale component simply by . It is de.ned 
as a random space-time vector .eld, a function u hi x t u s that assigns a random velocity to each pointin 
space-time [15]. We shall invoke the standard Gaussian assumption [7]: that the random vector .eld is 
entirely determined by its second-order moments. These moments are obtained by statistically averaging 
(denoted by ) components of the evolving random velocity .eld. We will assume that the mean values of 
each component (123) of are constant and equal . i x t h u i x t i i u x t to zero. The cross-correlation 
between different components of the velocity .eld at two different points in space-timeand are given 
by the functions x ij0 rt ij0 (h x u t ix (0 h tu 0 t o 0 W u h tu o i rxu i th xu 0 jiW xx 0 t 0 i 
i jij x t x 0 t 0 G; 1234 2222 Where 1 2 3denotes the variance of the velocity .eld and physically is 
equal to twice the kinetic energy of the .eld. We will assume that the velocity .eld is homogeneous in 
space and stationary in time, which means that the cross-correlation only depends on the difference between 
the two points and the differencebetween the two times: G; G. Homogeneous velocity .elds have a corresponding 
representa­tion in spatial-frequency domain via a spatial Fourier transform. Intuitively this transformation 
can be thought of as a decompo­sition of the velocity .eld into eddies of different sizes: large eddies 
correspond to small spatial frequencies and conversely for small eddies. The stationarity of the velocity 
.eld allows it to be represented in frequency domain by a temporal Fourier transform. We will denote 
spatial frequencies by 123and tempo­ral frequencies by.1 We represent the velocity .eld in frequency 
domain via the usual Fourier transform:   u kij k  Z h Z u W iu k x t u j2 k W ki k i c x ki W jki 
 kt d x dt exp5 Writing the transform in this manner facilitates its separation into spatial and temporal 
frequency components. The Fourier-domain equivalent of the cross-correlation functions are the cross-spectral 
density functions: F 1236 where the denotes the complex conjugation. Conveniently for us, the cross-spectral 
density functions and the cross-correlation functions are Fourier-transform pairs [15]. Finally, we assume 
that the velocity .eld is spatially isotropic, meaning that the cross-correlation functions are invariant 
under rotations. Thus the cross-correlation functions only depend on the distance between two points. 
Isotropy and incompress­ibility (Eq. 2) imply that the cross-spectral density functions are of the form 
[5] ijp ij kr k w k r E k E lkk  kp ij W k i k j i j F421237 4 where is the Kronecker delta, is the 
length of the spatial frequency and is a positive function called the energyspectrum function. Its physical 
interpretation is that it gives the contribution of all spatial frequencies of length and frequencyto 
the total kinetic energy of the velocity .eld: 12 h u i Z 1 Z 11kE k d dk 82 0 3.2 The Energy Spectrum 
Function Eq. 7 states that the structure of a velocity .eld (via its cross­spectral density functions) 
is entirely determined by its energy spectrum function. In other words, an animator can control the qualities 
of turbulent motion by specifying the shape of the energy spectrum. This function can be arbitrary as 
long as the integral of 1In the turbulence literature, the term wave number is often used instead of 
spatial frequency. We will use spatial frequency, which is more common in computer graph­ics, but we 
shall denote spatial frequencies by k, reserving the letterfor temporal frequencies. Eq. 8 exists. In 
the turbulence literature one can .nd a wide variety of different energy spectra for various phenomena. 
These models are either determined from experimental data or obtained from simplifying assumptions about 
the .uid. The best-known example of the latter for turbulence that has reached a steady-state (i.e., 
 R 1 1 Ekd d . Ekk.k ) is the Kolmogorov energy spectrum [5]: 0 if inertial 3252 9 1 5 otherwise This 
spectrum results from an energy cascade, where energy intro­duced at frequency inertial is propagated 
to higher frequencies at a constant rate . Instead of invoking Taylor s Hypothesis [13] we model the 
temporal frequency dependence of the energy spectrum function by multiplying the Kolmogorov energy spectrum 
by a temporal spread function subject to: E K Zk 1 E Ek K tGk k d kE p K tk Zk 1 G G k W k kd y Ek ijK 
kG k 10 This guarantees conservation of kinetic energy (cf. Eq. 8). Fur­thermore, we want the small eddies 
to be less correlated in time than the large eddies. Spatially, this means that small eddies spin, ebb 
and .ow more quickly than large eddies; this behaviour can be observed when watching a water stream or 
smoke rising from a cigarette. We can achieve this behaviour by setting to a Gaussian with a standard 
deviation proportional to : exp11 22 klk d k d y 1 G k Indeed, for large eddies (as 0), is a spike at 
the origin, corresponding to the spectral distribution of a highly-correlated signal; for small eddies 
(as ) the spectral density becomes constant, denoting an uncorrelated signal. 3.3 Generating the Small 
Scale Component We now describe an algorithm to generate a random velocity .eld having speci.ed cross-spectral 
density functions F. The algo­rithm is a generalization of Voss s inverse FFT method[16]. The idea is 
to .lter an uncorrelated white noise velocity .eld in the Fourier domain, and then to take an inverse 
Fourier transform to obtain the desired random velocity .eld. The challenge is thus to .nd the right 
.lter such that the resulting velocity .eld has the desired statistics. We .rst compute the velocity 
.eld in the frequency domain for discrete spatial frequencies and temporal frequencies .2 Let us assume 
that the discretization is uniform and that there are samples per dimension. Then the discrete Fourier 
transform (DFT) of the velocity .eld is de.ned on a discrete lattice of size 34. To ensure that the resulting 
space-time velocity .eld is real valued, the elements of the DFT must satisfy the following  NN u i 
i j kj l kl symmetries: , where the indices are taken modulo , i.e., 0 0[9]. In the special case when 
the indices on both sides of the equality are identical (e.g., 2022 ) we have to set the imaginary parts 
of to zero. The following algorithm generates a DFT with the required properties. u N N N u i j kN l 
u 2N W N 1 i N W j Nk N 2l 2 u i j k l for in 0... 2do compute , , , , , , ,  i N j u k i j ki N l N 
u j N f i j kl l u k N u N i Nl e i j k g u j kW l u i NNi j Nj k lku li j N u Nki j k l Nl  u i NNi 
j Nj N iw kk N N jwk ll u Ni N W i j k j k l l 2The choice of here as indices should not be confused 
with their different use above. u N i j N j r i N l kj kj Nk Nl N k ll u g Nu W i N u a b c dj k lk 
X m r m e i m m   i i N k j kj N N f l e W i j N i j k N l l end for for in0 2do set imaginary parts 
of to zero end for To compute each element in the .rst loop, three indepen­dent complex random variables 
2(1 2 3) are generated with normally distributed gaussian random ampli­ tudes and with uniformly distributed 
random phases. The components of that element are then calculated as m m 1 11 1 2 211222  u a b c 
d h mn hh i i j j k kll X X o h i j k l X o 3 311322 33 3 The functions are derived from the cross-spectral 
density functions as shown in Appendix A (Eq. 21). The velocity .eld is then obtained by taking three 
inverse DFT s: 1 invFFT4D 1 u uu u 2 invFFT4D 2 3 invFFT4D 3 The resulting velocity .eld is de.ned 
on a discrete lattice and is periodic in space and time. Thus even a small lattice de.nes a .eld everywhere 
in space-time. The spacing of this grid determines the smallest scale of the turbulence. 4 Animation 
of Gaseous Phenomena Physicallyagasiscomposedofmanyparticles. Wecouldtherefore animate a gas by moving 
its particles about the wind .eld, but this would require a vast set of particles. We shall instead consider 
the density of particles at space-time point . Assuming that the particles have no effect on the wind 
.eld, the evolution of the density distribution is given by an advection-diffusion (A-D) equation [5] 
to which we have added a dissipation term: 2 12 The .rst term on the right hand side is the advection 
term that accounts for the effects of the wind .eld on the density. The sec­ond term accounts for molecular 
diffusion at rate . This term can also be used to model turbulent diffusion from scales smaller than 
the smallest scale of the modelled turbulence. The third term accounts for dissipation of density at 
rate . Since the velocity is given, the equation is linear in and can be solved by .nite differ­ences. 
The density distribution is then resolved on a .nite grid and can be rendered using an ef.cient voxel-based 
volume renderer [1, 6]. Figure 1 depicts the evolution of an initially square dis­tribution evolving 
under the in.uence of a two-dimensional wind .eld calculated using a standard PDE solver [9]. Computations 
for four-dimensional wind .elds become rapidly prohibitive both in computation time and memory. To obtain 
tractable animations we propose an alternative strategy. We shall assume that the density distribution 
is a weighted sum of a simple distribution :  n x tn x X i i t n m i u ntt f kW xu Wr x nn i o ts kr 
t W not W i on X i i n x nts i f x t u 13 11 Figure 1: Evolution of a density distribution In other 
words the density distribution is a fuzzy blobby with time-dependent .eld function , where is the centre 
of mass, is the time at which the blob is created and is its mass. Ifwesupposethat isagaussiandistributionwithastandard 
deviation 0 much smaller than the smallest scale of the turbulent wind .eld, the wind .eld can be assumed 
to be constant on each blob. The advection term therefore only moves the blob, but does not deform its 
shape. The movement of the blob is hence given by integrating its centre of mass over the wind .eld: 
 t x tfy x sryt i t W t o tt ii Z y t i t f u y o x s i fts W ts i ds ii t W siry ccc i n tt W t i 1 
14 The deformation of the shape of the blob is given by the diffusion term. Here we note that the diffusion 
at rate after time of a gaussian with variance 02 is equivalent to convolving a gaussian of variance 
with a gaussian of variance 02 (cf. [18]). Gaussians are closed under convolution, and the resulting 
gaussian 22 has variance : 1 33exp 2215 22 2 Thus diffuses outward with variance that increases with 
. The normalization factor 22 3guarantees that the mass of the blob is invariant under diffusion. Once 
the variance of a blob becomes comparable to the smallest scale of the turbulent wind .eld we can replace 
it by smaller blobs and distribute the mass equally among them. The effect of the dissipation term is 
an exponential decay of the masses over time: tfm i tmlly i ty W i otty W i tt i y i t 0 exp16 5 Ef.cient 
Rendering of Gas In conventional ray-tracing, light-object interactions are only com­puted at object 
boundaries. Hence light travelling along a ray is only modi.ed at its endpoints. In the presence of a 
participating medium, the light carried by a ray can be attenuated and increased: attenuation is caused 
by light absorbed and scattered away by the gas; an increase may arise from light scattered in the direction 
of the ray from other directions and by self-emission of the gas. These effects can be included into 
a standard ray-tracer, by modifying the intensity value returned along any ray in the ray-tree. For each 
such ray we .rst determine which blobs have domains intersecting the ray (in practice we truncate the 
domain of each gaussian). For each such blob we store in a sorted list the parameter value both for the 
entry and exit points of the ray. This subdivides the ray sN i (C total C 0 I i s y s 3 i s i a i 2 ccc 
N W s into disjoint intervals 1(0 1) as illustrated in Figure 2, with 0 0 being the origin of the ray 
and the being points of ray/blob intersections. Once the ordered list of blobs intersecting the ray is 
calculated, the intensity of light reaching the origin of the ray is computed by shading the list from 
front to back [6]: 1 0  s  s  C i (C i 1 (iC total C o C((((( 7 9 1011 0 8 Figure 2: Subdivision 
of ray into intervals ray s ii ( i distance min Figure 3: Calculation of transparencies for 1to 2do end 
for , Here, is the transparency of the density distribution on interval , I i and is the intensity of 
light emitted on that interval by the density distribution. These values are de.ned in Appendix B, in 
which we also derive the illumination model. is the intensity returned by the standard ray-tracer. In 
case the ray is cast to determine a shadow, only has to be returned. The transparency along an interval 
due to a single blob is a function only of the distance of the ray to the centre of the blob and the 
endpoints and 1 of the interval as shown in Figure 3. The exact relationship and an ef.cient way to compute 
them is given in Appendix B. The transparency of the interval is then computed by combining the transparency 
values calculated for each blob that intersects the ray along that interval. Instead of testing separately 
for an intersection of the ray with each blob, we traverse a tree data structure of bounding spheres. 
The tree is constructed prior to rendering a frame as follows. First all the blobs are put in a linked 
list. The tree is then constructed by the following algorithm: (s totali s i ( i CI Ni while list has 
at least two elements do for each blob in the list do search for blob closest to remove from list create 
new blob which bounds and  bb 0 bb 0 1 b 00 bb 000 b 00 bbb 0 set and to children of replace by in list 
end for end while  There are some obvious optimizations that can be made to this brute-force algorithm, 
such as non-binary blob groupings and the use of a -d tree to accelerate the search, but the cost of 
ray tracing overwhelms even brute-force preprocessing cost. On average, the use of the tree data structure 
has reduced rendering times by an order of magnitude. The tree can be thought of as a multi-scale representation 
of the density distribution and hence could be used to render the distribution at different levels of 
detail. 6 Interactive Field Modelling/Results In our implementation, modelling wind .elds and their effects 
consists of several steps. First the energy spectrum for the spatial component of the small-scale turbulence 
is speci.ed by providing  ktky numerical values for the rate and the inertial frequency inertial of 
the Kolmogorov energy cascade. The standard deviation for the temporal component of the energy spectrum 
is also speci.ed. The overall energy spectrum (cf. Section 3.2) is the product of the temporal and spatial 
(Kolmogorov) energy spectra. A 4-D vector .eld is then generated (cf. Section 3.3) which can be placed 
in a library (although its computation is swift). We have developed an interactive animation system in 
which an animator can design a complex wind .eld and visualize its effect on a gas density. Complex wind 
.elds are formed by the superposition of small-scale turbulence with large-scale .elds such as directional, 
spherical, and exponentially decaying .elds. The user is also able to change the grid spacing of the 
small scale independently in each component of space and time, allowing the speci.cation of non­homogeneous 
.elds. This also permits the same prototypical small­scale .eld to be given different behaviours in different 
contexts (which is precisely what has been for the images shown below). Our animation system also simulates 
the effect of a wind .eld on a gas. A speci.c gaseous phenomenon is speci.ed as a particle system characterized 
by the following values: the region over which blobs of particles are born, their birth rate, and the 
initial standard deviation and the initial mass of each blob. During a simulation, the system introduces 
blobs at the given rate, animates their motion by advection, modi.es the standard deviations by diffusion 
and the masses by dissipation, as described in Section 4. Additionally, particles can be given illumination 
parameters such as a colour. In this modelling step the centre of each blob is depicted (with intensity 
modulated by parameters such as duration), but positions and other data can be piped into a high-quality 
renderer for image synthesis. About 6 000 particles can be animated in real time on an SGI Indigo. The 
parameters needed for rendering include (Appendix B): the extinction coef.cient , which describes the 
decay of light pQ e s t 2 ya in inverse proportion to distance; the albedo O 0 1, which de.nes the proportion 
of light scattered at a given point; the phase function , giving the spherical distribution of scattered 
light; and self-emission , which is the amount of light emitted by a blob at a given position. The illumination 
computation for gas densities at a resolution of 640 480 typically requires from one to ten minutes, 
although 1-2 hour computations are possible when rendering scenes of high optical complexity. For the 
images presented below, we have assumed that the phase function is constant and we have ignored shadows 
cast onto the density distribution for all but one image sequence. In all simula­tions the same statistical 
parameters were used for the small scale tky component: 1, inertial 4 and 1. Steam from a mug: One global 
directional wind .eld was used to model the rising of the steam due to thermals. The particles were generated 
uniformly on a disk. Psychedelicsteam: Three trails of smoke of different colours were combined. As for 
the steam we used a directional wind .eld, this time tilted in the direction of the teapot spout. Particles 
were again generated on small disks. Cigarette smoke: Two smoke trails originating from the tip of a 
cigarette are derived from the similar small-scale turbulence as the steam with a directional heat source. 
Interaction of a sphere with smoke: This simulation shows how objects can interact with our wind .eld 
model. Instead of testing for collision of particles with the objects, we de.ne a repulsion .eld around 
each object. We modelled the repulsion force by a radial potential .eld. The sphere is moved along a 
path given by a spline curve. Note that this image sequence depicts self-shadowing. Three-dimensional 
morphing: The cylindrical range data of two human heads was converted into two sets of blobs and input 
to the animation system. The scene was illuminated by setting the self-illumination parameter (in Eq. 
24) of each blob to the Q illumination given by the range data. The albedo was set to zero and dissipation 
was set to a large value to allow rapid dissolution of each set of blobs (with one run in reverse). 7 
Conclusions and Extensions We have presented a new model for the visual simulation of gaseous phenomena 
in turbulent wind .elds. Our model provides an ani­mator with control over both the large-scale motion 
and the statis­tical features of the small-scale turbulence. This model has been successfully applied 
to the animation of gaseous phenomena. Our model, however, can be applied to many other phenomenaresulting 
from the interactions of objects with a wind .eld. For example, the wind .eld model can be included in 
any existing physically-based animation system. Our model can in fact generate a random vector .eld of 
any dimension, not only three-dimensional vector .elds with a four dimensional domain. The derivation 
of the algorithm can be adapted in a straightforward manner. Our fast rendering algorithm can be used 
to visualize sparsely sampled data. The rendering of the heads in the morphing animation is a good exam­ple. 
Also our animation system could be used to visualize wind .elds calculated by direct numerical simulation 
for .uid dynamics applications. There are many other extensions to our model that we will explore in 
future research. We have assumed that the large scale motions of the wind do not modify the small turbulent 
scale. This is implausible. One possible solution is to warp the domain of the turbulent scale according 
to the large scales. We would require the use of a global deformation algorithm. Also it is possible 
to use a physical model for the large scales. A numerical technique in computational .uid dynamics known 
as Large Eddie Simulation (LES) solves the Navier-Stokes equations on a coarse grid using a statistical 
model for the small scales [11]. However, a physical simulation might not be relevant in computer graphics 
when a speci.c behaviour is intended. A Inverse FFT Method Derivation A white noise velocity .eld has 
cross-spectral density functions de.ned by:3 F 17 A random .eld with cross-spectral density functions 
Fcan be obtained by cross-convolving this white noise with a set of deter­ministic kernels : 3 u k x 
t wk X u ll i k h k k k Z l R Zh w 1 X l i k W h k k h lk x l k W w l yk wt W l k i sw l p w k y w l 
s ij dsd y 3 1 18 which in the Fourier domain becomes 3 19 1 3All subscripted indices in this appendix 
take on the values 1 23. We obtain an equation for the transformed kernels in terms of the cross-spectral 
density functions Fby inserting the expressions for the Fourier velocity components and given by Eq. 
19 into the de.nition of the cross-spectral density function F(see Eq. 6). ij k h X nk u i W i X h k 
l i W in h k u ik W j k h ij u jin h i k h jkkl u k j h kwkll k ij F F 1 1 1 We thus have 9 equations 
for the 9 kernels in terms of the cross­spectral density functions. Because of the symmetry of the cross­spectral 
density functions (FF), only 6 of these kernels are  h hh p ijji h independent and three kernels can 
be chosen arbitrarily. If we set 12 13 23 0, then the system of equations given by Eq. 20 becomes diagonal 
and can easily be solved as follows. F21 F31 11 F11 21 31 11 11 F32 31 2 22 F2221 32 21 22 2 2 h 
q W h W hhh W hhh 33 F33 31 32 (21) B Illumination Model Consider a ray , with origin and direction 
. Let be the intensity of light reaching along the ray from point in the absence of a density distribution 
(i.e., given by a conventional ray-tracer). If we ignore multiple scattering effects, then the illumination 
0 reaching point along the ray for each visible wavelengthis [3]  C x Nb C x s J Z C b O 3 (3 o 3sDs 
n x s s t OC O x O s ds (20)D 0 22 0 0 where exp (23) O1 O(24) and is the extinction coef.cient, and 
O is the albedo. The term is the contribution due to light sources: L x s t ( C sL0 x s 00 x ss X N l 
p Lb cos W x Nss ltk Z o x s 0 ss 00 n W S k x s x s dsQL k x s 25 1 where is the phase function characterizing 
the scattering proper­ties of the density distribution, the are the angles between the ray and the vectors 
pointing to the light sources, determines if the light source is in shadow and is the colour of the light 
source. The term accounts for self-emission and can be used to approximate the effects of multiple scattering. 
If we assume that is constant on each interval , which is reasonable in the case of many small blobs, 
then Eq. 22 becomes 1 1 C x s pCQ C i x sNN XX i i k i CC i i Z L(s i sk i k (s i W s(I i nS x ks s 
i s t ds 0 0 0 1 0 0 1 (26) 0 If we de.ne 1 as the transparency along interval then the equation becomes 
I 1 1 2 2 2 32 3 exp 2 2 2 1 2 32 2exp 2 2 i (i C ( i N X i i s i b s i Y ji i (j Ci W (i 0 0 We now 
show how the integral occurring in the calculations of the transparencies can be computed ef.ciently. 
Let us assume that the blobs 1 intersect the ray on interval . Thes Z t s W i s n i j t j x s (i dsmn 
jj cc b l c m W j n j I j yst n ijt i ZX kn i i s i Z s i s W i r min n j k o x s sy W dsj s min y j 
I i yds o transparency on interval is then 1 exp 28 1 As we render for a particular frame in time we 
de.ne 202 and . Using these de.nitions, each integral in Eq. 28 can be written as [8]: lm j yT j 0 W 
Tr min sy j 1T Z s s 1i W y j W s min u 1 W du Ts i W ys jmin 27 The .rst equality results from the geometry 
of Figure 3. The function is the following integral: 2 exp 29 2 0 and can be precomputed and stored in 
a table for ef.ciency. References [1] D. S. Ebert and R. E. Parent. Rendering and Animation of Gaseous 
Phenomena by Combining Fast Volume and Scan­line A-buffer Techniques .ACM Computer Graphics (SIG-GRAPH 
90), 24(4):357 366, August 1990. [2] A. F. Fournier, D. Fussell, and L. Carpenter. Computer Rendering 
of Stochastic Models . Communications of the ACM, 25(6):371 384, June 1982. [3] A. Ishimaru. VOLUME 1. 
Wave Propagation and Scattering in Random Media. Single Scattering and Transport Theory . Academic Press, 
New York, 1978. [4] M. Kass and G. Miller. Rapid, Stable Fluid Dynamics for Computer Graphics .ACM Computer 
Graphics (SIGGRAPH 90), 24(4):49 57, August 1990. [5] M. Lesieur. Turbulence in Fluids: Stochastic and 
Numeri­cal Modelling. Kluwer Academic Publisher, Dordrecht, The Netherlands, 1990. [6] M. Levoy. Ef.cient 
Ray Tracing of Volume Data .ACM Transactions on Computer Graphics , 9(3):245 261, July 1990. [7] J. P. 
Lewis. Generalized Stochastic Subdivision . ACM Transaction on Graphics, 6(3):167 190, July 1987. [8] 
N. Max, R. Craw.s, and D. Williams. Visualizing Wind Velocities by Advecting Cloud Textures . InProceedings 
of Visualization 92, pages 179 183, Los Alamitos CA, October 1992. IEEE CS Press. [9] W. H. Press, B. 
P. Flannery, S. A. Teukolsky, and W. T. Vetterling. Numerical Recipes in C. The Art of Scienti.c Computing. 
Cambridge University Press, Cambridge, 1988. [10] W. T. Reeves and R. Blau. Approximate and Probabilis­tic 
Algorithms for Shading and Rendering Structured Parti­cle Systems .ACM Computer Graphics (SIGGRAPH 85), 
19(3):313 322, July 1985. [11] R. S. Rogallo and P. Moin. Numerical Simulation of Turbu­lent Flows .Annual 
Review of Fluid Mechanics, 16:99 137, 1984. [12] G. Sakas. Modeling and Animating Turbulent Gaseous Phe­nomena 
Using Spectral Synthesis . The Visual Computer, 9:200 212, 1993. [13] M. Shinya and A. Fournier. Stochastic 
Motion -Motion Un­der the In.uence of Wind . InProceedings of Eurographics 92, pages 119 128, September 
1992. [14] K. Sims. Particle Animation and Rendering Using Data Par­allel Computation .ACM Computer Graphics 
(SIGGRAPH 90), 24(4):405 413, August 1990. [15] E. Vanmarcke. Random Fields. MIT Press, Cambridge, Mas­sachussetts, 
1983. [16] R. P. Voss. Fractal Forgeries . In R. A. Earnshaw, editor, Fundamental Algorithms for Computer 
Graphics. Springer-Verlag, 1985. [17] J. Wejchert and D. Haumann. Animation Aerodynamics . ACM Computer 
Graphics (SIGGRAPH 91), 25(4):19 22, July 1991. [18] A. Witkin and M. Kass. Reaction-Diffusion Textures 
.ACM Computer Graphics (SIGGRAPH 91), 25(4):299 308, July 1991. A strange brew Sphere interacting with 
a gas (note the shadowing) The lonely cigarette From David to Heidi  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166164</article_id>
		<sort_key>377</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>47</seq_no>
		<title><![CDATA[Real virtuality]]></title>
		<subtitle><![CDATA[StereoLithography&#8212;rapid prototyping in 3-D]]></subtitle>
		<page_from>377</page_from>
		<page_to>378</page_to>
		<doi_number>10.1145/166117.166164</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166164</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Biology and genetics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010444.10010087</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Computational biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010095</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Systems biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010935</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Genetics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31070967</person_id>
				<author_profile_id><![CDATA[81100349862]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jack]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bresenham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166165</article_id>
		<sort_key>379</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>48</seq_no>
		<title><![CDATA[Visual thinkers in an age of computer visualization]]></title>
		<subtitle><![CDATA[problems and possibilities]]></subtitle>
		<page_from>379</page_from>
		<page_to>380</page_to>
		<doi_number>10.1145/166117.166165</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166165</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39081306</person_id>
				<author_profile_id><![CDATA[81544926456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kenneth]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[O'Connell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166166</article_id>
		<sort_key>381</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>49</seq_no>
		<title><![CDATA[Updating computer animation (panel)]]></title>
		<subtitle><![CDATA[an interdisciplinary approach]]></subtitle>
		<page_from>381</page_from>
		<page_to>382</page_to>
		<doi_number>10.1145/166117.166166</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166166</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39080648</person_id>
				<author_profile_id><![CDATA[81339533859]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Veeder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166167</article_id>
		<sort_key>383</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>50</seq_no>
		<title><![CDATA[Facilitating learning with computer graphics and multimedia]]></title>
		<page_from>383</page_from>
		<page_to>384</page_to>
		<doi_number>10.1145/166117.166167</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166167</url>
		<categories>
			<primary_category>
				<cat_node>I.3.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Biology and genetics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.3.1</cat_node>
				<descriptor>Computer-assisted instruction (CAI)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010444.10010095</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Systems biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010935</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Genetics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010489.10010490</concept_id>
				<concept_desc>CCS->Applied computing->Education->Computer-assisted instruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010087</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Computational biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14123111</person_id>
				<author_profile_id><![CDATA[81100342338]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[G.]]></first_name>
				<middle_name><![CDATA[Scott]]></middle_name>
				<last_name><![CDATA[Owen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166168</article_id>
		<sort_key>385</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>51</seq_no>
		<title><![CDATA[Visualizing environmental data sets]]></title>
		<page_from>385</page_from>
		<page_to>386</page_to>
		<doi_number>10.1145/166117.166168</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166168</url>
		<categories>
			<primary_category>
				<cat_node>I.3.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.2</cat_node>
				<descriptor>Earth and atmospheric sciences</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010432.10010437</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Earth and atmospheric sciences</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P280014</person_id>
				<author_profile_id><![CDATA[81350590594]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Theresa]]></first_name>
				<middle_name><![CDATA[Marie]]></middle_name>
				<last_name><![CDATA[Rhyne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Martin Marietta]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166169</article_id>
		<sort_key>387</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>52</seq_no>
		<title><![CDATA[How to lie and confuse with visualization]]></title>
		<page_from>387</page_from>
		<page_to>388</page_to>
		<doi_number>10.1145/166117.166169</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166169</url>
		<categories>
			<primary_category>
				<cat_node>I.3.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>K.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456</concept_id>
				<concept_desc>CCS->Social and professional topics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P206638</person_id>
				<author_profile_id><![CDATA[81100493939]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nahum]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Gershon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166170</article_id>
		<sort_key>389</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>53</seq_no>
		<title><![CDATA[The applications of evolutionary and biological processes to computer art and animation]]></title>
		<page_from>389</page_from>
		<page_to>390</page_to>
		<doi_number>10.1145/166117.166170</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166170</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.3</cat_node>
				<descriptor>Probabilistic algorithms (including Monte Carlo)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Biology and genetics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010444.10010935</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Genetics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010095</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Systems biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010087</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Computational biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003677</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Markov-chain Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003682</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Sequential Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003671</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31083053</person_id>
				<author_profile_id><![CDATA[81100234905]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Joblove]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166171</article_id>
		<sort_key>391</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>54</seq_no>
		<title><![CDATA[Urban tech-gap]]></title>
		<subtitle><![CDATA[how the museum/university liaisons propose to create a learning ladder for visual literacy]]></subtitle>
		<page_from>391</page_from>
		<doi_number>10.1145/166117.166171</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166171</url>
		<categories>
			<primary_category>
				<cat_node>I.3.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>K.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>K.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456</concept_id>
				<concept_desc>CCS->Social and professional topics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003527</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010489</concept_id>
				<concept_desc>CCS->Applied computing->Education</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P242623</person_id>
				<author_profile_id><![CDATA[81332518034]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Navin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166172</article_id>
		<sort_key>392</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>55</seq_no>
		<title><![CDATA[Virtual reality and computer graphics programming]]></title>
		<page_from>392</page_from>
		<page_to>393</page_to>
		<doi_number>10.1145/166117.166172</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166172</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Software support</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10011007.10011074</concept_id>
				<concept_desc>CCS->Software and its engineering->Software creation and management</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P31271</person_id>
				<author_profile_id><![CDATA[81100044107]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bob]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Liang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166173</article_id>
		<sort_key>393</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>56</seq_no>
		<title><![CDATA[Ubiquitous computing and augmented reality]]></title>
		<page_from>393</page_from>
		<page_to>394</page_to>
		<doi_number>10.1145/166117.166173</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166173</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14177477</person_id>
				<author_profile_id><![CDATA[81332501009]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rich]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gold]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166174</article_id>
		<sort_key>395</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>57</seq_no>
		<title><![CDATA[Merging 3-D graphics and imaging]]></title>
		<subtitle><![CDATA[applications and issues]]></subtitle>
		<page_from>395</page_from>
		<doi_number>10.1145/166117.166174</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166174</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P299765</person_id>
				<author_profile_id><![CDATA[81100243058]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Pickering]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166175</article_id>
		<sort_key>396</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>58</seq_no>
		<title><![CDATA[Nan-o-sex and virtual seduction]]></title>
		<page_from>396</page_from>
		<page_to>397</page_to>
		<doi_number>10.1145/166117.166175</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166175</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P141664</person_id>
				<author_profile_id><![CDATA[81100088065]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Joan]]></first_name>
				<middle_name><![CDATA[I.]]></middle_name>
				<last_name><![CDATA[Staveley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P63943</person_id>
				<author_profile_id><![CDATA[81332529827]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Steiling]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166176</article_id>
		<sort_key>398</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>59</seq_no>
		<title><![CDATA[Critical art/interactive art/virtual art]]></title>
		<subtitle><![CDATA[rethinking computer art]]></subtitle>
		<page_from>398</page_from>
		<page_to>399</page_to>
		<doi_number>10.1145/166117.166176</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166176</url>
		<categories>
			<primary_category>
				<cat_node>I.3.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Arts, fine and performing**</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31074867</person_id>
				<author_profile_id><![CDATA[81332496882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Timothy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Druckrey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166177</article_id>
		<sort_key>400</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>60</seq_no>
		<title><![CDATA[Digital illusion]]></title>
		<subtitle><![CDATA[theme park visualization]]></subtitle>
		<page_from>400</page_from>
		<doi_number>10.1145/166117.166177</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166177</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39071237</person_id>
				<author_profile_id><![CDATA[81100055532]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Clark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dodsworth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166179</article_id>
		<sort_key>401</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>61</seq_no>
		<title><![CDATA[Multimedia and interactivity in the antipodes]]></title>
		<page_from>401</page_from>
		<page_to>402</page_to>
		<doi_number>10.1145/166117.166179</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166179</url>
		<categories>
			<primary_category>
				<cat_node>I.3.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Arts, fine and performing**</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>K.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456</concept_id>
				<concept_desc>CCS->Social and professional topics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P174928</person_id>
				<author_profile_id><![CDATA[81332523969]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lynne]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Roberts-Goodwin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166178</article_id>
		<sort_key>401</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>62</seq_no>
		<title><![CDATA[Man vs. mouse]]></title>
		<page_from>401</page_from>
		<doi_number>10.1145/166117.166178</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166178</url>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Ergonomics</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Health</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010444.10010446</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Consumer health</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010449</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Health informatics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10011738</concept_id>
				<concept_desc>CCS->Human-centered computing->Accessibility</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10011748</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Empirical studies in HCI</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P147345</person_id>
				<author_profile_id><![CDATA[81332513783]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luskin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>166180</article_id>
		<sort_key>403</sort_key>
		<display_label></display_label>
		<article_publication_date>09-01-1993</article_publication_date>
		<seq_no>63</seq_no>
		<title><![CDATA[The integrative use of computers in a medical university]]></title>
		<page_from>403</page_from>
		<page_to>404</page_to>
		<doi_number>10.1145/166117.166180</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=166180</url>
		<categories>
			<primary_category>
				<cat_node>I.3.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Health</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.3.1</cat_node>
				<descriptor>Computer-assisted instruction (CAI)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010444.10010449</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Health informatics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010489.10010490</concept_id>
				<concept_desc>CCS->Applied computing->Education->Computer-assisted instruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010446</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Consumer health</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31099034</person_id>
				<author_profile_id><![CDATA[81546992856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dave]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Warner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1993</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
</content>
</proceeding>
