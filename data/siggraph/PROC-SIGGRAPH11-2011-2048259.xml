<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>08/07/2011</start_date>
		<end_date>08/11/2011</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Vancouver]]></city>
		<state>British Columbia</state>
		<country>Canada</country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>2048259</proc_id>
	<acronym>SIGGRAPH '11</acronym>
	<proc_desc>ACM SIGGRAPH 2011 Emerging Technologies</proc_desc>
	<conference_number>2011</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-1-4503-0969-1</isbn13>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2011</copyright_year>
	<publication_date>08-07-2011</publication_date>
	<pages>22</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>Interact with the latest systems before they become hot topics in mainstream media and blogs. Emerging Technologies presents innovative technologies and applications in several fields, from displays and input devices to collaborative environments and robotics, and technologies that apply to film and game production.</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<chair_editor>
		<ch_ed>
			<person_id>P2839626</person_id>
			<author_profile_id><![CDATA[81490689020]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[Cole]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Krumbholz]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[]]></affiliation>
			<role><![CDATA[Conference Chair]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>2048260</article_id>
		<sort_key>10</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>1</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A dynamic BRDF display]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048260</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048260</url>
		<abstract>
			<par><![CDATA[<p>Traditional computer displays are designed to be invariant to illumination and viewing conditions. However, this is not what the real world behaves like. Real materials reflect the surrounding light in a characteristic way that depends on the angles of illumination and observation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839627</person_id>
				<author_profile_id><![CDATA[81365598554]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matthias]]></first_name>
				<middle_name><![CDATA[B.]]></middle_name>
				<last_name><![CDATA[Hullin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI Informatik]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839628</person_id>
				<author_profile_id><![CDATA[81100504611]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hendrik]]></first_name>
				<middle_name><![CDATA[P. A.]]></middle_name>
				<last_name><![CDATA[Lensch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universit&#228;t Ulm]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839629</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839630</person_id>
				<author_profile_id><![CDATA[81100315426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hans-Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seidel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI Informatik and Universit&#228;t des Saarlandes]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839631</person_id>
				<author_profile_id><![CDATA[81331495034]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ivo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ihrke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universit&#228;t des Saarlandes and MPI Informatik]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1360657</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fuchs, M., Raskar, R., Seidel, H.-P., and Lensch, H. P. A. 2008. Towards passive 6D reflectance field displays. ACM Trans. Graph. <i>(Proc. ACM SIGGRAPH)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hullin, M. B., Lensch, H. P. A., Raskar, R., Seidel, H.-P., and Ihrke, I. 2011. Dynamic display of BRDFs. In <i>Computer Graphics Forum (Proc. EUROGRAPHICS)</i>, O. Deussen and M. Chen, Eds.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134078</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ward, G. J. 1992. Measuring and modeling anisotropic reflection. <i>Computer Graphics (Proc. SIGGRAPH) 26</i>, 2, 265--272.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531338</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Weyrich, T., Peers, P., Matusik, W., and Rusinkiewicz, S. 2009. Fabricating microgeometry for custom surface reflectance. <i>ACM Trans. Graph. (Proc. ACM SIGGRAPH</i>).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Dynamic BRDF Display Matthias B. Hullin 1 , Hendrik P. A. Lensch2 , Ramesh Raskar3 , Hans-Peter Seidel1,4 
and Ivo Ihrke4,1 1MPI Informatik 2Universität Ulm 3MIT Media Lab 4Universität des Saarlandes 1 Introduction 
Traditional computer displays are designed to be invariant to illu­mination and viewing conditions. However, 
this is not what the real world behaves like. Real materials re.ect the surrounding light in a characteristic 
way that depends on the angles of illumination and observation. We envision a future generation of devices 
that behave more like showcase windows through which the real and virtual worlds can interact with each 
other. To this end, it may become necessary to display the re.ectance of materials, instead of .xed colors. 
As a very .rst step towards this goal, we present a device that can display programmable degrees of surface 
roughness. The under­lying principle is inspired by the microfacet theory. By exciting traveling surface 
waves, a liquid surface is deformed dynamically. The normal statistics of the resulting surface, averaged 
over time, can be controlled so as to follow an anisotropic Gaussian distribu­tion. The re.ectance displayed 
by our device is thus related to some of the most popular analytical models, including Ward s anisotropic 
BRDF [1992]. An in-depth treatment of the idea and theory behind our device is provided in [Hullin et 
al. 2011]. Our Emerging Technologies ex­hibit will showcase a revised and improved prototype of our BRDF 
display in action. In the associated talk, we will focus more on the big picture , i.e., the general 
problem of dynamically displaying BRDFs, and we will outline various approaches to its solution.  2 
Problem De.nition The long-term goal of our work is the creation of a device that can be programmed to 
look like different materials, e.g., like metal, plastic, chalk, etc., in the surrounding light and under 
all view­ing conditions. In other words, we want to dynamically display a bidirectional re.ectance distribution 
function (BRDF) on a surface. This implies a set of fundamental requirements that our device has to meet: 
First, of course, the surface of the device needs to re.ect incident light into variable angular distributions. 
Its response must be immediate and deal with irradiance over a wide dynamic range. In order for the device 
to deliver a convincing viewing experience, its light ef.ciency should be comparable to that of real 
materials. Given these requirements and the capabilities of today s imaging devices, any feasible implementation 
must be optically passive, i.e., the light path must not involve active components such as cameras and/or 
projectors. Since real materials re.ect light in an optically passive way, we argue that it must be possible 
to build such a device. There are two fundamentally different ways to approach the prob­lem. [Fuchs et 
al. 2008] showed that high-dimensional re.ectance .elds can be encoded in multiplexing setups that provide 
a dedi­cated light path for every combination of incident and exitant light direction, and shape the 
distribution by multiplicative modulation. Such devices are inherently inef.cient and limited in their 
resolu­tion. On the other hand, [Weyrich et al. 2009] showed how light can be redistributed by carefully 
designed microgeometry, result­ing in the desired re.ection lobe. Our implementation follows this second, 
more natural, approach of redistributing the available light.   Figure 1: Left: Idea of a BRDF display 
that mimics the re.ectance of different materials and its variation with the illumination and viewing 
angle. Right: Re.ections of a checkerboard pattern in the surface of our device at different anisotropic 
blur settings. 3 Our Prototype Inspired by the scattering mechanism of real materials, we generate rough­ness 
by deforming a re.ecting sur­face to achieve a desired normal dis­tribution. Our device excites waves 
on a liquid surface, but other media could be imagined as well. The me­chanics behind this principle 
are well understood [Hullin et al. 2011] and can be controlled to obtain a normal probability density 
function that fol­lows an elliptical Gaussian lobe. Our BRDF display (left) consists of components that 
can be ob­tained for less than U.S. $30: a microcontroller-based multichannel signal generator with ampli.cation 
stage, actuators from used hard disk drives, a mounting frame and a re­ceptacle .lled with water.  References 
 FUCHS, M., RASKAR, R., SEIDEL, H.-P., AND LENSCH, H. P. A. 2008. Towards passive 6D re.ectance .eld 
displays. ACM Trans. Graph. (Proc. ACM SIGGRAPH). HULLIN, M. B., LENSCH, H. P. A., RASKAR, R., SEIDEL, 
H.-P., AND IHRKE, I. 2011. Dynamic display of BRDFs. In Com­puter Graphics Forum (Proc. EUROGRAPHICS), 
O. Deussen and M. Chen, Eds. WARD, G. J. 1992. Measuring and modeling anisotropic re.ection. Computer 
Graphics (Proc. SIGGRAPH) 26, 2, 265 272. WEYRICH, T., PEERS, P., MATUSIK, W., AND RUSINKIEWICZ, S. 2009. 
Fabricating microgeometry for custom surface re­.ectance. ACM Trans. Graph. (Proc. ACM SIGGRAPH). Copyright 
is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. 
ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2048261</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>2</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A medical mirror for non-contact health monitoring]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048261</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048261</url>
		<abstract>
			<par><![CDATA[<p>Digital medical devices promise to transform the future of medicine because of their ability to produce exquisitely detailed individual physiological data. As ordinary people start to have access and control over their own physiological data, they can play a more active role in the management of their health. This revolution must take place in our everyday lives, not just in the doctor's office or research lab. However, current techniques for physiological monitoring typically require users to strap on bulky sensors, chest straps or sticky electrodes. This discourages regular use because the sensors can be uncomfortable or encumbering. In this work, we propose a new mirror interface for <i>real-time, contact-free</i> measurements of heart rate without the need for external sensors. Users can have the experience of remote health monitoring by simply looking into the Medical Mirror.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839632</person_id>
				<author_profile_id><![CDATA[81444606075]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ming-Zher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Poh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard-MIT Division of Health Sciences and Technology, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[zher@mit.edu]]></email_address>
			</au>
			<au>
				<person_id>P2839633</person_id>
				<author_profile_id><![CDATA[81490688111]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McDuff]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Laboratory, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839634</person_id>
				<author_profile_id><![CDATA[81100496593]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Rosalind]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Picard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Laboratory, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Poh, M.-Z., McDuff, D. J. and Picard, R. W. 2010. Non-contact, Automated Cardiac Pulse Measurements Using Video Imaging and Blind Source Separation. <i>Optics Express</i>, vol. 18, no. 10, 10762--10774.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2048262</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>3</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Device-independent imaging system for high-fidelity colors]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048262</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048262</url>
		<abstract>
			<par><![CDATA[<p>An imaging system, in general, consists of a capturing system (i.e., a camera), a signal transmission process, and an output device (display and/or printing systems). When capturing a picture, colors are captured by an imaging sensor on a camera, whose spectral responses are, in general, highly different from the color matching functions of CIE 1931. Such cameras show non-colorimetric response and provides inaccurate color information of a captured object. Additionally, when storing data, different digital cameras generate different signals in red, green, and blue (RGB), whose reproduction is highly dependent each device. Then, a display device applies its own color conversion based on RGB format. As a result, no colors are correctly captured and reproduced. A severe miscommunication happens between capturing and displaying systems as far as using device-dependent imaging systems. In order to really reproduce images of high-fidelity colors on a display, it is essential for the imaging system to adopt colorimetric method not only in displaying and but also in capturing images. Therefore, we propose a device-independent imaging system which can accurately capture colors and faithfully reproduce colors.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839635</person_id>
				<author_profile_id><![CDATA[81490642709]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kazunari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tomizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SHARP Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839636</person_id>
				<author_profile_id><![CDATA[81365592580]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Akiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SHARP Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839637</person_id>
				<author_profile_id><![CDATA[81490677945]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Makoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hasegawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SHARP Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839638</person_id>
				<author_profile_id><![CDATA[81466641193]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yasuhiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SHARP Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839639</person_id>
				<author_profile_id><![CDATA[81430651940]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Shinichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Katoh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ikegami Tsushinki]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839640</person_id>
				<author_profile_id><![CDATA[81488649164]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Kenichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ikegami Tsushinki]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839641</person_id>
				<author_profile_id><![CDATA[81488672973]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Osamu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ozawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ikegami Tsushinki]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839642</person_id>
				<author_profile_id><![CDATA[81490655524]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Yoshifumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shimodaira]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Shizuoka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Device-Independent Imaging System for High-Fidelity Colors Kazunari Tomizawa, Akiko Yoshida, Makoto 
Hasegawa, Yasuhiro Yoshida SHARP Corporation Shinichi Katoh, Kenichi Nishizawa, Osamu Ozawa Ikegami Tsushinki 
  (a) (b) (c) (d) Figure 1: (a) Device-independent imaging system working at real-time. Multi-Primary 
Color (MPC) display system (right) and a comparable RGB-based display (left) are located with our capturing 
system (center). (b) Capturing system to take the whole visible spectrum. (c) Representation on an MPC 
display system. (d) Representation on a comparable conventional display. Note that the cyan parts of 
Morpho are accurately captured and faithfully reproduced by our imaging system. 1 Problem Statement An 
imaging system, in general, consists of a capturing system (i.e., a camera), a signal transmission process, 
and an output device (dis­play and/or printing systems). When capturing a picture, colors are captured 
by an imaging sensor on a camera, whose spectral responses are, in general, highly different from the 
color match­ing functions of CIE 1931. Such cameras show non-colorimetric response and provides inaccurate 
color information of a captured object. Additionally, when storing data, different digital cameras generate 
different signals in red, green, and blue (RGB), whose re­production is highly dependent each device. 
Then, a display device applies its own color conversion based on RGB format. As a result, no colors are 
correctly captured and reproduced. A severe miscom­munication happens between capturing and displaying 
systems as far as using device-dependent imaging systems. In order to really reproduce images of high-.delity 
colors on a display, it is essential for the imaging system to adopt colorimetric method not only in 
displaying and but also in capturing images. Therefore, we propose a device-independent imaging system 
which can accurately capture colors and faithfully reproduce colors.  2 Capturing System There are two 
major ways to capture colors accurately: employing a camera with a single sensor whose sensitivities 
satisfy Luther-Ives conditionor using a multi-spectral camera. We chose to de­velop a camera with three 
sensors and three .lters which captures all signals from different bands of spectral distribution simultane­ously 
like conventional digital cameras.This camera has the equiv­alent spectral distribution to the color 
matching functions and is in a handheld size. Monochrome CMOS image sensors are mounted with the resolution 
of 3.2 mega-pixels at 12 bits/pixel. The camera is adopted color spectral sensitivities of three .lters 
which satisfy the Luther-Ives condition.In the end, matrix calculation is applied to convert the spectral 
sensitivity functions to the tristimulus values of an object. The least square error method is used to 
obtain the matrix in order to closely .t the spectral sensitivity curves of the CIE color matching functions. 
As a result, our capturing system can take the whole range of the visible spectrum. Accuracy of capturing 
colors of the camera is examined by comparing the data taken by our cam­era and by a spectral colorimeter 
for MacBeth Color Checker (24 colors). The color difference between measured and captured data is quite 
small at the average of .E =0.27 in CIE L*a*b*. High accuracy is achieved in capturing colors by our 
camera system.  3 Display System One of the multi-primary color (MPC) display systems, QuintPixel, was 
presented in 2010.While conventional liquid crystal displays (LCD) are assembled with three primary colors: 
RGB. QuintPixel employs additional Yellow and Cyan primaries in addition to RGB. The primary goal of 
QuintPixel was to accurately reproduce the real-surface colors with high ef.ciency and, in the end, it 
achieved over 99% reproduction of the real-surface colors. Besides its wide color gamut, some other bene.ts 
of MPC display systems are al­ready known based on MPC s characteristic of color reproduction redundancy. 
For our new device-independent imaging system, we employ QuintPixel display system with six sub-pixels 
(the area for red is doubled) for the resolution of 1920 × 1080 pixels in 60-inch size. 4 Device-Independent 
Imaging System Now, there is a device-independent imaging system consisting of a capturing system, signal 
transmission, and a display system. Our capturing system provides output signals in one of the device­independent 
formats, XYZ, via CameraLink with the resolution of 1920 × 1080 pixels for 30Hz. After doubling the video 
frequency from 30 to 60Hz, QuintPixel converts the XYZ signals to its own in­put signals in red, green, 
blue, yellow, and cyan and reproduces im­ages. A comparable conventional display applies a gamut-mapping 
and reproduces images within RGB-structured color gamut. Ad­ditionally, our imaging system can be working 
at real-time. As shown in Figure 1, if some colors are located out of the RGB­based color gamut (e.g., 
sRGB), such colors are clipped onto the gamut boundary and cannot be reproduced on conventional RGB display 
while QuintPixel display system can accurately reproduce those colors. Overall, our device-independent 
imaging system can accurately capture and faithfully reproduce colors. Acknowledgments We would like 
to express our gratitude to Mr. Makoto Katoh at PaPaLaB Ltd. for his support on developing our camera 
prototype. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, 
August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2048263</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>4</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Floating avatar]]></title>
		<subtitle><![CDATA[blimp-based telepresence system for communication and entertainment]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048263</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048263</url>
		<abstract>
			<par><![CDATA[<p>We have created a unique telepresence system for communications and entertainment whereby users can share visual and sound information and express their feelings and impressions more directly than when using conventional meeting systems. We mainly focused on two features to create a unique avatar---its presence in the real world and its ability to interact with people---and created a system based on a blimp. Blimps are physical, not virtual, so they can be used as avatars in the real world. We installed a projector as the output function inside the blimp so that our system can work as a display and express the user's attributes. A camera and microphone mounted on the outside of the blimp provide the input function, which means the user can control the blimp from a distance through the network. The proposed system makes unique network communications between floating avatars and humans a legitimate possibility.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839643</person_id>
				<author_profile_id><![CDATA[81100039448]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tobita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony CSL]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tobita@csl.sony.co.jp]]></email_address>
			</au>
			<au>
				<person_id>P2839644</person_id>
				<author_profile_id><![CDATA[81100537957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shigeaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maruyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony CSL]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[Shigeaki.Maruyama@jp.sony.com]]></email_address>
			</au>
			<au>
				<person_id>P2839645</person_id>
				<author_profile_id><![CDATA[81490690585]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takuya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kuji]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UEC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[quiz@vogue.is.uec.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1180865</ref_obj_id>
				<ref_obj_pid>1180639</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Berk, J., and Mitter, N. Autonomous light air vessels (ALAVs), <i>in Proceedings of ACM Multimedia '06</i>, pp. 1029--1030, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1501874</ref_obj_id>
				<ref_obj_pid>1501750</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Yoshimoto, H., Jo, K., and Hori, K. Design of Installation with Interactive UAVs, <i>in Proceedings of ACM ACE '08</i>, p. 424, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>274686</ref_obj_id>
				<ref_obj_pid>274644</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Paulos, E., and Canny, J. PRoP: Personal Roving Presence, in Proceedings of ACM CHI '98, pp. 296--303, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2048264</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>5</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[FuwaFuwa]]></title>
		<subtitle><![CDATA[detecting shape deformation of soft objects using directional photoreflectivity measurement]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048264</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048264</url>
		<abstract>
			<par><![CDATA[<p>Soft objects are widely used in our day-to-day lives, and provide both comfort and safety in contrast to hard objects. Also, soft objects are able to provide a natural and rich haptic sensation. In human-computer interaction, soft interfaces have been shown to be able to increase emotional attachment between human and machines, and increase the entertainment value of the interaction. We propose the FuwaFuwa sensor, a small, flexible and wireless module to effectively measure shape deformation in soft objects using IR-based directional photoreflectivity measurements. By embedding multiple FuwaFuwa sensors within a soft object, we can easily convert any soft object into a touch-input device able to detect both touch position and surface displacement. Furthermore, since it is battery-powered and equipped with wireless communication, it can be easily installed in any soft object. Besides that, because the FuwaFuwa sensor is small and wireless, it can be inserted into the soft object easily without affecting its soft properties.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839646</person_id>
				<author_profile_id><![CDATA[81470654587]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gota]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TEAMLAB Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839647</person_id>
				<author_profile_id><![CDATA[81442608773]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sugiura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University and JST, ERATO, IGARASHI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839648</person_id>
				<author_profile_id><![CDATA[81456616335]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Anusha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Withana]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839649</person_id>
				<author_profile_id><![CDATA[81490674734]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Calista]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839650</person_id>
				<author_profile_id><![CDATA[81317498050]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Naohisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nagaya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839651</person_id>
				<author_profile_id><![CDATA[81329491765]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Daisuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839652</person_id>
				<author_profile_id><![CDATA[81100344143]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Maki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sugimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University and JST, ERATO, IGARASHI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839653</person_id>
				<author_profile_id><![CDATA[81100424140]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Masahiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University and JST, ERATO, IGARASHI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839654</person_id>
				<author_profile_id><![CDATA[81100444444]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Takeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Igarashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 FuwaFuwa: Detecting Shape Deformation of Soft Objects Using Directional Photoreflectivity Measurement 
 Gota Kakehi1, Yuta Sugiura2&#38;4, Anusha Withana2, Calista Lee2, Naohisa Nagaya2, Daisuke Sakamoto3, 
Maki Sugimoto2&#38;4, Masahiko Inami2&#38;4 and Takeo Igarashi3&#38;4 1TEAMLAB Inc. 2Keio University 
3The University of Tokyo 4JST, ERATO, IGARASHI Design Interface Project  objects are able to provide 
a natural and rich haptic sensation. In human-computer interaction, soft interfaces have been shown to 
be able to increase emotional attachment between human and machines, and increase the entertainment value 
of the interaction. We propose the FuwaFuwa sensor, a small, flexible and wireless module to effectively 
measure shape deformation in soft objects using IR-based directional photoreflectivity measurements. 
By embedding multiple FuwaFuwa sensors within a soft object, we can easily convert any soft object into 
a touch-input device able to detect both touch position and surface displacement. Furthermore, since 
it is battery-powered and equipped with wireless communication, it can be easily installed in any soft 
object. Besides that, because the FuwaFuwa sensor is small and wireless, it can be inserted into the 
soft object easily without affecting its soft properties. 2. FuwaFuwa Overview Soft objects such as stuffed 
toys and pillows usually consist of light materials such as wool, hair or cotton as padding. The random 
arrangement and grainy structure of such materials scatter incident light, resulting in a diffused energy 
pattern. However, when these materials are tightly packed, (i.e. when the density is increased), the 
amount of directed and specular reflected light increases considerably (Figure 2). The FuwaFuwa sensor 
consists of five pairs of IR photo sensors, with IR light emitters positioned in five directions perpendicular 
to each other (Figure 1-b). Each sensor measures the reflected light intensity from each direction. When 
a user interacts with the soft object, its shape changes, resulting in a change in density of the padding 
material. This changes the amount of reflected light as described above. In this way, the FuwaFuwa sensor 
can derive the direction and depth of deformation according to the five sensor measurements of reflected 
IR intensity. Furthermore, by installing multiple modules in a larger soft object, collective sensor 
measurements can be used to interpolate the point of touch on the surface. Therefore with these modules, 
we can convert any soft object into a touch-sensitive surface. Additionally, it gives a measurement of 
the depth of touch to a higher depth than hard touch surfaces. Since the object is soft and sensing is 
contactless, the natural soft haptic sensation of the Figure 2: Directionality of reflected light is 
lower at low density (left) than at high density (right). object is preserved. We have developed a software 
driver to connect the FuwaFuwa sensor to a PC as an input device. The software allows the user to detect 
sensor locations, calibrate the sensors, and visualize the sensed data (Figure 1-a). Once sensor locations 
are defined and calibrated, any application can use the FuwaFuwa sensor s data as input. 3. Applications 
Home Media Controllers. Soft objects are commonly found in the home environment. With this in mind, we 
created a media control application for the living room, converting a cushion into a remote controller 
for a digital home theatre system. Three FuwaFuwa sensors were installed inside the cushion, and a media 
control menu is projected onto the cushion. The user can manipulate the cushion to browse, play and control 
the home theatre system (Figure 1-d). In addition to this, we also created a music composer with a stuffed 
toy. The user can select different tones by pressing at different points, and change the pitch according 
to the depth of deformation. We believe that this will be promising as a future musical instrument. Gaming 
Peripherals. We installed the FuwaFuwa sensor in a small spherical soft object to create a game controller 
for a simple game. By manipulating the surface of the sphere, the user can control the game character. 
Different actions such as squeeze, hit, push and rub can be used as different action triggers. Soft Robots. 
We developed a soft pet robot using our sensor. The robot can detect user interaction and react accordingly 
(Figure 1-c). The robot is designed to move its eyes towards the location of the user s touch, and change 
its voice according to the depth of touch. 4. Future Work We plan to develop a finer filtering algorithm 
to improve the sensor s reading accuracy, and increase the number of sensors to enable multi-touch on 
soft objects. We are also in the process of developing games and music creation applications to utilize 
the multi-dimensional sensor data with the sensing technology. Copyright is held by the author / owner(s). 
SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2048265</article_id>
		<sort_key>60</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>6</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[HAPMAP]]></title>
		<subtitle><![CDATA[haptic walking navigation system with support by the sense of handrail]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048265</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048265</url>
		<abstract>
			<par><![CDATA[<p>In this study, we propose a method of displaying unaware-usage haptic sensation of navigation. People view a map when they visit an unfamiliar place, but when they are taken away their eyes from the map, their attention is diverted and margin of the heart. However, when we are relaxed and do not worry about getting lost, we can discover the intrinsic beauty of the unfamiliar land. Therefore, we have focused on the sense of touch and ensured security by using a support such as a wall or a handrail in the streets, along with the sense of touch. Therefore, we propose a haptic navigation system to release human eyes from the requirement of constantly looking into a map in order to enhance the experience of our daily walk or sightseeing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>C.5.3</cat_node>
				<descriptor>Portable devices (e.g., laptops, personal digital assistants)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839655</person_id>
				<author_profile_id><![CDATA[81488642068]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Imamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Yokohama, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yuki.imr@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P2839656</person_id>
				<author_profile_id><![CDATA[81488647459]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hironori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Arakawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Yokohama, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[lamza@kmd.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2839657</person_id>
				<author_profile_id><![CDATA[81421597185]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kamuro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kamuro@tachilab.org]]></email_address>
			</au>
			<au>
				<person_id>P2839658</person_id>
				<author_profile_id><![CDATA[81331499667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kouta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minamizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Yokohama, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kouta@tachilab.org]]></email_address>
			</au>
			<au>
				<person_id>P2839659</person_id>
				<author_profile_id><![CDATA[81365592882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Susumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Yokohama, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tachi@tachilab.org]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1187315</ref_obj_id>
				<ref_obj_pid>1187297</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Taro Maeda, Hideyuki Ando, Tomohiro Amemiya, Masahiko Inami, Naohisa Nagaya, Maki Sugimoto, "Shaking The World: Galvanic Vestibular Stimulation As A Novel Sensation Interface", ACM SIGGRAPH 2005 Emerging Technologies, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1402239</ref_obj_id>
				<ref_obj_pid>1402236</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[T. Amemiya, H. Ando and T. Maeda: "Lead-Me Interface" for a Pulling Sensation from Hand-held Devices, ACM Trans. On Applied Perception, Vol.5, No.4 (2008)]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2048266</article_id>
		<sort_key>70</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>7</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[An immersive multitouch workspace]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048266</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048266</url>
		<abstract>
			<par><![CDATA[<p>Multitouch interaction has some unique strengths, one of them being that direct touch on 2D screens makes it fast and easy. Unfortunately, because of stereo disparity and content occlusion, direct touch interaction becomes an issue as soon as 3D stereoscopic content is visualized. We propose a new system that combines efficient direct multitouch interaction with co-located 3D stereoscopic visualization. In our approach, users benefit from well-known 2D metaphors and widgets displayed on a monoscopic touchscreen while visualizing 3D objects floating above the surface at an optically correct distance (see Figure 1(a)).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839660</person_id>
				<author_profile_id><![CDATA[81472653983]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Benoit]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bossavit]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA Bordeaux]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839661</person_id>
				<author_profile_id><![CDATA[81381609918]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jean-Baptiste]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[de la Rivi&#232;re]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Immersion SAS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jb.delariviere@immersion.fr]]></email_address>
			</au>
			<au>
				<person_id>P2839662</person_id>
				<author_profile_id><![CDATA[81448600281]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Toni]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Da Luz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Immersion SAS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839663</person_id>
				<author_profile_id><![CDATA[81466641118]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Mathieu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Courtois]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Immersion SAS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839664</person_id>
				<author_profile_id><![CDATA[81381608476]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[C&#233;dric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kervegant]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Immersion SAS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839665</person_id>
				<author_profile_id><![CDATA[81100326317]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Martin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hachet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA Bordeaux]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[Martin.Hachet@inria.fr]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Immersive Multitouch Workspace Benoit Bossavit1 Jean-Baptiste de la Rivi`Toni Da Luz2 C´ere2 * Mathieu 
Courtois2 edric Kervegant2 Martin Hachet1 1INRIA Bordeaux 1 Introduction Multitouch interaction has some 
unique strengths, one of them be­ing that direct touch on 2D screens makes it fast and easy. Unfor­tunately, 
because of stereo disparity and content occlusion, direct touch interaction becomes an issue as soon 
as 3D stereoscopic con­tent is visualized. We propose a new system that combines ef.cient direct multitouch 
interaction with co-located 3D stereoscopic vi­sualization. In our approach, users bene.t from well-known 
2D metaphors and widgets displayed on a monoscopic touchscreen while visualizing 3D objects .oating above 
the surface at an op­tically correct distance (see Figure 1(a)). 2 Setup Our system is composed of a 
32 multitouch screen on which users visualize and interact with monoscopic content. The stereoscopic 
visualization is provided by a 3D screen that is hung upside-down at the top of the system, and that 
re.ects on a semi transparent mirror located between the user s head and the multitouch screen. Hence, 
stereoscopic objects are perceived as if they were displayed above the hands, between the mirror and 
the touchscreen. Head tracking ensures that virtual objects are displayed from a correct point of view. 
By mapping the virtual stereoscopic volume to the physical space, we can then produce a rich and consistent 
interactive visu­alization space. With this setup, the user sees in the same space the .oating 3D objects, 
their hands, and the data displayed on the touchscreen. Our system inherently provides relevant occlusions 
clues. Indeed, the stereoscopic objects appear above the user s hands, while the monoscopic content remains 
below. Contrary to standard immersive 3D systems that only rely on affecting the eyes convergence to 
simulate 3D objects depth, our proposal also takes advantage of the eye accommodation and its capability 
to focus at different depths. 3 Interaction techniques The monoscopic touchscreen provides a direct 
multitouch interac­tion space. Consequently, we bene.t from the large variety of mul­titouch interfaces 
that were previously designed for such interfaces. Users are therefore able to interact in a fast and 
easy way with the content beneath the .ngers, as they would do with a standard touch­screen. We have 
also designed a special widget that takes advan­tage of our setup to support the manipulation of 3D objects 
.oating above the surface (see Figure 1(b)). It is displayed on the touch­screen, below the 3D object. 
Dual-touch gestures on the central disk of this widget result in Rotation-Scale-Translate (RST) operations 
on a plane parallel to the touchscreen, while additionnal rotation as well as scaling widgets allow the 
control of the remaining degrees­of-freedom. Hence, users are able to control 9 DOF (plus uniform scaling) 
of the manipulated 3D object. Moreover, bi-manual in­teraction allows the manipulation of two objects 
at the same time. Since the 3D object always remains above the user s .ngers and directly reacts to any 
motion of the control disk located below, this *e-mail:jb.delariviere@immersion.fr e-mail:Martin.Hachet@inria.fr 
2Immersion SAS (a) (b) Figure 1: The setup and a 3D object being manipulated. provides a near-direct 
interaction paradigm. Hence, users bene.t from the advantages of both direct and indirect interaction. 
 4 User Experience Such a setup combines immersive visualization and multitouch in­put within a seamless 
workspace, leading to speci.c strengths: fast and accurate interaction, easy 3D manipulation, immersive 
visu­alization, bimanual input, 2D system control, and so on. It pro­vides a compelling experience to 
the user. We propose to focus our Siggraph demonstration on a virtual archeology reassembly expe­rience, 
as such a task is a good example that illustrates the bene.t of our setup. Attendees will indeed be able 
to browse through nu­merous 3D archaeological fragments and sort them out. To exper­iment with the solving 
of such a large puzzle, they will enjoy our occlusion free and head-tracked 3D visualization setup, and 
they will play with our speci.c multitouch widget to try different frag­ment combinations out. They will 
also enjoy bi-manual input. In addition to this scenario, we will also propose other demo appli­cations 
to illustrate complementary use cases, in particular in the scope of architecture and medical visualization. 
The combination of multitouch paradigms and immersive visualization in a uni.ed workspace opens new, 
unexplored, interfaces for interactive appli­cations in these areas. 5 Conclusion We propose to combine 
both multitouch input and immersive visu­alization into a single setup that relies on the strengths of 
each tech­nology, while taking advantage of the speci.c capabilities that are introduced by a unique 
design. We believe the resulting proposal combines various research issues (2D/3D visualization, 2D/3D 
in­teraction, immersion, direct and indirect multitouch input...), and it may inspire new usages. It 
will offer a new interactive and im­mersive experience to attendees and may contribute to push such technologies 
forward. 6 Acknowledgments This work was supported by the ANR project ANR-09-CORD-013 InSTInCT -http://anr-instinct.cap-sciences.net. 
Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 
7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2048267</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>8</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[InteractiveTop]]></title>
		<subtitle><![CDATA[an entertainment system that enhances the experience of playing with tops]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048267</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048267</url>
		<abstract>
			<par><![CDATA[<p>A top is a simple toy, yet its behavior is unique when compared with the behaviors of other physical toys. Its attractive behavior is based on the gyroscopic effect acting within the top. We sense the force of the gyroscopic effect when touching or handling them. We consider that this experience helps children become aware of a basic phenomenon of physics. In traditional research[Ishii et al. 1999], a visual and audio effect are often used to augment playging experience of sports or physical toys. In this project, we especially focused on augmentation of physical contact between the tops and the players by using force feedback.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839666</person_id>
				<author_profile_id><![CDATA[81421592614]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Toshiki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[den@atsuage.net]]></email_address>
			</au>
			<au>
				<person_id>P2839667</person_id>
				<author_profile_id><![CDATA[81490686533]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matoba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[matoba@vogue.is.uec.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2839668</person_id>
				<author_profile_id><![CDATA[81100297951]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hideki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koike]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[koike@acm.org]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>303115</ref_obj_id>
				<ref_obj_pid>302979</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ishii, H., Wisneski, C., Orbanes, J., Chun, B., and Paradiso, J. 1999. Pingpongplus: design of an athletic-tangible interface for computer-supported cooperative play. In <i>Proceedings of the SIGCHI conference on Human factors in computing systems: the CHI is the limit</i>, ACM, New York, NY, USA, CHI '99, 394--401.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 InteractiveTop: An Entertainment System that Enhances the Experience of Playing with Tops Toshiki Sato 
* Yasushi Matoba Hideki Koike The University of Electro-Communications  Figure 1: The Tops and the 
Accelerator Figure 2: Table Hardware Figure 3: Top Battle Game 1 INTRODUCTION A top is a simple toy, 
yet its behavior is unique when compared with the behaviors of other physical toys. Its attractive behavior 
is based on the gyroscopic effect acting within the top. We sense the force of the gyroscopic effect 
when touching or handling them. We consider that this experience helps children become aware of a basic 
phenomenon of physics. In traditional research[Ishii et al. 1999], a visual and audio effect are often 
used to augment playging experience of sports or physical toys. In this project, we especially focused 
on augmentation of physical contact between the tops and the players by using force feedback. In this 
project, .rst we attempted to create feedback from the top to the user. We focus on three top behaviors: 
(1) on-axis rotation, (2) horizontal translation and (3) collisions between tops. We de­veloped a system 
to augment these top behaviors and provide feed­back to the user with visual and audio effects and force 
feedback. Second, as the action from the user to the top, we allowed the user to control the top. In 
the case of traditional tops, there are ways to control the top such as direct touching with the hand 
or using a special item such as a whip. These methods require special tech­niques and are a signi.cant 
element of the fun of playing with tops, however, it is dif.cult for a beginner to control a top. We 
increased the controllability of the top to open up new possibilities of playing with tops. 2 TECHNOLOGY 
On the basis of these concepts, we developed a prototype system that extends the possibilities of playing 
with tops. Our system com­prises three components: (1) a top accelerator, (2) top stage and (3) top device. 
IR LEDs and a small battery are embedded within the top. The IR light is tracked by a high-speed camera 
under the stage(Figure2). In addition, if the top rotates on the stage, the IR LEDs appear to blink on 
and off because of the action of a linear polarizer on the camera and another on the top. Our system 
calculates the positions and rotation speeds of multiple tops on the stage simultaneously *e-mail:den@atsuage.net 
e-mail:matoba@vogue.is.uec.ac.jp e-mail:koike@acm.org by detecting and counting the high-frequency IR 
blinking using the high-speed camera and real-time image processing exceeding 750 fps. The positions 
and rotation speeds of the tops are converted to the parameters of a physics simulator running on a computer. 
This simulation result is used to overlay visual and audio effects and detect collisions between tops 
or between tops and virtual objects. We also developed an accelerator to increase the rotation speed 
of the top(Figure1). This accelerator is compact and easy to use(Figure3). The user increases the rotation 
speed of a top by po­sitioning the accelerator 3 cm above the top without any physical contact. Furthermore, 
by moving the accelerator slowly toward the top, the user can position the top through a magnetic attraction 
be­tween the accelerator and top. The accelerator also provides force feedback to the user. The user 
receives vibration feedback based on the rotation speed of the top because of magnetic attraction and 
repulsion between the top and accelerator. This vibration provides the user with a feel for the acceleration 
of the top. In addition, a motor and small disk are embedded within the accelerator to create a gyroscopic 
force. Furthermore, an electromagnetic solenoid gen­erates a virtual impact force at the userfs hand 
when the top collides with other tops. These devices are controlled by a microcontroller within the accelerator 
in synchronization with the top. 3 APPLICATION We developed a novel game application called the Battle 
of Tops, which is based on the traditional top battle game(Figure3). In this game, tops can be moved 
and accelerated freely with the playerfs accelerator. References ISHII,H., WISNESKI,C., ORBANES,J., 
CHUN,B., AND PAR-ADISO, J. 1999. Pingpongplus: design of an athletic-tangible interface for computer-supported 
cooperative play. In Proceed­ings of the SIGCHI conference on Human factors in computing systems: the 
CHI is the limit, ACM, New York, NY, USA, CHI 99, 394 401. Copyright is held by the author / owner(s). 
SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2048268</article_id>
		<sort_key>90</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>9</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[MoleBot]]></title>
		<subtitle><![CDATA[mole in a table]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048268</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048268</url>
		<abstract>
			<par><![CDATA[<p>What would it be like to have a mole live under your table and push around objects on the table surface? We have developed MoleBot, a robotic mole living in a coffee table that interacts with small items laid on the table surface. The MoleBot projects a molehill on the surface, which moves simultaneously with the movement of the MoleBot. In order to make the molehill move with fluidity, the table surface needs to be rigid yet flexible. Various techniques used in shape display and organic user interfaces of previous projects (Feelex, Pop Up!, Lumen1, Relief2, and HypoSurface) were assessed to determine the feasibility of the molehill idea. The projects adopted servo motors, shape memory alloys, electric slide potentiometers, and pneumatic actuators. However, in generating the MoleBot, these techniques were unable to concurrently provide the necessary speed of actuation, resolution, and pixel size for desired level of physical interactivity with objects on the surface. As a result, we conceived a new way to implement this concept.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.2.9</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010213.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods->Robotic planning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010553.10010554</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems->Robotics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010199.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Planning and scheduling->Robotic planning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839669</person_id>
				<author_profile_id><![CDATA[81484644413]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Narae]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Design Media Lab, KAIST, Daejeon, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[narae40@kaist.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P2839670</person_id>
				<author_profile_id><![CDATA[81416603647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Juwhan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Design Media Lab, KAIST, Daejeon, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[juwhan.k@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P2839671</person_id>
				<author_profile_id><![CDATA[81490660528]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jungsoo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Design Media Lab, KAIST, Daejeon, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[lune89@kaist.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P2839672</person_id>
				<author_profile_id><![CDATA[81490651313]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Myeongsoo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Design Media Lab, KAIST, Daejeon, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[zommestyle@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P2839673</person_id>
				<author_profile_id><![CDATA[81100384795]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Woohun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Design Media Lab, KAIST, Daejeon, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[woohun.lee@kaist.ac.kr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1186173</ref_obj_id>
				<ref_obj_pid>1186155</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ivan Poupyrev, Tatsushi Nashida, Shigeaki Maruyama, Jun Rekimoto, and Yasufumi Yamaji. 2004. Lumen: interactive visual and shape display for calm computing. In <i>ACM SIGGRAPH 2004 Emerging technologies</i>, ACM, New York, NY, USA, 17-.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1709928</ref_obj_id>
				<ref_obj_pid>1709886</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Daniel Leithinger and Hiroshi Ishii. 2010. Relief: a scalable actuated shape display. In <i>Proceedings of the fourth international conference on Tangible, embedded, and embodied interaction (TEI '10)</i>. ACM, New York, NY, USA, 221--222.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2048269</article_id>
		<sort_key>100</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>10</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA["Mommy Tummy" a pregnancy experience system simulating fetal movement]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048269</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048269</url>
		<abstract>
			<par><![CDATA[<p>We propose a pregnancy experience system called "Mommy Tummy". Mommy Tummy simulates the physical burden of pregnancy including fetal movement and fetal weight. Through this experience the user can feel the joys and difficulties of pregnancy. In addition, the user can affect the virtual fetus's behavior with gentle or violent movements.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010444</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839674</person_id>
				<author_profile_id><![CDATA[81448597336]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takayuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kosaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kosaka@kosaka-lab.com]]></email_address>
			</au>
			<au>
				<person_id>P2839675</person_id>
				<author_profile_id><![CDATA[81488671582]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hajime]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Misumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[misumi@kosaka-lab.com]]></email_address>
			</au>
			<au>
				<person_id>P2839676</person_id>
				<author_profile_id><![CDATA[81541399156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takuya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanazawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[takuya-i@kosaka-lab.com]]></email_address>
			</au>
			<au>
				<person_id>P2839677</person_id>
				<author_profile_id><![CDATA[81490695990]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Songer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanazawa Technical]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[collegersonger@neptune.kanazawa-it.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2839678</person_id>
				<author_profile_id><![CDATA[81100214479]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Junichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanazawa University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[akita@is.t.kanazawa-u.ac.jp]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Mommy Tummy A pregnancy experience system simulating fetal movement \\202.223.149.27\project\ISVRI\.....-3.bmp 
\\202.223.149.27\project\ISVRI\..... 10.bmp \\202.223.149.27\project\ISVRI\ab01.bmp C:\Users\takayuki\Pictures\Picasa\..... 
.....\MommyTummy_3.0 20110121 161042.bmp C:\Users\takayuki\Pictures\Picasa\..... .....\MommyTummy_3.0 
20110121 161046.bmp C:\Users\takayuki\Pictures\Picasa\..... .....\MommyTummy_3.0 20110121 161055.bmp 
 Takayuki Kosaka Kanagawa Institute of Technology kosaka@kosaka-lab.com Hajime Misumi Kanagawa Institute 
of Technology misumi@kosaka-lab.com Takuya Iwamoto Kanazawa Institute of Technology takuya-i@kosaka-lab.com 
Robert Songer Kanazawa Technical collegersonger@neptune.kanazawa-it.ac.jp Junichi Akita Kanazawa University 
akita@is.t.kanazawa-u.ac.jp 1. Introduction We propose a pregnancy experience system called Mommy Tummy 
. Mommy Tummy simulates the physical burden of pregnancy including fetal movement and fetal weight. Through 
this experience the user can feel the joys and difficulties of pregnancy. In addition, the user can affect 
the virtual fetus`s behavior with gentle or violent movements. 2. Mommy Tummy concept Mommy Tummy is 
an interactive system that simulates pregnancy. A user wearing the Mommy Tummy Jacket can feel the fetus 
s temperature, movement and heartbeat; also, by rubbing the jacket, communication with the fetus can 
be experienced. Within a few minutes the jacket will change in weight and size simulating a fetus s growth 
over nine months. The moods and activity of the fetus will come in a manner natural to how a normal fetus 
develops. An auxiliary screen displays the condition of both the fetus and the mother in each simulated 
month with a 3D model of the fetus. 3. System architecture Figure 1 shows the construction of Mommy 
Tummy. The jacket is composed of a water bag, touch sensors, a vibrator, an acceleration sensor and a 
fetal activity belt. To simulate the growth and weight gain of a fetus, warm water is pumped into the 
water bag. Using Japanese pregnancy as a reference, the weight of a fetus at nine months is usually about 
3000g (7 lb.) and amnion liquid is 1000g (2.5 lb.). This system uses about 4000g (9.5 lb.) to fill the 
water bag. The warm water is kept in a connected tank at 37-38°C (99-100°F) using an electric water heater. 
Figure 1 shows how the jacket is worn. The vibrator in the jacket simulates the heartbeat of the virtual 
fetus. In the front of the jacket, touch sensors are attached to measure the force of a hand stroking 
the tummy. The fetal movement belt simulates the kicking and moving of the fetus. Balloons settled in 
the chest expand to simulate swelling of the breasts.  Figure 1. System architecture. 3.1 Results Figure 
2 shows the fetal activity belt. It is composed of 45 air actuators. The air from an air compressor (0.2MPa) 
is controlled by an electromagnetic valve. This control of the air flow allows the actuators to inflate 
and deflate simulating fetal activity on the user s abdomen (Figure 3). There are 2 types of fetal movement, 
kicking and wiggling . Kicking simulation is not so difficult; however, the simulation of wiggling is 
difficult. Simulating a wiggling sensation was achieved by using a technique called phantom sensation 
(PhS) . PhS was initially discovered by Von Bekesy as a type of a funneling illusion, and is an illusory 
tactile sensation that arises between two points of simultaneous vibration or electric stimulation. Using 
PhS, a wiggling sensation is simulated through continuous, temporally displaced operation of multiple 
air actuators.  left:deflated right:inflated Figure 2.Fetal activity belt Figure 3.Air actuator  
 4. Virtual simulation and interaction Mommy Tummy simulates a fetus s growth over nine months in the 
course of two minutes. The system simulates fetal weight, movement; and heartbeat while displaying the 
condition of both the fetus and mother (Figure 4).  1st Month 4th Month 8th Month Figure4.Simulating 
 The user can interact with the virtual fetus, which will respond according to a mood model. When the 
user moves violently, the fetus enters a bad mood state and makes intense movements. On the other hand, 
when the user caresses the abdomen, the fetus enters a good mood state and makes steady movements. Heavy 
physical exercise is discouraged until a stable period is reached. The system can simulate influence 
on the fetus resulting from such exercise. In the case of a fetus in breech position, the system can 
also account for the differences in fetal movement. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2048270</article_id>
		<sort_key>110</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>11</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Photochromic sculpture]]></title>
		<subtitle><![CDATA[volumetric color-forming pixels]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048270</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048270</url>
		<abstract>
			<par><![CDATA[<p>In contrast to light-emitting displays like plasma display panels (PDPs) and liquid crystal displays (LCDs), color-forming displays like "E-Ink" which displays information by reflecting surrounding light are being actively researched as a technology that is easy on the eye and can even be applied in bright places such as outdoors in sunlight. Applying photochromic materials (PM) for controlling color in this manner, Photochromic Canvas [Hashida et al.2010] and Slow Display [Saakes et al.2010], which are combined with projected-light systems, make it possible to control color without contact with the surface of an object. In this paper, the concept, namely, "contactless color control," is extended to volumetric space, and "volumetric color-forming pixels" are successfully created. This paper proposes a system referred to as "photochromic sculpture" which can generate a dynamically changeable 3D sculpture. (see Figure 1).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839679</person_id>
				<author_profile_id><![CDATA[81100411762]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tomoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hashida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[photochromic@nae-lab.org]]></email_address>
			</au>
			<au>
				<person_id>P2839680</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839681</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1836873</ref_obj_id>
				<ref_obj_pid>1836845</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hashida, T. et.al. 2010. Photochromic Canvas: Drawing with Patterned Light In <i>Proceedings of SIGGRAPH2010 Poster</i> 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1836843</ref_obj_id>
				<ref_obj_pid>1836821</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Saakes, D. et.al 2010. Slow Display In <i>Proceedings of SIGGRAPH2010 Emerging Technologies</i> 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1665196</ref_obj_id>
				<ref_obj_pid>1665137</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Parker, M 2009. Lumarca In <i>Proceedings of SIGGRAPH ASIAf09 Art Gallery and Emerging Technologies</i> 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778813</ref_obj_id>
				<ref_obj_pid>1833349</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Barnum, P. et.al. 2010. A Multi-Layered Display with Water Drops In <i>Proceedings of SIGGRAPH2010 paper</i> 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1935808</ref_obj_id>
				<ref_obj_pid>1935701</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Hashida, T. et.al. 2011. SolaColorFSpace Coloration with Solar Light In <i>Proceedings of TEI'11 Art Explorations</i> 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Photochromic Sculpture -Volumetric Color-forming Pixels TomokoHashida* YasuakiKakehi TakeshiNaemura 
TheUniversity ofTokyo KeioUniversity TheUniversity ofTokyo  Figure 1: Volumetric color-forming pixels 
Figure 2: Multicolored display Figure 3: Controlling UV illumination patterns 1 Introduction In contrast 
to light-emitting displays like plasma display panels (PDPs)and liquidcrystaldisplays(LCDs),color-forming 
displays like E-Ink which displays information by re.ecting surrounding light are being actively researched 
as a technology that is easy on the eye and can even be applied in bright places such as outdoors in 
sunlight. Applyingphotochromic materials(PM)for controlling color in this manner, Photochromic Canvas 
[Hashida et al.2010] and Slow Display [Saakes et al.2010], which are combined with projected-light systems, 
make it possible to control color with­out contact with the surface of an object. In this paper, the 
con­cept, namely, contactless color control, is extended to volumetric space, and volumetric color-forming 
pixels are successfully cre­ated. This paper proposes a system referred to as photochromic sculpture 
which cangenerate adynamically changeable3D sculp­ture.(seeFigure1). 2 Photochromic Sculpture Photochromic 
sculpture consists of two parts: a presentation part composed of laminatedplatescoated withPMgranulesand 
acon­trolpartformanipulatingprojected light.Asfor thePM,Spiropy­ran,whichproducescolorunderultraviolet 
light(UV) and returns gradually tocolorlessandtransparentwhen theUVlight isblocked, is used. In detail, 
.ve types of PM were used: PSP-7, 12, 21, 24, and 33 purchased from Yamada Chemical Co., Ltd. Each PM 
requires a different time for producing and loosing its respective color. As the transparentplatesonwhich 
theSpiropyran iscoated, glasswhichtransmitsUVisused.FortheUV source,along wave­length in the invisiblerange(i.e.,365 
nm)isused. In regard to the photochromic sculpture system, the following three technological innovations 
are noteworthy. First, introducing a structure with high transparency makes it possible to create a sim­plepresentationpartwhichdoes 
not require electronic control. Ac­cordingly, it is possible to design the presentation part on various 
scales. Second, controlling the time and strength of the UV illu­minationmakesitpossible tocontrol variouscolorssynchronously. 
Thatis,by controllingthestrengthand timeoftheUV illumination inaccordancewitheach typeofSpiropyranwithdifferent 
response timesforeach color, it ispossible toproduceand eliminate the in­tended colorattheintended timing.Third,thepositionof 
* e-mail: photochromic@nae-lab.org the produced colors is controlled three-dimensionally according to 
thepatternoftheprojectedUV light.Todothis,thegranulatedPM is arranged in such a way that it does not 
appear to be stacked up from theviewpoint of theUV-light source. Asdescribed in[Parker2009] and[Barnumetal.2010], 
this inno­vation isanapplicationof amethodforcontrolling3D imageswith a2Dprojector tocolor-productioncontrolbyUV 
light. 3 Experimental Results and Future Works Aprototypesystem,composed of10 laminatedglassplates(1010 
cm) spaced1 cmapart,wasconstructed.ThePM wascoated on the glass plates in a pattern of 3-mm-diameter 
dots. An example of a written3Dgeometricalpattern(inthiscase, acone)isshown inFig­ure1.Asshown in thisexample, 
the3Dgeometricalpatterncanbe viewed from various angles when the transparent cube containing it isheld 
in thehand. Anexampleof aglass photochromicsculp­ture formed by using Spiropyran of .ve different colors 
is shown in Figure 2. An example of projecting a different UV pattern for thesamepresentationpart isshown 
inFigure3.In thismanner, the photochromicsculpturecanbedynamicallychanged inaccordance withtheprojectedpatternofUV 
lightfromthecontrolpart. A system called photochromic sculpture -for controlling vol­umetric color-forming 
pixels with a UV-light pattern -was pro­posed and implemented.Photochromicsculpturecandisplay vol­ume 
data by producing pleasing colors even in bright places. At present, aiming atprojection control of more 
complicatedUVpat­terns,developmentof aUVprojectorusing a digital micro-mirror device isprogressing well.Moreover,investigationaiming 
tocre­ate larger-scalephotochromicsculptures inoutdoor locations in the samemannerasthesundialprinciple 
-namely, using sunlight which contains a considerable amount of UV light as the light source -is underway[Hashida 
et al.2011]. References HASHIDA, T. ET.AL. 2010. Photochromic Canvas: Drawing with PatternedLightInProceedings 
of SIGGRAPH2010 Poster 2010. SAAKES, D.ET.AL 2010. Slow Display In Proceedings of SIG-GRAPH2010 Emerging 
Technologies 2010. PARKER, M 2009. Lumarca In Proceedings of SIGGRAPH ASIAf09 Art Gallery and Emerging 
Technologies 2009. BARNUM,P. ET.AL. 2010.AMulti-LayeredDisplaywithWater Drops In Proceedings of SIGGRAPH2010 
paper 2010. HASHIDA,T. ET.AL. 2011. SolaColorFSpaceColorationwithSo­larLight In Proceedings of TEI 11 
Art Explorations 2011.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2048271</article_id>
		<sort_key>120</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>12</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[PocoPoco]]></title>
		<subtitle><![CDATA[a tangible device that allows users to play dynamic tactile interaction]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048271</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048271</url>
		<abstract>
			<par><![CDATA[<p>Today a tremendous amount of audio and visual information is stuffed onto the flat displays of smart phones. It is no doubt a very convenient age, but a little bit uniform. Our research focuses on interfaces with dynamic movements and interfaces that can change their own shape dynamically [1][2]. We have developed a box-shaped device called <i>PocoPoco</i> which controls the movement of columnar units with built-in solenoid actuators and utilizes them to give users dynamic tactile sensations (see Figure 1). <i>PocoPoco</i> is an input/output device that can be used without visual information, because it can indicate all input/output information through tactile sensations. This device is a versatile interface that can be used in a wide range of applications including games, telecommunication, and musical performance. It was conceived as a "new kind of interface which can be used by both people with visual impairments and people with normal vision".</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839682</person_id>
				<author_profile_id><![CDATA[81490670247]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takaharu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kanai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University, Hino, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kanai-takaharu@sd.tmu.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2839683</person_id>
				<author_profile_id><![CDATA[81488673349]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kikukawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University, Hino, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839684</person_id>
				<author_profile_id><![CDATA[81442606589]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tatsuhiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Suzuki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University, Hino, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839685</person_id>
				<author_profile_id><![CDATA[81319488008]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tetsuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University, Hino, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839686</person_id>
				<author_profile_id><![CDATA[81100274861]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kumiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kushiyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University, Hino, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1400970</ref_obj_id>
				<ref_obj_pid>1400885</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baba, T., Ushiama, T., and Tomimatsu, K. 2008. Emerging keys: interactive electromagnetic levitation keys. In ACM SIGGRAPH 2008 Posters (Los Angeles, California, August 11-15, 2008). SIGGRAPH '08. ACM, New York, NY, 1--1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>986050</ref_obj_id>
				<ref_obj_pid>985921</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Michelitsch, G., Williams, J., Osen, M., Jimenez, B., and Rapp, S. 2004. Haptic chameleon: a new concept of shape-changing user interface controls with force feedback. In CHI '04 Extended Abstracts on Human Factors in Computing Systems (Vienna, Austria, April 24 - 29, 2004). CHI '04. ACM, New York, NY, 1305--1308.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2048272</article_id>
		<sort_key>130</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>13</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Recompose]]></title>
		<subtitle><![CDATA[direct and gestural interaction with an actuated surface]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048272</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048272</url>
		<abstract>
			<par><![CDATA[<p>We present <i>Recompose</i>, a new system for manipulation of an actuated surface. By collectively utilizing the body as a tool for direct manipulation alongside gestural input for functional manipulation, we show how a user is afforded unprecedented control over an actuated surface. Our invention consists of the actuated surface and the interaction technique utilizing free-hand and touch gestures to manipulate the 3D geometry of the pin array, thus changing the of shape of the virtual object.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839687</person_id>
				<author_profile_id><![CDATA[81319495312]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Leithinger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[daniell@media.mit.edu]]></email_address>
			</au>
			<au>
				<person_id>P2839688</person_id>
				<author_profile_id><![CDATA[81484657398]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[D&#225;vid]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lakatos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[dlakatos@media.mit.edu]]></email_address>
			</au>
			<au>
				<person_id>P2839689</person_id>
				<author_profile_id><![CDATA[81482658252]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Anthony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[DeVincenzi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tonyd@media.mit.edu]]></email_address>
			</au>
			<au>
				<person_id>P2839690</person_id>
				<author_profile_id><![CDATA[81484657723]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Blackshaw]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mab@media.mit.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>258715</ref_obj_id>
				<ref_obj_pid>258549</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ishii, H. and Ullmer, B. 1997. Tangible bits: towards seamless interfaces between people, bits and atoms. In <i>Proceedings of the SIGCHI conference on Human factors in computing systems</i> (CHI '97). ACM, New York, NY, USA, 234--241.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1079775</ref_obj_id>
				<ref_obj_pid>1078037</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Benko, H., Ishak, E. W., and Feiner, S. 2005. Cross-dimensional gestural interaction techniques for hybrid immersive environments. In <i>Proceedings of IEEE Virtual Reality - VR '05</i>, (2005), 209--216.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1166292</ref_obj_id>
				<ref_obj_pid>1166253</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wilson, A. 2006. Robust computer vision-based detection of pinching for one and two-handed gesture input. In <i>Proceedings of the 19th annual ACM symposium on User interface software and technology</i> (UIST '06). ACM, New York, NY, USA, 255--258.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2048273</article_id>
		<sort_key>140</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>14</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Surround Haptics]]></title>
		<subtitle><![CDATA[sending shivers down your spine]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048273</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048273</url>
		<abstract>
			<par><![CDATA[<p>Surround Haptics is a new tactile technology that uses a <i>low-resolution</i> grid of <i>inexpensive</i> vibrating actuators to generate high-resolution, continuous, moving tactile strokes on human skin [1]. The user would not feel the discrete tactile pulses and buzzes that are so common today, but rather a <i>smooth tactile motion</i>, akin to what we feel when someone drags a finger across our skin.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839691</person_id>
				<author_profile_id><![CDATA[81416593491]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ali]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Israr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research Pittsburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839693</person_id>
				<author_profile_id><![CDATA[81100593278]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ivan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Poupyrev]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research Pittsburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839694</person_id>
				<author_profile_id><![CDATA[81490687397]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ioffreda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[School of Design, CMU]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839695</person_id>
				<author_profile_id><![CDATA[81490641994]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cox]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Black Rock Studio, Disney Interactive Studios, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839696</person_id>
				<author_profile_id><![CDATA[81490689320]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Nathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gouveia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Black Rock Studio, Disney Interactive Studios, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839697</person_id>
				<author_profile_id><![CDATA[81466648161]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Huw]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bowles]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Black Rock Studio, Disney Interactive Studios, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839698</person_id>
				<author_profile_id><![CDATA[81490695853]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Anastasios]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brakis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Black Rock Studio, Disney Interactive Studios, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839699</person_id>
				<author_profile_id><![CDATA[81490651426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Baylor]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Knight]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Black Rock Studio, Disney Interactive Studios, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839700</person_id>
				<author_profile_id><![CDATA[81490663992]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Kenny]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitchell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Black Rock Studio, Disney Interactive Studios, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839692</person_id>
				<author_profile_id><![CDATA[81490670696]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Williams]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Black Rock Studio, Disney Interactive Studios, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1979235</ref_obj_id>
				<ref_obj_pid>1978942</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Israr, A., Poupyrev, I. <i>Tactile Brush: Drawing on skin with a tactile grid Display</i>. To appear in ACM CHI 2011, May: Vancouver, Canada.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2048274</article_id>
		<sort_key>150</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>15</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Telenoid]]></title>
		<subtitle><![CDATA[tele-presence android for communication]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048274</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048274</url>
		<abstract>
			<par><![CDATA[<p>In this research, a new system of telecommunication called "Telenoid" is presented which focuses on the idea of transferring human's "presence". Telenoid was developed to appear and behave as a minimal design of human features. (Fig. 2(A)) A minimal human conveys the impression of human existence at first glance, but it doesn't suggest anything about personal features such as being male or female, old or young. Previously an android with more realistic features called Geminoid was proposed. However, because of its unique appearance, which is the copy of a model, it is too difficult to imagine other people's presence through Geminoid while they are operating it. On the other hand, Telenoid is designed as it holds an anonymous identity, which allows people to communicate with their acquaintances far away regardless of their gender and age. We expect that the Telenoid can be used as a medium that transfers human's presence by its minimal feature design.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010520.10010553.10010554</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems->Robotics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010199.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Planning and scheduling->Robotic planning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010213.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods->Robotic planning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839701</person_id>
				<author_profile_id><![CDATA[81381594236]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kohei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ogawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Intelligent Robotics and Communication Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ogawa@atr.jp]]></email_address>
			</au>
			<au>
				<person_id>P2839702</person_id>
				<author_profile_id><![CDATA[81456613369]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shuichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishio]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Intelligent Robotics and Communication Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839703</person_id>
				<author_profile_id><![CDATA[81490694594]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kensuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Intelligent Robotics and Communication Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839704</person_id>
				<author_profile_id><![CDATA[81490648731]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Koichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Taura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Intelligent Robotics and Communication Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839705</person_id>
				<author_profile_id><![CDATA[81100438978]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Intelligent Robotics and Communication Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839706</person_id>
				<author_profile_id><![CDATA[81490670938]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Carlos]]></first_name>
				<middle_name><![CDATA[Toshinori]]></middle_name>
				<last_name><![CDATA[Ishii]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Intelligent Robotics and Communication Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839707</person_id>
				<author_profile_id><![CDATA[81490674905]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Hiroshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishiguro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR and Osaka-University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Figure 1: (A) &#38; (B): A mother talking to her child through Telenoid. (C): Elementary school students 
discuss with Telenoid. (D): An elderly woman talking to Telenoid. (E): Dementia woman giving the Telenoid 
an emotion hug with tears. 1 Introduction In this research, a new system of telecommunication called 
Te­lenoid is presented which focuses on the idea of transferring hu­man s presence .Telenoidwasdevelopedto 
appearandbehave asa minimal design of human features. (Fig. 2(A))A minimal human conveys the impression 
of human existence at .rst glance, but it doesn tsuggest anything about personal features such as be­ing 
male or female, old or young. Previously an android with more realistic features called Geminoidwas proposed. 
However, because ofits unique appearance, whichisthecopyofa model,itistoodif­.cult to imagine other people 
s presence through Geminoid while they are operating it. On the other hand, Telenoid is designed as it 
holds an anonymous identity, which allows people to communi­cate with their acquaintances far away regardless 
of their gender and age.Weexpect that theTelenoid canbe used asa medium that transfers human s presence 
by its minimal feature design. 2 Telenoid The design ofTelenoid aims at creating an android with minimal 
human appearance. Such an appearance will allow different people to useTelenoid to transfer their presence 
to distant places regard­less of their personal features. In order to achieve this purpose, we considered 
a real human model and removed as many unneces­sary features as possible that were not crucial for communication 
with people. These unnecessary features were found from the re­sults of previous empirical studies. TheTelenoid, 
minimal design of human that is created by removing unnecessary features, might beable substituteanykindof 
person.TheTelenoid system consists of theTelenoid robot andatele-operation system. TheTelenoid has nine 
DOF(3 foreyes,1for mouth,3for neck,2for hands), which is enoughto represent the minimalfacialexpression 
and presence ofatele-operator. TheTelenoid s height is 80 cm, and its weight is about6kg.Thecoveringskinismadeofhighquality 
siliconsothat it feels as pleasant and soft as human skin when touched. The tele­operation systemofTelenoidis 
designedina simple and intuitive way that it can be controlled by even novice users. We have em­ployedaface 
tracking technology for the tele-operation interface. Theface-tracking system automatically captures 
the operator sface directions, mouthmovementsandfacialexpressions.Theextracted features are used to create 
commands that are sent to theTelenoid. Some speci.c behaviors, such as bye-bye or happy , can *e-mail: 
ogawa@atr.jp Figure 2: (A): Telenoid. (B): Tele-operation interface. Operator s face was captured by 
the webcam embedded on the laptop. be controlledby GUIbuttons. Also operator s voice is outputted froma 
loud speaker, whichis embedded insideof theTelenoid. 3 Field Tests Several .eld tests have been run 
in public to investigate the reaction of people towardTelenoid. In an art museum (Fig. 1(A)(B)) and an 
elementary school(Fig. 1(C)), participants experienced the position of both operator and interlocutor. 
In most cases of interlocutor they tendedtohavea strange feelingtowarditinthebeginningbutaf­ter a while 
theygot used to it and changed their mind to positive. While operating theTelenoid, participants could 
adapt to operation immediately, and theyseemed to enjoytheir interaction with their acquaintances throughTelenoid. 
Asa remarkable point,in the el­der carefacilities, the elderly people hadavery positive impression towardTelenoid 
at the .rst sight. (Fig. 1(D)). Although the con­ceptof tele-operationfortheTelenoidwasdif.culttomake 
sense for them, theydidn t want to stop the conversation when they were talking to it. One of them especially 
had tears of happiness when she was giving a hug to Telenoid. (Fig. 1(E)). Therefore for the elderly,Telenoid 
may play the role of an attractive conversational agent. These .eld tests show that theTelenoid canbe 
accepted asa communication medium for not only youthbut also the elderly peo­ple. However, we need to 
carefully consider the speci.c use case for each generation. Acknowledgement This research was supported 
by JST, CREST. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, 
Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2048275</article_id>
		<sort_key>160</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>16</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[The cyclone display]]></title>
		<subtitle><![CDATA[rotation, reflection, flicker and recognition combined to the pixels]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048275</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048275</url>
		<abstract>
			<par><![CDATA[<p>Most of us remember playing with colorful spinning tops when we were children. Here in our research, we made mechanical pixels with the spinning tops which introduces the mechanical display. This enables to enlarge the pixels without the direct lighting. Those mechanical pixels could be applied to make an ambient atmosphere in small or large space.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>B.4.2</cat_node>
				<descriptor>Image display</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839708</person_id>
				<author_profile_id><![CDATA[81466641621]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ochiai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ochyai@acm.org]]></email_address>
			</au>
			<au>
				<person_id>P2839709</person_id>
				<author_profile_id><![CDATA[81488664879]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiromu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Cyclone Display: Rotation, Re.ection, Flicker and Recognition combined to the pixels. Yoichi Ochiai* 
and Hiromu Takai* *University of Tsukuba, School of Information Science, College of Media Arts, Science 
and Technology  Figure 1: (top-left)Overviews of 37 pixels, (top-center) shows character 3 (top-right) 
Interaction with hand shadows and .icker (bottom-left)rainbow color on the pixels (bottom-center) enlargement 
of pixels, (bottom-right)texture variation of rotation disksIt s very dif.cult to take the picture as 
we see this display. It combines our color recognition and projected light. 1. Introduction Most of us 
remember playing with colorful spinning tops when we were children. Here in our research, wemade mechanical 
pixels with the spinning tops whichintroduces the mechanical display. This enables to en­large the pixels 
without the direct lighting. Those me­chanical pixels could be applied to make an ambientatmosphere in 
small or large space. We added some essentials factors to the tops such as.icker, projection, rotation 
speed. The combination en­ables us to make multicolored expression and to recog­nize the difference of 
rotating speed of the disks. This system should be able to contribute to a new kind of display engineering 
and interactive communication between display and humans. 2. Design The prototype of the Cyclone display 
is made up of 37 rotating mechanical pixels. The pixels consist of PWMmotor and pattern printing disk(Fig.1). 
Each pixel iscontrolled individually for its rotating speed. Now on .icker. Whether we use the .icker 
light or not we can see the colorful lines on rotating disks with variously printed patterns. So you 
could use rotating disk as apixel of no illuminated display. We can also observeseveral striped patterns 
by using white .icker or just by *email: ochyai@acm.org blinking the eyes. The colors and patterns depend 
on thespeeds of rotation and .icker. It is possible to change factors for each pixel according to the 
need and circumstance on the Cyclone display. Also I might add that if we use .ickered time-divided RGB 
lights, Cyclone display can express several color gradations. Cyclone display has many interesting factors 
and in­teractivity with the human(Fig. 2). By using natural body movement such as hands or eyes, the 
different color and different texture and different patterns are seen on theCyclone Display. 3. Application 
Cyclone Display offers the new and different approach for the interactions with the humans. We just have 
to wave our hand or blink the eyes. That enables us an in­dividual communication with the display. The 
individualcommunicability has a possibility for entertainment ap­plication. At the point of ambient display, 
it suggest thenew way to express Rotation and Recognition Pat­terns . Also, Cyclone pixels are easy to 
enlarge their sizes with low cost. Besides, we could choose the disk patterns to .t circumstances. 4. 
Future Work We introduced the prototype of Cyclone Display madeof printed-paper disks. If it was made 
of liquid crystalsdisplay we could change the patterns of each disk. On top of that, with projection 
images it would work as ro­tating disk screen for the new expression. It makes us to produce diffuse 
effect or re.ection effect on certain diskssuch as re.ective or unre.ective disks.They deepen thetexture 
expression of projected graphics. Furthermore, alittle illuminated Cyclone Display is good for decoration 
for buildings because of its low cost expansion fee and characterizing the individual/ambient communications. 
Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 
7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2048276</article_id>
		<sort_key>170</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>17</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[The Virtual Crepe Factory]]></title>
		<subtitle><![CDATA[6DoF haptic interaction with fluids]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048276</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048276</url>
		<abstract>
			<par><![CDATA[<p>The Virtual Crepe Factory illustrates our novel approach for 6DoF haptic interaction with fluids. It showcases a 2-handed interactive haptic scenario: a recipe consisting in using different types of fluid in order to make a special pancake also known as "crepe". The scenario guides the user through all the steps required to prepare a crepe: from the stirring and pouring of the dough to the spreading of different toppings, without forgetting the challenging flipping of the crepe. With the Virtual Crepe Factory, users can experience for the first time 6DoF haptic interactions with fluids of varying viscosity. Our novel approach is based on a Smoothed-Particle Hydrodynamics (SPH) physically-based simulation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839710</person_id>
				<author_profile_id><![CDATA[81447604105]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gabriel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cirio]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA Rennes]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[gcirio@inria.fr]]></email_address>
			</au>
			<au>
				<person_id>P2839711</person_id>
				<author_profile_id><![CDATA[81384592295]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Maud]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marchal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INSA/INRIA Rennes]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mmarchal@inria.fr]]></email_address>
			</au>
			<au>
				<person_id>P2839712</person_id>
				<author_profile_id><![CDATA[81340489558]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sebastien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hillaire]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA Rennes/Orange Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shillaire@inria.fr]]></email_address>
			</au>
			<au>
				<person_id>P2839713</person_id>
				<author_profile_id><![CDATA[81100289712]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Anatole]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[L&#233;cuyer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA Rennes]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[alecuyer@inria.fr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1006069</ref_obj_id>
				<ref_obj_pid>1006058</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baxter, W., and Lin, M. C. 2004. Haptic interaction with fluid media. In <i>Proceedings of Graphics Interface</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1180515</ref_obj_id>
				<ref_obj_pid>1180495</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Dobashi, Y., Sato, M., Hasegawa, S., Yamamoto, T., Kato, M., and Nishita, T. A fluid resistance map method for real-time haptic interaction with fluids. In <i>ACM VRST'06</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Virtual Crepe Factory: 6DoF Haptic Interaction with Fluids Gabriel Cirio Maud Marchal Sebastien 
Hillaire Anatole L´ecuyer INRIA Rennes* INSA/INRIA Rennes* INRIA Rennes/Orange Labs* INRIA Rennes*  
Figure 1: The Virtual Crepe Factory allows users to interact with different viscous .uids to achieve 
a crepe preparation recipe. The user follows the different steps of the crepe preparation process (stirring 
and scooping the dough, pouring the dough into the pan, .ipping the crepe, spreading the toppings) and 
feels corresponding force and torque feedback through two haptic devices, one in each hand. Abstract. 
The Virtual Crepe Factory illustrates our novel ap­proach for 6DoF haptic interaction with .uids. It 
showcases a 2­handed interactive haptic scenario: a recipe consisting in using dif­ferent types of .uid 
in order to make a special pancake also known as crepe . The scenario guides the user through all the 
steps re­quired to prepare a crepe: from the stirring and pouring of the dough to the spreading of different 
toppings, without forgetting the chal­lenging .ipping of the crepe. With the Virtual Crepe Factory, users 
can experience for the .rst time 6DoF haptic interactions with .uids of varying viscosity. Our novel 
approach is based on a Smoothed-Particle Hydrodynamics (SPH) physically-based simulation. Introduction. 
Fluids are present in many applications such as for industrial or medical manipulations -involving for 
instance blood .ow and natural liquids. The haptic simulation of .uids is partic­ularly challenging, 
especially to achieve realistic, stable and real­time force feedback. Previous methods for haptic .uid 
interaction [Baxter and Lin 2004] [Dobashi et al. 2006] were limited to 3DoF, simple objects or pre-computed 
forces. Thus, as for today, there is a lack of haptic rendering techniques handling complex interactions 
with viscous .uids. Our approach allows real-time 6DoF haptic in­teraction with .uids of variable viscosity, 
through arbitrary shaped rigid bodies and 6DoF haptic devices. Particularly, .uid containers can be created 
to hold .uid, hence transmiting to the user strong forces such as .uid resistance and weight, as well 
as light forces like the inertia of the .uid inside the container. Virtual Crepe Preparation Scenario. 
As shown in Figure 2, the user holds a 6DoF haptic device in each hand. The scenario can be divided in 
4 distinct consecutives stages. The .rst stage requires the user to stir and scoop the dough from a bowl 
on the table with his scooping object. It enables the user to feel the torque generated by the resistance 
from a highly viscous .uid opposing his stirring movement, and the weight and inertia of the .uid inside 
the hand­held object. In the second stage, the content of the scoop is poured into the pan held with 
the second hand. The user feels the weight transferring from one hand to the other. The third stage is 
the solid­i.cation and .ipping of the crepe. The user feels how mass shifts slow down as the viscosity 
of the dough raises with the solidi.ca­tion process. When the crepe is ready, it becomes a deformable 
ob­ject, and the user can try to .ip the crepe by throwing it into the air, illustrating the underlying 
phase-change capabilities of our model. The fourth and last stage is the spreading of toppings on the 
crepe, which combines all the previous haptic interaction possibilities by making the user scoop, pour 
and spread a low viscosity .uid (maple sirup) and a high viscosity .uid (Brittany s salt-butter caramel). 
*e-mail: {gcirio,mmarchal,shillaire,alecuyer}@inria.fr Figure 2: Setup: Bimanual interaction with two 
6-DoF haptic de­vices (Virtuose, Haption). Left hand for virtual bowl manipulation, Right hand for virtual 
pan manipulation. Technical Description. Our approach is based on an SPH physically-based simulation. 
We use a uni.ed model to achieve a real-time simulation by using SPH particles for both .uids and rigid 
bodies. Our approach allows real-time simulation of solid­.uid interactions with arbitrary-shaped rigid 
bodies and .uids of different viscosities. Furthermore, we use a novel 6DoF haptic ren­dering based on 
SPH, providing a soft and continuous haptic feed­back particularly well suited for the haptic interaction 
with .uids. This allows the seamless 6DoF haptic coupling between a haptic device and any rigid body 
of the virtual scene, but also between N haptic devices and the same rigid body (as holding a bucket 
with two hands). Our method is implemented on GPU and preliminary testings with standard PC led to 170Hz 
with 25,000 particles. Vision. The Virtual Crepe Factory proves that .uids are now ready for complex 
6DoF haptic interactions, and opens a new hori­zon of applications in Virtual Reality. Applications span 
from the medical .eld (organic .uids) to industrial scenarios (painting, ma­nipulating dangerous .uids) 
and entertainment simulations (natural scenes, water sports). We challenge the SIGGRAPH audience: are 
you a good crepe maker... or a bad one? Acknowledgements This work was supported by the European Community 
under FP7 FET-Open grant n.222107 NIW -Natural Interactive Walking. References BAXTER, W., AND LIN, 
M. C. 2004. Haptic interaction with .uid media. In Proceedings of Graphics Interface. DOBASHI, Y., SATO, 
M., HASEGAWA, S., YAMAMOTO, T., KATO, M., AND NISHITA, T. A .uid resistance map method for real-time 
haptic interaction with .uids. In ACM VRST 06. Copyright is held by the author / owner(s). SIGGRAPH 2011, 
Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2048277</article_id>
		<sort_key>180</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>18</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[Thermal Interactive Media]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048277</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048277</url>
		<abstract>
			<par><![CDATA[<p>The Thermal Interactive Media (TIM) display represents an engaging new form of tangible user interface that enables users to control the interaction and blending of multiple video streams through the physical mixing of water at different temperatures.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839714</person_id>
				<author_profile_id><![CDATA[81341493929]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Mine]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Imagineering, Glendale, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mark.mine@disney.com]]></email_address>
			</au>
			<au>
				<person_id>P2839715</person_id>
				<author_profile_id><![CDATA[81490681868]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dustin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barnard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Imagineering, Glendale, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[dustin.barnard@disney.com]]></email_address>
			</au>
			<au>
				<person_id>P2839716</person_id>
				<author_profile_id><![CDATA[81490681996]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Imagineering, Glendale, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[bei.yang@disney.com]]></email_address>
			</au>
			<au>
				<person_id>P2839717</person_id>
				<author_profile_id><![CDATA[81490677230]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Imagineering, Glendale, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[daniel.l.baker@disney.com]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2048278</article_id>
		<sort_key>190</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>19</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[Touch interface on back of the hand]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048278</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048278</url>
		<abstract>
			<par><![CDATA[<p>In this paper we propose a new computer---human interface which uses the back of the hand for pointer control.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839718</person_id>
				<author_profile_id><![CDATA[81421601844]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakatsuma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tsuma@alab.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2839719</person_id>
				<author_profile_id><![CDATA[81100555829]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shinoda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shino@alab.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2839720</person_id>
				<author_profile_id><![CDATA[81100425693]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yasutoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Makino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[makino@sdm.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2839721</person_id>
				<author_profile_id><![CDATA[81508684469]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Katsunari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sato@tachilab.org]]></email_address>
			</au>
			<au>
				<person_id>P2839722</person_id>
				<author_profile_id><![CDATA[81100377136]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maeno]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[maeno@sdm.keio.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1449746</ref_obj_id>
				<ref_obj_pid>1449715</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Butler, A., Izadi, S., and Hodges, S. 2008. Sidesight: multi-"touch" interaction around small devices. In <i>Proceedings of the 21st annual ACM symposium on User interface software and technology</i>, ACM, New York, NY, USA, UIST '08, 201--204.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1753394</ref_obj_id>
				<ref_obj_pid>1753326</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Harrison, C., Tan, D., and Morris, D. 2010. Skinput: appropriating the body as an input surface. In <i>Proceedings of the 28th international conference on Human factors in computing systems</i>, ACM, New York, NY, USA, CHI '10, 453--462.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Kei Nakatsuma:, Hiroyuki Shinoda The UniversityofTokyo 1 Introduction In this paper we propose a new 
computer human interface which uses the back of the hand for pointer control. Recently we can use many 
touch based interface including touch padonPCs,cell phones, tabletdevicesand portablegames. With these 
devices, they can arrange manydifferent types ofbuttons on the screen depending on applications. It enables 
small devices to achieve various functions on a small screen. There is one drawback for these touch panel 
devices. Users cannot feel haptic feedback like click since they do not have physical buttons. As a result, 
it is dif.cult to input data without anyvisual or auditory cues. User cannot input date while theyarewalking, 
for example. In order to solve the issue, we propose a new interface which uses the back of the hand 
as an input surface of pointing device. If we utilize our skin as an input surface, we can feel and perceive 
which area is tapped. There are previous researches that used user s body as an input in­terface. Harrison 
et al. proposed Skinput which detects tapping sound with microphones put on the skin surface to localize 
tapping position [Harrison et al. 2010]. User can input commandby chang­ing the tapping position on their 
body. We apply the similar idea to the small area: opisthenar (back of the hand). The area is relatively 
.at and user can use there like a conventional touch pad device. User can feel a precise tactile feedback 
since the sensitivity of a hand is higher than the other body parts. One of the important advantages 
is that manypeople use a wristwatch. If we improve a wristwatch so that it can detect .nger positiononthe 
opishtenar,wedonotneed additionalexternal devices. Since we used IR re.ection for detecting .nger position, 
ourdevice canbeusednotonlyforthebackofthehand,butforany .at surfaces. In our demo, we show how it works. 
We also show how easy you can input data by using your body with a wristwatch size device. 2 Prototype 
System Figure1shows our system.It consistsof three parts. Oneisapo­sition detection unit. We used an 
infrared re.ector for measuring two dimensional positionof.ngeronthebackofthehand.Weput 7 infrared LED 
detector pairs at the side of the wristwatch. The basic principle of .nger position detection has been 
also used in SideSight [Butler et al. 2008]. The second unit is a piezoelectric sensor which detects 
tapping sound on the opisthenar. It might pos­sible to detect tapping motion only from the infrared sensors, 
how­ever,it becomeseasyandrobustwhenweuseapiezoelectric sensor in addition. The sensor also can detect 
friction sound. This can be used for identifying various gestures on the back of the hand. The e-mail: 
tsuma@alab.t.u-tokyo.ac.jp e-mail: shino@alab.t.u-tokyo.ac.jp e-mail: makino@sdm.keio.ac.jp §e-mail: 
sato@tachilab.org ¶e-mail: maeno@sdm.keio.ac.jp  Figure1:Touch interfacedevice.Auser can input datebytouching 
the back of the hand. The user can feel the position of the manip­ulation from his/her own hand, the 
system do not need anytactile display device to give touch sensation. thirdunitisthedisplay.Wecan controlthe 
cursoronthe screenjust beside the screen. It makes user easy to input data to small devices. 3 Applications 
We showed one example of application. Auser can control his/her presentation slides with our proposed 
interface. The device can make it easy to control the slides. Since we can know the contact position 
by tactile perception at the opisthenar, we do not need to see the surface for input. This advan­tage 
can be used when a user is walking on the street, for example. The user can change music or call someone 
by touching his/her back of the hand. Acknowledgements Thisworkwas supportedinpartbyGrant in AidforYoung 
Scien­tistsB(21700185) and Grant in Aid for JSPS Fellows (21-5508) in Japan. References BUTLER,A.,IZADI,S., 
ANDHODGES,S. 2008. Sidesight: multi­ touch interaction around small devices. In Proceedings of the 21st 
annual ACM symposium on User interface software and technology,ACM,NewYork,NY, USA, UIST 08, 201 204. 
HARRISON, C., TAN, D., AND MORRIS, D. 2010. Skinput: ap­propriating the body as an input surface. In 
Proceedings of the 28th international conference on Human factors in computing systems,ACM,NewYork,NY, 
USA, CHI 10, 453 462. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, 
Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2048279</article_id>
		<sort_key>200</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>20</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[True 3D display]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048279</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048279</url>
		<abstract>
			<par><![CDATA[<p>Realizing a true three-dimensional (3D) display environment has been an ultimate goal of visual computing communities. Burton Inc. in Japan and others built upon the modern laser-plasma technology to come up with <i>3D Aerial Display</i> device in 2006, with which the users are allowed to plot a unicursal series of illuminants freely in the midair, and thus the surrounding audience can enjoy watching different aspects of the 3D image from different positions, <i>without any eye strain</i> [Kimura et al. 2006].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839723</person_id>
				<author_profile_id><![CDATA[81331496567]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hidei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Burton Inc., Aomi, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kimura@burton-jp.com]]></email_address>
			</au>
			<au>
				<person_id>P2839724</person_id>
				<author_profile_id><![CDATA[81100160727]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Akira]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Asano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Burton Inc., Aomi, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[asano@burton-jp.com]]></email_address>
			</au>
			<au>
				<person_id>P2839725</person_id>
				<author_profile_id><![CDATA[81100355096]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Issei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fujishiro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Yokohama, Kanagawa, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[fuji@ics.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2839726</person_id>
				<author_profile_id><![CDATA[81490651442]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ayaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakatani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Yokohama, Kanagawa, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2839727</person_id>
				<author_profile_id><![CDATA[81490670748]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hayato]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Watanabe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Yokohama, Kanagawa, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1179154</ref_obj_id>
				<ref_obj_pid>1179133</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kimura, H., Uchiyama, T., and Yoshikawa, H. 2006. Laser produced 3D display in the air. In <i>ACM SIGGRAPH 2006 Emerging technologies</i>, ACM, New York, NY, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Truue 3D Dissplay Hidei Kimurra, Akira Asanno Burtton Inc. Aomi, Tookyo, JAPAN {{kimura,asanoo}@burton-jp. 
com  1. Introductiion RRealizing a truee three-dimensiional (3D) dispplay environmennt has bbeen an 
ultimaate goal of visuual computing ccommunities. BBurton IInc. in Japan and others buuilt upon the mmodern 
laser-plasma ttechnology to ccome up with 3D Aerial Dispplay device in 2006, wwith which thee users are 
alloowed to plot aa unicursal seriies of iilluminants freeely in the mmidair, and thuus the surrouunding 
aaudience can eenjoy watching different aspeccts of the 3D iimage ffrom different positions, withhout 
any eye sttrain [Kimura et al. 22006]. TThrough the suubsequent updattes after the succcessful exhibit 
ion at SSIGGRAPH 20006, we hereinn present our llatest version oof the ddevice, called SRV (Superr Real 
Visionn)-5000, wheree we ccondensed our state of the aart laser-plasmaa technology iinto a ssmall acrylic 
caase (Fig. 1). (a) Full-bbody photo (b)) Magnified snaap of display unnit Figuure 1. True 3D ddisplay 
device SSRV-5000. 22. Hardwaree Performan ce Advancees TTable 1 compaares the SRV-50000 model withh 
our previous mmodel eexhibited at SIGGGRAPH 2006 Emerging Techhnologies in terrms of pperformance. Table 
1. PPerformance addvances during these five yearss. SRV-5000 22006 Model LLight source 6W 2200MJ RResolution 
50,000 poinnts/sec. 3300 points/sec. DDisplay volumee 20.×20..×20. 550.×50.×500. PPower source 100V 2200V 
 OOne remarkablee advance lies inn its resolution:: from 300 poinnts/sec. tto 50,000 poinnts/sec. whereaas 
the display volume gets a bit ddiminished. Addditional good nnews is that by vvery recent inveention 
oof red and greeen as well as b lue laser plasmma display capabbility, tthe current monnochromatic problem 
will be ccompletely solvved at oone burst by thee next version. Issei Fujishhiro, Ayaka NNakatani, Hayaato 
Watanabe Keio UUniversity YYokohama, Kaanagawa, JAPAAN fuji@icss.keio.ac.jp  Figg. 2 exemplifiees how 
faithfullly the SRV-50000 can display the objjects in real timme. The accomppanying video shhows more 
cleaarly thee big difference from the 2006 model. We beliieve that the SRRV­50000 model will increase 
the ppotency of util ization in varioous appplication fields. (a) TTornado (b) Humann face Figure 2. 
Exxamples of displayed objects wiith SRV-5000.  3. Software Suupport Thhough the preseent device has 
been substanttially updated, the currrent version of hardware still suffers from the spaarse apppearance 
probl lem, which shouuld lead infallibbly to inhibitionn of disssemination of tthe device to aapplication 
fieldds where solid aand reaalistic renderinng plays a maajor role, suchh as design aand filmm/game industrries. 
Wee are currentlly considering a software-bbased solution to ammeliorate this prroblem. To thiss end, 
a curvatture-based surfface desscriptor is desiggned to adaptivvely control the point density aand briightness 
reflec ting the geommetric features of the origiinal pollygonal modelss. We will demmonstrate at ouur 
booth how the sofftware solution can enhance thhe SRV-5000 s display capabi lity furrther. Gaaining such 
softftware supports,, our Super Reaal Vision seriess is surrely providing aa promising envvironment for 
paaving a way to the truue 3D display annd natural user iinteraction. Accknowledgeements Thhis work 
has bbeen supported by "Foundatioon of Technoloogy Suupporting the CCreation of DDigital Media CContents" 
Projject (CRREST, JST), J apan, and CORRE7 Project off Microsoft Jappan Coo., Ltd. Reeferences KIMURA, 
H., UCCHIYAMA, T., AND YOSHIKKAWA, H. 20066. LLaser producedd 3D display in tthe air. In ACMM SIGGRAPH 
20006 EEmerging technnologies, ACM,, New York, NYY, USA. Copyright is held by the author / owner(s). 
SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2048280</article_id>
		<sort_key>210</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>21</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA["Vection field" for pedestrian traffic control]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048280</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048280</url>
		<abstract>
			<par><![CDATA[<p>Public pathways, in areas such as stations and concert halls, can present pedestrian crowds with difficulties, for example, congestions and collisions (Figure 1-a). Although visual signs and audio cues are commonly used for smoother pedestrian flow, they are often ignored or neglected because they require some time to be understood. For more intuitive navigation, wearable devices have been proposed, some of which induce motion using a haptic device [Kojima et al. 2009] or vestibular stimuli [Maeda et al. 2005]. However, these <i>personal</i> navigation devices are not suitable for congestion scenarios because <i>numerous</i> devices are required for controlling pedestrian flow, which is quite impractical. In this paper, we propose a novel method to control pedestrian flow using an environmental visual cue, which is produced by the placement of large lenticular lenses on the floor.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839728</person_id>
				<author_profile_id><![CDATA[81482652074]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiromi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yoshikawa@kaji-lab.jp]]></email_address>
			</au>
			<au>
				<person_id>P2839729</person_id>
				<author_profile_id><![CDATA[81482659894]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Taku]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hachisu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hachisu@kaji-lab.jp]]></email_address>
			</au>
			<au>
				<person_id>P2839730</person_id>
				<author_profile_id><![CDATA[81350575122]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shogo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fukushima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shogo@kaji-lab.jp]]></email_address>
			</au>
			<au>
				<person_id>P2839731</person_id>
				<author_profile_id><![CDATA[81335490502]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Masahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Furukawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[furukawa@kaji-lab.jp]]></email_address>
			</au>
			<au>
				<person_id>P2839732</person_id>
				<author_profile_id><![CDATA[81100131163]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kajimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kajimoto@kaji-lab.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kojima, Y. et al., Pull Navi. Emerging Technologies Session, <i>ACM SIGGRAPH</i>, 2009]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187315</ref_obj_id>
				<ref_obj_pid>1187297</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Maeda, T. et al., "Shaking The World: Galvanic Vestibular Stimulation As A Novel Sensation Interface", Emerging Technologies Session, <i>ACM SIGGRAPH</i>, 2005]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lishman, J. R. and Lee, D. N. 1973. The autonomy of visual kinaesthesis. <i>Perception 2</i>, 287--294]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Vection Field for Pedestrian Traffic Control Hiromi Yoshikawa*  Taku Hachisu*  Shogo Fukushima*  
Masahiro Furukawa*  Hiroyuki Kajimoto*  Figure 1 (a)Congestions and collisions occur in a public pathway, 
(b)Proposed method: visual cues produced, (c)Proposed method keeps the pedestrians on the right of the 
pathway and prevents congestions and collisions. 1. Introduction Public pathways, in areas such as stations 
and concert halls, can present pedestrian crowds with difficulties, for example, congestions and collisions 
(Figure 1-a). Although visual signs and audio cues are commonly used for smoother pedestrian flow, they 
are often ignored or neglected because they require some time to be understood. For more intuitive navigation, 
wearable devices have been proposed, some of which induce motion using a haptic device [Kojima et al. 
2009] or vestibular stimuli [Maeda et al. 2005]. However, these personal navigation devices are not suitable 
for congestion scenarios because numerous devices are required for controlling pedestrian flow, which 
is quite impractical. In this paper, we propose a novel method to control pedestrian flow using an environmental 
visual cue, which is produced by the placement of large lenticular lenses on the floor. 2. Method We 
focused on large moving visual cues, which induce a sense of self-movement. For example, a large right-moving 
image on the floor induces a shift in walking direction to the right, even if the pedestrian tries to 
walk straight. This phenomenon occurs because of the dominance of vision on body balance [Lishman and 
Lee 1973]. Based on this observation, we placed a visual cue on the floor, which moved to the right relative 
to pedestrian. Consequently, the cue guided the pedestrians to the right (Figure 1-b). 3. Implementation 
To implement our plan without a power source we used lenticular lenses. The lenticular lenses have a 
sheeted array of cylindrical lenses. Different images appear depending on the angle, achieving a type 
of animation. We prepared the lenticular lenses containing images of striped patterns. When the pedestrians 
moved forward, the stripes shifted to the right, inducing the pedestrians to the right side (Figure 2). 
Note that the configuration is exactly the same for the two opposite-facing pedestrians. As shown in 
Figure 1-b, a woman in red walks on the lenticular lenses and watches the right-moving pattern. In contrast, 
the other pedestrian (the man in blue) observes the left-moving (for him, right-moving) pattern. Thus, 
his pathway bends to the left. As a result, the single sheet of lenticular lenses simultaneously works 
in two ways, avoiding the collision of pedestrians (Figure 1-c).  Figure 2 The lenticular images change 
depending on the angle of the line of sight. The advantages of our method are summarized as follows. 
First, it achieves intuitive navigation that does not require a recognition process. Second, it does 
not require pedestrians to wear any devices. Third, our method does not require an electrical power supply. 
Fourth, a single sheet of lenticular lenses naturally works for both two opposite-facing pedestrians. 
References KOJIMA, Y. et al., Pull Navi. Emerging Technologies Session, ACM SIGGRAPH, 2009 MAEDA, T. 
et al., Shaking The World: Galvanic Vestibular Stimulation As A Novel Sensation Interface , Emerging 
Technologies Session, ACM SIGGRAPH, 2005 LISHMAN, J. R. AND LEE, D. N. 1973.The autonomy of visual kinaesthesis. 
Perception 2, 287 294 * The University of Electro-Communications *{yoshikawa, hachisu, shogo, furukawa, 
kajimoto}@kaji-lab.jp 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2048281</article_id>
		<sort_key>220</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>22</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[A volumetric display based on a rim-driven varifocal beamsplitter and LED backlit LCD]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2048259.2048281</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2048281</url>
		<abstract>
			<par><![CDATA[<p>We demonstrate a volumetric display providing high resolution, full color, real images with 10 addressable depth planes and correct focus and vergence cues for the viewer. The display is centered around a novel optical element; a large-aperture, rim-driven, adjustable-resonance, varifocal beamsplitter.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2839733</person_id>
				<author_profile_id><![CDATA[81466647221]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lanny]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Smoot]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research, Glendale, Ca]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[lanny.s.smoot@disney.com]]></email_address>
			</au>
			<au>
				<person_id>P2839734</person_id>
				<author_profile_id><![CDATA[81440616541]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Quinn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Smithwick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research, Glendale, Ca]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[quinn@disneyresearch.com]]></email_address>
			</au>
			<au>
				<person_id>P2839735</person_id>
				<author_profile_id><![CDATA[81384603319]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reetz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research, Glendale, Ca]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[daniel.reetz@disneyresearch.com]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ABasedona-Driven VolumetricDisplayRim h VarifocalBacklitLCD Beam splitterandLED h h Lanny Smooth Disney 
Researchh Glendale, Ca,USAh lanny.s.smoot@disney.comh n 1. Introductionn h h h QuinnSmithwickh h h h 
h DisneyResearchh h h h h Glendale,Ca,USAh h h h h quinn@disneyresearch.comh h DanielReetzh DisneyResearc 
hh Glendale,h Ca,h USAh daniel.reetz@disneyresearch.comh h h h h Wedemonstrateavolumetricdisplayprovidinghighh 
resolution,fullh color,depthplanes andcorrectfocus realimageswith10addressableandvergencecuesfortheviewer.Thedisplayisc 
enteredaroundah novelh opticalh element;h ah large-aperture,rim-driven ,h adjustable­ resonance,varifocalbeamsplitter.h 
h h h Ah fullcolorLiquidCrystalDisplayh (LCD),backlith byh anactivelyh addressedLEDarray,istheprimaryimagesource.h 
Thebacklightish synchronizedh totheh vibratingbeam-splitterandstr obedh tofixh theh positionofimageplanesinavolume.Afixedcon 
cavemirrorintheh opticalpathmakestheimagesreal,andplacesthem outinfrontofh theapparatus.Depthsexceeding30cmareachieved 
witha30cmh diagonaldisplayedimage.h 2.Backgroundn Theh principleforh thish typeh ofh displaywash initiall 
yh proposedbyh i[Traub],withsubsequentrefinementsbyh ii[Rawson]andwasbrieflyh commercializedh h ash 
theh SpaceGraphh displayh fromh iiiGenisco/BBNh duringtheearly1980s.h h  Inh earlysystemsh (fig.h 1)h 
usersh viewedsequentially -presentedh 2Dh imageslicesonahighspeedCRTdisplayviareflec tioninanopaqueh 
flexingh mirror.h Themirrorwascomprisedofah refl ectiveh Mylarh membranestretchedoverthemouthofaloudspeaker. 
Theeffecth wasopticalsweepofthe2Ddisplay sreflectionth roughavirtualh (behind themirror)volume.h h h 
 Figure1n Earlynumberofdrawbacks: systemshadah Noise Theclosed-backloudspeakerradiatedintot he room. 
 Blockedh Viewing Theuser sviewingapertureispa rtiallyh obscuredprimarydisplay. bythe  Virtual images,behindthevibrati 
 imagesRemotengmirror.h  RequirementforhighresolutionANDhighspeedfrom primaryh image source.  Depthh 
inversion Theopticscausedremoteobjectsh toh lookh larger,h  requiringelectronicpre-distortion.h  obsolete)CRTtechnology 
 Bulky(andnowh n 3.Driven RimBeam-SplitterwithActiveBacklightn h Ash showninfig.h 2,h weh employanewh 
opticalh element ;h alarge­apertureh (1h meter),h adjustable-resonance,h rim-drive n,h varifocalh beamsplittertoprovideanunobstructedviewofthe 
outputimage.h Thebeamsplitterisresonatedatvibrationratesof 20to30Hzorh higher,bymeansofahooptensioningring,andish 
drivenby3audioh vibratorsalongitsperipheryleavingitsapertureh open.Sinceitish openatfrontandback,radiatedsoundisminimized 
.Wedecoupleh therequirementsforhigh-speedrenderingandhigh-resolutionfull­colorh imagery,h byh useofh 
ahighresolution,off-th e-shelfh LCD,h backlitbyahighframerate(>300fps)addressabl e(currently16xh 16)LEDs. 
arrayofwhite-lighth h  Figure2n Afixedconcavemirrorcorrectsthedepthinversion inherentintheh vibratingmirrorsystem,andmakestheimagesrealh 
sothattheyfloath outthesystemwheretheyareeuser.h infrontofaccessibl totheh h h iTraub,A1967.StereoscopicDisplayUsingRapidVa 
rifocal Mirrorh Oscillations.h 5-1087h AppliedOptics,Vol.6,No.6June108 iiga Rawson,E.1968.3-DComputer-GeneratedMoviesUsin 
Varifocal8 Mirror.AppliedOptics,Vol.7,No.1505 -1511iii h Favalora,G.2005.Volumetric3DDisplaysandAppli 
cationh Infrastructure, Computer,August,37-44 Copyright is held by the author / owner(s). SIGGRAPH 2011, 
Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
</content>
</proceeding>
