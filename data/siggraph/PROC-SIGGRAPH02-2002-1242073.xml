<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>07/21/2002</start_date>
		<end_date>07/26/2002</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[San Antonio]]></city>
		<state>Texas</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>1242073</proc_id>
	<acronym>SIGGRAPH '02</acronym>
	<proc_desc>ACM SIGGRAPH 2002 conference abstracts and applications</proc_desc>
	<conference_number>2002</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn>1-58113-525-4</isbn>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2002</copyright_year>
	<publication_date>07-21-2002</publication_date>
	<pages>337</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>This publication contains material presented at the ACM SIGGRAPH 2002 conference.</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<chair_editor>
		<ch_ed>
			<person_id>P283344</person_id>
			<author_profile_id><![CDATA[81100578452]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[Tom]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Appolloni]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[Harris Corporation]]></affiliation>
			<role><![CDATA[Conference Chair]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
</proceeding_rec>
<content>
	<section>
		<section_id>1242074</section_id>
		<sort_key>15</sort_key>
		<section_seq_no>1</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Educators program]]></section_title>
		<section_page_from>15</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP42060439</person_id>
				<author_profile_id><![CDATA[81341494013]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Mohler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1242075</article_id>
		<sort_key>15</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A knowledge base for the computer graphics discipline]]></title>
		<page_from>15</page_from>
		<page_to>15</page_to>
		<doi_number>10.1145/1242073.1242075</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242075</url>
		<abstract>
			<par><![CDATA[<p>Computer graphics is a powerful medium used to communicate information and knowledge. It is a discipline whose time has come. Until recently it was a mysterious specialty involving expensive display hardware, considerable computing resources, and specialized software. In the last few years, computer graphics has found its way into the mainstream of society, from entertainment, to engineering design, to the web, and virtually every industry. Much of this has been the result of spectacular improvements in the price and performance of computer graphics hardware and software. Interactive computer graphics is finding its way into nearly every discipline, industry, home, hospital, theatre, football stadium, automobile, appliance, and engineering office. Computer graphics is or will have an impact on nearly everything we do. The discipline of computer graphics is like a wide-open frontier where no matter which direction you move you will find more opportunities and undiscovered applications.</p> <p>Computer graphics is the pictorial synthesis of real or imagined objects from their computer-based models. A related field is image processing which is the reconstruction of models of 2D and 3D objects from their pictures (Foley, et al, 1996). Traditionally, computer graphics is contained within the well-recognized disciplines of computer science, electrical and computer engineering, and art and design. The discipline of computer graphics has traditionally focused on developing new software algorithms (computer science) and hardware innovations (electrical and computer engineering) with an attempt to force art and design principles into the mix. Those types of developments will continue but the discipline is beginning to mature to a point where it is more than just about hardware and software developments and innovations. More attention must be given to the effective and novel use of the hardware and software developments in the context of graphics communication. This is a much more holistic approach to computer graphics as a discipline. This holistic approach is more focused on the end user and how these tools can be used in much more profound and revolutionary ways.</p> <p>Obviously there have been many major and profound applications of computer graphics in its short history. However, the recent past of computer graphics is but a prelude of what is to come. Computer graphics can and will touch every human in a number of ways from entertainment to assisting in finding the cure of the most dreaded diseases. Computer graphics can and will have a profound effect on every type of business, industry, government, education, and the home. But it will take a very special type of education to prepare this next generation of computer graphics specialists.</p> <p>Computer graphics is a powerful medium but only if combined with the principles of information design. The principles of information design are universal, like mathematics, and are not bound to unique features of a particular language or culture. The universal nature of the graphic language makes it a powerful tool in today's society where collecting, analyzing, and communicating all the available information is so important. Information becomes knowledge and knowledge can be communicated more efficiently through computer graphics. Knowledge can become power and is the catalyst for stunning new developments in virtually every field.</p> <p>The pure computer graphics discipline of the future will not be in computer science, art and design, technology, or electrical and computer engineering. The computer graphics discipline of the future will have its legacy in all these disciplines but will look to merge the software and hardware technology with the human communication process, which will result in novel ways of solving problems and disseminating information. As shown in Figure 1, computer graphics is the overlap between art, science and technology, and psychology.</p> <p>The ideal student of this emerging discipline is bright, articulate, visual, analytic, and motivated by a passion for computer graphics. This student uses both sides of their brain but is keenly focused on the visual mode to solve problems. They are the modern-day Da Vincis, capable of visualizing what is nonexistent and finding solutions to complex problems. New opportunities are unfolding that require special talents and abilities for people with high visualization abilities who can use computer graphics tools to visualize scientific concepts and for the analysis and manipulation of complex three-dimensional information. As these new opportunities continue to unfold, the skill in manipulating and creating imagery may become more important than skill with words and numbers. Different kinds of tools may require different talents and favor a different type of discipline.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P93859</person_id>
				<author_profile_id><![CDATA[81100372630]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gary]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Bertoline]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14195825</person_id>
				<author_profile_id><![CDATA[81100565005]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Cary]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Laxer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rose-Hulman Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>208249</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Foley, J. D., Van Dam, A., Feiner, S. K. and Hughes, J. F. 1996. <i>Computer Graphics: Principles and Practice</i>. Addison-Wesley: Reading, MA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242076</article_id>
		<sort_key>16</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Game development, design and analysis curriculum]]></title>
		<page_from>16</page_from>
		<page_to>16</page_to>
		<doi_number>10.1145/1242073.1242076</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242076</url>
		<abstract>
			<par><![CDATA[<p>Electronic gaming has changed. What was a curiosity twenty years ago is now one of the most popular forms of entertainment for all age groups. Games are now an accepted, pervasive part of US and world culture. The new ubiquity of games demands that we understand them as commercial products, aesthetic objects, learning contexts and cultural phenomena. This, in turn, creates a demand for people to fill a variety of roles associated with the creation, consumption and analysis of games.</p> <p>For gaming to have a healthy future, industry and academia must find or create new contributors with new goals, ideas and experiences. We believe that educators have the best chance of creating and fostering these new goals and ideas while industry brings to the table a vital experiential understanding of the realities of game development. Academia and industry must work together, then, if progress is to be made.</p> <p>Schools have begun to recognize the cultural and academic relevance of games. In addition to their increasing presence in the fabric of student life, games are rapidly evolving -- diversifying and broadening their role as a communication medium. As universities design academic programs for the critical analysis of games and begin researching related interactive entertainment technologies, they see the value of including industry veterans in their plans.</p> <p>The International Game Developers Association's Education Committee, comprised of both developers and academics, has been working on a curriculum framework and set of guidelines to aid in the development of game-oriented curriculum. During this session, the Education Committee's Curriculum Subcommittee will present its work to date, and solicit input and criticism on the framework. The goal is to generate significant feedback from the academic community to further refine the framework.</p> <p>Here are some of the guiding principles used to develop the curriculum framework:</p> <p>&#8226; <b><i>Gaming is an interdisciplinary field</i></b>. This curriculum framework presents a wide range of topics encompassed by the acts of game creation, analysis and criticism. We strongly advocate cross-disciplinary study in any game-related course or curriculum.</p> <p>&#8226; <b><i>Analysis, practice and context are equally important</i></b>. Game development students must be exposed to analytical, practical and contextual materials.</p> <p>&#8226; <b><i>The framework was designed to be versatile</i></b>. Acknowledging the realities of educational institutions, especially when it comes to the introduction of new disciplines to the curriculum, we felt we had to create a game studies outline that was as versatile as possible.</p> <p>&#8226; <b><i>The framework is not vocational but is career-oriented</i></b>. The curriculum framework is focused not on churning out developers, per se, but on turning out well-rounded students of gaming, some of whom will no doubt choose to become developers.</p> <p>The IGDA's curriculum framework and related endeavors can be found online at the IGDA web site: http://www.igda.org/Committees/education.htm</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31062444</person_id>
				<author_profile_id><![CDATA[81100044344]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[Della]]></middle_name>
				<last_name><![CDATA[Rocca]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IGDA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864756</person_id>
				<author_profile_id><![CDATA[81328488717]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Robin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hunike]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Northwestern University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35038560</person_id>
				<author_profile_id><![CDATA[81328490438]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Warren]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Spector]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ION Storm Austin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37034667</person_id>
				<author_profile_id><![CDATA[81322510969]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zimmerman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[gameLab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242077</article_id>
		<sort_key>17</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Inspiring the renaissance person]]></title>
		<page_from>17</page_from>
		<page_to>17</page_to>
		<doi_number>10.1145/1242073.1242077</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242077</url>
		<abstract>
			<par><![CDATA[<p>Originally, "university" was given its name because it was believed that a student would go to the university to find his or her place within the universe. Today's liberal arts education has limited this universal process and forces an early decision: the "major." Students follow a well-defined curriculum that often precludes interdisciplinary study and cross-campus interaction. While most universities provide structure for in-depth, discipline-specific education; few offer curricula that take the student over many academic routes before making a decision on the major. Technical requirements can be the most constraining due to the vast foundational knowledge that must be mastered in the sciences and engineering.</p> <p>Educational reward and economic systems force segregation of the disciplines. Faculty are often rewarded to be discipline specific, in both research and teaching. Students are not able to take classes across campus because the "major's" students fill the courses leaving no room for non-majors. Universities are struggling due to low budgets and cutbacks. And it is very difficult to manage students who want to create new degrees that span campus boundaries.</p> <p>In contrast to this segregation of disciplines, the computer graphics industry often demands generalist knowledge. Often a programmer is required to design or a designer is required to have advanced technical skills. It is apparent in the area of computer graphics, visual literacy is one of the most critical skills that span both art and science [West 1991]. A National Research Council committee studying these issues found a strain on the talent pool in 3-D computer graphics and that universities are failing to provide adequate cross-disciplinary training: "The development of workers with a mixture of technical and artistic capabilities represents a particular challenge because of its interdisciplinary nature. Whereas computer science and electrical engineering departments will train technical workers to address questions about networking and distributed simulation, the creation of visually literate workers demands cooperation between engineering and art departments, which are separated by large cultural and institutional gaps" [Zyda et al. 1997]. These cultural divisions have been talked about since C. P. Snow's essay on "Two Cultures." However there are many cultures and cultures within cultures. Within the educational system, there are many illogical divisions within and among departments. Certainly the divide between science and art is the greatest in education. How does this type of education prepare the student to find himself or herself in the universe? Given the constraints of contemporary educational systems, how can we educators inspire the Renaissance person, one who has truly interdisciplinary knowledge and skills?</p> <p>Issues to consider:</p> <p>&#8226; How do we focus students' attention without constraining the breadth of knowledge?</p> <p>&#8226; As educators, we are asked to prepare students for the "market"...do we really know the market?</p> <p>&#8226; Should artists be taught to develop software as part of their curriculum?</p> <p>&#8226; What are examples of organizing projects that encourage cross-disciplinary thinking?</p> <p>&#8226; How does one find willing collaborative partners on large campuses?</p> <p>&#8226; How can technology help in this quest to match collaborators and facilitate interdisciplinary activity?</p> <p>&#8226; How can we change or by-pass organizational structures that inhibit the Renaissance person?</p> <p>&#8226; Do collaborative teams in the classroom facilitate or hinder this kind of thinking?</p> <p>&#8226; Given the vast knowledge that an interdisciplinary study must cover, should college terms be extended?</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39062826</person_id>
				<author_profile_id><![CDATA[81100290741]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Donna]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cox]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Illinois at Urbana-Champaign]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>523145</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[West, T., 1991. <i>In the Mind's Eye: Visual Thinkers, Gifted People with Learning Difficulties, Computer Images, and the Ironies of Creativity</i>. Prometheus Books, NY. p. 254.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>273033</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Zyda, M., Cox, D., Katz, W., Larson-Mogal, J., Louie, G., Lypaczewski, P., Pausch, R., Singer, A., Wesiman, J., 1997. <i>Modeling and Simulation Linking Entertainment and Defense</i>. National Academic Press: Washington, D.C. p. 93.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242078</article_id>
		<sort_key>18</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Teaching gems for art and design]]></title>
		<page_from>18</page_from>
		<page_to>18</page_to>
		<doi_number>10.1145/1242073.1242078</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242078</url>
		<abstract>
			<par><![CDATA[<p>Despite the artistic maturity computer graphics has gained throughout the 1990s and into the new millennium, what we call the art student "wow" factor in computer graphics---the phenomenon of student obsession over new technology instead of artistic substance---is still a point of contention. It remains a challenge to teach computer graphics to art students while maintaining a balance between thoughtful art and sophisticated technology. In this session we will, with the help of the participants, reveal pedagogical solutions and uncover various approaches to teaching art students to use digital media in rich and expressive ways.</p> <p>When the curve of new computer graphics technology was steeper and interfaces were not as intuitive as they now are, much of student learning was centered on the hardware and software and students struggled to comprehend digital media's place in the art world. In essence, we were in what Marshall McLuhan referred to as the first phase of a new technology---a stage in which, in the context of teaching digital media in the arts, the students were trying to understand computer graphics technology in terms of what they already knew. A digital image became a painting or a photograph, and 3D modeling and animation were understood in terms of film studies. Further, many students did not understand the scope of digital art and how it encompassed sculptural digital installations, how the art did not have to result in physical objects, or how the pieces could exist as interactive programs or websites. Students also worried more about the technology---what it could do and how to use it---rather than understanding the unique ways in which they could create aesthetically pleasing and profound works of art.</p> <p>Today, computer graphics-based art is now closer to being established in the art world and embraces a more intuitive and sensible mode in which to create. However, the "wow" factor is still a problem and the changing pace of technology still keeps students and teachers alike tuned into the tool. Students are still trying to understand what the technology can do and often forget why or what they are making. Art instructors have a genuine struggle with balancing the time to teach students how to do something digitally with teaching them how to express themselves visually. Further, some software programs are truly complex and the concepts behind them require the student to understand, at least intuitively, the laws of physics, lighting, and how things change over time. Perhaps in some ways we are still in the first phase of digital technology in the arts.</p> <p>From uncomplicated to elaborate software, how can instructors approach lessons without being bogged down with too much technology? Should instructors teach software or should they teach concepts, thus emphasizing the importance of technology as a means to an end rather than the end? What are some examples of lessons, assignments, or overall approaches that give proper weight to art and technology? How can teachers facilitate unique and expressive works of art?</p> <p>Even if instructors are successful teaching individual expression with computer graphics technology, what domain within this vast field is most important? What are some areas on which teachers could focus their energies? Are some computer graphics disciplines harder to teach than others? Is it more difficult to express ideas and compose formally beautiful works in some forms of digital graphics? Why or why not?</p> <p>The best lessons and the best pedagogical approaches still will not hinder the dedicated special effects gurus from obsessing about what the software can do instead of the resulting work. How can instructors help these students care more about the art works?</p> <p>Finally, and maybe most importantly, how can teachers keep up with the technology? Perhaps it is the teacher's concern with keeping current that influences the way they teach and what the students perceive as important. What are some solutions for instructors to keep up with changing computer graphics technology without compromising the core of the art they want to teach?</p> <p>We ask the participants to come prepared to talk about some of these ideas and more. Once we target issues to resolve, we will ask the forum participants to put forward their gems: lessons, approaches, philosophical bends, and ideas that have worked or they believe will work. Attendees will leave the session with a notebook full of ideas and solutions to teaching students how to be expressive and create aesthetically pleasing works of computer graphics based art.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P65384</person_id>
				<author_profile_id><![CDATA[81100501759]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dena]]></first_name>
				<middle_name><![CDATA[Elisabeth]]></middle_name>
				<last_name><![CDATA[Eber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bowling Green State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28021471</person_id>
				<author_profile_id><![CDATA[81319498084]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bonnie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitchell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bowling Green State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31063442</person_id>
				<author_profile_id><![CDATA[81328488214]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Heather]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Elliott]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bowling Green State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242079</article_id>
		<sort_key>19</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[The role of creativity in computer graphics education]]></title>
		<page_from>19</page_from>
		<page_to>19</page_to>
		<doi_number>10.1145/1242073.1242079</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242079</url>
		<abstract>
			<par><![CDATA[<p>This forum will give attendees a chance to present their own views on creativity and curriculum, as well as hear those of educators from a diverse group of colleges. Computer graphics education has grown tremendously in the last five years, particularly on the department level. Many issues have arisen related to the place of computer graphics education within a specific department's curriculum. They include the type of courses offered, challenges arising from the impact of this added educational component, and the desire to maintain traditional art education elements, such as theory and critique.</p> <p>Creativity has historically been addressed in theory and critique classes. It is also now being taught along with software in computer graphics classes. Outcomes from this forum will allow educators to gain insight into their approach to nurturing creativity as it relates to computer graphics education. Other topics to be discussed include interdisciplinary approaches to curriculum, creating content for courses, and the relationship of computer graphics to traditional art education. The role of creativity in computer science classes, particularly programming, will also be discussed.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP30040917</person_id>
				<author_profile_id><![CDATA[81100113498]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bruce]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wands]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[School of Visual Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242080</article_id>
		<sort_key>20</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Teaching gems for computer science and engineering]]></title>
		<page_from>20</page_from>
		<page_to>20</page_to>
		<doi_number>10.1145/1242073.1242080</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242080</url>
		<abstract>
			<par><![CDATA[<p>Moderators and attendees of this forum will present, discuss, and assess examples of "best practices" for teaching computer graphics to computer science and engineering students. Types of "teaching gems" to be presented include: classroom lectures and demonstrations, lab exercises, homework projects, self-instruction techniques such as tutorials, demo programs, and Web applets, use of analog models, examples from industry and the rest of the "real world", and field trips (real and virtual). The forum moderators will contribute presentations by electronic submission from virtual attendees prior to the conference and by attendees on site during the forum session. Three outcomes are anticipated from this forum: 1. presentation of several teaching gems, 2. group analysis and discussion of how best to apply each gem in a course and it's learning effectiveness, and 3. posting of the results for public access on the SIGGRAPH Education Committee web site, http://www.siggraph.org/education.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39062775</person_id>
				<author_profile_id><![CDATA[81324487523]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bailey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39048689</person_id>
				<author_profile_id><![CDATA[81100562918]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cunningham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California State University, Stanislaus]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31028717</person_id>
				<author_profile_id><![CDATA[81418597492]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Lew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hitchner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California Polytechnic State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>574853</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Glassner, A. 1990. <i>Graphics Gems I.</i> Morgan Kaufmann Publishers.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Paeth, A. 1995. <i>Graphics Gems V.</i> Morgan Kaufmann Publishers.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>563466</ref_obj_id>
				<ref_obj_pid>563340</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Parlente, N., Estell, J. K., Garcia, D., Levine, D. B., Reed, D., and Zelenski, J. 2002. Nifty assignments. In <i>Proceedings of the 33rd SIGCSE Technical Symposium on Computer Science Education</i>, ACM Press, D. Knox, Ed., ACM, 319--320.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242081</article_id>
		<sort_key>21</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[K-12 and industry partnering]]></title>
		<page_from>21</page_from>
		<page_to>22</page_to>
		<doi_number>10.1145/1242073.1242081</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242081</url>
		<abstract>
			<par><![CDATA[<p>Professional animators and digital effects artists and companies often work with high school teachers to explore relationships that can be developed and maintained between high schools and industry. The goals of this panel are to make more educators aware of the on-line and in person mentoring, sharing, and peer training that can occur in graphics, animation and related fields for 1) improving curricula and integrated technologies, 2) creating or furthering personal student interest or as a career vehicle and foundation, and 3) encouraging teachers to become involved in, and comfortable with, the new media.</p> <p>When Josh Spector wrote "Studios drawn to CGI features" for the <i>Hollywood Reporter</i> [January 2, 2002], he noted the studios#8482; big interest in animation and computer graphics based on Shrek, Monsters, Inc. and the realization that there were audiences for the non-Disney animated feature. Youth of all ages, as well as adults, not only enjoy CGI, but can also create it themselves with the current technology on home computers.</p> <p>Today's high school students, with the current technology available in home machines, are able to create products comparable to that of professionals, using computer technology that was very expensive just a few years ago. Although many students do not have access to the Internet or recent computer technology, those that do are producing programs for the Internet, their schools, and even, for sales. To gain and maintain the skill, one must practice. To give every child an even chance, the technology must be made available through education. Ideally, students would be given or lent computers with appropriate software for their classes, skills and academic or hobby interests.</p> <p>In order to do that, administrators and teachers today must first recognize the convergence of electronic media, information technology, communication, and entertainment. Then plans must be made or programs adapted to further the educational needs and foundations of the student (at any age).</p> <p>Therefore, it is imperative that teachers and administrators integrate 21st century technology into the curricula. However, we are sometimes afraid or hesitant to make changes we do not thoroughly understand, much less that are new to us. Many of today's educators are nearing retirement and there is a growing shortage of teachers. There is usually a shortage of funds in public education, too. Schools, therefore, often fall behind in the technology learning curve. New and exciting curriculum may be welcomed with open arms and with great desire but a lack of resources, combined with fear and trepidation by the very people who must learn and teach it, may negate its implementation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864647</person_id>
				<author_profile_id><![CDATA[81328491074]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Darlene]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wolfe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dr. Phillips High School]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864684</person_id>
				<author_profile_id><![CDATA[81328490274]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jeff]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Scheetz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The DAVE School]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864757</person_id>
				<author_profile_id><![CDATA[81328487827]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Roger]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cotton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The DAVE School]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864779</person_id>
				<author_profile_id><![CDATA[81328487710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Timothy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Comolli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[South Burlington High School]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31062584</person_id>
				<author_profile_id><![CDATA[81100196635]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stapleton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UCF Institute for Simulation and Training]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242082</article_id>
		<sort_key>23</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Animating art history for teaching]]></title>
		<page_from>23</page_from>
		<page_to>24</page_to>
		<doi_number>10.1145/1242073.1242082</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242082</url>
		<abstract>
			<par><![CDATA[<p>"Animating Art History for Teaching" is a creative learning methodology that incorporates new perspectives for introductory art history courses through digital technology created by computer animation and art history professors and their students.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[3D animation]]></kw>
			<kw><![CDATA[art history]]></kw>
			<kw><![CDATA[contrapposto]]></kw>
			<kw><![CDATA[digital media]]></kw>
			<kw><![CDATA[dome construction]]></kw>
			<kw><![CDATA[holy trinity]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864755</person_id>
				<author_profile_id><![CDATA[81100456869]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Roberta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tarbell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rutgers University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653614</person_id>
				<author_profile_id><![CDATA[81100314807]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[LiQin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rutgers University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hartt, F. 1993. <i>A History of Italian Renaissance Art</i>. 4&#60;sup&#62;th&#60;/sup&#62; Ed. Prentice-Hall.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>517500</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Maraffi, C. 2000. <i>Softimage XSI Character Animation f/x and Design</i>. Paperback]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Odoms, L. S. 2001. <i>Art Across Time</i>. 2&#60;sup&#62;nd&#60;/sup&#62; Ed. McGraw-Hill.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Rossano, A. and Arima S. 2000. <i>XSI Illuminated: Foundation 2</i>. Paperback]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Stokstad, M. 2002. <i>Art History</i>. Prentice-Hall, Inc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242083</article_id>
		<sort_key>25</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Problems with using components in educational software]]></title>
		<page_from>25</page_from>
		<page_to>29</page_to>
		<doi_number>10.1145/1242073.1242083</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242083</url>
		<abstract>
			<par><![CDATA[<p>Reuse is vital in the education world because the time and money necessary to create high quality educational software is prohibitive. Estimates for the cost of creating a single well designed, highly graphical and interactive online course in the commercial domain range from several hundred thousand dollars to a million or more. Thus the idea of reusable software components that can be easily shared is tremendously appealing. In fact, "component" has become a buzzword in the educational software community, with millions of dollars from the National Science Foundation and other sponsors funding a wide variety of "component-based" projects. But few, if any, of these projects, have approached the grand vision of creating repositories of easy to reuse components for developers and educators. This paper investigates some of the factors that stand in the way of achieving this goal.</p> <p>We begin by defining the word component and looking at several projects using components, with a focus on our Exploratories project at Brown University. We then discuss challenges in: Searching and Metadata, Quality Assurance, Programming in the University Environment, Platform and System Specificity, Social Issues, Intellectual Property Issues, and Critical Mass. We look at relevant software engineering issues and describe why we believe educational applications have unique factors that should be considered when using components.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[components]]></kw>
			<kw><![CDATA[education]]></kw>
			<kw><![CDATA[educational software]]></kw>
			<kw><![CDATA[reuse]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P19914</person_id>
				<author_profile_id><![CDATA[81100583889]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Anne]]></first_name>
				<middle_name><![CDATA[Morgan]]></middle_name>
				<last_name><![CDATA[Spalter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Agentsheets. http://www.agentsheets.com/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>141351</ref_obj_id>
				<ref_obj_pid>141344</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Agresti, W. and Evanco, W. 1992. Projecting Software Defects. In Analyzing Ada Designs in <i>IEEE Transactions on Software Engineering</i>, 18 {11}, pp. 988--997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bea Systems Success Story. http://www.componentsource.com/SellComponents/SuccessStories/BEASystems.asp.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Componentsource. http://www.componentsource.com/ and http://www.componentsource.com/Welcome/NewVisitor/WhyBuyFromComponentSource.asp for their testing statement.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Create (A Component Repository and Environment for Assembly of Teaching Environments) Project. http://www.smete.org/nsdl/projects/services.html#brwn]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[E-Slate. http://e-slate.cti.gr/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Escot (Educational Software Components of Tomorrow). http://www.escot.org/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Exploratories Project, http://www.cs.brown.edu/exploratories]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>379444</ref_obj_id>
				<ref_obj_pid>379437</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Laleuf, J. R. and Spalter, A. M. 2001. A Component Repository for Learning Objects: A Progress Report. In <i>Proceedings of the ACM/IEEE Joint Conference on Digital Libraries</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Merlot. http://www.merlot.org/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Nsf Nsdl (National Science Digital Library) Program. http://www.smete.org/nsdl/projects/index.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>301440</ref_obj_id>
				<ref_obj_pid>301353</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Poulin, J. 1999. Been There, Done That. In <i>CACM (Communications of the ACM)</i>, 42 {5}.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Sei (Software Engineering institute), Carnegie Mellon. A Framework for Software Product Line Practice -- Version 3.0, http://www.sei.cmu.edu/plp/framework.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1254524</ref_obj_id>
				<ref_obj_pid>1253530</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Spalter, A. M., Legrand, M., Taichi, S., and Simpson, R. M. 2000. Considering a Full Range of Teaching Techniques for Use in Interactive Educational Software: A Practical Guide and Brainstorming Session. In <i>Proceedings of IEEE Frontiers in Education '00</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>343134</ref_obj_id>
				<ref_obj_pid>343048</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Spalter, A. M., and Simpson, R. M. 2000. Integrating Interactive Computer-Based Learning Experiences Into Established Curricula. In <i>Proceedings of ACM ITiCSE</i> (Innovation and Technology in Computer Science Education) 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>273030</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Szyperski, C. 1999. <i>Component Software: Beyond Object-Oriented Programming</i>, Addison-Wesley, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Van Dam, A. 2000. Reflections on Next-Generation Educational Software. In <i>Enseigner L'Informatique: Melanges en Hommage 'a Bernard Levrat</i>. Pellegrini, C. and Jacquesson, A. editors, Georg Editeur, pp. 153--166. http://www.cs.brown.edu/people/avd/LevratPaper.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Web/Comp Project. diSessa, A. A. http://dewey.soe.berkeley.edu/boxer/webcomp/index.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242084</article_id>
		<sort_key>30</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[What do computers eat?]]></title>
		<subtitle><![CDATA[teaching beginners to think critically about technology and art]]></subtitle>
		<page_from>30</page_from>
		<page_to>32</page_to>
		<doi_number>10.1145/1242073.1242084</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242084</url>
		<abstract>
			<par><![CDATA[<p>This paper presents new curriculum for an introductory course in art and technology in which students compare the software industry with fast food to investigate patterns of consumption in our culture.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31063565</person_id>
				<author_profile_id><![CDATA[81328488802]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tiffany]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Holmes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[School of the Art Institute of Chicago]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>359530</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bardini, T. 2000. <i>Bootstrapping Douglas Engelbart, Coevolution, and the Origins of Personal Computing</i>. Stanford, CA: Stanford University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kaplan, D A. 2000. <i>The Silicon Boys and Their Valley of Dreams</i>. New York: Harper Perennial.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lasen, K. (editor). <i>Adbusters 'Zine</i>, Adbusters Media Foundation (http://adbusters.org).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Penny, S. 1995. Consumer Culture and the Technological Imperative: The Artist in Dataspace. <i>Critical Issues in Electronic Media</i>. SUNY Press. My students read an online version at: http://www-art.cfa.cmu.edu/penny/texts/Artist_in_D'space.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Schlosser, E. 2001. Why McDonald's Fries Taste So Good, excerpt from <i>Fast Food Nation</i>. Published in the <i>Atlantic Monthly</i>; January 2001; Volume 287, No. 1, pages 50--56. This piece is available online at: http://www.mathaba.net/data/whymcdonalds.htm.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242085</article_id>
		<sort_key>33</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[SIGGRAPH as textbook]]></title>
		<subtitle><![CDATA[learning skills for undergraduates]]></subtitle>
		<page_from>33</page_from>
		<page_to>36</page_to>
		<doi_number>10.1145/1242073.1242085</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242085</url>
		<abstract>
			<par><![CDATA[<p>This paper describes the design and implementation of a student-centered course in advanced undergraduate computer graphics. Instead of providing students with all necessary background before exposing them to recent research, students start with the latest SIGGRAPH proceedings and discover what topics they need to learn to understand them. The power of the group of students is exploited by having each student write a tutorial on one of the topics. Students then trade these written tutorials before being tested on the SIGGRAPH papers themselves. The course aims to nurture lifelong learning skills as well as an understanding of state of the art methods in computer graphics.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P161126</person_id>
				<author_profile_id><![CDATA[81100215225]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Novins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Auckland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383313</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baxter, B., Scheib, V., Lin, M. C., and Manocha, D. 2001. Dab: Interactive haptic painting with 3d virtual brushes. In <i>Computer Graphics Proceedings</i>, Annual Conference Series.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383315</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Cassell, J., Vilhj? Almsson, H. H., and Bickmore, T. 2001. Beat: The behavior expression animation toolkit. In <i>Computer Graphics Proceedings</i>, Annual Conference Series.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[CC2001. 2001. Computing curricula 2001. Tech. rep., Computer Society of the Institute for Electrical and Electronic Engineers (IEEE-CS) and the Association for Computing Machinery (ACM), December, http://www.computer.org/education/cc2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Donelan, M., and Wallace, J. 1998. Peer assisted learning: A truly cooperative initiative. In <i>Students Supporting Students</i>, J. Dolan and A. Castely, Eds., SEDA Paper 105. Staff and Educational Development Association, 11--22. http://www.ucl.ac.uk/epd/pal/SEDApaperl.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383290</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Noh, J., and Neumann, U. 2001. Expression cloning. In <i>Computer Graphics Proceedings</i>, Annual Conference Series.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Novins, K., and McCane, B. 2001. Incorporating primary source material into the undergraduate computer vision curriculum. <i>International Journal of Pattern Recognition and Artificial Intelligence</i> 15, 5, 775--787.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Novins, K., and Ming-Wong, S. L. 2001. A student centered approach to acquiring background knowledge in computer vision. In <i>Proceedings of the IEEE Workshop on Combined Research-Curriculum Development in Computer Vision</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Pigott, H. E., Fantuzzo, J. W., and Clement, P. W. 1986. The effects of reciprocal peer tutoring and group contingencies on the academic performance of elementary school children. <i>Journal of Applied Behavior and Analysis 19</i>, 93--98.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Ploetzner, R., Dellenbourg, P., Praier, M., and Traum, D. 1999. Learning by explaining to oneself and to others. In <i>Collaborative Learning: Cognitive and Computational Approaches</i>, P. Dillenbourg, Ed. Elsevier, Oxford, 103--121.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383288</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Sun, H. C., and Metaxas, D. N. 2001. Automating gait generation. In <i>Computer Graphics Proceedings</i>, Annual Conference Series.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Wills, C. E., Dermer, D., McCauley, R. A., and Null, L. 1997. Studying the use of peer learning in the introductory computer science curriculum. Tech. Rep. WPI-CS-TR-97-11, Computer Science Department, Worcester Polytechnic Institute, September.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242086</article_id>
		<sort_key>37</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Mathematics and geometry education with collaborative augmented reality]]></title>
		<page_from>37</page_from>
		<page_to>41</page_to>
		<doi_number>10.1145/1242073.1242086</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242086</url>
		<abstract>
			<par><![CDATA[<p>Construct3D is a three-dimensional geometric construction tool specifically designed for mathematics and geometry education. It is based on the mobile collaborative augmented reality system <i>"Studierstube."</i> We describe our efforts in developing a system for the improvement of spatial abilities and maximization of transfer of learning. In order to support various teacher-student interaction scenarios we implemented flexible methods for context and user dependent rendering of parts of the construction. Together with hybrid hardware setups they allow the use of Construct3D in today's classrooms and provide a test bed for future evaluations. Means of application and integration in mathematics and geometry education at the high school, as well as the university, level are being discussed. Anecdotal evidence supports our claim that Construct3D is easy to learn, encourages experimentation with geometric constructions, and improves spatial skills.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[augmented reality]]></kw>
			<kw><![CDATA[geometry education]]></kw>
			<kw><![CDATA[mathematics education]]></kw>
			<kw><![CDATA[spatial intelligence]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP17000005</person_id>
				<author_profile_id><![CDATA[81100002987]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hannes]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaufmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vienna University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P550213</person_id>
				<author_profile_id><![CDATA[81100091694]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dieter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schmalstieg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vienna University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Azuma, R. 1997. A Survey of Augmented Reality. <i>PRESENCE: Teleoperators and Virtual Environments</i>, Vol. 6, No. 4, pp. 355--385.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618862</ref_obj_id>
				<ref_obj_pid>616073</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Azuma, R., Baillot, Y., Behringer, R., Feiner, S., Julier, S., and MacIntyre, B. 2001. Recent Advances in Augmented Reality. <i>IEEE Computer Graphics & Applications</i>, Vol. 21, No. 6, pp. 34--47.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bell, J. T., and Fogler, H. S. 1995. The Investigation and Application of Virtual Reality as an Educational Tool. In <i>Proceedings of the American Society for Engineering Education</i> 1995 Annual Conference, Session number 2513, Anaheim, CA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bowman, D. A. 1996. Conceptual Design Space -- Beyond Walk-through to Immersive Design. In Bertol, D., <i>Designing Digital Space</i>, John Wiley & Sons, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Bricken, M., and Byrne, C. 1993. Summer Students in Virtual Reality: A Pilot Study on Educational Applications of VR Technology. In A. Wexelblat (Ed.) <i>Virtual Reality, Applications and Explorations</i>. Cambridge, MA: Academic Press Professional.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>238009</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Byrne, C. M. 1996. <i>Water on Tap: The Use of Virtual Reality as an Educational Tool</i>. Unpublished Ph.D. thesis, University of Washington, College of Engineering.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>260001</ref_obj_id>
				<ref_obj_pid>259963</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Chapin, W. L., Lacey, T. A. and Leifer, L. 1994. DesignSpace: A Manual Interaction Environment of Computer Aided Design. In <i>Proceedings of the ACM's SIGCHI 1994 Conference: CHI'94 Human Factors In Computing Systems</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1246913</ref_obj_id>
				<ref_obj_pid>1246908</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Durlach, N. I., Allen, G., Darken, R., Garnett, R. L., Loomis, J., Templeman, J., and Von Wiegand, T. E. 2000. Virtual Environments and the Enhancement of Spatial Behavior: Towards a Comprehensive Research Agenda. In <i>PRESENCE -Teleoperators and Virtual Environments</i>, Vol. 9, No. 6, pp. 593--615.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Gittler, G. and Gluck, J. 1998. Differential Transfer of Learning: Effects of Instruction in Descriptive Geometry on Spatial Test Performance. <i>Journal of Geometry and Graphics</i>, Volume 2, No. 1, 71--84, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Jackiw, N. 1995. The Geometer's Sketchpad Version 3. Key Curriculum Press, Berkeley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>593845</ref_obj_id>
				<ref_obj_pid>593722</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Kaufmann, H., Schmalstieg, D. and Wagner, M. 2000. Construct3D: A Virtual Reality Application for Mathematics and Geometry Education. <i>Education and Information Technologies</i> 5:4, pp. 263--276.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614948</ref_obj_id>
				<ref_obj_pid>614665</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Kiyokawa, K., Takemura, H., and Yokoya, N. 2000. SeamlessDesign for 3D Object Creation, <i>IEEE MultiMedia magazine</i>, 7 (1), pp. 22--33.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Laborde, J. M. and Bellemain, F. 1998. <i>Cabri-Geometry II</i>. Texas Instruments. Copyright Texas Instruments and Universite Joseph Fourier, CNRS. URL: http://www-cabri.imag.fr/index-e.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Larson, P., Rizzo, A. A., Buckwalter, J. G., Van Rooyen, A., Kratz, K., Neumann, U., Kesselman, C., Thiebaux, M. and Van Der Zaag, C. 1999. Gender Issues in the Use of Virtual Environments. In <i>CyberPsychology and Behavior</i>, 2(2).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Maier, P. H. 1994. <i>Raumliches Vorstellungsvermogen</i>. Peter Lang GmbH, Europaische Hochschulschriften: Reihe 6, Bd. 493, Frankfurt am Main.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Mantovani, F. 2001. VR Learning: Potential and Challenges for the Use of 3D Environments in Education and Training. In <i>Towards CyberPsychology: Mind</i>, Cognitions and Society in the Internet Age, Giuseppe Riva & Carlo Galimberti (Eds.), Amsterdam, IOS Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Mechling, R. 2000. Euklid Dynageo. http://www.dynageo.com]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>897884</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Mine, M. 1996. <i>Working in a Virtual World: Interaction Techniques Used in the Chapel Hill Immersive Modeling Program</i>. UNC Chapel Hill Computer Science Technical Report TR96-029.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Mine, M. 1997. ISAAC: A Meta-CAD System for Virtual Environments. <i>Computer-Aided Design</i>, 29(8).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[OpenCascade 4.0. 2001. Open-Source Toolkit for 3D modeling. URL: http://www.opencascade.com]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Osberg, K. 1997. Spatial Cognition in the Virtual Environment, <i>Technical R-97-18</i>. Seattle: Human Interface Technology Lab.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>881310</ref_obj_id>
				<ref_obj_pid>582828</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Reitmayr, G. and Schmalstieg, D. 2001. Mobile Collaborative Augmented Reality. In <i>Proceedings of the 2nd ACM/IEEE International Symposium on Augmented Reality</i> (ISAR'01), New York NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1198898</ref_obj_id>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Richter-Gebert, J. and Kortenkamp, U. H. 1999. <i>The Interactive Geometry Software Cinderella: Version 1.2</i> (Interactive Geometry on Computers). URL: http://www.cinderella.de/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Rizzo, A. A., Buchwalter, J. G., Neumann, U., Kesselman, C., Thiebaux, M., Larson, P., and Van Rooyen, A. 1998. The Virtual Reality Mental Rotation Spatial Skills Project. In <i>CyberPsychology and Behavior</i>, 1(2), pp. 113--120.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>641636</ref_obj_id>
				<ref_obj_pid>641633</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Schmalstieg, D., Fuhrmann, A., Hesina, G., Szalavari, Z., Encarnocao, M., Gervautz, M. and Purgathofer, W. 2002. The Studierstube Augmented Reality Project. In <i>PRESENCE-Teleoperators and Virtual Environments</i>, MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Szalavari, Z. and Gervautz, M. 1997. The Personal Interaction Panel -- A Two-Handed Interface for Augmented Reality, <i>Computer Graphics Forum</i>, 16, 3, pp. 335--346.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Taxen, G. and Naeve, A. 2001. CyberMath: Exploring Open Issues in VR-Based Learning. SIGGRAPH 2001 Educators Program. In <i>SIGGRAPH 2001 Conference Abstracts and Applications</i>, 49--51.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Winn, W. 1997. The Impact of Three-Dimensional Immersive Virtual Environments on Modern Pedagogy. <i>HITL Technical Report R-97-15</i>. Discussion paper for NSF Workshop. Human Interface Technology Laboratory, University of Washington.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242087</article_id>
		<sort_key>42</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Assignment]]></title>
		<subtitle><![CDATA[scene graphs in computer graphics courses]]></subtitle>
		<page_from>42</page_from>
		<page_to>45</page_to>
		<doi_number>10.1145/1242073.1242087</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242087</url>
		<abstract>
			<par><![CDATA[<p>A number of published papers recommend teaching scene graphs in the introductory computer graphics course [Bouvier 2002; Cunningham 1999; Hitchner and Sowizral 1999; Wolfe 1999]. However, little has been published concerning how to effectively use scene graphs in the introductory computer graphics course. This paper summarizes possible scene graphs exercises and teaching experience of the author.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P65610</person_id>
				<author_profile_id><![CDATA[81100014789]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dennis]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Bouvier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Saint Louis University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>261190</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ames, A., Nadeau, D. and Moreland, J. 1997. <i>VRML 2.0 Sourcebook</i>, Wiley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Baker S. 2002. SSG: A Simple Scene Graph Api for OpenGL, http://www.woodsoup.org/projs/plib/ssg/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bouvier, D. 2001. From Pixels to Scene Graphs: The Evolution of Computer Graphics Courses, <i>SIGGRAPH 2001 Conference Abstracts and Applications</i>, August 2001]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bouvier, D. 2002. From Pixels to Scene Graphs in Introductory Computer Graphics Courses, to appear <i>Computers and Graphics</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Brown, M. 1985. <i>Understanding PHIGS: The Hierarchical Computer Graphics Standard</i>, Template Software Division of Megatek Corporation.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Bresenham, J. 1999. Teaching the Graphics Processing Pipeline: Cosmetic and Geometric Attribute Implications, <i>Proceedings of Graphics and Visualization Education 1999</i>, July 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Cunningham, S. 1999. Re-Inventing the Introductory Computer Graphics Course: Providing Tools for a Wider Audience, <i>Proceedings of Graphics and Visualization Education 1999</i>, July 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>331809</ref_obj_id>
				<ref_obj_pid>330908</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Cunningham, S. 2000. Powers of 10: The Case for Changing the First Course in Computer Graphics, <i>The Proceedings of the Thirty-first Technical Symposium on Computer Science Education</i>, Austin, TX, March 2000, 46--49.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Cunningham, S. and Bailey, M. 2001. Lessons from Scene Graphs: Using Scene Graphs to Teach Hierarchical Modeling, <i>Computers & Graphics</i> 2001, number 4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>208249</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Foley, J. Et al 1996. <i>Computer Graphics: Principles and Practice in C</i>, 2/e, Addison Wesley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Hitchner, L. and Sowizral, H. 1999. Adapting Computer Graphics Curricula to Changes in Graphics, <i>Proceedings of Graphics and Visualization Education 1999</i>, July 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Open Inventor 2002. http://oss.sgi.com/projects/inventor/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[OpenRm 2002. OpenRm Project Overview http://openrm.sourceforge.net/overview.shtml.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Open Scene Graph 2002. http://openscenegraph.org.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[OpenSg 2002. http://opensg.org.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Performer 2002. <i>SGI-OpenGL Performer Overview</i>, http://www.sgi.com/software/performer/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Sowizral, H., Rushforth, K. and Deering, M. 1995. <i>The Java 3D#8482; 3D API Specification</i>, Addison-Wesley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Sun 2002. <i>Java 3D API home page</i>, http://java.sun.com/products/java-media/3D.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Web3D Consortium 2002. <i>X3D home page</i>, http://www.web3d.org/x3d/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Wernecke, J. 1994. <i>The Inventor Mentor</i>, Addison-Wesley, 1994]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Wolfe, R. 1999. Bringing the Introductory Computer Graphics Course into the 21st Century, <i>Proceedings of Graphics and Visualization Education 1999</i>, July 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242088</article_id>
		<sort_key>46</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Building computer graphics education in developing African countries]]></title>
		<page_from>46</page_from>
		<page_to>48</page_to>
		<doi_number>10.1145/1242073.1242088</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242088</url>
		<abstract>
			<par><![CDATA[<p>Computer graphics is becoming a tool for communication, change, and development throughout the world, and in order for this tool to be effective, there must be an educational base for its use. In southern Africa the development of computer graphics is underway, but is has not made the impact it could have. We believe that one of the main reasons for this is that there has not been as much educational development as is needed in the region. This note will describe some of the challenges that are faced by educational institutions in southern Africa as they try to develop the computer graphics education in the region, and will discuss some of the ways these institutions are working to meet these challenges.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864765</person_id>
				<author_profile_id><![CDATA[81328487651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sampson]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Asare]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Botswana]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864745</person_id>
				<author_profile_id><![CDATA[81100240264]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Petros]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Mashwama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Swaziland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39048689</person_id>
				<author_profile_id><![CDATA[81100562918]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cunningham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California State University, Stanislaus]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Afrigraph. 2001. Afrigraph organization and conference pages, retrieved March 11, 2002 from http://www.afrigraph.org/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Brown, J. 2001. Report on the Afrigraph conference, retrieved March 11, 2002 from http://www.siggraph.org/conferences/Reports/Afrigraph2001.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Chalmers, A. 2001. Report of the Acm Siggraph visit, retrieved March 11, 2002 from http://www.siggraph.org/project-grants/SA/sigsareport.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>331809</ref_obj_id>
				<ref_obj_pid>330908</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Cunningham, S. 2000. Powers of 10: The Case for Changing the First Course in Computer Graphics, <i>Proceedings</i> of the SIGCSE 2000 conference, Austin, TX, March 2000, pp. 293--296]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[McCormick, B., DeFanti, T, and Brown, M, eds. 1987. <i>Visualization in Scientific Computing, Computer Graphics</i> 21(6), November 1987]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242089</article_id>
		<sort_key>49</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Macromedia flash in physics education aspire's interactive online labs and lessons]]></title>
		<page_from>49</page_from>
		<page_to>51</page_to>
		<doi_number>10.1145/1242073.1242089</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242089</url>
		<abstract>
			<par><![CDATA[<p>Flash provides the ASPIRE team a means to create interactive virtual learning environments, where teachers and students can explore places and things previously limited by the classroom or simply by time constraints. Flash can become an immersive world for online science education. Producing curriculum support material in Flash provides many benefits for teachers and students.</p> <p>Teachers are provided with an affordable tool, which aligns to local and national curriculum standards. This material provides a rich classroom experience at little cost to the educator, while also satisfying the curriculum goals. Students can participate in an engaging learning experience, where science becomes an experience beyond a textbook lesson or a classroom lab. Situations are presented that would be either difficult to reproduce due to cost or scale. For example, students can investigate the causes of tides in the ASPIRE Flash activity "Gravity and Tides."</p> <p>Flash, the software available for authoring these activities, can produce a mathematically correct, physically based model that students can observe and investigate. These lessons provide a high-quality experience; meanwhile, the cost and speed of production of these activities continue to decrease.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864696</person_id>
				<author_profile_id><![CDATA[81328487839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Julie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Callahan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864631</person_id>
				<author_profile_id><![CDATA[81328488718]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Charles]]></first_name>
				<middle_name><![CDATA[C. H.]]></middle_name>
				<last_name><![CDATA[Jui]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Gentry, C. G. 1994. Introduction to <i>Instructional Development Process and Technique</i>. Wadsworth Publishing Co.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Roseman, J. Kesidou, S. Stern, L. and Caldwell, A. 1999. Heavy Books Light on Learning, AAAS Project 2061 Evaluates Middle Grades Science Textbooks. In <i>Science Books & Films</i> 35, 6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Utah state Office of Education 1995. <i>Earth Systems - 3600</i> - 1995. http://www.uen.org/cgi-bin/websql/lessons/c3.hts?core=3&course num=3600.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242090</article_id>
		<sort_key>52</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[The Vertex project]]></title>
		<subtitle><![CDATA[exploring the creative use of shared 3D virtual worlds in the primary (K-12) classroom]]></subtitle>
		<page_from>52</page_from>
		<page_to>54</page_to>
		<doi_number>10.1145/1242073.1242090</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242090</url>
		<abstract>
			<par><![CDATA[<p>Children invent imaginary worlds and enact scenarios within them on a daily basis as part of their imaginative play. Given the opportunity and the tools, what kind of worlds would children create for themselves within a virtual space, and what kind of learning can emerge within these playful, child-centered spaces?</p> <p>In VERTEX, young children inhabit an imaginary virtual world that they have designed and created using 3D modeling tools and net-based virtual worlds software. Crossing traditional subject disciplines and involving local and remote collaboration, the project demonstrates children's design and communication abilities above and beyond the expectations of the curriculum.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P336043</person_id>
				<author_profile_id><![CDATA[81100615814]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fiona]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bailey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Middlesex University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28021516</person_id>
				<author_profile_id><![CDATA[81100362429]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Magnus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Middlesex University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1150243</ref_obj_id>
				<ref_obj_pid>1150240</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bers, M. 1999. Zora: a Graphical Multi-user Environment to Share Stories about the Self. In <i>Proceedings of Computer Support for Collaborative Learning</i> (CSCL) 1999 pp 33--40. Lawrence Erlbaum Associates.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311724</ref_obj_id>
				<ref_obj_pid>311625</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Johnson, A., Moher, T., and Ohlsson, S. 1999. The Round Earth Project -- Collaborative VR for Elementary School Kids. In <i>Proceedings of ACM SIGGRAPH 1999 Conference Abstracts and Applications</i> 1999, pp. 90--93.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1095592</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Papert, S. 1980. <i>Mindstorms: Children Computers and Powerful Ideas</i>. Basic Books, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>262264</ref_obj_id>
				<ref_obj_pid>262171</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Roussos, M., Johnson, A.,; Leigh, J., Vasilakis, C., Barnes, C., and Moher, T. 1997. Nice. Combining Constructionism, Narrative and Collaboration in a Virtual Learning Environment. In <i>Proceedings of ACM SIGGRAPH 1997</i> Computer Graphics Volume 31 No.3 pp. 62--63.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Vygotsky, L. S. 1978. <i>Mind in Society: The development of higher psychological processes</i>. Cambridge, Mass: Harvard University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242091</article_id>
		<sort_key>55</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[Building an affordable projective, immersive display]]></title>
		<page_from>55</page_from>
		<page_to>55</page_to>
		<doi_number>10.1145/1242073.1242091</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242091</url>
		<abstract>
			<par><![CDATA[<p>Immersive, interactive virtual reality is a tool with hypothetically limitless uses. However, so far it has been put to serious use primarily in technical application areas such as computational science, automotive engineering, and chemical exploration. Groups working in these fields often have large budgets and can afford expensive, advanced displays. VR should also be of value to schools and museums, but most of them have much smaller budgets than major research labs, or are not able to support high-end graphics workstations. A simple, affordable, projection based display system can make VR far more accessible. In schools, displays could be put into individual classrooms and not just a central computer lab. In the museum world, small institutions would be capable of showing cutting edge digital work that previously has been restricted to a few large museums.</p> <p>This workshop describes the construction of a single screen, passive stereo, VR display based on commodity, or otherwise low-cost, components. There are many options available for the major elements of such a system and the basic system can be modified or adapted to many different styles of use. Figure 1 shows a photo of such a system in use at the University at Buffalo.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28021964</person_id>
				<author_profile_id><![CDATA[81100415993]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dave]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pape]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Res Umbrae]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14187573</person_id>
				<author_profile_id><![CDATA[81100539989]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Josephine]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Anstey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University at Buffalo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242092</article_id>
		<sort_key>56</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[Life drawing and 3D figure modeling with MAYA]]></title>
		<page_from>56</page_from>
		<page_to>56</page_to>
		<doi_number>10.1145/1242073.1242092</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242092</url>
		<abstract>
			<par><![CDATA[<p>This workshop, first given at SIGGRAPH 2001 [Garvey 2002] is now offered in two-parts. Part I introduces the process of transferring life drawings into 3D models using MAYA followed by a life drawing session. In Part II, workshop attendees working in the CAL import digitized life drawings into MAYA for setup as image planes to guide the modeling process. The goal of the workshop is to explore and develop skills of observational figure drawing and integrate them with the process of 3D modeling.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35023041</person_id>
				<author_profile_id><![CDATA[81100031851]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gregory]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Garvey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Quinnipiac University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Garvey, G. 2002. Life Drawing and 3D Figure Modeling with MAYA. <i>Leonardo 35</i>, 3. Excerpts reprinted courtesy of the MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>555685</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Harovas, P., Kundert-Gibbs, J. and Lee, P. 2000. <i>Mastering MAYA Complete 2</i>. SYBEX, Alameda, CA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Nicolaides, K. 1941. <i>The Natural Way to Draw</i>. Houghton Mifflin Company, Boston. p. 15.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242093</article_id>
		<sort_key>57</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[Integrating web 3D into 3D animation curricula]]></title>
		<page_from>57</page_from>
		<page_to>57</page_to>
		<doi_number>10.1145/1242073.1242093</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242093</url>
		<abstract>
			<par><![CDATA[<p>3D curriculum, once focused on modeling, rendering and animation, now includes web 3D and interactivity. But what options exist for new courses in web 3D? How in-depth are some web 3D tools and their learning curves? Should the curriculum expand to make 3D artists into programmers? What considerations should we give to the end-users' experience in viewing an online web 3D portfolio? This session examines how these issues are being addressed in teaching "Interactive Web 3D Media" at UC Berkeley, UC Irvine and UCLA Digital Arts and Entertainment Studies Extension. The session also provides hands-on experience with various tools and technologies.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28022160</person_id>
				<author_profile_id><![CDATA[81328490911]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mitch]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Williams]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[3D-Online]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242094</article_id>
		<sort_key>58</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[Hi tech - lo tech]]></title>
		<subtitle><![CDATA[K-12 science visualization]]></subtitle>
		<page_from>58</page_from>
		<page_to>58</page_to>
		<doi_number>10.1145/1242073.1242094</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242094</url>
		<abstract>
			<par><![CDATA[<p>Integrating multimedia applications in the classroom can be overwhelming. Grants may address the cost of computer hardware, but where can instructors find the time to explore available software? Many visualization programs are free or low cost, but students will not grasp the importance of what they are viewing without proper conceptual introduction. Furthermore, many K-12 instructors are now expected to teach topics, including basic chemistry concepts, in which they may lack proper training.</p> <p>The STArt! teaching Science Through Art program was developed to help teachers prepare for these educational challenges. Using an "Artist in Residence" format, workshops are developed in collaboration with participating teachers. Specifically, STArt! focuses on basic concepts addressed in the new California K-12 Science Content Standards. The program introduces molecular visualization software using narrative discussions, educational animation, and hands-on workshops using art materials and everyday objects. By exploring different learning modes, it makes basic science concepts more understandable to a broader audience. Furthermore, by collaborating with instructors within their classrooms, the program provides a creative resource for teachers in meeting the academic standards.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P421499</person_id>
				<author_profile_id><![CDATA[81100494520]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Susana]]></first_name>
				<middle_name><![CDATA[Maria]]></middle_name>
				<last_name><![CDATA[Halpine]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Candle Light Productions]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Egan, K. 1992. <i>Imagination in Teaching and Learning</i>, The Middle School Years. Univ. of Chicago Press, 67--89.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Glenn, J. and Commission. 2000. Before It's Too Late: A Report to the Nation from the National Commission on Mathematics and Science Teaching for the 21st Century, The Middle School Years. U.S. Dept. of Education, 7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>615036</ref_obj_id>
				<ref_obj_pid>614670</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Halpine, S. M. 2001. Molecular Visualization, a Microcosm of the E-Revolution. IEEE MultiMedia April-June, 4--7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Halpine, S. M. 2001. Science Visualization and Educational Animation at SIGGRAPH 2001: The Next Big Deal. <i>Animation World Magazine:</i> http://mag.awn.com/index.php3?Itype=pageone&article no=855.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1095592</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Papert, S. 1980. <i>Mindstorms, Children, Computers and Powerful Ideas</i>. Basic Books, Inc., 38--54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Rich, M. Mathmol K-12 math and science visualization website. http://www.nyu.edu/pages/mathmol.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Sadket, M. and Sadker, D. 1994. <i>Failing at Fairness: How Our Schools Cheat Girls</i>. Touchstone/Simon & Schuster, 123.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Silver, H. F., Strong, R. and Perini, M. J. 2000. <i>So Each May Learn, Integrating Learning Styles And Multiple Intelligences</i>. Association for Supervision and Curriculum Development.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242095</article_id>
		<sort_key>59</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Teaching human facial modeling through plaster face casting]]></title>
		<page_from>59</page_from>
		<page_to>59</page_to>
		<doi_number>10.1145/1242073.1242095</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242095</url>
		<abstract>
			<par><![CDATA[<p>As 3D student skills progress, sooner or later they begin work on accurately proportioned human models. While caricature design is often very forgiving in its realization, human designs need to be very close to "right on." This presents a tremendous challenge for 3D animation students, many of whom have had limited anatomy training.</p> <p>The traditional solution to source material has been to provide photographic references. The standard front and side shots provide a good starting point for students to work from but provide no information on appropriate polygon topology or details such as the curvature of the head between the eye and the temple. Yet, it is impractical to get a live model to provide 3D reference by sitting next to the student as he or she models. The difficulty of finding a good 3D reference for students to work from becomes the challenge.</p> <p>At the University of the Incarnate Word, we have begun using a traditional method of face casting. Traditionally, this technique is used to create plaster masks or molds upon which prosthetics can be constructed. The mold is reusable and can be constructed from plaster or even lighter cements.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28022454</person_id>
				<author_profile_id><![CDATA[81100079178]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Watkins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of the Incarnate Word]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	</section>
	<section>
		<section_id>1242096</section_id>
		<sort_key>62</sort_key>
		<section_seq_no>2</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Emerging technologies]]></section_title>
		<section_page_from>62</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP31063855</person_id>
				<author_profile_id><![CDATA[81332526549]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Senften]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SGI]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1242097</article_id>
		<sort_key>62</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Tomorrow's yesterday]]></title>
		<subtitle><![CDATA[mapping the E-Tech continuum]]></subtitle>
		<page_from>62</page_from>
		<page_to>62</page_to>
		<doi_number>10.1145/1242073.1242097</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242097</url>
		<abstract>
			<par><![CDATA[<p>In computer graphics, history frames the possible, imagination paints the impossible, and passion fills in the rest.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864692</person_id>
				<author_profile_id><![CDATA[81539134356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Fujii]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hewlett-Packard Company]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242098</article_id>
		<sort_key>63</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A new step-in-place locomotion interface for virtual environment with large display system]]></title>
		<page_from>63</page_from>
		<page_to>63</page_to>
		<doi_number>10.1145/1242073.1242098</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242098</url>
		<abstract>
			<par><![CDATA[<p>The project presents a new locomotion interface for virtual environment with large display system. Users will be able to direct and control the traveling in the VE by in-place stepping and turning actions. Using a turntable technology, Visual feedback is continuously provided though the use of screen of limited size.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P346627</person_id>
				<author_profile_id><![CDATA[81100371012]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Laroussi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bouguila]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP36030069</person_id>
				<author_profile_id><![CDATA[81100567066]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Makoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P434687</person_id>
				<author_profile_id><![CDATA[81100270860]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hasegawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864667</person_id>
				<author_profile_id><![CDATA[81328490002]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hashimoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naoki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28021910</person_id>
				<author_profile_id><![CDATA[81547954556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matsumoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864617</person_id>
				<author_profile_id><![CDATA[81540597556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Atsushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Toyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37034491</person_id>
				<author_profile_id><![CDATA[81328488195]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Jelel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ezzine]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ecole Nationale d'Ingenieurs de Tunis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864641</person_id>
				<author_profile_id><![CDATA[81328489379]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Dalel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maghrebi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ecole Nationale d'Ingenieurs de Tunis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242099</article_id>
		<sort_key>64</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[ARS BOX with palmist]]></title>
		<subtitle><![CDATA[advanced VR-system based on commodity hardware]]></subtitle>
		<page_from>64</page_from>
		<page_to>64</page_to>
		<doi_number>10.1145/1242073.1242099</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242099</url>
		<abstract>
			<par><![CDATA[<p>The ARS BOX is a projection-based (cave-like), PC-based VR system. It significantly reduces the time and money needed to develop and present Immersive Virtual Environment (IVE) applications while simultaneously expanding the options available compared to similar systems. A handheld PC serves as its interaction interface, making possible numerous innovative applications.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[commodity hardware]]></kw>
			<kw><![CDATA[device]]></kw>
			<kw><![CDATA[interface]]></kw>
			<kw><![CDATA[virtual environment]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>B.4.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405</concept_id>
				<concept_desc>CCS->Applied computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P864674</person_id>
				<author_profile_id><![CDATA[81310494188]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Horst]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[H&#246;rtner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ars Electronica, Futurelab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P773681</person_id>
				<author_profile_id><![CDATA[81310483773]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lindinger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ars Electronica, Futurelab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864754</person_id>
				<author_profile_id><![CDATA[81328490041]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Praxmarer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ars Electronica, Futurelab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864614</person_id>
				<author_profile_id><![CDATA[81328490259]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Andreas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Riedler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ars Electronica, Futurelab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242100</article_id>
		<sort_key>65</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Augmenting the reality with 3D sound sources]]></title>
		<page_from>65</page_from>
		<page_to>65</page_to>
		<doi_number>10.1145/1242073.1242100</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242100</url>
		<abstract>
			<par><![CDATA[<p>Augmented reality (AR) is not only a new type of computer entertainment, but it can also be used for serious applications. Because the user is directly involved in the virtual world, enhanced reality can be more engaging than traditional computer work. Especially for a better sound impression, AR could be a solution for many problems. The sound component is still missing in current AR applications, which combine live video and computer graphics to produce real-time visual effects.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP37034474</person_id>
				<author_profile_id><![CDATA[81100048383]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Haller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fachhochschule Hagenberg (MTD)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28020829</person_id>
				<author_profile_id><![CDATA[81328488238]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dobler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fachhochschule Hagenberg (MTD)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28021944</person_id>
				<author_profile_id><![CDATA[81328490619]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Philipp]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stampfl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fachhochschule Hagenberg (MTD)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242101</article_id>
		<sort_key>66</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Audio haptics]]></title>
		<page_from>66</page_from>
		<page_to>66</page_to>
		<doi_number>10.1145/1242073.1242101</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242101</url>
		<abstract>
			<par><![CDATA[<p>Audio Haptics is a rendering technique for auditory and haptic sensations. Sound and force are generated by using a physical model of virtual objects. We developed a software for real-time calculation of a physical model of virtual objects. A speaker is set at the grip of the haptic interface for spatial localization of the sound.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[physical model]]></kw>
			<kw><![CDATA[sound rendering haptics]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP40026665</person_id>
				<author_profile_id><![CDATA[81100382304]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P110718</person_id>
				<author_profile_id><![CDATA[81100255856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864671</person_id>
				<author_profile_id><![CDATA[81328488746]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hiromi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Igawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242102</article_id>
		<sort_key>67</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Block jam]]></title>
		<page_from>67</page_from>
		<page_to>67</page_to>
		<doi_number>10.1145/1242073.1242102</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242102</url>
		<abstract>
			<par><![CDATA[<p>Block Jam is a musical interface controlled by the arrangement of 24 tangible blocks. By positioning the blocks (Figure 1), musical phrases and sequences are created, allowing multiple users to play and collaborate.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[collaborative systems]]></kw>
			<kw><![CDATA[interactive music]]></kw>
			<kw><![CDATA[tangible interface]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28022042</person_id>
				<author_profile_id><![CDATA[81328489897]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Henry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Newton-Dunn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony CSL Interaction Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P743326</person_id>
				<author_profile_id><![CDATA[81100633327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Design Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35038538</person_id>
				<author_profile_id><![CDATA[81100256687]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gibson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Design Center Europe]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Poupyrev, I. "Augmented Groove: Collaborative Jamming in Augmented Reality." in SIGGRAPH 2000 Conference Abstracts and Applications, ACM Press, NY, p.77]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>274652</ref_obj_id>
				<ref_obj_pid>274644</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Gorbet, M., Orth, M., and Ishii, H. "Triangles: Tangible Interface for Manipulation and Exploration of Digital Information Topography." Proceedings of SIGCHI1998: ACM Press]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>503397</ref_obj_id>
				<ref_obj_pid>503376</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Jun Rekimoto. "SmartSkin: An Infrastructure for Freehand Manipulations on Interactive Surfaces." to be published in ACM SIGCHI2002]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242103</article_id>
		<sort_key>68</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Cyberarium knowledge fountain]]></title>
		<subtitle><![CDATA[center for really neat research]]></subtitle>
		<page_from>68</page_from>
		<page_to>68</page_to>
		<doi_number>10.1145/1242073.1242103</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242103</url>
		<abstract>
			<par><![CDATA[<p>Cyberarium knowledge fountain is a opportunity:</p> <p>&#8226; To demonstrate socially responsible applications of communication technology</p> <p>&#8226; To provide a social exchange at which multiple communities can explore unconventional applications of advanced technological concepts</p> <p>&#8226; To identify key areas where information technologies can be effectively applied to improve quality of life</p> <p>&#8226; To encourage young minds to explore science and think about how to make the world a better place</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31063945</person_id>
				<author_profile_id><![CDATA[81328490871]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dave]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Warner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864730</person_id>
				<author_profile_id><![CDATA[81328487841]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Carbone]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242104</article_id>
		<sort_key>69</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Distributed systems of self-reconfiguring robots]]></title>
		<page_from>69</page_from>
		<page_to>69</page_to>
		<doi_number>10.1145/1242073.1242104</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242104</url>
		<abstract>
			<par><![CDATA[<p>A robot designed for a single purpose can perform a specific task very well, but it may perform poorly on a different task, or in a different environment. This is acceptable if the environment is structured; however, if the task is in an unknown environment, then a robot with the ability to change shape to suit the environment and the required functionality will be more likely to succeed. We wish to create more versatile robots by using self-reconfiguration: hundreds of small modules will autonomously organize and reorganize as geometric structures to best fit the terrain on which the robot has to move, the shape of the object the robot has to manipulate, or the sensing needs for the given task. For example, a robot could synthesize a snake shape to travel through a narrow tunnel, and then morph into a six-legged insect to navigate on rough terrain upon exit.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P343879</person_id>
				<author_profile_id><![CDATA[81100547834]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zack]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Butler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP33031915</person_id>
				<author_profile_id><![CDATA[81100317526]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fitch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P159058</person_id>
				<author_profile_id><![CDATA[81100432590]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Keith]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kotay]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14213988</person_id>
				<author_profile_id><![CDATA[81100620553]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Daniela]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rus]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kotay, K., and Rus, D. 1999. Locomotion versatility through self-reconfiguration. <i>Robotics and Autonomous Systems 26</i>, 217--232.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>591839</ref_obj_id>
				<ref_obj_pid>591547</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Rus, D., and Chirikjian, G. 2001. Special issue on self-reconfigurable robots. <i>Autonomous Robots 10</i>, 1, 1--124.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>504752</ref_obj_id>
				<ref_obj_pid>504729</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Rus, D., Butler, Z., Kotay, K., and Vona, M. 2002. Self-reconfiguring robots. Communications of the ACM 45, 3, 39--45.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242105</article_id>
		<sort_key>70</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Focus plus context screens]]></title>
		<subtitle><![CDATA[visual context and immersion on the desktop]]></subtitle>
		<page_from>70</page_from>
		<page_to>70</page_to>
		<doi_number>10.1145/1242073.1242105</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242105</url>
		<abstract>
			<par><![CDATA[<p>Focus plus context screens are wall-size, low-resolution displays with an embedded, high-resolution display region. Focus plus context screens allow users to view details of a document up close, while simultaneously seeing peripheral parts of the document in lower resolution. Application areas range from geographic information systems to interactive simulations and games.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14058749</person_id>
				<author_profile_id><![CDATA[81100137268]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baudisch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xerox PARC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28030654</person_id>
				<author_profile_id><![CDATA[81423596141]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Nathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Good]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xerox PARC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>503423</ref_obj_id>
				<ref_obj_pid>503376</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baudisch, P., Good, N., Bellotti, V., Schraedley, P.; Keeping Things in Context: A Comparative Evaluation of Focus Plus Context Screens, Overviews, and Zooming, to appear in <i>Proc. of CHI'02</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>502354</ref_obj_id>
				<ref_obj_pid>502348</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Baudisch, P., Good, N., Stewart, P. Focus plus context screens: Combining display technology with visualization techniques, <i>Proc. of UIST '01</i>, p. 31--40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618741</ref_obj_id>
				<ref_obj_pid>616065</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Hereld, M., Judson, I., Stevens, R., Introduction to Building Projection-based Tiled Display Systems <i>IEEE Computer Graphics & Applications</i> 20(4), 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>836018</ref_obj_id>
				<ref_obj_pid>527216</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Yoshida, A., Rolland, J. & Reif, J. (1995). Design and applications of a high resolution insert head-mounted-display, Virtual Reality Annual International Symposium '95; 84--93.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242106</article_id>
		<sort_key>71</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Immersive and interactive rear-projected stereo DLP reality center]]></title>
		<page_from>71</page_from>
		<page_to>71</page_to>
		<doi_number>10.1145/1242073.1242106</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242106</url>
		<abstract>
			<par><![CDATA[<p>The Immersive and Interactive Rear-Projected Stereo DLP Reality Center aims to improve the quality of immersive visualization and increase the efficacy of seamless real-time interaction with complex stereoscopic data.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864615</person_id>
				<author_profile_id><![CDATA[81328488700]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Joel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[BARCO Simulation Products, Xenia, OH]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37034464</person_id>
				<author_profile_id><![CDATA[81328487835]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brown]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keith Tushingham, Schlumberger, Malte Zoeckler, Indeed]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864613</person_id>
				<author_profile_id><![CDATA[81328488174]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Almos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Elekes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keith Tushingham, Schlumberger, Malte Zoeckler, Indeed]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242107</article_id>
		<sort_key>72</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Lewis the robotic photographer]]></title>
		<page_from>72</page_from>
		<page_to>72</page_to>
		<doi_number>10.1145/1242073.1242107</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242107</url>
		<abstract>
			<par><![CDATA[<p>Lewis is a (short) human-sized mobile robot that wanders through crowded rooms, taking pictures of people, much like a photographer at a wedding reception does. The goal is take high-quality, well-composed photographs of people non-intrusively, and to offer these pictures as keep-sakes of the conference.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P48640</person_id>
				<author_profile_id><![CDATA[81100553787]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cindy]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Grimm]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis, St. Louis, MO, United States]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P300012</person_id>
				<author_profile_id><![CDATA[81100105515]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Smart]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis, St. Louis, MO, United States]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Schulte, J., Rosenberg, C., and Thrun, S. 1999. Spontaneous short-term interaction with mobile robots in public places. In <i>Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Thrun, S., Beetz, M., Bennewitz, M., Burgard, W., Cremers, A. B., Dellaert, F., Fox, D., H&#228;hnel, D., Rosenberg, C., Roy, N., Schulte, J., and Schulz, D. 2000. Probabilistic algorithms and the interactive museum tour-guide robot Minerva. <i>International Journal of Robotics Research 19</i>, 11, 972--999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242108</article_id>
		<sort_key>73</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[NONA-vision]]></title>
		<page_from>73</page_from>
		<page_to>73</page_to>
		<doi_number>10.1145/1242073.1242108</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242108</url>
		<abstract>
			<par><![CDATA[<p>The NONA-Vision is a high-resolution, wide-angle video capture and projection system. The display is composed of nine rear-projection screens. Images for the nine screens are captured by a specialized camera-head, in which the optical centers of nine video cameras are located at the identical position.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[immersive projection display]]></kw>
			<kw><![CDATA[telepresence]]></kw>
			<kw><![CDATA[video image]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP40026665</person_id>
				<author_profile_id><![CDATA[81100382304]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P110718</person_id>
				<author_profile_id><![CDATA[81100255856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864736</person_id>
				<author_profile_id><![CDATA[81328490606]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Motohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsuzuki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14030322</person_id>
				<author_profile_id><![CDATA[81100054772]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Fumitaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakaizumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864775</person_id>
				<author_profile_id><![CDATA[81328491216]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Takayuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshioka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864798</person_id>
				<author_profile_id><![CDATA[81328489349]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Yutaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miyakita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242109</article_id>
		<sort_key>74</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Occlusive optical see-through displays in a collaborative setup]]></title>
		<page_from>74</page_from>
		<page_to>74</page_to>
		<doi_number>10.1145/1242073.1242109</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242109</url>
		<abstract>
			<par><![CDATA[<p>There are several approaches to realize a 3D display that can be viewed by multiple co-located users, and each has its own pros and cons. A volumetric display utilizes afterimage effects, and it usually can only show transparent images within its volume. Some projection-based systems support independent viewpoints for more than two users [1], however, virtual objects can be shown only within a viewing frustum in front of the screen. On the other hand, head mount display (HMD) based Augmented Reality (AR) can show virtual objects at arbitrary locations [2]. Besides, an HMD can potentially show correct occlusion phenomena between virtual and real scenes. While a video see-through HMD severely degrades the quality of the real scene and adds a certain system delay, an optical see-through HMD keeps the intrinsic quality of the real scene. However, a virtual scene had to be a semi-transparent ghost due to a half-silvered optical combiner so far.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP36037213</person_id>
				<author_profile_id><![CDATA[81100605685]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kiyoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kiyokawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Communications Research Lab.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28021706</person_id>
				<author_profile_id><![CDATA[81540773656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ohno]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Communications Research Lab.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864794</person_id>
				<author_profile_id><![CDATA[81328489295]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yoshinori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kurata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Topcon Co.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>618863</ref_obj_id>
				<ref_obj_pid>616073</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bimber, O., Frohlich, B., Schmalstieg, D., and Encarnacao, L. M., "The Virtual Showcase," <i>IEEE Computer Graphics & Applications</i>, vol. 21, no.6, pp. 48--55, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Schmalsteig, D., Fuhrmann, A., Szalavari, Z., Gervautz, M., "Studierstube -- An Environment for Collaboration in Augmented Reality," <i>CVE '96 Workshop Proceedings</i>, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kiyokawa, K., Kurata, Y., and Ohno, H., "An Optical See-through Display for Mutual Occlusion with a Real-time Stereo Vision System," <i>Elsevier Computer & Graphics, Special Issue on "Mixed Realities --- Beyond Conventions"</i>, Vol.25, No.5, pp.2765--779, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242110</article_id>
		<sort_key>75</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Physiological reaction and presence in stressful virtual environments]]></title>
		<page_from>75</page_from>
		<page_to>75</page_to>
		<doi_number>10.1145/1242073.1242110</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242110</url>
		<abstract>
			<par><![CDATA[<p>A common metric of VE quality is presence --- the degree to which the user feels like they are <i>in</i> the virtual scene as opposed to the real world. Presence is important for many VE applications [Hodges <i>et al.</i> 1994]. Since presence is a subjective condition, it is most commonly measured by self-reporting, either during the VE experience or immediately afterwards by questionnaires. There is vigorous debate in the literature as to how to best measure presence [Meehan 2001].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39037855</person_id>
				<author_profile_id><![CDATA[81100327214]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Meehan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina, Chapel Hill and Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P193375</person_id>
				<author_profile_id><![CDATA[81100122627]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mary]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Whitton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P346641</person_id>
				<author_profile_id><![CDATA[81100150932]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sharif]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Razzaque]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P552024</person_id>
				<author_profile_id><![CDATA[81100019926]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zimmons]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382422</person_id>
				<author_profile_id><![CDATA[81100140336]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Brent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Insko]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864664</person_id>
				<author_profile_id><![CDATA[81540220556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Combe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864621</person_id>
				<author_profile_id><![CDATA[81100447782]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Ben]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lok]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28014695</person_id>
				<author_profile_id><![CDATA[81100098694]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Thorsten]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Scheuermann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31063717</person_id>
				<author_profile_id><![CDATA[81100223903]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Samir]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28021123</person_id>
				<author_profile_id><![CDATA[81328488809]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jerald]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39028112</person_id>
				<author_profile_id><![CDATA[81100116782]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>11</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Harris]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28020484</person_id>
				<author_profile_id><![CDATA[81324487448]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>12</seq_no>
				<first_name><![CDATA[Angus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Antley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P86821</person_id>
				<author_profile_id><![CDATA[81100077256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>13</seq_no>
				<first_name><![CDATA[Frederick]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Brooks]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>933179</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M. Meehan (2001). Physiological reaction as an objective measure of presence in virtual environments. Doctoral Dissertation. Computer Science. University of North Carolina, Chapel Hill, NC, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1242110</ref_obj_id>
				<ref_obj_pid>1242073</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[M. Meehan, B. Insko, M. Whitton, F. P. Brooks (2002). Physiological Measures of Presence in Stressful Virtual Environments. In Proc. of ACM SIGGRAPH 2002. ACM Press/ACM SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Hodges, L., B. Rothbaum, R. Kooper, D. Opdyke, J. Willford, T. Meyer and M. North (1994). Presence as the defining factor in a VR application. Technical Report GVU-94-06. Georgia Tech University, Graphics, Visualization, and Usability Center.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242111</article_id>
		<sort_key>76</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Public anemone]]></title>
		<subtitle><![CDATA[an organic robot creature]]></subtitle>
		<page_from>76</page_from>
		<page_to>76</page_to>
		<doi_number>10.1145/1242073.1242111</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242111</url>
		<abstract>
			<par><![CDATA[<p>We have created an articulated robotic creature and situated it within an interactive terrarium to explore the aesthetic, expressive, and interactive qualities that give robots an organic and engaging presence to people.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[autonomous robotics]]></kw>
			<kw><![CDATA[interactive characters]]></kw>
			<kw><![CDATA[organic robotics]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39034833</person_id>
				<author_profile_id><![CDATA[81100258451]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cynthia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Breazeal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28023112</person_id>
				<author_profile_id><![CDATA[81540234256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brooks]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P527589</person_id>
				<author_profile_id><![CDATA[81100618234]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hancher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864695</person_id>
				<author_profile_id><![CDATA[81100131241]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Josh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Strickon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864635</person_id>
				<author_profile_id><![CDATA[81100434347]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Cory]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kidd]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P527587</person_id>
				<author_profile_id><![CDATA[81100572473]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McBean]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P527585</person_id>
				<author_profile_id><![CDATA[81100235266]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stiehl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>515422</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Breazeal, C. 2002. <i>Designing Sociable Robots</i>. The MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Burke, R., Isla, D., Downie, M., Ivanov, Y., and Blumberg, B. 2001. Creature smarts: The art and architecture of a virtual brain. In <i>Proceedings of the Game Developers Conference</i>, 147--166.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242112</article_id>
		<sort_key>77</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Regeneration of real objects in the real world]]></title>
		<page_from>77</page_from>
		<page_to>77</page_to>
		<doi_number>10.1145/1242073.1242112</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242112</url>
		<abstract>
			<par><![CDATA[<p>The vision of this work is to regenerate real objects that had existed at some moment in the past and/or at some remote location as if they have been transferred to the present across space and time. The objects could be museum pieces or items in stores, for example. For this work, we have developed a quick and fully automated system that can capture a three-dimensional image of real objects. This success brought us close to our goal of "regeneration of real objects in the real world."</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864673</person_id>
				<author_profile_id><![CDATA[81328489467]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matsuoka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Lifestyle and Environmental Technology Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP311047700</person_id>
				<author_profile_id><![CDATA[81542686156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Akira]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Onozawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Lifestyle and Environmental Technology Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14196039</person_id>
				<author_profile_id><![CDATA[81100565553]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hidenori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Lifestyle and Environmental Technology Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P111234</person_id>
				<author_profile_id><![CDATA[81100066409]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hisao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nojima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Lifestyle and Environmental Technology Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Schmalstieg, D., Billinghurst, M., Azuma, R., Hollerer, T., Kato, H., and Poupyrev, I. 2001. "Augmented Reality: The Interface is Everywhere." <i>Course Notes of SIGGRAPH 2001</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242113</article_id>
		<sort_key>78</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[SmartFinger]]></title>
		<subtitle><![CDATA[nail-mounted tactile display]]></subtitle>
		<page_from>78</page_from>
		<page_to>78</page_to>
		<doi_number>10.1145/1242073.1242113</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242113</url>
		<abstract>
			<par><![CDATA[<p>"Smart Finger" is a novel type of tactile display for Augmented Reality (AR) that is wearable, like a press-on fingernail. This device allows the user to feel various textures while tracing his or her fingers along smooth objects. This wearable AR interface can supplement bump mapping information to real objects.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[SmartTool]]></kw>
			<kw><![CDATA[bump mapping]]></kw>
			<kw><![CDATA[parasitic humanoid]]></kw>
			<kw><![CDATA[tactile display]]></kw>
			<kw><![CDATA[wearable device]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P715176</person_id>
				<author_profile_id><![CDATA[81100486051]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hideyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ando]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Science and Technology Corporation (JST)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864776</person_id>
				<author_profile_id><![CDATA[81328489468]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40033081</person_id>
				<author_profile_id><![CDATA[81100424140]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Masahiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P278536</person_id>
				<author_profile_id><![CDATA[81100065999]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Taro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maeda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo / JST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[T. Nojima, M. Inami, Y Kawabuchi, T. Maeda, K. Mabuchi and S. Tachi, "An Interface for Touching the Interface", ACM SIGGRAPH 2001 Conference Abstracts and Applications, p.125, 2001]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Mascaro S. and Asada H., 2001, "Photoplethysmograph Fingernail Sensors for Measuring Finger Forces Without Haptic Obstruction," IEEE Transactions on Robotics and Automation, Vol. 17, No. 5, pp. 698--708]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[T. Maeda, H. Ando, J. Watanabe, Y. Nonura and T. Miki, "A behavior modeling with wearable robotics "The study of Parasitic Humanoid (VI)", The 6th VRSJ Annual Conference, pp. 153--154 (Japanese)]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242114</article_id>
		<sort_key>79</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[The interactive window]]></title>
		<page_from>79</page_from>
		<page_to>79</page_to>
		<doi_number>10.1145/1242073.1242114</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242114</url>
		<abstract>
			<par><![CDATA[<p>We have developed a very simple retrofit to a large display surface that enables knocks or taps to be located and characterized (e.g., determining type of hit --- metallic tap, knuckle tap, or bash --- and intensity) in real time. We do this by analyzing the waveforms captured by 4 piezoelectric transducers (one mounted in each corner of the surface) and a dynamic microphone (mounted anywhere on the glass) in a digital signal processor. Differential timing yields the position, frequency content infers the kind of hit, and peak amplitude reflects the intensity. This technique was first explored in collaboration between Paradiso and Ishii [Ishii et. al. 1999] to make an interactive ping-pong table. Moving to glass display surfaces introduced significant problems, however --- knuckle taps are low-frequency impulses that vary considerably hit-to-hit, and the bending waves propagating through the glass are highly dispersive. A heuristically-guided cross-correlation algorithm [Paradiso et al. 2002] was developed to counteract these effects and provide spatial measurements that can resolve knuckle impacts to within &sigma; = 2-4 cm (depending on the material thickness) across a 2-meter sheet of glass. As the requisite hardware is minimal, and everything is mounted on the inside sheet of glass, this is a very simple retrofit to, for example, store window displays, ushering in an entirely new concept of interactive window browsing, where passers-by can interact with information on the store's products by simply knocking. We have explored this concept in retail, where one of our trackers was installed on the main display window of an American Greetings store near Rockefeller Center in Manhattan for this year's Christmas-Valentine's Day season (right figure), and in museums (e.g., left figure, which shows the system running at the Ars Electronica Center in Linz, Austria).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP15021952</person_id>
				<author_profile_id><![CDATA[81100099789]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Joseph]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Paradiso]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P344900</person_id>
				<author_profile_id><![CDATA[81100206343]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Che]]></first_name>
				<middle_name><![CDATA[King]]></middle_name>
				<last_name><![CDATA[Leo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP311038700</person_id>
				<author_profile_id><![CDATA[81545558256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nicholas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP30028746</person_id>
				<author_profile_id><![CDATA[81100369938]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Downie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>303115</ref_obj_id>
				<ref_obj_pid>302979</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ishii, H., <i>et al.</i>, 1999. PingPongPlus: Design of an Athletic-Tangible Interface for Computer-Supported Cooperative Play. In <i>Proceedings of the Conference on Human Factors in Computing Systems (CHI '99)</i>. ACM Press, 1999. 394--401.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Paradiso, J., <i>et al.</i>, 2002. Passive Acoustic Sensing for Tracking Knocks Atop Large Interactive Displays. To appear in the <i>Proceedings of the 2002 IEEE Sensors Conference</i>, June 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1120391</ref_obj_id>
				<ref_obj_pid>1120212</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Paradiso, J., <i>et al.</i>, 1997. The Magic Carpet: Physical Sensing for Immersive Environments. In the <i>Proceedings of the Conference on Human Factors in Computing Systems (CHI '97), Extended Abstracts</i>. ACM Press, 1997. 277--278.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242115</article_id>
		<sort_key>80</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[The virtual showcase]]></title>
		<subtitle><![CDATA[a projection-based multi-user augmented reality display]]></subtitle>
		<page_from>80</page_from>
		<page_to>80</page_to>
		<doi_number>10.1145/1242073.1242115</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242115</url>
		<abstract>
			<par><![CDATA[<p>The Virtual Showcase is a new projection-based and application-specific Augmented Reality display that offers an innovative way of accessing, presenting, and interacting with scientific and cultural content. Conceptually, the Virtual Showcase is compatible with the conventional showcases used, for instance, by museums. However, it allows the display of computer generated 3D graphics and animations together with real artifacts within the same space. From the technological point of view, the Virtual Showcase provides perspective correct stereoscopic viewing for multiple users, high resolution, low parallax (reflected projection plane inside the showcase), and support for mutual occlusion between real and virtual objects.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP40029142</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer Center for Research in Computer Graphics, Providence, RI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35025232</person_id>
				<author_profile_id><![CDATA[81100162399]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bernd]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fr&#246;hlich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus University Weimar, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P550213</person_id>
				<author_profile_id><![CDATA[81100091694]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Dieter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schmalstieg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vienna University of Technology, Austria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P165553</person_id>
				<author_profile_id><![CDATA[81100063439]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[L.]]></first_name>
				<middle_name><![CDATA[Miguel]]></middle_name>
				<last_name><![CDATA[Encarna&#231;&#227;o]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer Center for Research in Computer Graphics, Providence, RI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>618863</ref_obj_id>
				<ref_obj_pid>616073</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bimber, O., Fr&#246;hlich, B., Schmalstieg, D., and Encarna&#231;&#227;o, L. M. "The Virtual Showcase." <i>IEEE Computer Graphics & Applications</i>, vol. 21, no.6, pp. 48--55, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242116</article_id>
		<sort_key>81</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[TWISTER]]></title>
		<subtitle><![CDATA[a media booth]]></subtitle>
		<page_from>81</page_from>
		<page_to>81</page_to>
		<doi_number>10.1145/1242073.1242116</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242116</url>
		<abstract>
			<par><![CDATA[<p>TWISTER, (Telexistence Wide-angle Immersive STERe-oscope) is an immersive full-color autostereoscopic display, designed for a face-to-face telecommunication system called "mutual telexistence", where people in distant locations can communicate as if they were in the same virtual three dimensional space.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28014948</person_id>
				<author_profile_id><![CDATA[81100134429]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kenji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P681641</person_id>
				<author_profile_id><![CDATA[81100069321]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Junya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hayashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31062250</person_id>
				<author_profile_id><![CDATA[81328489108]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yutaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kunita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40033081</person_id>
				<author_profile_id><![CDATA[81100424140]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Masahiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P278536</person_id>
				<author_profile_id><![CDATA[81100065999]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Taro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maeda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P272463</person_id>
				<author_profile_id><![CDATA[81100411569]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Susumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>312168</ref_obj_id>
				<ref_obj_pid>311625</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kunita, Y., Inami, M., Maeda, T., and Tachi, S. 1999. Prototype system of mutual tele-existence. In <i>ACM SIGGRAPH '99 Conference Abstracts and Applications</i>, 267.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tachi, S., Maeda, T., Yanagida, Y., Koyanagi, M., and Yokoyama, H. 1996. A method of mutual tele-existence in a virtual environment. In <i>Proceedings of the ICAT'96</i>, 9--18.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242117</article_id>
		<sort_key>82</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Ultrasound visualization with the sonic flashlight]]></title>
		<page_from>82</page_from>
		<page_to>82</page_to>
		<doi_number>10.1145/1242073.1242117</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242117</url>
		<abstract>
			<par><![CDATA[<p>From the discovery of X-rays over a century ago, clinicians have been presented with a wide assortment of imaging modalities yielding maps of localized structure and function within the patient. Some imaging modalities are <i>tomographic</i>, meaning that the data are localized into voxels, rather than projected along lines of sight as with conventional X-ray images. Tomographic modalities include magnetic resonance (MR), computerized tomography (CT), ultrasound, and others. Tomographic images, with their spatially distinct voxels, are essential to our present work.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P749411</person_id>
				<author_profile_id><![CDATA[81100100690]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Damion]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shelton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP45025986</person_id>
				<author_profile_id><![CDATA[81339530484]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stetten]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Pittsburgh / Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864792</person_id>
				<author_profile_id><![CDATA[81328487885]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Wilson]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Pittsburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242118</article_id>
		<sort_key>83</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[Virtual Chanbara]]></title>
		<page_from>83</page_from>
		<page_to>83</page_to>
		<doi_number>10.1145/1242073.1242118</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242118</url>
		<abstract>
			<par><![CDATA[<p>Chanbara is the name of our game, which reproduces the original ancient art of "samurai sword fighting". For this work, we created a new force feedback device called "GEK12". The player holds the sword and wears an HMD. In Chanbara, the player walks around a virtual arena to battle against CG characters.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Chanbara]]></kw>
			<kw><![CDATA[force display]]></kw>
			<kw><![CDATA[virtual reality]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864639</person_id>
				<author_profile_id><![CDATA[81328489305]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daijiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864774</person_id>
				<author_profile_id><![CDATA[81328488884]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Itagaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	</section>
	<section>
		<section_id>1242119</section_id>
		<sort_key>86</sort_key>
		<section_seq_no>3</section_seq_no>
		<section_type>PANEL SESSION</section_type>
		<section_title><![CDATA[Panels]]></section_title>
		<section_page_from>86</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP39067025</person_id>
				<author_profile_id><![CDATA[81100396860]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shaw]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1242120</article_id>
		<sort_key>86</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[When will ray-tracing replace rasterization?]]></title>
		<page_from>86</page_from>
		<page_to>87</page_to>
		<doi_number>10.1145/1242073.1242120</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242120</url>
		<abstract>
			<par><![CDATA[<p>Ray-tracing produces images of stunning quality but is difficult to make interactive. Rasterization is fast but making realistic images with it requires splicing many different algorithms together. Both GPU and CPU hardware grow faster each year. Increased GPU performance facilitates new techniques for interactive realism, including high polygon counts, multipass rendering, and texture-intensive techniques such as bumpmapping and shadows. On the other hand, increased CPU performance and dedicated ray-tracing hardware push the potential framerate of ray-tracing ever higher.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39048699</person_id>
				<author_profile_id><![CDATA[81100563035]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kurt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akeley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39066840</person_id>
				<author_profile_id><![CDATA[81100166914]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kirk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31063823</person_id>
				<author_profile_id><![CDATA[81100625268]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Larry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seiler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATI Research, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40024383</person_id>
				<author_profile_id><![CDATA[81100159926]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Philipp]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Slusallek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Saarland University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31062084</person_id>
				<author_profile_id><![CDATA[81321492040]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Brad]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grantham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Silicon Graphics Incorporated]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242121</article_id>
		<sort_key>88</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Digital humans]]></title>
		<subtitle><![CDATA[what roles will they play?]]></subtitle>
		<page_from>88</page_from>
		<page_to>89</page_to>
		<doi_number>10.1145/1242073.1242121</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242121</url>
		<abstract>
			<par><![CDATA[<p>Computer graphics technology has progressed to the point where it is possible to create digital humans that are virtually indistinguishable from the real items. The potential benefits are immense, but there are implications to consider as well, in a range of applications that includes film, video, the Web, and gaming. This panel of experts from diverse disciplines of computer graphics will discuss how far we have come in the use of digital humans, where they are heading, and what they will mean to us.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP15038212</person_id>
				<author_profile_id><![CDATA[81452608047]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Norman]]></first_name>
				<middle_name><![CDATA[I.]]></middle_name>
				<last_name><![CDATA[Badler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Pennsylvania]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864737</person_id>
				<author_profile_id><![CDATA[81328489620]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Nadia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Magenat-Thalmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Geneva]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864715</person_id>
				<author_profile_id><![CDATA[81328489376]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Laurie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McCulloch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Animation Group]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864658</person_id>
				<author_profile_id><![CDATA[81335491561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Evan]]></first_name>
				<middle_name><![CDATA[Marc]]></middle_name>
				<last_name><![CDATA[Hirsch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864746</person_id>
				<author_profile_id><![CDATA[81100575149]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Phil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[LoPiccolo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics World]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242122</article_id>
		<sort_key>90</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Extending interface practice]]></title>
		<subtitle><![CDATA[an ecosystem approach]]></subtitle>
		<page_from>90</page_from>
		<page_to>92</page_to>
		<doi_number>10.1145/1242073.1242122</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242122</url>
		<abstract>
			<par><![CDATA[<p>Interface ecology is an emerging metadisciplinary approach, in which the creation of rich interactive experiences spans n disciplines --- such as computer graphics, mathematics, gaming, visual art, performance, and cultural theory. Interfaces extend beyond interactive artifacts, activities, and social spaces, forming intricate ecosystems. Interfaces are the catalytic border zones where systems of representation meet, mix, and recombine. Through this recombination, interface ecosystems generate fundamental innovations of form, experience, knowledge, and technology. This panel brings together a diverse range of practitioners who work from concept to experience not in terms of a particular discipline, m&#233;tier, or medium but with a practice that interconnects multiple systems, forming a whole.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[interface]]></kw>
			<kw><![CDATA[interface ecology]]></kw>
			<kw><![CDATA[metadiscipline]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31063613</person_id>
				<author_profile_id><![CDATA[81100087535]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Natalie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jeremijenko]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yale Univ]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37026902</person_id>
				<author_profile_id><![CDATA[81409594757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Thecla]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schiphorst]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser Univ]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40033117</person_id>
				<author_profile_id><![CDATA[81100130074]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mateas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CMU Computer]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31063867</person_id>
				<author_profile_id><![CDATA[81100648272]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Wolfgang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Strauss]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GMD Inst Media Communication]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28022214</person_id>
				<author_profile_id><![CDATA[81332536188]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Will]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wright]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Maxis Software]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39063008</person_id>
				<author_profile_id><![CDATA[81100203284]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Andruid]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kerne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Creating Media]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242123</article_id>
		<sort_key>93</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[The future of computer graphics]]></title>
		<subtitle><![CDATA[an enabling technology?]]></subtitle>
		<page_from>93</page_from>
		<page_to>94</page_to>
		<doi_number>10.1145/1242073.1242123</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242123</url>
		<abstract>
			<par><![CDATA[<p>Computer graphics research and hardware has matured as a field to the point that high-quality computer graphics is becoming ubiquitous. Computer graphics shortly will be where word processing is today: everyone uses it, but there are very few people doing basic research in word processing. All of the challenges lie in the applications and use of this technology to enable advances in many fields. This panel will combine experts in computer graphics and associated technology with experts from a few applications areas to discuss the possibilities and future ways that computer graphics can advance discovery in many fields.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P61351</person_id>
				<author_profile_id><![CDATA[81100113675]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Ebert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP38028873</person_id>
				<author_profile_id><![CDATA[81452616426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Buxton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Alias | Wavefront, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864741</person_id>
				<author_profile_id><![CDATA[81318493278]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Patricia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Davies]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P77052</person_id>
				<author_profile_id><![CDATA[81100469189]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Elliot]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Fishman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johns Hopkins Hospital]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35038344</person_id>
				<author_profile_id><![CDATA[81100017052]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Glassner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Coyote Wind Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242124</article_id>
		<sort_key>95</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Symposium on computer animation in fast forward]]></title>
		<page_from>95</page_from>
		<page_to>95</page_to>
		<doi_number>10.1145/1242073.1242124</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242124</url>
		<abstract>
			<par><![CDATA[<p>"Three-minute madness." Papers from the new ACM SIGGRAPH Symposium on Computer Animation are summarized in three minutes or less, followed by a discussion of new directions in computer animation research.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39062872</person_id>
				<author_profile_id><![CDATA[81100342764]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gleicher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Wisconsin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40024256</person_id>
				<author_profile_id><![CDATA[81406592138]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cohen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39063141</person_id>
				<author_profile_id><![CDATA[81100495142]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nancy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pollard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P139385</person_id>
				<author_profile_id><![CDATA[81100049661]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jessica]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hodgins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Symposium]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40030394</person_id>
				<author_profile_id><![CDATA[81319502903]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michiel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[van de Panne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Symposium]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242125</article_id>
		<sort_key>96</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[The demo scene]]></title>
		<page_from>96</page_from>
		<page_to>97</page_to>
		<doi_number>10.1145/1242073.1242125</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242125</url>
		<abstract>
			<par><![CDATA[<p>For 20 years, an underground movement has produced short real-time animations running on home computers. This group, the "demoscene," primarily consists of students who pursue their technical and artistic interests beyond the classroom, to create inspiring works of real-time art. These productions encompass a broad range of computer graphics techniques such as procedural geometry, real-time ray-tracing, and real-time shading.</p> <p>Game developers have been utilizing this talent pool yet it has little visibility in the SIGGRAPH community. This panel explores the demoscene, technical tricks used in demos, and how scene educational and creative aspects can contribute to the SIGGRAPH community.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28021743</person_id>
				<author_profile_id><![CDATA[81100646589]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Vincent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Scheib]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UNC Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864777</person_id>
				<author_profile_id><![CDATA[81328488126]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Theo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Engell-Nielsen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[hybris/NEMESIS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864762</person_id>
				<author_profile_id><![CDATA[81328489207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Saku]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lehtinen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Remedy Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28021028</person_id>
				<author_profile_id><![CDATA[81100474675]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Haines]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Autodesk, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864747</person_id>
				<author_profile_id><![CDATA[81328490473]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Phil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Taylor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft DirectX]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242126</article_id>
		<sort_key>98</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Animation's turning tide]]></title>
		<page_from>98</page_from>
		<page_to>98</page_to>
		<doi_number>10.1145/1242073.1242126</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242126</url>
		<abstract>
			<par><![CDATA[<p>This panel seeks to demystify, debunk and goad the dialogue of 3D character animation. This panel is motivated by the sea change that is currently effecting our industry. In the last year and a half what was once a battle to get 3D into Hollywood has completely crossed over --- Hollywood can't get enough. 11 years ago the quote from top Disney management was "there will never be a digital character in a Disney feature film." Now traditional animators and artists are giving up their fear for pragmatism and embracing 3D in droves. What does that mean for those already working in 3D? What does it portend? What can we learn from the traditional animator?</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864768</person_id>
				<author_profile_id><![CDATA[81328488277]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Elson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864653</person_id>
				<author_profile_id><![CDATA[81328487859]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eamonn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Butler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28020612</person_id>
				<author_profile_id><![CDATA[81537210256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Clark]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864630</person_id>
				<author_profile_id><![CDATA[81328490152]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Carlos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Saldanha]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blue Sky Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242127</article_id>
		<sort_key>99</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Unsolved problems in mobile computer graphics and interaction]]></title>
		<page_from>99</page_from>
		<page_to>100</page_to>
		<doi_number>10.1145/1242073.1242127</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242127</url>
		<abstract>
			<par><![CDATA[<p>With the number of mobile devices exceeding PCs, research is required in many areas with respect to graphics and interaction. There are problems with interaction, streaming, graphics algorithms, bandwidth with current and future devices. This panel examines the state of the art from both an industrial and research point of view, and provides directions for future work in this area.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP30040818</person_id>
				<author_profile_id><![CDATA[81100359615]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ollila]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Link&#246;ping University, Norrk&#246;ping, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P266789</person_id>
				<author_profile_id><![CDATA[81100363935]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Staffan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bj&#246;rk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Interactive Institute, G&#246;teborg, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864704</person_id>
				<author_profile_id><![CDATA[81328487856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bradshaw]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Machines That Go Ping!, Edinburgh, Scotland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39042315</person_id>
				<author_profile_id><![CDATA[81100427474]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Feiner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Columbia University, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39063155</person_id>
				<author_profile_id><![CDATA[81100567347]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pulli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nokia Mobile Phones, Oulu, Finland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242128</article_id>
		<sort_key>101</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Graphics in the large]]></title>
		<subtitle><![CDATA[is bigger better?]]></subtitle>
		<page_from>101</page_from>
		<page_to>102</page_to>
		<doi_number>10.1145/1242073.1242128</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242128</url>
		<abstract>
			<par><![CDATA[<p>The world of display devices is expanding rapidly, both literally and figuratively. New commercial and research devices come in larger sizes (measured in meters, not inches) and different physical forms (e.g. rectangular surfaces, cylindrical segments, truncated spheres). Such expansion means that graphics and interactive techniques are becoming far more amenable to group activities and can display more and more data at once.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31063403</person_id>
				<author_profile_id><![CDATA[81100040066]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Loren]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Carpenter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35038336</person_id>
				<author_profile_id><![CDATA[81100550016]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fisher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28021562</person_id>
				<author_profile_id><![CDATA[81544835156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[May]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39067079</person_id>
				<author_profile_id><![CDATA[81100032890]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Norbert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Streitz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer IPSI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P62287</person_id>
				<author_profile_id><![CDATA[81100541084]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Kasik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Boeing]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242129</article_id>
		<sort_key>103</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[How does motion capture affect animation?]]></title>
		<page_from>103</page_from>
		<page_to>104</page_to>
		<doi_number>10.1145/1242073.1242129</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242129</url>
		<abstract>
			<par><![CDATA[<p>The application of motion capture in the movie industry has continued to increase in the last couple of years, ranging from background action to major characters. Using motion capture in a production pipeline requires both motion capture and animation experience. An animator needs an understanding of biomechanics and how the body moves both for planning marker placement and to be able to apply motion capture to a character in such a way that it fits the character and the story development. For motion capture to be usable, it requires planning well in advance, knowing what needs to be captured and how it's going to be applied.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P188359</person_id>
				<author_profile_id><![CDATA[81100412047]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Margaret]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Geroch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Wheeling Jesuit University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP36037163</person_id>
				<author_profile_id><![CDATA[81335491561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Evan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirsch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts Worldwide Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31063904</person_id>
				<author_profile_id><![CDATA[81100088065]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Joan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Staveley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[VP Faust Logic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864782</person_id>
				<author_profile_id><![CDATA[81328490460]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tolles]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[hOuse Of mOves]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864618</person_id>
				<author_profile_id><![CDATA[81365593679]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Barb]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Helfer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ACCAD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31063975</person_id>
				<author_profile_id><![CDATA[81328490979]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Suba]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Varadarajan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ACCAD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242130</article_id>
		<sort_key>105</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Interactive stories]]></title>
		<subtitle><![CDATA[real systems, three solutions]]></subtitle>
		<page_from>105</page_from>
		<page_to>106</page_to>
		<doi_number>10.1145/1242073.1242130</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242130</url>
		<abstract>
			<par><![CDATA[<p>While pure theory can be interesting, this panel starts where the rubber meets the road for interactive stories --- real systems, and the practice-oriented insights of creators. Expect to hear concrete examples and solutions to hard problems, ranging from the technical to the artistic. Of course, there's also a lot of controversy about what interactive stories are, and how to best make them, which this panel doesn't plan to ignore. It includes people working on major systems that represent three different approaches, and puts them together with a moderator that knows the hot buttons (as well as the points of agreement). The three approaches can be called: narrative game, interactive drama, and massively multiplayer storytelling.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31047440</person_id>
				<author_profile_id><![CDATA[81100554401]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Noah]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wardrip-Fruin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31062555</person_id>
				<author_profile_id><![CDATA[81328490646]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stern]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Interactivestory.net]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864743</person_id>
				<author_profile_id><![CDATA[81328489637]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Molyneux]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lionhead Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40024036</person_id>
				<author_profile_id><![CDATA[81100130074]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mateas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28022195</person_id>
				<author_profile_id><![CDATA[81543809256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Bernard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[En-Tranz Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242131</article_id>
		<sort_key>107</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Games]]></title>
		<subtitle><![CDATA[the dominant medium of the future]]></subtitle>
		<page_from>107</page_from>
		<page_to>108</page_to>
		<doi_number>10.1145/1242073.1242131</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242131</url>
		<abstract>
			<par><![CDATA[<p>Driven by trends in silicon and software, computer gaming is the medium that will define the recreational and cultural experience of the twenty-first century in the way that motion pictures and their offspring television, defined the recreation of the twentieth. Or so we assert! This panel will debate the truth of the statement that gaming is the dominant medium of the future with believers, skeptics, and outside referees.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28021711</person_id>
				<author_profile_id><![CDATA[81320493396]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nicoll]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Graphics Training Manager]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31063448</person_id>
				<author_profile_id><![CDATA[81100426724]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Glenn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Entis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864742</person_id>
				<author_profile_id><![CDATA[81328488371]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gilmore]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28020902</person_id>
				<author_profile_id><![CDATA[81328488395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Herz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864612</person_id>
				<author_profile_id><![CDATA[81328489809]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Alex]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28022482</person_id>
				<author_profile_id><![CDATA[81332536188]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Will]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wright]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31063749</person_id>
				<author_profile_id><![CDATA[81100250413]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Perlin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	</section>
	<section>
		<section_id>1242132</section_id>
		<sort_key>111</sort_key>
		<section_seq_no>4</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[sigKIDS]]></section_title>
		<section_page_from>111</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P864722</person_id>
				<author_profile_id><![CDATA[81319488110]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Middle Tennessee State University]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1242133</article_id>
		<sort_key>111</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Interactive animation as an educational tool in "Winter Dreams"]]></title>
		<page_from>111</page_from>
		<page_to>112</page_to>
		<doi_number>10.1145/1242073.1242133</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242133</url>
		<abstract>
			<par><![CDATA[<p>Being interested in Interactive Animation, Illustration, Multimedia, and the creative use of programming languages, I explored these areas to produce an educational game for preschool children. The following four chapters present the concept and development of an experimental interactive game titled "Winter Dreams".</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35038719</person_id>
				<author_profile_id><![CDATA[81335498954]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daria]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsoupikova]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Syracuse University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fox M. 1999. <i>Sleepy Bears</i>. Harcourt Brace & Company.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bilibin I. 1899-1900. <i>Vassilisa the Beautiful</i>. GOZNAK, Moscow 1976.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242134</article_id>
		<sort_key>113</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Gaming as an educational tool]]></title>
		<subtitle><![CDATA[internet scavenger hunt]]></subtitle>
		<page_from>113</page_from>
		<page_to>113</page_to>
		<doi_number>10.1145/1242073.1242134</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242134</url>
		<abstract>
			<par><![CDATA[<p>Can games be educational as well as fun?</p> <p>Educators and children's media developers have long been trying to answer this question. Much of the software produced for kids and marketed as educational is comprised of the type of games that Seymour Papert has so adeptly defined as "edutainment".<sup>1</sup> Many of these games are drill-type memorization exercises where the game activity has little or no relation to the "educational" component. Mainstream educational software might be fun, even engaging, but very little of it challenges the user intellectually or encourages open-ended discovery.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864698</person_id>
				<author_profile_id><![CDATA[81328489371]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Karen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Monahan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Parsons School of Design]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Papert, S. 1998. Does Easy Do It? Children, Games, and Learning. <i>Game Developer</i>, 88.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242135</article_id>
		<sort_key>114</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[FORM]]></title>
		<page_from>114</page_from>
		<page_to>114</page_to>
		<doi_number>10.1145/1242073.1242135</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242135</url>
		<abstract>
			<par><![CDATA[<p>FORM is an educational software prototype designed to encourage children's creative play with fundamental geometric shapes. The open-ended program serves as a creative tool to help form a deep and lasting understanding of the beauty, elegance and underlying unity of math, science, design and nature.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864668</person_id>
				<author_profile_id><![CDATA[81328490976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hilary]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Wright]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nancy Hyland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Brosterman, N. 1997. <i>Inventing Kindergarten</i>. New York: Henry N. Abrams. p. 12]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Froebel, F. 1887. <i>Education of Man</i>. New York: D. Appleton & Co. Translated by W. N. Hailman, AM. p. vi. Editors preface]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Froebel, F. 1912. <i>Froebel's Chief Writings on Education</i>. London: E. Arnold. Translated by S. S. Fletcher, MA, PhD and J. Welton, MA. p. 50]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Varnelis, K. 1998 <i>The Education of the Innocent Eye</i>. Journal of Architectural Education, May]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242136</article_id>
		<sort_key>115</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[The floating words for kids]]></title>
		<subtitle><![CDATA[interactive installation for learning alphabet]]></subtitle>
		<page_from>115</page_from>
		<page_to>116</page_to>
		<doi_number>10.1145/1242073.1242136</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242136</url>
		<abstract>
			<par><![CDATA[<p>"The Floating Words" is a new device for playing with words. This installation provides the viewer with a new feeling: If you speak to the microphone, your voice drips into a water pool as water drops, and will begin to float. You can stir or ladle the "letters" with a stick or ladle.</p> <p>The installation of version_1 was exhibited at SIGGRAPH2001 Art Gallery, and it was succeeded very much.</p> <p>This time, we add new expression to new version: "The Floating Words for KIDS", and the expression is device to learn alphabet playing with the "The Floating Words". The expression is: If you ladle some letters with a ladle, they will turn to the characters showing word that has the alphabet in an initial.</p> <p>This project is a proposal of using this installation for learning alphabet for kids.</p> <p>These feeling: it seem like a magic are results from tricks of combination of voice recognition system, 3D magnetic sensor, simple computer graphics and real water and so on.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P828725</person_id>
				<author_profile_id><![CDATA[81320493013]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Satoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moroi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Denki University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35038713</person_id>
				<author_profile_id><![CDATA[81540114856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shinji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sasada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Electronics College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864760</person_id>
				<author_profile_id><![CDATA[81328490136]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ryoji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shibata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Denki University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242137</article_id>
		<sort_key>117</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Toys to teach]]></title>
		<subtitle><![CDATA[mathematics as a collaborative climbing exercise]]></subtitle>
		<page_from>117</page_from>
		<page_to>119</page_to>
		<doi_number>10.1145/1242073.1242137</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242137</url>
		<abstract>
			<par><![CDATA[<p>Whereas traditional research on collaborative educational systems has primarily focused on how to define better modes of digital interaction, this approach is found lacking when applied to developing collaborative systems for elementary school aged children. It is creatively and collaboratively restrictive to filter the enthusiastic interactions of these excited 12-year-old children through progressively more complicated GUIs. Current E-GEMS research examines design factors of educational systems that recognize and facilitate the social context of the classroom and hopes to encourage, rather than restrict, peer-to-peer social discussion and interaction. By correlating observed interactions in the digital domain with those in the social domain, we hope to shed light on design factors of collaborative systems that can be an integral and exciting part of a child's mathematical education. The vessel of our current research is the two-player collaborative mathematical exercise PrimeClimb, developed at E-GEMS. This paper describes the study conducted with PrimeClimb and documents the methodology of data capture and analysis used in the study -- methods borrowed from ethnography, education research and sociology.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[collaborative learning]]></kw>
			<kw><![CDATA[computer support for collaborative learning]]></kw>
			<kw><![CDATA[educational technology]]></kw>
			<kw><![CDATA[peer to peer discussion]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28020782</person_id>
				<author_profile_id><![CDATA[81543988156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP38029141</person_id>
				<author_profile_id><![CDATA[81328490929]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39066632</person_id>
				<author_profile_id><![CDATA[81328487850]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cohen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864785</person_id>
				<author_profile_id><![CDATA[81328491052]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Troy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39063051</person_id>
				<author_profile_id><![CDATA[81339509657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Maria]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Klawe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>772100</ref_obj_id>
				<ref_obj_pid>772072</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cassell, J. Genderizing HCI. In Press. 2002. Jacko, J. and Sears, A. (eds.), <i>The Handbook of Human-Computer Interaction</i>. Mahwah, NJ: Lawrence Erlbaum.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>222164</ref_obj_id>
				<ref_obj_pid>222020</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Inkpen, K., Booth, K. S., Klawe, M. and Upitis, R. 1995. Playing Together Beats Playing Apart, Especially for Girls. In <i>Proceedings of Computer Support for Collaborative Learning 1995</i>. Bloomington, Indiana.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Klawe, M. M. 1999. Computer Games, Education and Interfaces: The E-GEMS Project, http://taz.cs.ubc.ca/egems/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Klawe, M. M., Westrom, M., Davidson, K. and Super, S. 1996. Phoenix Quest: lessons in developing an educational computer game for girls.. and boys. Presented at ICMTM96. http://taz.cs.ubc.ca/egems/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>902038</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[McGrenere, J. 1996. Experimental Design: Educational Electronic Multi-Player Games -- A Literature Review. Technical Report 96-12, the University of British Columbia.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Pirie, S. E. B. 1996. Classroom video recording: When, why and how does it offer a valuable data source for qualitative research? Paper presented at the <i>18&#60;sup&#62;th&#60;/sup&#62; Annual Meeting of the North American Chapter of the International Group for the Psychology of Mathematics Education</i>. Panama City, Florida. ERIC ED 401 128, 17 pages.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Pirie, S. E. B. 1998. Working towards a design for qualitative research. In A. Teppo (Ed.), <i>Qualitative Research Methods in Mathematics Education</i>. Journal of Research in Mathematics Education Monograph Series, Reston, VA: National Council of Teachers of Mathematics, pp. 79--97.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>702644</ref_obj_id>
				<ref_obj_pid>646684</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Sedighian, K. and Westrom, M. 1997. Direct Object Manipulation vs. Direct Concept Manipulation: Effect of Interface Style on Reflection and Domain Learning. <i>HCI 97: Premier European Conference on Human-Computer Interaction</i>. Bristol, UK.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>212948</ref_obj_id>
				<ref_obj_pid>212925</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Suchman, Lucy A and Trigg, Randall H. 2000. Understanding Practice: Video as a Medium for Reflection and Design. Taken from Baecker et al (2000). <i>Readings in Human-Computer Interactions: Toward the Year 2000</i>. pp. 233--240.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242138</article_id>
		<sort_key>120</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Jollee-Mail Playground]]></title>
		<page_from>120</page_from>
		<page_to>121</page_to>
		<doi_number>10.1145/1242073.1242138</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242138</url>
		<abstract>
			<par><![CDATA[<p>JOLLEE-MAIL PLAYGROUND is a simulated environment where children can experience the Internet communication in a more involving fashion.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31063534</person_id>
				<author_profile_id><![CDATA[81328488772]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kamata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cyber Network Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242139</article_id>
		<sort_key>122</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[The virtual dig]]></title>
		<page_from>122</page_from>
		<page_to>123</page_to>
		<doi_number>10.1145/1242073.1242139</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242139</url>
		<abstract>
			<par><![CDATA[<p>The Virtual Dig is an interactive adventure for young people and provides an educational experience in archaeology and historical architecture. The web site was produced for the Israel Museum, Jerusalem, in collaboration with museum archaeologists and can be seen online at the addresses: "http://www.imj.org.il/arc-tel" and "http://www.arcv.org".</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31062045</person_id>
				<author_profile_id><![CDATA[81100095584]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dunn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Arc Vertuel, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242140</article_id>
		<sort_key>124</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Virtual studio]]></title>
		<subtitle><![CDATA[virtual reality in art]]></subtitle>
		<page_from>124</page_from>
		<page_to>124</page_to>
		<doi_number>10.1145/1242073.1242140</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242140</url>
		<abstract>
			<par><![CDATA[<p>Virtual Reality or VR is a computer-designed environment that allows the user to interact within the digital environment as he or she would in a real world situation. VR has been successfully used in game development, medical technology and flight simulation. Virtual Studio is a partial immersive, interactive virtual learning environment that is based on the metaphor of the artist studio. Its primarily use will be to teach elementary students about the techniques, styles and history of artist and art. Each part of Virtual Studio is named after major artists, who used or pioneered a particular style or technique. There are three core sections of Virtual Studio: 1) Architectural Design, 2) Sculptural Design, 3) Digital Painting and Color Theory.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864638</person_id>
				<author_profile_id><![CDATA[81328488555]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[DaShawn]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Hall]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of the Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864703</person_id>
				<author_profile_id><![CDATA[81328490179]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kenneth]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakatani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of the Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>138168</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Pimentel, K. and Teixeira, K. 1993. <i>Virtual Reality: Through the new looking glass</i>. McGraw Hill.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Youngblut, C. 1998. <i>Educational Uses of Virtual Reality Technology</i>. The Institute for Defense Analyses]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242141</article_id>
		<sort_key>125</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[An application of tangible interfaces in collaborative learning environments]]></title>
		<page_from>125</page_from>
		<page_to>126</page_to>
		<doi_number>10.1145/1242073.1242141</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242141</url>
		<abstract>
			<par><![CDATA[<p>Over the years, educators and government officials have searched for ways to improve learning in our schools, particularly in the areas of math and science. Many have come to recognize that collaborative activities, learning through play, and teacher guidance can help children to get over their initial fears and even begin to enjoy these subjects. Yet, at the same time, shrinking school budgets are making it harder to support these approaches to learning. Tangible Interfaces for Collaborative Learning Environments (TICLE) was conceived in response to this need [Scarlatos 2002].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P173229</person_id>
				<author_profile_id><![CDATA[81100155873]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Scarlatos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brooklyn College, Brooklyn, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Artzt, A. F. and Armour-Thomas, E. 1992. Development of a Cognitive-Metacognitive Framework for Protocol Analysis of Mathematical Problem Solving in Small Groups, <i>Cognition and Instruction</i> 9(2), 137--175.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>603517</ref_obj_id>
				<ref_obj_pid>603512</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Scarlatos, L. L. 2002. TICLE: Using Multimedia Multimodal Guidance to Enhance Learning, <i>Information Sciences 140</i>, 85--103.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>274717</ref_obj_id>
				<ref_obj_pid>274644</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Underkoffler, J. and Ishii, H. 1998. Illuminating Light: an Optical Design Tool with a Luminous-Tangible Interface, in <i>Proceedings of ACM SIGCHI 1998</i>, ACM Press / ACM SIGCHI, New York, 542--549.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242142</article_id>
		<sort_key>127</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[GollyGee Blocks#8482;]]></title>
		<subtitle><![CDATA[a 3D modeler for children]]></subtitle>
		<page_from>127</page_from>
		<page_to>127</page_to>
		<doi_number>10.1145/1242073.1242142</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242142</url>
		<abstract>
			<par><![CDATA[<p><b>GollyGee Blocks#8482;</b> is a 3D modeling program for children. Children use it to stack, transform, color and texture 3D objects in a 3D scene which can be viewed from any angle. <b>GollyGee Blocks</b>#8482; was designed as an educational, open-ended creativity tool for 3D graphics.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864694</person_id>
				<author_profile_id><![CDATA[81328487421]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Blocksom]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GollyGee Software, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242143</article_id>
		<sort_key>128</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Anansi's world of folklore]]></title>
		<page_from>128</page_from>
		<page_to>128</page_to>
		<doi_number>10.1145/1242073.1242143</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242143</url>
		<abstract>
			<par><![CDATA[<p>Can oral storytelling live in a digital medium? Anansi's World of Folklore is a celebration of the art of storytelling, created as a broadband site for showcasing and collecting folktales. It was created to discover a way to give traditional oral storytelling a meaningful place on the Internet.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864680</person_id>
				<author_profile_id><![CDATA[81328489913]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jacqueline]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nuwame]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242144</article_id>
		<sort_key>129</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[The ToyScouts' immersive jukebox]]></title>
		<subtitle><![CDATA[University of Central Florida]]></subtitle>
		<page_from>129</page_from>
		<page_to>130</page_to>
		<doi_number>10.1145/1242073.1242144</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242144</url>
		<abstract>
			<par><![CDATA[<p><b><i>The Immersive Jukebox</i></b> offers users a choice of musical experiences exploring various influences of African American Blues music: traditional African music, spirituals, work songs, and others. Inspired by the curriculum of the International House of Blues Foundation (IHOBF)* Blues SchoolHouse program, it introduces students and teachers to the music, art and history of the blues and its cultural origins.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28022016</person_id>
				<author_profile_id><![CDATA[81100196635]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stapleton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	</section>
	<section>
		<section_id>1242145</section_id>
		<sort_key>133</sort_key>
		<section_seq_no>5</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Sketches and applications]]></section_title>
		<section_page_from>133</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P864651</person_id>
				<author_profile_id><![CDATA[81100308346]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Roble]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1242146</article_id>
		<sort_key>133</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[2D/3D hybrid character animation on "Spirit"]]></title>
		<page_from>133</page_from>
		<page_to>133</page_to>
		<doi_number>10.1145/1242073.1242146</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242146</url>
		<abstract>
			<par><![CDATA[<p>DreamWorks Feature animated film "Spirit" takes new steps in the hybridization of 2D and 3D production techniques for Character Animation, even to the extent of using both approaches within the same shot.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28021037</person_id>
				<author_profile_id><![CDATA[81328487742]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Doug]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cooper]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242147</article_id>
		<sort_key>134</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[3D browser for interactive television]]></title>
		<page_from>134</page_from>
		<page_to>134</page_to>
		<doi_number>10.1145/1242073.1242147</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242147</url>
		<abstract>
			<par><![CDATA[<p>Digital media convergence is creating a vast new parallel universe, the digital dimension, where the range of available services is growing exponentially. Different forms of media and communication are integrating into consumer devices. This means new challenges for information user interfaces, especially from the point of view of user experience. The sketch introduced here is a functional prototype, a proof-of-concept, of a new kind of graphical user interface for interactive television. It integrates different services for television, Internet, radio, personal communication and games under one user interface.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864710</person_id>
				<author_profile_id><![CDATA[81328489721]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kotro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Petri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Valkeus Interactive Ltd, Finland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864721</person_id>
				<author_profile_id><![CDATA[81328489845]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mannerkoski]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Olli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Valkeus Interactive Ltd, Finland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864717</person_id>
				<author_profile_id><![CDATA[81328488584]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Lesonen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hannu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Valkeus Interactive Ltd, Finland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864718</person_id>
				<author_profile_id><![CDATA[81328490221]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Lustila]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Risto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Valkeus Interactive Ltd, Finland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242148</article_id>
		<sort_key>135</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[3D haptic shape perception using a 2D device]]></title>
		<page_from>135</page_from>
		<page_to>135</page_to>
		<doi_number>10.1145/1242073.1242148</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242148</url>
		<abstract>
			<par><![CDATA[<p>Is a 2-dimensional (2D) force feedback device capable of presenting 3-dimensional (3D) shapes? The answer is a qualified "yes." "Force shading", a haptic counterpart of bump mapping in computer graphics, presents a non-flat shape on a nominally flat surface by varying the force vector direction in haptic rendering [Morgenbesser and Srinivasan 1996][Robles-De-La-Torre and Hayward 2001]. To our knowledge, such phenomena have been qualitatively measured only by 3D devices, and a quantitative comparison to 2D devices has not been made. We compare thresholds of human shape perception of the plane experimentally, using 2D and 3D force feedback devices.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864675</person_id>
				<author_profile_id><![CDATA[81328488620]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Huirong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Han]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ochanomizu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31063939</person_id>
				<author_profile_id><![CDATA[81100617642]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Juli]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamashita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[AIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39039141</person_id>
				<author_profile_id><![CDATA[81100355096]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Issei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fujishiro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ochanomizu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Morgenbesser, H., and Srinivasan, M. 1996. Force Shading for Haptic Shape Perception. In <i>Proc. ASME Dynamics Systems and Control Division</i>, 407--412.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Robles-De-La-Torre, G., and Hayward, V. 2001. Force Can Overcome Object Geometry in the Perception of Shape through Active Touch. <i>Nature 412</i>, 26, 445--448.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[TM URL. http://www.fujixerox.co.jp/tangible_mouse/]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242149</article_id>
		<sort_key>136</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[3D layout and propagation of environmental phenomena for <i>Ice Age</i>]]></title>
		<page_from>136</page_from>
		<page_to>136</page_to>
		<doi_number>10.1145/1242073.1242149</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242149</url>
		<abstract>
			<par><![CDATA[<p>This sketch presents how the 3D Layout Department at Blue Sky Studios dealt with the production pipeline issues of transforming their first feature film <i>Ice Age</i> from sequences of hand-drawn storyboards into blocked out, ready-to-animate scenes in 3D for the Animation Department. To achieve this, custom tools were developed to automate sequence set-up and distribution of shots, in order to minimize the artists' concern with data management and maximize the creative time available to accommodate revisions in the story.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864706</person_id>
				<author_profile_id><![CDATA[81365595618]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thomason]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blue Sky Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864751</person_id>
				<author_profile_id><![CDATA[81328487959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[V.]]></middle_name>
				<last_name><![CDATA[Cavaleri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blue Sky Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242150</article_id>
		<sort_key>137</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[3D reconstruction of walking behaviour using a single camera]]></title>
		<page_from>137</page_from>
		<page_to>137</page_to>
		<doi_number>10.1145/1242073.1242150</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242150</url>
		<abstract>
			<par><![CDATA[<p>A consumer-grade camera is used to view the motion of individuals moving in an area, such as that observed in the lobby of a building. By assuming individuals are engaged in fairly normal walking behavior, we can extract sufficient information to synthetically reconstruct the scene using simple image processing techniques. The reconstructed scene can then be viewed from any angle and used to track interesting individuals. Features of the person such as shirt and pants color can be extracted from the video and applied to the synthetic model thus allowing the real individual to be recognized from viewing the reconstruction. The technique proposed has the advantage of using a single camera and would find applications in gait analysis and security. This is a work in progress and we present some initial results of a single moving figure.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P526017</person_id>
				<author_profile_id><![CDATA[81100431569]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Arunachalam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Somasundaram]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P243504</person_id>
				<author_profile_id><![CDATA[81100414668]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Rick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Parent]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Tsai, R. Y. 1987. A versatile camera calibration technique for high accuracy 3D machine vision metrology using off-the shelf tv cameras and lenses. <i>IEEE JOURNAL OF ROBOTICS AND AUTOMATON RA - 3</i>, 4, 323--344.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242151</article_id>
		<sort_key>138</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[A 2D sketch interface for a 3D model search engine]]></title>
		<page_from>138</page_from>
		<page_to>138</page_to>
		<doi_number>10.1145/1242073.1242151</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242151</url>
		<abstract>
			<par><![CDATA[<p>This sketch describes our experiences with creating query interfaces for an online search engine for 3D models. With the advent of affordable powerful 3D graphics hardware, and the improvement of model acquisition methods, an increasing number of 3D models is available on the web, creating a need for a 3D model search engine [Paquet and Rioux 2000; Suzuki 2001]. An important problem that arises for such an application is how to create an effective query interface. To investigate this issue, we created several different query interfaces, and tested them both in controlled experiments as well as in a publicly available 3D model search engine.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39033868</person_id>
				<author_profile_id><![CDATA[81100236265]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Min]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P403346</person_id>
				<author_profile_id><![CDATA[81100119189]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Joyce]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14073484</person_id>
				<author_profile_id><![CDATA[81100182132]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Funkhouser]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Paquet, E., and Rioux, M. 2000. Nefertiti: A tool for 3-d shape databases management. <i>SAE Transactions: Journal of Aerospace 108</i>, 387--393.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Suzuki, M. T. 2001. A web-based retrieval system for 3d polygonal models. <i>Joint 9th IFSA World Congress and 20th NAFIPS International Conference (IFSA/NAFIPS2001)</i> (July), 2271--2276.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242152</article_id>
		<sort_key>139</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[A dynamic motion control middleware for computer games]]></title>
		<page_from>139</page_from>
		<page_to>139</page_to>
		<doi_number>10.1145/1242073.1242152</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242152</url>
		<abstract>
			<par><![CDATA[<p>In this sketch, we present a middleware for computer games that has the ability to realize dynamic motion control of characters. The specific novelty of the middleware is to produce dynamically changing motions in response to physical interaction between the character and environments such as collision impulses and external forces as shown in Figure 1.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP37034538</person_id>
				<author_profile_id><![CDATA[81335495744]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Masaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Oshita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39031417</person_id>
				<author_profile_id><![CDATA[81100183835]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Akifumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Makinouchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Oshita, M., and Makinouchi, A. 2001. A dynamic motion control technique for human-like articulated figures. <i>Computer Graphics Forum (EUROGRAPHICS 2001) 20</i>, 3, 192--202.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242153</article_id>
		<sort_key>140</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[A handheld virtual mirror]]></title>
		<page_from>140</page_from>
		<page_to>140</page_to>
		<doi_number>10.1145/1242073.1242153</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242153</url>
		<abstract>
			<par><![CDATA[<p>This sketch presents the design and construction of a handheld virtual mirror device. The perception of the world reflected through a mirror depends on the viewer's position with respect to the mirror and the 3-D geometry of the world. In order to simulate a real mirror on a computer screen, images of the observed world, consistent with the viewer's position, must be synthesized and displayed in real-time. Our system is build around a flat LCD screen manipulated by the user, a single camera fixed on the screen, and a tracking device. The continuous input video stream and tracker data is used to synthesize, in real-time, a continuous video stream displayed on the LCD screen. The synthesized video stream is a close approximation of what the user would see on the screen surface if it were a real mirror.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39062838</person_id>
				<author_profile_id><![CDATA[81452602965]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alexandre]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fran&#231;ois]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864654</person_id>
				<author_profile_id><![CDATA[81328488944]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Elaine]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864787</person_id>
				<author_profile_id><![CDATA[81328489389]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Umberto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Malesci]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fran&#231;ois A. 2001. <i>Modular Flow Scheduling Middleware</i>. http://mfsm.sourceForge.net/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lazzari, M., McLaughlin, M. L., Jaskowiak, J., Wong, W., and Akbarian, M. 2002. A haptic exhibition of daguerreotype cases for USC's Fisher Gallery. In <i>Touch in Virtual Environments: Haptics and the Design of Interactive Systems</i>, McLaughlin, M. L., Hespanha, J., and Sukhatme, G., Eds., IMSC Series in Multimedia, Prentice-Hall.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242154</article_id>
		<sort_key>141</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[A new interface for the virtual world foot motion sensing input device]]></title>
		<page_from>141</page_from>
		<page_to>141</page_to>
		<doi_number>10.1145/1242073.1242154</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242154</url>
		<abstract>
			<par><![CDATA[<p>Presently, most virtual reality systems use upper body parts to interact with objects in the virtual environment. This situation is caused by technological limitations of current interface devices. Starting from this viewpoint we developed a new interface for detecting ankle motions relative to the knee. We believe that hands-free navigation, unlike the majority of navigation techniques based on hand motions, has the greatest potential for maximizing the interactivity of virtual environments since navigation modes are more direct motion of the feet.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864619</person_id>
				<author_profile_id><![CDATA[81328490148]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Barrera]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Salvador]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39045395</person_id>
				<author_profile_id><![CDATA[81100491257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takahashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39063105</person_id>
				<author_profile_id><![CDATA[81100441141]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Masayuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakajima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>836041</ref_obj_id>
				<ref_obj_pid>832290</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Iwata, H., and Fujii, T. 1996. Virtual perambulator: A novel interface device for locomotion in virtual environment. In <i>Proceedings of IEEE 1996</i>, Virtual Reality Annual International Symposium, IEEE.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>90715</ref_obj_id>
				<ref_obj_pid>90692</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Shinogawa, Y., and Young, Y. 1990. Automating view function generation for walkthrough animation using a reeb graph. In <i>Proceedings of Computer Animation 90</i>, Springer, Tokyo, 227--238.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242155</article_id>
		<sort_key>142</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[A phenomenological model for bokeh rendering]]></title>
		<page_from>142</page_from>
		<page_to>142</page_to>
		<doi_number>10.1145/1242073.1242155</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242155</url>
		<abstract>
			<par><![CDATA[<p>"Bokeh" is a Japanese word used to describe the quality of the out-of-focus areas as rendered on film by a physical lens. In areas that are out of focus, the circle of confusion (i.e., the distribution of light absorbed onto film from a single point of light in the scene) is bigger than in areas that are focused. Different lenses distribute light within the circle of confusion differently, depending on a number of factors, like the shape and number of diaphragm blades and the optical design of the lens itself. For example, lenses that are designed to correct for optical aberrations tend to render a circle of confusion that is more intense on the edges and slightly less so in the center. Other lenses, like the so called "mirror" lenses, which include reflective elements besides refractive ones, typically produce donut-shaped circles of confusion since some of the light paths are cut from the center by the reflective element.</p> <p>Moreover, in a real lens the circle of confusion sometimes becomes elliptical on the areas outside of the center of the image, with the shorter axis of the ellipsis oriented radially from it.</p> <p>Standard rendering techniques model depth of field using point sampling techniques which converge towards a bokeh model with uniform density across the circle of confusion. This represents an idealized lens that does not exist in reality. Real lenses have non-uniform bokeh distributions. For example, a smooth, or "creamy" lens will have a Gaussian distribution across the circle of confusion. Real lenses may have different distributions at different points on the film plane.</p> <p>Our technique allows the animator to specify an arbitrary probability density function to represent the distribution of intensity within the circle of confusion. The probability curve is used by the renderer to jitter the location of the sample point on the lens. For example, if the density function is a Gaussian, more samples will be taken towards the center of the lens than at the edges. Conversely, for a density function that simulates a mirror lens, fewer samples are taken near the center of the lens because the mirror blocks light as it moves through the lens. By generating enough sample points, the model converges on the true bokeh density function.</p> <p>We will also show the effect of specifying two shapes for the light distribution, one for close and the other for far focused areas, and the deformation of the circle of confusion so it becomes elliptical on the edges of the image.</p> <p>Future work includes the specification of the diaphragm shape, and the possible implementation of this technique as a particle renderer, which would provide efficient sampling at a much lower cost.</p> <p>Additional work to add support for stratified sampling is also possible.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39066628</person_id>
				<author_profile_id><![CDATA[81322489561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Juan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Buhler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/Dream Works]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864645</person_id>
				<author_profile_id><![CDATA[81342515703]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wexler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Research and Development, PDI/Dream Works]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Merklinger, Harold M. 1997 A Technical View of Bokeh. <i>Photo Techniques Magazine</i>, May/June 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Zimmerman, Peter, article on bokeh. http://www.minox.org/minoxencyclopedia/b/bokeh.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wexler, Daniel. <i>Bokeh rendering</i>. http://www.flarg.com/bokeh.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>806818</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Potmesil, M. and Chakravarty, I. 1981. A lens and aperture camera model for synthetic image generation. In <i>Proceedings of ACM SIGGRAPH 1981</i>, 15 (3), pp. 297--305.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>357300</ref_obj_id>
				<ref_obj_pid>357299</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Potmesil, M. and Chakravarty, I. 1982. Synthetic Image Generation with a Lens and Aperture Model. In <i>Transactions on Graphics</i>, Vol 1 No 2.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242156</article_id>
		<sort_key>143</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[A semiotic approach to narrative manipulation]]></title>
		<page_from>143</page_from>
		<page_to>143</page_to>
		<doi_number>10.1145/1242073.1242156</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242156</url>
		<abstract>
			<par><![CDATA[<p>In the design of multimedia communication artifacts few, if any, tools support the early stage of a creative process: the heuristic project. In this work we give a proof of concept of an application addressed to a specific kind of heuristic project: given the logical sequence of episodes of a narrative, the <i>fabula</i>, the goal is to obtain different plots expressed in multi-modal language. The case study is provided by the task of transposing a written synopsis to the multi-modal language of a movie. We adopted the semiotic theory of Greimas to analyze the narrative and reveal its deep structure. The application enables users to interact with this structure in order to simulate and anticipate the effects of meaning resulting from their manipulation.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[multimedia communication artifacts]]></kw>
			<kw><![CDATA[narratology]]></kw>
			<kw><![CDATA[structural semiotics]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P188636</person_id>
				<author_profile_id><![CDATA[81100440757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Maria]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Alberti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universit&#224; degli Studi di Milano]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P451761</person_id>
				<author_profile_id><![CDATA[81100077622]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dario]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maggiorini]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universit&#224; degli Studi di Milano]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864740</person_id>
				<author_profile_id><![CDATA[81320495746]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Paola]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Trapani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Politecnico di Milano]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Anceschi, G. Monogrammi e figure. Teorie e storie della progettazione di artefatti comunicativi. La Casa Usher, Firenze, 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Greimas, A. J. Du sens. Seuil, Paris, 1970.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Propp, V. Morphology of the Folktale. University of Texas Press, Austin, 1968.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242157</article_id>
		<sort_key>144</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[A virtual reconstruction of the Cone sisters' apartments]]></title>
		<page_from>144</page_from>
		<page_to>144</page_to>
		<doi_number>10.1145/1242073.1242157</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242157</url>
		<abstract>
			<par><![CDATA[<p>This sketch presents how the real-time 3D interactive simulation, "The Virtual Tour of the Cone Sisters' Apartments", from the SIGGRAPH 02 Art Gallery exhibition was designed, authored and produced by the Imaging Research Center (IRC) at UMBC for two installations at the Baltimore Museum of Art (BMA).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35038485</person_id>
				<author_profile_id><![CDATA[81335496510]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Price]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Imaging Research Center, UMBC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28020887</person_id>
				<author_profile_id><![CDATA[81328487596]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bailey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Imaging Research Center, UMBC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Pair Et Al. 2000. <i>The NAVE: Design and Implementation of a Non-Expensive Immersive Virtual Environment</i>. In <i>Conference Abstracts and Applications</i>, ACM Siggraph 2000, 238.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242158</article_id>
		<sort_key>145</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Accurate image based re-lighting through optimization]]></title>
		<page_from>145</page_from>
		<page_to>145</page_to>
		<doi_number>10.1145/1242073.1242158</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242158</url>
		<abstract>
			<par><![CDATA[<p>Image-based relighting represents a class of techniques that apply new lighting conditions to a scene, given a set of basis images. In this sketch we present a relighting technique that, for a single viewpoint, accurately captures the reflectance field of objects, without restrictions on their geometrical complexity or material properties. Once the reflectance field is captured, the objects can be relit under arbitrary lighting conditions. To achieve such accurate results, our method combines the strengths of both the Light Stage [Debevec et al. 2000] and environment matting [Zongker et al. 1999; Chuang et al. 2000] into a single framework.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP40033142</person_id>
				<author_profile_id><![CDATA[81100016992]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pieter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[K. U. Leuven]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39030961</person_id>
				<author_profile_id><![CDATA[81100172909]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Philip]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dutr&#233;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[K. U. Leuven]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>344844</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chuang, Y.-Y., Zongker, D. E., Hindorff, J., Curless, B., Salesin, D. H., and Szeliski, R. 2000. Environment matting extensions: Towards higher accuracy and real-time capture. In <i>Siggraph 2000, Computer Graphics Proceedings</i>, K. Akeley, Ed., Annual Conference Series, ACM Siggraph, 121--130.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344855</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Debevec, P., Hawkins, T., Tchou, C., Duiker, H.-P., Sarokin, W., and Sagar, M. 2000. Acquiring the reflectance field of a human face. In <i>Siggraph 2000, Computer Graphics Proceedings</i>, K. Akeley, Ed., Annual Conference Series, ACM Siggraph, 145--156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311558</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Zongker, D. E., Werner, D. M., Curless, B., and Salesin., D. H. 1999. Environment matting and compositing. In <i>Sigggraph 1999, Computer Graphics Proceedings</i>, A. Rock-wood, Ed., Annual Conference Series, ACM Siggraph, 205--214.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242159</article_id>
		<sort_key>146</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[An inexpensive 3D camera]]></title>
		<page_from>146</page_from>
		<page_to>146</page_to>
		<doi_number>10.1145/1242073.1242159</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242159</url>
		<abstract>
			<par><![CDATA[<p>We describe the implementation of a portable 3D-camera prototype based on a consumer grade digital camera and an inexpensive laser raster generator. Such hand-held device can be used to capture smooth shapes by acquiring one or more images. Its output can be either a 3D wireframe or a textured model.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[3D camera]]></kw>
			<kw><![CDATA[3D shape acquisition]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>B.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010583</concept_id>
				<concept_desc>CCS->Hardware</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P864616</person_id>
				<author_profile_id><![CDATA[81100504887]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Askold]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Strat]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Symbol Technologies and SUNY at Stony Brook]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP36036591</person_id>
				<author_profile_id><![CDATA[81100516478]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Manuel]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Oliveira]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SUNY at Stony Brook]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242160</article_id>
		<sort_key>147</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Art and education using direct manipulation of a sensor array]]></title>
		<page_from>147</page_from>
		<page_to>147</page_to>
		<doi_number>10.1145/1242073.1242160</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242160</url>
		<abstract>
			<par><![CDATA[<p>We describe here two installations of direct manipulation systems in the art and education domain. In these installations we use a pressure sensitive computer-projected canvas for user manipulation. As the user presses the canvas, an art piece is created, or image layers are revealed.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P414108</person_id>
				<author_profile_id><![CDATA[81100179155]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Taly]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sharon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Laboratory, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Tekscan 2002, http://www.tekscan.com/industrial/system.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Slivovsky L. A. & Tan HZ. 2000, A real-time sitting posture tracking system, Purdue Univ. Technical Report, TR-ECE 00-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kapoor A., Mota S & Picard R. W. 2001, Towards a Learning Companion that Recognizes Affect, AAAI Fall Symposium.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242161</article_id>
		<sort_key>148</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[ASR]]></title>
		<subtitle><![CDATA[augmented sound reality]]></subtitle>
		<page_from>148</page_from>
		<page_to>148</page_to>
		<doi_number>10.1145/1242073.1242161</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242161</url>
		<abstract>
			<par><![CDATA[<p>This sketch describes the Mixed Reality application ASR (Augmented Sound Reality) which uses the overlay of virtual images on the real world to support the placement of three dimensional sound sources. Our system allows to place sound sources in a virtual or real room with the advantage of feeling, seeing and hearing them. This implies a more intuitive and better feeling of space and 3D sound.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28020722</person_id>
				<author_profile_id><![CDATA[81328488238]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dobler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fachhochschule Hagenberg (MTD)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37034496</person_id>
				<author_profile_id><![CDATA[81100048383]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Haller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fachhochschule Hagenberg (MTD)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28022045</person_id>
				<author_profile_id><![CDATA[81328490619]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Philipp]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stampfl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fachhochschule Hagenberg (MTD)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Creative. 2001. EAX 2.0. Tech. rep. http://developer.creative.com.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kato, H., Billinghurst, M., Blanding, B., and May, R. 1999. ARToolKit. Technical report, Hiroshima City University, December.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242162</article_id>
		<sort_key>149</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[Automatic generation of pencil drawing using LIC]]></title>
		<page_from>149</page_from>
		<page_to>149</page_to>
		<doi_number>10.1145/1242073.1242162</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242162</url>
		<abstract>
			<par><![CDATA[<p>Line Integral Convolution (LIC)[Cabral and Leedom 1993]] is a texture based vector field visualization technique. Why using LIC for pencil drawing generation? Let us look at the two images shown in Figure 1. Figure 1(a) is a digitized sample of a real pencil drawing. Look over it, we can perceive the traces of parallel pencil strokes and a gray scale tone built with the strokes. If we look at any local area of the image, however, we can find that the direction of strokes and the intensity of pixels vary randomly. The variance of intensity results from the interaction of lead material and drawing paper. The LIC image shown in Figure 1(b), however, presents the very similar features. Since an LIC image is obtained by low-pass filtering a white noise along the streamlines of a vector field, we can see traces along streamlines. On the other hand, the intensities of pixels within any local area vary randomly as the input image is a white noise. Such similarity suggests us that we can imitate the tone of pencil drawings with an LIC image.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP17013398</person_id>
				<author_profile_id><![CDATA[81100278242]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xiaoyang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yamanashi University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864795</person_id>
				<author_profile_id><![CDATA[81328489392]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yoshinori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nagasaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yamanashi University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P22828</person_id>
				<author_profile_id><![CDATA[81100135621]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Atsumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Imamiya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yamanashi University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>166151</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cabral B. and Leedom C. 1993, "Imaging Vector Field Using Line Integral Convolution", <i>SIGGRAPH93 conference Proceedings</i>, pages 263--270.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>558817</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Gooch A. and Gooch B. 2001, <i>Non-Photorealistic Rendering</i>, A. K. Peters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242163</article_id>
		<sort_key>150</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[Backseat gaming]]></title>
		<subtitle><![CDATA[augmented reality with speed]]></subtitle>
		<page_from>150</page_from>
		<page_to>150</page_to>
		<doi_number>10.1145/1242073.1242163</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242163</url>
		<abstract>
			<par><![CDATA[<p>This sketch presents a prototype developed as part of the Backseat gaming project. The aim of the project is to explore how to make use of mobile properties for developing compelling and fun game experiences. The prototype is developed for use in a highly mobile situation, that of a car passenger and is realized by the use of mobile devices and the users physical location during speed to merge the virtual content and surrounding road context into an augmented reality game. In this research, in addition to location, we also introduce variables such as speed, direction, timing, changing surrounding, fast movement of manipulative objects and multiple entry and exits.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28020619</person_id>
				<author_profile_id><![CDATA[81100284583]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Liselott]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brunnberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Interactive Institute, Stockholm, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP30040811</person_id>
				<author_profile_id><![CDATA[81100359615]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ollila]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Link&#246;ping, Norrk&#246;ping Campus, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242164</article_id>
		<sort_key>151</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[BioMorphic typography]]></title>
		<page_from>151</page_from>
		<page_to>151</page_to>
		<doi_number>10.1145/1242073.1242164</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242164</url>
		<abstract>
			<par><![CDATA[<p>BioMorphic Typography is a new conception of writing and a morphing typeface driven by biofeedback. It enables users to become aware of their autonomic physiological functions while they type, in real-time. In doing so, BioMorphic Typography seeks to challenge longstanding Western notions about the relationship among the senses, representation, and technology.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35038351</person_id>
				<author_profile_id><![CDATA[81100453293]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Diane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gromala]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Tech, Atlanta, GA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>521040</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Eisenstein, Elizabeth. <i>Printing Revolution as an Agent of Change</i>. Cambridge: Cambridge University Press, 1980.; Hayles, N. Katherine. <i>How We Became Posthuman: Virtual Bodies in Cybernetics, Literature, and Informantics</i> (Chicago: The University of Chicago Press, 1999).; McLuhan, Marshall. <i>Understanding Media</i>. Cambridge: The MITPress. <i>(1964)</i> Reprinted, 1994.; Ong, Walter J. <i>Orality & Literacy: The Technologizing of the Word</i>. Padstow: T.J. Press, 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>322594</ref_obj_id>
				<ref_obj_pid>320719</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lewis, Jason E., Weyers, Alex. "ActiveText: A Method for Creating Dynamic and Interactive Texts," in <i>UIST '99</i>, pp.131--140. One exception is <i>Beowulf;</i> though this font morphs, it is not interactive.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242165</article_id>
		<sort_key>152</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[Body building through weight training]]></title>
		<subtitle><![CDATA[using fitting techniques for skin animation]]></subtitle>
		<page_from>152</page_from>
		<page_to>152</page_to>
		<doi_number>10.1145/1242073.1242165</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242165</url>
		<abstract>
			<par><![CDATA[<p>This sketch describes a new method for deforming the skin of a digital character around its skeleton by computing statistical fit to an input <i>training exercise</i>. In this input, the skeleton and the skin move together, by arbitrary external means, through a range of motion representative of what the character is expected to achieve in practice. Using least-squares fitting techniques, we compute the coefficients, or "weights," of our new deformation equation. The result is that the equation, which is compact and efficient to evaluate, generalizes the motion represented in the input. Once the training process is complete, even characters with high levels of geometric detail can move at interactive frames rates.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28022234</person_id>
				<author_profile_id><![CDATA[81100219029]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xiaohuan]]></first_name>
				<middle_name><![CDATA[Corina]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light & Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31062339</person_id>
				<author_profile_id><![CDATA[81328489716]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Cary]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Phillips]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light & Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>344862</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lewis, J. P., Cordner, M., and Fong, N. 2000. Pose space deformation: A unified approach to shape interpolation and skeleton-driven deformation. In <i>Proceedings of SIGGRAPH 2000</i>, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM, 165--172.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364382</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sloan, P., Rose, C., and Cohen, M. 2001. Shape by example. In <i>Proceedings of the 2001 Symposium on Interactive 3D Graphics</i>, ACM Press / ACM SIGGRAPH, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242166</article_id>
		<sort_key>153</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Bringing computer graphics to everyday environments with informative art]]></title>
		<page_from>153</page_from>
		<page_to>153</page_to>
		<doi_number>10.1145/1242073.1242166</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242166</url>
		<abstract>
			<par><![CDATA[<p>The field of display technology is rapidly developing, and LCD-and plasma-displays are already invading our surroundings. Alternative technologies such as "electronic ink", electro-luminescent materials, and even color-changing textiles [Holmquist and Melin 2001] will further increase the number of possibilities to integrate computer graphics in our everyday lives. We believe that computer graphics for everyday life will have requirements that are very different from those of a Web page or a movie special effect. To explore this, we have developed a type of applications that anticipates a future use of computer graphics, so-called <i>Informative Art</i>.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31062506</person_id>
				<author_profile_id><![CDATA[81100413870]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tobias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Skog]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PLAY Studio, Interactive Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P507594</person_id>
				<author_profile_id><![CDATA[81100194686]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sara]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ljungblad]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Future Applications Lab, Viktoria Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P168847</person_id>
				<author_profile_id><![CDATA[81100144729]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Lars]]></first_name>
				<middle_name><![CDATA[Erik]]></middle_name>
				<last_name><![CDATA[Holmquist]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Future Applications Lab, Viktoria Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Holmquist, L. E. and Melin, L. 2001. Using Color-Changing Textiles as a Computer Graphics Display. <i>Conference Abstracts and Applications of SIGGRAPH 2001</i>, ACM Press / ACM SIGGRAPH, New York, 272.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Skog, T., Holmquist, L. E., Redstr&#246;m, J and Halln&#228;s, L. 2001. Informative Art. <i>Conference Abstracts and Applications of SIGGRAPH 2001</i>, ACM Press / ACM SIGGRAPH, New York, 124.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242167</article_id>
		<sort_key>154</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[Calming visual spaces]]></title>
		<subtitle><![CDATA[learning from Kyoto Zen gardens]]></subtitle>
		<page_from>154</page_from>
		<page_to>154</page_to>
		<doi_number>10.1145/1242073.1242167</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242167</url>
		<abstract>
			<par><![CDATA[<p>Zen gardens exhibit sophisticated visual designs achieved with minimal compositions and engender a calm, contemplative atmosphere. Here we use perceptual models to study Zen garden design with the aim of discovering guidelines for the creation of calm visual spaces.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP48024846</person_id>
				<author_profile_id><![CDATA[81100493577]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Lyons]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Media Information Science]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864663</person_id>
				<author_profile_id><![CDATA[81324494719]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gert]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Van Tonder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyoto University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864677</person_id>
				<author_profile_id><![CDATA[81328490256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shortreed]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR M.I.S.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P423255</person_id>
				<author_profile_id><![CDATA[81100105873]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Nobuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tetsutani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR M.I.S.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Shortreed, I. 2001. <i>Kyoto Gardens: A virtual stroll through Zen landscapes</i>. Mercury Software CD-ROM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242168</article_id>
		<sort_key>155</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[Cameras and point-of-view in the gamespace]]></title>
		<page_from>155</page_from>
		<page_to>155</page_to>
		<doi_number>10.1145/1242073.1242168</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242168</url>
		<abstract>
			<par><![CDATA[<p>This technical sketch surveys the wide variety of projection techniques that are employed in today's video and computer games. We will discuss the evolution of the "game view" and the reasons behind the choice of perspective, some of which are practical, while others are legacies. We will also explore the limits inherent in choosing a particular technique.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864682</person_id>
				<author_profile_id><![CDATA[81328490117]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jay]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Riddle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242169</article_id>
		<sort_key>156</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>24</seq_no>
		<title><![CDATA[Challenges of the homeland pan in "Spirit"]]></title>
		<page_from>156</page_from>
		<page_to>156</page_to>
		<doi_number>10.1145/1242073.1242169</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242169</url>
		<abstract>
			<par><![CDATA[<p>The opening shot in "Spirit" runs a full 3 minutes without scene cuts, and introduces the audience to the breathtaking scenery of the mythic Old West. This elaborate scene was accomplished with every possible technique by mixing traditional drawing and painting art forms with 3D environments, effects, and digital characters. This Sketch will attempt to summarize some of the key challenges in producing this scene, which took over two years to complete.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28020583</person_id>
				<author_profile_id><![CDATA[81328487742]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Doug]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cooper]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242170</article_id>
		<sort_key>157</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>25</seq_no>
		<title><![CDATA[Color transformation based on the basic color categories of a painting]]></title>
		<page_from>157</page_from>
		<page_to>157</page_to>
		<doi_number>10.1145/1242073.1242170</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242170</url>
		<abstract>
			<par><![CDATA[<p>Each painter renders a painting in her own style. This style can be distinguished by looking at elements such as motif, color, shape deformation and texture. Previously, [Hertzmann et al. 2001] suggested a method for applying the texture of an image to a photograph. In this paper, we will focus on the element of "color".</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14095277</person_id>
				<author_profile_id><![CDATA[81100248575]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Youngha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35032784</person_id>
				<author_profile_id><![CDATA[81100652296]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Suguru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Saito]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology, Precision and Intelligence Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39042929</person_id>
				<author_profile_id><![CDATA[81100441141]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Masayuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakajima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Berlin, B., and Kay, P. 1969. <i>Basic Color Terms: Their Universality and Evolution</i>. University of California Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383295</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hertzmann, A., Jacobs, C. E., Oliver, N., Curless, B., and Salesin, D. H. 2001. Image analogies. In <i>SIGGRAPH2001 Proceedings</i>, ACM, 327--340.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618848</ref_obj_id>
				<ref_obj_pid>616072</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Reinhard, E., Ashikhmin, M., Gooch, B., and Shirley, P. 2001. Color transfer between images. In <i>Computer Graphics and Applications</i>, IEEE, 34--41.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Uchikawa, K., Kuriki, I., and Shinoda, H. 1994. Expression of color appearance in aperture and surface color modes with a category rating estimation method, <i>J. Ilium. Engng. Inst. Jpn 78</i>, 2, 41--51.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242171</article_id>
		<sort_key>158</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>26</seq_no>
		<title><![CDATA[Computer generated clay animation]]></title>
		<page_from>158</page_from>
		<page_to>158</page_to>
		<doi_number>10.1145/1242073.1242171</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242171</url>
		<abstract>
			<par><![CDATA[<p>In this sketch, we propose a new technique for computer generated clay animation. Unlike the traditional approaches based on physical simulations, we focus on generating various animation effects produced by the clay animator.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[clay animation]]></kw>
			<kw><![CDATA[deformation]]></kw>
			<kw><![CDATA[motion effects]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Hierarchy and geometric transformations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010244</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Hierarchical representations</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P864640</person_id>
				<author_profile_id><![CDATA[81328488570]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daisuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39066832</person_id>
				<author_profile_id><![CDATA[81451595006]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Junichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoshino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba/PRESTO, JST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>193633</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Shimada, K., MA 1993. Automated Trangulation of Surfaces and Volumes via Bubble Packing. Ph.D.thesis, Massachusetts Institute of Technology, Cambridge, MA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[W. Welch, and A. Witkin, 1992. Variational surface nokeling, Computer Graphics and Applications, vol. 17, no. 5, pp. 40--46.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242172</article_id>
		<sort_key>159</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>27</seq_no>
		<title><![CDATA[Computer graphics to illustrate the development of a human embryo for professional medical education]]></title>
		<page_from>159</page_from>
		<page_to>159</page_to>
		<doi_number>10.1145/1242073.1242172</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242172</url>
		<abstract>
			<par><![CDATA[<p>This sketch describes three dimensional (3D) computer graphics (CG) produced to illustrate the development of a human embryo for education in embryology, which is one of the basic subjects in professional medical education (Fig. 1). Although similar CG have already been produced for TV programs, they are insufficient in precision for professional medical education.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P162584</person_id>
				<author_profile_id><![CDATA[81100383517]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Koh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakusho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyoto University, Kyoto, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864797</person_id>
				<author_profile_id><![CDATA[81328489609]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yutaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minekura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyoto University, Kyoto, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39024244</person_id>
				<author_profile_id><![CDATA[81100036035]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michihiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minoh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyoto University, Kyoto, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37034549</person_id>
				<author_profile_id><![CDATA[81328489411]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mizuta]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyoto University, Kyoto, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864783</person_id>
				<author_profile_id><![CDATA[81328489440]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Tomoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakatsu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyoto University, Kyoto, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864707</person_id>
				<author_profile_id><![CDATA[81328490358]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Kohei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shiota]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyoto University, Kyoto, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[B. R. Smith, D. S. Huff, and G. A. Johnson. 1999. Magnetic resonance imaging of embryos: An internet resource for the study of embryonic development. <i>Computerized Medical Imaging and Graphics 23</i>, 33--40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[R. O'Rahilly, and F. Muller. 1987. <i>Developmental Stages in Human Embryos</i>. Carnegie Institution of Washington.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242173</article_id>
		<sort_key>160</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>28</seq_no>
		<title><![CDATA[Creating 3D painterly environments for Disney's "Treasure Planet"]]></title>
		<page_from>160</page_from>
		<page_to>160</page_to>
		<doi_number>10.1145/1242073.1242173</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242173</url>
		<abstract>
			<par><![CDATA[<p>Since the first color short, Walt Disney animated films have been known for their beautiful and breathtaking environments that seamlessly integrate with the hand drawn characters that populate them. Over the years, technological innovations have streamlined the process of creating these environments while still maintaining the extremely high artistic standards that have made our films famous. Other advances, like the multi-plane camera, have helped to revolutionize how the camera moves through these environments. In general, such technological innovations are inspired by an artistic need which must be achieved with out being prohibitively expensive. On Walt Disney's "Treasure Planet", new innovations to the DeepCanvas renderer were inspired by such artistic needs and desires.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864712</person_id>
				<author_profile_id><![CDATA[81328489991]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kyle]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Odermatt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28022384</person_id>
				<author_profile_id><![CDATA[81320495035]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Springfield]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242174</article_id>
		<sort_key>161</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>29</seq_no>
		<title><![CDATA[Creation of a photo-real CG human]]></title>
		<page_from>161</page_from>
		<page_to>161</page_to>
		<doi_number>10.1145/1242073.1242174</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242174</url>
		<abstract>
			<par><![CDATA[<p>Our team was tasked with the project of creating a full-screen, completely believable, CG stunt double for a well-known motion picture star. Successful project completion required adapting and applying many years worth of computer graphics research in a new and novel way. Most importantly, development had to be done within usual production constraints---ultimately guaranteed results, known pipeline, quick turnaround on tests, and a final product that would rival any work in this genre ever attempted.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864629</person_id>
				<author_profile_id><![CDATA[81328488485]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goldberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>344855</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Debevec, P., Hawkins, T., Tchou, C., Duiker, H., Sarokin, W. and Sagar, M. 2000. Acquiring the reflectance field of a human face. <i>Proceedings of ACM SIGGRAPH 2000</i>, 145--156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166139</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hanrahan, P., and Krueger, W. 1993. Reflection from layered surfaces due to subsurface scattering. <i>Proceedings of ACM SIGGRAPH 1993</i>, 165--174.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Henyey, L. G. and Greenstein, J. L. 1941. Diffuse radiation in the galaxy, <i>Astrophysical Journal 93</i>, 70--83.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242175</article_id>
		<sort_key>162</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>30</seq_no>
		<title><![CDATA[CT (city tomography)]]></title>
		<page_from>162</page_from>
		<page_to>162</page_to>
		<doi_number>10.1145/1242073.1242175</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242175</url>
		<abstract>
			<par><![CDATA[<p>CT is a project to reconstruct an existing urban space as a 3D information city on the web. A visitor can browse the city with "building wall browsers" and communicate with other visitors.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[XVL]]></kw>
			<kw><![CDATA[city tomography]]></kw>
			<kw><![CDATA[geographical structure]]></kw>
			<kw><![CDATA[information city]]></kw>
			<kw><![CDATA[space communication]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P636439</person_id>
				<author_profile_id><![CDATA[81100316016]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fumio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matsumoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Plannet Architectures]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P11884</person_id>
				<author_profile_id><![CDATA[81100476406]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Akira]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wakita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sphere System Design]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Matsumoto, F., and Matsukawa, S. 2000. Ginga. In <i>Conference Abstracts and Applications of SIGGRAPH 2000</i>, ACM Press/ACM SIGGRAPH, 209.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>330174</ref_obj_id>
				<ref_obj_pid>330160</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Wakita, A., Yajima, M., Harada, T., Toriya, H., and Chiyokura, H. 2000. Xvl: A compact and qualified 3d representation with lattice mesh and surface for the internet. <i>Web3D-VRML 2000</i>, 45--51.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242176</article_id>
		<sort_key>163</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>31</seq_no>
		<title><![CDATA[Curvature driven sampling of displacement maps]]></title>
		<page_from>163</page_from>
		<page_to>163</page_to>
		<doi_number>10.1145/1242073.1242176</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242176</url>
		<abstract>
			<par><![CDATA[<p>Displacement Mapping [Cook 1984] is commonly used in commercial rendering software for adding surface detail. In contrast to Bump Mapping not only the normal is perturbed, the surface is modified as well. A given point <b>P</b> on a surface is displaced to a new point <b>P'</b> in the direction of the surface normal of the point <b>P</b>, <b>N</b> scaled with the value stored in the Displacement Map:</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P142584</person_id>
				<author_profile_id><![CDATA[81100077515]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Johannes]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirche]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[WSI/GRIS University of T&#252;bingen, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P679017</person_id>
				<author_profile_id><![CDATA[81100343384]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alexander]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ehlert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[WSI/GRIS University of T&#252;bingen, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>808602</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cook, R. L. 1984. Shade Trees. <i>Computer Graphics (Proceedings of SIGGRAPH 84) 18</i>, 3, 223--231. Held in Minneapolis, Minnesota.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>348220</ref_obj_id>
				<ref_obj_pid>346876</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Doggett, M., and Hirche, J. 2000. Adaptive view dependent tessellation of displacement maps. In <i>Proc. of Eurographics/SIGGRAPH workshop on graphics hardware 2000</i>, 59--66.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242177</article_id>
		<sort_key>164</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>32</seq_no>
		<title><![CDATA[Custom designs for digital imaging on textiles]]></title>
		<page_from>164</page_from>
		<page_to>164</page_to>
		<doi_number>10.1145/1242073.1242177</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242177</url>
		<abstract>
			<par><![CDATA[<p>In working with custom designs for digital imaging on textiles, source images and garment patterns are scanned and the images mapped to fit the pattern design in Adobe Photoshop. For these pieces the images were scanned on the Eclipse 48" x 96" flatbed scanner.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864627</person_id>
				<author_profile_id><![CDATA[81328489140]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bonny]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lhotka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Boulder, CO]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242178</article_id>
		<sort_key>165</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>33</seq_no>
		<title><![CDATA[Depth-complexity based occluder selection]]></title>
		<page_from>165</page_from>
		<page_to>165</page_to>
		<doi_number>10.1145/1242073.1242178</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242178</url>
		<abstract>
			<par><![CDATA[<p>Densely occluded regions containing many stacked objects along the line of view generally show a high local depth-complexity (DC). With respect to occlusion culling such regions require dense occluder-sets, in order to provide gap free occlusion. Using pure size or distance based selection heuristics fail to consider such regions. This sketch presents a novel dynamic occluder selection approach, which selects occluders that cover regions of high DC. Depth-Complexity Based Occluder Selection [Kurka 2001] is based on the evaluation of a low resolution (128x128 pixel) raw scene depiction (RSD), which is rendered once per frame by a simple but fast image-based rendering (IBR) technique.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28030759</person_id>
				<author_profile_id><![CDATA[81341492539]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gerhard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kurka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University, Linz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kurka, G. 2001. <i>Bildbasierte Auswahl verdeckender Objekte (image-based occluder selection)</i>. PhD thesis (205 pages, German), Johannes Kepler University Linz, Austria.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258781</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Zhang, H., Manocha, D., Hudson, T., Hoff, E. K. 1997. Visibility Culling using Hierarchical Occlusion Maps. In <i>Proceedings of SIGGRAPH 1997</i>, Addison Wesley. Whitted, T., Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 77--88.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192262</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Rohlf, J., Helman, J. 1994. IRIS performer: a high performance multiprocessing toolkit for real-time 3D graphics. In <i>Proceedings of SIGGRAPH 1994</i>, ACM Press / ACM SIGGRAPH, New York. Glasner, A., Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 381--395.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242179</article_id>
		<sort_key>166</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>34</seq_no>
		<title><![CDATA[Detail calibration for out-of-core model simplification through interlaced sampling]]></title>
		<page_from>166</page_from>
		<page_to>166</page_to>
		<doi_number>10.1145/1242073.1242179</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242179</url>
		<abstract>
			<par><![CDATA[<p>Instead of generating an initial in-core model in first pass and performing a second pass adaptive processing over the original model according to the detail analysis of the in-core model, in this paper we propose a single-pass algorithm to realize detail calibration by introducing a scheme of interlaced sampling in order to obtain higher efficiency.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP311133900</person_id>
				<author_profile_id><![CDATA[81538398256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Guangzheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fei]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Geneva]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40029497</person_id>
				<author_profile_id><![CDATA[81100651801]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Nadia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Magnenat-Thalmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Geneva]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P790484</person_id>
				<author_profile_id><![CDATA[81314488488]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kangying]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chinese Academy of Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40033492</person_id>
				<author_profile_id><![CDATA[81100657893]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Enhua]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Macao]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fei, G., Cai, K., Guo, B., and Wu, E. An adaptive sampling scheme for out-of-core simplification. <i>Computer Graphics Forum, to appear</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344912</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lindstrom, P. 2000. Out-of-core simplification of large polygonal models. In <i>Proceedings of SIGGRAPH 2000</i>, Computer Graphics Proceedings, Annual Conference Series, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>601690</ref_obj_id>
				<ref_obj_pid>601671</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Shaffer, E., and Garland, M. 2001. Efficient adaptive simplification of massive meshes. In <i>Proceedings of IEEE Visualiztion 2001</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242180</article_id>
		<sort_key>167</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>35</seq_no>
		<title><![CDATA[Digital pyro for <i>Reign of Fire</i>]]></title>
		<page_from>167</page_from>
		<page_to>167</page_to>
		<doi_number>10.1145/1242073.1242180</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242180</url>
		<abstract>
			<par><![CDATA[<p>Our goal was to create realistic digital pyrotechnic effects for the film "Reign of Fire". The specific effects we needed were fire, smoke, fog, flying embers and dust kicked up from dragons flying overhead. We also needed the effects to interact with the environment as well as the dragons. To make the challenge more difficult, we realized that it wasn't good enough to create real world physical interactions; the film needed imagery that was larger than life and exagerated. We needed to be able to art direct the simulations.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28020749</person_id>
				<author_profile_id><![CDATA[81100572197]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dalton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Secret Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864750</person_id>
				<author_profile_id><![CDATA[81328490049]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Rob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rosenblum]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Secret Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864771</person_id>
				<author_profile_id><![CDATA[81328488926]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shyh-Chyuan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Secret Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP30040770</person_id>
				<author_profile_id><![CDATA[81328489170]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Lawrence]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Secret Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28021103</person_id>
				<author_profile_id><![CDATA[81328488131]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Driskill]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Secret Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>258838</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Foster, N. and Metaxas, D. 1997. Modeling the Motion of a Hot, Turbulent Gas. In <i>Proceedings of ACM SIGGRAPH 1997</i>, pp. 181--188.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383260</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Fedkiw, R., Stam, J., and Jensen, H. W. 2001. Visual Simulation of Smoke. In <i>Proceedings of ACM SIGGRAPH 2001</i>, pp. 15--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lamorlette et al. 2001. Shrek: The Story Behind The Screen, <i>SIGGRAPH 2001</i> Course 19.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242181</article_id>
		<sort_key>168</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>36</seq_no>
		<title><![CDATA[Diminishing head-mounted display for shared augmented reality]]></title>
		<page_from>168</page_from>
		<page_to>168</page_to>
		<doi_number>10.1145/1242073.1242181</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242181</url>
		<abstract>
			<par><![CDATA[<p>Eye-contact plays a very important role in the face-to-face human communication. In the shared augmented reality space, however, the eye-contact between two people is blocked by the head-mounted displays. An HMD is a necessary device for merging the virtual and real worlds to realize the augmented/mixed reality. [Ohta and Tamura 1999] As a side effect, it covers the user eyes. In a shared AR/MR space, multiple users want to share the real world as well as the virtual objects. However, the eye-contact information in the real world is dropped because of the HMDs.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P571203</person_id>
				<author_profile_id><![CDATA[81320495374]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Masayuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba, Tsukuba, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P307848</person_id>
				<author_profile_id><![CDATA[81100136649]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ohta]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba, Tsukuba, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>553930</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ohta, Y. and Tamura, H. Eds: Mixed Reality, Merging real and virtual worlds, Splinger-Verlag, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
&#60;b&#62;Demonstration Video:&#60;/b&#62; http://www.image.esys.tsukuba.ac.jp/~takemura/LAB/study/siggraph/index.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242182</article_id>
		<sort_key>169</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>37</seq_no>
		<title><![CDATA[Diorama engine]]></title>
		<subtitle><![CDATA[a 3D video storyboard editor for 3D computer animation]]></subtitle>
		<page_from>169</page_from>
		<page_to>169</page_to>
		<doi_number>10.1145/1242073.1242182</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242182</url>
		<abstract>
			<par><![CDATA[<p>In this sketch, we will demonstrate a video storyboard tool targeted for 3D computer animation. Our tool provides limited but specialized functions of standard 3D computer graphics software, focusing on ease of scene construction, camera control and the ability to preview in realtime. This allows for quicker and easier creation of video storyboards over existing approaches.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864708</person_id>
				<author_profile_id><![CDATA[81328489444]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Koji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mikami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864784</person_id>
				<author_profile_id><![CDATA[81328490628]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Toru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tokuhara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864734</person_id>
				<author_profile_id><![CDATA[81328488662]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mitsuru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaneko]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242183</article_id>
		<sort_key>170</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>38</seq_no>
		<title><![CDATA[DIVIPRO]]></title>
		<subtitle><![CDATA[Distributed Interactive VIrtual PROtotoyping]]></subtitle>
		<page_from>170</page_from>
		<page_to>170</page_to>
		<doi_number>10.1145/1242073.1242183</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242183</url>
		<abstract>
			<par><![CDATA[<p>DIVIPRO is a prototype tool for simulating the assembly and disassembly of mechanical engineering components. It is aimed particularly at situations where the responsibility for decision making is shared between geographically dispersed design teams, and provides a collaborative environment in which different team members concurrently visualize and manipulate models. It is currently being evaluated using models from the aerospace and medical industries.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP38024227</person_id>
				<author_profile_id><![CDATA[81408602556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mashhuda]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Glencross]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Manchester, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14117370</person_id>
				<author_profile_id><![CDATA[81100322829]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marsh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Manchester, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P146848</person_id>
				<author_profile_id><![CDATA[81100111573]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cook]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Manchester, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P552644</person_id>
				<author_profile_id><![CDATA[81100260318]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Sylvain]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Daubrenet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Manchester, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P269439</person_id>
				<author_profile_id><![CDATA[81100508955]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pettifer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Manchester, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P247930</person_id>
				<author_profile_id><![CDATA[81100561115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Roger]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hubbold]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Manchester, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>502397</ref_obj_id>
				<ref_obj_pid>502390</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Pettifer, S., Cook, J., Marsh, J., and West, A. 2000. Deva3: Architecture for a large scale virtual reality system. In <i>Proceedings of ACM Symposium in Virtual Reality Software and Technology</i>, ACM Press, 33--39.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242184</article_id>
		<sort_key>171</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>39</seq_no>
		<title><![CDATA[DocuDrama conversations]]></title>
		<page_from>171</page_from>
		<page_to>171</page_to>
		<doi_number>10.1145/1242073.1242184</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242184</url>
		<abstract>
			<par><![CDATA[<p>This sketch describes DocuDrama, a tool that offers a generation of interactive narratives that are based on activities in a collaborative virtual environment. DocuDrama [Sch&#228;fer et al. 2001] is built as part of TOWER [2002], a Theatre of Work Enabling Relationships, which allows project members to be aware of project relevant activities as well as to establish and maintain the social relationships that intensify team coherence.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P170885</person_id>
				<author_profile_id><![CDATA[81100656135]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Leonie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sch&#228;fer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer Institute, FIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P76184</person_id>
				<author_profile_id><![CDATA[81100078614]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Elaine]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Raybourn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sandia National Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P15820</person_id>
				<author_profile_id><![CDATA[81100610695]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Amanda]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Oldroyd]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[BTexact Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>713638</ref_obj_id>
				<ref_obj_pid>646956</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Sch&#228;fer, L., Prinz, W., Oldroyd, A., Gavin, L., 2001. <i>Virtual Storytelling of Cooperative Activities in a Theatre of Work</i>. In <i>Proceedings of the International Conference ICVS 2001: Virtual Storytelling</i>, Balet, O.; Subsol, G.; Torguet, P. (ed.), Berlin: Springer, 191--200.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tower Project, 2002. http://tower.gmd.de/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[McCloud, S. 1993. <i>Understanding Comics</i>. Kitchen Sink Press, Northampton, MA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242185</article_id>
		<sort_key>172</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>40</seq_no>
		<title><![CDATA[Dragon scales]]></title>
		<subtitle><![CDATA[the evolution of scale tool for <i>Reign of Fire</i>]]></subtitle>
		<page_from>172</page_from>
		<page_to>172</page_to>
		<doi_number>10.1145/1242073.1242185</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242185</url>
		<abstract>
			<par><![CDATA[<p>In recent years there have been several movies starring creatures with scaled surfaces. Among these are <i>Jurassic Park, Dragonheart</i>, and <i>Lake Placid</i>. The surfaces of these creatures have generally been constructed by layering painted textures atop displacement maps. This gives the model texture, but the scales stretch and shrink under the movement of the creature, giving a rubbery look that is not realistic.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864657</person_id>
				<author_profile_id><![CDATA[81100305675]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ernest]]></first_name>
				<middle_name><![CDATA[J]]></middle_name>
				<last_name><![CDATA[Petti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864778</person_id>
				<author_profile_id><![CDATA[81328490545]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[V]]></middle_name>
				<last_name><![CDATA[Thompson]]></last_name>
				<suffix><![CDATA[II]]></suffix>
				<affiliation><![CDATA[Walt Disney Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864609</person_id>
				<author_profile_id><![CDATA[81328489617]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Adolph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lusinsky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Secret Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28021108</person_id>
				<author_profile_id><![CDATA[81328488131]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Driskill]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Secret Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242186</article_id>
		<sort_key>173</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>41</seq_no>
		<title><![CDATA[Drawing with feeling]]></title>
		<subtitle><![CDATA[designing tactile display for pen]]></subtitle>
		<page_from>173</page_from>
		<page_to>173</page_to>
		<doi_number>10.1145/1242073.1242186</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242186</url>
		<abstract>
			<par><![CDATA[<p>There is a sense of satisfaction using a pen or pencil to write or draw. Feeling the imperfections on the paper as the pen tip moves over it and observing marks emerge create the inherently physical and intimate feeling of drawing. Our extensive discussions with artists and designers suggest that this physicality and intimacy with drawing not only brings enjoyment, but also perhaps assist in the artist's creative process.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P117489</person_id>
				<author_profile_id><![CDATA[81100593278]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ivan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Poupyrev]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Interaction Lab, Sony CSL]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P345093</person_id>
				<author_profile_id><![CDATA[81100537957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shigeaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maruyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Semiconductor Network Company]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383313</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baxter, B., et al. DAB: Interactive Haptic Painting with 3D Virtual Brushes, <i>Proceedings of SIGGRAPH 2001</i>, pp 461--468]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242187</article_id>
		<sort_key>174</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>42</seq_no>
		<title><![CDATA[Dynamic simulation of wing motion on "Reign of Fire"]]></title>
		<page_from>174</page_from>
		<page_to>174</page_to>
		<doi_number>10.1145/1242073.1242187</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242187</url>
		<abstract>
			<par><![CDATA[<p>The challenge of animating believable dragons was presented to Disney Feature Animation and The Secret Lab (TSL) for the film "Reign of Fire". The film called for 100ft creatures with wing spans of 300ft that could undergo enormous speeds and accelerations. The artistic direction required each dragon to have wings that transition between a variety of physical behaviors and interact with the environment. To solve this challenge the Muscle and Skin system used in Disney's Dinosaur [Eskuri 2000] was extended with a variety of controls to do physical simulation. In this sketch we discuss some of the issues encompassing the creation of this simulator and give an overview of the successful and unsuccessful paths taken during its development.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31062121</person_id>
				<author_profile_id><![CDATA[81100124724]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carlos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gonzalez-Ochoa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28021116</person_id>
				<author_profile_id><![CDATA[81100652943]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eberle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864749</person_id>
				<author_profile_id><![CDATA[81328488007]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Rob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dressel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Eskuri, N. 2000. The art and technology of disney's dinosaur. ACM Press, ACM Siggraph 2000 Course Notes, ACM Siggraph.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Provot, X. 1995. Deformation constraints in a mass-spring model to describe rigid cloth behavior. <i>Graphics Interface</i>, 147--154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Provot, X. 1997. Collision and self-collision handling in cloth model dedicated to design garments. <i>Graphics Interface</i>, 177--189.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242188</article_id>
		<sort_key>175</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>43</seq_no>
		<title><![CDATA[Dynamic skin deformation and animation controls using maya cloth for facial animation]]></title>
		<page_from>175</page_from>
		<page_to>175</page_to>
		<doi_number>10.1145/1242073.1242188</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242188</url>
		<abstract>
			<par><![CDATA[<p>This sketch presents a solution for creating expressive computer facial animation using off-the-shelf software. We will describe how to apply dynamic skin deformation on a CG character using Maya Cloth and how to create artist-friendly animation controls. Historically, facial animation with sophisticated skin deformation based on physical characteristics of skin could only be done using proprietary software in big studios, which are not accessible to small studios, independent animators and students. The solution that we have developed is easy to understand and does not involve any programming knowledge.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28021049</person_id>
				<author_profile_id><![CDATA[81320488877]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jimmy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MFA Computer Art]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP33031950</person_id>
				<author_profile_id><![CDATA[81328488929]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hyunsuk]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MFA Computer Art]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>249651</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Parke, F. I., and Waters, K. 1996. <i>Computer Facial Animation</i>. A. K. Peters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Larrabee, W. 1986. A finite element model of skin deformation I: Biomechanics of skin and soft tissue: A review. <i>Laryngoscope</i>, vol. 96, 399--405]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242189</article_id>
		<sort_key>176</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>44</seq_no>
		<title><![CDATA[Dynamics and dodos]]></title>
		<subtitle><![CDATA[rigging and animation methods for Ice Age]]></subtitle>
		<page_from>176</page_from>
		<page_to>176</page_to>
		<doi_number>10.1145/1242073.1242189</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242189</url>
		<abstract>
			<par><![CDATA[<p>This sketch explains how the Animation Team at Blue Sky Studios executed a challenging sequence for "Ice Age" and describes in detail a particularly useful animation software tool called <i>followThrough</i>.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864608</person_id>
				<author_profile_id><![CDATA[81328487819]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Burr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blue Sky Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864758</person_id>
				<author_profile_id><![CDATA[81328490366]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ross]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Scroble]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blue Sky Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242190</article_id>
		<sort_key>177</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>45</seq_no>
		<title><![CDATA[Editable dynamic textures]]></title>
		<page_from>177</page_from>
		<page_to>177</page_to>
		<doi_number>10.1145/1242073.1242190</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242190</url>
		<abstract>
			<par><![CDATA[<p>This technical sketch presents a simple and efficient algorithm for editing realistic sequences of images of dynamic scenes that exhibit some form of temporal regularity. Such scenes include flowing water, steam, smoke, flames, foliage of trees in wind, crowds, dense traffic flow etc. We call this kind of scenes <i>dynamic textures</i>.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP17000202</person_id>
				<author_profile_id><![CDATA[81100067978]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gianfranco]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Doretto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UCLA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39028692</person_id>
				<author_profile_id><![CDATA[81100126437]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Stefano]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Soatto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UCLA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>344880</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Popovi&#263;, J., Seitz, S. M., Erdmann, M., Popovi&#263;, Z., and Witkin, A. 2000. Interactive manipulation of rigid body simulations. In <i>Proceedings of SIGGRAPH 2000</i>, 209--218.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345012</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sch&#246;dl, A., Szeliski, R., Salesin, D. H., and Essa, I. 2000. Video textures. In <i>Proceedings of SIGGRAPH 2000</i>, 489--498.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Soatto, S., Doretto, G., and Wu, Y. 2001. Dynamic textures. In <i>Proceedings of IEEE International Conference on Computer Vision</i>, vol. 2, 439--446.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242191</article_id>
		<sort_key>178</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>46</seq_no>
		<title><![CDATA[Embodied interaction]]></title>
		<page_from>178</page_from>
		<page_to>178</page_to>
		<doi_number>10.1145/1242073.1242191</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242191</url>
		<abstract>
			<par><![CDATA[<p>Embodied Interaction, explores the components of interactive media, with a primary focus on the role that interaction design has on virtual experiences. My electronic artwork is concerned with the transformation of the human species, specifically its biological components and its behavioral characteristics. This transformation or evolution is an environmental reaction to the manifestations of science and technology. This presentation examines the need to address the physical body and how the action of users needs to be interconnected with the interface and content of a interactive piece. From the development of opaque sculptural input devices to the use of transparent technologies my interactive installations seeks to examine the process of conditioning users; their predetermined interaction; and the physicality of computing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP38029049</person_id>
				<author_profile_id><![CDATA[81539038556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hill]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Jacksonville University, Jacksonville, FL]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242192</article_id>
		<sort_key>179</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>47</seq_no>
		<title><![CDATA[Evolution of a VFX voxel tool]]></title>
		<page_from>179</page_from>
		<page_to>179</page_to>
		<doi_number>10.1145/1242073.1242192</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242192</url>
		<abstract>
			<par><![CDATA[<p>High-resolution voxel rendering is no longer the pipe dream it once was. In the age of multi-gigabyte RAM, voxels are finding an ever-increasing role in creating cutting-edge visual effects. Digital Domain has developed a unique, animator-friendly way of working with voxels that is revolutionizing the way volumetric effects are done.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35038389</person_id>
				<author_profile_id><![CDATA[81100036373]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kapler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242193</article_id>
		<sort_key>180</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>48</seq_no>
		<title><![CDATA[Example-based interpolation of human motion]]></title>
		<page_from>180</page_from>
		<page_to>180</page_to>
		<doi_number>10.1145/1242073.1242193</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242193</url>
		<abstract>
			<par><![CDATA[<p>Generating realistic human motions from sparse pose descriptions are important for computer animation and virtual human applications. In this sketch, we propose an example-based motion interpolation technique using independent component analysis (ICA). The proposed method is also useful for human motion conversion, motion blending, and converting symbolic descriptions to continuous motion.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[ICA]]></kw>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[character behavior]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>E.4</cat_node>
				<descriptor>Data compaction and compression</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.3</cat_node>
				<descriptor>Time series analysis</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Performing arts (e.g., dance, music)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10002952.10002971.10003451.10002975</concept_id>
				<concept_desc>CCS->Information systems->Data management systems->Data structures->Data layout->Data compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003688.10003693</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Statistical paradigms->Time series analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>PP28021831</person_id>
				<author_profile_id><![CDATA[81100501102]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P470682</person_id>
				<author_profile_id><![CDATA[81100230368]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jun'ichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoshino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba / PRESTO, JST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2326422</ref_obj_id>
				<ref_obj_pid>2325767</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[A. Hyvarinen: Fast and Robust Fixed-Point Algorithms for Independent Component Analysis, IEEE Trans. on Neural Networks, 1999]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Junichi Hoshino, Yumi Hoshino: Intelligent Storyboard for Prototyping Animation, IEEE Int. Conf on Multimedia and Expo, ICME2001, Conference CD-ROM FAI.03, 2001]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242194</article_id>
		<sort_key>181</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>49</seq_no>
		<title><![CDATA[Exploiting screen space]]></title>
		<page_from>181</page_from>
		<page_to>181</page_to>
		<doi_number>10.1145/1242073.1242194</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242194</url>
		<abstract>
			<par><![CDATA[<p>This sketch concerns the development of a renderer which takes advantage of the two dimensional nature of the frames it produces. The project is chiefly inspired by the way in which artists have used 2D abstraction in the past, from scribbled cartoons to cubist paintings. It would seem that exploiting the plane of the canvas can help us to make more interesting compositions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P679074</person_id>
				<author_profile_id><![CDATA[81100064197]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mackinnon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NCCA, Bournemouth University, Fern Barrow, Poole, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242195</article_id>
		<sort_key>182</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>50</seq_no>
		<title><![CDATA[Expressive features for movement exaggeration]]></title>
		<page_from>182</page_from>
		<page_to>182</page_to>
		<doi_number>10.1145/1242073.1242195</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242195</url>
		<abstract>
			<par><![CDATA[<p>Given a single motion-capture sequence of a person performing a dynamic activity at a particular <i>intensity</i> (or effort), our goal is to automatically warp that movement into a natural-looking exaggerated version of that action. Consider warping a movement of a person lifting a lightweight box to make the movement appear as if the box were actually very heavy. We describe an efficient data-driven approach applicable to animation re-use that learns the underlying regularity in an action to select the most "expressive" features to exaggerate. Other "style-based" approaches are presented in [Gleicher 1998; Brand and Hertzmann 2000; Vasilescu 2001].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39050396</person_id>
				<author_profile_id><![CDATA[81100603566]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Davis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P541963</person_id>
				<author_profile_id><![CDATA[81100598801]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Vignesh]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Kannappan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>344865</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Brand, M., and Hertzmann, A. 2000. Style machines. In <i>SIGGRAPH 00 Conference Proceedings</i>, 183--192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280820</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Gleicher, M. 1998. Retargetting motion to new characters. In <i>SIGGRAPH 98 Conference Proceedings</i>, 33--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Vasilescu, M. A. O. 2001. Human motion signatures for character animation. In <i>SIGGRAPH 01 Conference Abstracts and Applications</i>, 200.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242196</article_id>
		<sort_key>183</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>51</seq_no>
		<title><![CDATA[Firefighter training virtual environment]]></title>
		<page_from>183</page_from>
		<page_to>183</page_to>
		<doi_number>10.1145/1242073.1242196</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242196</url>
		<abstract>
			<par><![CDATA[<p>The Firefighter Training Simulation is a virtual environment being developed at Georgia Tech in collaboration with the Atlanta Fire Department. The VE user is a commanding officer trainee who instructs teams of virtual firefighters to perform different actions to help put out virtual fires. This simulation was developed using the Simple Virtual Environment (SVE) Library, an extensible framework for building VE applications [Kessler 2000].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P642978</person_id>
				<author_profile_id><![CDATA[81100630351]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tazama]]></first_name>
				<middle_name><![CDATA[U.]]></middle_name>
				<last_name><![CDATA[St. Julien]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GVU Center, Georgia Tech, Atlanta, GA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43120132</person_id>
				<author_profile_id><![CDATA[81100396860]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Shaw]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GVU Center, Georgia Tech, Atlanta, GA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1246876</ref_obj_id>
				<ref_obj_pid>1246870</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[D. Kessler, D. Bowman, L. Hodges 2000. The Simple Virtual Environment Library: An Extensible Framework for Building VE Applications, <i>Presence: Teleoperators and Virtual Environments</i>. 9(2): pp. 187--208.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>509759</ref_obj_id>
				<ref_obj_pid>509740</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Jang, J., W Ribarsky, CD SHAW, N. FAUST 2002. View-Dependent Multiresolution Splatting of Non-Uniform Data. In <i>Proceedings of IEEE Visualization Symposium</i>, Barcelona.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242197</article_id>
		<sort_key>184</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>52</seq_no>
		<title><![CDATA[Folded]]></title>
		<subtitle><![CDATA[negotiating the space between real and virtual worlds]]></subtitle>
		<page_from>184</page_from>
		<page_to>184</page_to>
		<doi_number>10.1145/1242073.1242197</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242197</url>
		<abstract>
			<par><![CDATA[<p>A painting is a landscape of possibilities (fig.1) - a form field, a material experiment, a background or foreground, a place of play and imagination. Photographs of people looking at paintings reveal them looking into them, finding things in them that were never consciously put down, never put <i>on</i> the canvas. The way people look at paintings requires a painter to remember the world outside of the work that it, itself, points to. Not only is the materiality of the painted field interesting--its image, vibrant hues and liquid surfaces. What is increasingly interesting, given the possibilities of expanded visualization systems, is how a painting might move into and out of itself, towards and away from its material existence, based on what may or may not be found in its home medium.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[CAVEs]]></kw>
			<kw><![CDATA[immersive environments]]></kw>
			<kw><![CDATA[interactivity]]></kw>
			<kw><![CDATA[interdisciplinarity]]></kw>
			<kw><![CDATA[kinetics]]></kw>
			<kw><![CDATA[media]]></kw>
			<kw><![CDATA[virtual reality]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010476.10011187</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P864764</person_id>
				<author_profile_id><![CDATA[81328489077]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Samantha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Krukowski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Convergent Media]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Foucault, M., on Velazquez' <i>Las Meninas</i>. 1966 in <i>Les Mots et les choses</i>, Paris.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sartre, J-P., 1948. <i>La recherche de l'absolu</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Cage, J., 1969 <i>Notations</i>, New York: Various Works]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Swiss Minimalism, by Herzog, DeMeuron and Zumthor]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Saper C., 2001. <i>Networked Art</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Dollens, D. 2001. <i>D2A Digital to Analog</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Krueger M., 1991. <i>Artificial Reality II</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Koolhaas R., 1995. <i>Delirious New York</i>. New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Lynn G., 1993. <i>Folding in Architecture</i> in <i>Architecture Design Profile No 102</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Deleuze, G., 1988. <i>Le Pli Leibniz et Le Baroque</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Cache, B. 1995. <i>Earth Moves</i>, MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242198</article_id>
		<sort_key>185</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>53</seq_no>
		<title><![CDATA[FlowField]]></title>
		<subtitle><![CDATA[investigating the semantics of caress]]></subtitle>
		<page_from>185</page_from>
		<page_to>185</page_to>
		<doi_number>10.1145/1242073.1242198</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242198</url>
		<abstract>
			<par><![CDATA[<p>We have created a new interactive experience piece called Flow-Field. Participants touch and caress a multi-point touchpad, the MTC Express, in a CAVE (CAVE Automatic Virtual Environment), directly controlling a flowing particle field. Collisions in the particle field emit musical sounds providing a new type of musical interface that uses a dynamic flow process for its underlying musical structure. The particle flow field circles around the participant in a cylindrical path. Obstructions formed by whole hand input disturb the flow field like a hand in water. The interaction has very low latency and a fast frame rate, providing a visceral, dynamic experience. In FlowField, participants explore interaction through caress, suggesting reconnection with a sense of play, and experiencing a world through touch.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P828863</person_id>
				<author_profile_id><![CDATA[81414595593]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Timothy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia, Vancouver, B.C., Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39062061</person_id>
				<author_profile_id><![CDATA[81327488395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sidney]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fels]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia, Vancouver, B.C., Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP77047936</person_id>
				<author_profile_id><![CDATA[81409594757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Thecla]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schiphorst]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technical University of British Columbia, Surrey, B.C., Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fels, S., and Mase, K. 1999. Iamascope: A Graphical Musical Instrument. <i>Computers and Graphics, 2(23)</i>, 277--286.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>847675</ref_obj_id>
				<ref_obj_pid>846221</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Fels, S., Nishimoto, K., and Mase, K. 1997. MusiKalScope: A Graphical Musical Instrument. <i>Proceedings of IEEE International Conference on Multimedia Computing and Systems</i>, 55--62.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242199</article_id>
		<sort_key>186</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>54</seq_no>
		<title><![CDATA[Foamy creatures]]></title>
		<subtitle><![CDATA[Digital Domain wrangles whitewater for "Lord of the Rings"]]></subtitle>
		<page_from>186</page_from>
		<page_to>186</page_to>
		<doi_number>10.1145/1242073.1242199</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242199</url>
		<abstract>
			<par><![CDATA[<p>This sketch explains the technical and artistic techniques used by Digital Domain to produce the <i>Ford of Bruinen</i> sequence in New Line Cinema's <i>Lord of the Rings</i>.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35038612</person_id>
				<author_profile_id><![CDATA[81328489165]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Markus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kurtz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP30040311</person_id>
				<author_profile_id><![CDATA[81319490942]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Duda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242200</article_id>
		<sort_key>187</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>55</seq_no>
		<title><![CDATA[Fracture generation on polygonal meshes using Voronoi polygons]]></title>
		<page_from>187</page_from>
		<page_to>187</page_to>
		<doi_number>10.1145/1242073.1242200</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242200</url>
		<abstract>
			<par><![CDATA[<p>This sketch describes a way of generating realistic cracks and fragments to visually simulate brittle fracture on polygonal surfaces.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[natural phenomena]]></kw>
			<kw><![CDATA[visual simulation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35038650</person_id>
				<author_profile_id><![CDATA[81100537885]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Saty]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raghavachary]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dream Works Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>378522</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[D. Terzopoulos, K. Fleischer: SIGGRAPH 1988 Proceedings, p. 269--278]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>115248</ref_obj_id>
				<ref_obj_pid>115244</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[A. Norton et. al.: The Visual Computer, Vol.7, 1991, p. 210--217]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311550</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[J. O'Brien, J. Hodgins: SIGGRAPH 1999 Proceedings, p. 137--146]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[J. Smith, A. Witkin, D. Baraff: Graphics Interface 2000, p. 27--34]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Saty Raghavachary, M.S. Thesis (CIS Department), The Ohio State University, 1992]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242201</article_id>
		<sort_key>188</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>56</seq_no>
		<title><![CDATA[Generating feather coats using Bezier curves]]></title>
		<page_from>188</page_from>
		<page_to>188</page_to>
		<doi_number>10.1145/1242073.1242201</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242201</url>
		<abstract>
			<par><![CDATA[<p>Feathers, like hair and fur, dramatically alter the appearance of a surface. However, unlike hair and fur, feathers have a wide range of colours and patterns and a well-defined branching structure. The variety of individual feather structures and patterns contribute to the surface appearance. Thus, it is important to model the complete range of feather types found in a feather coat. Since a feather coat can consist of thousands of structurally unique feathers, it is desirable to automatically generate most of the feathers, while maintaining intuitive control over the coat design and the creation of a wide variety of feather types. Previous work on feathers [Dai et al. 1995] does not address the intuitive generation of such a coat.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28022090</person_id>
				<author_profile_id><![CDATA[81100615261]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[L.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Streit]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864790</person_id>
				<author_profile_id><![CDATA[81100644737]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[W.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heidrich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Dai, W.-K., Shih, Z.-C., and Chang, R.-C. 1995. Synthesizing feather textures in Galliformes. <i>Computer Graphics Forum 14</i>, 3 (Aug.), 407--420.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>850438</ref_obj_id>
				<ref_obj_pid>518910</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Iones, A., Krupkin, A., Volodarsky, S., and Zhukov, S. 2000. Fur and hair: practical modeling and rendering techniques. <i>The Proceedings of the IEEE International Conference on Information Visualization (IV'00)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lucas, A. M., and Stettenheim, P. R. 1972. <i>Avian Anatomy Integument</i>. U.S. Goverment Printing Office, Washington D.C.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242202</article_id>
		<sort_key>189</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>57</seq_no>
		<title><![CDATA[GEO-COSMOS]]></title>
		<subtitle><![CDATA[world's first spherical display]]></subtitle>
		<page_from>189</page_from>
		<page_to>189</page_to>
		<doi_number>10.1145/1242073.1242202</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242202</url>
		<abstract>
			<par><![CDATA[<p>Display technology has been advancing every year. However, the topologically speaking, almost all of these use the same "Planar" surface for their displays. The topology of our display, to be introduced here, is completely different, in that we use "Spherical" surface. GEO-COSMOS, the world's first full color spherical display, is truly remarkable. It is also the main exhibit of the National Museum of Emerging Science and Innovation, opened in Tokyo on July 10, 2001. As shown in Figure 1 and 2, the images/movies on this display are intended for the viewers from all directions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28021805</person_id>
				<author_profile_id><![CDATA[81328489455]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tamotsu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Machida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ViNO azul, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242203</article_id>
		<sort_key>190</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>58</seq_no>
		<title><![CDATA[Hardware-accelerated texture and edge antialiasing using FIR filters]]></title>
		<page_from>190</page_from>
		<page_to>190</page_to>
		<doi_number>10.1145/1242073.1242203</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242203</url>
		<abstract>
			<par><![CDATA[<p>Texture mapping is a standard feature of 3D graphics systems. To avoid aliasing artifacts, proper filtering is mandatory. We have developed a novel algorithm for texture mapping and filtering that is suited for hardware implementation. To eliminate texture aliasing artifacts, our algorithm uses higher order FIR filters that are know from digital signal processing. Compared to current state-of-the-art "anisotropic level 8 filtering", that can be found in commercially available advanced graphics accelerators, our method produces higher image quality at equal costs.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P384464</person_id>
				<author_profile_id><![CDATA[81100459795]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Koen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Meinds]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Philips Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P384458</person_id>
				<author_profile_id><![CDATA[81100309833]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bart]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barenbrug]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Philips Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864660</person_id>
				<author_profile_id><![CDATA[81100523318]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Frans]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peters]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Philips Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242204</article_id>
		<sort_key>191</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>59</seq_no>
		<title><![CDATA[Harmonic 3D shape matching]]></title>
		<page_from>191</page_from>
		<page_to>191</page_to>
		<doi_number>10.1145/1242073.1242204</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242204</url>
		<abstract>
			<par><![CDATA[<p>With the advent of the world wide web, the number of available 3D models has increased substantially and the challenge has changed from "How do we generate 3D models?" to "How do we find them?" In this sketch we describe a new 3D model matching and indexing algorithm that uses spherical harmonics to compute discriminating similarity measures without requiring repair of model degeneracies or alignment of orientations. It provides 46-245% better performance than related shape matching methods during precision-recall experiments, and it is fast enough to return query results from a repository of 20,000 models in under half a second.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39023410</person_id>
				<author_profile_id><![CDATA[81100017967]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kazhdan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14073484</person_id>
				<author_profile_id><![CDATA[81100182132]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Funkhouser]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>719092</ref_obj_id>
				<ref_obj_pid>647226</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ankerst, M., Kastenm&#252;ller, G., Kriegel, H.-P., and Seidl, T. 1999. 3d shape histograms for similarity search and classification in spatial databases. In <i>Proc. SSD</i>, 207--226.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>778678</ref_obj_id>
				<ref_obj_pid>778665</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Elad, M., Tal, A., and Ar, S. 2001. Content based retrieval of vrml objects - an iterative and interactive approach. <i>EG Multimedia</i> (September), 97--108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Horn, B. 1984. Extended gaussian images. <i>Proc. of the IEEE 72</i>, 12 (December), 1671--1686.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>884103</ref_obj_id>
				<ref_obj_pid>882486</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Osada, R., Funkhouser, T., Chazelle, B., and Dobkin, D. 2001. Matching 3d models with shape distributions. <i>Shape Modeling International</i> (May).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242205</article_id>
		<sort_key>192</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>60</seq_no>
		<title><![CDATA[Hiding spaces]]></title>
		<subtitle><![CDATA[a CAVE of elusive immateriality]]></subtitle>
		<page_from>192</page_from>
		<page_to>192</page_to>
		<doi_number>10.1145/1242073.1242205</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242205</url>
		<abstract>
			<par><![CDATA[<p><i>Hiding Spaces</i> is an immersive VR Cave artwork which pushes past the limitations of physical media by exploring the new ambiguities that can delight the viewer in the virtual world. By using innovative tools developed especially for creative work within the Cave environment, in combination with more established digital methods and artistic practice, the authors collaborated to produce a work which transgresses the usual borders of 2D and 3D, including those that are common even in VR environments.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P50910</person_id>
				<author_profile_id><![CDATA[81100267776]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cynthia]]></first_name>
				<middle_name><![CDATA[Beth]]></middle_name>
				<last_name><![CDATA[Rubin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhode Island School of Design]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P58957</person_id>
				<author_profile_id><![CDATA[81100317632]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Keefe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242206</article_id>
		<sort_key>193</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>61</seq_no>
		<title><![CDATA[High-speed conversion of floating point images to 8-bit]]></title>
		<page_from>193</page_from>
		<page_to>193</page_to>
		<doi_number>10.1145/1242073.1242206</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242206</url>
		<abstract>
			<par><![CDATA[<p>Most rendering software today destroys their accurate lighting and shading calculations by doing an inaccurate linear conversion to a screen image. This sketch presents a technique that quickly converts floating point data to a screen image while preserving the correct brightness levels and original detail.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864626</person_id>
				<author_profile_id><![CDATA[81328490602]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Spitzak]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Floyd, R., and Steinberg, L. 1975. An adaptive algorithm for spatial gray scale. <i>Society for Information Display 1975 Symposium Digest of Technical Papers</i>, 36.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[International Electrotechnical Commission. <i>The sRGB Specification</i>, IEC 61966-2-1 ed.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242207</article_id>
		<sort_key>194</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>62</seq_no>
		<title><![CDATA[Hover]]></title>
		<subtitle><![CDATA[conveying remote presence]]></subtitle>
		<page_from>194</page_from>
		<page_to>194</page_to>
		<doi_number>10.1145/1242073.1242207</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242207</url>
		<abstract>
			<par><![CDATA[<p>This sketch presents <i>Hover</i>, a device that enhances remote telecommunication by providing a sense of the activity and presence of remote users. The motion of a remote persona is manifested as the playful movements of a ball floating in midair. <i>Hover</i> is both a communication medium and an aesthetic object.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P387560</person_id>
				<author_profile_id><![CDATA[81100068218]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maynes-Aminzade]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[One Cambridge Centre, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864622</person_id>
				<author_profile_id><![CDATA[81328490430]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Beng-Kiang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard Graduate School of Design, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864702</person_id>
				<author_profile_id><![CDATA[81328488564]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goulding]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P344897</person_id>
				<author_profile_id><![CDATA[81452607629]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Catherine]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vaucelle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Strong, R., and Gaver, W. W. 1996. Feather, scent and shaker: Supporting simple intimacy. In <i>Proceedings of CSCW 1996</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>634135</ref_obj_id>
				<ref_obj_pid>634067</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ishii, H., ren, S. and Frei, P. 2001. Pinwheels: Visualizing Information Flow in an Architectural Space. In <i>Extended Abstracts of CHI 2001</i>, ACM Press, 111--112.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258715</ref_obj_id>
				<ref_obj_pid>258549</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ishii, H., and Ulmer, B. 1997. Tangible Bits: Towards Seamless Interfaces between People, Bits, and Atoms. In <i>Proceedings CHI 1997</i>, ACM Press, 234--241.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242208</article_id>
		<sort_key>195</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>63</seq_no>
		<title><![CDATA[How a CSG-based raytracer saves time]]></title>
		<subtitle><![CDATA[lighting and scripting for Ice Age]]></subtitle>
		<page_from>195</page_from>
		<page_to>195</page_to>
		<doi_number>10.1145/1242073.1242208</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242208</url>
		<abstract>
			<par><![CDATA[<p>At Blue Sky Studios, two overriding principles have guided the development of the renderer and production lighting tools: 1) the lighting model should be as physically accurate as possible, and 2) be straightforward and easy to use so that the computers take care of the technical work leaving the artists free to concentrate on the creative aspects of lighting a scene. Blue Sky's proprietary renderer, CGIStudio#8482;, has one of the most robust lighting models in the industry. The renderer's realistic approach to how light actually behaves in the real world lets the artist add details like soft shadows, reflections, and radiosity at the flip of a switch. Artists are able to achieve complex and subtle lighting with relatively simple lighting rigs, allowing them to get the image as 'right' as possible in the original render.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864648</person_id>
				<author_profile_id><![CDATA[81328488084]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Esneault]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blue Sky Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864732</person_id>
				<author_profile_id><![CDATA[81328489039]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mitch]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kopelman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blue Sky Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864689</person_id>
				<author_profile_id><![CDATA[81328490861]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jodi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Whitsel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blue Sky Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242209</article_id>
		<sort_key>196</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>64</seq_no>
		<title><![CDATA[How to dress like a Jedi]]></title>
		<subtitle><![CDATA[techniques for digital clothing]]></subtitle>
		<page_from>196</page_from>
		<page_to>196</page_to>
		<doi_number>10.1145/1242073.1242209</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242209</url>
		<abstract>
			<par><![CDATA[<p>This sketch presents how cloth simulation is used at Industrial Light + Magic to create clothing for digital characters in Star Wars: Episode 2 and other films. Many costumes have to match complex physical costumes worn by live actors in other shots. Digital costumes have not only to look the same as their physical counterparts, but also move identically in response to their wearers' movements and environmental influences..</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28021890</person_id>
				<author_profile_id><![CDATA[81320494129]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rapkin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light + Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242210</article_id>
		<sort_key>197</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>65</seq_no>
		<title><![CDATA[ICARUS]]></title>
		<subtitle><![CDATA[interactive reconstruction from uncalibrated image sequences]]></subtitle>
		<page_from>197</page_from>
		<page_to>197</page_to>
		<doi_number>10.1145/1242073.1242210</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242210</url>
		<abstract>
			<par><![CDATA[<p>The ICARUS system is a suite of software packages, developed at the University of Manchester, that allows geometric models to be quickly and easily reconstructed from image and video sequences captured with uncalibrated digital cameras. The system combines automatic and semi-automatic camera calibration algorithms with an easy-to-use interactive model-building phase (Figure 1). Surface textures are automatically extracted from images and mapped onto the reconstructed models.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P265425</person_id>
				<author_profile_id><![CDATA[81100256430]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Simon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gibson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Manchester, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P146848</person_id>
				<author_profile_id><![CDATA[81100111573]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cook]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Manchester, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31062207</person_id>
				<author_profile_id><![CDATA[81100328773]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Toby]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Howard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Manchester, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P247930</person_id>
				<author_profile_id><![CDATA[81100561115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Roger]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hubbold]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Manchester, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>237191</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Debevec, P., and Malik, J. 1996. Modeling and rendering architecture from photographs: A hybrid geometry and image-based approach. In <i>Proceedings of SIGGRAPH 1996</i>, 11--20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hakim, S. E. 2000. A practical approach to creating precise and detailed 3d models from single and multiple views. <i>Int. Archives of Photogrammetry and Remote Sensing 33</i> (July), 122--129.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>331271</ref_obj_id>
				<ref_obj_pid>331269</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Pollefeys, M., Koch, R., and Gool, L. V. 1999. Self-calibration and metric reconstruction in spite of varying and unknown internal camera parameters. <i>Int. Journal of Computer Vision 32</i>, 1, 7--25.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242211</article_id>
		<sort_key>198</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>66</seq_no>
		<title><![CDATA[Image-based environment matting]]></title>
		<page_from>198</page_from>
		<page_to>198</page_to>
		<doi_number>10.1145/1242073.1242211</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242211</url>
		<abstract>
			<par><![CDATA[<p>Environment matting is a powerful technique for modelling the complex light-transport properties of real-world <i>optically active elements:</i> transparent, refractive and reflective objects. Zongker <i>et al</i> [1999] and Chuang <i>et al</i> [2000] show how environment mattes can be computed for real objects under carefully controlled laboratory conditions. However, for many objects of interest, such calibration is difficult to arrange. For example, we might wish to determine the distortion caused by filming through an ancient window where the glass has flowed; we may have access only to archive footage; or we might simply want a more convenient means of acquiring the matte.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39042236</person_id>
				<author_profile_id><![CDATA[81100425952]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yonatan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wexler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Oxford]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P17898</person_id>
				<author_profile_id><![CDATA[81100376259]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Fitzgibbon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Oxford]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14162839</person_id>
				<author_profile_id><![CDATA[81100465760]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zisserman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Oxford]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>344844</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chuang, Y.-Y., Zongker, D. E., Hindorff, J., Curless, B., Salesin, D. H., and Szeliski, R. 2000. Environment matting extensions: Towards higher accuracy and real-time capture. In <i>Proceedings of ACM SIGGRAPH</i>, 12--130.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>181448</ref_obj_id>
				<ref_obj_pid>181447</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Irani, M., Rousso, B., and Peleg, S. 1994. Computing occluding and transparent motions. <i>Intl. J. Computer Vision 12</i>, 1, 5--16.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311558</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Zongker, D. E., Werner, D. M., Curless, B., and Salesin., D. H. 1999. Environment matting and compositing. In <i>Proceedings of ACM SIGGRAPH</i>, 205--214.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242212</article_id>
		<sort_key>199</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>67</seq_no>
		<title><![CDATA[Image-based illumination for electronic display of artistic paintings]]></title>
		<page_from>199</page_from>
		<page_to>199</page_to>
		<doi_number>10.1145/1242073.1242212</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242212</url>
		<abstract>
			<par><![CDATA[<p>Visual impressions from two-dimensional artistic paintings greatly vary under different illumination conditions, but this effect has been largely overlooked in most poster productions and electronic display. The light-dependent impressions are more pronounced in oil paintings and they arise mainly from the non-diffuse specular reflectances. We present an efficient method of representing the variability of lighting conditions on artistic paintings utilizing both simple empirical reflectance models and an image-based lighting method. The Lambertian and Phong models account for a significant portion of image variations depending on illumination directions, and residual intensity and color variations that cannot be explained by the reflection models are processed in a manner that is similar to the image-based lighting methods. Our technique allows brush strokes and paint materials to be clearly visible with relatively low data dimensionality.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864637</person_id>
				<author_profile_id><![CDATA[81328488985]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Da]]></first_name>
				<middle_name><![CDATA[Young]]></middle_name>
				<last_name><![CDATA[Ju]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sogang University, Seoul, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864688</person_id>
				<author_profile_id><![CDATA[81328491194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jin-Ho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sogang University, Seoul, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864713</person_id>
				<author_profile_id><![CDATA[81328490050]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kyoung]]></first_name>
				<middle_name><![CDATA[Chin]]></middle_name>
				<last_name><![CDATA[Seo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sogang University, Seoul, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40025508</person_id>
				<author_profile_id><![CDATA[81100267842]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Gregory]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sharp]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Michigan, Ann Arbor, Michigan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40026952</person_id>
				<author_profile_id><![CDATA[81414595677]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Sang]]></first_name>
				<middle_name><![CDATA[Wook]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sogang University, Seoul, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242213</article_id>
		<sort_key>200</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>68</seq_no>
		<title><![CDATA[Implementing the continuous staircase illusion in OpenGL]]></title>
		<page_from>200</page_from>
		<page_to>200</page_to>
		<doi_number>10.1145/1242073.1242213</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242213</url>
		<abstract>
			<par><![CDATA[<p>The inspiration for the project described in this sketch was certainly the M. C. Escher lithograph &lt;u&gt;ASCENDING AND DESCENDING&lt;/u&gt;. An image of this drawing can be seen at &lt;u&gt;http://www.worldofescher.com/gallery&lt;/u&gt;. It is based on the continuous staircase illusion of L. S. and Roger Penrose and depicts a three-dimensional scene that appears to have properties that contradict what is possible in an actual Cartesian representation of spatial objects. The goal of this project was to add animation and a navigable viewpoint to this illusion by using the three-dimensional environment of OpenGL.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864724</person_id>
				<author_profile_id><![CDATA[81328490220]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Scott]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242214</article_id>
		<sort_key>201</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>69</seq_no>
		<title><![CDATA[Improving frameless rendering by focusing on change]]></title>
		<page_from>201</page_from>
		<page_to>201</page_to>
		<doi_number>10.1145/1242073.1242214</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242214</url>
		<abstract>
			<par><![CDATA[<p>Realtime rendering requires accurate display of a dynamic scene with minimal delay. Frameless rendering [Bishop et al. 1994] offers unique flexibility in this regard: because it samples time per pixel, it can respond to change with very little delay, and at any location in the image. However, sampling is random, resulting in blurring in changing image regions. We present an approach for improving frameless rendering by making sampling sensitive to change in the image, as suggested in [Bishop et al. 1994]. By measuring this change in visual terms, we are able to direct sampling to those regions of change. The resulting algorithm produces sharper imagery, while introducing minimal overhead into the standard frameless algorithm.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P437427</person_id>
				<author_profile_id><![CDATA[81100598236]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Abhinav]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dayal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Northwestern University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39040047</person_id>
				<author_profile_id><![CDATA[81100375996]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Benjamin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Watson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Northwestern University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14056692</person_id>
				<author_profile_id><![CDATA[81100131290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luebke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Virginia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>192195</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bishop, G., Fuchs, F., McMillan, L. and Zagier, E. 1994. Frameless Rendering: Double Buffering Considered Harmful. In <i>Proceedings of SIGGRAPH 1994</i>, ACM Press/ACM SIGGRAPH, New York. A. Glassner, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 175--176.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300537</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Parker, S., Martin, W., Sloan, P., Shirley, P., Smits, B. and Hansen, C. 1999. Interactive Ray Tracing. In <i>Proceedings of ACM Symposium on Interactive 3D Computer Graphics</i>, 119--126.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15889</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bergman, L., Fuchs, H., Grant, E. and Spach, S 1986. Image Rendering by Adaptive Refinement. In <i>Proceedings of SIGGRAPH 1986, Computer Graphics</i> 20, 4, 29--37.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242215</article_id>
		<sort_key>202</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>70</seq_no>
		<title><![CDATA[Integrating lenticular into digital printmaking]]></title>
		<page_from>202</page_from>
		<page_to>202</page_to>
		<doi_number>10.1145/1242073.1242215</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242215</url>
		<abstract>
			<par><![CDATA[<p>On September 11 I was in the Middle East on my continuing quest to understand why, in the name of religion, one would kill another who did not share their beliefs. The evening before I had crossed the border to Israel from Jordan, where I had photographed the crumbling sandstone ruins of Petra.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864650</person_id>
				<author_profile_id><![CDATA[81328489147]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dorothy]]></first_name>
				<middle_name><![CDATA[Simpson]]></middle_name>
				<last_name><![CDATA[Krause]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts College of Art]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242216</article_id>
		<sort_key>203</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>71</seq_no>
		<title><![CDATA[Interactive level-set tools for photo editing]]></title>
		<page_from>203</page_from>
		<page_to>203</page_to>
		<doi_number>10.1145/1242073.1242216</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242216</url>
		<abstract>
			<par><![CDATA[<p>This sketch presents a suite of interactive image-editing tools based on properties of and manipulation of image level sets. This suite includes level-set smoothing, level-set constrained sharpening, and level-set "nudging" (image distortion).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28021089</person_id>
				<author_profile_id><![CDATA[81406600916]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Howard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brigham Young University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP36027931</person_id>
				<author_profile_id><![CDATA[81100405205]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bryan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morse]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brigham Young University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Alvarez, L., Guichard, F., Lions, P.-L., and Morel, J.-M. 92. Axioms and fundamental equations of image processing. Tech. Rep. 9231, Ceremade, Universite Paris-Dauphine, Place de Lattre de Tassigny, 75775 Paris Cedex 16 France.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Morse, B. S., and Schwartwald, D. 2001. Level-set image reconstruction. In <i>Proceedings Computer Vision and Pattern Recognition (CVPR)</i>, IEEE Computer Society Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1204651</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Sapiro, G. 2001. <i>Geometric Partial Differential Equations and Image Analysis</i>. Cambridge University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Sethian, J. A. 1996. <i>Level Set Methods</i>. Cambridge University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242217</article_id>
		<sort_key>204</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>72</seq_no>
		<title><![CDATA[Interactive visualization of large scale time varying data sets]]></title>
		<page_from>204</page_from>
		<page_to>204</page_to>
		<doi_number>10.1145/1242073.1242217</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242217</url>
		<abstract>
			<par><![CDATA[<p>Visual analysis of time varying scientific data can be divided into four different categories with an increasing degree of user interaction.</p> <p>1) Production of static images representing scientific data at selected times.</p> <p>2) Production of video sequences in which graphical representation, time line and viewpoints are predefined.</p> <p>3) Interactive streaming of logged data sets, allowing the user to alter graphical representation, filtering, time lines and viewpoints.</p> <p>4) Real time interaction with the simulation or experiment that produces the data, allowing the user to alter parameters, graphical representation, filtering, time lines and viewpoints.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP48022671</person_id>
				<author_profile_id><![CDATA[81100015118]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Patric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ljung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Link&#246;ping, Campus Norrk&#246;ping, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P317303</person_id>
				<author_profile_id><![CDATA[81100310843]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dieckmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Link&#246;ping, Campus Norrk&#246;ping, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P316938</person_id>
				<author_profile_id><![CDATA[81100419859]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Anders]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ynnerman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Link&#246;ping, Campus Norrk&#246;ping, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[McClements, K. G., Dieckmann, M. E., Ynnerman, A., Chapman, S. C. & Dendy, R. O. 2001. Surfatron and stochastic acceleration of electrons at supernova remnant shocks, In <i>Phys. Rev. Lett.</i>, 25, No. 255002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>375301</ref_obj_id>
				<ref_obj_pid>375213</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ljung, P., Dieckmann, M., Andersson, N. & Ynnerman, A. 2000. Interactive Visualization of Particle In Cell Simulations. In <i>Proceedings of IEEE Visualization 2000</i>, pages 469--472.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Dieckmann, M. E., Ljung, P., Ynnerman, A., McClements, K. G. 2002. Three-dimensional visualization of electron acceleration in a magnetized plasma, In <i>IEEE Transactions on Plasma Science</i>, accepted for publication.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242218</article_id>
		<sort_key>205</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>73</seq_no>
		<title><![CDATA[Interruptible rendering]]></title>
		<page_from>205</page_from>
		<page_to>205</page_to>
		<doi_number>10.1145/1242073.1242218</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242218</url>
		<abstract>
			<par><![CDATA[<p>Interruptible rendering is a novel approach to the fidelity-versus-performance tradeoff ubiquitous in real-time rendering. Interruptible rendering unifies <i>spatial error</i>, caused by rendering coarse approximations for speed, and <i>temporal error</i>, caused by the delay imposed by rendering, into a single image-space error metric. The heart of this approach is a progressive rendering framework that renders a coarse image into the back buffer and continuously refines it, while tracking the temporal error. When the temporal error exceeds the spatial error caused by coarse rendering, further refinement is pointless and the image is displayed. We discuss the requirements for a rendering algorithm to be suitable for interruptible use, and describe one such algorithm based on hierarchical splatting. Interruptible rendering provides a low-latency, self-tuning approach to interactive rendering. Interestingly, it also leads to a "one-and-a-half buffered" approach that renders sometimes to the back buffer and sometimes to the front buffer.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864679</person_id>
				<author_profile_id><![CDATA[81328490912]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[Cliff]]></middle_name>
				<last_name><![CDATA[Woolley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Virginia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14056692</person_id>
				<author_profile_id><![CDATA[81100131290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luebke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Virginia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39040048</person_id>
				<author_profile_id><![CDATA[81100375996]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ben]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Watson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Northwestern University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>344940</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Rusinkiewicz, S., and Levoy, M. 2000. QSplat: A Multiresolution Point Rendering System for Large Meshes. In <i>Proceedings of ACM SIGGRAPH 2000</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344935</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sander, P., Gu, X., Gortler, S., Hoppe, H., Snyder, J. 2000. Silhouette Clipping. In <i>Proceedings of ACM SIGGRAPH 2000</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242219</article_id>
		<sort_key>206</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>74</seq_no>
		<title><![CDATA[Inverse texture warping]]></title>
		<page_from>206</page_from>
		<page_to>206</page_to>
		<doi_number>10.1145/1242073.1242219</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242219</url>
		<abstract>
			<par><![CDATA[<p>Standard texturing of a geometric surface works by mapping a 2D image into the parametric space of the surface. This technique works great whenever the texture image is created (painted, etc) for a specific surface with predetermined contours. Serious problems arise when the same texture is applied to the same surface but after some deformation has been applied. The result is the all too familiar rubbery texture look. This stretchy appearance can be particularly noticeable on surfaces that have identifiable traits such as scars, holes, text, repeating patterns, or in our case reptilian scales.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864752</person_id>
				<author_profile_id><![CDATA[81328488132]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Falco]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Secret Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28021153</person_id>
				<author_profile_id><![CDATA[81328488131]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Driskill]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Secret Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242220</article_id>
		<sort_key>207</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>75</seq_no>
		<title><![CDATA[Investigating face space]]></title>
		<page_from>207</page_from>
		<page_to>207</page_to>
		<doi_number>10.1145/1242073.1242220</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242220</url>
		<abstract>
			<par><![CDATA[<p>Imagine an n-dimensional space describing every conceivable humanoid face, where each dimension represents a different facial characteristic. Within this continuous space, it would be possible to traverse a path from any face to any other face, morphing through locally similar faces along that path.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39062840</person_id>
				<author_profile_id><![CDATA[81328488119]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[DiPaola]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[DiPaola, 1991. Extending the Range of Facial Types, <i>Journal of Visualization and Computer Animation</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>249651</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Parke, <i>Waters</i>, 1996. <i>Computer Facial Animation</i>. 101--4, 214--19.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122752</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Sims, 1992. Artificial Evolution for Computer Graphics, <i>ACM Siggraph</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311556</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Blanz, Vetter, 1999. A Morphable Model For The Synthesis of 3D Faces, <i>ACM Siggraph</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242221</article_id>
		<sort_key>208</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>76</seq_no>
		<title><![CDATA[Lewis the robotic photographer]]></title>
		<page_from>208</page_from>
		<page_to>208</page_to>
		<doi_number>10.1145/1242073.1242221</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242221</url>
		<abstract>
			<par><![CDATA[<p>Lewis is a (short) human-sized mobile robot that wanders through crowded rooms taking pictures of people, much like a photographer at a wedding reception does. The goal is to take high-quality, well-composed photographs of people non-intrusively, and to offer these pictures as keep-sakes of the conference.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P48640</person_id>
				<author_profile_id><![CDATA[81100553787]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cindy]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Grimm]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis, St. Louis, MO, United States]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P300012</person_id>
				<author_profile_id><![CDATA[81100105515]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Smart]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis, St. Louis, MO, United States]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242222</article_id>
		<sort_key>209</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>77</seq_no>
		<title><![CDATA[Loops]]></title>
		<subtitle><![CDATA[a digital portrait]]></subtitle>
		<page_from>209</page_from>
		<page_to>209</page_to>
		<doi_number>10.1145/1242073.1242222</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242222</url>
		<abstract>
			<par><![CDATA[<p>This sketch outlines some of the background to a collaborative artwork created in September 2001 by Marc Downie, Shelley Eshkar and Paul Kaiser. This digital portrait of dance legend Merce Cunningham uses as a point of departure a motion-captured recording of 'Loops', his solo dance for hands and fingers; it uses new real-time non-photorealistic rendering techniques; and it exploits an advanced behavior architecture to structure the performance of the piece. In the resulting animation motion-captured joints become nodes in a network that sets them into fluctuating relationships with one another, at times suggesting the hands underlying them, but more often depicting complex cat's-cradle variations.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP30040685</person_id>
				<author_profile_id><![CDATA[81100369938]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Downie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mrr Media Lab.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566597</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Blumberg, B., Downie, M., Ivanov, Y., Berlin, M., Johnson M. P., and Tomlinson, B. 2002. Integrated Learning for Interactive Synthetic Characters. In: <i>Proceedings of Acm Siggraph 2002</i>, Acm Press / Acm Siggraph, New York. Computer Graphics Proceedings, Annual Conference Series, Acm.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Downie, M. 2001. music creatures. In: <i>Proceedings of Siggraph 2001: Conference Abstracts and Applications</i> (Art Gallery).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kaiser, P. online at: http://www.riverbed.com]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242223</article_id>
		<sort_key>210</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>78</seq_no>
		<title><![CDATA[Lord of the Rings]]></title>
		<subtitle><![CDATA[animation that was not there]]></subtitle>
		<page_from>210</page_from>
		<page_to>210</page_to>
		<doi_number>10.1145/1242073.1242223</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242223</url>
		<abstract>
			<par><![CDATA[<p>The need to recreate reality is a driving force behind art. Mimesis-imitation of the real world as a main function of art was observed and analyzed as early as the fourth century B. C. by Socrates, Plato, and Aristotle. Although artists searched for inner inspiration, the psychological need to record reality was always present, appearing during the renaissance, in baroque painting, the art of film, and probably reaching it's most sophisticated form in computer graphics and animation. Seventy years ago, Polish critic Karol Irzykowski, in his book "The 10th Muse", predicted that one day animated film would evolve into the most important film genre of the future. He also contemplated the possibility of animation reaching a state of realism in which it could show "ordinary things and people". He would be surprised to learn how true his words were to become. Digital media has given us an opportunity to go even further towards that goal of "registering reality". In this sketch, I present these concepts in relation to Digital Domain's work on New Line Cinema's release of J. R. R. Tolkien's classic tale "Lord of the Rings: Fellowship of the Ring".</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31063569</person_id>
				<author_profile_id><![CDATA[81332507972]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Piotr]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Karwas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242224</article_id>
		<sort_key>211</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>79</seq_no>
		<title><![CDATA["Low level" intelligence for "low level" character animation]]></title>
		<page_from>211</page_from>
		<page_to>211</page_to>
		<doi_number>10.1145/1242073.1242224</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242224</url>
		<abstract>
			<par><![CDATA[<p>It has been suggested that investing animated characters with low-level cognitive models can allow a rich set of low-level behavior to be produced automatically. For example, a character that has the cognitive capability of Object Persistence can intelligently direct its gaze over a scene and even respond emotionally to certain world events. This level of cognitive modeling allows for complete behavioral control by a human controller or a script, if that degree of control is necessary for the application.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[AI and behavior]]></kw>
			<kw><![CDATA[animation techniques]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.2.0</cat_node>
				<descriptor>Cognitive simulation</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010216.10010217</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Philosophical/theoretical foundations of artificial intelligence->Cognitive science</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010187.10010194</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Knowledge representation and reasoning->Cognitive robotics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010216.10010217</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Philosophical/theoretical foundations of artificial intelligence->Cognitive science</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010187.10010194</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Knowledge representation and reasoning->Cognitive robotics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P864642</person_id>
				<author_profile_id><![CDATA[81100038347]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Damian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Isla]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P33300</person_id>
				<author_profile_id><![CDATA[81100047544]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bruce]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Blumberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>545132</ref_obj_id>
				<ref_obj_pid>545056</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Isla, D. and Blumberg, B. 2002. Object Persistence for Synthetic Creatures. In <i>Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS)</i>, Bologna, Italy.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242225</article_id>
		<sort_key>212</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>80</seq_no>
		<title><![CDATA[Magical face]]></title>
		<subtitle><![CDATA[integrated tool for muscle based facial animation]]></subtitle>
		<page_from>212</page_from>
		<page_to>212</page_to>
		<doi_number>10.1145/1242073.1242225</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242225</url>
		<abstract>
			<par><![CDATA[<p>In recent years, tremendous advances have been achieved in the 3D computer graphics used in the entertainment industry, and in the semiconductor technologies used to fabricate graphics chips and CPUs. However, although good reproduction of facial expressions is possible through 3D CG, the creation of realistic expressions and mouth motion is not a simple task.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P648127</person_id>
				<author_profile_id><![CDATA[81100506136]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tatsuo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yotsukura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seikei University/SEGA Corporation, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864733</person_id>
				<author_profile_id><![CDATA[81328490573]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mitsunori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takahashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seikei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37023348</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seikei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864701</person_id>
				<author_profile_id><![CDATA[81328489491]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kazunori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SEGA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864669</person_id>
				<author_profile_id><![CDATA[81328489189]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hirokazu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kudoh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SEGA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Waters, K., and Frisbie, J. 1995. A coordinate muscle model for speech animation. Proc. Graphics INterface' 95, pp. 163--170.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218407</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lee, Y., Terzopoulos, D., and Waters, K. 1995. Realistic modeling for facial animation. Proceedings of SIGGRAPH' 95, pp. 55--62.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242226</article_id>
		<sort_key>213</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>81</seq_no>
		<title><![CDATA[MARE]]></title>
		<subtitle><![CDATA[multiuser augmented reality environment on table setup]]></subtitle>
		<page_from>213</page_from>
		<page_to>213</page_to>
		<doi_number>10.1145/1242073.1242226</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242226</url>
		<abstract>
			<par><![CDATA[<p>Using augmented reality (AR) in collaborative situations is appealing: it combines the use of natural metaphors of communication (gesture, voice, expression), with the power of virtual ones (simulation, animation, persistent data). But few 3D AR collaborative systems are devoted to keep human's ability (like grasping, writing). The motivation of this research is to mix together virtual reality techniques ([Schmalstieg et al. 2000]) and computer human interaction techniques ([Fjeld et al. 2002]), so to have the best of both worlds.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP43119846</person_id>
				<author_profile_id><![CDATA[81100435833]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rapha&#235;l]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grasset]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA Rh&#244;ne-Alpes, Montbonnot, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P136482</person_id>
				<author_profile_id><![CDATA[81100519130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jean-Dominique]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gascuel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA Rh&#244;ne-Alpes, Montbonnot, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>593409</ref_obj_id>
				<ref_obj_pid>593367</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fjeld, M., Lauche, K., Bichsel, M., Voorhorst, F., Krueger, H., and Rauterberg, M. 2002. Physical and virtual tools: Activity theory applied to the design of groupware. <i>Computer Supported Cooperative Work (CSCW) 11</i>, 1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kato, H., Billinghurst, M., Poupyrev, I., Imamoto, K., and Tachibana, K. 2000. Virtual object manipulation on a table-top ar environment. In <i>Proceedings of ISAR 2000</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Schmalstieg, D., Fuhrmann, A., and Hesina, G. 2000. Bridging multiple user interface dimensions with augmented reality. In <i>Proceedings of ISAR 2000</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242227</article_id>
		<sort_key>214</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>82</seq_no>
		<title><![CDATA[MasterMotion]]></title>
		<subtitle><![CDATA[full body wireless virtual reality for Tai Chi]]></subtitle>
		<page_from>214</page_from>
		<page_to>214</page_to>
		<doi_number>10.1145/1242073.1242227</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242227</url>
		<abstract>
			<par><![CDATA[<p>We present a wireless virtual reality system and MasterMotion, the full-body training application built with it. We combine realtime full-body optical motion capture with wireless audio/video broadcast, belt-worn electronics, and a lightweight head-mounted display (HMD), to provide a wide-area, untethered virtual environment system that allows exploration of new application areas.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P552054</person_id>
				<author_profile_id><![CDATA[81100547457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Philo]]></first_name>
				<middle_name><![CDATA[Tan]]></middle_name>
				<last_name><![CDATA[Chua]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P552254</person_id>
				<author_profile_id><![CDATA[81100642692]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Rebecca]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Crivella]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P549834</person_id>
				<author_profile_id><![CDATA[81100489506]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Daly]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP36036539</person_id>
				<author_profile_id><![CDATA[81100253857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ning]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P552322</person_id>
				<author_profile_id><![CDATA[81100420801]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Russ]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schaaf]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28022437</person_id>
				<author_profile_id><![CDATA[81328490887]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ventura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P552806</person_id>
				<author_profile_id><![CDATA[81100652172]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Todd]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Camill]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P139385</person_id>
				<author_profile_id><![CDATA[81100049661]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Jessica]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hodgins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39045485</person_id>
				<author_profile_id><![CDATA[81100493478]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Randy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pausch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242228</article_id>
		<sort_key>215</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>83</seq_no>
		<title><![CDATA[Maximum entropy light source placement]]></title>
		<page_from>215</page_from>
		<page_to>215</page_to>
		<doi_number>10.1145/1242073.1242228</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242228</url>
		<abstract>
			<par><![CDATA[<p>Several methods have been proposed to help the user to position lighting sources for a given view of a 3d-scene. In the first kind of methods the user defines a desired illumination through highlight and shadow locations for which the lighting system optimizes the light positions. The more automatic approaches are based on perceptual image metrics. [Marks et al. 1997] define an image metric that measures how different two images are perceived. A collection of maximally different images is presented to the user for selection. [Shacked and Lischinski 2001] define a perceptual based image quality metric composed of six contributing terms, for which the user has to specify weights before the system searches for a locally optimal light source placement.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31063519</person_id>
				<author_profile_id><![CDATA[81100408275]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[S.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gumhold]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of T&#252;bingen]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>258887</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Marks, J., Andalman, B., Beardsley, P. A., Freeman, W., Gibson, S., Hodgins, J., Kang, T., Mirtich, B., Pfister, H., Ruml, W., Ryall, K., Seims, J., and Shieber, S. 1997. Design galleries: A general approach to setting parameters for computer graphics and animation. In <i>SIGGRAPH 97</i>, 389--400.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Shacked, R., and Lischinski, D. 2001. Automatic lighting design using a perceptual quality metric. In <i>EUROGRAPHICS 2001</i>, 215--226.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242229</article_id>
		<sort_key>216</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>84</seq_no>
		<title><![CDATA[MOCAP game reserve]]></title>
		<subtitle><![CDATA[a study of puppetry and motion capture]]></subtitle>
		<page_from>216</page_from>
		<page_to>216</page_to>
		<doi_number>10.1145/1242073.1242229</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242229</url>
		<abstract>
			<par><![CDATA[<p>This animation sketch presents a study of human-driven character animation with motion capture as presented as part of the 2002 SIGGRAPH Course <i>Motion Capture: Pipeline, Applications, and Use</i>.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864632</person_id>
				<author_profile_id><![CDATA[81328487590]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Charlotte]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Belland]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242230</article_id>
		<sort_key>217</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>85</seq_no>
		<title><![CDATA[Modal analysis for real-time viscoelastic deformation]]></title>
		<page_from>217</page_from>
		<page_to>217</page_to>
		<doi_number>10.1145/1242073.1242230</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242230</url>
		<abstract>
			<par><![CDATA[<p>This technical sketch describes how a standard analysis technique known as modal decomposition can be used for real-time modeling of viscoelastic deformation. While most prior work on interactive deformation has relied on geometrically simple models and advantageously selected material parameters to achieve interactive speeds, the approach described here has two qualities that we belive should be required of a real-time deformation method: the simulation cost is decoupled from both the model's geometric complexity and from stiffness of the material's parameters. Additionally, the simulation may be advanced at arbitrarily large time-steps without introducing objectionable errors such as artificial damping.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP37026597</person_id>
				<author_profile_id><![CDATA[81451596212]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864711</person_id>
				<author_profile_id><![CDATA[81540562156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kris]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Hauser]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P362238</person_id>
				<author_profile_id><![CDATA[81100144881]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Christine]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Gatchalian]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P133156</person_id>
				<author_profile_id><![CDATA[81100311781]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[O'Brien]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Maia, N. M. M., and Silva, J. M. M., Eds. 1998. <i>Theoretical and Experimental Modal Analysis</i>. Research Studies Press, Hertfordshire, England.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311550</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[O'Brien, J. F., and Hodgins, J. K. 1999. Graphical modeling and animation of brittle fracture. In <i>Proceedings of SIGGRAPH 99</i>, 137--146.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74355</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Pentland, A., and Williams, J. 1989. Good vibrations: Modal dynamics for graphics and animation. In <i>Proceedings of SIGGRAPH 89</i>, 215--222.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Wu, K., and Simon, H. 1999. TRLAN user guide. Tech. Rep. LBNL-42953, Lawrence Berkeley National Laboratory.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242231</article_id>
		<sort_key>218</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>86</seq_no>
		<title><![CDATA[Modeling the accumulation of wind-driven snow]]></title>
		<page_from>218</page_from>
		<page_to>218</page_to>
		<doi_number>10.1145/1242073.1242231</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242231</url>
		<abstract>
			<par><![CDATA[<p>This technical sketch presents a method for modeling the appearance of snow drifts formed by the accumulation of wind-blown snow near buildings and other obstacles. Our method combines previous work on snow accumulation [Fearing] with techniques for incompressible fluid flows [Fedkiw et al.]. By computing the three-dimensional flow of air in the volume around the obstacles our method is able to model how the snow is convected, deposited, and lifted by the wind. The results demonstrate realistic snow accumulation patterns with deep windward and leeward drifts, furrows, and low accumulation in wind shadowed areas. (See figure.)</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P587702</person_id>
				<author_profile_id><![CDATA[81100064338]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bryan]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Feldman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P133156</person_id>
				<author_profile_id><![CDATA[81100311781]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[O'Brien]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>344809</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fearing, P. 2000. Computer modelling of fallen snow. In <i>Proceedings of ACM SIGGRAPH 2000</i>, 37--46.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383260</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Fedkiw, R., Stam, J., and Jensen, H. W. 2001. Visual simulation of smoke. In <i>Proceedings of ACM SIGGRAPH 2001</i>, 15--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242232</article_id>
		<sort_key>219</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>87</seq_no>
		<title><![CDATA[Modified marching octahedra for optimal regular meshes]]></title>
		<page_from>219</page_from>
		<page_to>219</page_to>
		<doi_number>10.1145/1242073.1242232</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242232</url>
		<abstract>
			<par><![CDATA[<p>Theu&#223;l et al. [Theu&#223;l et al. 2001] showed that volumetric data sampled on a body-centred cubic (BCC) lattice is nearly 30% more efficient than data sampled on a cubic lattice, and produced volume renderings using splatting. We extend this work to generate isosurfaces based on the BCC lattice, and also on the hexagonal-close packed (HCP) grid. This sketch presents a modified version of marching octahedra that simplifies the BCC mesh to an octahedral mesh to reduce the number of triangles generated for the isosurface.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39025890</person_id>
				<author_profile_id><![CDATA[81100072897]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hamish]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Carr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia, Vancouver, BC, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P281744</person_id>
				<author_profile_id><![CDATA[81100175829]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Theu&#223;l]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vienna University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39066886</person_id>
				<author_profile_id><![CDATA[81332516480]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Torsten]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[M&#246;ller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser University, Vancouver, BC, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>55285</ref_obj_id>
				<ref_obj_pid>55279</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bloomenthal, J. 1988. Polygonization of implicit surfaces. <i>Computer Aided Geometric Design</i>, 341--355.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378271</ref_obj_id>
				<ref_obj_pid>378267</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[D&#252;rst, M. 1988. Letters: Additional Reference to "Marching Cubes". <i>Computer Graphics 22</i>, 4, 65--74.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37422</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lorenson, W. E., and Cline, H. E. 1987. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. <i>Computer Graphics 21</i>, 4, 163--169.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>601684</ref_obj_id>
				<ref_obj_pid>601671</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Theussl, T., M&#246;ller, T., and Gr&#246;ller, M. E. 2001. Optimal Regular Volume Sampling. In <i>Proceedings of Visualization 01</i>, IEEE, IEEE, 91--98.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Wyvill, G., McPheeters, C., and Wyvill, B. 1986. Data Structure for Soft Objects. <i>Visual Computer 2</i>, 227--234.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242233</article_id>
		<sort_key>220</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>88</seq_no>
		<title><![CDATA[Motion capture done dirt cheap]]></title>
		<page_from>220</page_from>
		<page_to>220</page_to>
		<doi_number>10.1145/1242073.1242233</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242233</url>
		<abstract>
			<par><![CDATA[<p>Modern real time motion capture systems are complex, large scale installations. They focus on accuracy, reliability, ease of use for technical staff and convenience for performers, but unfortunately they are also prohibitively expensive for many applications where motion capture might otherwise prove useful. Low budget work and experiments, VR applications and university research could all benefit from a low cost alternative for motion capture. Such applications could also tolerate a somewhat lower quality, and possibly even some slight inconvenience for users and performers.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P819754</person_id>
				<author_profile_id><![CDATA[81320490266]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stefan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gustavson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Linkoping University, Norrkoping, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242234</article_id>
		<sort_key>221</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>89</seq_no>
		<title><![CDATA[Motion-based shape illustration]]></title>
		<page_from>221</page_from>
		<page_to>221</page_to>
		<doi_number>10.1145/1242073.1242234</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242234</url>
		<abstract>
			<par><![CDATA[<p>Motion provides strong visual cues for the perception of shape and depth, as demonstrated by cognitive scientists and visual artists. We present a novel visualization technique that uses particle systems to add supplemental motion cues. Based on a set of rules following perceptual and physical principles, particles flowing over the surface of an object not only bring out, but also attract attention to essential shape information of the object that might not be readily visible with conventional rendering. Replacing still images with animations in this fashion, we show with both surface and volumetric models that the resulting visualizations effectively enhance the perception of three-dimensional shape and structure.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P345953</person_id>
				<author_profile_id><![CDATA[81100586413]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[B.]]></middle_name>
				<last_name><![CDATA[Lum]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Davis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P412738</person_id>
				<author_profile_id><![CDATA[81100258597]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Aleksander]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stompel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Davis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15032496</person_id>
				<author_profile_id><![CDATA[81452595774]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kwan-Liu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Davis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242235</article_id>
		<sort_key>222</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>90</seq_no>
		<title><![CDATA[Music-driven motion editing]]></title>
		<page_from>222</page_from>
		<page_to>222</page_to>
		<doi_number>10.1145/1242073.1242235</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242235</url>
		<abstract>
			<par><![CDATA[<p>Sounds are generally associated with motion events in the real world, and as a result there is an intimate connection between the two. Hence, producing effective animations requires the correct association of sound and motion, which remains an essential, yet difficult, task. But unlike prior, application-specific systems such as [Lytle90; Singer97], we address this problem with a general framework for synchronizing motion curves to perceptual cues extracted from the music. The user is able to modify existing motions rather than needing to incorporate unadapted musical motions into animations. An additional fundamental feature of our system is the use of music analysis techniques on complementary MIDI and analog audio representations of the same soundtrack.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P522819</person_id>
				<author_profile_id><![CDATA[81100360278]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cardle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Cambridge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP54031734</person_id>
				<author_profile_id><![CDATA[81100076680]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brooks]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Cambridge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35024313</person_id>
				<author_profile_id><![CDATA[81100111550]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Loic]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barthe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Cambridge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864735</person_id>
				<author_profile_id><![CDATA[81430655137]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Mo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hassan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Cambridge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39066995</person_id>
				<author_profile_id><![CDATA[81350590544]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Robinson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Cambridge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lytle, W. 1990. Driving Computer Graphics Animation From A Musical Score. <i>Scientific Excellence in Supercomputing: IBM 1990 Papers</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Singer, E., and Rowe, R., 1997. Two highly integrated real-time music and graphics performance systems. <i>ICMC</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242236</article_id>
		<sort_key>223</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>91</seq_no>
		<title><![CDATA[Now and then, here and there]]></title>
		<subtitle><![CDATA[industrilandskapet]]></subtitle>
		<page_from>223</page_from>
		<page_to>223</page_to>
		<doi_number>10.1145/1242073.1242236</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242236</url>
		<abstract>
			<par><![CDATA[<p>As cultural heritages go, or rather stand firm, the Norrk&#246;ping "industrial landscape" (industrilandskapet in Swedish) has been the object of cultural regeneration over the last two decades. Once, one of Sweden's largest city center industrial sites, major supplier of textile and paper it has now become the nexus of cultural activity. Museums, a concert hall and, the university, have now made the scenery different. In turning the landscape over we might think that old industrial life has succumbed to the changes in industrialisation making Norrk&#246;ping move from manufacture to knowledge industry. In museums, this era of "old" is depicted in nostalgic ways - as "lost" cultural heritage rather than as living imagery. And, where did manufacturing go? Is it the case that industry has changed or has it just moved.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864709</person_id>
				<author_profile_id><![CDATA[81328488271]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Konstantin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Economou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Link&#246;ping, Norrk&#246;ping Campus, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP30040438</person_id>
				<author_profile_id><![CDATA[81100359615]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ollila]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Link&#246;ping, Norrk&#246;ping Campus, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864728</person_id>
				<author_profile_id><![CDATA[81328488135]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Martin]]></first_name>
				<middle_name><![CDATA[Etherton]]></middle_name>
				<last_name><![CDATA[Friberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Link&#246;ping, Norrk&#246;ping Campus, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P316938</person_id>
				<author_profile_id><![CDATA[81100419859]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Anders]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ynnerman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Link&#246;ping, Norrk&#246;ping Campus, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242237</article_id>
		<sort_key>224</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>92</seq_no>
		<title><![CDATA[Occidio]]></title>
		<page_from>224</page_from>
		<page_to>224</page_to>
		<doi_number>10.1145/1242073.1242237</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242237</url>
		<abstract>
			<par><![CDATA[<p>This sketch presents an artwork entitled <i>Occidio</i>, which has been produced to interpret the alarming phenomena of global warming in the form of a computer-controlled sound and DVD installation. <i>Occidio</i> interprets NASA scientific visualizations through the interplay of video projections, event-triggered synthetic sound generated by optical theremins, and metaphoric sculptural forms. Visitors to <i>Occidio</i> shall "see," <i>and</i>, "hear" interpretations of global warming.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864780</person_id>
				<author_profile_id><![CDATA[81328490024]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Timothy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nohe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Maryland Baltimore County, Baltimore, MD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242238</article_id>
		<sort_key>225</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>93</seq_no>
		<title><![CDATA[Painterly fire]]></title>
		<page_from>225</page_from>
		<page_to>225</page_to>
		<doi_number>10.1145/1242073.1242238</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242238</url>
		<abstract>
			<par><![CDATA[<p>This sketch describes how to construct a painterly 'wall' of fire, one which exhibits realistic motion while managing to maintain an artistic look.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[effects animation]]></kw>
			<kw><![CDATA[natural phenomena]]></kw>
			<kw><![CDATA[rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Splines</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Graphics data structures and data types</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010394</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics file formats</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003649.10003657.10003659</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic representations->Nonparametric representations->Spline models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>PP35038484</person_id>
				<author_profile_id><![CDATA[81100537885]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Saty]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raghavachary]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dream Works Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864659</person_id>
				<author_profile_id><![CDATA[81328487656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fernando]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Benitez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dream Works Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242239</article_id>
		<sort_key>226</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>94</seq_no>
		<title><![CDATA[Painting with light]]></title>
		<page_from>226</page_from>
		<page_to>226</page_to>
		<doi_number>10.1145/1242073.1242239</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242239</url>
		<abstract>
			<par><![CDATA[<p>This sketch presents fractal art work created by ray tracing the specular highlights of point lights on the inside surface of a hollow sphere. The sphere has a mirror surface on the inside that contributes no colour to the images, but there is spread in the local specular reflection. The resulting images consist entirely of specular highlights and their reflections. I call this painting with light because, although recursive ray tracing is used, no objects are visible.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864705</person_id>
				<author_profile_id><![CDATA[81100550434]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[G]]></middle_name>
				<last_name><![CDATA[Suffern]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Technology, Sydney, Australia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Musgrave F. K. 1998. A Brief Introduction to Fractals, in D. S. Ebert Ed., <i>Texturing and Modelling, A Procedural Approach</i>, Second Edition, AP Professional, Boston.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Perlin K. 1985. An Image Synthesizer, In <i>Proceedings of ACM SIGGRAPH 1985</i>, ACM Press / ACM SIGGRAPH, New York, B. A. Barsky Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 287--296.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242240</article_id>
		<sort_key>227</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>95</seq_no>
		<title><![CDATA[Pastel-like rendering considering the properties of pigments and support medium]]></title>
		<page_from>227</page_from>
		<page_to>227</page_to>
		<doi_number>10.1145/1242073.1242240</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242240</url>
		<abstract>
			<par><![CDATA[<p>There has been any works specifically on computer generated 3D pastel rendering. Some rendering or imaging softwares can generate pastel drawing-like images recently, but most of them cannot modeling and rendering objects as 3D, and do not have enough power of expression in comparison with real-pastel drawings.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP37034525</person_id>
				<author_profile_id><![CDATA[81328489571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kyoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Murakami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu Institute of Design Graduate School]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37034653</person_id>
				<author_profile_id><![CDATA[81331505544]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Reiji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsuruno]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu Institute of Design]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>558817</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Gooch, A., Gooch, B. 2001. <i>Non-Photorealistic Rendering</i>. A. K. Peters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sousa, M. C., Buchanan, J. W. 1999. <i>Computer-Generated Graphite Pencil Rendering of 3D Polygonal Models</i>. Computer Graphics Forum 18(3), 195--208.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>312375</ref_obj_id>
				<ref_obj_pid>311625</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Takagi, S., Nakajima, M., and Fujishiro, I. 2000. <i>Volumetric Modeling of Additional Techniques in Colored Pencil Drawing</i>. In Conference Abstracts and Applications of ACM SIGGRAPH2000 (Technical Sketches), 276.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242241</article_id>
		<sort_key>228</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>96</seq_no>
		<title><![CDATA[Perceptual gaze extent & level of detail in VR]]></title>
		<subtitle><![CDATA[looking outside the box]]></subtitle>
		<page_from>228</page_from>
		<page_to>228</page_to>
		<doi_number>10.1145/1242073.1242241</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242241</url>
		<abstract>
			<par><![CDATA[<p>A common assumption exploited in perceptual Virtual Reality studies is that eye movements made while immersed in VR generally do not deviate more than 30&#176; (visual angle) from the head-centric view direction (e.g., see Barnes [1979]). In this sketch we report eye tracking evidence which generally supports this observation in the context of peripheral Level Of Detail management during a visual search task in VR. We present results from experiments based on the work of Watson et al. [1997] and discuss an extension to the peripheral degradation paradigm to include a dynamic eye-slaved high-resolution inset.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864676</person_id>
				<author_profile_id><![CDATA[81328489695]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hunter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Murphy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Clemson University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P17869</person_id>
				<author_profile_id><![CDATA[81100175263]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Duchowski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Clemson University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Barnes, G. R. 1979. Vestibulo-Ocular Function During Co-ordinated Head and Eye Movements to Acqurie Visual Targets. <i>Journal of Physiology</i>, 127--147.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Murphy, H., and Duchowski, A. T. 2001. Gaze-Contingent Level Of Detail. In <i>EuroGraphics (Short Presentations)</i>, EuroGraphics.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>267137</ref_obj_id>
				<ref_obj_pid>267135</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Watson, B., Walker, N., Hodges, L. F., and Worden, A. 1997. Managing Level of Detail through Peripheral Degradation: Effects on Search Performance with a Head-Mounted Display. <i>ACM Transactions on Computer-Human Interaction 4</i>, 4 (December), 323--346.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242242</article_id>
		<sort_key>229</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>97</seq_no>
		<title><![CDATA[Perceptual tone mapping operators for high dynamic range scenes]]></title>
		<page_from>229</page_from>
		<page_to>229</page_to>
		<doi_number>10.1145/1242073.1242242</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242242</url>
		<abstract>
			<par><![CDATA[<p>A major goal of realistic image synthesis is to generate images that are both physically and perceptually indistinguishable from reality. One of the practical obstacles in reaching this goal is that the natural world exhibits a wide range of colors and intensities. The range of the luminances in the real world can vary from 10<sup>-4</sup>cd/m<sup>2</sup> (for starlight) to 10<sup>5</sup>cd/m<sup>2</sup> (for a daylight scene). Reproducing these luminances on a cathode-ray tube (CRT) display is currently not possible as the achievable intensities are about 100 cd/m<sup>2</sup> and the practical ratio between maximum and minimum pixel intensity is approximately 100:1. At the University of Bristol, we have constructed a High Dynamic Range (HDR) viewer that is capable of achieving a 10,000:1 contrast ratio. This sketch investigates, by means of psychophysical experiments, the benefits such a HDR device has to offer realistic computer graphics.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P399602</person_id>
				<author_profile_id><![CDATA[81100307825]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ledda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bristol, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40023541</person_id>
				<author_profile_id><![CDATA[81100086839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chalmers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bristol, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39063332</person_id>
				<author_profile_id><![CDATA[81100633003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Exponent]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ayres T., Psychophysical Validation of Photographic Representations, ASME 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. E., and Malik, J. 1997. Recovering High Dynamic Range Radiance Maps from Photographs. In <i>procedings of ACM SIGGRAPH'97</i>, 369--379.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300783</ref_obj_id>
				<ref_obj_pid>300776</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Tumblin, J. and Hodgings, J. K., Guenter, B. 1999. Two Methods for Display of High Contrast Images. <i>ACM Transactions on Graphics, Vol. 18, No. 1</i>, 56--94.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Ward, G. HDRgen software, private communcation, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Ward, G. A Wide Field, High Dynamic Range, Stereographic Viewer. Exponent - Failure Analysis Assoc. (USA)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614380</ref_obj_id>
				<ref_obj_pid>614268</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Ward, G., Rushmeier, H. and Piakto, C. 1997. A Visibility Matching Tone Reproduction Operator for High Dynamic Range Scenes. <i>IEEE Transactions on Visualization and Computer Graphics 3</i>, 4, 291--306.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242243</article_id>
		<sort_key>230</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>98</seq_no>
		<title><![CDATA[Performance driven computer graphics making <i>Odyssey</i>]]></title>
		<page_from>230</page_from>
		<page_to>230</page_to>
		<doi_number>10.1145/1242073.1242243</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242243</url>
		<abstract>
			<par><![CDATA[<p>This animation sketch presents a production case study of how computer graphics work was designed to support and allow for director Jonathan Glazer's live action shooting style on "Odyssey", his latest internationally acclaimed commercial for Levi's. The film features a man and a woman in an ecstatic state of freedom to move. They achieve this by first escaping their drab interior surroundings, relentlessly running through a succession of walls. Once outside, they run vertically up two enormous trees and upon reaching the very tip take a huge leap and launch themselves up towards the stars.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864727</person_id>
				<author_profile_id><![CDATA[81328489372]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Markus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Manninen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Framestore CFC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242244</article_id>
		<sort_key>231</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>99</seq_no>
		<title><![CDATA[Pop.eye]]></title>
		<subtitle><![CDATA[a pop-out video camera system for personal use]]></subtitle>
		<page_from>231</page_from>
		<page_to>231</page_to>
		<doi_number>10.1145/1242073.1242244</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242244</url>
		<abstract>
			<par><![CDATA[<p>A new video camera system, Pop.eye, is able to capture a realtime 3D video based on color image and reflection image. The current prototype system (Figure 1) can capture 160x120 sized images at a rate of 25 frames per second. The captured image of a stuffed toy is viewed as the "Pop-out" image frame shown in Figure 2-left. The "Pop-out" shape is constructed from the captured reflection image. Furthermore, this system is able to output an enhanced "Pop-out" movie that extracts only the target object from background by using depth-key (Figure 2-right).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P429822</person_id>
				<author_profile_id><![CDATA[81100172874]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Isao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mihara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toshiba Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864773</person_id>
				<author_profile_id><![CDATA[81328488559]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Harashima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toshiba Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P264807</person_id>
				<author_profile_id><![CDATA[81100025957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shunichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Numazaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toshiba Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14101029</person_id>
				<author_profile_id><![CDATA[81100266546]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Miwako]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Doi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toshiba Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>286723</ref_obj_id>
				<ref_obj_pid>286498</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Numazaki, S., et al., 1998. A kinetic and 3D image input device. <i>CHI '98 Extended Abstracts</i>, 237--238.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Yamauchi, Y., et al. 2000. Gesture-Based Ping-Pong Game Using Real-Time Depth-Image Input Device. In <i>Proceedings of ACM SIGGRAPH 2000</i>, 207.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242245</article_id>
		<sort_key>232</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>100</seq_no>
		<title><![CDATA[Probabilistically placing primitives]]></title>
		<page_from>232</page_from>
		<page_to>232</page_to>
		<doi_number>10.1145/1242073.1242245</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242245</url>
		<abstract>
			<par><![CDATA[<p>Non-photorealistic rendering often requires placing drawing primitives onto a 2D canvas in such a way that the resulting tone approximates that of a greyscale reference image. Several iterative methods have been used where each stroke is tentatively placed on the canvas and the resulting tone is evaluated with respect to the reference image. [Salisbury et al. 1994; Salisbury et al. 1997; Praun et al. 2001] If the stroke over-darkens the output image it is rejected, otherwise it is accepted. While this back-and-forth iteration between the output and the reference image is capable of producing high-quality results, it is extremely costly in terms of computation and memory references.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31038729</person_id>
				<author_profile_id><![CDATA[81542737756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Adrian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Secord]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39052214</person_id>
				<author_profile_id><![CDATA[81100644737]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wolfgang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heidrich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Pitman, J. 1992. <i>Probability</i>. Springer.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383328</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Praun, E., Hoppe, H., Webb, M., and Finkelstein, A. 2001. Real-time hatching. In <i>Proceedings of SIGGRAPH 2001</i>, 579--584.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192185</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Salisbury, M., Anderson, S., Barzel, R., and Salesin, D. 1994. Interactive pen-and-ink illustration. In <i>Proceedings of SIGGRAPH 94</i>, 101--108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258890</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Salisbury, M., Wong, M., Hughes, J., and Salesin, D. 1997. Orientable textures for image-based pen-and-ink illustration. In <i>Proceedings of SIGGRAPH 97</i>, 401--406.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581924</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Secord, A., Wolfgang, H., and Streit, L. 2002. Fast primitive distribution for illustration. In <i>Proc. 13th Eurographics Rendering Workshop, Pisa, Italy, June 26-June 28, 2002</i>. To appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242246</article_id>
		<sort_key>233</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>101</seq_no>
		<title><![CDATA[Probability paint]]></title>
		<subtitle><![CDATA[controlling group characteristics with PDFs]]></subtitle>
		<page_from>233</page_from>
		<page_to>233</page_to>
		<doi_number>10.1145/1242073.1242246</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242246</url>
		<abstract>
			<par><![CDATA[<p>A plague of frogs, a river of stones, a dragon's back of scales... When a large group should exhibit variation in some characteristic, how can we describe and experiment with that variation and the look and feel of the group as a whole?</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864699</person_id>
				<author_profile_id><![CDATA[81328488644]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kathleen]]></first_name>
				<middle_name><![CDATA[Gretchen]]></middle_name>
				<last_name><![CDATA[Greene]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Herman, Daniel L. 2001. RainMan: Fluid Pseudodynamics with Probabilistic Control for Stylized Raindrops. <i>Conference Abstracts and Applications ACM SIGGRAPH 2001</i>, ACM Press/ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM, 246.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Modesto and Rangaraju 2001. Generic Character Variations, "Shrek" The Story Behind the Screen. Conference Lecture, ACM SIGGRAPH 2001, Los Angeles.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242247</article_id>
		<sort_key>234</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>102</seq_no>
		<title><![CDATA[Pushing the limits of L-systems for time-lapse vine growth in "The Time Machine"]]></title>
		<page_from>234</page_from>
		<page_to>234</page_to>
		<doi_number>10.1145/1242073.1242247</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242247</url>
		<abstract>
			<par><![CDATA[<p>Dreamworks Pictures film "The Time Machine" presented a number of interesting problems related to creating time-lapse photography using computer animation. I was assigned with the task of making realistic vines creep and grow along the surface of the green house as the main character began to move forward in time. In order to keep such a complicated task manageable, I needed to organize the order in which things were processed to keep things workable as changes were made. The pipeline needed to be designed in such a way so that the maximum amount of processing is taken care of at the earliest stage.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28020905</person_id>
				<author_profile_id><![CDATA[81328488404]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jonah]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hall]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242248</article_id>
		<sort_key>235</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>103</seq_no>
		<title><![CDATA[Quikwriting on the responsive workbench]]></title>
		<page_from>235</page_from>
		<page_to>235</page_to>
		<doi_number>10.1145/1242073.1242248</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242248</url>
		<abstract>
			<par><![CDATA[<p>Virtual environments (VEs) like the projection-based Responsive Workbench have greatly enhanced the interactive visualization and manipulation of 3D objects. In these configurations, the classical desktop interaction techniques have to be reconsidered. In particular, a simple operation like text typing, although needed for basic operations like saving one's work under a specific file name or entering a precise numerical value inside an application, is still problematic.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864687</person_id>
				<author_profile_id><![CDATA[81418592562]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jerome]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grosjean]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[i3D INRIA FRANCE]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39042187</person_id>
				<author_profile_id><![CDATA[81100424950]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sabine]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Coquillart]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[i3D INRIA FRANCE]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bowman, D., Ly, V., and Campbell, C. 2001. Pinch keyboard: Natural text input for immersive virtual environments. Tech. Rep. TR-01-15, Virginia Tech Dept. of Computer Science.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Evans, F., Skiena, S., and Varshney, A. Vtype: Entering text in a virtual world. submitted to International Journal of Human-Computer Studies.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288613</ref_obj_id>
				<ref_obj_pid>288392</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Perlin, K. 1998. Quikwriting: Continuous stylus-based text entry. In <i>Proceedings of the ACM Symposium on User Interface Software and Technology</i>, Fast Pen Input, 215--216.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242249</article_id>
		<sort_key>236</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>104</seq_no>
		<title><![CDATA[RAPTOR]]></title>
		<subtitle><![CDATA[towards augmented paleontology]]></subtitle>
		<page_from>236</page_from>
		<page_to>236</page_to>
		<doi_number>10.1145/1242073.1242249</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242249</url>
		<abstract>
			<par><![CDATA[<p>Paleontology is filled with mysteries about organisms such as plants and animals that lived thousands, millions, and billions of years before the first modern humans walked the earth. To solve these mysteries, paleontologists rely on the excavation, analysis, and interpretation of fossils. Fossils are the remains or traces of ancient life forms that are usually preserved in stones and rocks. Examples include bones, teeth, shells, leaf imprints, nests, and footprints. Such fossil discoveries reveal what life on our planet was like long ago. Fossils also disclose the evolution of organisms over time and how they are related to one another. While fossils reveal what ancient living things looked like, they keep us guessing about their color, sounds, and most of all their behavior. Each year, paleontologists continue to piece together the stories of the past.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP40029142</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer Center for Research In Computer Graphics]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P165553</person_id>
				<author_profile_id><![CDATA[81100063439]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[L.]]></first_name>
				<middle_name><![CDATA[Miguel]]></middle_name>
				<last_name><![CDATA[Encarna&#231;&#227;o]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer Center for Research In Computer Graphics]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>618863</ref_obj_id>
				<ref_obj_pid>616073</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bimber, O., Fr&#246;hlich, B., Schmalstieg, D., and Encarna&#231;&#227;o, L. M. 2001. The Virtual Showcase. <i>IEEE Computer Graphics & Applications</i>, vol. 21, no.6, pp. 48--55, <i>ACM SIGGRAPH Sketches and Applications</i>, pp. 227.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242250</article_id>
		<sort_key>237</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>105</seq_no>
		<title><![CDATA[Real time video effects on a PlayStation2]]></title>
		<page_from>237</page_from>
		<page_to>237</page_to>
		<doi_number>10.1145/1242073.1242250</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242250</url>
		<abstract>
			<par><![CDATA[<p>The Sony PlayStation2, with its powerful rendering and vector processing capabilities, built-in MPEG decoder, and large capacity hard disc drive possesses fundamental assets that make it an ideal platform for processing video. Research done at Sony BPRL, where we have extensive experience of video effects processing, has shown that this inexpensive games console can, in fact, create in real time the type of video effects which normally require investment in dedicated hardware (or endless patience as a PC renders them slowly).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864767</person_id>
				<author_profile_id><![CDATA[81328491104]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sarah]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Witt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony B&P Research Labs, Basingstoke, Hants, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242251</article_id>
		<sort_key>238</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>106</seq_no>
		<title><![CDATA[Real-time 3D interaction for augmented and virtual reality]]></title>
		<page_from>238</page_from>
		<page_to>238</page_to>
		<doi_number>10.1145/1242073.1242251</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242251</url>
		<abstract>
			<par><![CDATA[<p>We demonstrate a real-time 3-D augmented reality video-conferencing system. The observer sees the real world from his viewpoint, but modified so that the image of a remote collaborator is rendered into the scene. For each frame, we estimate the transformation between the camera and a fiducial marker using techniques developed in Kato and Billinghurst [1999]. We use a shape-from-silhouette algorithm to generate the appropriate view of the collaborator in real time. This is based on simultaneous measurements from fifteen calibrated cameras that surround the collaborator. The novel view is then superimposed upon the real world image and appropriate directional audio is added. The result gives the strong impression that the virtual collaborator is a real part of the scene.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28021989</person_id>
				<author_profile_id><![CDATA[81100003947]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Simon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Prince]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864610</person_id>
				<author_profile_id><![CDATA[81100418633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Adrian]]></first_name>
				<middle_name><![CDATA[David]]></middle_name>
				<last_name><![CDATA[Check]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39040972</person_id>
				<author_profile_id><![CDATA[81100397017]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Farzam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Farbiz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35038567</person_id>
				<author_profile_id><![CDATA[81100169852]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Todd]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Williamson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Zaxel Systems]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28021577</person_id>
				<author_profile_id><![CDATA[81100448709]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Nik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Johnson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Zaxel Systems]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39045788</person_id>
				<author_profile_id><![CDATA[81100499261]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Billinghurst]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37026465</person_id>
				<author_profile_id><![CDATA[81409593320]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Hirokazu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hiroshima State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>858134</ref_obj_id>
				<ref_obj_pid>857202</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kato, H. and Billinghurst, M. 1999. Marker tracking and HMD calibration for a video based augmented reality conferencing system. In <i>Proc. IWAR</i>, 85--94.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344951</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Matusik, W., Buehler, R., Raskar, R., Gortler, S. J. and McMillan, L. 2000. Image-Based Visual Hulls. <i>In Proc. SIGGRAPH</i>, 369--374.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242252</article_id>
		<sort_key>239</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>107</seq_no>
		<title><![CDATA[Real-time image-space outlining for non-photorealistic rendering]]></title>
		<page_from>239</page_from>
		<page_to>239</page_to>
		<doi_number>10.1145/1242073.1242252</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242252</url>
		<abstract>
			<par><![CDATA[<p>In Non-Photorealistic Rendering (NPR), outlines at object silhouettes, shadow edges and texture boundaries are important visual cues which have previously been difficult to generate in real-time. We present an image-space technique which uses pixel shading hardware to generate these three classes of outlines in real time. In all three cases, we render alternate representations of the desired scene into texture maps which are subsequently processed by pixel shaders to find discontinuities corresponding to outlines in the scene. The outlines are then composited with the shaded scene.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35034553</person_id>
				<author_profile_id><![CDATA[81320492890]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Mitchell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATI Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864633</person_id>
				<author_profile_id><![CDATA[81536532356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brennan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATI Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864652</person_id>
				<author_profile_id><![CDATA[81328487800]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Drew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Card]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATI Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Decaudin, P. 1996. Cartoon-looking rendering of 3D-scenes. Technical Report INRIA 2919, Universit&#233; de Technologie de Compi&#232;gne, France.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hourcade, J. C. and Nicolas, A. 1985. Algorithms for Antialiased Cast Shadows. In <i>Computer and Graphics</i>, 9, 3, 259--265.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Mitchell, J. L. 2002. Image Processing with Direct3D Pixel Shaders. In <i>Vertex and Pixel Shader Tips and Tricks</i>, Wolfgang Engel editor, Wordware,.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97901</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Saito, T and Takahashi, T. 1990. Comprehensible Rendering of 3-D Shapes. In <i>Proceedings of SIGGRAPH 1990</i>, ACM Press/ACM SIGGRAPH, New York. 197--206]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242253</article_id>
		<sort_key>240</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>108</seq_no>
		<title><![CDATA[Real-time view synthesis using commodity graphics hardware]]></title>
		<page_from>240</page_from>
		<page_to>240</page_to>
		<doi_number>10.1145/1242073.1242253</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242253</url>
		<abstract>
			<par><![CDATA[<p>We present a novel use of commodity graphics hardware that effectively combines a plane-sweeping algorithm [Collins 1996] and view synthesis in a single step for real-time, on-line 3D view synthesis. Unlike typical stereo algorithms that use image-based metrics to estimate <i>depths</i>, we focus on using image-based metrics to directly estimate <i>images</i>. Using real-time imagery from a few calibrated cameras, our method can generate new images from nearby viewpoints, without any prior geometric information or requiring any user interaction, in real time and on line.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP77028931</person_id>
				<author_profile_id><![CDATA[81409593896]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ruigang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP77028418</person_id>
				<author_profile_id><![CDATA[81407593950]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Welch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14081142</person_id>
				<author_profile_id><![CDATA[81100206034]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gary]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bishop]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43134419</person_id>
				<author_profile_id><![CDATA[81342513704]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Herman]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Towles]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>794659</ref_obj_id>
				<ref_obj_pid>794190</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Collins, R. T. 1996. A Space-Sweep Approach to True Multi-image Matching. In <i>Proceedings of Conference on Computer Vision and Pattern Recognition</i>, 358--363.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kilgard, M. J. 2000. A Practical and Robust Bump-mapping Technique for Today's GPUs. In <i>Game Developers Conference 2000</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242254</article_id>
		<sort_key>241</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>109</seq_no>
		<title><![CDATA[Recent exact aesthetics applications]]></title>
		<page_from>241</page_from>
		<page_to>241</page_to>
		<doi_number>10.1145/1242073.1242254</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242254</url>
		<abstract>
			<par><![CDATA[<p>Exact aesthetics is a challenging field of the computer-aided visual creativity, reconstructing the methods of design and criticism on an algorithmic basis and integrating a computer into processes of an artistic creation and aesthetic evaluation. The discipline involves principles of mathematics, geometry, theory of communication, perceptual psychology, computer graphics, or generative arts into classifying and assessing the aesthetic phenomena. The sketch introduces recent applications in this domain.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864781</person_id>
				<author_profile_id><![CDATA[81328490546]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tom&#225;&#353;]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Staudek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Masaryk University of Brno, Czechia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864744</person_id>
				<author_profile_id><![CDATA[81328489476]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Petr]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Machala]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Masaryk University of Brno, Czechia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Klinger, A., and Salingaros, N. A., 1999. A pattern measure. Centre for Advanced Spatial Analysis, University College London, Pion, Ltd. URL: http://www.math.utsa.edu/sphere/salingar/-PatternMeasure.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Machala, P., and Staudek, T., 2002. <i>Arthur</i>; an algorithmic system for a computer-aided generation of abstract art. Project's URL:http://fosforos.fi.muni.cz/arthur/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Stiny, G., and Gips, J. 1978. <i>Algorithmic Aesthetics: Computer Models for Criticism and Design in the Arts</i>. University of California Press, Berkeley, Cal.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242255</article_id>
		<sort_key>242</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>110</seq_no>
		<title><![CDATA[Reconstructing or inventing the past]]></title>
		<subtitle><![CDATA[a computer simulation of the <i>unbuilt</i> church by Alvar Aalto]]></subtitle>
		<page_from>242</page_from>
		<page_to>242</page_to>
		<doi_number>10.1145/1242073.1242255</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242255</url>
		<abstract>
			<par><![CDATA[<p>There is a certain mystery surrounding the <i>unbuilt</i> projects or unrealized ideas of famous architects. Often there is an expectation of deeper meaning and hidden genius present in unfulfilled buildings. Some critics go as far as to claim that the best and most interesting projects remain unrealized because of the progressiveness of the ideas associated with those buildings.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35038773</person_id>
				<author_profile_id><![CDATA[81328491221]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrzej]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zarzycki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242256</article_id>
		<sort_key>243</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>111</seq_no>
		<title><![CDATA[Regeneration of real objects in the real world]]></title>
		<page_from>243</page_from>
		<page_to>243</page_to>
		<doi_number>10.1145/1242073.1242256</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242256</url>
		<abstract>
			<par><![CDATA[<p>The vision of this work is make it possible to regenerate real objects that had existed at some moment in the past and/or at some remote location as if they have been transported to the present across space and time. The objects could be museum pieces or items in stores, for example. In this work, we have developed a quick and fully automated system that can capture a three-dimensional image of real objects. This success has brought us close to realizing our vision.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864673</person_id>
				<author_profile_id><![CDATA[81328489467]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matsuoka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Lifestyle and Environmental Technology Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP311117400</person_id>
				<author_profile_id><![CDATA[81542686156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Akira]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Onozawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Lifestyle and Environmental Technology Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14196039</person_id>
				<author_profile_id><![CDATA[81100565553]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hidenori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Lifestyle and Environmental Technology Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P111234</person_id>
				<author_profile_id><![CDATA[81100066409]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hisao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nojima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Lifestyle and Environmental Technology Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Matsuoka, H., Takeuchi, T., Kitazawa, H., and Onozawa, A. 2002. Representation of Pseudo Inter-reflection and Transparency by Considering Characteristics of Human Vision. <i>Proceedings of Eurographics 2002 (to be published)</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Schmalstieg, D., Billinghurst, M., Azuma, R., Hollerer, T., Kato, H., and Poupyrev, I. 2001. Augmented Reality: The Interface is Everywhere. <i>Course Notes of SIGGRAPH 2001</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242257</article_id>
		<sort_key>244</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>112</seq_no>
		<title><![CDATA[Rich curve drawing]]></title>
		<page_from>244</page_from>
		<page_to>244</page_to>
		<doi_number>10.1145/1242073.1242257</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242257</url>
		<abstract>
			<par><![CDATA[<p>In simple style drawings, like Comics and traditional cel animation, curved strokes are relatively important. Of course, the shape of the curve is the most important. However, subtle changes of curve width cannot be ignored. We propose a powerful method allowing subtle width changes to be applied to general 2D curve data. The algorithm is based on curvature information of the input curve, and keeps carefully the impression of the original curvature. The resulting image expresses a pen-and-ink drawing style.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35032784</person_id>
				<author_profile_id><![CDATA[81100652296]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Suguru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Saito]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Inst. of Tech.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864611</person_id>
				<author_profile_id><![CDATA[81328488710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Akane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Inst. of Tech.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14095277</person_id>
				<author_profile_id><![CDATA[81100248575]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Youngha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Inst. of Tech.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39042929</person_id>
				<author_profile_id><![CDATA[81100441141]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Masayuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakajima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Inst. of Tech.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Nakajima, N., Kurihara, Y., Harada, T., Takagi, S., and Yoshimoto, F. 2001. Analysis of characteristics of curves in natural and artifical objects. <i>IPSJ SIGNotes 2001-CG-104 2001</i>, 89, 33--36.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>289871</ref_obj_id>
				<ref_obj_pid>289867</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Steiner, A., Kimmel, R., and Bruckstein, A. M. 1998. Planar shape enhancement and exaggeration. <i>Graphical Models and Image Processing 60</i>, 2, 112--124.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242258</article_id>
		<sort_key>245</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>113</seq_no>
		<title><![CDATA[Sandwiching surfaces]]></title>
		<page_from>245</page_from>
		<page_to>245</page_to>
		<doi_number>10.1145/1242073.1242258</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242258</url>
		<abstract>
			<par><![CDATA[<p>Given a composite spline surface, we show how to efficiently construct two matching triangulations that sandwich the surface. Such a two-sided enclosure, <b>(s<sup>-</sup>, s<sup>+</sup>)</b>, supports collision detection, re-approximation for format conversion, meshing with tolerance, one-sided smoothing and silhouette detection as illustrated in Figure 2.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39046853</person_id>
				<author_profile_id><![CDATA[81100524297]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J&#246;rg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peters]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P687717</person_id>
				<author_profile_id><![CDATA[81300347801]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xiaobin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2246224</ref_obj_id>
				<ref_obj_pid>2246198</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lutterkort, D., and Peters, J. 2001. Optimized refinable enclosures of multivariate polynomial pieces. <i>Comput. Aided Geom. Design 18</i>, 9, 851--863.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lutterkort, D., and Peters, J. 2001. Tight linear bounds on the distance between a spline and its B-spline control polygon. <i>Numerische Mathematik 89</i>, 4, 735--784.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>933760</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lutterkort, D. 2000. <i>Envelopes for Nonlinear Geometry</i>. PhD thesis, Purdue University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242259</article_id>
		<sort_key>246</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>114</seq_no>
		<title><![CDATA[Shader analytical approximations for terrain animation in "The Time Machine"]]></title>
		<page_from>246</page_from>
		<page_to>246</page_to>
		<doi_number>10.1145/1242073.1242259</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242259</url>
		<abstract>
			<par><![CDATA[<p>For <i>The Time Machine</i> the CG effects team faced the daunting task of producing imagery that represented long exposure and time-lapse photography of various types of terrain. The scope of the effort and the shot design largely preempted the use of simulated or exclusively explicit techniques for terrain construction. The challenge we faced was to procedurally model surface detail features into terrain that would visually represent the erosion of volumes of earth and rock over time. These features had to be consistent over time and through the entire volume of earth and rock through which the terrain would erode. These restrictions gave rise to two shader techniques used both in the procedural construction of geometry and in the displacement phase of terrain shading: bouldering and gulleying.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28021362</person_id>
				<author_profile_id><![CDATA[81320489933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Johnny]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gibson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242260</article_id>
		<sort_key>247</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>115</seq_no>
		<title><![CDATA[Shape analogies]]></title>
		<page_from>247</page_from>
		<page_to>247</page_to>
		<doi_number>10.1145/1242073.1242260</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242260</url>
		<abstract>
			<par><![CDATA[<p>This sketch presents "Shape Analogies," a method for learning line styles from examples. With this approach, an artist or end-user simply draws in the desired style; the system analyzes the drawings and generates new imagery in the same style. For example, to design an outline style for a nervous character, one may draw a jittery stroke; to design an outline style for a robot, one may draw a very rigid style with many sharp angles.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P9996</person_id>
				<author_profile_id><![CDATA[81100015154]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aaron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hertzmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39063144</person_id>
				<author_profile_id><![CDATA[81100082450]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Nuria]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Oliver]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14202804</person_id>
				<author_profile_id><![CDATA[81100587184]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Curless]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P270322</person_id>
				<author_profile_id><![CDATA[81407593498]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Seitz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383296</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Efros, A. A., and Freeman, W. T. 2001. Image Quilting for Texture Synthesis and Transfer. In <i>Proceedings of SIGGRAPH 2001</i>, 341--346.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Freeman, W. T., Tenenbaum, J. B., and Pasztor, E. 1999. An example-based approach to style translation for line drawings. Tech. Rep. TR99-11, MERL, Feb.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Harrison, P. 2001. A Non-Hierarchical Procedure for Re-Synthesis of Complex Textures. <i>Proc. WCSG</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383295</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Hertzmann, A., Jacobs, C. E., Oliver, N., Curless, B., and Salesin, D. H. 2001. Image Analogies. <i>Proceedings of SIGGRAPH 2001</i>, 327--340.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>105525</ref_obj_id>
				<ref_obj_pid>105514</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Umeyama, S. 1991. Least-squares estimation of transformation parameters between two point patterns. <i>IEEE Trans. Pattern Anal. Machine Intell. 13</i>, 4, 376--380.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242261</article_id>
		<sort_key>248</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>116</seq_no>
		<title><![CDATA[Shape from distortion]]></title>
		<subtitle><![CDATA[3D range scanning of mirroring objects]]></subtitle>
		<page_from>248</page_from>
		<page_to>248</page_to>
		<doi_number>10.1145/1242073.1242261</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242261</url>
		<abstract>
			<par><![CDATA[<p>Objects with mirroring surfaces are left out of the scope of most recent 3D scanning methods. We developed a new acquisition approach, shape-from-distortion, that focuses on that category of objects, requires only a still camera and a monitor, and generates high quality range scans (plus a normal field). Our contributions are a novel acquisition technique based on environment matting [Chuang et al. 2000] and corresponding geometry reconstruction method that recovers a very precise geometry model for mirroring objects.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P686338</person_id>
				<author_profile_id><![CDATA[81100072135]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marco]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tarini]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Visual Computing Group, Pisa, Italy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P718200</person_id>
				<author_profile_id><![CDATA[81100504611]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hendrik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lensch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Max-Planck-Institut f&#252;r Informatik, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31063483</person_id>
				<author_profile_id><![CDATA[81100051843]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goesele]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Max-Planck-Institut f&#252;r Informatik, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P107233</person_id>
				<author_profile_id><![CDATA[81100315426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hans-Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seidel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Max-Planck-Institut f&#252;r Informatik, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>344844</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chuang, Y. et al. Environment matting extensions: Towards higher accuracy and Real-Time capture. In <i>SIGGRAPH 2000</i>, 121--130.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Rocchini, C., Cignoni, P., Montani, C., Pingi, P., and Scopigno, R. A low cost optical 3D scanner based on structured light. In <i>EG 2001</i>, 299--308.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242262</article_id>
		<sort_key>249</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>117</seq_no>
		<title><![CDATA[Shape-based character animation]]></title>
		<page_from>249</page_from>
		<page_to>249</page_to>
		<doi_number>10.1145/1242073.1242262</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242262</url>
		<abstract>
			<par><![CDATA[<p>In character animation, it is often said that a strong pose is supported by its' silhouette. Since the shape of a 3D character represented in a graphical, 2D screen-space is ultimately our goal, shouldn't our tools better support posing the silhouette? Yet commercial software packages have focused on implementations to pose the character from the inside out, rather than looking at the shape inward.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864655</person_id>
				<author_profile_id><![CDATA[81328488540]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guaglione]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31063912</person_id>
				<author_profile_id><![CDATA[81328490683]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Doug]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sweetland]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242263</article_id>
		<sort_key>250</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>118</seq_no>
		<title><![CDATA[Sketchy rendering]]></title>
		<page_from>250</page_from>
		<page_to>250</page_to>
		<doi_number>10.1145/1242073.1242263</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242263</url>
		<abstract>
			<par><![CDATA[<p>The animated short "a flatpack project" was intended to emulate the aesthetics of traditional art techniques in a digital medium. This necessitated the creation of custom rendering and image processing code to reproduce the appearance of line drawings in both pencil and ink, along with other effects such as the bleeding of ink in water and the application of pastel to paper. This sketch describes some of the techniques used in achieving these ends.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39066722</person_id>
				<author_profile_id><![CDATA[81100303363]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Haddon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Moving Picture Company]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>258896</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Curtis C., Anderson S., Seims J., Fleischer, K. and Salesin, D. 1997. Computer-Generated Watercolor. In <i>Proceedings of ACM SIGGRAPH 1997</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242264</article_id>
		<sort_key>251</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>119</seq_no>
		<title><![CDATA[Slow growing volumetric subdivision]]></title>
		<page_from>251</page_from>
		<page_to>251</page_to>
		<doi_number>10.1145/1242073.1242264</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242264</url>
		<abstract>
			<par><![CDATA[<p>In recent years subdivision methods have been successfully applied to the multi-resolution representation and compression of surface meshes. Unfortunately their use in the volumetric case has remained impractical because of the use of tensor-product generalizations that induce an excessive growth of the mesh size before sufficient number is preformed. This technical sketch presents a new subdivision technique that refines volumetric (and higher-dimensional) meshes at the same rate of surface meshes. The scheme builds adaptive refinements of a mesh without using special decompositions of the cells connecting different levels of resolution. Lower dimensional "sharp" features are also handled directly in a natural way. The averaging rules allow to reproduce the same smoothness of the two best known previous tensor-product refinement methods [Bajaj et al. 2001;MacCracken and Joy 1996].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP40024703</person_id>
				<author_profile_id><![CDATA[81408602369]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Valerio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pascucci]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CASC -- Lawrence Livermore National Laboratory, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>900115</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bajaj, C. L., Warren, J., and Xu, G. 2001. A smooth subdivision scheme for hexaedral meshes. Submitted to <i>"The Visual Computer"</i>. TR-01-05, UT at Austin, April.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237247</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[MacCracken R., and Joy, K. I. 1996. Free-form deformations with lattices of arbitrary topology. In <i>Computer Graphics</i> (SIGGRAPH'96), pages 181--188.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2246206</ref_obj_id>
				<ref_obj_pid>2246196</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Velho, L., and Zorin, D. 2001. 4-8 subdivision. <i>Computer Aided Geometric Design</i>, volume 18, Issue 5, pp 397--427.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242265</article_id>
		<sort_key>252</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>120</seq_no>
		<title><![CDATA[Star fields in 2D]]></title>
		<page_from>252</page_from>
		<page_to>252</page_to>
		<doi_number>10.1145/1242073.1242265</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242265</url>
		<abstract>
			<par><![CDATA[<p>Night scenes filmed on indoor stage sets, and in day or evening light, all have one thing in common-the absence of a starry sky. Even shots filmed at night, under perfect conditions, fail to capture stars. Since the focus is on local subjects which need adequate lighting to be captured, the sky is too under exposed to capture stars.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864723</person_id>
				<author_profile_id><![CDATA[81328488628]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Maria]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Giannakouros]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242266</article_id>
		<sort_key>253</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>121</seq_no>
		<title><![CDATA["Still I Rise" painterly animation off the shelf]]></title>
		<page_from>253</page_from>
		<page_to>253</page_to>
		<doi_number>10.1145/1242073.1242266</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242266</url>
		<abstract>
			<par><![CDATA[<p>Painterly animation has till now been a part of research groups and enthusiast. This film attempts to bring it closer to the masses using commercially available software.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[painterly renderings]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864788</person_id>
				<author_profile_id><![CDATA[81444605095]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Umesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shukla]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Los Angeles, AC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242267</article_id>
		<sort_key>254</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>122</seq_no>
		<title><![CDATA[Stylized flowing hair controlled with NURBS surfaces]]></title>
		<page_from>254</page_from>
		<page_to>254</page_to>
		<doi_number>10.1145/1242073.1242267</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242267</url>
		<abstract>
			<par><![CDATA[<p>A project currently in production at Walt Disney Feature Animation requires realistic hair with a high degree of artistic control. In this sketch we present a set of techniques that achieve these goals along the production pipeline.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864748</person_id>
				<author_profile_id><![CDATA[81474643213]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ramon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Montoya-Vozmediano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28021357</person_id>
				<author_profile_id><![CDATA[81100544780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hammel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242268</article_id>
		<sort_key>255</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>123</seq_no>
		<title><![CDATA[Synchronous pronouncement]]></title>
		<page_from>255</page_from>
		<page_to>255</page_to>
		<doi_number>10.1145/1242073.1242268</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242268</url>
		<abstract>
			<par><![CDATA[<p>Synchronous Pronouncement is a generative interactive installation that will explore the aesthetics of an interactive medium as an extension of our body. The irregularity and unpredictability of our world is a result of unique patterns left behind by movements. The installation will be a representation of interacting processes that operate in nature. Autonomous behaviors generated by code will be triggered by the presence of users for the creation of a unique real-time audiovisual experience.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864766</person_id>
				<author_profile_id><![CDATA[81328491092]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sandra]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Villarreal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pratt Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242269</article_id>
		<sort_key>256</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>124</seq_no>
		<title><![CDATA[Tangible viewpoints]]></title>
		<subtitle><![CDATA[a physical interface for exploring character-driven narratives]]></subtitle>
		<page_from>256</page_from>
		<page_to>256</page_to>
		<doi_number>10.1145/1242073.1242269</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242269</url>
		<abstract>
			<par><![CDATA[<p>Over the centuries, stories have moved from the physical environment (around campfires and on the stage), to the printed page, then to movie, television, and computer screens. Today, using wireless and tag sensing technologies, story creators are able to bring digital stories back into our physical environment. The Tangible Viewpoints project explores how physical objects and augmented surfaces can be used as tangible embodiments of different character perspectives in a multiple point-of-view interactive narrative. These graspable surrogates provide a more direct mode of navigation to the story world, bringing us closer to bridging the gap between the separate realms of bits and atoms within the field of digital storytelling.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP77040052</person_id>
				<author_profile_id><![CDATA[81324491962]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ali]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mazalek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hiroshi Ishii MIT Media Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P98573</person_id>
				<author_profile_id><![CDATA[81100068833]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Glorianna]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Davenport]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hiroshi Ishii MIT Media Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Mazalek, A. 2001. <i>Tangible Interfaces for Interactive Point-of-View Narratives</i>. MS thesis, MIT Media Laboratory.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242270</article_id>
		<sort_key>257</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>125</seq_no>
		<title><![CDATA[Techniques for interactive audience participation]]></title>
		<page_from>257</page_from>
		<page_to>257</page_to>
		<doi_number>10.1145/1242073.1242270</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242270</url>
		<abstract>
			<par><![CDATA[<p>At SIGGRAPH in 1991, Loren and Rachel Carpenter unveiled an interactive entertainment system that allowed members of a large audience to control an onscreen game using red and green reflective paddles. In the spirit of this approach, we present a new set of techniques that enable members of an audience to participate, either cooperatively or competitively, in shared entertainment experiences. Our techniques allow audiences with hundreds of people to control onscreen activity by (1) leaning left and right in their seats, (2) batting a beach ball while its shadow is used as a pointing device, and (3) pointing laser pointers at the screen. All of these techniques can be implemented with inexpensive, off the shelf hardware.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P387560</person_id>
				<author_profile_id><![CDATA[81100068218]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maynes-Aminzade]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP79026283</person_id>
				<author_profile_id><![CDATA[81100493478]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Randy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pausch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP77028538</person_id>
				<author_profile_id><![CDATA[81407593498]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seitz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Carpenter, L., Cinematrix, Video Imaging Method and Apparatus for Audience Participation. US Patents #5210604 (1993) and #5365266 (1994).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242271</article_id>
		<sort_key>258</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>126</seq_no>
		<title><![CDATA[Textures from nonlinear dynamical cascades]]></title>
		<page_from>258</page_from>
		<page_to>258</page_to>
		<doi_number>10.1145/1242073.1242271</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242271</url>
		<abstract>
			<par><![CDATA[<p>A great deal of attention has been paid to the problem of texture synthesis. Procedural techniques have become commonplace. Yet while a proliferation of models, and the success of nonparametric synthesis-from-example methods, has made an extraordinary variety of textures realizable, the creation of novel textures remains a challenge.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31062297</person_id>
				<author_profile_id><![CDATA[81100539828]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mould]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43117278</person_id>
				<author_profile_id><![CDATA[81100188679]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eugene]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fiume]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Devaney, R. 1989. <i>Introduction to Chaotic Dynamical Systems</i>. Addison-Wesley, Redwood City.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ott, E. 1993. <i>Chaos in Dynamical Systems</i>. Cambridge University Press, Melbourne.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383297</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Turk, G. 2001. Texture synthesis on surfaces. In <i>Computer Graphics (SIGGRAPH '01 Proceedings)</i>, 347--354.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122750</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Witkin, A., and Kass, M. 1991. Reaction-diffusion textures. <i>Computer Graphics (Proceedings of SIGGRAPH 91) 25</i>, 4 (July), 299--308.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242272</article_id>
		<sort_key>259</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>127</seq_no>
		<title><![CDATA[The AcceleGlove]]></title>
		<subtitle><![CDATA[a whole-hand input device for virtual reality]]></subtitle>
		<page_from>259</page_from>
		<page_to>259</page_to>
		<doi_number>10.1145/1242073.1242272</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242272</url>
		<abstract>
			<par><![CDATA[<p>We present The AcceleGlove, a novel whole-hand input device to manipulate three different virtual objects: a virtual hand, icons on a virtual desktop and a virtual keyboard using the 26 postures of the American Sign Language (ASL) alphabet.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P747375</person_id>
				<author_profile_id><![CDATA[81100298281]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jose]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Hernandez-Rebollar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The George Washington University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P664843</person_id>
				<author_profile_id><![CDATA[81100146010]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Nicholas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kyriakopoulos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The George Washington University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40033111</person_id>
				<author_profile_id><![CDATA[81100438849]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Lindeman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The George Washington University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>143615</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Sturman, D. J. 1992. <i>Whole-Hand Input</i>. PhD. Thesis, Media Lab, MIT, Cambridge, Mass.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242273</article_id>
		<sort_key>260</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>128</seq_no>
		<title><![CDATA[The AR-ENIGMA]]></title>
		<subtitle><![CDATA[a PDA based interactive illustration]]></subtitle>
		<page_from>260</page_from>
		<page_to>260</page_to>
		<doi_number>10.1145/1242073.1242273</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242273</url>
		<abstract>
			<par><![CDATA[<p>The AR-ENIGMA combines a PDA (personal digital assistant) with a camera, a high-speed wireless network connection and AR (augmented reality) technology to enable museum visitors to interact with an Enigma encryption machine.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P292248</person_id>
				<author_profile_id><![CDATA[81100564325]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Volker]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Paelke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Paderborn, Paderborn, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864690</person_id>
				<author_profile_id><![CDATA[81328490731]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Joerg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stoecklein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Paderborn, Paderborn, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864716</person_id>
				<author_profile_id><![CDATA[81328488656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Lennart]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Groetzbach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Paderborn, Paderborn, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP30040336</person_id>
				<author_profile_id><![CDATA[81100325482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Geiger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Paderborn, Paderborn, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P46788</person_id>
				<author_profile_id><![CDATA[81100532477]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reimann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Paderborn, Paderborn, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P413022</person_id>
				<author_profile_id><![CDATA[81100087629]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Waldemar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rosenbach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Paderborn, Paderborn, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{AR-Toolkit} www.hitl.washington.edu/research/shared_space/download/ {HNF} www.hnf.de/index_en.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242274</article_id>
		<sort_key>261</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>129</seq_no>
		<title><![CDATA[The development of a functional visualization system for the creation of digital human models]]></title>
		<page_from>261</page_from>
		<page_to>261</page_to>
		<doi_number>10.1145/1242073.1242274</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242274</url>
		<abstract>
			<par><![CDATA[<p>Inadequately or poorly designed environments and tools of daily living impose barriers to people with a disability. This issue needs to be addressed in order for people with disabilities to lead full and purposeful lives. To accomplish this goal it is imperative that designers of environments and artifacts have an in-depth knowledge of human functioning in the performance of tasks and problem-solving strategies to develop environments and products that best accommodate performance of these tasks. They require an understanding and useful characterizations of the abilities of people with disabilities and relevant mechanisms to incorporate this into a modern design process.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864693</person_id>
				<author_profile_id><![CDATA[81328489596]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[Jay]]></middle_name>
				<last_name><![CDATA[Miller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mississippi State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28022416</person_id>
				<author_profile_id><![CDATA[81328490797]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Weidong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mississippi State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864662</person_id>
				<author_profile_id><![CDATA[81328488938]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gavin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jenkins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mississippi State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Miller, John J and Wang Weidong. 2001. Application of Computer Visualization Techniques for the Use of Anthropometric Data in the Design Process, In <i>Proceedings of the International Conference on Affective Human Factors Design</i>, ASEAN Academic Press LTD, England UK]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Miller, John J, Wang, W., Jenkins G. 2000a The Development of Computer-based Representations of Anthropometric and Biomechanical Data for Use in Environmental and Product Design, Poster at <i>Infodesign 2000</i>, Coventry England]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Miller, John J, Wang, W., Jenkins G. 2000b The Anthropometric Measurement and Modeling Project, In <i>Proceedings of ACADIA2000</i>, Washington DC]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242275</article_id>
		<sort_key>262</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>130</seq_no>
		<title><![CDATA[The free-form light stage]]></title>
		<page_from>262</page_from>
		<page_to>262</page_to>
		<doi_number>10.1145/1242073.1242275</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242275</url>
		<abstract>
			<par><![CDATA[<p>The Free-form Light Stage captures the reflectance field of an object using a freely movable, hand-held light source. By photographing the object under different illumination conditions, we are able to render the object under any lighting condition, using a linear combination of basis images. Our technique builds on recent techniques, such as the Light Stage [Debevec et al. 2000], where light sources are placed at fixed and known positions (e.g. using a gantry). We remove this limitation, and put no restrictions on light source placement.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P396767</person_id>
				<author_profile_id><![CDATA[81100443133]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Vincent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Masselus]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[K.U.Leuven]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39030961</person_id>
				<author_profile_id><![CDATA[81100172909]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Philip]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dutr&#233;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[K.U.Leuven]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P396744</person_id>
				<author_profile_id><![CDATA[81100458997]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Frederik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Anrys]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[K.U.Leuven]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>344855</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Debevec, P., Hawkins, T., Tchou, C., Duiker, H.-P., Sarokin, W., and Sagar, M. 2000. Acquiring the reflectance field of a human face. In <i>Proceedings of SIGGRAPH 2000</i>, K. Akeley, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM Siggraph, 145--156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>3644</ref_obj_id>
				<ref_obj_pid>3643</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lee, C. H., and Rosenfeld, A. 1985. Improved methods of estimating shape from shading using the light source coordinate system. <i>Artificial Intelligence 26</i>, 125--143.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242276</article_id>
		<sort_key>263</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>131</seq_no>
		<title><![CDATA[The meditation chamber]]></title>
		<subtitle><![CDATA[a debriefing]]></subtitle>
		<page_from>263</page_from>
		<page_to>263</page_to>
		<doi_number>10.1145/1242073.1242276</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242276</url>
		<abstract>
			<par><![CDATA[<p>During the Emerging Technologies exhibition at Siggraph 2001, over 400 attendees experienced The Meditation Chamber. This immersive, bio-interactive environment was designed to use visual, audio, and tactile cues to create, guide, and maintain a user's guided relaxation and meditation experience. During this sketch, the project's producers will discuss the design and implementation of this unique installation. We will also show footage from the experience and discuss the subjective relaxation measures and the GSR, heart rate and respiration data generated by 411 Siggraph attendees.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P344862</person_id>
				<author_profile_id><![CDATA[81100119487]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[A.]]></first_name>
				<middle_name><![CDATA[Fleming]]></middle_name>
				<last_name><![CDATA[Seay]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35038539</person_id>
				<author_profile_id><![CDATA[81100453293]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Diane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gromala]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40022831</person_id>
				<author_profile_id><![CDATA[81100023789]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Larry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hodges]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39040956</person_id>
				<author_profile_id><![CDATA[81100396860]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shaw]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242277</article_id>
		<sort_key>264</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>132</seq_no>
		<title><![CDATA[The nail-mounted tactile display for the behavior modeling]]></title>
		<page_from>264</page_from>
		<page_to>264</page_to>
		<doi_number>10.1145/1242073.1242277</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242277</url>
		<abstract>
			<par><![CDATA[<p>The "SmartFinger" is a new type of tactile display, which is worn on the nail side of the finger. It does not inhibit our tactile sensation, since the ball of the finger is naked and we can feel the environment directly. It is important to insert nothing between a finger and an object.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P715176</person_id>
				<author_profile_id><![CDATA[81100486051]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hideyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ando]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Science and Technology Corporation (JST)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864776</person_id>
				<author_profile_id><![CDATA[81328489468]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40033081</person_id>
				<author_profile_id><![CDATA[81100424140]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Masahiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P278536</person_id>
				<author_profile_id><![CDATA[81100065999]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Taro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maeda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo / JST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[T. Nojima, M. Inami, Y Kawabuchi, T. Maeda, K. Mabuchi and S. Tachi, An Interface for Touching the Interface, ACM SIGGRAPH 2000 Conference Abstracts and Applications, p.125, 2001]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Mascaro S. and Asada H., 2001, "Photoplethysmograph Fingernail Sensors for Measuring Finger Forces Without Haptic Obstruction," IEEE Transactions on Robotics and Automation, Vol. 17, No. 5, pp. 698--708]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[T. Maeda, H. Ando, J. Watanabe, Y. Nonura and T. Miki, "A behavior modeling with wearable robotics -The study of Parasitic Humanoid (VI)", The 6th VRSJ Annual Conference, pp. 153--154 (Japanese)]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242278</article_id>
		<sort_key>265</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>133</seq_no>
		<title><![CDATA[The spatial bi-directional reflectance distribution function]]></title>
		<page_from>265</page_from>
		<page_to>265</page_to>
		<doi_number>10.1145/1242073.1242278</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242278</url>
		<abstract>
			<par><![CDATA[<p>Combining texture mapping with bi-directional reflectance distribution functions (BRDFs) yields a representation of surface appearance with both spatial and angular detail. We call a texture map with a unique BRDF at each pixel a spatial bi-directional reflectance distribution function, or SBRDF. The SBRDF is a six-dimensional function representing the reflectance from each incident direction to each exitant direction at each surface point. Because of the high dimensionality of the SBRDF, previous appearance capture and representation work has focused on <i>either</i> spatial <i>or</i> angular detail, has relied on a small set of basis BRDFs, or has only treated spatial detail statistically [Dana 1999; Lensch 2001].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39063103</person_id>
				<author_profile_id><![CDATA[81100125718]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[McAllister]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UNC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P20146</person_id>
				<author_profile_id><![CDATA[81100264426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Anselmo]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Lastra]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UNC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864623</person_id>
				<author_profile_id><![CDATA[81328487874]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Benjamin]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Cloward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vicious Cycle Software, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39052214</person_id>
				<author_profile_id><![CDATA[81100644737]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Wolfgang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heidrich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UBC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>300778</ref_obj_id>
				<ref_obj_pid>300776</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Dana, K. J., B. V. Ginneken, et al. 1999. Reflectance and texture of real-world surfaces. <i>ACM Transactions on Graphics</i> 18(1): 1--34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258801</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lafortune, E. P. F., S.-C. Foo, et al. 1997. Non-Linear Approximation of Reflectance Functions. In <i>Proc. of SIGGRAPH 1997</i>, 117--126.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732303</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lensch, H., J. Kautz, et al. 2001. Image-Based Reconstruction of Spatially Varying Materials. In <i>Rendering Techniques '01</i>, 103--114.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>348214</ref_obj_id>
				<ref_obj_pid>346876</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Kautz, J. and H.-P. Seidel. 2000. <u>Towards Interactive Bump Mapping with Anisotropic Shift-Variant BRDFs</u>. In <i>Eurographics / SIGGRAPH Workshop on Graphics Hardware</i>, 51--58.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>936281</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[McAllister, D. K. 2002. <i>A Generalized Surface Appearance Representation</i>. Ph.D. Dissertation. University of North Carolina. http://www.cs.unc.edu/~davemc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383276</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[McCool, M., J. Ang, et al. 2001. <u>Homomorphic Factorization of BRDFs for High-Performance Rendering</u>. In <i>Proc. of SIGGRAPH 2001</i>, 171--178.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242279</article_id>
		<sort_key>266</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>134</seq_no>
		<title><![CDATA[The simulation of fluid-rigid body interaction]]></title>
		<page_from>266</page_from>
		<page_to>266</page_to>
		<doi_number>10.1145/1242073.1242279</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242279</url>
		<abstract>
			<par><![CDATA[<p>This sketch describes the modeling of the interaction between fluid and rigid bodies, how to simulate scenes in which fluid pressure acts on a rigid body, and conversely, in which rigid body motion drives a force back to fluid. We construct the interface between fluid simulation using the Cubic Interpolated Propagation (CIP) method and rigid body simulation using the impulse-based method. For fast simulation, we apply the CIP method to uniform structured meshes. For treating the interaction between rigid bodies and fluid efficiently, we use Volume of Solid (VOS) for rigid bodies, and for the collision among rigid bodies, we use Polygon-Polygon collision detection. For fast response to rigid body's collision, we use smaller time step for rigid body than for fluid.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864786</person_id>
				<author_profile_id><![CDATA[81328490769]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tsunemi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takahashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toshiba Corp.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP45027179</person_id>
				<author_profile_id><![CDATA[81343507482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Heihachi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ueki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toshiba CAE Systems, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P22866</person_id>
				<author_profile_id><![CDATA[81100183033]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Atsushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kunimatsu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toshiba Corp.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864670</person_id>
				<author_profile_id><![CDATA[81545353456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hiroko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fujii]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toshiba Corp.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hirt, C. W., and Nichols, B. D. 1981. Volume of Fluid (VOF) Method for the Dynamics of Free Boundaries. <i>Journal of Computational Physics</i>, 39, 201--225.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Yabe, T. 1997. Unified Solver CIP for Solid, Liquid and Gas. <i>Computational Fluid Dynamics Review</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lin, M. C., and Canny, J. 1991. A fast algorithm for incremental distance calculation. <i>International Conference on Robotics and Automation</i>, 1008--1014.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>924581</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Mirtich, B. 1996. <i>Impulse-based Dynamic Simulation of Rigid Body Systems</i>. PhD thesis, University of California, Berkeley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242280</article_id>
		<sort_key>267</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>135</seq_no>
		<title><![CDATA[The time geography project]]></title>
		<subtitle><![CDATA[using computer graphics to visualize problems in social science]]></subtitle>
		<page_from>267</page_from>
		<page_to>267</page_to>
		<doi_number>10.1145/1242073.1242280</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242280</url>
		<abstract>
			<par><![CDATA[<p>In natural sciences the practice of using computers to visualize and analyze data was adopted early. In social and cultural studies, however, computer technology has not been as commonly used for visualization purposes. This project to visualize Time Geographical datasets, the Time Geography Project, is therefore somewhat groundbreaking.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864697</person_id>
				<author_profile_id><![CDATA[81328488055]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kajsa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Elleg&#229;rd]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Link&#246;ping, Norrk&#246;ping Campus, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864691</person_id>
				<author_profile_id><![CDATA[81328490634]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Johan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Torne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Link&#246;ping, Norrk&#246;ping Campus, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P828920</person_id>
				<author_profile_id><![CDATA[81320490915]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Henric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Joanson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Link&#246;ping, Norrk&#246;ping Campus, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P316938</person_id>
				<author_profile_id><![CDATA[81100419859]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Anders]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ynnerman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Link&#246;ping, Norrk&#246;ping Campus, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39062787</person_id>
				<author_profile_id><![CDATA[81406596906]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cooper]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Link&#246;ping, Norrk&#246;ping Campus, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP30040438</person_id>
				<author_profile_id><![CDATA[81100359615]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ollila]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Link&#246;ping, Norrk&#246;ping Campus, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242281</article_id>
		<sort_key>268</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>136</seq_no>
		<title><![CDATA[Three angry men]]></title>
		<subtitle><![CDATA[dramatizing point-of-view using augmented reality]]></subtitle>
		<page_from>268</page_from>
		<page_to>268</page_to>
		<doi_number>10.1145/1242073.1242281</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242281</url>
		<abstract>
			<par><![CDATA[<p>"Three Angry Men" is a novel augmented reality experience that explores the use of Augmented Reality (AR) as a dramatic medium. The user participates in an AR version of the famous twentieth-century play, "Twelve Angry Men," [Rose 1983] which for practical reasons we have abbreviated into a scene involving 3 characters (thus, "Three Angry Men"). The participant finds herself immersed in a physical jury-room, where virtual characters (jurors in the drama, rendered as video-based characters overlaid at appropriate 3D locations around the physical table using a see-through head-worn display) debate the guilt of a young man on trial for murder (see Figure 1).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14044595</person_id>
				<author_profile_id><![CDATA[81100099162]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Blair]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[MacIntyre]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31040749</person_id>
				<author_profile_id><![CDATA[81332490551]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jay]]></first_name>
				<middle_name><![CDATA[David]]></middle_name>
				<last_name><![CDATA[Bolter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864683</person_id>
				<author_profile_id><![CDATA[81328490859]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jeannie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vaughan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P584143</person_id>
				<author_profile_id><![CDATA[81100261533]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Brendan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hannigan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP48022703</person_id>
				<author_profile_id><![CDATA[81430655196]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Emmanuel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moreno]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864725</person_id>
				<author_profile_id><![CDATA[81328488357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Markus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Haas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14184845</person_id>
				<author_profile_id><![CDATA[81100532510]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Maribeth]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gandy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>298574</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bolter, J. D., and Grusin, R. 1999. Remediation: Understanding New Media. Cambridge: MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>159587</ref_obj_id>
				<ref_obj_pid>159544</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Feiner, S., Macintyre, B. and Seligmann, D. 1993. Knowledge-Based Augmented Reality. <i>Communications of the ACM</i> 36(7): 53--62.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Rose, R. 1983. Twelve Angry Men: A Play in Three Acts. Dramatic Publications Company.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242282</article_id>
		<sort_key>269</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>137</seq_no>
		<title><![CDATA[Towards visualizing HCI for immersive environments]]></title>
		<subtitle><![CDATA[the meta-situational tracker]]></subtitle>
		<page_from>269</page_from>
		<page_to>269</page_to>
		<doi_number>10.1145/1242073.1242282</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242282</url>
		<abstract>
			<par><![CDATA[<p>The Metaverse is a synthesized world that combines computer-generated elements with the real-world to either augment of supplant a users view reality. To fully participate in this meta-world, users must access it through a unique interface that is visually immersive and non-restrictive, interactive and collaborative. Our ongoing research program in immersive environments emphasizes techniques that make widespread deployment and use of the Metaverse feasible. To that end, we are addressing technical challenges related to self-configuring, self-monitoring displays. Other researchers have made significant progress in overcoming the obstacles related to building tilted displays. Although these technical advances have made low-cost immersive displays feasible for a larger community of users, there is little understanding of the remaining HCI obstacles that prevent widespread deployment of such systems. Our early work, as part of a five-year evaluation project, focuses on defining the methods that will be required to understand usability and utility of the technology. Additionally, we are developing of tools that will assist researchers in characterizing the behavior of users within nonrestrictive immersive environments.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP40033380</person_id>
				<author_profile_id><![CDATA[81452607405]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jaynes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Kentucky, Lexington, KY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P141622</person_id>
				<author_profile_id><![CDATA[81100593964]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Joan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mazur]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Kentucky, Lexington, KY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864634</person_id>
				<author_profile_id><![CDATA[81100131378]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Cindy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lio]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Kentucky, Lexington, KY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Jaynes, C., Webb, S., Steele, M. "A Scalable Framework for High-Resolution Immersive Displays", <i>International Journal of IETE, Special Issue on Visual Media Processing</i>,, Arvind Nayak (eds.), 2002 (to appear).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>181512</ref_obj_id>
				<ref_obj_pid>181505</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Gay, G., Mazur, J. and Lentini, M. (1994). The use of hypermedia data to enhance design. Computer Graphics: &#60;u&#62;SIGGRAPH&#60;/u&#62;, 28(19), 34--37.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242283</article_id>
		<sort_key>270</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>138</seq_no>
		<title><![CDATA[Tracking 3D puzzle pieces for collaborative learning environments]]></title>
		<page_from>270</page_from>
		<page_to>270</page_to>
		<doi_number>10.1145/1242073.1242283</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242283</url>
		<abstract>
			<par><![CDATA[<p>Children naturally learn about their world by manipulating objects within it. Playing with blocks and puzzles helps to develop their understanding of spatial relationships and other mathematical concepts. Using physical objects also allows them to work and learn in groups. Yet sometimes they need outside intervention from an adult or knowledgeable guide to help them learn more and stay engaged longer. Unfortunately, instructors often have too many students to give each one adequate attention. Our work focuses on developing computer-based "guides on the side" that can "watch" as children play with physical puzzles, and offer help or suggestions as needed. Our approach is to use the physical puzzle pieces as parts of a tangible interface. With our system, children are free to explore and collaborate without a computer, yet they can benefit from the computer's instruction as they need it. We have successfully implemented and tested a 2D Tangram puzzle using this approach [Scarlatos 2002].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P173227</person_id>
				<author_profile_id><![CDATA[81100155873]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lori]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Scarlatos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brooklyn College, Brooklyn, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864761</person_id>
				<author_profile_id><![CDATA[81328489941]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Saira]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Qureshi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brooklyn College, Brooklyn, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P430773</person_id>
				<author_profile_id><![CDATA[81100508897]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shalva]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Landy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CUNY Graduate Center, New York, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>344960</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Anderson, D., Frankel, J. L., Marks, J., Agarwala, A., Beardsley, P., Hodgins, J., Leigh, D., Ryall, K., Sullivan, E., and Yedidia, J. S. 2000. Tangible Interaction + Graphical Interpretation: A New Approach to 3D Modeling. In <i>Proceedings of ACM SIGGRAPH 2000</i>, ACM Press / ACM SIGGRAPH, New York, 393--402.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>603517</ref_obj_id>
				<ref_obj_pid>603512</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Scarlatos, L. L. 2002. TICLE: Using Multimedia Multimodal Guidance to Enhance Learning, <i>Information Sciences 140</i>, 85--103.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>551277</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Trucco, E. and Verri, A. 1998. <i>Introductory Techniques for 3-D Computer Vision</i>, Prentice Hall.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242284</article_id>
		<sort_key>271</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>139</seq_no>
		<title><![CDATA[TWISTER]]></title>
		<subtitle><![CDATA[a media booth]]></subtitle>
		<page_from>271</page_from>
		<page_to>271</page_to>
		<doi_number>10.1145/1242073.1242284</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242284</url>
		<abstract>
			<par><![CDATA[<p>TWISTER (Telexistence Wide-angle Immersive STERe-oscope) is an immersive full-color autostereoscopic display, designed for a face-to-face telecommunication system called "mutual telexistence", where people in distant locations can communicate as if they were in the same virtual three dimensional space.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28014948</person_id>
				<author_profile_id><![CDATA[81100134429]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kenji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P681641</person_id>
				<author_profile_id><![CDATA[81100069321]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Junya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hayashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31062250</person_id>
				<author_profile_id><![CDATA[81328489108]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yutaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kunita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40033081</person_id>
				<author_profile_id><![CDATA[81100424140]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Masahiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P278536</person_id>
				<author_profile_id><![CDATA[81100065999]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Taro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maeda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P272463</person_id>
				<author_profile_id><![CDATA[81100411569]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Susumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>312168</ref_obj_id>
				<ref_obj_pid>311625</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kunita, Y., Inami, M., Maeda, T., and Tachi, S. 1999. Prototype system of mutual tele-existence. In <i>ACM SIGGRAPH '99 Conference Abstracts and Applications</i>, 267.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tachi, S., Maeda, T., Yanagida, Y., Koyanagi, M., and Yokoyama, H. 1996. A method of mutual tele-existence in a virtual environment. In <i>Proceedings of the ICAT'96</i>, 9--18.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242285</article_id>
		<sort_key>272</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>140</seq_no>
		<title><![CDATA[Uber destruction in "The Time Machine"]]></title>
		<page_from>272</page_from>
		<page_to>272</page_to>
		<doi_number>10.1145/1242073.1242285</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242285</url>
		<abstract>
			<par><![CDATA[<p>Digital Domain's character animation team was asked to create a complex destruction sequence for Time Machine's main villain... the Uber Morlock. This was to be no ordinary demise. Our villain had to age rapidly. His skin was to wrinkle, discolor, and disintegrate. His hair was to recede, his clothing to oxidize and fall apart. His muscles were to atrophy, eventually releasing his bones one by one. Finally his bones were to break apart and turn to dust.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864628</person_id>
				<author_profile_id><![CDATA[81328490001]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brad]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Parker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242286</article_id>
		<sort_key>273</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>141</seq_no>
		<title><![CDATA[User customizable real-time fur]]></title>
		<page_from>273</page_from>
		<page_to>273</page_to>
		<doi_number>10.1145/1242073.1242286</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242286</url>
		<abstract>
			<par><![CDATA[<p>Recent advances in real-time fur rendering have enabled the development of more realistic furry characters. In this sketch, we outline a number of advances to the shell and fin based fur rendering technique by Lengyel et al [2001] using the pixel and vertex shader capabilities of modern 3D hardware.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39039632</person_id>
				<author_profile_id><![CDATA[81100365724]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Isidoro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Boston University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35034553</person_id>
				<author_profile_id><![CDATA[81320492890]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Mitchell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATI Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>311554</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Heidrich, W. and Seidel, H.-P. Realistic, Hardware-accelerated Shading and Lighting. In <i>Proceedings of SIGGRAPH 1999</i>, ACM Press / ACM SIGGRAPH, New York. 171--178.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364407</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lengyel, J., Praun, E., Finkelstein, A. and Hoppe, H. Real-Time Fur over Arbitrary Surfaces. In <i>ACM 2001 Symposium on Interactive 3D Graphics</i>, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242287</article_id>
		<sort_key>274</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>142</seq_no>
		<title><![CDATA[Using cellular phones to interact with virtual environments]]></title>
		<page_from>274</page_from>
		<page_to>274</page_to>
		<doi_number>10.1145/1242073.1242287</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242287</url>
		<abstract>
			<par><![CDATA[<p>Distributed VR promises to change the way we work and collaborate [Singhal and Zyda 1999]. In this sketch we will extend the accessibility of the virtual world originally developed in [Larsen and Eriksen 1998] by introducing the use of the modern cellular phone as a platform for primitive interfaces to VR applications. We believe that our use of a cellular phone has led to the first completely pocketable platform for VR user interfaces.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864624</person_id>
				<author_profile_id><![CDATA[81100221826]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bent]]></first_name>
				<middle_name><![CDATA[Dalgaard]]></middle_name>
				<last_name><![CDATA[Larsen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technical University of Denmark]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864681</person_id>
				<author_profile_id><![CDATA[81100362705]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jakob]]></first_name>
				<middle_name><![CDATA[Andreas]]></middle_name>
				<last_name><![CDATA[B&#230;rentzen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technical University of Denmark]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14106349</person_id>
				<author_profile_id><![CDATA[81100285054]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Niels]]></first_name>
				<middle_name><![CDATA[J&#248;rgen]]></middle_name>
				<last_name><![CDATA[Christensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technical University of Denmark]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Larsen, B. D., and Eriksen, M. A. 1998. <i>Multi-User Interactive Virtual Environments</i>. Master's thesis, Technical University of Denmark.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>316724</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Singhal, S., and Zyda, M. 1999. <i>Networked Virtual Environments</i>. Addison Wesley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242288</article_id>
		<sort_key>275</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>143</seq_no>
		<title><![CDATA[Using the virtual terrain project to plan real cities]]></title>
		<subtitle><![CDATA[alternative futures for Hangzhou, China]]></subtitle>
		<page_from>275</page_from>
		<page_to>275</page_to>
		<doi_number>10.1145/1242073.1242288</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242288</url>
		<abstract>
			<par><![CDATA[<p>Traditional architectural software tools are well suited to individual building design, but don't scale up well to large landscapes and cities. In city and regional planning applications, it is important to visualize city blocks and neighborhoods, a task which requires integration of data from geographic information systems and 3D surface models from CAD. However, the development of semantically and visually rich real-time representations of cities and large landscapes has proven difficult. Two major problems are the acquisition of appropriate data, and the volume of data once acquired. A typical city or suburban landscape may have thousands of streets, tens of thousands of buildings, and hundreds of thousands of trees and bushes. Three dimensional data specifically describing all of these objects is not usually available, and visual simulations must be based on abstract classifications originally developed for entirely different purposes, such as tax assessor's databases or generalized vegetation maps.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28021188</person_id>
				<author_profile_id><![CDATA[81320489270]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Flaxman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard Graduate School of Design]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Discoe, B. The Virtual Terrain Project website. http://www.vterrain.org]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Steinitz, C. et al. Alternative Futures for Hangzhou, China. Harvard Graduate School of Design report. 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242289</article_id>
		<sort_key>276</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>144</seq_no>
		<title><![CDATA[Video textures using the auto-regressive process]]></title>
		<page_from>276</page_from>
		<page_to>276</page_to>
		<doi_number>10.1145/1242073.1242289</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242289</url>
		<abstract>
			<par><![CDATA[<p>Recently, there have been attempts at creating 'video textures', that is, synthesising new video clips based on existing ones. Schodl et al. showed new video clips by carefully choosing sub-loops of an original video sequence that could be replayed.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P571380</person_id>
				<author_profile_id><![CDATA[81100210477]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Neill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Campbell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bristol, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P414508</person_id>
				<author_profile_id><![CDATA[81100572315]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Colin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dalton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bristol, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39034770</person_id>
				<author_profile_id><![CDATA[81100256862]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gibson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bristol, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP36028021</person_id>
				<author_profile_id><![CDATA[81100411658]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Barry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thomas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bristol, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242290</article_id>
		<sort_key>277</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>145</seq_no>
		<title><![CDATA[Virtual human interface]]></title>
		<subtitle><![CDATA[towards building an intelligent animated agent]]></subtitle>
		<page_from>277</page_from>
		<page_to>277</page_to>
		<doi_number>10.1145/1242073.1242290</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242290</url>
		<abstract>
			<par><![CDATA[<p>We describe our ongoing research on creating a <i>Virtual Human Interface</i> that employs photo-realistic virtual people and animated characters to provide digital media users with information, learning services and entertainment in a highly personalized manner. Our system was designed to be able to create emotional engagement between the virtual character and the user, thus increasing the efficiency of learning and/or absorbing any information broadcasted through this device. We developed innovative technologies for (i) photo-real facial modeling & animation, (ii) context dependent motion libraries with on-line retargeting, (iii) artificial emotions to modulate the characters' behavior and (iv) artificial vision to make the virtual human "aware" of its surroundings. The second key aspect of our solution is a simple to use high level content authoring process, comprising of video-based MPEG4 facial tracking and an innovative interface called the "Disc Controller", which allows users to create new actors, make them move and even direct them to achieve a final rendered output within minutes.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[AI]]></kw>
			<kw><![CDATA[MPEG4]]></kw>
			<kw><![CDATA[artificial emotion]]></kw>
			<kw><![CDATA[digital actor]]></kw>
			<kw><![CDATA[facial modeling]]></kw>
			<kw><![CDATA[intelligent & autonomous agents]]></kw>
			<kw><![CDATA[motion retargeting]]></kw>
			<kw><![CDATA[real-time animation]]></kw>
			<kw><![CDATA[synthespian]]></kw>
			<kw><![CDATA[virtual human]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P549813</person_id>
				<author_profile_id><![CDATA[81100063196]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bernadette]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kiss]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[VerAnim, Budapest]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P668960</person_id>
				<author_profile_id><![CDATA[81100056991]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[G&#225;bor]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Szij&#225;rt&#243;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[VerAnim, Budapest]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P27894</person_id>
				<author_profile_id><![CDATA[81100248976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Barnab&#225;s]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tak&#225;cs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Elite, Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242291</article_id>
		<sort_key>278</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>146</seq_no>
		<title><![CDATA[Virtual reality interfaces using Tweek]]></title>
		<page_from>278</page_from>
		<page_to>278</page_to>
		<doi_number>10.1145/1242073.1242291</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242291</url>
		<abstract>
			<par><![CDATA[<p>Developers of virtual environments often face a difficult problem: users must have some way to interact with the virtual world. The application developers must determine how to map available inputs (buttons, gestures, etc.) to actions within the virtual environment (VE). As a result, user interfaces may be limited by the input hardware available with a given virtual reality (VR) system.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P511626</person_id>
				<author_profile_id><![CDATA[81100017580]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hartling]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Iowa State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P511580</person_id>
				<author_profile_id><![CDATA[81100305046]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Allen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bierbaum]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Iowa State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39045197</person_id>
				<author_profile_id><![CDATA[81100487435]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Carolina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cruz-Neira]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Iowa State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Elmqvist, N. 2001. <i>3Dwm: Three-Dimensional User Interfaces Using Fast Constructive Solid Geometry</i>. Master's thesis, Chalmers University of Technology, G&#246;teborg.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>186897</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Gamma, E., Helm, R., Johnson, R., and Vlissides, J. 1995. <i>Design Patterns: Elements of Reusable Object-Oriented Software</i>. Addison-Wesley Professional Computing Series. Addison-Wesley Publishing Company, New York, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Hill, L. C. 2000. <i>Usability of 2D Palmtop Interaction Device in Immersive Virtual Environments</i>. Master's thesis, Iowa State University, Ames, IA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[JavaSoft. 1997. <i>JavaBeans 1.01 Specification</i>. Sun Microsystems, Mountain View, CA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[OMG. 2001. <i>The Common Object Request Broker: Architecture and Specification</i>, 2.6 ed. Object Management Group.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242292</article_id>
		<sort_key>279</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>147</seq_no>
		<title><![CDATA[Virtual view generation by linear processing of two differently focused images]]></title>
		<page_from>279</page_from>
		<page_to>279</page_to>
		<doi_number>10.1145/1242073.1242292</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242292</url>
		<abstract>
			<par><![CDATA[<p>In this sketch, we present a novel approach to image-based rendering (IBR) techniques for generating a virtual view image from different positions with arbitrary focus using two differently focused images captured from a fixed position. In the conventional approach [Potmesil and Chakravarty 1981], focus blur have been produced by applying a realistic camera model to given 3D objects. We propose here a much simpler and more effective method only using space-invariant filters to render both parallax and focusing effects on objects in a scene, according to the virtual camera's position and focus depth respectively. The proposed method does not need any segmentation and 3D modeling.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP30040390</person_id>
				<author_profile_id><![CDATA[81100483813]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Akira]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kubota]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37027496</person_id>
				<author_profile_id><![CDATA[81100430459]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kiyoharu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2323062</ref_obj_id>
				<ref_obj_pid>2322505</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Aizawa, K., Kodama, K., and Kubota, A. 2000. Producing object-based special effects by fusing multiple differently focused images. <i>IEEE transactions on Circuits and Systems for Video Technology, 3D special issue 10</i>, 2, 323--330.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>806818</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Potmesil, M., and Chakravarty, I. 1981. A lens and aperture camera model for synthetic image generation. <i>Computer Graphics 15</i>, 3, 297--305.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242293</article_id>
		<sort_key>280</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>148</seq_no>
		<title><![CDATA[Wegzeit]]></title>
		<subtitle><![CDATA[the geometry of relative distance]]></subtitle>
		<page_from>280</page_from>
		<page_to>280</page_to>
		<doi_number>10.1145/1242073.1242293</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242293</url>
		<abstract>
			<par><![CDATA[<p>This web3d project explores how the concept of non-linear space -- that is space structured by relative units -- can be used in VR and architecture. It offers a dynamic view on Los Angeles' structure, radically different from usual architectural representations.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39063140</person_id>
				<author_profile_id><![CDATA[81332519345]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dietmar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Offenhuber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Vienna / Ars Electronica Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	</section>
	<section>
		<section_id>1242294</section_id>
		<sort_key>283</sort_key>
		<section_seq_no>6</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Special session - industrial light & magic]]></section_title>
		<section_page_from>283</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP28022477</person_id>
				<author_profile_id><![CDATA[81341498317]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jackie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[White]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California State University - Los Angeles]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1242295</article_id>
		<sort_key>283</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Yoda and beyond]]></title>
		<subtitle><![CDATA[creating the digital cast of Star Wars episode II]]></subtitle>
		<page_from>283</page_from>
		<page_to>283</page_to>
		<doi_number>10.1145/1242073.1242295</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242295</url>
		<abstract>
			<par><![CDATA[<p>Computer graphics play a starring role in the production of Star Wars Episode II: Attack of the Clones. This session focuses on the creation of the digital cast of the latest prequel to the Star Wars saga. Industrial Light & Magic developed a variety of systems to make the computer-generated characters in this film stand up to the actors with whom they share the screen, both in visual quality and physical realism. These systems also made it possible for digital doubles to stand in for actors in scenes either too difficult or too dangerous to shoot practically. In an effort to match the fidelity of motion of the computer graphics characters to that of their live-action counterparts, physically based simulation was used extensively throughout the production of this film. Multi-layered clothing, skin with underlying musculoskeletal structures, and the motion of rigid bodies each played a key role in imparting a new level of physical realism into the performance of computer graphics elements. The challenge of employing this level of proceduralism is also providing methods for directing the resulting performances. In this session, we will present an overview of the pipeline and systems used to produce Episode II, with the focus of the discussion being on the specialization required to evolve technologies, deeply rooted in academic research, into effective filmmaking tools. The panel will include individuals who played key roles in the development of key digital characters for the latest prequel to Star Wars.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864649</person_id>
				<author_profile_id><![CDATA[81328490977]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dawn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light + Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31061968</person_id>
				<author_profile_id><![CDATA[81100210443]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Geoff]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Campbell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864769</person_id>
				<author_profile_id><![CDATA[81100602513]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sebastian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28021431</person_id>
				<author_profile_id><![CDATA[81320490688]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Zoran]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kacic-Alesic]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP29035212</person_id>
				<author_profile_id><![CDATA[81320495363]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tooley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242296</article_id>
		<sort_key>284</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[The fate of play]]></title>
		<subtitle><![CDATA[game industry revolutionaries speak out]]></subtitle>
		<page_from>284</page_from>
		<page_to>284</page_to>
		<doi_number>10.1145/1242073.1242296</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242296</url>
		<abstract>
			<par><![CDATA[<p>Prominent members of the International Game Developers Association will investigate and discuss the direction of the game industry and the impact interactive entertainment will have on our future. This panel of game industry revolutionaries will explore how game design, character development, online connectivity, business models, and social and cultural implications all weave together with advances in technology to drive the industry forward.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31024610</person_id>
				<author_profile_id><![CDATA[81100044344]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[Della]]></middle_name>
				<last_name><![CDATA[Rocca]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[International Game Developers Association]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28021681</person_id>
				<author_profile_id><![CDATA[81343497290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Raph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koster]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Online Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28021688</person_id>
				<author_profile_id><![CDATA[81100009520]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Lorne]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lanning]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Oddworld Inhabitants]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39066923</person_id>
				<author_profile_id><![CDATA[81328489583]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[3D Realms]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35038527</person_id>
				<author_profile_id><![CDATA[81328490438]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Warren]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Spector]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ION Storm Austin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28022199</person_id>
				<author_profile_id><![CDATA[81332536188]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Will]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wright]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Maxis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242297</article_id>
		<sort_key>285</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Spider-Man]]></title>
		<subtitle><![CDATA[behind the mask]]></subtitle>
		<page_from>285</page_from>
		<page_to>285</page_to>
		<doi_number>10.1145/1242073.1242297</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242297</url>
		<abstract>
			<par><![CDATA[<p>Sony Pictures Imageworks takes you for a spin through the virtual world of Spider-Man. Scott Stokdyk and his team reveal how the effects, buildings, and characters were created in the computer and integrated with the live action. CG Supervisor Ken Hahn discusses the complexities involved in creating the buildings of New York City. CG Supervisor Peter Nofz explains the challenges of setting up characters for animation, and Character look Lead Greg Anderson shows us the process of look development for character lighting.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31063821</person_id>
				<author_profile_id><![CDATA[81332522853]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mary]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reardon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242298</article_id>
		<sort_key>287</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[What's up doc?]]></title>
		<subtitle><![CDATA[a fond remembrance of Chuck Jones]]></subtitle>
		<page_from>287</page_from>
		<page_to>287</page_to>
		<doi_number>10.1145/1242073.1242298</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242298</url>
		<abstract>
			<par><![CDATA[<p>Cartoons turned a corner when Chuck Jones came to town. He transformed Walt Disney's vision to one of wit, humor and mischief. We watched as Wile E. Coyote repeatedly attempted to trap the Roadrunner, only to fall victim to his own falling anvils; Pepe Le Pew's aromatic expressions taught us everything we need to know about unsuccessful romance; Marvin Martian's Gladiator skirt, tennis shoes and romanesque helmet gave us an alternate view of aliens out to destroy the earth. And of course, Bugs Bunny's gregarious self-confidence enabled him to outwit, outsmart, and outsing any adversary. For most of the 20<sup>th</sup> Century, Chuck Jones has shaped the way we see a particular side of the world, and our art and our souls are all the better for it.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39063240</person_id>
				<author_profile_id><![CDATA[81537750556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Smolin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Independent]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28020610</person_id>
				<author_profile_id><![CDATA[81537210356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Clark]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Directing Animator]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864686</person_id>
				<author_profile_id><![CDATA[81328488320]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jennifer]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Emberly]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light & Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864772</person_id>
				<author_profile_id><![CDATA[81328488057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Fossati]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31062587</person_id>
				<author_profile_id><![CDATA[81328490683]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Doug]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sweetland]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Animator/Story Artist]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864620</person_id>
				<author_profile_id><![CDATA[81100499935]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Barry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weiss]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	</section>
	<section>
		<section_id>1242299</section_id>
		<sort_key>291</sort_key>
		<section_seq_no>7</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Studio]]></section_title>
		<section_page_from>291</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P864643</person_id>
				<author_profile_id><![CDATA[81538494356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Collins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Arizona State Unviersity - PRISM]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1242300</article_id>
		<sort_key>291</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[BLUIsculpt#8482;]]></title>
		<page_from>291</page_from>
		<page_to>291</page_to>
		<doi_number>10.1145/1242073.1242300</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242300</url>
		<abstract>
			<par><![CDATA[<p>BLUIsculpt#8482; is an interactive virtual reality application that permits a user to freely sketch voxels inside a ten foot cube for output as physical objects. It connects the imagination to physical form. Our interface presents the user with real-time feedback in the form of surfaces. We save a file representing the surface in rapid protyping format. Generating a solid-by-rapid prototyping completes the cycle of perception and imagination that starts in the physical world, proceeds through vision, thought, imagination that starts in the physical world, proceeds through vision, thought, imagination and the dance of drawing to finally arrive at tangible sculpture. Our enterprise is based on the premise that symbolic representation, of which drawing is an exemplar, derives from perception, imagination and thought. We hold that these mental activities recruit and employ the same brain machinery that is fundamental to voluntary physical movement. Such is the common wisdom of generations of artists, and is well-supported by a mass of more recent studies in cognitive and evolutionary neuroscience.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864625</person_id>
				<author_profile_id><![CDATA[81328487918]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brody]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Aalska Fair-banks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P98389</person_id>
				<author_profile_id><![CDATA[81100092894]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Glenn]]></first_name>
				<middle_name><![CDATA[G.]]></middle_name>
				<last_name><![CDATA[Chappell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Aalska Fair-banks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28020953</person_id>
				<author_profile_id><![CDATA[81328488500]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hartman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Aalska Fair-banks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242301</article_id>
		<sort_key>292</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Cinema 4D XL]]></title>
		<subtitle><![CDATA[advanced 3D software for educators and studio professionals]]></subtitle>
		<page_from>292</page_from>
		<page_to>292</page_to>
		<doi_number>10.1145/1242073.1242301</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242301</url>
		<abstract>
			<par><![CDATA[<p>From the Oscar winning film, <i>Gladiator</i>, to studios, such as Pacific Data Images (PDI), Cinema 4D XL has been used for background design, modeling, and pre-visualization artwork. With its user-friendly interface, powerful animation, advanced modeling tools, and high-speed rendering technology, major film and animation studios, as well as educators have utilized it. Cinema 4D XL's advanced tools, such as a VRML plug-in and various modeling systems, can be used to integrate art with other subjects in the classroom. Because of its user-friendly architecture, Cinema 4D XL is well suited for introductory courses in 3D animation and modeling. With one of the fastest radiosity rendering engines, advanced students and studio professionals will also value the application. Students at all levels will have the capability of building professional digital portfolios.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864638</person_id>
				<author_profile_id><![CDATA[81328488555]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[DaShawn]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Hall]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of the Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>580194</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Koenigsmarck, A. 2002. <i>Maxon Cinema 4D 7: A Workshop for 2D/3D Graphic Pros.</i> Peachpit Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>516344</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Watkins, A. 2001. <i>The Cinema 4D XL Handbook</i>. Charles Rivers Media]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242302</article_id>
		<sort_key>293</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Drawing circle]]></title>
		<page_from>293</page_from>
		<page_to>293</page_to>
		<doi_number>10.1145/1242073.1242302</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242302</url>
		<abstract>
			<par><![CDATA[<p>Drawing Circle borrows the traditional structure and activities of a life drawing class to explore that structure's potential for digital media. It proposes using the familiar structure of a well-lit model or still life surrounded by easels, within the Studio at SIGGRAPH. We intend to explore a merger of this time-tested convention with 3D modeling in an attempt to provide a setting for the further development of participant's skills, and to promote the Studio as a place for active learning and group investigation.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[art]]></kw>
			<kw><![CDATA[life drawing]]></kw>
			<kw><![CDATA[mimesis]]></kw>
			<kw><![CDATA[pedagogy]]></kw>
			<kw><![CDATA[praxis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864739</person_id>
				<author_profile_id><![CDATA[81328490485]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[O.]]></first_name>
				<middle_name><![CDATA[Makai]]></middle_name>
				<last_name><![CDATA[Smith]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Venturi Scott Brown Associates]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35038553</person_id>
				<author_profile_id><![CDATA[81328490613]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stewart]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Arizona State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Barcsay, Jen&#246; (trans. P&#225;l Antal). <i>Anatomy for the Artist</i>. 15th ed. Corvina; Budapest. 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bridgman, George Brant. <i>The Human Machine</i>. Bridgeman Publishers, Pelham, NY. 1939.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>175170</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Holtzman, Steven R. <i>Digital Mantras: The Languages of Abstract and Virtual Worlds</i>. MIT Press, Cambridge. 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>521472</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[McCullough, Malcolm. <i>Digital Craft: The Practiced Digital Hand</i>. MIT Press; Cambridge. 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>550613</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Mitchell, William J., McCullough, Malcolm. <i>Digital Design Media: A Handbook for Architects and Design Professionals</i>. Van Nostrand Reinhold, NY. 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242303</article_id>
		<sort_key>294</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Collaborative frameworks]]></title>
		<subtitle><![CDATA[a proposal for an archive in the studio]]></subtitle>
		<page_from>294</page_from>
		<page_to>294</page_to>
		<doi_number>10.1145/1242073.1242303</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242303</url>
		<abstract>
			<par><![CDATA[<p>This presentation describes a concept proposal for a system that will create an archive of the activities of the Studio at SIGGRAPH 2002. The initial parameters that constrain the design process are that the materials of such repository must be displayed and archived during the conference itself, and they must be formatted in such a manner that facilitates their retrieval and replay at a later date. Additionally, in order to reflect the diversity of content present, the archive must be created in a collaborative manner, and with the help of other Studio participants. The amplitude of the desired coverage is yet another factor to be considered. All of these factors must be considered in the light of the quantity of data that can be potentially generated by such endeavor.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28021145</person_id>
				<author_profile_id><![CDATA[81100198728]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lily]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[D&#237;az-Kommonen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Art and Design Helsinki]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	</section>
	<section>
		<section_id>1242304</section_id>
		<sort_key>297</sort_key>
		<section_seq_no>8</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Web graphics]]></section_title>
		<section_page_from>297</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP39062771</person_id>
				<author_profile_id><![CDATA[81341487525]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Simon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Allardice]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[clingfish.com]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1242305</article_id>
		<sort_key>297</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A case study in web3d film-making]]></title>
		<subtitle><![CDATA[Horses for Courses]]></subtitle>
		<page_from>297</page_from>
		<page_to>297</page_to>
		<doi_number>10.1145/1242073.1242305</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242305</url>
		<abstract>
			<par><![CDATA[<p>This is a presentation of the web3d pipeline and production process used to deliver the multi-lingual interactive animated short film <i>Horses for Courses</i> --- winner of the Web3d Roundup art prize at <i>SIGGRAPH 2001</i>. The film features English, French and Spanish soundtracks, different endings, mouse triggered hotspots, and manual camera controls. Photography, low-polygon modelling, and optimised texture maps, were key to publishing the film as low-bandwidth streaming media in July 2001. The pipeline developed for this project has since been used to achieve higher resolution output for offline applications.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864731</person_id>
				<author_profile_id><![CDATA[81328489290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michela]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ledwidge]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242306</article_id>
		<sort_key>298</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A distributed interactive composition tool]]></title>
		<page_from>298</page_from>
		<page_to>298</page_to>
		<doi_number>10.1145/1242073.1242306</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242306</url>
		<abstract>
			<par><![CDATA[<p>As an interactive, computerized, network oriented musical composition tool, <i>Rabisco</i> allows users to create stream of MIDI data in real-time by drawing simple sketches using a simple 2-D graphical tablet (pad). Utilizing a client-server architecture, <i>Rabisco</i> allows virtual joint compositions where several musicians interact over the network or the internet, each using a <i>Rabisco</i> client. Current applications includes Distance Learning, Interactive Music Composition, and Interaction with real world devices such as robots.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[algorithmic composition]]></kw>
			<kw><![CDATA[computational music]]></kw>
			<kw><![CDATA[java]]></kw>
			<kw><![CDATA[remote application]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P435023</person_id>
				<author_profile_id><![CDATA[81100281462]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[M&#225;rcio]]></first_name>
				<middle_name><![CDATA[O.]]></middle_name>
				<last_name><![CDATA[Costa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute of Computing, UNICAMP, Brazil]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14085207</person_id>
				<author_profile_id><![CDATA[81100217326]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[J&#244;natas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Manzolli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UNICAMP, Brazil]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P435004</person_id>
				<author_profile_id><![CDATA[81100085596]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sharoni]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UNICAMP, Brazil]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Costa, M. O., and Manzolli, J. 2001. Toolbox para Aplica&#231;&#245;es Musicais na Internet. In <i>Proceedings of SBC&M 2001</i>, Recife, Brazil, 125--128.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Wassermann, K. C., Blanchard, M., Bernardet, U., Manzolli, J., and Verschure, P. F. M. J. 2000. Roboser - An Autonomous Interactive Musical Composition System. In <i>Proceedings of ICMC 2000</i>, Berlin, Ed. The International Computer Music Association, 531--534.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242307</article_id>
		<sort_key>299</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Adaptive solid texturing for web graphics]]></title>
		<page_from>299</page_from>
		<page_to>299</page_to>
		<doi_number>10.1145/1242073.1242307</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242307</url>
		<abstract>
			<par><![CDATA[<p>Solid texturing [Peachey 1985; Perlin 1985] has become a well-known computer graphics technology since it was first presented more than fifteen years ago. However, solid texturing still remains problems today, because it consumes too much time and has a very high memory requirement. Although some methods have recently been proposed to solve these problems, almost all of them need the support of hardware accelerators. Hence, these methods could not be applied to all kinds of machines, especially the low-cost ones available over the Internet. Therefore, we present a new method for procedural solid texturing in this paper. Our approach could almost render an object with procedural solid texturing in real-time using only a software solution. The basic idea of this approach is similar to the cache mechanism used for main memory control. Furthermore, to demonstrate that our approach is widely applicable we choose pure Java for it's implementation, since this could not receive any benefit from the hardware and could be executed on the Internet directly.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP37023797</person_id>
				<author_profile_id><![CDATA[81100108039]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bing-Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP36036593</person_id>
				<author_profile_id><![CDATA[81100539710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tomoyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>325246</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Peachey, D. R. 1985. Solid texturing of complex surfaces. <i>Computer Graphics (Proceedings of SIGGRAPH 85) 19</i>, 3, 279--286.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Perlin, K. 1985. An image synthesizer. <i>Computer Graphics (Proceedings of Siggraph 85) 19</i>, 3, 287--296.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>801126</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Williams, L. 1983. Pyramidal parametrics. <i>Computer Graphics (Proceeding of SIGGRAPH 83) 17</i>, 3, 1--11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242308</article_id>
		<sort_key>300</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Architecting a distributed dynamic image server for the web]]></title>
		<page_from>300</page_from>
		<page_to>300</page_to>
		<doi_number>10.1145/1242073.1242308</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242308</url>
		<abstract>
			<par><![CDATA[<p>One of the most labor-intensive tasks in the development of web sites is the creation of derivative images for display of a visual element at multiple sizes or in multiple styles. For instance, a typical online catalog will have at least three different visual representations for each product sold: one thumbnail sized image for visual browsing, one mid sized image for viewing a product description and one large sized image for viewing product detail. The approach outlined in this presentation is to enable the web server to actively generate any derivative image from a high-resolution base image. Derivative images are then cached on the server to speed delivery of subsequent requests for the same derived content.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28020585</person_id>
				<author_profile_id><![CDATA[81100621886]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alain]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chesnais]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TrueSpectra, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28020452</person_id>
				<author_profile_id><![CDATA[81556939756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Beck]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TrueSpectra, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864759</person_id>
				<author_profile_id><![CDATA[81328491184]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Rudy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ziegler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TrueSpectra, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242309</article_id>
		<sort_key>301</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Architectural studios online]]></title>
		<subtitle><![CDATA[the "internet studio network"]]></subtitle>
		<page_from>301</page_from>
		<page_to>301</page_to>
		<doi_number>10.1145/1242073.1242309</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242309</url>
		<abstract>
			<par><![CDATA[<p>The "Internet Studio Network" is an initiative designed to create academic relationships among architectural schools to work on semester-studio projects collaborating via the Internet. Past participants included a maximum of 300 architectural students from Miami, Argentina, Chile, Ecuador and Venezuela, who collaborated in semester-long design studios via the Internet and videoconference technology during the Fall, 2001.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[architecture]]></kw>
			<kw><![CDATA[design studio]]></kw>
			<kw><![CDATA[distance education]]></kw>
			<kw><![CDATA[low-bandwidth and high-bandwidth collaboration]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>K.3.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010432.10010439</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010489</concept_id>
				<concept_desc>CCS->Applied computing->Education</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>PP39066581</person_id>
				<author_profile_id><![CDATA[81100404924]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alfredo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Andia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Florida International University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Andia, Alfredo. 2001. Internet Studios: Teaching Architectural Design Online in the United States and Latin America. In <i>Conference Abstracts and Applications of ACM SIGGRAPH 2001</i>, ACM Press / ACM SIGGRAPH, C. Case, Ed., Computer Graphics Annual Conference Series, ACM, 9--12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242310</article_id>
		<sort_key>302</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Banja]]></title>
		<subtitle><![CDATA[flash programming system & game design]]></subtitle>
		<page_from>302</page_from>
		<page_to>302</page_to>
		<doi_number>10.1145/1242073.1242310</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242310</url>
		<abstract>
			<par><![CDATA[<p>Banja is the first communal adventure game in 3D flash on the web.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864665</person_id>
				<author_profile_id><![CDATA[81328487688]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Guillaume]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Clary]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Team cHmAn]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242311</article_id>
		<sort_key>303</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Color aesthetics for web graphics creation]]></title>
		<page_from>303</page_from>
		<page_to>303</page_to>
		<doi_number>10.1145/1242073.1242311</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242311</url>
		<abstract>
			<par><![CDATA[<p>Color has long played an essential role in successful visual design, but the web has some distinct constraints and considerations that warrant further study by the web graphics and SIGGRAPH community. The objective of this lecture is to establish what special constraints related to color exist, share design principles and techniques to address those constraints, and critique existing examples of web design on the success or lack thereof of color communication.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28022244</person_id>
				<author_profile_id><![CDATA[81100346150]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lynda]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weinman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[lynda.com, inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242312</article_id>
		<sort_key>304</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Combining procedural, polygonal, and bitmap representations using XML]]></title>
		<page_from>304</page_from>
		<page_to>304</page_to>
		<doi_number>10.1145/1242073.1242312</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242312</url>
		<abstract>
			<par><![CDATA[<p>Hybrid representations - that of using image and procedural methods to synthesize images. Procedural methods allow us to describe media (2D images, 3D objects etc.) with very little information and render it photorealistically. Since the procedure is run on the client (for instance a PC or a mobile phone with limited network), it makes sense to adapt the procedure to the properties of the client.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP15038558</person_id>
				<author_profile_id><![CDATA[81309511102]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Anders]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Henrysson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Link&#246;ping, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP30028601</person_id>
				<author_profile_id><![CDATA[81100359615]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ollila]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Link&#246;ping, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242313</article_id>
		<sort_key>305</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Content creation with Anark studio]]></title>
		<page_from>305</page_from>
		<page_to>305</page_to>
		<doi_number>10.1145/1242073.1242313</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242313</url>
		<abstract>
			<par><![CDATA[<p>While setting out to create a multimedia authoring tool, the Anark development team set out to use both common design paradigms and evolving technologies. Considering current Web graphics and general multimedia trends, there is a thrust for incorporating multiple types of media into a single visual presentation. Technologies currently exist to bring the concept of pure 3D to Web graphics, but much of the difficulty in creating 3D content comes in the form of multiple and confusing workflows. What started from this thought is an investigation into the technology, use and exposure of 3D technology not only as a core component, but also as a technique to achieve new design metaphors and practices.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864656</person_id>
				<author_profile_id><![CDATA[81328489634]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Anark Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242314</article_id>
		<sort_key>306</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Creating and implementing photographic 3D on the web]]></title>
		<page_from>306</page_from>
		<page_to>306</page_to>
		<doi_number>10.1145/1242073.1242314</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242314</url>
		<abstract>
			<par><![CDATA[<p>Over the past few years, 3D on the web has been emerging as a viable format. The growth in both computing power and bandwidth has made this technically possible. 3D has many advantages over more traditional frame animations, such as QTVR object movies. These include a greater ability to display objects from any angle, and a greater ability to animate objects and have them react to user interaction. One of the challenges of creating compelling 3D, has been to make it "photorealistic". Recently a new type of 3D has emerged, which is "photographic" 3D. Photography is now being used to create both textures and geometry for 3D objects, and not just in the lab. Diginiche is doing this efficiently and commercially now, using Viewpoint technology.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864678</person_id>
				<author_profile_id><![CDATA[81328489917]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[Paul]]></middle_name>
				<last_name><![CDATA[Nykamp]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Diginiche]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242315</article_id>
		<sort_key>307</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Creating interactive 3D web content using AXEL]]></title>
		<page_from>307</page_from>
		<page_to>307</page_to>
		<doi_number>10.1145/1242073.1242315</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242315</url>
		<abstract>
			<par><![CDATA[<p>AXEL is a complete authoring software to create 3d interactive animation for the Internet. Developed by MindAvenue, AXEL makes it easy for designers to create interactive 3D content without scripting.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28021755</person_id>
				<author_profile_id><![CDATA[81328489306]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Teresa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242316</article_id>
		<sort_key>308</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[CT (city tomography)]]></title>
		<page_from>308</page_from>
		<page_to>308</page_to>
		<doi_number>10.1145/1242073.1242316</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242316</url>
		<abstract>
			<par><![CDATA[<p>CT is a project to reconstruct an existing urban space as a 3D information city on the web. A visitor can browse the city with "building wall browsers" and communicate with other visitors.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[X3D]]></kw>
			<kw><![CDATA[XVL]]></kw>
			<kw><![CDATA[city tomography]]></kw>
			<kw><![CDATA[geographical structure]]></kw>
			<kw><![CDATA[information city]]></kw>
			<kw><![CDATA[space communication]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P11884</person_id>
				<author_profile_id><![CDATA[81100476406]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Akira]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wakita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sphere System Design]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P636439</person_id>
				<author_profile_id><![CDATA[81100316016]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fumio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matsumoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Plannet Architectures]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>330174</ref_obj_id>
				<ref_obj_pid>330160</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Wakita, A., Yajima, M., Harada, T., Toriya, H., and Chiyokura, H. 2000. Xvl: A compact and qualified 3d representation with lattice mesh and surface for the internet. <i>Web3D-VRML 2000</i>, pp.45--51.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Web3D Consortium. 2002. X3D (Extensible 3D): New-generation open web3d standard. http://www.web3d.org/x3d/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242317</article_id>
		<sort_key>309</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[EventScope]]></title>
		<subtitle><![CDATA[discovering Mars with internet-based virtual environments]]></subtitle>
		<page_from>309</page_from>
		<page_to>309</page_to>
		<doi_number>10.1145/1242073.1242317</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242317</url>
		<abstract>
			<par><![CDATA[<p><b><i>Telepresence</i></b> is experiencing a place without physically being there. Telepresence interfaces receive information from robots or sensors in distant, hard to reach places. Scientists use telepresence to explore places that are inaccessible to human beings, such as Mars. However, the technology used on such missions is so complex that the missions themselves are as inaccessible to the public as the extreme environments being studied. Subsequently, design and engineering barriers have kept this vast resource off-limits to America's classrooms despite the Internet's widespread proliferation. Existing public telepresence interfaces either do not scale well to worldwide dissemination or do not fully engage school students.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14160463</person_id>
				<author_profile_id><![CDATA[81100456679]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Coppin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon U]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31063446</person_id>
				<author_profile_id><![CDATA[81328488240]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Karl]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fischer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon U]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864738</person_id>
				<author_profile_id><![CDATA[81328489092]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Natalie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon U]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864646</person_id>
				<author_profile_id><![CDATA[81328489549]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Dana]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Martinelli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon U]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P706754</person_id>
				<author_profile_id><![CDATA[81100486972]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[W.]]></first_name>
				<middle_name><![CDATA[Ronald]]></middle_name>
				<last_name><![CDATA[McCloskey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon U]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31030368</person_id>
				<author_profile_id><![CDATA[81100167346]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wagner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242318</article_id>
		<sort_key>310</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Flash MX live]]></title>
		<subtitle><![CDATA[real-time video and audio delivery in multi-user environments]]></subtitle>
		<page_from>310</page_from>
		<page_to>310</page_to>
		<doi_number>10.1145/1242073.1242318</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242318</url>
		<abstract>
			<par><![CDATA[<p>To date, the most common forms of real-time messaging between individuals on the Internet have been text-based, including e-mail and instant messaging. Studies have suggested that Internet users overall spend more hours online with e-mail and instant messaging (IM) than with all other web browsing.<sup>1</sup> Macromedia's new technology opens up exciting possibilities for rich media applications that allow Internet users to hear and see each other instantly, using standard microphones and video devices. Although the reach of broadband connectivity is expanding into more businesses and homes, most Internet users have yet to utilize the full potential of two-way, high-speed communication available with broadband connections.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28021744</person_id>
				<author_profile_id><![CDATA[81100224886]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reinhardt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[theMAKERS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
<i>Study: Four sites account for half of Web surfing</i>. (2001, June 5), Retrieved April 8, 2002, from http://www.cnn.com/2001/TECH/internet/06/05/internet.consolidation/]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242319</article_id>
		<sort_key>311</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Gearation]]></title>
		<subtitle><![CDATA[the web3D content for children]]></subtitle>
		<page_from>311</page_from>
		<page_to>311</page_to>
		<doi_number>10.1145/1242073.1242319</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242319</url>
		<abstract>
			<par><![CDATA[<p>This web content adopts the web3D technology designed for children, especially for younger children.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864661</person_id>
				<author_profile_id><![CDATA[81328489880]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fusako]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishikubo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tomy Company Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864720</person_id>
				<author_profile_id><![CDATA[81328490551]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Manabu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tomy Company Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864770</person_id>
				<author_profile_id><![CDATA[81328489786]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shinta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ookino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tomy Company Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864672</person_id>
				<author_profile_id><![CDATA[81328489790]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hiroshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ogihara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tomy Company Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864700</person_id>
				<author_profile_id><![CDATA[81328488112]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kazuhito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ezawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tomy Company Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242320</article_id>
		<sort_key>312</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Geometry compression for ASCII scenes]]></title>
		<page_from>312</page_from>
		<page_to>312</page_to>
		<doi_number>10.1145/1242073.1242320</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242320</url>
		<abstract>
			<par><![CDATA[<p>The most popular way of distributing 3D content on the Web is in form of a textual representation of the scene such as VRML and its variants. The advantage of such a description is that it is very author friendly in the sense of being meaningful to the human reader. A scene represented in a textual format can be viewed, understood, and modified with any text editor (see Figure 1). Most importantly, anyone can do this, even without knowledge about the specific software package that generated the 3D content.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P192290</person_id>
				<author_profile_id><![CDATA[81100297450]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Martin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Isenburg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Isenburg, M. and Alliez, P. Compressing polygon mesh geometry with parallelogram prediction. <i>submitted for publication</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>504503</ref_obj_id>
				<ref_obj_pid>504502</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Isenburg, M. and Snoeyink, J. Compressing polygon meshes as compressable ASCII. In <i>Proceedings of Web3D Symposium'02</i> (Best Paper), pages 1--10, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Isenburg, M. Compressing polygon mesh connectivity with degree duality prediction. In <i>Graphics Interface'02 Conference Proceedings</i>, pages 161--170, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>290162</ref_obj_id>
				<ref_obj_pid>290159</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Moffat, A., Neal., R. M., and Witten, I. H. Arithmetic coding revisited. In <i>ACM Transactions on Information Systems 16</i>, 3, pages 256--294, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Touma, C. and Gotsman, C. Triangle mesh compression. In <i>Graphics Interface'98 Conference Proceedings</i>, pages 26--34, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242321</article_id>
		<sort_key>313</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[Integrating multiple narratives]]></title>
		<subtitle><![CDATA[the mirror that changes]]></subtitle>
		<page_from>313</page_from>
		<page_to>313</page_to>
		<doi_number>10.1145/1242073.1242321</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242321</url>
		<abstract>
			<par><![CDATA[<p>This presentation describes an approach to interface and experience design in which fact and fiction are mixed in multiple narrative modes. <i>The Mirror That Changes</i> is a Flash-based Web project that integrates fact and fiction, introspection and information in several simultaneous narratives of animated text, voiceover, ambient sound and moving image. Designed to explore issues of water and sustainability from the multiple perspectives of personal use and global resource, <i>The Mirror</i> merges extensive research on water use with a sensual evocation of water so that information and symbolism mesh within a minimalist envelope.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28022443</person_id>
				<author_profile_id><![CDATA[81100372570]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Annette]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weintraub]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[City College of New York, New York, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242322</article_id>
		<sort_key>314</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[Interactive 3D characters for web-based learning and accessibility]]></title>
		<page_from>314</page_from>
		<page_to>314</page_to>
		<doi_number>10.1145/1242073.1242322</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242322</url>
		<abstract>
			<par><![CDATA[<p>Interactive, life-like characters can enhance motivation, communication, and knowledge retention in computer-based learning [Lester et al. 2000]. However, until now, the development cost and computational requirements for highquality animation have limited its widespread use.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P74590</person_id>
				<author_profile_id><![CDATA[81100429078]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ed]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sims]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vcom3D, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864644</person_id>
				<author_profile_id><![CDATA[81328490625]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Silverglate]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vcom3D, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>371560</ref_obj_id>
				<ref_obj_pid>371552</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lester, J., Towns, S., Callaway, C., Voerman, J., and FitzGerald, P. (2000), <i>Deictic and Emotive Communication in Animated Pedagogical Agents</i>, in <i>Embodied Conversational Agents</i>, ed. by J. Cassell et al., Massachusetts Institute of Technology, April 2000: 123--154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242323</article_id>
		<sort_key>315</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[<i>_knowscape</i>, a 3D multi-user experimental web browser]]></title>
		<page_from>315</page_from>
		<page_to>315</page_to>
		<doi_number>10.1145/1242073.1242323</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242323</url>
		<abstract>
			<par><![CDATA[<p><i>_knowscape</i> builds new networked communities of knowledge. It explores original forms of online [user's] representation and builds new kinds of virtual world that carry on information.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P46502</person_id>
				<author_profile_id><![CDATA[81100482745]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Babski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P828751</person_id>
				<author_profile_id><![CDATA[81100167978]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[St&#233;phane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Carion]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31063601</person_id>
				<author_profile_id><![CDATA[81100011330]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Keller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P684145</person_id>
				<author_profile_id><![CDATA[81100170893]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Christophe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guignard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>363506</ref_obj_id>
				<ref_obj_pid>363361</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Christian Babski, Patrick Keller, '<i>rhizoreality.mu</i>', Web3D symposium, 2001, ACM SIGGRAPH, pages 109--116.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>352327</ref_obj_id>
				<ref_obj_pid>351440</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Patrick Keller, Christian Babski, '<i>la_fabrique: a Web3D electronic museum for the binary years</i>', Computer Graphics, 2000, ACM SIGGRAPH, Volume 34, Number 2, pages 66--68.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242324</article_id>
		<sort_key>316</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[Multilingual flash applications]]></title>
		<page_from>316</page_from>
		<page_to>316</page_to>
		<doi_number>10.1145/1242073.1242324</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242324</url>
		<abstract>
			<par><![CDATA[<p>Flash has simplified deployment of content on multiple platforms and browsers. Now with the ability to handle Unicode, it takes web applications into the next level. We will be looking at ways for creating multilingual content and a sample multilingual application using FlashMX.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[graphics]]></kw>
			<kw><![CDATA[web-based services]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>K.8.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003227</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003260</concept_id>
				<concept_desc>CCS->Information systems->World Wide Web</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003130.10003233</concept_id>
				<concept_desc>CCS->Human-centered computing->Collaborative and social computing->Collaborative and social computing systems and tools</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P864789</person_id>
				<author_profile_id><![CDATA[81328489998]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Viswanath]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Parameswaran]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[FastCurve Pte. Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242325</article_id>
		<sort_key>317</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Pseudo-3D photo collage]]></title>
		<page_from>317</page_from>
		<page_to>317</page_to>
		<doi_number>10.1145/1242073.1242325</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242325</url>
		<abstract>
			<par><![CDATA[<p>Pseudo-3D photo collage is a new technique for creating extensive pseudo-3D scenes on the Web. This technique enables users to create, publish, navigate and share pseudo-3D scenes by easy operations. Our basic idea comes from an artistic representation "photo collage" on 2D canvases, that is, a general method of scanning and arranging original photos. Photo collage is originally 2D and static graphic representation, while our proposed representation is pseudo-3D and interactive one. Our developed system for pseudo-3D photo collage is called STAMP (Spatio-Temporal Associations with Multiple Photographs). STAMP includes basic two components as tools: <i>STAMP-Maker</i> and <i>STAMP-Navigator</i>, for creating and navigating pseudo-3D scenes respectively.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35024783</person_id>
				<author_profile_id><![CDATA[81100135119]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14096696</person_id>
				<author_profile_id><![CDATA[81100253521]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masatoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Arikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40033424</person_id>
				<author_profile_id><![CDATA[81100609541]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ryosuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shibasaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242326</article_id>
		<sort_key>318</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[Rhythm engine]]></title>
		<page_from>318</page_from>
		<page_to>318</page_to>
		<doi_number>10.1145/1242073.1242326</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242326</url>
		<abstract>
			<par><![CDATA[<p>"Rhythm Engine" (REg) is a spatial communication tool with "Music" and "visual effects" beyond the space time. "REg" proposes an ideal way of new un-simultaneous communication to the current web-world where mainly "exchanging words" on "real time" is getting more focused.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28022445</person_id>
				<author_profile_id><![CDATA[81365592172]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hidenori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Watanave]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Photon, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hidenori "Wtnv" Watanave (PHOTON) 2002. Concept Creation for REg & Producing. System Direction and Web master In Reg entire Project.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tomoyoshi "Tom" Saito (FROM MOST-MUSIC) 2002. System Direction and System Programming.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Hiroya "Tagnya" Tanaka (FROM SPACE-TIME-DESIGNS) 2002. System Direction.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Eishin "Voyage" Kawakami(PHOTON) 2002. Sound Direction, Sound Programming and Texture Design.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Mari "Marim" Asada (PHOTON) 2002. Art Direction and Character Design. Aco "H" Hashimoto (PHOTON) 2002. Team management.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242327</article_id>
		<sort_key>319</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[Scalable vector graphics (SVG)]]></title>
		<subtitle><![CDATA[the world wide web consortium's recommendation for high quality web graphics]]></subtitle>
		<page_from>319</page_from>
		<page_to>319</page_to>
		<doi_number>10.1145/1242073.1242327</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242327</url>
		<abstract>
			<par><![CDATA[<p>Scalable Vector Graphics (SVG) is a language for representing two-dimensional graphics. It was developed by the World Wide Web Consortium (W3C) to be the open standard format for both static and animated vector graphics on Web appliances, from desktop machines to mobile devices. The SVG 1.0 specification, whose authors Include representatives from Adobe, Microsoft, Sun, Kodak, Corel, Macromedia, IBM and Apple, became a W3C Recommendation in September 2001. SVG Is rapidly becoming the open standard of choice for graphics on the Web, and the many SVG implementations already in existence ensure the SVG documents can be viewed on a wide range of platforms.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28021126</person_id>
				<author_profile_id><![CDATA[81100097381]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dean]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jackson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[World Wide Web Consortium (W3C)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242328</article_id>
		<sort_key>320</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>24</seq_no>
		<title><![CDATA[Scalable visualization of super high resolution 3D images for museum archiving]]></title>
		<page_from>320</page_from>
		<page_to>320</page_to>
		<doi_number>10.1145/1242073.1242328</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242328</url>
		<abstract>
			<par><![CDATA[<p>In this report we describe and demonstrate our technology for creating and browsing super high resolution (SHR) & 3D digital content for a variety of applications including museum artifacts and galleries, archeology, anthropology, art design and heritage conservation. SHR 3D images and associated wireframes require large bandwidth to be transmitted in full detail. To address limitations resulting from limited bandwidth, two operations are performed: (i) bandwidth is optimally monitored using a statistical model and (ii) the quality of the 3D objects transmitted are adjusted to best fit the measured bandwidth. Regions of interest (ROIs) specified by users are stored in multiple levels of detail hierarchy. Our approach extends past systems [Martinez et al. 2000] for 2D image browsing, and supports quality of service (QoS) [Vogel et al. 1995] based retrieval. Experimental results demonstrate the feasibility of the proposed approach.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31061943</person_id>
				<author_profile_id><![CDATA[81100646309]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[A.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Basu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Alberta, Canada and SGI, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864714</person_id>
				<author_profile_id><![CDATA[81100098097]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[L]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Alberta, Canada and SGI, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864607</person_id>
				<author_profile_id><![CDATA[81328489687]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[A.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mistri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Alberta, Canada and SGI, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864636</person_id>
				<author_profile_id><![CDATA[81328491084]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[D.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wolford]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Alberta, Canada and SGI, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>346417</ref_obj_id>
				<ref_obj_pid>347319</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Martinez, K. et al. 2000 Object browsing using the Internet imaging protocol. In <i>Proceedings of WWW9</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122748</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Laur, D. and Hanrahan, P. 1991 Hierarchical splatting, In <i>Proceedings of SIGGRAPH</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614621</ref_obj_id>
				<ref_obj_pid>614555</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Vogel, A. et al. 1995 Distributed Multimedia and <i>QoS, IEEE Multimedia</i>, Summer 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Basu, A. and Wiebe, K. J. 1998 Variable resolution teleconferencing, <i>IEEE Transaction on Systems, Man, and Cybernetics</i>, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242329</article_id>
		<sort_key>321</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>25</seq_no>
		<title><![CDATA[SMIL]]></title>
		<subtitle><![CDATA[an introduction]]></subtitle>
		<page_from>321</page_from>
		<page_to>321</page_to>
		<doi_number>10.1145/1242073.1242329</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242329</url>
		<abstract>
			<par><![CDATA[<p>The Synchronized Multimedia Integration Language (SMIL; pronounced "smile") enables authors to bring interactive audiovisual content to the Web. With SMIL, producing audio-visual content is easy; it does not require learning a programming language and can be done using a simple text editor.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28021061</person_id>
				<author_profile_id><![CDATA[81100585043]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Philipp]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoschka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[World Wide Web Consortium (W3C)/INRIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242330</article_id>
		<sort_key>322</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>26</seq_no>
		<title><![CDATA[Spoiral]]></title>
		<subtitle><![CDATA[an online ad-lib mystery]]></subtitle>
		<page_from>322</page_from>
		<page_to>322</page_to>
		<doi_number>10.1145/1242073.1242330</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242330</url>
		<abstract>
			<par><![CDATA[<p>"Spoiral" is a multi-user online mystery game, which is implemented with web3D technology. (Fig1) Five players experience an ad-lib drama together, as if they are playing a role in a detective novel, in which the story proceeds in real time with real people.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28022073</person_id>
				<author_profile_id><![CDATA[81328489924]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kazuyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39041135</person_id>
				<author_profile_id><![CDATA[81335492043]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inakage]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242331</article_id>
		<sort_key>323</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>27</seq_no>
		<title><![CDATA[The bridge]]></title>
		<subtitle><![CDATA[an environment for collaborative design learning]]></subtitle>
		<page_from>323</page_from>
		<page_to>323</page_to>
		<doi_number>10.1145/1242073.1242331</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242331</url>
		<abstract>
			<par><![CDATA[<p>The idea behind the <i>Bridge</i> as a collaborative groupware arose initially with the need to find appropriate tools and environments to facilitate international design collaborations. In particular, this tool needs to support on-line collaborative activities in design learning where language is a barrier and conventional groupware fails to facilitate effective communication. Moreover, designers are inherently dependent on visual elements in all levels and phases of design development, something which current text-based groupware again fails to support.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864793</person_id>
				<author_profile_id><![CDATA[81328490107]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yasuhiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Santo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Hong Kong Polytechnic University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31063564</person_id>
				<author_profile_id><![CDATA[81544498256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Catherine]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Hong Kong Polytechnic University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864719</person_id>
				<author_profile_id><![CDATA[81328490091]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mamata]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Hong Kong Polytechnic University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242332</article_id>
		<sort_key>324</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>28</seq_no>
		<title><![CDATA[The evolution of animation]]></title>
		<subtitle><![CDATA[bedrock revisited]]></subtitle>
		<page_from>324</page_from>
		<page_to>324</page_to>
		<doi_number>10.1145/1242073.1242332</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242332</url>
		<abstract>
			<par><![CDATA[<p>Flash is the future of television animation. It cuts costs, saves time, and empowers the artist. There is a gap in the education and understanding of how to effectively utilize this program for broadcast animation purposes. This lecture will serve to educate this gap from both an artistic and production point of view. Creating broadcast animation has long been limited to having lots of money and time. The average half hour cartoon runs between 300,000 to 1.5 million dollars to produce. The traditional animation process takes over twelve weeks to produce one episode with most of the grunt work being done overseas. Flash is an artistically empowering program that will change the face of animation within the next year. Flash is slowly creeping into broadcast. Some traditionalists have hesitated to accept Flash animation as viable method of production is because the taint from the dot com era. The reason Flash has not been accepted as a viable mainstream production method is because for the most part, the animation that has been produced on the web lacked the quality of television and film. Currently, there are very few animators who have walked on both sides of the fence. But any of them will tell you Flash Animation will become a necessity in the television industry.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28020650</person_id>
				<author_profile_id><![CDATA[81100570853]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sandro]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Corsaro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[sandro corsaro animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242333</article_id>
		<sort_key>325</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>29</seq_no>
		<title><![CDATA[The Raisio archaeology archive]]></title>
		<subtitle><![CDATA[using design to build collaborations]]></subtitle>
		<page_from>325</page_from>
		<page_to>325</page_to>
		<doi_number>10.1145/1242073.1242333</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242333</url>
		<abstract>
			<par><![CDATA[<p>Sometimes web sites get built to promote a company or a product. It is also possible that a website is constructed as part of the documentation strategy in a project. In my presentation I want to talk about how a website can be used to foster collaboration among the partners in a research project.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28021184</person_id>
				<author_profile_id><![CDATA[81100198728]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lily]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[D&#237;az-Kommonen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Art and Design Helsinki, Helsinki, Finland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fox Michael J., & Wilkerson Peter L. 1998. <i>Introduction to Archival Organization and Description</i>, The Getty Information Institute, Suzanne Warren, Ed., 20--21.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242334</article_id>
		<sort_key>326</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>30</seq_no>
		<title><![CDATA[The reality cluster]]></title>
		<subtitle><![CDATA[realtime multimedia communication with persistence]]></subtitle>
		<page_from>326</page_from>
		<page_to>326</page_to>
		<doi_number>10.1145/1242073.1242334</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242334</url>
		<abstract>
			<par><![CDATA[<p>The behavioral and cognitive principles of collaboration are well understood, i.e. how people negotiate common meaning in order to work together. During the implementation of collaborative systems, however, the significance of these principles in humancomputer interaction are often shadowed by the low-level challenges of building networked applications. The REALITY CLUSTER project explores the question "What if building networked applications was easy?" by utilizing new technologies recently introduced by Macromedia and the Flash 6 plugin.</p> <p>The REALITY CLUSTER allows multiple users to manipulate a graphical representation of both real-time and stored information in a common repository. The user interface for REALITY CLUSTER borrows principles found in information visualization literature to show relationships between multiple nodes of information while providing users with both focus and context in navigating the nodes. Each node may consist of either recordings or real-time channels for video, audio, text, and static graphics. Hopefully, the REALITY CLUSTER prototype will open web developers to new perspectives in designing web applications. We believe that these technologies from Macromedia, combined with strong grounding in HCI principles and software engineering, will fulfill the promise of a truly disintermediated network communication.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P364420</person_id>
				<author_profile_id><![CDATA[81100011633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Branden]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hall]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fig Leaf Software]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28022151</person_id>
				<author_profile_id><![CDATA[81100388359]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Samuel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Michigan School of Information]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242335</article_id>
		<sort_key>327</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>31</seq_no>
		<title><![CDATA[The Xj3D browser]]></title>
		<subtitle><![CDATA[community-based 3D software development]]></subtitle>
		<page_from>327</page_from>
		<page_to>327</page_to>
		<doi_number>10.1145/1242073.1242335</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242335</url>
		<abstract>
			<par><![CDATA[<p>This presentation outlines the development process of the Xj3D browser. Xj3D is an open source API for developing X3D and VRML 97 applications. It is also the sample implementation and test bed for the next generation VRML specification known as Extensible 3D (X3D). Indeed, Xj3D was initiated by the Web3D Consortium to provide input to the X3D authors and the 3D graphics community with input concerning problems and ambiguities with the specification.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P433207</person_id>
				<author_profile_id><![CDATA[81100566886]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Hudson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yumetech, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28017983</person_id>
				<author_profile_id><![CDATA[81100436566]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Justin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Couch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yumetech, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28018475</person_id>
				<author_profile_id><![CDATA[81320492266]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[N.]]></middle_name>
				<last_name><![CDATA[Matsuba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yumetech, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA["Xj3D Open Source VRML/X3D Toolkit." http://www.web3d.org/fs_workinggroups.htm.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242336</article_id>
		<sort_key>328</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>32</seq_no>
		<title><![CDATA[TTT]]></title>
		<subtitle><![CDATA[a web community tool mediated by friends]]></subtitle>
		<page_from>328</page_from>
		<page_to>328</page_to>
		<doi_number>10.1145/1242073.1242336</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242336</url>
		<abstract>
			<par><![CDATA[<p>TTT is a Web community tool that promotes new encounters mediated by friends.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P864796</person_id>
				<author_profile_id><![CDATA[81328488660]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Haraguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864763</person_id>
				<author_profile_id><![CDATA[81328490660]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sakura]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Toyabe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864729</person_id>
				<author_profile_id><![CDATA[81328489653]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Masaru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Murata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242337</article_id>
		<sort_key>329</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>33</seq_no>
		<title><![CDATA[VisSheet redux]]></title>
		<subtitle><![CDATA[redesigning a visualization exploration spreadsheet for the web]]></subtitle>
		<page_from>329</page_from>
		<page_to>329</page_to>
		<doi_number>10.1145/1242073.1242337</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242337</url>
		<abstract>
			<par><![CDATA[<p>The exploration of complex data sets requires interfaces to present and navigate through the visualization of the data. In recent work [Jankun-Kelly and Ma 2001], we produced a visualization exploration spreadsheet to address this issue. The developed application, however, was implemented for off-line use only. For data sets on remote sites, this approach is not appropriate. Thus, a web-based version of the visualization exploration spreadsheet is needed. This abstract discusses the process of transforming the interface from an off-line to an on-line design.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P317478</person_id>
				<author_profile_id><![CDATA[81100034517]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[T.]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Jankun-Kelly]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Davis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15032496</person_id>
				<author_profile_id><![CDATA[81452595774]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kwan-Liu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Davis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>614499</ref_obj_id>
				<ref_obj_pid>614283</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Jankun-Kelly, T. J., and Ma, K.-L., 2001. Visualization exploration and encapsulation via a spreadsheet-like interface. <i>IEEE Transactions on Visualization and Computer Graphics 7</i>, 3 (July/Sept.), 275--287.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1242338</article_id>
		<sort_key>330</sort_key>
		<display_label></display_label>
		<article_publication_date>07-21-2002</article_publication_date>
		<seq_no>34</seq_no>
		<title><![CDATA[Wegzeit]]></title>
		<subtitle><![CDATA[the geometry of relative distance]]></subtitle>
		<page_from>330</page_from>
		<page_to>330</page_to>
		<doi_number>10.1145/1242073.1242338</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1242338</url>
		<abstract>
			<par><![CDATA[<p>This web3d project explores how the concept of non-linear space - that is space structured by relative units - can be used in VR and architecture. It offers a dynamic view on Los Angeles' structure, radically different from usual architectural representations.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39063149</person_id>
				<author_profile_id><![CDATA[81332519345]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dietmar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Offenhuber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Vienna / Ars Electronica Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
	</article_rec>
	</section>
</content>
</proceeding>
