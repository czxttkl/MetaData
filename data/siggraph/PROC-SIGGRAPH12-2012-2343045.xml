<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>08/05/2012</start_date>
		<end_date>08/09/2012</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Los Angeles]]></city>
		<state>California</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>2343045</proc_id>
	<acronym>SIGGRAPH '12</acronym>
	<proc_desc>ACM SIGGRAPH 2012 Talks</proc_desc>
	<conference_number>2012</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-1-4503-1683-5</isbn13>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2012</copyright_year>
	<publication_date>08-05-2012</publication_date>
	<pages>51</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>A broad spectrum of presentations on recent achievements in all areas of computer graphics and interactive techniques, including art, design, animation, visual effects, interactivity, research, and engineering.</p> <p>Talks are a less formal alternative to formal publication. They often highlight the latest developments before publication, present ideas that are still in progress, or showcase how computer graphics and interactive techniques are actually implemented and used, in graphics production or other fields. Talks take you behind the scenes and into the minds of SIGGRAPH 2012 creators.</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
</proceeding_rec>
<content>
	<section>
		<section_id>2343046</section_id>
		<sort_key>10</sort_key>
		<section_seq_no>1</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Pointed illumination]]></section_title>
		<section_page_from>1</section_page_from>
	<article_rec>
		<article_id>2343047</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>1</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Progressive Lightcuts for GPU]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343047</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343047</url>
		<abstract>
			<par><![CDATA[<p>Lightcuts [Walter et al. 2005] is an attractive rendering algorithm that efficiently handles complex lighting by adaptively approximating the illumination using clusters of virtual point light (VPL) sources. Two of its limitations are the infinite memory footprint required for variance convergence and the bias introduced by VPL contribution clamping. We present a progressive Lightcuts algorithm, which is consistent and converges using a fixed amount of intermediate memory. This is essential for high quality rendering, especially considering the tight memory budget on the GPU.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737648</person_id>
				<author_profile_id><![CDATA[81474653776]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tom&#225;&#353;]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Davidovi&#269;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Saarland University, Intel VCI Saarbr&#252;cken]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737649</person_id>
				<author_profile_id><![CDATA[81436597914]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Iliyan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Georgiev]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Saarland University, Intel VCI Saarbr&#252;cken]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737650</person_id>
				<author_profile_id><![CDATA[81100159926]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Philipp]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Slusallek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DFKI, Saarland University, Intel VCI Saarbr&#252;cken]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kollig, T., and Keller, A. 2004. Illumination in the presence of weak singularities. In <i>MCQMC Methods</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073318</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Walter, B., Fernandez, S., Arbree, A., Bala, K., Donikian, M., and Greenberg, D. P. 2005. Lightcuts: a scalable approach to illumination. In <i>ACM SIGGRAPH 2005 Papers</i>, ACM, New York, NY, USA, SIGGRAPH '05, 1098--1107.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Progressive Lightcuts for GPU Tom´a.s Davidovi.c Iliyan Georgiev Philipp Slusallek Saarland University 
Saarland University DFKI, Saarland University Intel VCI Saarbr¨ucken Intel VCI Saarbr¨ucken Intel VCI 
Saarbr¨ucken  Figure 1: We test our progressive GPU Lightcuts algorithm on a scene with strong secondary 
lighting and detailed geometry. In the same time, Lightcuts clearly remains superior to path tracing. 
However, one million virtual point lights (VPLs) cannot capture the illumination on the blinds in full 
detail. Increasing the number of VPLs per iteration improves quality but consumes more memory; further 
improvements require accumulating multiple iterations (bottom). In order to remove the VPL clamping bias, 
we additionally apply our progressive clamping relaxation scheme (top). The log-log plot on the right 
compares the convergence of different relaxation schemes, measured as RMS difference against the reference 
image on the left. Abstract Lightcuts [Walter et al. 2005] is an attractive rendering algorithm that 
ef.ciently handles complex lighting by adaptively approxi­mating the illumination using clusters of virtual 
point light (VPL) sources. Two of its limitations are the in.nite memory footprint required for variance 
convergence and the bias introduced by VPL contribution clamping. We present a progressive Lightcuts 
algo­rithm, which is consistent and converges using a .xed amount of intermediate memory. This is essential 
for high quality rendering, especially considering the tight memory budget on the GPU. Overview Figure 
1 demonstrates that on the GPU Lightcuts remains superior to brute-force path tracing on scenes with 
non-uniform illumina­tion. However, it also shows that the limited amount of GPU mem­ory is often insuf.cient 
to store enough VPLs, e.g. more than 1 million in 1.5GB, to capture the illumination on complex geometry 
in full detail. We address this problem by performing rendering it­eratively, and obtain a high .delity 
result via progressive averaging of Lightcuts images, each generated using a different set of VPLs. This 
is made possible by noting that Lightcuts is essentially an un­biased adaptive VPL strati.cation algorithm 
[Walter et al. 2005]. Since variance can be in.nite due to a weak singularity at cor­ners, a common approach 
is to clamp the VPL contribution to some maximum value b. This, however, introduces bias that Kollig 
and Keller [2004] compensate for using solid angle integration, which avoids the weak singularity. This 
path tracing based solution, in turn, again increases variance, which is particularly noticeable at geometric 
corners in the otherwise smooth Lightcuts images. Our approach to making Lightcuts consistent, while 
maintaining its low variance, is based on the observation that the amount of clamping necessary is closely 
related to the number of samples, i.e. total number VPLs, used in the estimate. We develop a progres­sive 
clamping relaxation scheme, which reduces clamping as more VPLs are accumulated to the result over the 
rendering iterations. The clamping parameter b trades off variance and bias. An asymp­totically unbiased 
image can be obtained if both the variance and bias of the accumulated result diminish over time. We 
show that consistency is achieved if the variance of the individual rendering iterations increases sub-linearly. 
For an arbitrary signal bounded by b, its variance is bounded by b2/4. We show that for the speci.c case 
of light transport around corners, where the weak singularities occur, pixel variance increases only 
linearly with b. Therefore, at each iteration i> 1, we use a clamping constant bi = b1i, where b1 is 
the initial bound. With this scheme, both the cumulative variance and bias diminish over time, ensuring 
the estimate consistency. Results We numerically compare different clamping relaxation schemes, with 
a clamping constant computed as bi = b1ia with different values for a. We measure the root mean squared 
(RMS) differ­ence against a reference path tracing solution over time, plotted in Figure 1 right. Without 
clamping relaxation (i.e. a =0), after 20 minutes the image converges to a wrong solution. Both the gener­ally 
conservative a =0.5 scheme as well as the speci.c a =1 scheme converge to the correct solution with rates 
O(N-0.2) and O(N -0.35) respectively, while the schemes with a = 2 are already too aggressive. While 
we present the clamping relaxation is an alternative to clamp­ing compensation, the two can in fact complement 
each other. If used together, clamping relaxation progressively reduces the amount of necessary compensation. 
 References KOLLIG, T., AND KELLER, A. 2004. Illumination in the presence of weak singularities. In MCQMC 
Methods. WALTER, B., FERNANDEZ, S., ARBREE, A., BALA, K., DONIKIAN, M., AND GREENBERG, D. P. 2005. Lightcuts: 
a scalable approach to illumination. In ACM SIGGRAPH 2005 Pa­pers, ACM, New York, NY, USA, SIGGRAPH 05, 
1098 1107. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 
5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343048</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>2</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[SGRT]]></title>
		<subtitle><![CDATA[a scalable mobile GPU architecture based on ray tracing]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343048</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343048</url>
		<abstract>
			<par><![CDATA[<p>Recently, with the increasing demand for photorealistic graphics and the rapid advances in desktop CPUs/GPUs, real-time raytracing has attracted considerable attention. Unfortunately, raytracing in the current mobile environment is difficult because of inadequate computing power, memory bandwidth, and flexibility in mobile GPUs. In this work, we present a novel mobile GPU architecture called the SGRT (Samsung reconfigurable GPU based on RayTracing) by enhancing our previous works with the following features: 1) a fast compact hardware engine that accelerates a traversal and intersection operation, 2) a flexible reconfigurable processor that supports software ray generation and shading, and 3) a parallelization framework that achieves scalable performance. Unlike our previous work, the current architecture is designed for both static and dynamic scenes with a smaller area. Experimental results show that the SGRT can be a versatile graphics solution, as it supports compatible performance compared to desktop GPU raytracers. To the best of our knowledge, the SGRT is the first mobile GPU based on full Whitted raytracing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737651</person_id>
				<author_profile_id><![CDATA[81440599617]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Won-Jong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SAIT, Samsung Electronics, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[joe.w.lee@samsung.com]]></email_address>
			</au>
			<au>
				<person_id>P3737652</person_id>
				<author_profile_id><![CDATA[81485641922]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shi-Hwa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SAIT, Samsung Electronics, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737653</person_id>
				<author_profile_id><![CDATA[81474665022]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jae-Ho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nah]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737654</person_id>
				<author_profile_id><![CDATA[81406599725]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jin-Woo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737655</person_id>
				<author_profile_id><![CDATA[81485647592]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Youngsam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SAIT, Samsung Electronics, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737656</person_id>
				<author_profile_id><![CDATA[81496658721]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Jaedon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SAIT, Samsung Electronics, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737657</person_id>
				<author_profile_id><![CDATA[81440603283]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Seok-Yoon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SAIT, Samsung Electronics, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2024194</ref_obj_id>
				<ref_obj_pid>2070752</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Nah, J.-H., et al. 2011. T&I Engine: Traversal and Intersection Engine for Hardware Accelerated Ray Tracing. In <i>ACM Transaction on Graphics (Proceedings of SIGGRAPH ASIA 2011)</i>, 30, 6,160:1--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lee, W.-J., et al. 2011. A Scalable GPU Architecture based on Dynamically Embedded Reconfigurable Processor. In <i>High Performance Graphics 2011, Posters</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SGRT: A Scalable Mobile GPU Architecture based on Ray Tracing Won-Jong Lee* Shi-Hwa Lee* Jae-Ho Nah 
 Jin-Woo Kim  Youngsam Shin* Jaedon Lee* Seok-Yoon Jung* *SAIT, Samsung Electronics, Korea Yonsei 
University, Korea Figure 1: (a) Our system architecture including the SGRT cores and host processor. 
(b) Rendered images by the SGRT simulator: Ferrari (left, 210K trian­gles, 1 light source) and Fairy 
(right, 170K triangles, 2 light sources). The SGRT (4-core) is predicted to render at 67.83fps (Ferrari) 
and at 87.82fps (Fairy). 1. Introduction Recently, with the increasing demand for photorealistic graphics 
and the rapid advances in desktop CPUs/GPUs, real-time raytrac­ing has attracted considerable attention. 
Unfortunately, raytracing in the current mobile environment is difficult because of inade­quate computing 
power, memory bandwidth, and flexibility in mobile GPUs. In this work, we present a novel mobile GPU 
archi­tecture called the SGRT (Samsung reconfigurable GPU based on RayTracing) by enhancing our previous 
works with the following features: 1) a fast compact hardware engine that accelerates a traversal and 
intersection operation, 2) a flexible reconfigurable processor that supports software ray generation 
and shading, and 3) a parallelization framework that achieves scalable performance. Unlike our previous 
work, the current architecture is designed for both static and dynamic scenes with a smaller area. Experimental 
results show that the SGRT can be a versatile graphics solution, as it supports compatible performance 
compared to desktop GPU raytracers. To the best of our knowledge, the SGRT is the first mobile GPU based 
on full Whitted raytracing. 2. SGRT Core Architecture Dedicated Hardware for Traversal and Intersection: 
The lack of computational power (<68GFLOPS) and memory bandwidth (<6.4GBPS) of current mobile GPUs motivated 
us to design a dedicated hardware for traversal and intersection, which are com­putation-intensive operations 
in raytracing. Our hardware, called the T&#38;I engine, is based on our previous work [Nah et al. 2011]. 
However, unlike our previous work, the new T&#38;I engine is de­signed for handling dynamic scenes with 
a bounding volume hier­archy (BVH). Moreover, the T&#38;I engine has a smaller area (3.89 mm2 per core, 
65nm), because BVH is an object hierarchy, which negates the need for LIST units to manage primitives. 
High per­formance features like the MIMD architecture for incoherent rays and a ray accumulation unit 
for latency hiding are directly reused. We can selectively utilize a specific BVH between the variants 
(e.g. Full SAH, Binned, SBVH, and LBVH) that are supported by the T&#38;I engine. The T&#38;I engine 
also has other outstanding fea­tures such as ray-AABB intersection units and a compact node layout. A 
full paper version will be announced in the near future. Reconfigurable Processor for Shading: We utilize 
a proprietary low-power DSP core developed in our previous work [Lee et al. 2011]; it is called the SRP 
(Samsung Reconfigurable Processor). The SRP is very flexible for supporting full programmability (standard 
C language); thus, various shaders (e.g. material and illumination) can be easily implemented. Unlike 
the conventional mobile GPU, the VLIW engine of the SRP can fully support con­trol-flow such as branch, 
which make recursive raytracing possi­ble. In addition, the SRP is capable of highly parallel data pro­ 
*e-mail: joe.w.lee@samsung.com cessing. The coarse-grained reconfigurable array (CGRA) of the SRP makes 
full use of the software pipeline technique to allow loop acceleration. Therefore, the ray packet stream 
processing can be done in ray generation and shading kernels, which maximizes the utilization of the 
functional units. Furthermore, the use of the SRP s reconfigurable feature might enable hybrid rendering 
that combines the OpenGL|ES rasterizer and raytracing. Parallelization Framework: For scalable performance, 
we built a parallelization framework based on the Samsung Micro Kernel (SMK), a real-time operating system 
for embedded system. The SMK supports multi-tasking by systematic scheduling in the task queues, and 
it allows developers to create and use tasks easily. We define an individual task for each SGRT core 
that is responsible for different pixels (or pixel tiles), then the scheduler can distrib­ute the next 
tasks to the idle SGRT core first, which results in dynamic load balancing. According to preliminary 
experiments, we could determine the performance scalability: 3.8x speedup on 4 SGRT cores compared to 
a single core. 3. Results Figure 1(a) shows the overall system architecture including the SGRT cores 
and host CPUs. Our architecture is based on an asyn­chronous BVH that is a combination of the rebuild 
(CPU), the refit (H/W) and the rendering (SGRT). The validity of the SGRT is verified and its performance 
is evaluated during cycle accurate simulation. The Ferrari and Fairy scene has been thoroughly test­ed 
(Figure 1(b)). Table 1 lists the performance results of raytrac­ing performed by the SGRT (4 cores), 
including shadow, reflec­tion and refraction with WVGA (800x480) resolution at 1GHz clock speed. We achieve 
around 170M RPS (T&#38;I engine), 255M RPS (SRP) and 87.82 fps (Fairy), which may be equivalent to the 
performance of recent desktop GPU ray tracers (200-300M RPS). We are now implementing the T&#38;I engine 
at the RTL level, and we will release the complete product supporting fully dynamic scenes in the future. 
Table 1. Performance results of the SGRT architecture Scene # of tri. # of ray T&#38;I Engine (usage 
&#38; cache hit ratio) SRP FPS Pipe TRV $ IST $ MRPS MRPS Fairy 170K 1.7M 87.27 93.83 96.53 171.32 255.72 
87.82 Ferrari 210K 1.5M 79.75 92.56 92.92 122.48 319.56 67.83  References NAH, J.-H., ET AL. 2011. T&#38;I 
Engine: Traversal and Intersection Engine for Hardware Accelerated Ray Tracing. In ACM Transaction on 
Graphics (Proceedings of SIGGRAPH ASIA 2011), 30, 6,160:1-10. LEE, W.-J., ET AL. 2011. A Scalable GPU 
Architecture based on Dynami­cally Embedded Reconfigurable Processor. In High Performance Graphics 2011, 
Posters. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 
 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2343049</section_id>
		<sort_key>40</sort_key>
		<section_seq_no>2</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Surf & turf]]></section_title>
		<section_page_from>2</section_page_from>
	<article_rec>
		<article_id>2343050</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>3</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[From a calm puddle to a stormy ocean]]></title>
		<subtitle><![CDATA[rendering water in <i>Uncharted</i>]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343050</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343050</url>
		<abstract>
			<par><![CDATA[<p>The Uncharted series of video games for the PS3 have been recognized by their distinct cinematic gaming experience and high quality graphics. Although the core game mechanics are centered around exploration and combat; and less on water game play, water has still been a major design element of the game. The complexity of the game mechanics and rendering of the water has increased over the three games in the series. The range of water types goes from simple puddles, pools to lakes and rivers, and, finally an ocean storm environment. The games art style is realistic, but not fully photo-realistic and the water rendering has to match the style. Our water shader uses general optical principles but adds artistic controls to realize the style of the game. One of the biggest challenges we solved was processing and rendering water with dramatic movement and striking shading while using a small portion of the available resources (memory, SPU and GPU). The water system runs in parallel with other game systems and everything is running at 30 fps. In Uncharted 3, the water system was used to drive a cruise ship, in which the whole level resided and thus became a central element of the gameplay. The water would drive the cruise-ship which in turn moves the player, enemies, effects and physics objects. In addition there were levels with swimming in rough water, flooding water, crashing waves and floating platforms that had their own technological challenges. Our engine uses different render engines depending on the type of water. All of these engines are procedural systems and, because of the game design, not one uses real-time physics simulations. One renders non-LOD meshes with calm and semi-moving water bodies (rivers, lakes, and puddles), another uses a hierarchical LOD system with displacement for open bodies of water, like the ocean, and the last uses a skinning mechanism with particles for the flooding events.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737658</person_id>
				<author_profile_id><![CDATA[81100124724]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carlos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gonzalez-Ochoa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Naughty Dog, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[cgonzoo@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P3737659</person_id>
				<author_profile_id><![CDATA[81504685783]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Doug]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Holder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Naughty Dog, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[doug@dougvfx.com]]></email_address>
			</au>
			<au>
				<person_id>P3737660</person_id>
				<author_profile_id><![CDATA[81504685007]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Eben]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cook]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Naughty Dog, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[supereben@gmail.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1015799</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Losasso, F., and Hoppe, H. 2004. Geometry clipmaps: Terrain rendering using nested regular grids.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Max, N., and Becker, B. 1996. Flow visualization using moving textures.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Yuksel, C., House, D. H., and Keyser, J. 2007. Wave particles.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 From a calm puddle to a stormy ocean -Rendering water in Uncharted Carlos Gonzalez-Ochoa * Introduction 
The Uncharted series of video games for the PS3 have been rec­ognized by their distinct cinematic gaming 
experience and high quality graphics. Although the core game mechanics are centered around exploration 
and combat; and less on water game play, water has still been a major design element of the game. The 
complex­ity of the game mechanics and rendering of the water has increased over the three games in the 
series. The range of water types goes from simple puddles, pools to lakes and rivers, and, .nally an 
ocean storm environment. The games art style is realistic, but not fully photo-realistic and the water 
rendering has to match the style. Our water shader uses general optical principles but adds artistic 
con­trols to realize the style of the game. One of the biggest challenges we solved was processing and 
rendering water with dramatic move­ment and striking shading while using a small portion of the avail­able 
resources (memory, SPU and GPU). The water system runs in parallel with other game systems and everything 
is running at 30 fps. In Uncharted 3, the water system was used to drive a cruise ship, in which the 
whole level resided and thus became a central ele­ment of the gameplay. The water would drive the cruise-ship 
which in turn moves the player, enemies, effects and physics objects. In addition there were levels with 
swimming in rough water, .ood­ing water, crashing waves and .oating platforms that had their own technological 
challenges. Our engine uses different render engines depending on the type of water. All of these engines 
are procedural systems and, because of the game design, not one uses real-time physics simulations. One 
renders non-LOD meshes with calm and semi-moving water bodies (rivers, lakes, and puddles), another uses 
a hierarchical LOD system with displacement for open bodies of water, like the ocean, and the last uses 
a skinning mechanism with particles for the .ooding events. Shader To achieve a stylized look for the 
water we use a complex shader with multiple controls of refraction, re.ection, foam and a depth based 
effect (called churn) to give the water a volumetric effect. The most important feature is .ow, which 
gives a distinct water movement. The movement is created by advecting the triangle s or pixels uv coordinates 
of the normal maps [1996]. We blend two or more normal maps, that are offset in phase, resulting in a 
continuous movement effect. A surface wide vector .eld is used to de.ne the local direction of .ow (per 
triangle or pixel), we also use other maps to determine displacement magnitude, bump strength, foam placement 
and other properties. A feature of real life rivers that we wanted to achieve are stationary or semi-stationary 
waves, which are different from the rolling, curling waves of oceans. For such, we displace vertices 
in a circular motion on a vertical plane parallel to the .ow direction. The vertices are animated in 
SPU processes before they are sent to the GPU for rendering.  Open Ocean rendering The ocean is rendered 
as a mesh using a hierarchical LOD system using an alternate scheme of Geometric Clipmaps [2004]. The 
mesh is composed as a series of concentric rings of polygons cen­tered from a point of interest close 
to the camera center; and each ring has successively coarser resolutions. Our scheme differs by how each 
ring is divided into different sized patches. This improves *cgonzoo@gmail.com, doug@dougvfx.com, supereben@gmail.com 
 Doug Holder Eben Cook Naughty Dog, Inc culling and visibility, balance SPU job loads and guarantees 
conti­nuity and blending across ring levels to avoid T-junctions. Each ver­tex of the clipmap will later 
be displaced using a procedural wave system. The displacement acts in every direction not only in the 
vertical so we can achieve sharper wave peaks. The LOD gener­ation and wave displacement is completely 
procedural and runs in several SPUs in parallel. The system is procedural, parametric and deterministic. 
It can be used to generate the renderable mesh or to drive game objects. To simulate the wave motion 
of an open ocean we generate a .eld of wave particles [2007]. This method was chosen instead of the well-known 
FFT or procedural noise methods, because it provided more intuitive controls to artists, didnt exhibit 
any tiling artifacts at low grid resolutions and was simple to optimize in the SPU. We add low frequency 
time modulated Gerstner waves to de.ne a general look and use an extruded NURBS curve pro.le to create 
speci.c wave shapes. In addition, the motion of the waves displacement drives the .ow and foam shader 
parameters. For collision queries (point and ray tracing) we dont perform tests against the rendered 
mesh, but instead use a search method on the displacement .eld, which gives faster and more accurate 
results. We compose the .nal wave displacement as a composition simpler waves and displace­ment grids. 
Our artists can add several grids at different scales to create octaves and create wave detail at different 
frequencies. These controls are .exible and intuitive for artists to use and expresive enough to create 
a calm swimming pool to a stormy ocean.  Flood For the .ooding events, we used a combination of skinned 
meshes driven by the result of an of.ine simulation and artfully placed par­ticle effects. The moving 
water was simulated of.ine to generate a high resolution mesh. From it a series of lower resolution animated 
meshes were created using a shrink-wrapping tool that was custom built in Houdini. The resulting meshes 
were guaranteed to have a de.ned vertex count that could be animated in the game engine at run-time. 
The simulation was also used by the artists as a guide for the placement and timing of particles. To 
limit overdraw particles are rendered in half-screen buffers. References LOSASSO, F., AND HOPPE, H. 
2004. Geometry clipmaps: Terrain rendering using nested regular grids. MAX, N., AND BECKER, B. 1996. 
Flow visualization using mov­ing textures. YUKSEL, C., HOUSE, D. H., AND KEYSER, J. 2007. Wave parti­cles. 
Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. 
ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343051</article_id>
		<sort_key>60</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>4</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[What if the earth was flat]]></title>
		<subtitle><![CDATA[the Globe UI system in SSX]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343051</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343051</url>
		<abstract>
			<par><![CDATA[<p>This talk shares our experiences and solutions on designing and implementing the Globe UI system for SSX. The Globe UI works like Google Earth for the game and it is one of the key features designed to help users navigate hundreds of tracks and events in the game. However the hardware resources are constantly fighting against the design. We needed a technique solution to reach the maximum requirements of the design but using as least resources as possible. This led us to think out of box, to find an alternative way to achieve the goal. Our final solution relies on 2 approaches. One is to procedurally generate various data or textures we needed from single common source. The other is to trade performance with resources whenever we can.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737661</person_id>
				<author_profile_id><![CDATA[81504685549]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Qing]]></first_name>
				<middle_name><![CDATA[Sandy]]></middle_name>
				<last_name><![CDATA[Shen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts, Burnaby, BC, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[qingshen@ea.com]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 What if the Earth was Flat - The Globe UI System in SSX  Qing Sandy Shen Electronic Arts Burnaby, BC, 
Canada qingshen@ea.com  1. Introduction This talk shares our experiences and solutions on designing 
and implementing the Globe UI system for SSX. The Globe UI works like Google Earth for the game and it 
is one of the key features designed to help users navigate hundreds of tracks and events in the game. 
However the hardware resources are constantly fighting against the design. We needed a technique solution 
to reach the maximum requirements of the design but using as least resources as possible. This led us 
to think out of box, to find an alternative way to achieve the goal. Our final solution relies on 2 approaches. 
One is to procedurally generate various data or textures we needed from single common source. The other 
is to trade performance with resources whenever we can. The earth globe you see in the game SSX is the 
final production result of our solution. This talk will reveal how the globe sphere is rendered from 
a flat plane, as well as how the perspective distortion of a sphere is achieved in this approach. It 
also shares other rendering techniques we used to refine the visuals, such as how the relaxed cone based 
relief mapping is applied on the globe rendering, how the Google-Earth-like camera transition is built, 
and how the runtime assets are managed to use less resource and still delivering smooth user experience. 
 2. Implementation 2.1 A 3D Sphere Inverse Projected From a Plane A typical way to render a globe shape 
in three dimensional space is to use polygon mesh based sphere. This method still costs a few hundreds 
kilobytes memory. Since almost all the GPU power is available to render the Globe UI, we decided to compute 
the sphere shape mathematically on GPU from a plane that is perpendicular to the view vector to the plane 
s centre. The basic idea is to render a plane with its orientation locked to the camera. Each pixel 
appears on the plane is considered as projected points from a 3D sphere. Then those pixels doesn t belong 
to the projected results of a sphere get rejected. As result, we get an axis aligned circle area in camera 
space. Then the depth is reconstructed based on sphere equation. Once we have the 3D coordinates of 
an axis-aligned sphere, we can compute its surface normals, tangents, and bi-normals. The UV coordinates 
are calculated based on the cylindrical projection on sphere. At last the orientation of its local-to-world 
transformation and camera are applied. While the 3D sphere is reconstructed from the 2D plane, perspective 
projection has to be considered in the calculation to match up with the render perspective. The solution 
used here relies on two conditions to simplify the calculation: 1. The 2D plane has to be perpendicular 
to the vector that goes from the camera to the centre of the plane, which is also the centre of the sphere. 
 2. The radius used to reject the pixels and all the valid pixels within the 2D sphere have to be remapped 
to the sphere surface using perspective projection  Figure 1. Correction of radius and pixels in inverse 
perspective projection. 2.1 Relief Mapping and Other Shading Techniques The relief mapping used on 
SSX is a version known as relaxed cone stepping (RCS) based relief mapping . This accelerates convergence 
in ray intersection search compare to the previous solutions in relief mapping. Once we know a ray is 
inside the surface, we can safely apply a binary search to refine the position of the intersection. The 
combination of RCS and binary search produces renderings of significantly higher quality. Figure 2. 
Rocky Mountains rendered by relief mapping at different view angels. Figure 3 shows the mountain range 
render results in the Globe UI. The major techniques used here include atmospheric scattering, HDR, tone 
mapping, and particles. The details of those techniques will be briefed in the talk. Figure 3. Mountain 
Range View in Globe UI 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343052</article_id>
		<sort_key>70</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>5</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Adaptive level-of-detail system for "End of Nations"]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343052</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343052</url>
		<abstract>
			<par><![CDATA[<p>One of many challenges in making a massive online RTS such as "End of Nations" is how to maintain a smooth framerate at all times without completely compromising on the visual quality of the game. Because "End of Nations" supports over 50 players in a battle together, there is a wide range of load on the graphics engine. Game developers have been employing many well-established Level-of-Detail (LoD) techniques when trying to solve the performance problem. We will show how we incorporated many of these existing techniques into a single over-arching LoD system that can monitor the game framerate in real-time and adapt to it to ensure a playable experience with the best possible visual quality.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737662</person_id>
				<author_profile_id><![CDATA[81504687803]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hjelstrom]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Petroglyph Games, Las Vegas, NV]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[greg_hjelstrom@petroglyphgames.com]]></email_address>
			</au>
			<au>
				<person_id>P3737663</person_id>
				<author_profile_id><![CDATA[81504682837]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Thanh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nguyen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Petroglyph Games, Las Vegas, NV]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[thanh_nguyen@petrolyphgames.com]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Adaptive Level-of-Detail System for End of Nations Greg Hjelstrom Petroglyph Games Las Vegas, NV, USA 
greg_hjelstrom@petroglyphgames.com 1. Introduction One of many challenges in making a massive online 
RTS such as End of Nations is how to maintain a smooth framerate at all times without completely compromising 
on the visual quality of the game. Because End of Nations supports over 50 players in a battle together, 
there is a wide range of load on the graphics engine. Game developers have been employing many well­established 
Level-of-Detail (LoD) techniques when trying to solve the performance problem. We will show how we incorporated 
many of these existing techniques into a single over-arching LoD system that can monitor the game framerate 
in real-time and adapt to it to ensure a playable experience with the best possible visual quality. 
2. Exposition The number of units on screen is often the most important factor to affect performance 
of a real-time strategy game. Techniques such as restricting the camera zoom, geometry LoD, etc can generally 
help keep the graphics engine s work load constrained as much as possible. While we do make extensive 
use of a screen-size -based LoD scheme, for End of Nations more is needed to have the game run fast. 
This is primarily because players are free to amass units at any location within the game world. On top 
of that, our camera is designed to zoom out quite far to give an epic feel to battles. What we need is 
a way to constantly monitor the framerate and dynamically adapt the work load over time. 2.1 Elaboration 
At run-time, many systems can modify the graphics engine s feature set, some of them even overlapping 
each other. For example, the user can, at any time, change the video settings via the UI, the camera-zoom 
system can decide to fade out a certain class of props or effects (e.g. footsteps are not needed/desired 
when the camera zoomed out) or the game-level projectile system can decide to reduce the spawn rate to 
75%. We needed a middle Thanh Nguyen Petroglyph Games Las Vegas, NV, USA thanh_nguyen@petrolyphgames.com 
 man to regulate the conversation across systems. The Dynamic LoD Interface does exactly that. It acts 
as a voting system in which different systems in the game submit their vote on how they think the graphics 
features set should be in a given frame. The Dynamic LoD Interface will then evaluate the votes, decide 
the best actions to take (generally those that increase the performance the most and degrade the visual 
quality the least) and execute them. While the Dynamic LoD Interface counts votes and acts on them, the 
Tactical LoD System watches the game performance closely and casts much of the votes in an effort to 
maintain a playable framerate at all time. This system uses a simple ring­buffer to track framerate within 
the last N frames, from which it can average out and guess how fast the game is running. It then needs 
to decide on what action to take.  3. Results The biggest danger with this type of adaptive LoD scheme 
is unstable visuals. The system could constantly degrade and upgrade features, creating a flickering 
type of effect as the framerate fluctuates. To prevent this problem, some hysteresis is needed. The system 
is not allowed to rapidly change directions . For example, it can rapidly execute several downgrade actions 
but some minimum amount of time is required before it can then execute an upgrade action. Extensive and 
on-going testing is critical. Once the system was thoroughly tested and fairly well­tuned, we observed 
that the engine was able to hold itself well under heavy battles without giving away much of its internal 
trickery. When you have a large number of units moving, shooting and exploding, it is quite hard to notice 
that bloom and heat distortion has been turned off or that 3 out of 10 times this one tank shot, the 
muzzle flash didn t show up. And because the game feels smooth and playable all the time, the overall 
experience is a better one. 4. Conclusions We have presented what motivated us to create an adaptive 
LoD system for End of Nations . We have shown the design as well as the algorithm to implement such a 
system. With careful tuning. the system can adjust the graphics engine s work load to produce significant 
performance boost without jarring flickering in the visual and create a better play experience. Copyright 
is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343053</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>6</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Screen space decals in Warhammer 40,000: Space Marine]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343053</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343053</url>
		<abstract>
			<par><![CDATA[<p>Traditionally, a decal is drawn as a textured mesh patch duplicated from the underlying geometry. This works most of the time, but often suffers from various problems, such as Z-fighting between the underlying mesh and the duplicated decal mesh and stretched decal textures when the underlying mesh has sparse vertices.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737664</person_id>
				<author_profile_id><![CDATA[81504683452]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pope]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Relic Entertainment/THQ Canada, Vancouver, BC, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[blindrenderer@gmail.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Persson, E. 2011. Volume Decals. In <i>GPU Pro 2</i>, A K Peters, 115--120]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Engel, W. 2009. Designing a Renderer for Multiple Lights -- The Light Pre-Pass Renderer. In <i>ShaderX7: Advanced Rendering Techniques</i>, Chares River Media.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Screen Space Decals in Warhammer 40,000: Space Marine Pope Kim Relic Entertainment/THQ Canada Vancouver, 
BC, Canada  1 Introduction Traditionally, a decal is drawn as a textured mesh patch duplicated from 
the underlying geometry. This works most of the time, but often suffers from various problems, such as 
Z­fighting between the underlying mesh and the duplicated decal mesh and stretched decal textures when 
the underlying mesh has sparse vertices. The most notable previous work in this area is Volume Decals 
by Emil Perrson. This technique projects a convex volume mesh onto the underlying geometry, stored in 
a depth buffer, and applies a volume texture on the pixels rasterized onto the screen. This approach 
solves the problems described above, but we couldn t use it because we already had implemented the full 
2D screen­space decal technique and had been using it in­game for about 2 years by the time his paper 
was published, at which point we looked into it but found out that our approach was more suitable for 
current­gen console games, such as Space Marine due to the memory footprint of volumetric textures. Furthermore, 
we also found that artists are very reluctant to author volume textures mostly due to the lack of acceptable 
volume texture support from mainstream art packages. 2 Screen Space Decals Our approach uses a simple 
3D box (projection box) mesh and 2D textures to achieve the same visual goal with a lower memory footprint 
and better artist workflow. Also we observed other unexpected benefits which will be described later. 
Once the geometry buffer pass is rendered, we project a 3D box mesh onto the depth buffer to find the 
local position of each pixel in the local space of projection box. Once the local positions are calculated, 
we run two rejection passes: out­of­box rejection and orientation­based rejection. Orientation­based 
rejection is intended to solve the inherent side­stretching problem of 2D texture projection. We sample 
the normal from the underlying geometry and perform a simple dot product operation to reject any pixels 
which are facing away more than a certain angle, which is defined by artists on each decal. This allows 
different decals to have different stretching tolerance. With our deferred lighting renderer, we also 
managed to support three different types of decals, such as decals modifying underlying geometry normals 
only, decals modifying underlying albedos only and decals modifying both normals and albedos. This greatly 
improved the re­usability of existing decals. 3 Results Our new decal system removed the visual glitches 
of traditional decals and the complexities involved in authoring volume decals. Also we successfully 
implemented a few switches artists can easily turn on and off. This allowed our artists to go through 
more iterations very quickly and reuse existing decals with tweak­able decal types and cut­off angles. 
These techniques helped us achieve an excellent visual quality for Warhammer 40,000: Space Marine. 4 
Other Benefits Other than better visual quality and simpler artist workflow, we also observed two other 
benefits that we did not intend to achieve originally. The biggest unexpected benefit was that our collision 
detection pass became much faster since there was no need for high­resolution collision meshes to support 
the duplication of fine mesh patches for traditional decals. At the end, most of the collision meshes 
became very basic shapes. With the deferred lighting engine, supporting transparent objects on current­gen 
console was not trivial. At the end, we used Screen Space Decals to support 90% of our transparent objects 
which are not particle effects. This was possible only because most of our transparent objects were essentially 
co­planar. 5 Conclusions We successfully implemented Screen Space Decals with a 3D box mesh and 2D textures 
to solve the problems with existing decal techniques and to simplify artist workflow, both of which contributed 
to the enhanced visual quality of Warhammer 40,000: Space Marine. References PERSSON, E. 2011. Volume 
Decals. In GPU Pro 2, A K Peters, 115 ­120 ENGEL, W. 2009. Designing a Renderer for Multiple Lights 
The Light Pre­Pass Renderer. In ShaderX7: Advanced Rendering Techniques, Chares River Media. Copyright 
is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2343054</section_id>
		<sort_key>90</sort_key>
		<section_seq_no>3</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Material: the gathering]]></section_title>
		<section_page_from>3</section_page_from>
	<article_rec>
		<article_id>2343055</article_id>
		<sort_key>100</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>7</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Estimating specular normals from spherical Stokes reflectance fields]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343055</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343055</url>
		<abstract>
			<par><![CDATA[<p>Despite being at the focal point of intense research in both computer graphics as well as in computer vision, accurately reproducing the shape and appearance of real-world scenes remains a challenging problem, especially under uncontrolled conditions. One cue that has been used to separate diffuse and specular reflectance is polarization. Recent work in computer graphics has explored polarization of incident illumination in conjunction with spherical gradient illumination to infer high quality diffuse-specular separation of both albedo as well as photometric normal information [Ma et al. 2007]. Ghosh et al. [2010] improved upon this by removing the view-dependence of the polarization scheme of Ma et al. by analyzing the Stokes reflectance field under incident circularly polarized spherical gradient illumination, and recover more detailed specular reflectance information including index of refraction as well as specular roughness.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737665</person_id>
				<author_profile_id><![CDATA[81384613270]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Giuseppe]]></first_name>
				<middle_name><![CDATA[Claudio]]></middle_name>
				<last_name><![CDATA[Guarnera]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737666</person_id>
				<author_profile_id><![CDATA[81100016992]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pieter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The College of William & Mary]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737667</person_id>
				<author_profile_id><![CDATA[81504685651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737668</person_id>
				<author_profile_id><![CDATA[81385599077]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Abhijeet]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ghosh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1866163</ref_obj_id>
				<ref_obj_pid>1882261</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ghosh, A., Chen, T., Peers, P., Wilson, C. A., and Debevec, P. 2010. Circularly polarized spherical illumination reflectometry. <i>ACM Trans. Graph. 29</i> (December), 162:1--162:12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383873</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ma, W.-C., Hawkins, T., Peers, P., Chabert, C.-F., Weiss, M., and Debevec, P. 2007. Rapid acquisition of specular and diffuse normal maps from polarized spherical gradient illumination. In <i>Rendering Techniques</i>, 183--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Estimating Specular Normals from Spherical Stokes Re.ectance Fields Giuseppe Claudio Guarnera Pieter 
Peers* Paul Debevec Abhijeet Ghosh USC Institute for Creative Technologies The College of William &#38; 
Mary*  (a) Subject (b) Specular normals (c) Circular Stokes (C.S.) (d) Normals from C.S. (e) Unpol. 
Stokes (U.S.) (f) Normals from U.S. Figure 1: Estimating specular normals from Stokes parameters of incident 
spherical illumination. Specular normals inferred from circularly polarized illumination (c-d), and unpolarized 
illumination (e-f), compared to measurement with polarized spherical gradients (b). Top-row: Plastic 
orange. Bottom-row: Marble statue. Introduction. Despite being at the focal point of intense research 
in both computer graphics as well as in computer vision, accurately reproducing the shape and appearance 
of real-world scenes remains a challenging problem, especially under uncontrolled conditions. One cue 
that has been used to separate diffuse and specular re­.ectance is polarization. Recent work in computer 
graphics has explored polarization of incident illumination in conjunction with spherical gradient illumination 
to infer high quality diffuse-specular separation of both albedo as well as photometric normal informa­tion 
[Ma et al. 2007]. Ghosh et al. [2010] improved upon this by removing the view-dependence of the polarization 
scheme of Ma et al. by analyzing the Stokes re.ectance .eld under incident cir­cularly polarized spherical 
gradient illumination, and recover more detailed specular re.ectance information including index of refrac­tion 
as well as specular roughness. Contribution. In this work we analyze the view-independent sym­metric 
Stokes re.ectance .eld under constant incident spherical illumination that is either circularly polarized 
(Fig. 1,c-d), or un­ polarized (Fig. 1, e-f). We demonstrate that both types of incident lighting can 
be used to reliably estimate specular normals, and show how this theory can be applied to normal estimation 
under uncon­trolled outdoor illumination. Normal Computation. We capture four photographs of the target 
object under uniform spherical illumination (either circu­larly polarized or unpolarized) with different 
polarizers in front of the camera as in Ghosh et al. [2010] (i.e., three linear polar­ izers (P0, P45, 
P90) and a (left) circular polarizer (P.)), and com­pute the four Stokes parameters of re.ected light 
(s0, s1, s2, s3) per pixel. Without loss of generality, we assume that the camera is looking down the 
-Z axis. It can be shown that in this case, h s¯3 = s3/ s21 + s22 + s2 relates to . = arccos(n · Z), 
where n is 3 the specular normal. Furthermore, the normalized linear compo­ h nents (s;1, s;2), with 
si;= si/ s21 + s22, i .{1, 2}, relates to the angle f = arccos(n · X). However, the mapping from (s;1, 
s2;) to f suffers from a rotational ambiguity: f and f + p map to the same (s;1, s;2). We propose two 
solutions to solve for this ambiguity: 1. An additional measurement under a gradient illumination (a) 
Plastic orange (b) Unpol. Stokes (U.S.) (c) Normals from U.S. Figure 2: Estimated specular normals from 
Stokes parameters of diffuse outdoor illumination. orthogonal to the camera viewing axis (i.e., X or 
Y gradi­ent) breaks the ambiguity. However, this reduces the view­independence of the normal computations. 
 2. For convex objects, we can grow the normals in from the silhouette, assuming that the normals at 
the silhouette are or­thogonal to silhouette edge and the view direction. Outdoor Illumination. We note 
that instead of using constant illumination, we can also observe the subject under a Y gradient. In this 
case, the incident lighting already provides the additional cue to break the rotational ambiguity for 
computation of f. Interestingly, outdoor illumination on a cloudy day, is approximately similar to unpolarized 
Y-gradient illumination. We employ this observation to compute the normals under such outdoor lighting 
condition in Figure 2. References GHOSH, A., CHEN, T., PEERS, P., WILSON, C. A., AND DE-BEVEC, P. 2010. 
Circularly polarized spherical illumination re­.ectometry. ACM Trans. Graph. 29 (December), 162:1 162:12. 
MA, W.-C., HAWKINS, T., PEERS, P., CHABERT, C.-F., WEISS, M., AND DEBEVEC, P. 2007. Rapid acquisition 
of specular and diffuse normal maps from polarized spherical gradient illumina­tion. In Rendering Techniques, 
183 194. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 
 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343056</article_id>
		<sort_key>110</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>8</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Estimating diffusion parameters from polarized spherical gradient illumination]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343056</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343056</url>
		<abstract>
			<par><![CDATA[<p>Accurately modeling and reproducing the appearance of real-world materials is crucial for the production of photoreal imagery of digital scenes and subjects. The appearance of many common materials is the result of subsurface light transport that gives rise to the characteristic "soft" appearance and the unique coloring of such materials. Jensen et al. [2001] introduced the dipole-diffusion approximation to efficiently model isotropic subsurface light transport. The scattering parameters needed to drive the dipole-diffusion approximation are typically estimated by illuminating a homogeneous surface patch with a collimated beam of light, or in the case of spatially varying translucent materials with a dense set of structured light patterns. A disadvantage of most existing techniques is that acquisition time is traded off with spatial density of the scattering parameters.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737669</person_id>
				<author_profile_id><![CDATA[81504682479]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yufeng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737670</person_id>
				<author_profile_id><![CDATA[81100016992]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pieter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The College of William & Mary]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737671</person_id>
				<author_profile_id><![CDATA[81504685651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737672</person_id>
				<author_profile_id><![CDATA[81385599077]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Abhijeet]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ghosh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ishimaru, A. 1978. <i>Wave Propagation and Scattering in Random Media</i>. Academic Press, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383319</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W., Marschner, S. R., Levoy, M., and Hanrahan, P. 2001. A practical model for subsurface light transport. In <i>Proceedings of ACM SIGGRAPH 2001</i>, 511--518.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383873</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ma, W.-C., Hawkins, T., Peers, P., Chabert, C.-F., Weiss, M., and Debevec, P. 2007. Rapid acquisition of specular and diffuse normal maps from polarized spherical gradient illumination. In <i>Rendering Techniques</i>, 183--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Estimating Diffusion Parameters from Polarized Spherical Gradient Illumination Yufeng Zhu Pieter Peers* 
Paul Debevec Abhijeet Ghosh USC Institute for Creative Technologies The College of William &#38; Mary* 
 (a) Diff. albedo (b) Red normals (c) Green normals (d) Blue normals (e) Translucency (f) Rendering 
Figure 1: Estimating spatially varying diffusion parameters from polarized spherical gradient illumination. 
RGB translucency parameters (e) inferred from diffuse albedo (a) and RGB diffuse normals (b-d). Introduction. 
Accurately modeling and reproducing the appear­ance of real-world materials is crucial for the production 
of pho­toreal imagery of digital scenes and subjects. The appearance of many common materials is the 
result of subsurface light transport that gives rise to the characteristic soft appearance and the unique 
coloring of such materials. Jensen et al. [2001] introduced the dipole-diffusion approximation to ef.ciently 
model isotropic sub­surface light transport. The scattering parameters needed to drive the dipole-diffusion 
approximation are typically estimated by illu­minating a homogeneous surface patch with a collimated 
beam of light, or in the case of spatially varying translucent materials with a dense set of structured 
light patterns. A disadvantage of most existing techniques is that acquisition time is traded off with 
spatial density of the scattering parameters. Polarized Spherical Gradients. Recently, Ma et al. [2007] 
pro­posed a technique to obtain high quality estimates of diffuse and specular albedo and photometric 
normal maps from just eight pho­tograph under four different polarized spherical gradient lighting conditions. 
In addition, Ma et al. also proposed a hybrid normal rendering technique that approximates the soft appearance 
of sub­surface scattering with local shading using measured RGB diffuse normals. This suggest a connection 
between spherical gradient il­lumination and subsurface scattering. In this work, we aim to formalize 
this apparent connection between subsurface scattering parameters and observations under spherical gradient 
illumination of translucent materials based on radiative transfer theory [Ishimaru 1978]. In particular, 
we show that dense per-surface-point scattering parameters can be directly obtained from observations 
under spherical gradient illumination (cross­polarized to discard specular re.ections), without resorting 
to any explicit .tting of observed scattering pro.les. Background. Light transport in highly scattering 
translucent mate­rials can be well approximated by diffusion theory [Ishimaru 1978; Jensen et al. 2001]. 
According to radiative transfer theory, diffu­sion can be accurately approximated by a two-term spherical 
har­ monic expansion of radiance: L(x, .) = 1 4p f(x) + 3 4p . · E (x), (1) where f(c) is the scalar 
.uence and E (x) is the vector irradi­ance. Substituiting Equation 1 in the radiative transfer equation 
and assuming semi-in.nite material, leads to the well-known dif­fuse BSSRF [Jensen et al. 2001]: (En 
· .f(xo)) Rd (r)= -D , (2) dFi(xi) where r = ||xo - xi||, and D = 1/3s:is the diffusion constant. t 
(a) Diff. albedo (b) Translucency (c) Rendering Figure 2: Spatially varying diffusion parameters of 
material samples estimated us­ing spherical gradient illumination. Top-row: Red wax. Bottom-row: Polished 
marble. Diffusion from Gradients. Relating Equations (1) and (2) yields a mechanism for estimating scattering 
parameters of dipole diffusion from observations of the 1st -order spherical gradients. Speci.cally, 
a BRDF approximation of Equation 2 relates the observed diffuse albedo Rd (cross-polarized 0th order 
spherical statistics) to the nor­mal aligned component of the estimated diffuse normal En (cross­polarized 
1st order spherical statistics [Ma et al. 2007]) via the diffusion constant D. Assuming that the surface 
normal is along the +Z direction, this leads to the following compact relation between the diffusion 
constant and polarized spherical gradients: Rd D . . (3) |Enz| Conclusion. Equation 3 provides a mechanism 
for directly ob­taining dense spatially-varying diffusion parameters from just four observations of translucent 
materials under polarized spherical gra­dient illumination. References ISHIMARU, A. 1978. Wave Propagation 
and Scattering in Random Media. Academic Press, New York. JENSEN, H. W., MARSCHNER, S. R., LEVOY, M., 
AND HANRA-HAN, P. 2001. A practical model for subsurface light transport. In Proceedings of ACM SIGGRAPH 
2001, 511 518. MA, W.-C., HAWKINS, T., PEERS, P., CHABERT, C.-F., WEISS, M., AND DEBEVEC, P. 2007. Rapid 
acquisition of specular and diffuse normal maps from polarized spherical gradient illumina­tion. In Rendering 
Techniques, 183 194. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, 
August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343057</article_id>
		<sort_key>120</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>9</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Measurement-based synthesis of facial microgeometry]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343057</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343057</url>
		<abstract>
			<par><![CDATA[<p>Current scanning techniques record facial <i>mesostructure</i> with sub-millimeter precision showing pores, wrinkles, and creases. However, surface roughness continues to shape specular reflection at the level of <i>microstructure</i>: micron scale structures. Here, we present an approach to increase the resolution of mesostructure-level facial scans using microstructure examples digitized about the face. We digitize the skin patches using polarized gradient illumination and 10 &mu;m resolution macro photography, and observe point-source reflectance measurements to characterize the specular reflectance lobe at this smaller scale. We then perform constrained texture synthesis to create appropriate surface microstructure per facial region, blending the regions to cover the whole entire face. We show that renderings of microstructure-augmented facial models preserve the original scanned mesostructure and exhibit surface reflections which are qualitatively more consistent with real photographs.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737673</person_id>
				<author_profile_id><![CDATA[81504686804]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Graham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737674</person_id>
				<author_profile_id><![CDATA[81442617187]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Borom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tunwattanapong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737675</person_id>
				<author_profile_id><![CDATA[81440600975]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jay]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Busch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737676</person_id>
				<author_profile_id><![CDATA[81440613130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Xueming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737677</person_id>
				<author_profile_id><![CDATA[81100556708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jones]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737678</person_id>
				<author_profile_id><![CDATA[81504685651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737679</person_id>
				<author_profile_id><![CDATA[81385599077]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Abhijeet]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ghosh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Graham, P., Tunwattanapong, B., Busch, J., Yu, X., Jones, A., Debevec, P., and Ghosh, A. 2012. Measurement-based synthesis of facial microgeometry. Tech. Rep. ICT-TR-01-2012, USC-ICT.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383295</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hertzmann, A., Jacobs, C. E., Oliver, N., Curless, B., and Salesin, D. H. 2001. Image analogies. In <i>Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i>, ACM, New York, NY, USA, SIGGRAPH '01, 327--340.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Measurement-Based Synthesis of Facial Microgeometry Paul Graham Borom Tunwattanapong Jay Busch Xueming 
Yu Andrew Jones Paul Debevec Abhijeet Ghosh USC Institute for Creative Technologies  (a) Rendering from 
original facial scan (b) Rendering using synthesized microstructure (c) Comparison photograph Figure 
1: (a) Scanned mesostructure with 4K displacement map. (b) Synthesized microstructure with 16K displacement 
map. (c) Real photograph under .ash illumination. 1 Introduction Current scanning techniques record facial 
mesostructure with sub­millimeter precision showing pores, wrinkles, and creases. How­ever, surface roughness 
continues to shape specular re.ection at the level of microstructure: micron scale structures. Here, 
we present an approach to increase the resolution of mesostructure-level fa­cial scans using microstructure 
examples digitized about the face. We digitize the skin patches using polarized gradient illumination 
and 10 µm resolution macro photography, and observe point-source re.ectance measurements to characterize 
the specular re.ectance lobe at this smaller scale. We then perform constrained texture synthesis to 
create appropriate surface microstructure per facial re­gion, blending the regions to cover the whole 
entire face. We show that renderings of microstructure-augmented facial models preserve the original 
scanned mesostructure and exhibit surface re.ections which are qualitatively more consistent with real 
photographs. 2 Recording Skin Microstructure (a) (b) Figure 2: Microgeometry acquisition setups (a) 
Twelve-light hemisphere capturing a patch on the cheek. (b) LED Sphere with camera inside, capturing 
the nose tip. We record the microstructure of skin patches under polarized gradi­ent illumination using 
either of two systems. For both, we stabilize the skin patch relative to the camera by placing the subject 
s skin against a 24mm × 16mm aperture in a thin metal plate. Our small capture system (Fig. 2(a)) is 
a 12-light dome, where each light can produce both linear polarization conditions. The difference be­tween 
images acquired under parallel-and cross polarization isolate surface re.ectance and attenuate the blur 
of subsurface scattering. For BRDF .tting, we additionally acquire a single-light polariza­tion difference 
image. For smooth or oily skin patches, twelve light positions can yield separated specular highlights, 
biasing surface normal measurement. For higher angular resolution, we can alter­nately acquire microstructure 
with the macro camera and aperture frame inside the same 2.5m-diameter polarized LED sphere used for 
facial scanning (Fig. 2(b)). 3 Facial Microstructure Synthesis AA B B Figure 3: We add microstructural 
detail to a scanned facial region B using the analogous relationship between an exemplar microstructure 
patch A1 and a blurred version of it A which matches the mesostructural detail of B. We segment the face 
into seperate regions (forehead, nose, temple, cheek, and chin) and use the surface mesostructure evident 
in the full facial scan to guide the texture synthesis process for each facial region. We then merge 
the synthesized facial regions into a full 16K map of the microstructure. To do this, we derive displacement 
maps of both surface meso-(A, B) and microstructure (A ) from the measured specular normal maps (Fig. 
1, inset) and then syn­ thesize displacement maps with microstructure (B ) for the entire face using 
constrained texture synthesis based on Image Analogies [Hertzmann et al. 2001]. Finally, we render with 
BRDFs measured during microgeometry acquisition; greater detail is provided in the supplemental technical 
report [Graham et al. 2012]. References GRAHAM, P., TUNWATTANAPONG, B., BUSCH, J., YU, X., JONES, A., 
DEBEVEC, P., AND GHOSH, A. 2012. Measurement-based synthesis of facial microgeometry. Tech. Rep. ICT-TR-01-2012, 
USC-ICT. HERTZMANN, A., JACOBS, C. E., OLIVER, N., CURLESS, B., AND SALESIN, D. H. 2001. Image analogies. 
In Proceedings of the 28th annual conference on Computer graphics and inter­active techniques, ACM, New 
York, NY, USA, SIGGRAPH 01, 327 340. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, 
California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343058</article_id>
		<sort_key>130</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>10</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[A single-shot light probe]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343058</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343058</url>
		<abstract>
			<par><![CDATA[<p>We demonstrate a novel light probe which can estimate the full dynamic range of a scene with multiple bright light sources. It places diffuse strips between mirrored spherical quadrants, effectively co-locating diffuse and mirrored probes to record the full dynamic range of illumination in a single exposure. From this image, we estimate the intensity of multiple saturated light sources by solving a linear system.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737680</person_id>
				<author_profile_id><![CDATA[81504685651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737681</person_id>
				<author_profile_id><![CDATA[81504686804]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Graham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737682</person_id>
				<author_profile_id><![CDATA[81440600975]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jay]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Busch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737683</person_id>
				<author_profile_id><![CDATA[81100467115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bolas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>280864</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. 1998. Rendering synthetic objects into real scenes. In <i>Proc. of ACM SIGGRAPH 98</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Reinhard, E., Heidrich, W., Pattanaik, S., and Debevec, P. 2005. Image-based lighting. In <i>High Dynamic Range Imaging</i>. Morgan Kauffman, ch. 9, 396--401.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Single-Shot Light Probe Paul Debevec Paul Graham Jay Busch Mark Bolas University of Southern California 
Institute for Creative Technologies  Figure 1: L-R: (a) Single-shot light probe (b) Reconstituted LDR 
mirrored ball image (c) Convolved clipped probe with overlaid irradiance samples (d) Computed sunlight 
(e) Virtual diffuse sphere lit by recovered HDR light (f) For validation, real diffuse sphere in recorded 
light. Abstract We demonstrate a novel light probe which can estimate the full dynamic range of a scene 
with multiple bright light sources. It places diffuse strips between mirrored spherical quadrants, ef­fectively 
co-locating diffuse and mirrored probes to record the full dynamic range of illumination in a single 
exposure. From this im­age, we estimate the intensity of multiple saturated light sources by solving 
a linear system. Introduction Recording on-set illumination to render virtual ob­jects into a real scene 
is often accomplished by acquiring a panoramic, high-dynamic range (HDR) image where the object is to 
be inserted, and to use this HDRI map as an image-based lighting source on the CG object [Debevec 1998]. 
The spherical panorama can be obtained by stitching HDR .sheye images from different directions, or more 
rapidly by photographing a mirrored sphere. Shooting HDR is necessary to capture light sources they 
typi­cally exceed ambient light by several orders of magnitude but it requires recording, aligning, 
and assembling a range of exposures which can be time-consuming and complicates dynamic capture. [Reinhard 
et al. 2005] showed that if there is just one bright light in the scene, its intensity can be determined 
from an image of a diffuse gray ball, with the remaining illumination imaged accurately in the mirrored 
sphere. Even so, two images are required, and only one saturated light source is estimated. We relieve 
both restrictions. Probe Construction We machined four 73mm-diameter hollow chrome spheres (originally, 
Baoding exercise balls) into segments slightly larger than fourths by making two cuts at right angles, 
each 53% across the diameter. We mounted them to a cross of intersect­ing 11.5mm-thick plexiglass sheets, 
laser cut to support the cross­sectional shape of the quadrants (see Fig. 1(a)). Before mounting, we 
spray painted the plexiglass with 32% re.ective .at gray primer. For color balance, we af.xed four color 
squares the red, green, blue, and 36% gray squares from a ColorChecker chart leaving a .at patch of 
the gray paint between them. Finally, we painted black lines indicating 30. and 60. rotation into grooves 
in the gray strips. Using the Probe We photographed the probe with a Canon 1D Mark III camera in RAW 
mode from 1.5m away using a 200mm lens, yielding a near-orthographic image, with the grey strips ori­ented 
diagonally to better cover the upward directions. We in­tentionally underexpose the ambient light by 
1 1 stops, allowing 2 saturated light sources to appear small enough to note their direc­tion with some 
precision. We composite the four quadrants into a complete mirrored sphere image, blending across the 
overlap of the larger-than-90. quadrants. Gathering Data From the clipped probe P, we determine the unit 
direction vector .i at the center of each saturated light source. We then measure the irradiance B from 
each of the gray strips at several direction vectors µj, including 0. , ±45., and optionally ±75. away 
from the frontal direction along each strip. We compute the diffuse Lambertian convolution D of P at 
the these same directions µj . Solving for Clipped Lights If no light sources clip, then D should equal 
B for all samples µj . We solve for which light source intensities ai, when added to the clipped light 
from the en­vironment, explain the irradiance samples B(µj ). The irradiance Li(µj) from a unit-intensity 
light i in direction .i onto the probe at direction µj is the positive-clamped cosine between these angles 
Li(µj)= max (.i · µj , 0). Thus: . j, D(µj)+ . aiLi(µj)= B(µj ) i This yields a set of m equations equal 
to the number of irradiance samples B(µj ) with n unknown light intensities ai. If m > n, we can solve 
for ai with linear least squares. If not, a regularization term which encourages nearby light sources 
to have similar intensities is required. (a) (b) (c) (d) Figure 2: L-R: (a) Single-shot probe indoors. 
(b,c) Solved illu­mination from left light and right sources (+2 stops). (d) Virtual version of probe 
lit by recovered light. Results Fig. 1 uses the single-shot probe to record sunny out­ door illumination 
conditions, with the sun as the one saturated light source. The recovered illumination lights both a 
diffuse sphere and a chrome sphere similarly to how they actually appeared in the en­vironment. Fig. 
2 shows studio-like lighting and solves for two saturated light sources of somewhat different colors 
and intensities. The recovered lighting produces a close match on a virtual version of the single-shot 
probe. Validation comparisons to full-HDR IBL are provided in the supplemental material References DEBEVEC, 
P. 1998. Rendering synthetic objects into real scenes. In Proc. of ACM SIGGRAPH 98. REINHARD, E., HEIDRICH, 
W., PATTANAIK, S., AND DEBEVEC, P. 2005. Image-based lighting. In High Dynamic Range Imag­ing. Morgan 
Kauffman, ch. 9, 396 401. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, 
August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343059</article_id>
		<sort_key>140</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>11</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Improved Linear Light Source material reflectance scanning]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343059</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343059</url>
		<abstract>
			<par><![CDATA[<p>We improve the resolution, accuracy, and efficiency of Linear Light Source (LLS) Reflectometry with several acquisition setup and data processing improvements, allowing spatially-varying reflectance parameters of complex materials to be recorded with unprecedented accuracy and efficiency.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737684</person_id>
				<author_profile_id><![CDATA[81100332743]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Meseth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RTT AG]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737685</person_id>
				<author_profile_id><![CDATA[81504684464]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shawn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hempel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RTT AG]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737686</person_id>
				<author_profile_id><![CDATA[81320496420]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andrea]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weidlich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RTT AG]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737687</person_id>
				<author_profile_id><![CDATA[81504688224]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Lynn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fyffe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RTT AG]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737688</person_id>
				<author_profile_id><![CDATA[81421597317]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Graham]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fyffe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RTT AG]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737689</person_id>
				<author_profile_id><![CDATA[81504683342]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RTT AG]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737690</person_id>
				<author_profile_id><![CDATA[81504683540]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Carroll]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RTT AG]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737691</person_id>
				<author_profile_id><![CDATA[81504685651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RTT AG and USC ICT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>882342</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Gardner, A., Tchou, C., Hawkins, T., and Debevec, P. 2003. Linear light source reflectometry. In <i>ACM SIGGRAPH 2003 Papers</i>, ACM, New York, NY, USA, SIGGRAPH '03, 749--758.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964940</ref_obj_id>
				<ref_obj_pid>1964921</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ren, P., Wang, J., Snyder, J., Tong, X., and Guo, B. 2011. Pocket reflectometry. In <i>ACM SIGGRAPH 2011 papers</i>, ACM, New York, NY, USA, SIGGRAPH '11, 45:1--45:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Improved Linear Light Source Material Re.ectance Scanning Jan Meseth1 Shawn Hempel1 Andrea Weidlich1 
Lynn Fyffe1 Graham Fyffe1 Craig Miller1 Paul Carroll1 Paul Debevec1,2 1RTT AG 2USC ICT Figure 1: Left 
to Right: (a) Scanner (b) Materials being scanned (c) Anisotropic specular parameters (d) Rendering with 
measured materials. Abstract We improve the resolution, accuracy, and ef.ciency of Linear Light Source 
(LLS) Re.ectometry with several acquisi­tion setup and data processing improvements, allowing spatially­varying 
re.ectance parameters of complex materials to be recorded with unprecedented accuracy and ef.ciency. 
Introduction Measuring the re.ectance of spatially-varying ma­terials is a topic of great interest in 
the graphics industry; however, no commercially available instrument measures spatially varying re.ectance 
parameters for a wide range of materials in a way which is high resolution, ef.cient, and accurate. The 
basic problem is that moving a light to thousands of incident angles is extremely time consuming, and 
still typically fails to record highly specular mate­rials accurately. This problem can be alleviated 
using linear light sources [Gardner et al. 2003]: instead of moving a point light source to thousands 
of positions, a linear light source (LLS) is passed across the sample at a few different orientations 
and per-pixel re.ectance is inferred from the resulting re.ectance traces using a re.ectance model. Our 
new measurement setup (Fig. 1a) yields high-quality results for anything ranging from Lambertian to sharp 
specular materials. Unlike [Ren et al. 2011], which aims at quick measurements using a mobile device, 
the quality of measurements does not depend on the existence of a reference material database or the 
skill of the operator. In addition, our device also handles anisotropic materials successfully. Scanner 
Design A signi.cant limitation of previous LLS ap­proaches is that the camera observes the sample at 
an oblique angle of approximately 45. . This keeps the light source from occluding sample points when 
the light is directly above, and separates the position of the diffuse and specular peaks so that they 
can be mod­eled independently. This limits resolution due to foreshortening and since the sample will 
likely extend beyond a high-resolution imag­ing system s limited depth of .eld. We instead place the 
camera 1m directly above the material, observ­ing the 30cm × 20cm sample region with an 11 megapixel 
Prosilica GE4000C camera with a 105mm lens. As expected, the LLS lights 10cm above the sample occlude 
the sample just as the LLS passes above it, obscuring important normal re.ectance angles. We solve this 
by illuminating the sample from these lighting angles via a sec­ond, virtual LLS, constrcuted by aiming 
an LLS into the underside of a strip of half-silvered glass mounted at approximately 45.. The light re.ects 
down to the sample and back up through the glass to the camera, yielding a light whose illumination is 
visible even though the lamp itself is unseen. Although the angular coverage of the virtual LLS is limited, 
it more than covers the angles blocked by the direct LLS, so all incident angles are covered. With the 
camera looking straight down, the peaks of the specular and diffuse lobes become largely coincident, 
which makes diffuse and specular re.ections challenging to separate and model inde­pendently. We place 
a strip of polarizing .lter gel over each LLS, and a .lter wheel in front of the camera so that polarization 
dif­ference imaging can separate the diffuse and specular components from two passes of each light source. 
In total, the scanner has four linear light sources: a pair of virtual and direct LLS s to scan in both 
the horizontal and vertical directions. The camera records at 5fps and moves 3mm (the width of each LED 
light strip) per captured image, completing a scan in 20 minutes. Data Processing As the lights pass 
over the sample, each sample point produces a re.ectance trace of pixel values. We composite the unoccluded 
regions from the direct and virtual LLS traces to­gether, and compute a specular-only trace by subtracting 
the cross­polarized diffuse-only trace from the parallel-polarized trace. We use Knuth s online algorithm 
to ef.ciently compute the sum, mean, and standard deviation of the re.ectance lobe in each trace, yielding 
estimates of the albedo, surface normal, and specular roughness of each material point for both the diffuse 
and specular components. For rendering, these parameters are used to drive the Ashikhmin-Shirley re.ectance 
model. Results Fig. 1 shows several brushed and stamped metal sam­ples being scanned, their recovered 
anisotropic re.ectance parame­ter maps, and a real-time rendering of a car interior using several of 
the materials. Other examples appear in the supplemental material. Acknowledgements We thank Ludwig Fuchs, 
Peter Roehner, Jeroen Snepvangers, Muybridge Digital Imaging, Happy Digital, Marco Tan, Danny Tierry, 
Andrea Gunschera, and Conny Denk for their contributions and support. Contact: Jan.Meseth@rtt.ag References 
GARDNER, A., TCHOU, C., HAWKINS, T., AND DEBEVEC, P. 2003. Linear light source re.ectometry. In ACM SIGGRAPH 
2003 Papers, ACM, New York, NY, USA, SIGGRAPH 03, 749 758. REN, P., WANG, J., SNYDER, J., TONG, X., AND 
GUO, B. 2011. Pocket re.ectometry. In ACM SIGGRAPH 2011 papers, ACM, New York, NY, USA, SIGGRAPH 11, 
45:1 45:10. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 
5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2343060</section_id>
		<sort_key>150</sort_key>
		<section_seq_no>4</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Model stories]]></section_title>
		<section_page_from>4</section_page_from>
	<article_rec>
		<article_id>2343061</article_id>
		<sort_key>160</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>12</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Introducing Processing 2.0]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343061</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343061</url>
		<abstract>
			<par><![CDATA[<p>The Processing programming language and environment [RF07] is an open source project initiated by Casey Reas and Ben Fry at the MIT back in 2001, and it is a widely adopted tool in the current practice of creative coding and computer arts and design. Processing has the "Design By Numbers" project [MA01], developed at the MIT Media Lab by John Maeda, as one of its predecessors. DBN's main motivation was to introduce visual designers and artists to computational design. Processing continues with this initial goal of broadening "computer literacy" by making programming more accessible for creators who do not necessarily have a formal training in CS such as artists, designers, and hobbyists in general.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737692</person_id>
				<author_profile_id><![CDATA[81504687843]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andres]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Colubri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fathom Information Design, Boston, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[andres@fathom.info]]></email_address>
			</au>
			<au>
				<person_id>P3737693</person_id>
				<author_profile_id><![CDATA[81100613376]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ben]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fry]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fathom Information Design, Boston, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ben@fathom.info]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>559503</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{MA01} Maeda J., Antonelli P.: <i>Design By Numbers</i>. The MIT Press, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1296181</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{RF07} Reas C., Fry B.: <i>Processing: A Programming Handbook for Visual Designers and Artists</i>. The MIT Press, 2007]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Introducing Processing 2.0 Andres Colubri Ben Fry Fathom Information Design Fathom Information Design 
Boston, MA, USA Boston, MA, USA andres@fathom.info ben@fathom.info 1. Introduction The Processing programming 
language and environment [RF07] is an open source project initiated by Casey Reas and Ben Fry at the 
MIT back in 2001, and it is a widely adopted tool in the current practice of creative coding and computer 
arts and design. Processing has the Design By Numbers project [MA01], developed at the MIT Media Lab 
by John Maeda, as one of its predecessors. DBN s main motivation was to introduce visual designers and 
artists to computational design. Processing continues with this initial goal of broadening computer literacy 
by making programming more accessible for creators who do not necessarily have a formal training in CS 
such as artists, designers, and hobbyists in general. After a long alpha stage, Processing 1.0 was finally 
released in 2008. With an increased focus on 3D and video performance, the version 2.0 has been in development 
during the past two years, and recently reached the beta stage. 2. Exposition Creative coders have been 
witnessing the increasing importance of mobile devices, the need for real-time data visualization, and 
the use of the web as an interaction platform. Processing 2.0 addresses these issues with the inclusion 
of new 3D and video libraries, as well as with a javascript mode which allows to run Processing applications 
directly into the browser. Processing's new 3D library incorporates an entirely overhauled OpenGL-based 
renderer, called P3D, designed to handle large geometries at interactive frame-rates, and to ease the 
cross­development on desktop and mobile platforms. P3D works in conjunction with the new video library, 
based on the GStreamer multimedia toolkit, in order to allow playback and capture of HD media. 2.1 Elaboration 
P3D has been implemented with OpenGL following the ES 2.0 specification, and its main features are the 
following: Two rendering modes: immediate and retained. The first is geared towards quick creation of 
dynamic geometry, while the second is optimized to render large static models.  Efficient use of GPU 
resources in a transparent manner: automatic tessellation of complex shapes, geometry batching to optimize 
data transfers between CPU and GPU memories.  Portability and feature parity across various platforms 
(PC, Mac, Android).  Modification of the rendering pipeline with custom GLSL shaders.   Figure 1. 
Benchmark comparing OpenGL and software rendering in Processing 1.x, P3D in Processing 2.0, and C++ with 
VBOs.  3. Results API-wise, the new 3D renderer in Processing 2.0 is fully backwards compatible with 
1.x, while offering substantial performance gains for most rendering operations. Compared with the previous 
OpenGL renderer in Processing 1.5.1, the speed-up can even reach 20X for very complex models. Performance 
is similar to that of equivalent C++ code in many situations. The chart in figure 1 summarizes some benchmarks 
comparing frame­rate as a function of the number of vertices in the scene. 4. Conclusions Processing 
2.0 is built on top of the current strengths of the language -simple API, library architecture, wide 
adoption in the design and arts communities while it extends its range of applicability by offering 
higher-performance graphics and video and better web support. In particular, the new 3D renderer covers 
a wider range of usage scenarios: immediate mode for quick prototyping, retained mode for rendering complex 
models, and a customizable shader pipeline for more advanced applications. References [MA01] MAEDA J., 
ANTONELLI P.: Design By Numbers. The MIT Press, 2001. [RF07] REAS C., FRY B.: Processing: A Programming 
Handbook for Visual Designers and Artists. The MIT Press, 2007 Copyright is held by the author / owner(s). 
SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343062</article_id>
		<sort_key>170</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>13</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Growing documentary]]></title>
		<subtitle><![CDATA[creating a collaborative computer-supported story telling environment]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343062</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343062</url>
		<abstract>
			<par><![CDATA[<p>Technological innovations in the fields of digital video production, distribution as well as broadband network access and speeds have made creating and sharing digital video contents into a simple process that can be performed by anyone, anywhere. Although the process of sharing contents has undergone a great change with respect to traditional distribution models, the contents themselves still tend to follow traditional linear narrative structures, construction and production workflows.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737694</person_id>
				<author_profile_id><![CDATA[81504687718]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Janak]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bhimani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Graduate School of Media Design]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[janak@kmd.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3737695</person_id>
				<author_profile_id><![CDATA[81504687794]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Annisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mahdia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Graduate School of Media Design]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737696</person_id>
				<author_profile_id><![CDATA[81504688331]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ali]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Almahr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Graduate School of Media Design]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737697</person_id>
				<author_profile_id><![CDATA[81311481862]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Daisuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shirai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Graduate School of Media Design]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737698</person_id>
				<author_profile_id><![CDATA[81332519162]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Naohisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ohta]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Graduate School of Media Design]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Renambot, L. et al. 2004. SAGE: the Scalable Adaptive Graphics Environment. In <i>Proceedings of WACE 2004, (The 4th Workshop on Advanced Collaborative Environments)</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1594953</ref_obj_id>
				<ref_obj_pid>1594943</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ursu, M. F. et al. 2009. Interactive documentaries: A Golden Age. <i>Comput. Entertain. 7</i>, 3, Article 41 (September 2009)]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Growing Documentary: Creating a Collaborative Computer-Supported Story Telling Environment C:\Users\kmd\Desktop\workflow_prototype 
(1).tif  Janak Bhimani*, Annisa Mahdia, Ali Almahr, Daisuke Shirai, Naohisa Ohta Keio University, Graduate 
School of Media Design Figure 1. Growing Documentary workflow implemented in the production and performance 
of lenses + landscapes Introduction Technological innovations in the fields of digital video production, 
distribution as well as broadband network access and speeds have made creating and sharing digital video 
contents into a simple process that can be performed by anyone, anywhere. Although the process of sharing 
contents has undergone a great change with respect to traditional distribution models, the contents themselves 
still tend to follow traditional linear narrative structures, construction and production workflows. 
 We believe the Growing Documentary concept can support and encourage more people to tell their stories 
together in new and innovative ways. The Growing Documentary is a platform for computer supported cooperative 
work (CSCW) using user-generated content to produce digital video productions that can be remixed, reworked 
and built upon as the story and story tellers change and adapt. Motivation The idea and concept for 
the Growing Documentary came about as a result of the Great East Japan Earthquake and Tsunami which occurred 
on March 11th, 2011. Not only did this event have an impact on Japan and the rest of the world because 
of the unprecedented level of damage caused by both terrestrial and oceanic natural disasters; but also 
because news of the impact was delivered to people with unprecedented expedience and quality via a variety 
of media to people everywhere. Traditional media outlets were utilizing new social communication tools 
to broadcast their contents. People in the most devastated areas who had no electricity or public services 
were using their smart phones or feature phones to receive information and communicate their stories 
to the outside world. Inspiration . First Prototype The first iteration of the Growing Documentary was 
a short documentary about the devastation and aftermath of March 11th from the point of view of three 
amateur and professional photographers from different walks of life who were all affected by the earthquake 
and tsunami. The film, lenses + landscapes , was produced by crowd-sourcing the photographers, translators 
and audio via social networking services (SNS). As the Growing Documentary is a social platform for individual 
and community cinematic expression, 4K (4096x2160) was chosen as the output resolution. Through the use 
of a cloud-based file sharing system, the photographers raw still images were combined with multi-framed 
HD video to render a very high resolution movie clip through the use of community resources. (Figure 
1) Hands-on Non-linear Narrative Collaboration The Growing Documentary is an important step in furthering 
the field of interactive documentaries [Ursu, M. F. et Al.]. The Growing Documentary platform allows 
collaborators to not only interact with one another in the production of a story, but also the elements 
of the story. Moreover, the story itself becomes an interactive experience: ideas, images, videos and 
sounds can be shared by the story tellers as well as the audience. By incorporating SAGE OptIPortables 
[Renambot, L. et al.] into the creative process of the Growing Documentary, digital contents over gigabit 
pipelines around and throughout the world, people, regardless of location, have the potential to collaborate 
at any time in order to produce high-quality digital contents by and for the global community. Implementation 
and Demonstration The flexibility in the narrative structure of the Growing Documentary allows for freedom 
in implementation. After lenses + landscapes was shown at a special screening during the 2011 Tokyo International 
Film Festival, attendees also had a chance to interact with the film via SAGE. Multiple SAGE walls were 
connected in San Diego at CineGrid 2011 and participants had a chance to immerse themselves in the images, 
videos and sounds of lenses + landscapes while interacting in real-time with others in Japan to create 
their own versions of the story. References RENAMBOT, L. ET AL. 2004. SAGE: the Scalable Adaptive Graphics 
Environment. In Proceedings of WACE 2004, (The 4th Workshop on Advanced Collaborative Environments) 
URSU, M. F. ET AL. 2009. INTERACTIVE DOCUMENTARIES: A GOLDEN AGE. COMPUT. ENTERTAIN. 7, 3, ARTICLE 41 
(SEPTEMBER 2009) 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343063</article_id>
		<sort_key>180</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>14</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Adapting curriculum to explore new 3D modeling technologies and work-flows]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343063</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343063</url>
		<abstract>
			<par><![CDATA[<p>This talk discusses adaptations by educators, students necessary to adjust to a fundamental shift in the creation of 3D modeling. We follow three main steps 1) identification-assessment, 2) justification-adoption, 3) integration-dissemination.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737699</person_id>
				<author_profile_id><![CDATA[81504683402]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shaun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Foster]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rochester Institute of Technology, Rochester, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[scffaa@rit.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737700</person_id>
				<author_profile_id><![CDATA[81504687394]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Halbstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rochester Institute of Technology, Rochester, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[dlhfaa@rit.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2037828</ref_obj_id>
				<ref_obj_pid>2037826</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bhat, Pravin and Burke, Sebastian: Weta Digital <u><i>PhotoSpace: a vision based approach for digitizing props</i></u> 2011, ACM SIGGRAPH 2011 Talks Article No. 1 New York, NY]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Adapting Curriculum to Explore new 3D Modeling Technologies and Work-flows IMG_0582.JPG Shaun Foster 
Rochester Institute of Technology Rochester, NY, USA scffaa@rit.edu David Halbstein Rochester Institute 
of Technology Rochester, NY, USA dlhfaa@rit.edu 1. Introduction This talk discusses adaptations by 
educators, students necessary to adjust to a fundamental shift in the creation of 3D modeling. We follow 
three main steps 1) identification-assessment, 2) justification-adoption, 3) integration-dissemination. 
 2. Exposition Over the past year we have focused on identification, assessment and justification. Two 
core technologies: computer vision based 3D mesh generation and retopologizing software have rapidly 
matured. The ability to build 3D models has moved from costly, time intensive, and proprietary to inexpensive, 
nearly instantaneous and widely accessible. At SIGGRAPH 2011 Weta Digital discussed PhotoSpace, their 
digital prop creation technology also based on 2D image to 3D mesh generation and Microsoft demonstrated 
Kinect Fusion demonstrating real-time 3D environment digitization. At RIT we have been testing Autodesk 
123D Catch for rapid mesh creation. We have also tested 3D-Coat for rapid retopologizing. This combination 
results in dramatic increases in both speed and quality for 3D model creation. It frees students from 
many of the constraints of software training allowing them to focus on their aesthetic education.  
          Figure 1. From additive to retopogize workflows.   2.1 Elaboration Adoption justification 
still faced several hurdles. Fortunately software costs were zero. However, introduction of new teaching 
methods means that old lectures must be condensed, addressed in a different class or eliminated from 
the curriculum entirely. We have begun to integrate this technology into our classes. We were able to 
partly able to accomplish this by increasing collaboration among different departments. Students in the 
areas of sculpture, industrial design, furniture design, interior design etc. now have a quick pathway 
to digital representation of their designs.  3. Results This has already led to some successful collaboration 
among students. Technically adept students get to work, learn and communicate their knowledge of modeling 
optimization, edge flow, UV Layout, and deformation, and in turn work with those focused on aesthetic 
development. They have succeeded in this challenging communication process for accurate anatomical or 
sculptural representations that might push the envelope of 3D design. In addition this cross discipline 
design collaborative work enhances student education, work-ethic, student portfolio and prospects for 
employment. The students who will speak demonstrate several different profiles of technical and artistic. 
They will present how they integrated this technology and how it affected their process. 4. Conclusions 
 Finally we will discuss evaluation of our integration process as connected to dissemination of our student 
work. We will present this analysis in the context of what was being done before, what new expressive 
abilities the students have gained as a result of these changes, what concepts may have been eliminated. 
While we foresee continued change and adjustment, the methods for adopting and integrating new 3D modeling 
workflows and technologies into curriculum look to be a successful, however, there will be continued 
monitoring and adjusting. References BHAT, PRAVIN AND BURKE, SEBASTIAN: WETA DIGITAL PhotoSpace: a 
vision based approach for digitizing props 2011, ACM SIGGRAPH 2011 Talks Article No. 1 New York, NY 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343064</article_id>
		<sort_key>190</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>15</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[3D diff]]></title>
		<subtitle><![CDATA[an interactive approach to mesh differencing and conflict resolution]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343064</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343064</url>
		<abstract>
			<par><![CDATA[<p>We introduce <i>3D diff</i>, a novel approach to interactively resolve conflicting mesh edits. Our method performs conflict detection and visualization and allows the user to quickly select one of the two revisions for each conflicted scene graph (SG) node. By further integrating the knowledge about a common ancestor of the two models, a. k. a. 3-way diff [Mens 2002], we are able to automatically resolve more conflicts than in a standard 2-way comparison. Our method is particularly useful for revision management of 3D models.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737701</person_id>
				<author_profile_id><![CDATA[81504684923]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jozef]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dobo&#353;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University College London]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[j.dobos@cs.ucl.ac.uk]]></email_address>
			</au>
			<au>
				<person_id>P3737702</person_id>
				<author_profile_id><![CDATA[81100501447]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Anthony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Steed]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University College London]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1965000</ref_obj_id>
				<ref_obj_pid>2010324</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chen, H.-T., Wei, L.-Y., and Chang, C.-F. 2011. Nonlinear revision control for images. <i>ACM Trans. Graph. 30</i>, 4 (Aug.), 105:1--105:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>567178</ref_obj_id>
				<ref_obj_pid>567176</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Mens, T. 2002. A state-of-the-art survey on software merging. <i>IEEE Transactions on Software Engineering 28</i>, 5, 449--462.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 3D Diff: An Interactive Approach to Mesh Differencing and Con.ict Resolution Jozef Dobo.s, Anthony 
Steed * University College London  Figure 1: 3-way 3D diff: Two revisions (left and right) of the same 
model are compared against their common ancestor and each other to suggest automatic con.ict resolution 
(in blue). Con.icts are in red, non-con.icting modi.cations in blue, user selection in orange and manually 
merged results in gray. Our user interface facilitates the 2-and 3-way 3D diff. Model attribution: Blender 
Foundation. 1 Introduction We introduce 3D diff, a novel approach to interactively resolve con­.icting 
mesh edits. Our method performs con.ict detection and visualization and allows the user to quickly select 
one of the two re­visions for each con.icted scene graph (SG) node. By further inte­grating the knowledge 
about a common ancestor of the two models, a.k.a. 3-way diff [Mens 2002], we are able to automatically 
resolve more con.icts than in a standard 2-way comparison. Our method is particularly useful for revision 
management of 3D models. In general, comparing and merging two models is a complex and time consuming 
task especially when concurrent edits by multiple people are to be integrated. When combining modi.ed 
versions of the same 3D scene, popular modelling packages such as Autodesk 3ds Max, Maya or Blendersuperimpose 
the meshes but do not aid the merging process any further. Despite the advantages of vertex­level editing, 
most of the time it would be satisfactory to simply preserve whole sections from the individual model 
versions to form a combined result. In their recent revision control system for images, Chen et al. [2011] 
implemented a visual diff as a playback of recorded image edits. State of the art side-by-side 3D model 
comparison is offered by Provenance Explorer for Maya1 which, similarly, relies on stored edit sequences. 
In contrast, our prototype works independently from editing software like many line-based differencing 
tools for source code management [Mens 2002]. Closest to our approach is the abandoned Art Diff for Subversion 
project2 that loads 3D .les for a basic visual comparison but does not detect con.icts nor does it support 
merging. 2 Our Approach During the modelling stage, we assign each SG node a universally unique identi.er 
(UUID) so that the same parts of a model can eas­ *e-mail: j.dobos@cs.ucl.ac.uk 1http://vistrails.com/maya.html 
 2http://code.google.com/p/artdiff/  ily be matched when comparing models different revisions. To ensure 
that our solution is independent from the modelling soft­ware, we decompose most common 3D .le formats 
into their SG components via the Open Asset Import Library (Assimp3). Our stand-alone model viewer performs 
an early reject byte-by-byte comparison of SG nodes from various revisions that share the same UUID. 
When there are discrepancies in the two models, they are treated as con.icting edits (red in Fig. 1). 
However, in a 3-way comparison, we add extra information about the common ancestor of the differenced 
models. If one of the versions is the same as the original, i.e. no changes have been made, the other 
must be the in­tended modi.cation to be automatically preserved during the merge process (blue). Nevertheless, 
the user has to con.rm all changes in­cluding modi.cations as these could visually interfere with other 
parts of the model. To perform a fast merge, the user can select a version for each con.icted SG node 
in a color-coded con.icts list and inspect the proposed combined result visually. 3 Evaluation and Conclusions 
Based on the initial evaluation of our prototype, we believe that such approach to mesh differencing 
and merging can signi.cantly speed up the revision control management of 3D models. What is more, this 
technique allows for comparison of otherwise incompat­ible 3D .le formats. Integration into modelling 
packages via their plug-in frameworks is also possible. In the near future, we plan to make this work 
open source and will investigate automated camera navigation for better context understanding, bounding 
box intersec­tion detection and vertex-level merging. References CHEN, H.-T., WEI, L.-Y., AND CHANG, 
C.-F. 2011. Nonlinear revision control for images. ACM Trans. Graph. 30, 4 (Aug.), 105:1 105:10. MENS, 
T. 2002. A state-of-the-art survey on software merging. IEEE Transactions on Software Engineering 28, 
5, 449 462. 3http://assimp.sourceforge.net Copyright is held by the author / owner(s). SIGGRAPH 2012, 
Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2343065</section_id>
		<sort_key>200</sort_key>
		<section_seq_no>5</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Three is a crowd]]></section_title>
		<section_page_from>5</section_page_from>
	<article_rec>
		<article_id>2343066</article_id>
		<sort_key>210</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>16</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[CageR]]></title>
		<subtitle><![CDATA[from 3D performance capture to cage-based representation]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343066</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343066</url>
		<abstract>
			<par><![CDATA[<p>Modern performance capture systems [de Aguiar et al. 2008] provide high resolution 3D mesh sequences which are becoming critical components for today's special effects. Unfortunately, such raw sequences have a large memory footprint and are difficult to edit. We propose CageR, a framework based on spatial deformation with cages to construct automatically a compact and editable high level representation of these raw sequences, resulting in high compression factors and allowing easier post processing. In particular, we formulate an automatic cage fitting algorithm embedding a new relaxation strategy based on Maximum Volume and a new regularization method based on sub-spectral analysis. As a result, we use the CageR representation in various applications, including compression, motion transfer and shape space modeling.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[GPU]]></kw>
			<kw><![CDATA[cage coordinates]]></kw>
			<kw><![CDATA[linear algebra]]></kw>
			<kw><![CDATA[maxvol]]></kw>
			<kw><![CDATA[reverse engineering]]></kw>
			<kw><![CDATA[spectral]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737703</person_id>
				<author_profile_id><![CDATA[81502774394]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jean-Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thiery]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Telecom ParisTech -- CNRS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[thiery@telecom-paristech.fr]]></email_address>
			</au>
			<au>
				<person_id>P3737704</person_id>
				<author_profile_id><![CDATA[81444596644]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Julien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tierny]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Telecom ParisTech -- CNRS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tierny@telecom-paristech.fr]]></email_address>
			</au>
			<au>
				<person_id>P3737705</person_id>
				<author_profile_id><![CDATA[81309487978]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tamy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boubekeur]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Telecom ParisTech -- CNRS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[boubekeur@telecom-paristech.fr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1360697</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[de Aguiar, E., Stoll, C., Theobalt, C., Ahmed, N., and Seidel, H. 2008. Performance capture from sparse multi-view video. <i>SIGGRAPH 27</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Goreinov, S., Oaeledets, L., Savostyanov, D., Tyrtyshnikov, E., and Zamarashkin, N. 2010. How to find a good submatrix. <i>Matrix Methods: Theory, Algorithms and Applications</i>, 247.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073229</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ju, T., Schaefer, S., and Warren, J. 2005. Mean value coordinates for closed triangular meshes. <i>SIGGRAPH 24</i>, 3, 561--566.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 CageR: From 3D Performance Capture to Cage-based Representation Jean-Marc Thiery Julien Tierny Tamy 
Boubekeur Telecom ParisTech CNRS *  subset of positional constraints for the cage coordinate inversion. 
Then, the cage coordinates are inverted for each frame of the input sequence. A selective enforcement 
of regularization terms is de.ned to affect the cage vertices where the inversion is the most unstable. 
The resulting smoothly varying cage sequence faithfully reconstructs the input sequence when applied 
to a single mesh frame. Keywords: cage coordinates, spectral, maxvol, GPU, linear alge­bra, reverse engineering 
1 Introduction Modern performance capture systems [de Aguiar et al. 2008] pro­ vide high resolution 3D 
mesh sequences which are becoming crit­ical components for today s special effects. Unfortunately, such 
raw sequences have a large memory footprint and are dif.cult to edit. We propose CAGER, a framework based 
on spatial deforma­tion with cages to construct automatically a compact and editable high level representation 
of these raw sequences, resulting in high compression factors and allowing easier post processing. In 
partic­ular, we formulate an automatic cage .tting algorithm embedding a new relaxation strategy based 
on Maximum Volume and a new regularization method based on sub-spectral analysis. As a result, we use 
the CAGER representation in various applications, including compression, motion transfer and shape space 
modeling. 2 Reverse Engineering Given a mesh M and a closed triangle cage mesh C, cage coor­dinate techniques, 
e.g. Mean Value Coordinates [Ju et al. 2005], allow to encode each vertex position pi of M w.r.t. vertex 
positions h cj of C by: pi = j fj (i) · cj , or M =F ·C. Given a set of poses Mk of the model (i.e., 
typical performance capture output), we want to generate a set of cages Ck such that F ·Ck .Mk. The L2-projection 
of Mk onto the space of admissible deformations is Mk =F · F Mk, which involves the pseudo-inverse F 
of F. Unfortunately, as nowadays cage coordinate systems are unstable to inversion (large condition number 
of the system, from 103 to 1010 in the results we present), the resulting cage has very large instabil­ities. 
This set of cages is not suited for compression or any other application, as the scheme becomes sensitive 
to numerical errors. To overcome this issue, we .rst propose to relax the system by tak­ing only the 
minimum number of positions as constraints, i.e. the dimension of the problem. The selection of the so-called 
handles (green spheres in Fig. 2) is performed by looking for the maximum volume square submatrix Fo 
of F (known as maxvol problem). We use the maxvol approximation model by Goreinov et al. [2010], for 
which we propose a GPU implementation. The inversion is .rst performed by computing the pseudo-inverse 
of Fo. Then, we add a geometrical regularity term on the cage geometry (e.g. minimiza­tion of the cage 
Laplacian) on a sub-part of the spectrum of Fo. This strategy automatically focuses the regularization 
on the cage *Email: {thiery,tierny,boubekeur}@telecom-paristech.fr vertices for which the inversion is 
the most unstable, leaving the other vertices unaffected. Then, the output cages evolve smoothly along 
the sequence and their initial shape features are preserved. Interestingly, our relaxation and our regularization 
strategies are highly compatible, as the use of MaxVol lowers cage instabilities and improves the spectral 
properties of the system to invert. 3 Results &#38; Applications. Some examples of CAGER reconstructions 
are presented in Fig. 2. Many others are provided as additional material and in the accom­panying video. 
Before applying any subsequent data compression scheme (e.g. wavelets), our representation already offers 
a high compression ratio (up to 97.76%), with high numerical stability. We implemented the iterative 
maxvol approximation algorithm on GPU with CUDA, to accelerate the pre-process. Beyond compres­sion, 
CAGER representations allow to transfer shape motion from captured model to synthetic ones (Fig. 2, bottom-left) 
and speed­ up by several orders of magnitude complex geometry interpolation techniques (Fig. 2, bottom-right). 
 References DE AGUIAR, E., STOLL, C., THEOBALT, C., AHMED, N., AND SEIDEL, H. 2008. Performance capture 
from sparse multi-view video. SIGGRAPH 27. GOREINOV, S., OAELEDETS, L., SAVOSTYANOV, D., TYRTYSHNIKOV, 
E., AND ZAMARASHKIN, N. 2010. How to .nd a good submatrix. Matrix Methods: Theory, Algorithms and Applications, 
247. JU, T., SCHAEFER, S., AND WARREN, J. 2005. Mean value coordinates for closed triangular meshes. 
SIGGRAPH 24, 3, 561 566. Figure 2: Top: Results of our reverse engineering method on two performance 
capture data sets. Bottom-left: motion transfer. Bottom-right: interactive shape space modeling. Copyright 
is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2343067</section_id>
		<sort_key>220</sort_key>
		<section_seq_no>6</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Play time]]></section_title>
		<section_page_from>6</section_page_from>
	<article_rec>
		<article_id>2343068</article_id>
		<sort_key>230</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>17</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A guided synthesizer for blendshape characters]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343068</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343068</url>
		<abstract>
			<par><![CDATA[<p>We have designed an interactive editing system that will help users to create new characters based on a limited number of training characters. The user provides a desired preliminary shape of the new character. The output of our system will be a convex linear combination of the existed characters in a local sense while maintaining the proximity to the user desired shape as much as possible. This feature guarantees the creation process is guided by the information of artists' previous works. The system first finds the best set of training characters and blending weights that represents the user desired shape, then computes the result shape, which matches the obtained composition, using an edge-based integration.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737706</person_id>
				<author_profile_id><![CDATA[81100447998]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wan-Chun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Weta Digital]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[wma@wetafx.co.nz]]></email_address>
			</au>
			<au>
				<person_id>P3737707</person_id>
				<author_profile_id><![CDATA[81409595942]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Lewis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Weta Digital]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[noisebrain@wetafx.co.nz]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2037903</ref_obj_id>
				<ref_obj_pid>2037826</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ma, W.-C., Fyffe, G., and Debevec, P. 2011. Optimized local blendshape mapping for facial motion retargeting. In <i>ACM SIGGRAPH 2011 Talks</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Guided Synthesizer for Blendshape Characters Wan-Chun Ma* J.P. Lewis Weta Digital  Training characters 
 We have designed an interactive editing system that will help users to create new characters based on 
a limited number of training char­acters. The user provides a desired preliminary shape of the new character. 
The output of our system will be a convex linear combi­nation of the existed characters in a local sense 
while maintaining the proximity to the user desired shape as much as possible. This feature guarantees 
the creation process is guided by the informa­tion of artists previous works. The system .rst .nds the 
best set of training characters and blending weights that represents the user desired shape, then computes 
the result shape, which matches the obtained composition, using an edge-based integration. Optimal Composition 
Assume both of the input shape s and character targets T = {ti|1 = i = nt} are corresponded and rep­resents 
a similar expression and (e.g. neutral). Following [Ma et al. 2011], the optimal composition is looking 
for the best set of train­ ing characters P = {pi|1 = i = nv} and blending weights that minimizes geometrical 
reconstruction error in a local sense, while maintaining a global spatial-coherence with respect to the 
character targets used. We can formulate the problem into an optimization based on the above criteria: 
min E = Eg + aEc. (1) P Eg is a function that measures the geometrical reconstruction error. It returns 
the sum of the Euclidean distance between r(s, i) , the local geometrical measurement of the input shape 
around the ith vertex, and its approximation from the character targets r(T,pi,i) based on the convex 
linear interpolation of a character set repre­sented by pi: Eg(P )= r(s, i) - r(T,pi,i) 2 , (2) i.V 
nk r(T,pi,i)= wji r(tpi ,i). (3) j j=1  nk ii w =1, wj = 0, and nk is a pre-de.ned number of targets 
j=1 j in a character set. Different than [Ma et al. 2011], we de.ne r to be the Laplacian operator because 
its translational invariant prop­erty, since the character targets are usually in different sizes and 
locations. Equation 3 can be solved by constrained least squares. Ec is a function that measures the 
cost of assigning a character set label p i to vertex i with respect to its neighbor vertex j. Ec returns 
*e-mail: wma@wetafx.co.nz e-mail: noisebrain@wetafx.co.nz User input Synthesized character the sum of 
the difference between each adjacent pair p i and pj such that it encourages adjacent vertices to use 
similar targets as much as possible. The Hamming distance function h is used to model Ec: Ec(P )=h(p 
i ,pj ). (4) (i,j).E Equation 1 can be regarded as .nding the maximum a posteriori (MAP) estimator of 
a Markov random .eld (MRF). Exact inference is computationally intractable in general. Loopy belief propagation 
is applied to provide an approximate solution. a controls the bal­ance between the geometrical error 
term and the coherence term. To reduce the complexity for speed-up, both of the input shape and the character 
targets are resampled into a lower resolution graph. The weights for each vertex in the original mesh 
can be interpo­lated from the nearby samples by radial basis functions or other scattered data interpolation 
techniques. Character Synthesis Once blending weights at each vertex are recovered, instead of directly 
blending the vertex positions (which is invalid, as the same reason of using the Laplacian operator for 
optimal composition), we blend the edge vector of each character target according to the weights: nt 
 dq iij = wk(vk - vk). (5) k=1 dq denotes the qth edge vector that connects the ith and jth ver­tices. 
The vertex positions u of the new shape is the solution of the linear system Mu = d, where M is the oriented 
incidence matrix based on the undirected graph G, and d stores all the edge vectors. Note that M is not 
full rank, so we have to set at least one vertex a .xed value as a constraint. Sine the edge-based integration 
fa­vors preserving the edge lengths of the character targets, locally the surface will conforms to the 
training character targets better. The same process of blending/integration can be repeated to construct 
other expressions in the character asset based on the same weights. The overall algorithm generates a 
greater variety of characters than would be possible using simple linear blending, while neverhteless 
re.ecting the types of geometric variation evident in the training shapes. References MA, W.-C., FYFFE, 
G., AND DEBEVEC, P. 2011. Optimized local blendshape mapping for facial motion retargeting. In ACM SIGGRAPH 
2011 Talks. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 
5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343069</article_id>
		<sort_key>240</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>18</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[dRig]]></title>
		<subtitle><![CDATA[an artist-friendly, object-oriented approach to rig building]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343069</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343069</url>
		<abstract>
			<par><![CDATA[<p>Ever increasing complexity and performance demands on animated characters in feature films necessitate unique rigging workflows and toolsets. We present dRig, a novel approach to rigging that allows for efficient reuse and extension of existing assets, fast authoring of per-element variations, and most importantly, accessibility of rig code to the entire department crew. By leveraging the powerful concepts of object-oriented programming and presenting it to the user through a well managed, artist-friendly interface, dRig provides a framework for sophisticated rig evolution and development.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737708</person_id>
				<author_profile_id><![CDATA[81504685734]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Smith]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737709</person_id>
				<author_profile_id><![CDATA[81504687154]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McLaughlin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737710</person_id>
				<author_profile_id><![CDATA[81504685114]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737711</person_id>
				<author_profile_id><![CDATA[81442598713]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Evan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goldberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737712</person_id>
				<author_profile_id><![CDATA[81458652386]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Frank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 dRig: An Artist-Friendly, Object-Oriented Approach to Rig Building Greg Smith Mark McLaughlin Andy Lin 
Evan Goldberg Frank Hanner Walt Disney Animation Studios  Figure 1: Characters created with the dRig 
system Introduction Ever increasing complexity and performance demands on animated characters in feature 
films necessitate unique rigging workflows and toolsets. We present dRig, a novel approach to rigging 
that allows for efficient reuse and extension of existing assets, fast authoring of per-element variations, 
and most importantly, accessibility of rig code to the entire department crew. By leveraging the powerful 
concepts of object-oriented programming and presenting it to the user through a well managed, artist-friendly 
interface, dRig provides a framework for sophisticated rig evolution and development. Object-Oriented 
Approach Unlike traditional procedural rig-building systems that automate the creation of element templates 
(faces, arms, spines, etc.), dRig is rig agnostic. Rather, it is a tool used to organize and modify the 
code used to create the rigs. Applying object-oriented design to rigging provides several benefits. These 
include encapsulation of a rig and its parts into reusable pieces and extension of any of these pieces 
by characters/types. These features allow the rigger to organize, override, and extend the individual 
features of each unique rig. The rigs are defined by simple, human-readable text files that hold small 
units of code and variables. These variables are organized into hierarchical groupings that follow logical 
rig structures (e.g. body|left|arm|hand|finger). This hierarchy produces unique paths that the artist 
can use to easily navigate to the desired piece of the rig build. The small size of each block (and the 
code within) reduces the scope of a modification, allowing for more targeted adjustments. To ease debugging, 
these groupings are represented in dRig s interface as a tree view and the code/variables are represented 
in an expanded details panel. These views are akin to Maya s Outliner and Attribute Editor, which users 
are already familiar with. In addition to the tree structures, dRig has the ability to layer these files. 
Each layer can mask the variables and code from the previous layer(s) by redefining the block at a given 
path. This allows for file A to inherit all of the information from file B while still allowing for surgical 
edits/additions to any aspect of the build. This inheritance allows for extensive reuse and customization 
from archetypal characters (bipeds, quadrupeds, etc.) or from other similar characters within a show. 
It is significant to note that the result of compiling these files is not the final asset, but rather, 
an aggregate composite of code and variables that can be executed within our primary animation package 
(Maya) to create the final rig. This build process is managed through an intuitive interface that is 
akin to a programming IDE. It allows artists to execute and debug their code by setting break points, 
stepping through the build and getting instant feedback on errors or warnings in their rig code. 3 Artist 
Accessibility One of the most important features of dRig within the rigging workflow at Disney is artist 
accessibility. Many rigging systems require highly technical users to create and augment rig pieces or 
the code that creates them. This limits quick iteration, innovation, and ultimately crew engagement. 
In contrast, dRig s simple, readable file format, its organization of code into navigable hierarchies 
of bite-sized code, and its intuitive interface allow even new artists and novice coders to debug and 
augment the rig setup. To further open the code base to multi-user and multi-show editing, a git repository 
back-end is used to track edits made to the rig files. This allows for flexibility of individual productions 
to iterate and improve in an agile fashion without disrupting other productions that need stability in 
their rig builds. The shows can use standard git management tools to merge divergences and updates of 
rig definitions. 4 Evolution By making the rigging code base more accessible, the rig builds see constant 
development and evolution. Through inheritance, organic, studio-wide standardization is realized as popular 
code blocks are layered/linked and propagated across rigs. Additionally, clear organization of code allows 
precise, incremental improvements in rig functionality as well as facilitates broader changes in base 
rig structures. A sophisticated rig and build procedure can evolve with the crew, and with each new show, 
to achieve new and demanding performances. Copyright is held by the author / owner(s). SIGGRAPH 2012, 
Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2343070</section_id>
		<sort_key>250</sort_key>
		<section_seq_no>7</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Building character]]></section_title>
		<section_page_from>7</section_page_from>
	<article_rec>
		<article_id>2343071</article_id>
		<sort_key>260</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>19</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Computer-assisted animation of line and paint in Disney's <i>Paperman</i>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343071</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343071</url>
		<abstract>
			<par><![CDATA[<p>We present a system which allows animators to combine CG animation's strengths -- temporal coherence, spatial stability, and precise control -- with traditional animation's expressive and pleasing line-based aesthetic. Our process begins as an ordinary 3D CG animation, but later steps occur in a light-weight and responsive 2D environment, where an artist can draw lines which the system can then automatically move through time using vector fields derived from the 3D animation, thereby maximizing the benefits of both environments. Unlike with an automated "toon-shader", the final look was directly in the hands of the artists in a familiar workflow, allowing their artistry and creative power to be fully utilized. This process was developed during production of the short film <i>Paperman</i> at Walt Disney Animation Studios, but its application is extensible to other styles of animation as well.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737713</person_id>
				<author_profile_id><![CDATA[81430624223]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Whited]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737714</person_id>
				<author_profile_id><![CDATA[81332495128]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Daniels]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737715</person_id>
				<author_profile_id><![CDATA[81466648514]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaschalk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737716</person_id>
				<author_profile_id><![CDATA[81504686664]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Osborne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737717</person_id>
				<author_profile_id><![CDATA[81328489991]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kyle]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Odermatt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2343072</section_id>
		<sort_key>270</sort_key>
		<section_seq_no>8</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[John Carter scales up]]></section_title>
		<section_page_from>8</section_page_from>
	<article_rec>
		<article_id>2343073</article_id>
		<sort_key>280</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>20</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[The stereoscopic conversion pipeline for John Carter]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343073</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343073</url>
		<abstract>
			<par><![CDATA[<p>We present the stereoscopic conversion technology developed at Cinesite to convert over 1500 shots for Disney's John Carter.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737718</person_id>
				<author_profile_id><![CDATA[81504687791]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Willman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cinesite]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737719</person_id>
				<author_profile_id><![CDATA[81504688243]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gregory]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Keech]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cinesite]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737720</person_id>
				<author_profile_id><![CDATA[81504688764]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grotelueschen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cinesite]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737721</person_id>
				<author_profile_id><![CDATA[81504687483]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michele]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sciolette]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cinesite]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2037843</ref_obj_id>
				<ref_obj_pid>2037826</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Katie Tucker-Fico et al. 2011. Design and realization of stereoscopic 3D for Disney classics. In ACM SIGGRAPH 2011 Talks (SIGGRAPH '11). ACM, New York, NY, USA, Article 12, 1 pages.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Stereoscopic Conversion Pipeline for John Carter Scott Willman Gregory Keech John Grotelueschen 
Michele Sciolette Cinesite  1. Introduction We present the stereoscopic conversion technology developed 
at Cinesite to convert over 1500 shots for Disney s John Carter. Traditional stereoscopic conversion 
techniques can be broadly divided into two main categories: disparity grading techniques, where disparity 
maps are manually generated by skilled artists [Tucker-Fico at al, 2011] and geometry projection techniques 
where images are projected onto 3d geometry and rendered from a different point of view. We developed 
a hybrid approach that allowed us to have the flexibility of the disparity grading techniques with the 
consistency and spatial coherency of a 3d geometry-based approach: we use accurate 3d geometry and camera 
tracking data to generate spatially correct depth maps that provide the basis for the image based part 
of our pipeline. 2. Implementation The first stage of our pipeline is to generate accurate depth maps 
for each frame. For live action elements this required two separate processes for each shot: a comprehensive 
camera and object tracking (rotomation) and a detailed traditional 2D rotoscoping of all elements. Efficiently 
managing these two independent processes was a key focus of our initial development effort and we created 
a large set of tools to streamline the process and ensure consistent outputs. Work on each shot started 
with our artists using an interactive tool to define which parts of the frame needed to be isolated and 
export these shot requirements into a standard format. Each artist in the rotomation and rotoscoping 
teams received a data file to help them visually and automatically check that their work matched the 
shot requirements. Being able to ensure consistency across these tasks allowed us to automate the setup 
for the following stages and played a key part in making the process scale to the required large volume 
of shots. Using a 3D rendering system, we projected the 2D rotoscoping mattes onto each corresponding 
3D object and rendered a depth map from the point of view of the original camera. The resulting object 
depth map often contained small errors or holes that we would fix using traditional image processing 
techniques. For CG generated elements, depth maps were directly available from the rendering system, 
though we had to ensure that all scaling of depth maps was accurate so that all components of the final 
depth map were consistent with each other. The rest of the pipeline is entirely based on two-dimensional 
image processing of the depth maps and image content. One key part of the process consists of accurately 
and efficiently warping the images based on the depth maps. For a desired stereoscopic interaxial distance, 
we compute a disparity map so that for each pixel in the original view we know its corresponding location 
in the new view. This data cannot be used directly into a standard backward-warping tool, which causes 
invalid pixels to overwrite valid ones and trailing or leading edges to disappear, depending on which 
eye is being recreated. To address this problem, we developed a proprietary depth aware forward-warping 
algorithm that generates a new warped disparity map that can be used by a backward-warping tool available 
in a standard compositing package. As each pixel in the new view can be mapped to several source pixels 
in the original view, we use depth sorting to determine the valid source pixel choice. Given a requirement 
that our stereo images be free of vertical disparity, we were able to heavily optimize our code relying 
on the fact that we only needed to shift data horizontally. One of the most difficult and time-consuming 
parts of the stereoscopic conversion process is the painting of occluded regions. We developed an occlusion 
mapping tool to identify all regions requiring paint and an automated depth aware inpainting technique 
that was extremely effective in dealing with small occlusions. In cases where the automated tool was 
unsuccessful, the occlusion maps allowed a paint artist to determine the exact size and location of all 
occluded areas, thus greatly reducing the amount of manual paint work required. 3. Conclusions Our hybrid 
solution to the conversion problem proved to deal effectively with a variety of shot types and production 
workflows. Artists were able to quickly address all requests from the stereographers and generate a final 
stereo output of the highest possible quality. Future developments of our stereo conversion process would 
be to use deep compositing to simplify the interaction with CG assets and allow volumetric renders to 
be placed within the live action environment more accurately. References Katie Tucker-Fico et al . 2011. 
Design and realization of stereoscopic 3D for Disney classics. In ACM SIGGRAPH 2011 Talks (SIGGRAPH '11). 
ACM, New York, NY, USA, , Article 12 , 1 pages. Copyright is held by the author / owner(s). SIGGRAPH 
2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343074</article_id>
		<sort_key>290</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>21</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Thern, the nano technology from John Carter's Mars]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343074</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343074</url>
		<abstract>
			<par><![CDATA[<p>Thern, or the Ninth Ray, as it was referred to in John Carter, covered a broad scope of related effects throughout the film. Twenty different variations fell under the umbrella of Thern, ranging from small standalone assets, to entire rooms filling the screen, and to effects that were only vaguely recognizable as Thern. Along with a wide range of concept art and reference footage of natural phenomenon, we keyed on a few conceptual ideas from the director. From "somewhere between organic and mechanical" we focused on a vegetation growth technique [Palubicki et al. 2009] with modifications specific to our needs. From "fractal", we derived a recursive technique that we could direct between different levels depending on how we wanted the viewer to interpret the image. Animation grew out of disparate references and the ingenuity of the team, culminating in a creative use of a very old algorithm.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737722</person_id>
				<author_profile_id><![CDATA[81504688747]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pickler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cinesite]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rpickler@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P3737723</person_id>
				<author_profile_id><![CDATA[81504687597]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Simon]]></first_name>
				<middle_name><![CDATA[Stanley]]></middle_name>
				<last_name><![CDATA[Clamp]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cinesite]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ssc@cinesite.co.uk]]></email_address>
			</au>
			<au>
				<person_id>P3737724</person_id>
				<author_profile_id><![CDATA[81504687776]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Artemis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Oikonomopoulou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cinesite]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[artemis@cinesite.co.uk]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Dijkstra, E. W. 1959. A note on two problems in connexion with graphs. <i>Numerische Mathematik 1</i>, 269--271. 10.1007/BF01386390.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531364</ref_obj_id>
				<ref_obj_pid>1576246</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Palubicki, W., Horel, K., Longay, S., Runions, A., Lane, B., M&#283;ch, R., and Prusinkiewicz, P. 2009. Self-organizing tree models for image synthesis. In <i>ACM SIGGRAPH 2009 papers</i>, ACM, New York, NY, USA, SIGGRAPH '09, 58:1--58:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Petrovic, L., Henne, M., and Anderson, J. 2003. Volumetric methods for simulation and rendering of hair. <i>Simulation</i>, 06-08, 1--6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Thern, the Nano Technology from John Carter s Mars Richard Pickler* Simon Stanley Clamp Artemis Oikonomopoulou 
Cinesite Cinesite Cinesite Figure 1: Close encounter with Thern 1 Introduction Thern, or the Ninth Ray, 
as it was referred to in John Carter, cov­ered a broad scope of related effects throughout the .lm. Twenty 
different variations fell under the umbrella of Thern, ranging from small standalone assets, to entire 
rooms .lling the screen, and to effects that were only vaguely recognizable as Thern. Along with a wide 
range of concept art and reference footage of natural phe­nomenon, we keyed on a few conceptual ideas 
from the director. From somewhere between organic and mechanical we focused on a vegetation growth technique 
[Palubicki et al. 2009] with modi.­ cations speci.c to our needs. From fractal , we derived a recursive 
technique that we could direct between different levels depending on how we wanted the viewer to interpret 
the image. Animation grew out of disparate references and the ingenuity of the team, cul­minating in 
a creative use of a very old algorithm. 2 Look Early concept work at Cinesite produced a static model 
of a small section of Thern. While this intricate prototype was hand modeled and textured with traditional 
techniques, it was never an option for the large scale set elements. L-Systems proved too dif.cult to 
direct into the complicated structures needed, and a Voronoi based system was rejected because the results 
were not visually organic and far too regular. Ultimately, we developed a system based upon vegeta­tion 
techniques entirely within Houdini. We implemented the , the biggest challenge came when we had to make 
the growth loop back upon itself. We accomplished this by turning the branch-avoidance method into branch-attraction 
gradually over time. Our initial brief on the structure described it as a fractal. Specif­ically, if 
we were to zoom in enough, the same structure should emerge to the viewer. Our original concept attempted 
this through an intricate texture, which proved limiting in both artists time and our animation abilities. 
Instead, we adopted a recursive concept directly in our modeling work.ow. We modeled the gross struc­ture 
through one level of simulation, then after setting relative sizes of individual branches and some manual 
massaging, we .lled that structure with the very same tool. Conceptually, the .rst simula­tion is the 
larger structure the audience perceives, and the .ll step is what it was actually made of. When rendering 
this mass of curves, *e-mail: rpickler@gmail.com e-mail: ssc@cinesite.co.uk e-mail: artemis@cinesite.co.uk 
however, the initial images were noisy and the higher level structure was lost. We borrowed from concepts 
used at other studios [Petro­ vic et al. 2003] to manipulate the normal of the high frequency ge­ ometry 
in order to bring out the higher level shapes of the model. The lighters could then balance between the 
details and structure as each shot required. 3 Animation In an effort to maintain .exibility in our 
system, no matter how we generated the geometry, we kept a strict division between the gen­eration of 
the geometry and the animation. Despite the generation happening through a growth algorithm, it would 
be dif.cult to di­rect both visually pleasing geometry and an appropriate animation at the same time. 
Therefore, we chose to animate our geometry based on values created from a simple Dijkstra Algorithm 
[Dijkstra 1959]. This allowed us to completely change where the geometry grew from and groom the speed 
of animation, signi.cantly improv­ing our reaction time to director requests. Our Dijkstra implementation 
allowed us to select the starting point we wanted to grow from, and based upon the cost along each edge, 
determine at what time every line segment would animate. We could vary the speed of each curve segment 
to guide the growth into particular shapes, or even to produce subtle patterns in the leading edges. 
The speed at which all segments would grow was expressed in frames, and the Dijkstra algorithm would 
produce a rank for ev­ery vertex which de.ned what frames it was animating on. Those values were then 
used to de.ne when the geometry moved. If the values of the two vertices on a segment were greater than 
the cur­rent frame, the geometry was simply deleted. If they were less, the geometry was static. If the 
values straddled the current time, the frame was normalized between the values and then fed through a 
library of canned animation styles. These values were also passed through to rendering, and fed into 
various shading parameters to focus the eye on the leading edge of animation and blend back into the 
static volume. 4 Acknowledgements The full development of Thern happened with a great team build­ing 
upon each other s work. None of this would have been possible without the creativity, collaboration, 
and hard work of everyone in­volved. References DIJKSTRA, E. W. 1959. A note on two problems in con­nexion 
with graphs. Numerische Mathematik 1, 269 271. 10.1007/BF01386390. PALUBICKI,W., HOREL,K., LONGAY,S., 
RUNIONS,A., LANE, B., M . ECH,R., AND PRUSINKIEWICZ, P. 2009. Self-organizing tree models for image synthesis. 
In ACM SIGGRAPH 2009 pa­pers, ACM, New York, NY, USA, SIGGRAPH 09, 58:1 58:10. PETROVIC,L., HENNE,M., 
AND ANDERSON, J. 2003. Volu­metric methods for simulation and rendering of hair. Simulation, 06-08, 1 
6. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. 
ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2343075</section_id>
		<sort_key>300</sort_key>
		<section_seq_no>9</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Hairy scary]]></section_title>
		<section_page_from>9</section_page_from>
	<article_rec>
		<article_id>2343076</article_id>
		<sort_key>310</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>22</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Curls gone wild]]></title>
		<subtitle><![CDATA[hair simulation in <i>Brave</i>]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343076</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343076</url>
		<abstract>
			<par><![CDATA[<p>In <i>Brave</i>, Merida's hair is an important extension of her character. Like Merida, it is fierce, tempestuous, and unpredictable. We were tasked with the challenge of creating hair that possesses its own dramatic and expressive personality, but still appears realistic and physically convincing. To accomplish this, we engineered a hair system that produces believable, natural movement while providing simulation artists with tools to direct the hair's motion as the story demanded. In this talk, we will discuss the techniques we developed in production to utilize a novel hair model described in [1].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737725</person_id>
				<author_profile_id><![CDATA[81421598566]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Olivier]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Soares]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[oliv@pixar.com]]></email_address>
			</au>
			<au>
				<person_id>P3737726</person_id>
				<author_profile_id><![CDATA[81504682717]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Samantha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raja]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ksam@pixar.com]]></email_address>
			</au>
			<au>
				<person_id>P3737727</person_id>
				<author_profile_id><![CDATA[81504682662]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Rich]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hurrey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rhurrey@pixar.com]]></email_address>
			</au>
			<au>
				<person_id>P3737728</person_id>
				<author_profile_id><![CDATA[81100183579]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hayley]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iben]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[iben@pixar.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Iben, H., Meyer, M., Petrovic, L., Soares, O., Anderson, J. and Witkin, A. 2012. Artistic simulation of curly hair. Technical Memo 12-03a, Pixar Animation Studios.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Curls Gone Wild: Hair Simulation in Brave Olivier Soares Samantha Raja Rich Hurrey Hayley Iben* Pixar 
Animation Studios In Brave, Merida s hair is an important extension of her character. Like Merida, it 
is .erce, tempestuous, and unpredictable. We were tasked with the challenge of creating hair that possesses 
its own dramatic and expressive personality, but still appears realistic and physically convincing. To 
accomplish this, we engineered a hair system that produces believable, natural movement while providing 
simulation artists with tools to direct the hair s motion as the story demanded. In this talk, we will 
discuss the techniques we developed in production to utilize a novel hair model described in [1]. Figure 
1: Hair simulation on Merida and Angus in Brave. c &#38;#169;2012 Disney/Pixar. All rights reserved. 
1 The hair model The simulator discussed in [1] de.nes the stretching, bending, and twisting behavior 
of the curls. It describes a hair model that in­cludes core springs applied to core hairs generated with 
a low pass .lter on the original hair. This spring helps Merida s curls to main­tain their tightly coiled 
shape, but also allows for some stretching. The simulator uses non-linear springs to dynamically increase 
the stiffness of the core and bending springs when the core hair length exceeds a threshold. This approach 
prevents hair from unwind­ing during high speed motions without sacri.cing its softness and bounce in 
periods of less intense movement. The simulator handles interaction between nearby hairs by mod­elling 
static friction with spring forces that break after the particles exceed a velocity threshold. 2 Articulation 
Merida and Elinor s hair rigs are set up to allow posing of differ­ent regions of hair. These were used 
by animators to create shapes for simulation artists to use as reference for the .nal simulation. Merida 
s hair rig is based on a multi-bend deformer that facilitates * oliv,ksam,rhurrey,iben@pixar.com accurate 
posing of the hairs. It also contains controls for deforma­tions such as pinching. The hair rig on Elinor 
manipulates her two braids and deforms a tetrahedral mesh that is then simulated. 3 Controlling the 
uncontrollable One of our goals for Brave was to provide simulation artists with a complex hair simulation 
setup that both maintains stability and gives the hair a consistently lively feel throughout the .lm, 
regard­less of the characters actions. A signi.cant challenge in animated .lms is working with a solver 
that expects physically plausible movement as input, but receives exaggerated, physics-defying character 
animation instead. To ad­dress this, we developed a special IFG (Inertial Field Generator) working only 
in the up/down direction. This force virtually adds or removes vertical acceleration from the hair particles. 
We can tone down the effect of exaggerated vertical movement on the hair, as well as amplifying the effect 
when we want to see more bounce. To ensure that the hair returns to a reasonable shape even after heavy 
motion, we applied high bending stiffness at the root of each hair, then ramped it down to a lower value 
at the tip to allow the ends of the hair to move freely. The result is hair that is active and responsive 
to movement, but retains its original form. This strategy proved especially useful for Angus s tail. 
The animation on the tail nub in.uences the movement of the stiff upper region of the tail, but the rest 
of the hair is still able to swish around naturally. It was important to produce coherent movement in 
the hair without making it look like a solid mass. We did this by manipulating the behavior of the hair-to-hair 
contacts. Hairs on the outer shell of the hair volume are more likely to break apart, while hairs on 
the inner shell easily cling to other hairs and create stronger contact forces. As a result, the hair 
moves in a roughly uni.ed manner, but still contains enough .yaway hairs to look messy and natural. We 
also wanted to introduce subtle variations in movement across different hairs. For each hair, we generated 
a random value which we added to some key parameters of the solver, including mass and stiffness. While 
global parameters control the overall movement, the slight noise from one hair to the next gives each 
strand a unique motion that is different from its neighbors. External forces such as wind are key elements 
in creating a real­istic simulation. We found that our traditional wind models could not accurately represent 
the decrease in wind intensity as it passes through a thick, layered volume such as Merida s hair. We 
devel­oped a new kind of wind that takes into account the occlusion com­ing from characters, sets, and 
props, as well as the hair itself. We generate a signed distance function from the collision body meshes 
and the hair curves, and for each hair particle we ray-march against the wind direction to compute its 
degree of occlusion. References [1] IBEN, H., MEYER, M., PETROVIC, L., SOARES, O., AN-DERSON, J. AND 
WITKIN, A. 2012. Artistic simulation of curly hair. Technical Memo 12-03a, Pixar Animation Studios. Copyright 
is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343077</article_id>
		<sort_key>320</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>23</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[High fidelity facial hair capture]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343077</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343077</url>
		<abstract>
			<par><![CDATA[<p>Modeling human hair from photographs is a topic of ongoing interest to the graphics community. Yet, the literature is predominantly concerned with the hair volume on the scalp, and it remains difficult to capture digital characters with interesting facial hair. Recent stereo-vision-based facial capture systems (e.g. [Furukawa and Ponce 2010][Beeler et al. 2010]) are capable of capturing extremely fine facial detail from high resolution photographs, but any facial hair present on the subject is reconstructed as a blobby mass. Prior work in facial hair photo-modeling is based on learned priors and image cues [Herrera et al.], and does not reconstruct the individual hairs belonging uniquely to the subject. We propose a method for capturing the three dimensional shape of complex, multi-colored facial hair from a small number of photographs taken simultaneously under uniform illumination. The method produces a set of oriented hair particles, suitable for point-based rendering.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737729</person_id>
				<author_profile_id><![CDATA[81421597317]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Graham]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fyffe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[fyffe@ict.usc.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1778777</ref_obj_id>
				<ref_obj_pid>1833349</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Beeler, T., Bickel, B., Beardsley, P., Sumner, B., and Gross, M. 2010. High-quality single-shot capture of facial geometry. <i>ACM Trans. on Graphics (Proc. SIGGRAPH) 29</i>, 3, 40:1--40:9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>117688</ref_obj_id>
				<ref_obj_pid>117684</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Freeman, W. T., and Adelson, E. H. 1991. The design and use of steerable filters. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence 13</i>, 891--906.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1830134</ref_obj_id>
				<ref_obj_pid>1829895</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Furukawa, Y., and Ponce, J. 2010. Accurate, dense, and robust multiview stereopsis. <i>IEEE Trans. Pattern Anal. Mach. Intell. 32</i>, 8, 1362--1376.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Herrera, T. L., Zinke, A., Gmbh, G., Weber, A., and Vetter, T. Toward image-based facial hair modeling.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 High Fidelity Facial Hair Capture Graham Fyffe USC Institute for Creative Technologies *  Figure 1: 
Far left: A captured 3D facial model with hair particles. Pairs: Renderings (left) paired with reference 
photographs (right). 1 Introduction Modeling human hair from photographs is a topic of ongoing inter­est 
to the graphics community. Yet, the literature is predominantly concerned with the hair volume on the 
scalp, and it remains dif­.cult to capture digital characters with interesting facial hair. Re­cent stereo-vision-based 
facial capture systems (e.g. [Furukawa and Ponce 2010][Beeler et al. 2010]) are capable of capturing 
extremely .ne facial detail from high resolution photographs, but any facial hair present on the subject 
is reconstructed as a blobby mass. Prior work in facial hair photo-modeling is based on learned priors 
and image cues [Herrera et al. ], and does not reconstruct the individual hairs belonging uniquely to 
the subject. We propose a method for capturing the three dimensional shape of complex, multi-colored 
facial hair from a small number of photographs taken simultane­ously under uniform illumination. The 
method produces a set of oriented hair particles, suitable for point-based rendering. 2 Our Approach 
We base our approach on multi-view stereo using six high­resolution photographs (2600×3908 pixels) captured 
simultane­ously from six digital SLR cameras surrounding the face. At this resolution, individual facial 
hairs are imaged and may be detected in 2D using a separable steerable .lter quadrature pair. Reconstruc­tion 
begins with the method of [Furukawa and Ponce 2010], up to the point of Poisson meshing to construct 
a low-resolution base mesh. We resample the base mesh into a smooth .ne mesh, and solve for a vertex 
displacement map that minimizes a multi-view stereo cost function. To achieve consistent geometric smoothing 
across data sets and varitations in vertex spacing, we de.ne a novel smoothing term that is invariant 
to scene scale, and automatically adapts to different resolutions. We next derive a novel stereo match­ing 
cost function, which we call equalized cross correlation, that properly accounts for both camera shot 
noise and pixel sampling variance. This allows us to discriminate between reconstruction er­ror caused 
by non-smooth features (i.e. hairs) versus that caused by noise. It also eliminates any ad-hoc weights 
for the smoothing term. We reconstruct a color texture map with the facial hair re­moved, to provide 
a plausible skin texture to underly the facial hair. First we compute a texture map by blending the input 
pixel values from the different camera views. Second, we measure the quality of the texture map by inspecting 
the stereo reconstruction error. We *e-mail: fyffe@ict.usc.edu use the quality measure, combined with 
the steerable .lter line de­tection, as an indicator of the presence of hair, enabling the hair to be 
painted out automatically, leaving cleanly shaved skin. We then smooth and inset the geometry that was 
painted over, to reduce the roughness of the hair blob and serve as underlying skin. We con­struct a 
cloud of oriented 3D hair particles by analyzing the detected 2D line segments and the facial geometry. 
The intersection of any pair of 2D line segments from two different camera views is a 3D line segment, 
and we may place an oriented 3D hair particle at its center. If we were to compute all such hair particles 
without further constraints, we would end up with many false positives that are not part of actual hair. 
Thus we de.ne a hair volume that extends 1cm out from the skin, and construct only the hair particles 
that lie in­side this volume. We score the hair particles based on how many nearby 2D line segments in 
any view may be explained by the hair particle. A hair particle may be discarded if its color is similar 
to the underlying skin texture in all views. Any pixel with a detected 2D line segment in any view retains 
only the highest scoring hair particle constructed from the 2D line segment. Finally, we revisit the 
facial texture to replace any pixels that were originally painted over, but turned out not to be occluded 
by the recovered hair. This produces a complete facial model, with hair particles to explain the .ne 
parallax occuring in the facial hair, and a textured face mesh to explain the underlying color. We show 
results for a male subject with blonde / light brown hair, a male with short dark hair, and a female 
subject with dark hair in Figure 1. The facial geometry and hair particles are of high quality. References 
BEELER, T., BICKEL, B., BEARDSLEY, P., SUMNER, B., AND GROSS, M. 2010. High-quality single-shot capture 
of facial geometry. ACM Trans. on Graphics (Proc. SIGGRAPH) 29, 3, 40:1 40:9. FREEMAN, W. T., AND ADELSON, 
E. H. 1991. The design and use of steerable .lters. IEEE Transactions on Pattern Analysis and Machine 
Intelligence 13, 891 906. FURUKAWA, Y., AND PONCE, J. 2010. Accurate, dense, and ro­bust multiview stereopsis. 
IEEE Trans. Pattern Anal. Mach. In­tell. 32, 8, 1362 1376. HERRERA, T. L., ZINKE, A., GMBH, G., WEBER, 
A., AND VET-TER, T. Toward image-based facial hair modeling. Copyright is held by the author / owner(s). 
SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2343078</section_id>
		<sort_key>330</sort_key>
		<section_seq_no>10</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[PANDA: panoramas, displays and acquisition]]></section_title>
		<section_page_from>10</section_page_from>
	<article_rec>
		<article_id>2343079</article_id>
		<sort_key>340</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>24</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Building indoor multi-panorama experiences at scale]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343079</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343079</url>
		<abstract>
			<par><![CDATA[<p>Google Street View has provided millions of users with the ability to visually locate businesses around the world using 360&#176; panoramic imagery. Due to the bulky, custom hardware required to precisely geo-locate the imagery, such as laser scanners and high precision GPS devices, Street View experiences have been limited to large areas that can provide cost-effective collections. This has prevented users from discovering places such as the interiors of small businesses.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737730</person_id>
				<author_profile_id><![CDATA[81309494438]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Colbert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Google, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mcolbert@google.com]]></email_address>
			</au>
			<au>
				<person_id>P3737731</person_id>
				<author_profile_id><![CDATA[81100366629]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jean-Yves]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bouguet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Google, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jyb@google.com]]></email_address>
			</au>
			<au>
				<person_id>P3737732</person_id>
				<author_profile_id><![CDATA[81504685635]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jeff]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Beis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Google, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jeffbeis@google.com]]></email_address>
			</au>
			<au>
				<person_id>P3737733</person_id>
				<author_profile_id><![CDATA[81504686373]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Spudde]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Childs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Google, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[satkin@google.com]]></email_address>
			</au>
			<au>
				<person_id>P3737734</person_id>
				<author_profile_id><![CDATA[81464656341]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Filip]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Google, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[djfilip@google.com]]></email_address>
			</au>
			<au>
				<person_id>P3737735</person_id>
				<author_profile_id><![CDATA[81502733839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Luc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vincent]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Google, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[luc@google.com]]></email_address>
			</au>
			<au>
				<person_id>P3737736</person_id>
				<author_profile_id><![CDATA[81504685000]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Jongwoo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hanyang University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jlim@hanyang.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P3737737</person_id>
				<author_profile_id><![CDATA[81435594053]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Satkin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[satkin@cmu.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>861369</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hartley, R. I., and Zisserman, A. 2004. <i>Multiple View Geometry in Computer Vision</i>, second ed. Cambridge University Press, ISBN: 0521540518.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>290094</ref_obj_id>
				<ref_obj_pid>290091</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Zhang, Z., and Kanade, T. 1998. Determining the epipolar geometry and its uncertainty: A review. <i>International Journal of Computer Vision 27</i>, 161--195.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Building Indoor Multi-Panorama Experiences at Scale Mark Colbert, Jean-Yves Bouguet, Jeff Beis, Spudde 
Childs, Jongwoo Lim Scott Satkin Daniel Filip and Luc Vincent* Hanyang University Carnegie Mellon University 
Google, Inc.  Figure 1: Left is a screenshot from the panorama moderation tool. Right is a zoomed selection 
of the map illustrating how the high-con.dence links appear by shifting and re-optimizing a capture point. 
Note how the less likely links appear lighter and wider. 1 Introduction Google Street View has provided 
millions of users with the ability to visually locate businesses around the world using 360.panoramic 
imagery. Due to the bulky, custom hardware re­quired to precisely geo-locate the imagery, such as laser 
scanners and high precision GPS devices, Street View experiences have been limited to large areas that 
can provide cost-effective collections. This has prevented users from discovering places such as the 
in­teriors of small businesses. Google Business Photos attempts to solve this problem by allowing photographers 
with commodity DSLR cameras and .sheye lenses to capture panoramas of business interiors and publish 
the data onto Street View. Photographers collect the data and upload the raw im­agery to backend storage. 
From there, the data is stitched to build 360.panoramas. However, without ancillary capture devices, 
the images cannot be precisely geo-located. Thus, our method uses a combination of iterative vision-based 
pose estimation with user­guided optimization enabling photographers to build Street View­like experiences. 
 2 Iterative Vision-based Pose Estimation Vision-based pose estimation is a well-studied problem [2004]. 
In a typical solution, sparse SIFT-like features are extracted from the 360.panoramas and RANSAC is used 
to determine the best epipole, or direction of motion, between each pair of panoramas. These epipoles 
provide the fundamental navigation mode in Street View and are represented by the chevrons that link 
the imagery. To precisely geo-locate panoramas, the images are grouped into clus­ters in which the poses 
of panoramas and the feature points are fully reconstructed. The panorama pair containing the largest 
number of matching sparse features from RANSAC is chosen as a cluster seed. Then, the next best matching 
panorama to the reconstructed feature points in the cluster is added. This is repeated until all panoramas 
containing sparse matches above a threshold can be added, after which a new cluster is initialized and 
loosely connected to an ex­isting cluster using a low-con.dence match. Finally, the locations of the 
panoramas on the map are determined and the connectivity graph of the panoramas is built. 3 User-guided 
Optimization While the vision-based pose optimization builds a statistically op­timal solution, the underlying 
result may still be inaccurate or even *email: {mcolbert,jwlim,satkin,jyb,jeffbeis,dj.lip,luc}@google.com 
email: jlim@hanyang.ac.kr email: satkin@cmu.edu completely misplaced [1998]. This can arise due to a 
variety of er­ rors, such as too many false matches from a repeating pattern on a wall or .oor, mirrors 
in the scene, or too few salient feature points. User input is then required to guide the remaining components 
to build a pose that faithfully reproduces the original geometry. A map-like display is presented to 
the user, who is able to manipu­late the 3 free parameters for each capture point (latitude, longitude, 
yaw). In order to aid the user in quickly deciding epipole validity, a visualization mechanism was added 
indicating the likelihood of a particular estimate to be correct. Unfortunately, it was found that the 
standard RANSAC score based on inliers did not yield enough useful information. As an alternative, we 
pick two feature matches, compute the relative planar motion between them, and bin the esti­mated epipoles 
into discretized yaw values. These histograms typ­ically .t well to a normal distribution whose standard 
deviation is inversely correlated to the certainty of an epipole plus a uniform distribution to measure 
the noise. This visualization provides a useful guide to assist users in pre­cisely geo-locating each 
image. However, for simplicity, the user must also be able to adjust subsets of the pose. For instance, 
if a room was misplaced, the user should roughly move the incorrectly placed data and then re-optimize 
only the manipulated section. To limit the data size for JavaScript processing, the statistics computed 
for visualization are used to ef.ciently optimize subsets of the ge­ometry using gradient descent on 
the client. 4 Conclusion The methods and tools presented here have provided photographers the functionality 
to push thousands of businesses and other loca­tions around the world to Street View. For an average 
collection of 12 panoramas, the vision-pose estimation typically runs in under 4 minutes and the human-based 
pose correction takes 7 minutes. The data from this toolchain provides users a novel virtual perspective 
on many small and medium sized businesses that has previously been unseen, especially for locations that 
are not easily visible from a driveable street. References HARTLEY, R. I., AND ZISSERMAN, A. 2004. Multiple 
View Geometry in Computer Vision, second ed. Cambridge University Press, ISBN: 0521540518. ZHANG, Z., 
AND KANADE, T. 1998. Determining the epipolar geometry and its uncertainty: A review. International Journal 
of Computer Vision 27, 161 195. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, 
California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343080</article_id>
		<sort_key>350</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>25</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Panorama light-field imaging]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343080</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343080</url>
		<abstract>
			<par><![CDATA[<p>We present a first approach towards panorama light-field imaging. By converting overlapping sub-light-fields into individual focal stacks, computing a panoramic focal stack from them, and converting the panoramic focal stack back into a panoramic light field, we avoid the demand for a precise reconstruction of scene depth.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[computational photography]]></kw>
			<kw><![CDATA[light fields]]></kw>
			<kw><![CDATA[panorama]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P3737738</person_id>
				<author_profile_id><![CDATA[81488672490]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Clemens]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Birklbauer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute of Computer Graphics, Johannes Kepler University Linz, Austria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[clemens.birklbauer@jku.at]]></email_address>
			</au>
			<au>
				<person_id>P3737739</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute of Computer Graphics, Johannes Kepler University Linz, Austria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[oliver.bimber@jku.at]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Levin, A., and Durand, F. 2010. Linear view synthesis using a dimensionality gap light field prior. In <i>Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</i>, 1831--1838.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Panorama Light-Field Imaging Clemens Birklbauer and Oliver Bimber* Institute of Computer Graphics, Johannes 
Kepler University Linz, Austria  Figure 1: Images rendered from a 22 MPixels (spatial resolution: 17885x1275 
pixels) panoramic light .eld at different focus settings (far: top, near: center), and close-ups in native 
resolution (bottom). We thank the Raytrix GmbH for capturing and providing the raw data. Abstract We 
present a .rst approach towards panorama light-.eld imag­ing. By converting overlapping sub-light-.elds 
into individual focal stacks, computing a panoramic focal stack from them, and convert­ing the panoramic 
focal stack back into a panoramic light .eld, we avoid the demand for a precise reconstruction of scene 
depth. CR Categories: I.4.1 [IMAGE PROCESSING AND COMPUTER VISION]: Digitization and Image Cap-ture;I.3.3COMPUTER 
GRAPHICSPicture/Image Generation Keywords: light .elds, computational photography, panorama 1 Introduction 
With increasing resolution of imaging sensors, light-.eld photog­raphy is now becoming increasingly practical, 
and .rst light-.eld cameras are already commercially available (e.g., Lytro, Raytrix, and others). Applying 
common digital image processing techniques to light-.elds, however, is in many cases not straight forward. 
The reason for this is, that the outcome must not only be spatially con­sistent, but also directionally 
consistent. Otherwise, refocussing and perspective changes will cause strong image artifacts. Panorama 
imaging techniques, for example, are an integral part of digital photography often being supported by 
camera hard­ware today. We present a .rst approach towards the construction *e-mail: {clemens.birklbauer,oliver.bimber}@jku.at 
of panoramic light-.elds (i.e., large .eld-of-view light-.elds com­puted from overlapping sub-light-.eld 
recordings). 2 Our Approach We capture overlapping sub-light-.elds of a scene during a cylindri­cal 
or spherical camera motion and convert each sub-light .eld into a focal stack using synthetic aperture 
reconstruction. For light-.eld cameras that directly deliver a focal stack, this step is not neces­sary. 
Next, we compute an all-in-focus image for each focal stack by extracting and composing the highest-frequency 
image content throughout all focal stack slices. The registration and blending pa­rameters are then computed 
for the resulting (overlapping) all-in­focus images. For this, we apply conventional panorama stitch­ing 
techniques, such as SURF feature extraction, pairwise feature matching and RANSAC outlier detection, 
bundle adjustment, wave correction, exposure compensation, and the computation of blend­ing seams. The 
registration and blending parameters that were de­rived for the all-in-focus images are now applied to 
all correspond­ing slices of the focal stacks. We use a four dimensional (three rotation and focal length) 
motion model for registration and multi­band blending for composition. The result is a registered and 
seam­lessly blended panoramic focal stack that can be converted to a light .eld with linear view synthesis, 
as described in [Levin and Durand 2010]. Since we chose an intermediate focal stack representation, common 
image panorama techniques can be applied for robustly computing a panoramic light-.eld without a precise 
reconstruction of scene depth. However, the focal stack of a scene covers no more than a 3D subset of 
the full 4D light .eld. This limits our current ap­proach to Lambertian scenes with modest depth discontinuities. 
In future, we will investigate panorama light-.eld imaging techniques that are applicable directly to 
4D ray-space.  References LEVIN, A., AND DURAND, F. 2010. Linear view synthesis using a dimensionality 
gap light .eld prior. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, 1831 
1838. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 
2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343081</article_id>
		<sort_key>360</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>26</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[A colloidal display]]></title>
		<subtitle><![CDATA[membrane screen that combines transparency, BRDF and 3D volume]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343081</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343081</url>
		<abstract>
			<par><![CDATA[<p>It is a common knowledge that the surface of soap bubble is a micro membrane. It allows light to pass through and displays the color on its structure. We developed an ultra thin and flexible BRDF screen using the mixture of two colloidal liquids. There have been several researches on dynamic BRDF display[1] in the past. However, our work is different in several points. Our membrane screen can be controlled using ultrasonic vibrations. Membrane can change its transparency and surface states depending on the scales of ultrasonic waves. Based on these facts, we developed several applications of the membranes such as 3D volume screen.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737740</person_id>
				<author_profile_id><![CDATA[81466641621]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ochiai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[the University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ochyai@me.com]]></email_address>
			</au>
			<au>
				<person_id>P3737741</person_id>
				<author_profile_id><![CDATA[81504682972]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alexis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Oyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737742</person_id>
				<author_profile_id><![CDATA[81482649155]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Keisuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Toyoshima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M. Hullin et al., Dynamic Display of BRDFs. In: Proceedings of Eurographics 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Colloidal Display:membrane screen that combines transparency, BRDF and 3D volume Yoichi Ochiai* Alexis 
Oyama** Keisuke Toyoshima*** *the University of Tokyo **Carnegie Mellon University ***University of Tsukuba 
 Figure 1: (top-left)controllable transparency, (top-center) sardine s back texture (top-right) wood 
texture on membrane screen (bottom-left) plane based 3D screen (bottom-center) metal texture on screen 
(bottom-right) piercing the object into the screenIt s very dif.cult to take the picture as we see this 
display. The re.ection of projector light is dif.cult to capture in cameras. 1. Introduction It is a 
common knowledge that the surface of soap bubble is a micro membrane. It allows light to passthrough 
and displays the color on its structure. We de­veloped an ultra thin and .exible BRDF screen using themixture 
of two colloidal liquids. There have been severalresearches on dynamic BRDF display[1] in the past. However, 
our work is different in several points. Our membrane screen can be controlled using ultrasonic vi­brations. 
Membrane can change its transparency and surface states depending on the scales of ultrasonic waves. 
Based on these facts, we developed several ap­plications of the membranes such as 3D volume screen. The 
combination of the ultrasonic waves and ultra thin membranes makes more realistic, distinctive, and vivid 
imageries on screen. This system contributes to open up a new path for display engineering with sharp 
imageries, transparency, BRDF and .exibility. 2. Design We developed the .rst prototype by using soap 
and milk. These are colloidal liquids and their moleculeshave different sizes and colors. With ultrasonic 
paramet­ric speakers, we could control their movements (liquidsand particles) on the membranes. If they 
move with in­tensity, the re.ections change while the membrane works as a projector screen. Since we 
can control thedynamics of the wavelengths, the state of the surface can be easily changed.    0-4000Hz 
Ultrasonic modulated by 50kHz Light membrane surface -40° to 40° display transparency transparency With 
the parametric speakers, this system can make thescreens drastically thinner since it does not need anyadditional 
systems or materials on the screen. The thin­ness of the screen is approximately 1 micrometer. Sincewe 
could control the state of the surface, we were able to have several interactions such as piercing a 
.nger through it or enlarging it by using its elasticity and .exi­bility. 3. Application First we developed 
a screen for displaying realisticmaterial. The display s state changes in correspondenceto the images 
the projector shows. Secondly, we developed the plane based 3D screen with three membranes using a single 
projector. (chang­ing frequency 25Hz) The transparency of each mem­brane is controllable by frequency 
of the sound from theparametric speakers. We set the projector and linked it with the transparency of 
each membrane. In addition, we developed polyhedrons made fromthese membranes and displayed several images 
on it. This system shows that this is useful to .exibly display 3D objects. 4. Future Work We introduced 
the .rst prototype of a new kind of dis­play by using colloidal liquids and a method of control­ling 
them by using ultrasonic waves. Due to its thinnessand transparency, the method could be applied to a 
vari­ety of cases in using display technologies. The 3Ds screen and texture screen are exemplary applications 
of this system. Currently our membrane can maintain its surface (screen) for 5 minutes. However, there 
are several solu­tions for this problem and there are rooms for new po­tential colloidal materials for 
the use. REFERENCES [1] M. Hullin et al., Dynamic Display of BRDFs. In: Proceed­ings of Eurographics 
2011. Figure2: ultrasonic &#38; membrane Copyright is held by the author / owner(s). SIGGRAPH 2012, 
Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 *email: ochyai@me.com 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343082</article_id>
		<sort_key>370</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>27</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[CoDAC]]></title>
		<subtitle><![CDATA[compressive depth acquisition using a single time-resolved sensor]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343082</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343082</url>
		<abstract>
			<par><![CDATA[<p>CoDAC is a method for compressive acquisition of scene depth with high spatial and range resolution using a single, omnidirectional, time-resolved photodetector and no scanning components [Kirmani et al.]. Light detection and ranging (LIDAR) systems use time of flight (TOF) in combination with raster scanning of the scene to form depth maps, and TOF cameras instead make TOF measurements in parallel by using an array of sensors. Moreover, existing depth sensing technologies do not use the high compressibility of scene depth to reduce acquisition costs. Here, we present a framework for compressive depth map acquisition using neither raster scanning by the illumination source nor an array of sensors.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737743</person_id>
				<author_profile_id><![CDATA[81466648528]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrea]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cola&#231;o]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[acolaco@mit.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737744</person_id>
				<author_profile_id><![CDATA[81488667605]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ahmed]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kirmani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737745</person_id>
				<author_profile_id><![CDATA[81542012156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Franco]]></first_name>
				<middle_name><![CDATA[N. C.]]></middle_name>
				<last_name><![CDATA[Wong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737746</person_id>
				<author_profile_id><![CDATA[81504684911]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Vivek]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Goyal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kirmani, A., Cola&#231;o, A., Wong, F., and Goyal, V. Exploiting sparsity in time-of-flight range acquisition using a single time-resolved sensor. <i>OSA Opt. Express</i> (Oct 2011).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 CoDAC: Compressive Depth Acquisition using a Single Time-resolved Sensor Andrea Colac¸o *, Ahmed Kirmani, 
Franco N. C. Wong, Vivek K Goyal Massachusetts Institute of Technology Figure 1: CoDAC depth sensing 
architecture uses a spatial light modulator (SLM) to spatially pattern a temporally-modulated light source. 
In step 1, time-resolved measurements from a single omnidirectional sensor are processed using parametric 
signal processing to recover scene depth information. We estimate the three distinct depths in this fronto-parallel 
scene with sub-centimeter resolution even with pulse width of 3ns (rise-time 0.7 ns). Spatial resolution 
was recovered through step 2 of the algorithm using only 205 SLM patterns. This corresponds to 5% of 
the total number of 4096 pixels in the depth map. 1 Introduction CoDAC is a method for compressive acquisition 
of scene depth with high spatial and range resolution using a single, omnidi­rectional, time-resolved 
photodetector and no scanning compo­nents [Kirmani et al. ]. Light detection and ranging (LIDAR) sys­ 
tems use time of .ight (TOF) in combination with raster scanning of the scene to form depth maps, and 
TOF cameras instead make TOF measurements in parallel by using an array of sensors. Moreover, existing 
depth sensing technologies do not use the high compress­ibility of scene depth to reduce acquisition 
costs. Here, we present a framework for compressive depth map acquisition using neither raster scanning 
by the illumination source nor an array of sensors. Our depth map reconstruction relies on parametric 
signal modeling of the impulse response of piecewise planar scenes. We use para­metric deconvolution 
to achieve much .ner depth resolution than dictated by the illumination pulse width and the detector 
bandwidth. Spatial resolution in our framework is rooted in patterned illumina­tion followed by decoupling 
the inverse problems of range estima­tion and spatial resolution recovery during computational process­ing. 
Spatial resolution equal to that of the spatial light modulator (SLM) is achieved despite using fewer 
SLM patterns than the num­ber of pixels in the SLM. Proof-of-concept experiments have veri­.ed the validity 
of our modeling, algorithms and improved spatial and range resolution over existing methods. CoDAC enables 
depth acquisition in a compact form factor, with signi.cantly-reduced hardware cost and complexity as 
compared to state-of-the-art LI-DAR systems and TOF cameras. 2 Modeling and Algorithmic Framework CoDAC 
has a two-step reconstruction procedure. Step 1, uses omnidirectional illumination or no spatial patterning, 
i.e., a fully­transparent SLM con.guration. Under the assumption that the scene is approximately piecewise 
planar, we have shown that the continuous-time light intensity signal at the single photodetector is 
well approximated by a piecewise linear signal [Kirmani et al. ]. For fronto-parallel scenes, this parametric 
signal is simply a series of short square pulses as shown in Figure 1. Estimation of the un­ derlying 
parametric signal P (t) implies recovery of the range or *email:acolaco@mit.edu depth content present 
in the scene. The use of a parametric sig­nal modeling and recovery framework enables us to achieve high 
depth resolution relative to the speed of the time sampling and the photodetector bandwidth. After depth 
identi.cation in step 1, the remaining problem is to .nd correspondences between spatial loca­tions and 
depths to form the depth map. Step 2, uses several pseudorandom binary patterns on the SLM. For each 
pattern, the received time-resolved signal is processed to yield amplitude data. Again for fronto-parallel 
scenes, the ampli­tude of each peak in the parametric signal is equal to the fraction of the scene at 
a particular depth that is illuminated by the pro­jected pattern. Since we assume that the scene is approximately 
piecewise planar, this translates to the Laplacian of the depth map being approximately sparse. We introduce 
a convex optimization problem [Kirmani et al. ] that .nds the depth map consistent with the measurements 
that approximately minimizes the number of nonzero entries in the Laplacian of the depth map. Solving 
this optimization problem yields the desired depth map. 3 Hardware Implementation We conducted proof-of-concept 
experiments to demonstrate the range and spatial resolution capabilities of the CoDAC framework. We used 
a pulsed laser source at 780nm and 70mW average power to illuminate the SLM with 64 × 64-pixel resolution. 
The re.ected light was collected at a Si PIN photodiode with rise-time of 0.7ns. The recovered depth 
maps for a fronto-parallel scene is shown in Figure 1 and for piecewise planar scenes is shown below. 
  References KIRMANI, A., COLAC¸ O, A., WONG, F., AND GOYAL, V. Ex­ploiting sparsity in time-of-.ight 
range acquisition using a single time-resolved sensor. OSA Opt. Express (Oct 2011). Copyright is held 
by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2343083</section_id>
		<sort_key>380</sort_key>
		<section_seq_no>11</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Silence! Eliminate the noise]]></section_title>
		<section_page_from>11</section_page_from>
	<article_rec>
		<article_id>2343084</article_id>
		<sort_key>390</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>28</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[ISHair]]></title>
		<subtitle><![CDATA[importance sampling for hair scattering]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343084</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343084</url>
		<abstract>
			<par><![CDATA[<p>We present an importance sampling method for the bidirectional scattering distribution function (<i>bsdf</i>) of hair, based on the multi-lobe hair scattering model presented in [Sadeghi et al. 2010]. Our algorithm is efficient, easy to implement and it has no significant memory overhead or need for precomputation. We have integrated our method into both a research raytracer and a micropolygon based production renderer. Figure 1 compares the rendering quality of our method to statified uniform sampling for both direct (environment) lighting rendered with our production renderer and indirect lighting rendered with path tracing. In both cases, our method delivers significantly better image quality than uniform sampling using the same number of samples.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737747</person_id>
				<author_profile_id><![CDATA[81490659114]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jiawei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation and Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737748</person_id>
				<author_profile_id><![CDATA[81504684267]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Feng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737749</person_id>
				<author_profile_id><![CDATA[81504687533]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Parashar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Krishnamachari]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737750</person_id>
				<author_profile_id><![CDATA[81100112064]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Fabio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pellacini]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College and Sapienza University of Rome]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1778793</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Sadeghi, I., Pritchett, H., Jensen, H. W., and Tamstorf, R. 2010. An artist friendly hair shading system. <i>ACM Transactions on Graphics 29</i>, 4 (July), 56:1--56:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ISHair: Importance Sampling for Hair Scattering Jiawei Ou1,2 Feng Xie1 Parashar Krishnamachari1 Fabio 
Pellacini2,3 1DreamWorks Animation 2Dartmouth College 3Sapienza University of Rome We present an importance 
sampling method for the bidirectional scattering distribution function (bsdf ) of hair, based on the 
multi­lobe hair scattering model presented in [Sadeghi et al. 2010]. Our algorithm is ef.cient, easy 
to implement and it has no signi.cant memory overhead or need for precomputation. We have integrated 
our method into both a research raytracer and a micropolygon based production renderer. Figure 1 compares 
the rendering quality of our method to stati.ed uniform sampling for both direct (environment) lighting 
rendered with our production renderer and indirect light­ing rendered with path tracing. In both cases, 
our method delivers signi.cantly better image quality than uniform sampling using the same number of 
samples.  Figure 1: Comparison of uniform sampling and our importance sampling method. (a). Global Illumination. 
(b). Direct Lighting. Hair Importance Sampling Sadeghi et al. [2010] proposed an artist-friendly hair 
shad­ing model, where they decomposed the hair scattering function S(.i,fi,.r,fr) into four components: 
re.ection (R), refractive transmission (TT), secondary re.ection without glint (TRT-g) and Glint (g); 
each lobe was further factored as a product of a longitu­dinal term M and an azimuthal term N: S(.i,fi,.r,fr)= 
IRMRNR/ cos 2 .d + ITTMTTNTT/ cos 2 .d 22 + ITRTMTRTNTRT-g/ cos .d + ITRTMTRTIgNg/ cos .d For importance 
sampling, we want to sample the incoming direc­tion .i with p(.i) . S(.i,fi,.r,fr) where .i and fi are 
the longitudinal and azimuthal components of .i. Since the longitu­dinal and azimuthal terms are independent, 
we can sample .i and fi separately then convert them into .i. The pdf of each sam­ple is p(.i)= p(.i)p(fi). 
Importance sampling the hair scat­tering model is challenging because all the longitudinal terms are 
Gaussian functions of the longitudinal half angle and the azimuthal transmission and Glint terms are 
Gaussian functions of the rela­tive azimuthal angle. Because Gaussians do not have a closed form anti-derivative, 
direct application of the inverse cdf sampling tech­nique is nontrivial. To overcome this challenge, 
we approximate the Gaussian using Cauchy distribution 1 . f(., x - x0)= p(x - x0)2 + .2 , a bell shaped 
function with a closed form anti-derivative: P (x)= an 1 -1 x-x0 tan. p. Sampling the individual lobes 
For the longitudinal terms de­.ned as Mx = g(ßx 2,ax,.h), x = {R, TT, TRT} where ßx and ax are the variance 
and mean of the Gaussian; using Results Figure 2 demonstrates our importance sampling algorithm is quite 
effective at distributing more samples towards regions with higher energy. This is also veri.ed by rendering 
comparison between en­vironment lighting, area lighting and bounce lighting of hair bsdf using strati.ed 
uniform sampling versus our importance sampling algorithm. In most of the directing lighting scenarios, 
our impor­tance sampling algorithm yields better quality than uniform sam­pling with 4x number of samples. 
The improvement is even more signi.cant in renders with multiple scattering and indirect lighting. a. 
Uniform Sampling b. Importance Sampling Figure 2: Comparison of samples distributed using (a) uniform 
and (b) importance sampling. References SADEGHI, I., PRITCHETT, H., JENSEN, H. W., AND TAMSTORF, R. 
2010. An artist friendly hair shading system. ACM Transac­tions on Graphics 29, 4 (July), 56:1 56:10. 
Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. 
ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343085</article_id>
		<sort_key>400</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>29</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Noise reduction for progressive photon mapping]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343085</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343085</url>
		<abstract>
			<par><![CDATA[<p>We present an adaptive noise reduction technique for progressive photon mapping, which preserves the consistency of the original algorithm. Although progressive photon mapping converges to the correct result in the limit, the image produced after a finite number of samples can suffer from both bias and noise [Hachisuka et al. 2010]. The bias can be seen as blurry lighting features while the noise can be seen as splotches.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737751</person_id>
				<author_profile_id><![CDATA[81504682735]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zhe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California at San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737752</person_id>
				<author_profile_id><![CDATA[81100640205]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Henrik]]></first_name>
				<middle_name><![CDATA[Wann]]></middle_name>
				<last_name><![CDATA[Jensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California at San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1866170</ref_obj_id>
				<ref_obj_pid>1882261</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hachisuka, T., Jarosz, W., and Jensen, H. W. 2010. A progressive error estimation framework for photon density estimation. <i>ACM Trans. Graph. 29</i>, 6 (Dec.), 144:1--144:12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Noise Reduction for Progressive Photon Mapping Zhe Fu Henrik Wann Jensen University of California at 
San Diego  (a) Cornell box by PPM (b) Cornell box by our extension (c) 1D visualization of samples along 
the line on .oor Figure 1: A scene of cornell box and a scene of a diffuse surface occluded by a small 
ceiling rendered by progressive photon mapping and our method. We use 10 million photons in 100 passes 
to render the two scenes. 1 Introduction and Motivation We present an adaptive noise reduction technique 
for progressive photon mapping, which preserves the consistency of the original algorithm. Although progressive 
photon mapping converges to the correct result in the limit, the image produced after a .nite number 
of samples can suffer from both bias and noise [Hachisuka et al. 2010]. The bias can be seen as blurry 
lighting features while the noise can be seen as splotches. Progressive photon mapping is primarily controlled 
by one param­eter a that determines the rate of the radius reduction around the sample points in the 
scene. A large a leads to a higher acceptance of incoming photons with a slower radius reduction, while 
a small a accepts fewer photons and reduces the radius faster. This effec­tively means that a can be 
used to adjust the amount of bias and noise present in the scene. A larger value of a results in more 
bias while a smaller value causes more noise. The original progressive photon mapping algorithm uses 
one a value for the entire scene, which means a nearly uniform radius reduction across the scene, and 
consequently a similar amount of smoothing and noise. In this paper we propose to adjust a adaptively 
across the scene to get a better balance between bias and noise in the rendered images. Intuitively, 
in some region of the scene where lighting condition changes slightly or gradually, we do not need to 
reduce the radius aggressively. Otherwise, these surfaces may end up with high fre­quency noise. On the 
other hand, details should be preserved by us­ing smaller radii at locations where light intensity changes 
rapidly, such as shadow boundaries. 2 Adaptive a adjustment Our goal is to pick a value for a that results 
in larger radii in re­gions with smooth lighting and a smaller radii in regions with rapid lighting changes. 
Hachisuka et al. [2010] showed how bias can be estimated using the laplacian of the radiance. Unfortunately, 
the laplacian itself is quite noisy and we found that using just the gra­dient of the radiance leads 
to a signi.cantly more robust estimate of the rate on how lighting changes locally. Based on this obser­vation, 
we have derived an empirical formula for adjusting a that uses a variable, s = I\L(x, w.)I/IL(x, w.)I, 
to indicate the rela­tive light intensity changes. Note, that the gradient is estimated as a by-product 
in kernel based progressive photon mapping . Given s we compute a local a value for each sample location 
as: a(x)= 1 (1) ß(s-µ) 1+ e where the S-shaped sigmoid function maps a small s to a large a, which in 
turn results in slow radius reduction and a smooth appear­ance. It does the opposite for a large s to 
preserve lighting details. It also ensures that a is between 0 and 1 in order to satisfy the conver­gence 
condition of progressive photon mapping. The two parame­ters µ and ß in Equation 1 control the center 
of the sigmoid function and how steep it is, respectively. We have found that µ can be com­puted automatically 
as the 90th percentile of s of all measurement points. In other words, points with 10% largest s are 
considered to hold scene details, and their radii are reduced more aggressively. ß is a new user-speci.ed 
parameter instead of a, which serves as the effectiveness of our extension. We use ß =0.7 in our results. 
The extreme case with ß =0 is a standard progressive photon mapping with a =0.5. Figure 1 shows our improvements 
over the original algorithm in two example scenes. Comparing the close-up in red boxes, our method removes 
most high frequency noise on the walls or .oors. On the other hand, it still preserves shadow boundaries, 
corners, and caustics as illustrated in the green boxes. Since we greedily lower the radius reduction 
rate, it is roughly 30% longer to render the images compared with standard progressive photon mapping. 
 References HACHISUKA, T., JAROSZ, W., AND JENSEN, H. W. 2010. A progressive error estimation framework 
for photon density esti­mation. ACM Trans. Graph. 29, 6 (Dec.), 144:1 144:12. Copyright is held by the 
author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343086</article_id>
		<sort_key>410</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>30</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Progressive volume photon tracing]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343086</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343086</url>
		<abstract>
			<par><![CDATA[<p>Over the past decade, photon mapping is being more and more used to simulate light transport in scenes containing participating media. However, photon mapping is limited practically because of its large memory requirements. Progressive photon mapping addresses this problem [Hachisuka et al. 2008; Knaus and Zwicker 2011]. This approach computes many images of the same scene from the same viewpoint using different sets of photons and different parameter values. The obtained low quality images, when combined together, generate images of higher quality. In this talk, we propose a volume photon mapping method which does not have to store the photon maps, ensures a faster convergence to the final solution, and allows the user to have a full control of the size of the used memory. We assign a beam to each primary or secondary ray crossing a participating medium, then build a beam hierarchy. As soon as the computation of an emmited photon's contribution to a beam or to a visible point is done, we throw away this photon. In this way there is no need to store any photon map, which solves the problem of memory storage.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737753</person_id>
				<author_profile_id><![CDATA[81504682406]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Charly]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Collin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[charly.collin@bobbyblues.com]]></email_address>
			</au>
			<au>
				<person_id>P3737754</person_id>
				<author_profile_id><![CDATA[81485655819]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mickael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ribardi&#232;re]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IRISA Rennes, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mickael.ribardiere@irisa.fr]]></email_address>
			</au>
			<au>
				<person_id>P3737755</person_id>
				<author_profile_id><![CDATA[81100617754]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Remi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cozot]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IRISA Rennes, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[remi.cozot@irisa.fr]]></email_address>
			</au>
			<au>
				<person_id>P3737756</person_id>
				<author_profile_id><![CDATA[81100332780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kadi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bouatouch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IRISA Rennes, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kadi.bouatouch@irisa.fr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1409083</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hachisuka, T., Ogaki, S., and Jensen, H. W. 2008. Progressive photon mapping. <i>ACM Trans. Graph. 27</i> (December), 130:1--130:8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Havran, V., Bittner, J., Herzog, R., and Seidel, H.-P. 2005. Ray Maps for Global Illumination. 43--54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1966404</ref_obj_id>
				<ref_obj_pid>1966394</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Knaus, C., and Zwicker, M. 2011. Progressive photon mapping: A probabilistic approach. <i>ACM Trans. Graph. 30</i> (May), 25:1--25:13.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Progressive volume photon tracing Charly Collin* Micka¨ere R´, el Ribardi`, emi Cozot Kadi Bouatouch§ 
University of Central Florida, USA IRISA Rennes, France IRISA Rennes, France IRISA Rennes, France Figure 
1: Final results of the two scenes. The chart on the right compares the cumulated rendering time (in 
seconds) over the iterations using our method (red) with that of the method entitled Progressive Photon 
Mapping: A Probabilistic Approach. Over the past decade, photon mapping is being more and more used to 
simulate light transport in scenes containing participating media. However, photon mapping is limited 
practically because of its large memory requirements. Progressive photon mapping addresses this problem 
[Hachisuka et al. 2008; Knaus and Zwicker 2011]. This approach computes many images of the same scene 
from the same viewpoint using different sets of photons and different parameter values. The obtained 
low quality images, when combined together, generate images of higher quality. In this talk, we propose 
a vol­ume photon mapping method which does not have to store the pho­ton maps, ensures a faster convergence 
to the .nal solution, and allows the user to have a full control of the size of the used mem­ory. We 
assign a beam to each primary or secondary ray crossing a participating medium, then build a beam hierarchy. 
As soon as the computation of an emmited photon s contribution to a beam or to a visible point is done, 
we throw away this photon. In this way there is no need to store any photon map, which solves the problem 
of memory storage. 1 Overview Our algorithm consists of two steps: a preprocessing step followed by a 
progressive photon tracing step. In the rest of the talk, each iteration of the progressive photon tracing 
step is called progressive pass. Preprocessing. Each view ray, cast from the camera through a par­ticipating 
medium, is assigned one beam, which is a cylinder rep­resenting the portion of the ray within the medium. 
The secondary rays (re.ected and refracted) are also assigned beams. All these beams are used to build 
a Kd tree beam hierarchy similar to [Havran et al. 2005]. Besides, the visible points resulting from 
the intersec­ tion of the view rays with the scene s surfaces are also stored in a Kd-tree (called surface 
Kd-tree) [Hachisuka et al. 2008]. Finally, for each pixel, the visible point as well as the beam associated 
with the corresponding view ray are stored. Each photon may contribute to neighboring visible points 
and to the beams (leaves of the beam Kd-tree) containing the photon. These beams are determined by a 
top-down traversal of the beam hierarchy. For each visible point and each beam, the current radiance 
is stored along with a temporary *e-mail: charly.collin@bobbyblues.com e-mail: mickael.ribardiere@irisa.fr 
e-mail: remi.cozot@irisa.fr §e-mail: kadi.bouatouch@irisa.fr variable to store the contributions of 
the photons cast during a pass of the progressive photon mapping. Note that only the two Kd-trees have 
to be stored during the progressive photon mapping process, which makes our approach less demanding in 
terms of memory for medium size scenes. Sending photons. Photons are emitted using classical photon based 
methods. Once a cast photon contributes to a visible point or a beam, it is thrown away. This contribution 
is added to the tempo­rary radiance of the neighboring visible points or the beam con­taining the photon. 
The cumulated radiance is updated whenever a progressive pass is completed. The radius of a visible point 
neigh­borhood must be updated as well by decreasing it after each pro­gressive pass, making the algorithm 
focus on the less updated parts of the scene. 2 Early results So far we have tested our method on simple 
scenes. The .rst one consists of a heterogeneous smoke in a box, and the second contains a diffuse dragon 
inside a heterogeneous smoke. To evaluate our method on these scenes, we compared it with [Knaus and 
Zwicker 2011]. These early results show that our method is faster to produce images of same quality. 
(see .gure 1). 3 Conclusion We proposed a new approach to progressive photon mapping allow­ing a constant 
memory cost and a faster rendering. The .rst version of the implementation provided results with quality 
similar to ex­isting methods and seems very promising. In the future, we would like to test a fully functional 
algorithm on more complex scenes. We would also like to explore other aspects of our method such as parallelism. 
 References HACHISUKA, T., OGAKI, S., AND JENSEN, H. W. 2008. Pro­gressive photon mapping. ACM Trans. 
Graph. 27 (December), 130:1 130:8. HAVRAN, V., BITTNER, J., HERZOG, R., AND SEIDEL, H.-P. 2005. Ray Maps 
for Global Illumination. 43 54. KNAUS, C., AND ZWICKER, M. 2011. Progressive photon map­ping: A probabilistic 
approach. ACM Trans. Graph. 30 (May), 25:1 25:13. Copyright is held by the author / owner(s). SIGGRAPH 
2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343087</article_id>
		<sort_key>420</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>31</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Volume-aware extinction mapping]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343087</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343087</url>
		<abstract>
			<par><![CDATA[<p>The simulation of the lighting effects produced by the interaction of participating media with the light contributes to the production of visually compelling effects such as translucence and volumetric shadowing. However, the complex inner structure of participating media requires vast amounts of memory for storage and costly computations for rendering.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737757</person_id>
				<author_profile_id><![CDATA[81100339836]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gautron]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technicolor Research & Innovation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pascal.gautron@technicolor.com]]></email_address>
			</au>
			<au>
				<person_id>P3737758</person_id>
				<author_profile_id><![CDATA[81466647298]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Cyril]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Delalandre]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technicolor Research & Innovation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[cyril.delalandre@technicolor.com]]></email_address>
			</au>
			<au>
				<person_id>P3737759</person_id>
				<author_profile_id><![CDATA[81100029219]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jean-Eudes]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marvie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technicolor Research & Innovation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jean-eudes.marvie@technicolor.com]]></email_address>
			</au>
			<au>
				<person_id>P3737760</person_id>
				<author_profile_id><![CDATA[81487641482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lecocq]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technicolor Research & Innovation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pascal.lecocq@technicolor.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1944751</ref_obj_id>
				<ref_obj_pid>1944745</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Delalandre, C., Gautron, P., Marvie, J.-E., and Fran&#231;ois, G. 2011. Transmittance function mapping. In <i>Proceedings of the I3D Symposium</i>, 31--38.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1730831</ref_obj_id>
				<ref_obj_pid>1730804</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Jansen, J., and Bavoil, L. 2010. Fourier opacity mapping. In <i>Proceedings of the I3D Symposium</i>, 165--172.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344958</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lokovic, T., and Veach, E. 2000. Deep shadow maps. In <i>Proceedings of SIGGRAPH</i>, 385--392.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Volume-Aware Extinction Mapping Pascal Gautron* Cyril Delalandre* Jean-Eudes Marvie* Pascal Lecocq* 
Technicolor Research &#38; Innovation  Figure 1: Our technique renders this animated scene at 1.5fps, 
featuring volcano smoke (10243) as well as clouds and plumes (75 × 5123). The simulation of the lighting 
effects produced by the interaction of participating media with the light contributes to the production 
of visually compelling effects such as translucence and volumetric shadowing. However, the complex inner 
structure of participating media requires vast amounts of memory for storage and costly com­putations 
for rendering. The cost of of.ine lighting estimation within volumes is usually re­duced by caching strategies 
such as Deep Shadow Maps [Lokovic and Veach 2000], in which lists of volume samples are built to rep­resent 
light attenuation. These lists are then ordered, .ltered and compressed prior to rendering. Real-time 
rendering methods such as [Jansen and Bavoil 2010; Delalandre et al. 2011] avoid the need for list management 
by projecting light attenuation into a Fourier basis. While effective, the empty sections between participating 
media are encoded as well in the Fourier transform, leading to ring­ing artifacts when used in complex 
environments. Volume-Aware Extinction Maps project and integrate spatially un­ordered sets of volume 
samples within a volume-aware functional space for high quality interactive rendering of production scenes. 
Volume-Aware Pseudometric Our technique borrows a principle of [Lokovic and Veach 2000]. Empty sections 
of the scene can be cancelled out without any im­pact on the representation quality (Figure 2c). Intuitively 
this is equivalent to replacing the euclidean distance by a pseudometric only accounting for the sections 
traversing the participating media. Instead of managing an explicit list, we aim at generating a pseu­dometric 
map, for which each texel contains a representation of the pseudometric using a small set of Fourier 
coef.cients. This map is obtained by rendering the bounding boxes of the participating media, and projecting 
the traversal length of each ray through the boxes, yielding g'(x) (Figure 2). This function is then 
integrated through analytical integration of the Fourier basis functions, yield­ing a representation 
of g(x). We use a similar principle to represent the transmittance function along a ray. While [Delalandre 
et al. 2011] require a successive traversal of the media to evaluate transmittance, we .rst represent 
each available sample of local extinction in terms of the pseudo­metric, that is st(x)= stg(g(x)). Then, 
we project each sample of this unsorted set into a Fourier basis, yielding a representation of the extinction 
along light rays. Then transmittance T at a point is then obtained by analytically integrating the projected 
extinction. *{pascal.gautron, cyril.delalandre, jean-eudes.marvie, pascal.lecocq}@technicolor.com  
(a) (b) (c) Figure 2: While the representation bounds can tightly .t a single medium (a), the transmittance 
function T features constant sections between multiple media (b). Using an appropriate pseudometric g, 
we represent transmittance without gaps (c). Results We implemented Volume-Aware Extinction Maps to 
render 1280 × 720 images on a Intel Xeon X5680 3.36GHz processor running a Nvidia GeForce GTX580 GPU. 
The skies of Figure 1 comprise a total of 10.5GB of volumetric data. We manage this complexity by introducing 
an easy-to-use out of core management of volumetric data for adaptive rendering. We also leverage the 
duality of our rep­resentation for interactive image-based lighting and multiple scat­tering. The Volume-Aware 
Extinction Maps used in these images have a resolution of 1024 × 1024 with a total of 12 coef.cients 
per pixel (4 for g, 8 for st). Our technique .nds a particular use for real-time editing and nav­igation 
in environments featuring massive volumetric datasets. As we do not introduce any precomputation, the 
components of the scene can be modi.ed in real-time without impacting rendering speed. This ability makes 
our method a highly valuable tool for the previsualization and tuning of complex volumetric environments 
for production rendering. References DELALANDRE, C., GAUTRON, P., MARVIE, J.-E., AND FRANC¸ OIS, G. 
2011. Transmittance function mapping. In Pro­ceedings of the I3D Symposium, 31 38. JANSEN, J., AND BAVOIL, 
L. 2010. Fourier opacity mapping. In Proceedings of the I3D Symposium, 165 172. LOKOVIC, T., AND VEACH, 
E. 2000. Deep shadow maps. In Proceedings of SIGGRAPH, 385 392. Copyright is held by the author / owner(s). 
SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2343088</section_id>
		<sort_key>430</sort_key>
		<section_seq_no>12</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Game worlds]]></section_title>
		<section_page_from>12</section_page_from>
	<article_rec>
		<article_id>2343089</article_id>
		<sort_key>440</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>32</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Creating vast game worlds]]></title>
		<subtitle><![CDATA[experiences from Avalanche Studios]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343089</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343089</url>
		<abstract>
			<par><![CDATA[<p>Creating vast game environments introduces several complex issues that are not present in smaller game worlds. As the size of the world grows issues with floating point precision arise, causing objects to jitter, simulation to become unstable, and rendering to exhibit artifacts. These problems were very apparent during development of Just Cause 2 and had to be addressed. Depending on location in the game animated foliage exhibited vertices snapping up to decimeter sized increments. Trees were at times jumping instead of swaying. In the distance there were severe z-fighting issues. Shadows were sometimes unstable. The shadow acne that occurred could not be effectively dealt with using the standard depth bias approach. Additionally, sampling depth buffers using vendor-provided methods yielded insufficient precision. Several techniques were developed to reduce precision loss at every part of the pipeline.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737761</person_id>
				<author_profile_id><![CDATA[81442598009]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Emil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Persson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[persson@avalanchestudios.se]]></email_address>
			</au>
			<au>
				<person_id>P3737762</person_id>
				<author_profile_id><![CDATA[81504687982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Avalanche]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Studios]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Creating vast game worlds - Experiences from Avalanche Studios Emil Persson, Avalanche Studios emil.persson@avalanchestudios.se 
 1. Issues with vast game worlds Creating vast game environments introduces several complex issues that 
are not present in smaller game worlds. As the size of the world grows issues with floating point precision 
arise, causing objects to jitter, simulation to become unstable, and rendering to exhibit artifacts. 
These problems were very apparent during development of Just Cause 2 and had to be addressed. Depending 
on location in the game animated foliage exhibited vertices snapping up to decimeter sized increments. 
Trees were at times jumping instead of swaying. In the distance there were severe z-fighting issues. 
Shadows were sometimes unstable. The shadow acne that occurred could not be effectively dealt with using 
the standard depth bias approach. Additionally, sampling depth buffers using vendor-provided methods 
yielded insufficient precision. Several techniques were developed to reduce precision loss at every part 
of the pipeline. Figure 1. View over the vast game world of Just Cause 2.  Meanwhile the amount of data 
to represent such a vast environment is overwhelming. Consequently significant amount of work was invested 
in a high performance streaming solution to dynamically load and unload the required data. In addition 
techniques were developed to make the world appear interesting from huge distances, even for areas currently 
not loaded. Furthermore, several novel approaches were developed for better utilizing the limited memory 
available on current generation consoles. This allowed more objects to stay resident in memory. Rendering 
a large game world with a lot of detail presents yet another challenge. The city in Just Cause 2 turned 
out to be particularly problematic, with a huge amount of objects to render. Several other locations 
were also equally challenging to render. A number of techniques were devised to deal with this problem, 
including a highly optimized occlusion culling system, a batching system, and a sophisticated level-of-detail 
system. Post Just Cause 2 the Avalanche Engine has seen several radical changes presenting new issues 
which require new solutions for the newer titles under development. With the switch to deferred rendering, 
the addition of an indoor mode, new features such as dynamic lights casting shadows, a new set of precision 
issues has occurred with a new set of solutions required. With the additional memory required for deferred 
rendering and full HDR rendering the memory constraints have become significantly tighter. 2. Summary 
of solutions To deal with precision issues we unchained matrices in shaders to avoid lossy dependencies. 
We eliminated large offsets by decomposing matrix transformation and merging intermittent translations. 
Floating point timers were reset on opportunity. Fixed point was used where appropriate. The depth-buffer 
was reversed and floating point depth was used on consoles. Sampling depth on PS3 was done with a custom 
resolve shader that preserved precision despite lossy results from texture unit and partial precision 
math. Unstable shadows were fixed using a sub-pixel jitter compensation shift merged into the shadow 
matrix. The shadow range was dynamically adjusted depending on player elevation over ground. Resizing 
of the shadow range was made discreet to avoid unstable shadows due to range expansion. Objects were 
culled from shadow passes based on clip-space size. Shadow rendering costs remained largely constant 
even when expanded. The shadow cascades were store in an atlas to allow single-sample shadows. Cascade 
transitions were hidden using a screen-space dithering approach. The streamer uses a double-buffered, 
concurrent, out-of-order engine where reading data and creating resources happens in parallel. The data 
is pre-optimized on disk in gigabyte sized archives and related resources placed adjacently. The streamer 
automatically reorders requests to minimize disk seeks. Memory was better utilized by analyzing temporal 
overlap and reusing render target memory for temporally non-overlapping surfaces. Luminance textures 
were merged into channels of DXT textures and extracted with hardware configurable swizzles on consoles. 
1ms of GPU time was traded for 6MB memory on Xenon by converting 32bit shadow maps to 16bit using a memory 
export shader. Vertices were compressed from float to shorts. This caused issues when vertices snapped 
differently for models placed adjacently, leaving ugly gaps. This was solved by quantizing models together 
using common settings, trading some precision for consistency. Normals and tangent vectors were compressed 
into single floats. Post-JC2 this was improved to store a complete tangent space (including bitangent 
flip-bit) in only four bytes. The general LOD system for models was complemented with a landmark system 
to keep the world alive and navigable at huge distances. See for instance the twin towers of Panau Falls 
Casino miles away, over Rico s right shoulder in Figure 1. A distant light system was implemented to 
give the world life at large distances, as seen in Figure 1, especially at night, but also during daytime. 
Particles were optimized using a novel particle trimming algorithm generating optimized polygons encapsulating 
the meat of the particle while cutting away as much of the empty space as possible given a target vertex 
count. This roughly doubled particle rendering performance on average (including clouds) without affecting 
visual quality at all. BFBC (Brute Force Box Culling), a low-level asynchronous SIMD optimized occlusion 
culling system was implemented that operates on artist provided occluder boxes and system provided bounding 
boxes. Higher level systems were built on top of this where objects could be managed grid-based, as compounds 
or instance-based depending on particular system needs. Post-JC2 the build system has been vastly improved 
with tools for running the latest build directly over http from a server. Hot-reload and direct link 
has been implemented to improve iteration times. Trust in the pipeline has been achieved. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343090</article_id>
		<sort_key>450</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>33</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Asking the impossible on SSX]]></title>
		<subtitle><![CDATA[creating 300 tracks on a ten track budget]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343090</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343090</url>
		<abstract>
			<par><![CDATA[<p>The world created for the latest release of the snowboarding game SSX posed the challenge of delivering massively increased volumes of content over previous releases without increasing the budget.</p> <p>The success of the game depended on drawing a new audience into an experience equal in scale and quality to the current generation of gaming console, while remaining true to the expectations of a large existing fan base.</p> <p>To retain the essential flavor of the game for the fans, while generating ten-to-twenty times the content required not only a new pipeline for content generation, but a whole new way of thinking about how content is created. To accomplish this, we implemented an evolving procedural pipeline. This type of approach is still relatively uncommon in game production, and so it required an educational process in parallel to the technical development.</p> <p>This talk discusses the technical, cultural, and production challenges faced in the creation of the massive world of SSX.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737763</person_id>
				<author_profile_id><![CDATA[81504686536]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Caleb]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Howard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[choward@ea.com]]></email_address>
			</au>
			<au>
				<person_id>P3737764</person_id>
				<author_profile_id><![CDATA[81504685982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Carlos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lemus]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[carlosl@ea.com]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Asking the Impossible on SSX: Creating 300 tracks on a ten track budget  Caleb Howard Electronic Arts 
Canada choward@ea.com Carlos Lemus Electronic Arts Canada carlosl@ea.com Abstract The world created 
for the latest release of the snowboarding game SSX posed the challenge of delivering massively increased 
volumes of content over previous releases without increasing the budget. The success of the game depended 
on drawing a new audience into an experience equal in scale and quality to the current generation of 
gaming console, while remaining true to the expectations of a large existing fan base. To retain the 
essential flavor of the game for the fans, while generating ten-to-twenty times the content required 
not only a new pipeline for content generation, but a whole new way of thinking about how content is 
created. To accomplish this, we implemented an evolving procedural pipeline. This type of approach is 
still relatively uncommon in game production, and so it required an educational process in parallel to 
the technical development. This talk discusses the technical, cultural, and production challenges faced 
in the creation of the massive world of SSX. 1. An evolving pipeline A key advantage available in building 
procedural production tools is that of quick turnaround. Using the rapid-prototyping capacity of Houdini 
to iteratively build and deploy a modular system of integrated tools has allowed us to respond to the 
changing needs of the Art Directors, and at the same time to harness the growing understanding of the 
level designers as they gained experience with a more analytic way of planning their work. Our approach 
was to deploy a simple model of a full pipeline as quickly as possible, while planning for continual 
revision. Putting this initial toolset into the hands of the designers early in development allowed them 
to identify needed enhancements right away. At the same time, by fostering a rapid deployment, and tight 
communication between the designers and the tool-builders, the designers were quickly educated to the 
analytic mindset most conducive to conceiving and implementing procedural tools. As their skills grew, 
the tools evolved to match their growing facility with procedural thinking. With each iteration, they 
were better able to track the changing demands of Art Direction, and the evolving requirements of the 
gameplay engine. 2. Improving terrain representation The first pipeline was the simplest possible implementation. 
Built in a matter of days from the proof-of-concept used to pitch a procedural approach, it initiated 
the development of the pipeline at the same time as priming the pipeline to start generating content 
immediately. Initially, the pipeline was based on 2D manifolds conformed to global elevation maps from 
N.A.S.A. together with a straightforward set of tools which allowed designers to sculpt the terrain manifolds 
into playable surfaces. This allowed for an immediate source of a lot of real-life data to test gameplay 
against, but was topologically limited to plane-homeomorphisms. As the game engine was developed, requirements 
for more complex topological features in the terrain suggested a Signed Distance Function (SDF) represented 
in a uniform, voxel-based volume representation. This was implemented with a suite of tools patterned 
on the familiar and mature workflow of image composition, applied to voxel arrays, rather than pixels. 
These second generation tools were developed to the designers specification while they worked uninterrupted 
on the First generation tools. With a volume representation, we implemented Boolean operators, and swept-path 
operations to allow for the introduction of creative elements like ledges, crevasses and tunnels. As 
a last stage to this pipeline, the SDF was transformed to a mesh for export to the game engine. Once 
the switch to a volume-based pipeline was made, the representation and tools were able to evolve without 
interruption. Resolution limitations of uniform voxel sets led to the development of hierarchical voxel 
sets to define features at various scales. This led to the need to develop a novel surfacing algorithm 
to transit the voxel set boundaries. Finally, memory constraints led to the implementation of a nonuniform 
point cloud representation with the challenges of implementing Boolean operations in that paradigm. Throughout 
this ongoing process of deployment and upgrade, the designers work was uninterrupted. 3. Results SSX 
delivered more than ten times the content of previous releases of the game. We faced challenges with 
dramatic production changes, while implementing a entirely new development and production pipeline, and 
facing the need to educate an entire team to a different way of thinking about production. We now have 
a pipeline, and expertise in the hands of tool builders and artists. In addition, we have proven the 
efficacy of procedural methods sufficiently to have seeded broad interest across all domains of game 
production at Electronic Arts. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343091</article_id>
		<sort_key>460</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>34</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Lighting the open world of New York Zero for Prototype 2]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343091</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343091</url>
		<abstract>
			<par><![CDATA[<p><i>Prototype 2</i> is an open-world action game that gives the player superhuman abilities to move anywhere in the world - including the ability to run at speed up the side of a building, leap off the rooftop, and glide across the city. This freedom presented a variety of technical challenges, such as the question of how to get indirect lighting for both static and dynamic parts of a world where verticality and space plays such an important role. We discuss our solution - a novel pseudo-volumetric light baking process which packs vertical slices of the world into different mipmaps of a lightmap - and how it integrates with the art authoring pipeline and the rest of the rendering engine.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737765</person_id>
				<author_profile_id><![CDATA[81504683940]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Keith]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[O'Conor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Radical Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737766</person_id>
				<author_profile_id><![CDATA[81504687698]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Josh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Blommestein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Radical Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Lighting the Open World of New York Zero for Prototype 2 lighting-banner.tif Keith O'Conor Radical 
Entertainment Josh Blommestein Radical Entertainment  Figure 1: (a) Indirect lighting component of a 
scene, (b) mipmaps of the lightmap representing vertical sections of the world, and (c) the final image 
 1. Overview Prototype 2 is an open-world action game that gives the player superhuman abilities to 
move anywhere in the world - including the ability to run at speed up the side of a building, leap off 
the rooftop, and glide across the city. This freedom presented a variety of technical challenges, such 
as the question of how to get indirect lighting for both static and dynamic parts of a world where verticality 
and space plays such an important role. We discuss our solution - a novel pseudo-volumetric light baking 
process which packs vertical slices of the world into different mipmaps of a lightmap - and how it integrates 
with the art authoring pipeline and the rest of the rendering engine. 2. Lighting workflow Each of 
the 3 zones in the game world have multiple times of day (dawn, night etc.). Each time of day is authored 
by lighting artists who have control over certain aspects of the direct lighting - sunlight direction/colour, 
local light placement in the world, material specular properties, environmental effects and so on. All 
lighting is done directly in the game engine (augmented with lighting-specific authoring tools), for 
immediate and accurate feedback of how the final in-game lighting looks. 2.1 Volumetric lightmap generation 
 These light locations and properties are then used for generating indirect lighting, which takes place 
offline. 3Delight is fed these values along with the world geometry, and generates a point cloud of the 
indirect bounced light of the entire zone. This is sampled at multiple height levels (a total of 9 are 
generated), and for each level a top-down projection of the zone is rendered that represents the indirect 
bounced light at that height. The first few levels are close to ground level and only a few meters apart, 
to give more coverage to the areas with the most variation in bounced light. At upper levels the sample 
heights are more spaced out, to give more coverage of the higher elevations of the world. All levels 
are then are used to generate a texture, with each height level being a lower mipmap in the texture - 
see Figure 1(b). Each texel in this texture represents the diffuse colour of the average directional 
irradiance at that point. 2.2 Pipeline integration This entire process is fully automated, to be as 
transparent as possible for world artists and others who are not directly involved in lighting. Upon 
any change in world geometry or lighting that is committed to source control, the content pipeline gathers 
the lighting properties and dispatches a render job to a network of render machines. They generate new 
light maps for every affected time of day and check in the updated textures. For the lighting artists 
there is also the ability to kick off a network render job with their local changes. This gives them 
the ability to refine and iterate on the lighting of a specific area with a faster turnaround time. 
3. Engine integration To retrieve the lighting value at runtime, all the engine has to do is convert 
a world-space position into a texture coordinate that will sample from the correct location and mipmap 
in this texture. Trilinear filtering will produce a continuous indirect lighting component for any position 
in the world. This value is used as the ambient component of the lighting equation and combined with 
the our direct lighting, shadowing, SSAO and other terms. While the main use of this system is for indirect 
lighting, the same system has also been employed to generate volumetric ambient occlusion maps to great 
effect, giving more depth to the avenues and alleys of the city. We have even used it to generate a height 
map of the highest points of the world, which is used to turn off rain effects in underpasses and other 
sheltered locations. 4. Results The use of this system for indirect lighting and ambient occlusion 
has a number of advantages. The memory usage is minimal - a single 512x512 mipmapped texture is all that 
is needed for an entire zone, much less than an alternative solution based on traditional lightmapping 
or spherical harmonics. Artists don't need to care about problems like UV unwrapping, as it just works 
transparently for them. It is also cheap to use - calculating the indirect lighting at any point is as 
simple as performing a few operations and a single trilinear texture fetch. This means it can be applied 
to everything in the world, from static geometry to dynamic props and characters (including LODs), and 
even particle effects and raindrops, with consistent and continuous results. Most importantly for the 
Prototype 2 universe, the result is indirect lighting that can be sampled at any point in the world, 
regardless of whether it's on the ground, running up a skyscraper, or gliding through the air. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343092</article_id>
		<sort_key>470</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>35</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Character customization of Soulcalibur 5 in-depth]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343092</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343092</url>
		<abstract>
			<par><![CDATA[<p>Recently, there is a growing need for character customization for the purpose of player's avatar. The character customization system was implemented for the fighting game "Soulcalibur" since the third of the game, and has evolved over time. Our latest iteration of Soulcalibur makes it possible to project color and normal textures to a character anywhere the player would like.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737767</person_id>
				<author_profile_id><![CDATA[81504684878]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NAMCO BANDAI Studios Inc., Shinagawa, Tokyo, JPN]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[s-tani@nbsi.jp]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Character Customization of Soulcalibur 5 In-Depth  Shiro Tani NAMCO BANDAI Studios Inc. Shinagawa, 
Tokyo, JPN s-tani@nbsi.jp 1. Introduction Recently, there is a growing need for character customization 
for the purpose of player s avatar. The character customization system was implemented for the fighting 
game Soulcalibur since the third of the game, and has evolved over time. Our latest iteration of Soulcalibur 
makes it possible to project color and normal textures to a character anywhere the player would like. 
We present the following two points. First, we introduce the Soulcalibur 5 character customization system 
as a whole which has been cultivated in the series. Second, we will explain some of the issues we faced 
in the texture projection to a character caused by the lack of affinity for the existing customization 
system, and provide practical solutions to them. 2. Character customization We implemented the following 
points of character customizations, changing the size of the body parts (e.g., pectorals, thighs), equipping 
clothes and accessories, changing the color of body or equipment, and projecting color and normal textures 
to a character. Players are able to share their own characters in battles with other players. Furthermore 
players are able to share their character s photo which was taken in customization mode. Figure 1. In-Game 
Screenshot of projected textures. 3. Implementation of the texture projection The texture projection 
system was at variance with other customization system and requirements as a fighting game. Thus we need 
to solve the following issues. First issue is the reconstruction of the texture projection. The customized 
character should be reconstructed from save data as well as opponent s data over the network. A character 
of our game has vast texture size from 10MB to over 20MB. Therefore we were not able to store or send 
all of the textures directly. For this reason, we stored meta-information of the texture projection 
where the projection point is, what is projected, what the equipment is, etc. Players exchange the meta-information 
of the projection, and reconstruct the characters in their local environment. Second issue is the re-editing. 
We should make it possible to re-edit the projected texture or size of the body parts thereby we enabled 
re-edit by reconstructing the character from the meta-information each time. Third issue is the speed. 
As previously explained we should reconstruct the character frequently and display the reconstructed 
result as soon as possible. We implemented a fast DXT compression pipeline using the GPU. Fourth issue 
is the artifacts. Projecting textures to the UV discontinuity area caused artifacts because of the difference 
between the projecting process and the fetching of the projected texture process. We suppressed the artifacts 
by expanding the edge of the projected area. The result of this algorithm is shown in Figure 2. Figure 
2. Result without expanding the edge (left), Result with expanding edge (right). 4. Conclusions We 
presented the entire system of the character customization of our game. We also introduced the issues 
of the character customization system and provided the practical solutions to them. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2343093</section_id>
		<sort_key>480</sort_key>
		<section_seq_no>13</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Fast, realistic lighting]]></section_title>
		<section_page_from>13</section_page_from>
	<article_rec>
		<article_id>2343094</article_id>
		<sort_key>490</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>36</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Local image-based lighting with parallax-corrected cubemaps]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343094</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343094</url>
		<abstract>
			<par><![CDATA[<p>Image-based lighting (IBL) is typically used for distant lighting represented by an infinite environment map. This technique has been used by many games. Games divide their scenes into several sectors and associate a cubemap (the environment mapping of choice due to its graphic hardware performance) to each of them. The cubemap of the sector where the camera is located is then used to light objects [1]. The problem with this approach is that it cannot accurately represent local reflections on specular and glossy objects.. Our game requires more accurate local reflections, which implies that a local image-based lighting technique must be used. Previous local image-based lighting approaches, such as the Half life 2 approach [2] consist of assigning an individual cubemap to each objects. These approaches suffer from lighting seams and parallax issues [3]. We introduce a new approach which avoids these artifacts while still preserving extremely high performance on current console generation hardware (PS3/XBOX360).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737768</person_id>
				<author_profile_id><![CDATA[81504687969]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lagarde]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[S&#233;bastien]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DONTNOD Entertainment Paris, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[Lagardese@hotmail.fr]]></email_address>
			</au>
			<au>
				<person_id>P3737769</person_id>
				<author_profile_id><![CDATA[81504688281]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Antoine]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zanuttini]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DONTNOD Entertainment Paris, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[lezanu@gmail.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Van der leeuw, The playstation 3 SPUs in the real world. http://www.slideshare.net/guerrillagames/the-playstation3s-spus-in-the-real-world-a-killzone-2-case-study-9886224]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[McTaggarts, Half-Life 2 Valve Source Shading http://www2.ati.com/developer/gdc/D3DTutorial10_Half-Life2_Shading.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Valve wiki http://developer.valvesoftware.com/wiki/Cubemaps]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Local image-based lighting with parallax-corrected cubemaps  Lagarde Sébastien (Lagardese@hotmail.fr) 
&#38; Antoine Zanuttini (lezanu@gmail.com) DONTNOD Entertainment Paris, France  Figure 1. Left: Render 
of default cubemap. Middle: Lighting artists Cubemap tools. Right: Render of parallax-corrected cubemap 
(256x256x6). 1. Introduction Image-based lighting (IBL) is typically used for distant lighting represented 
by an infinite environment map. This technique has been used by many games. Games divide their scenes 
into several sectors and associate a cubemap (the environment mapping of choice due to its graphic hardware 
performance) to each of them. The cubemap of the sector where the camera is located is then used to light 
objects [1]. The problem with this approach is that it cannot accurately represent local reflections 
on specular and glossy objects.. Our game requires more accurate local reflections, which implies that 
a local image-based lighting technique must be used. Previous local image-based lighting approaches, 
such as the Half life 2 approach [2] consist of assigning an individual cubemap to each objects. These 
approaches suffer from lighting seams and parallax issues [3]. We introduce a new approach which avoids 
these artifacts while still preserving extremely high performance on current console generation hardware 
(PS3/XBOX360). 2. Lighting seams In order to achieve seamless lighting transitions between objects 
we adopt the single-cubemap approach of the "infinite cubemap" technique; the same cubemap is applied 
to all objects of the scene. However in our case, this single cubemap is the result of blending several 
local cubemaps. Contrary to an infinite cubemap, each local cubemap has a position and must be generated 
in the game engine (in an offline preprocess) to accurately represent the local scene reflection. During 
the game's runtime, each frame all local cubemap influences which overlap the camera (or the player) 
are selected to be blended together to create the single cubemap which will be applied to the scene. 
We develop algorithms for calculating the blending weights for each selected local cubemaps and for blend 
them appropriately. The blend step is entirely done on the GPU and is very efficient even for current 
console generation hardware. Using a single cubemap (generated from local cubemaps) for all visible 
objects, introduces noticeable artifacts for distant reflective objects. To fight this problem we use 
the local lighting information available at the distant object locations to adapt the cubemap lighting. 
We also develop some automatic shader level of detail to smoothly disable cubemap lighting with distance. 
 3. Parallax issue A cubemap only defines accurate reflections at the location where it was generated. 
Furthermore, a cubemap represents an infinite box reflection resulting in parallax issues when used (reflected 
objects do not appear at the right positions). This is particularly obvious on planar objects. We develop 
a new technique to fix the parallax issue for planar objects (Figure 1 shows walls with a highly specular 
planar floor to illustrate the technique). A set of tools have been developed for lighting artists to 
help them to define the rectangular area that a local cubemap must represent and the provided information 
is then used at blend time to correctly adjust planar reflections. Coupled with our GPU blending algorithm 
we are able to blend several parallax-corrected cubemaps efficiently on current console hardware (Around 
0.28ms for 4 parallax-corrected 128x128x6 cubemaps on PS3). Figure 2 shows another sample of parallax-corrected 
cubemap use on a planar object.     Figure 2. Left: Cubemap. Right: Parallax-corrected cubemap. 
 4. Conclusions The advantage of our system is to provide an efficient way to simulate local reflections 
with seamless transitions between objects. It requires some tools to ease the work of lighting artists 
as well as some setup time. References [1] Van der leeuw, The playstation 3 SPUs in the real world. 
http://www.slideshare.net/guerrillagames/the-playstation3s-spus-in-the-real-world-a-killzone-2-case-study-9886224 
[2] McTaggarts, Half-Life 2 Valve Source Shading http://www2.ati.com/developer/gdc/D3DTutorial10_Half-Life2_Shading.pdf 
[3]Valve wiki http://developer.valvesoftware.com/wiki/Cubemaps 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343095</article_id>
		<sort_key>500</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>37</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Tiled and clustered forward shading]]></title>
		<subtitle><![CDATA[supporting transparency and MSAA]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343095</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343095</url>
		<abstract>
			<par><![CDATA[<p>We present details of Tiled and Clustered Forward Shading in its application to rendering transparent geometry and using Multi Sampling Anti Aliasing (MSAA). We detail how transparency and MSAA is supported, and present performance results measured on modern GPUs.</p> <p>Previous techniques for handling large numbers of lights are usually based on deferred shading [Andersson 2009; Lauritzen 2010]. However, deferred shading techniques struggle with impractically large frame buffers when MSAA is used, and make supporting transparency difficult. In addition, deferred shading makes it more difficult to support custom shaders on geometry.</p> <p>Tiled Forward Shading is a new and highly practical approach to real-time shading scenes with thousands of light sources, introduced by Olsson and Assarsson in 2011 [2011]. Their results, measured on an GTX 280 GPU, indicated that tiled forward shading was impractically slow. Performance on more recent GPUs has improved considerably (approaching that of tiled deferred), which opens up the possibility of using the technique to support transparency and MSAA.</p> <p>Clustered Shading further extends tiled shading by adding depth partitioning [Olsson et al. 2012]. We show how Clustered Forward Shading can be extended to support transparency efficiently.</p> <p>Forward shading naturally supports both transparency and MSAA, which has been shown in previous work. However, the performance and implementation details have not previously been investigated.</p> <p>We provide details on how to construct the light grid for use with transparency. When the transparent geometry is considered, the depth range optimization cannot be fully used. Instead, only a more conventional hierarchical depth test can be used. The grid structure can be built once, and quickly pruned to prepare a more efficient instance for opaque geometry. However, as each transparent layer must consider all the lights in the tile, performance does not scale linearly with the depth complexity, but far worse (Figure 1, right).</p> <p>To improve on this we extend clustered forward shading by constructing the grid using a pre-pass over all geometry (not just opaque), and flagging clusters as a side effect. This allows us to quickly find the unique clusters used. As clusters contain only space around actual samples that need shading, efficiency is much better (Figure 1, left).</p> <p>For deferred shading a single 1080p, 16x MSAA, 16-bit float RGBA buffer requires over 250Mb of memory. In addition, each sample may need to be shaded individually, effectively running shading at a per-sample frequency. For forward shading, no G-Buffers are required and MSAA is trivially enabled.</p> <p>A brief performance and memory comparison is shown in Figure 2, showing that clustered forward outperforms tiled forward by more than 2 times, and also outperforms tiled deferred, if MSAA is used.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737770</person_id>
				<author_profile_id><![CDATA[81351595938]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ola]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Olsson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chalmers University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737771</person_id>
				<author_profile_id><![CDATA[81440610411]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Markus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Billeter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chalmers University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737772</person_id>
				<author_profile_id><![CDATA[81100618307]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ulf]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Assarsson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chalmers University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Andersson, J., 2009. Parallel graphics in frostbite - current &amp; future. SIGGRAPH Course: Beyond Programmable Shading.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lauritzen, A., 2010. Deferred rendering for current and future rendering pipelines. SIGGRAPH Course: Beyond Programmable Shading.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Olsson, O., and Assarsson, U. 2011. Tiled shading. <i>Journal of Graphics, GPU, and Game Tools 15</i>, 4, 235--251.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383809</ref_obj_id>
				<ref_obj_pid>2383795</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Olsson, O., Billeter, M., and Assarsson, U. 2012. Clustered deferred and forward shading. In <i>HPG '12: Proceedings of the Conference on High Performance Graphics 2012</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Tiled and Clustered Forward Shading Supporting Transparency and MSAA Ola Olsson, Markus Billeter and 
Ulf Assarsson Chalmers University of Technology  Figure 1: Shot from the Crytek Sponza scene with semi-transparent 
bubbles added, lit by 1024 random lights. With 8x MSAA at 720p resolution, Tiled Deferred runs at 53 
FPS (without bubbles), Tiled Forward at 52 FPS and Clustered Forward at 161 FPS on a GTX 680. The diagrams 
illustrate how transparent geometry affects clustered and tiled forward shading. 1 Abstract We present 
details of Tiled and Clustered Forward Shading in its application to rendering transparent geometry and 
using Multi Sampling Anti Aliasing (MSAA). We detail how transparency and MSAA is supported, and present 
performance results measured on modern GPUs. Previous techniques for handling large numbers of lights 
are usu­ally based on deferred shading [Andersson 2009; Lauritzen 2010]. However, deferred shading techniques 
struggle with impractically large frame buffers when MSAA is used, and make supporting transparency dif.cult. 
In addition, deferred shading makes it more dif.cult to support custom shaders on geometry. Tiled Forward 
Shading is a new and highly practical approach to real-time shading scenes with thousands of light sources, 
intro­duced by Olsson and Assarsson in 2011 [2011]. Their results, mea­ sured on an GTX 280 GPU, indicated 
that tiled forward shading was impractically slow. Performance on more recent GPUs has improved considerably 
(approaching that of tiled deferred), which opens up the possibility of using the technique to support 
trans­parency and MSAA. Clustered Shading further extends tiled shading by adding depth partitioning 
[Olsson et al. 2012]. We show how Clustered Forward Shading can be extended to support transparency ef.ciently. 
Forward shading naturally supports both transparency and MSAA, which has been shown in previous work. 
However, the performance and implementation details have not previously been investigated. We provide 
details on how to construct the light grid for use with transparency. When the transparent geometry is 
considered, the depth range optimization cannot be fully used. Instead, only a more conventional hierarchical 
depth test can be used. The grid structure can be built once, and quickly pruned to prepare a more ef.cient 
instance for opaque geometry. However, as each transparent layer must consider all the lights in the 
tile, performance does not scale linearly with the depth complexity, but far worse (Figure 1, right). 
To improve on this we extend clustered forward shading by con­structing the grid using a pre-pass over 
all geometry (not just opaque), and .agging clusters as a side effect. This allows us to quickly .nd 
the unique clusters used. As clusters contain only space around actual samples that need shading, ef.ciency 
is much better (Figure 1, left). For deferred shading a single 1080p, 16x MSAA, 16-bit .oat RGBA buffer 
requires over 250Mb of memory. In addition, each sample may need to be shaded individually, effectively 
running shading at a per-sample frequency. For forward shading, no G-Buffers are required and MSAA is 
trivially enabled. A brief performance and memory comparison is shown in Figure 2, showing that clustered 
forward outperforms tiled forward by more than 2 times, and also outperforms tiled deferred, if MSAA 
is used. Figure 2: Left, performance for a view similar to Figure 1 (deferred without bubbles). Right, 
memory use of deferred vs. forward at 720p, assuming 32-bit depth and color targets, and 3 × 64-bit G­buffers. 
 References ANDERSSON, J., 2009. Parallel graphics in frostbite -current &#38; future. SIGGRAPH Course: 
Beyond Programmable Shading. LAURITZEN, A., 2010. Deferred rendering for current and fu­ture rendering 
pipelines. SIGGRAPH Course: Beyond Pro­grammable Shading. OLSSON, O., AND ASSARSSON, U. 2011. Tiled shading. 
Journal of Graphics, GPU, and Game Tools 15, 4, 235 251. OLSSON, O., BILLETER, M., AND ASSARSSON, U. 
2012. Clus­tered deferred and forward shading. In HPG 12: Proceedings of the Conference on High Performance 
Graphics 2012. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 
5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343096</article_id>
		<sort_key>510</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>38</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Art pipeline]]></title>
		<subtitle><![CDATA[transition from offline to realtime CG]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343096</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343096</url>
		<abstract>
			<par><![CDATA[<p>There are number of labor intensive and time consuming tasks in CG such as lighting setup, special effects and image post-process tuning, primarily due to lack of interactive feedback in traditional tools. Recently there are number of solutions which introduce interactivity via static scene relighting or by employing game engines for real-time rendering, however subject of full production pipeline involved is rarely touched.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737773</person_id>
				<author_profile_id><![CDATA[81504687439]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paulius]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liekis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Unity Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[paulius@unity3d.com]]></email_address>
			</au>
			<au>
				<person_id>P3737774</person_id>
				<author_profile_id><![CDATA[81504686556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Julian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hodgson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Passion Pictures]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[julian@passion-pictures.com]]></email_address>
			</au>
			<au>
				<person_id>P3737775</person_id>
				<author_profile_id><![CDATA[81504687519]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Renaldas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zioma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Unity Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rej@unity3d.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1073214</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Pellacini A., Vidim&#269;e K., Lefohn A., Leone M., Mohr A., Warren J., Lpics: a Hybrid Hardware-Accelerated Relighting Engine for Computer Cinematography. SIGGRAPH 2005, ACMTransactions on Graphics pp. 464--470]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Art pipeline: Transition from Of.ine to Realtime CG Paulius Liekis Julian HodgsonRenaldas Zioma Unity 
TechnologiesPassion Pictures Unity Technologiespaulius@unity3d.com julian@passion-pictures.com rej@unity3d.com 
1. Introduction There are number of labor intensive and time consuming tasks in CG such as lighting setup, 
special effects and image post-processtuning, primarily due to lack of interactive feedback in traditionaltools. 
Recently there are number of solutions which introduceinteractivity via static scene relighting or by 
employing gameengines for real-time rendering, however subject of full production pipeline involved is 
rarely touched.We will discuss challenges and solutions in transitioning fromtraditional of.ine CG production 
towards interactive approach used in creation of animated short Butter.y Effect. Butter.y Effectis a 
real-time CG-animated short developed in a collaboration between Unity Technologies and Passion Pictures. 
 2. Elaboration Main highlights of our approach: Initial scene setup including geometry, cameras, rough 
lighting, material properties and animation can be done in traditionalmodeling tools. Results are automatically 
exported into intermediate set of standard or human-readable formats. All changes applied to initial 
scene in modeling tool are automatically imported and available in Unity Editor almost instantaneously. 
 Lighting, material properties, object placement and non­character animations can be overridden on per-shot 
or globally across the whole .lm inside Unity.  Core library of lighting and surface shaders was implemented 
in hardware language to match shaders for of.ine renderer (Mental Ray Architectural shader library) allowing 
to convertmaterial properties directly.  Animated short scenario called for .apping robes and looseclothing 
which currently could not be achieved with suitableartistic control at run-time. Cloth is simulated in 
the of.ine tools and transferred into Unity Editor as a set of geometricaldeformations where it is applied 
to coarse cloth geometry before subdivision. Same approach is used for animations of facial expressions. 
 Cloth simulation developed in of.ine tools posed a challenge on exact synchronization with real-time 
entities on both geometry deformation and physical simulation level. First wassolved by introducing proper 
Catmull-Clark subdivision instead of approximations possible on GPUs today, latter was solved by persisting 
real-time physical simulations during time jumpswhen switching between shots.  Very coarse hair groom 
and hair animation was done in traditional modeling tool. GPU-based procedural hair and fur modeling 
system takes it from there to produce real-time results.  Custom tools for realtime editing of explosions 
and volumetriceffects based on pyroclastic noise were developed inside Unity Editor.   A set of caching 
mechanisms was used to capture globalillumination. Global illumination solution is requires of.inecomputation 
step, however number of tricks such as updating only selected region is employed to improve feedback 
loop.  A custom approach was developed to manage time jumpsbetween separate shots while preserving state 
of physically simulated bodies. This allowed edit decisions at any stages of the pipeline.  3. Results 
A set of custom tools and plugins was developed during production of Butter.y Effect to match the existing 
art pipeline of the studio with predominantly of.ine CG experience. Results were then carried over into 
an interactive environment (the Unity Editor), which was customized to provide a familiar set of toolsand 
attributes for artists. We found that direct illumination and surface shading on modern GPU can give 
almost pixel-perfect results compared to industry standard of.ine renderer (MentalRay in this case) given 
high order anti-aliasing. Real-time shadows and re.ections requirespecial approaches and yield visually 
different results, which however can be offset by artistic .exibility of interactive authoring. 4. Conclusions 
System has been used in the production of CG animated short, dramatically accelerating certain tasks 
and improving artistic creative freedom. References PELLACINI A., VIDIMCE K., LEFOHN A., LEONE M., MOHR 
A., WARREN J., Lpics: a Hybrid Hardware-Accelerated Relighting Engine for Computer Cinematography. SIGGRAPH 
2005, ACMTransactions on Graphics pp. 464-470 Copyright is held by the author / owner(s). SIGGRAPH 2012, 
Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2343097</section_id>
		<sort_key>520</sort_key>
		<section_seq_no>14</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Capture the world]]></section_title>
		<section_page_from>14</section_page_from>
	<article_rec>
		<article_id>2343098</article_id>
		<sort_key>530</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>39</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Kin&#202;tre]]></title>
		<subtitle><![CDATA[animating the world with the human body]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343098</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343098</url>
		<abstract>
			<par><![CDATA[<p>Imagine you are asked to produce a 3D animation of a demonic armchair terrorizing an innocent desk lamp. You may think about model rigging, skeleton deformation, and keyframing. Depending on your experience, you might imagine hours to days at the controls of Maya or Blender. But even if you have absolutely no computer graphics experience, it can be so much easier: grab a nearby chair and desk lamp, scan them using a consumer depth camera, and use the same camera to track your body, aligning your virtual limbs to the chair's geometry. At one spoken command, your limbs are attached to the chair model, which follows your movements in an intuitive and natural manner. Kin&#234;tre is such a system. Rather than targeting professional animators, it brings animation to a new audience of users with little or no CG experience. It allows realistic deformations of arbitrary static meshes, runs in real time on consumer hardware, and uses the human body for input in conjunction with simple voice commands. Kin&#234;tre lets anyone create playful 3D animations.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737776</person_id>
				<author_profile_id><![CDATA[81504685095]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jiawen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Cambridge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737777</person_id>
				<author_profile_id><![CDATA[81328488768]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shahram]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Izadi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Cambridge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737778</person_id>
				<author_profile_id><![CDATA[81100376259]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fitzgibbon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Cambridge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2047270</ref_obj_id>
				<ref_obj_pid>2047196</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Izadi, S., Kim, D., Hilliges, O., Molyneaux, D., Newcombe, R., Kohli, P., Shotton, J., Hodges, S., Freeman, D., Davison, A., and Fitzgibbon, A. 2011. Kinectfusion: real-time 3d reconstruction and interaction using a moving depth camera. In <i>UIST 2011</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276478</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sumner, R. W., Schmid, J., and Pauly, M. 2007. Embedded deformation for shape manipulation. <i>SIGGRAPH 2007</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Kin Etre: Animating the World with the Human Body Jiawen Chen Shahram Izadi Andrew Fitzgibbon Microsoft 
Research Cambridge Figure 1: Kin Etre in action. The user rapidly scans a real chair using a Kinect 
camera. He then sets the camera down, which tracks his motion as he moves into position. After possessing 
the chair, it walks and jumps with him. 1 Introduction Imagine you are asked to produce a 3D animation 
of a demonic armchair terrorizing an innocent desk lamp. You may think about model rigging, skeleton 
deformation, and keyframing. Depending on your experience, you might imagine hours to days at the controls 
of Maya or Blender. But even if you have absolutely no computer graphics experience, it can be so much 
easier: grab a nearby chair and desk lamp, scan them using a consumer depth camera, and use the same 
camera to track your body, aligning your virtual limbs to the chair s geometry. At one spoken command, 
your limbs are attached to the chair model, which follows your movements in an intuitive and natural 
manner. Kin Etre is such a system. Rather than targeting professional animators, it brings animation 
to a new au­dience of users with little or no CG experience. It allows realistic deformations of arbitrary 
static meshes, runs in real time on con­sumer hardware, and uses the human body for input in conjunction 
with simple voice commands. Kin Etre lets anyone create playful 3D animations. 2 Approach The user begins 
by scanning a physical artifact using KinectFu­sion [Izadi et al. 2011]. Using a depth camera, we can 
acquire and segment a human-scale object in minutes, producing a 3D mesh. Kin Etre lets a user scan an 
arbitrary object and make it move by moving his body. While we can track his motion with the same camera, 
using an off-the-shelf system, given that the object can be anything, how should this mapping be done? 
Rather than trying to discover properties of the object or guess the user s intentions, we opt for a 
direct manipulation approach. After scanning the object, he moves it out of the way, which automatically 
segments the mesh from the background. Then he embeds his body to position vacated by the object. On 
a voice command, possess , we attach his limbs to the parts of the mesh he overlaps. These attachments 
serve as constraints in our deformation model. Our mesh deformation model is inspired by the Embedded 
Defor­mation (ED) method of Sumneret al. [Sumner et al. 2007], a sys­ tem designed for interactive shape 
manipulation using the mouse and keyboard. Compared to other mesh deformation techniques, ED is also 
able to handle unstructured data such as polygon soups and particle systems. The ED model operates on 
a set of point constraints: when the user drags a point, a nonlinear optimization .nds the smoothest, 
non-stretching set of transformations that satis­.es the constraints. Qualitatively, the deformation 
feels rigid when constraints do not con.ict, and stretches like rubber when they do. For ef.ciency, the 
set of transformations are computed on a defor­mation graph a subsampled version of the input mesh. Kin 
Etre uses largely the same energy model, with the user s limbs serving as the constraints. Our contributions 
are: A new method for generating the deformation graph and an improved energy function that makes our 
system robust to the kinds of surfaces acquired by 3D scanning in particular, par­tial scans.  A fast 
nonlinear optimizer that can handle multiple simulta­  neous constraints that come from a tracked human 
skeleton. In our scenario, users rapidly scan objects, which can create in­complete 3D scans with numerous 
holes and islands. With tradi­tional ED, such inputs lead to an ill-posed optimization problem. We guarantee 
a well-posed problem by sampling the mesh with an orientation-aware metric, connecting disconnected components 
at critical points using a spanning-tree algorithm, and introducing an additional energy term that enforces 
rigid connections between is­lands. These extensions ensure that Kin Etre is robust on a wide variety 
of scenarios, described below. Kin Etre lets users scan arbitrary objects, we envision children us­ing 
the system to animate household objects for storytelling. Our accompanying video and supplementary materials 
show a number of examples, including a chair, bookcase, stepladder, and lamp. An­other application area 
for our system is in creating avatars for gam­ing or teleconferencing scenarios. Users can scan themselves 
to drop into a game, or each other and switch bodies. A key advantage of our system is its robustness 
to low-quality partial scans. Finally, meshes possessed by Kin Etre can be ef.ciently incorporated into 
physics-based interactions by approximating the surface with a set of spheres. Imagine taking over a 
friend s avatar and rampaging through a virtual world, or making a chair play dodgeball. References 
IZADI, S., KIM, D., HILLIGES, O., MOLYNEAUX, D., NEW-COMBE, R., KOHLI, P., SHOTTON, J., HODGES, S., FREE-MAN, 
D., DAVISON, A., AND FITZGIBBON, A. 2011. Kinect­fusion: real-time 3d reconstruction and interaction 
using a mov­ing depth camera. In UIST 2011. SUMNER, R. W., SCHMID, J., AND PAULY, M. 2007. Embedded deformation 
for shape manipulation. SIGGRAPH 2007. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los 
Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343099</article_id>
		<sort_key>540</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>40</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Computational retinal imaging via binocular coupling and indirect illumination]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343099</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343099</url>
		<abstract>
			<par><![CDATA[<p>The retina is a complex light-sensitive tissue that is an essential part of the human visual system. It is unique, as it can be optically observable with non-invasive methods through the eye's transparent elements. This has inspired a long history of retinal imaging devices for examination of optical function [Van Trigt 1852; Yates 2011] and for diagnosis of many of the diseases that manifest in the retinal tissue, such as diabetic retinophathy, hypertension, HIV/AIDS related retinitis, and age-related macular degeneration. These conditions are some of leading causes of blindness, especially in the developing world, but can often be prevented if screened and diagnosed in early stages.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737779</person_id>
				<author_profile_id><![CDATA[81487645512]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Everett]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lawson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737780</person_id>
				<author_profile_id><![CDATA[81361605701]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boggess]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737781</person_id>
				<author_profile_id><![CDATA[81504685987]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Siddharth]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Khullar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737782</person_id>
				<author_profile_id><![CDATA[81504685715]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Alex]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Olwal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737783</person_id>
				<author_profile_id><![CDATA[81335499586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wetzstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737784</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Abr&#224;moff, M., Garvin, M., and Sonka, M. 2010. Retinal imaging and image analysis. <i>Biomedical Engineering, IEEE Reviews in 3</i>, 169--208.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Keeler, C. 1997. 150 years since babbage's ophthalmoscope. <i>Archives of ophthalmology 115</i>, 11, 1456.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Van Trigt, A. 1852. De oogspiegel. <i>Nederlandisch Lancet, third series, Utrecht 1853</i>, 417--509.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Yates, Paul Andrew; Tran, K., 2011. Hand-held portable fundus camera for screening photography, March.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computational Retinal Imaging via Binocular Coupling and Indirect Illumination Everett Lawson Jason 
Boggess Siddharth Khullar Alex Olwal Gordon Wetzstein Ramesh Raskar MIT Media Lab  Figure 1: How can 
we capture images of the human retina using a standalone, hand-held and user-interactive device? Our 
system comprises a unique co-design of a low-cost CMOS camera, programmable stimulus control, and indirect 
diffusive illumination to create an interactive and portable alternative to conventional retinal (fundus) 
imaging devices. Using this imaging platform, we add the dimension of user­interaction to the .eld of 
wireless health and tele-medicine by enabling unsupervised and ef.cient retinal imaging for measuring 
longitudinally modulating parameters within the human retina. Introduction and Overview The retina is 
a complex light­sensitive tissue that is an essential part of the human visual sys­tem. It is unique, 
as it can be optically observable with non-invasive methods through the eye s transparent elements. This 
has inspired a long history of retinal imaging devices for examination of op­tical function [Van Trigt 
1852; Yates 2011] and for diagnosis of many of the diseases that manifest in the retinal tissue, such 
as di­abetic retinophathy, hypertension, HIV/AIDS related retinitis, and age-related macular degeneration. 
These conditions are some of leading causes of blindness, especially in the developing world, but can 
often be prevented if screened and diagnosed in early stages. Unfortunately, the majority of retinal 
imaging devices employ high quality optical elements for illumination and observation and re­quire precise 
alignment with the eye [Keeler 1997; Abr` amoff et al. 2010]. Available devices are bulky, expensive, 
and virtually not ac­ cessible in developing countries. We present an inexpensive, self­directed, interactive 
device to capture and visualize images of the retina (Figure 1). For this purpose, we exploit a combination 
of two novel ideas to overcome the alignment and illumination require­ments of traditional devices. First, 
we use indirect diffuse illumi­nation in a small form factor close to the eye. This avoids the chal­lenges 
of direct pupillary illumination via focused beams. Second, we put the user in the loop and exploit the 
natural biological cou­pling of binocular vision. We use a close-up display on one eye to help the user 
self-align the pupil. In addition, we lock the gaze and focus of the same eye for convenient imaging 
of the other eye. Approach Our prototypes use ultrabright 1W dichromatic white LEDs with a luminous ef.cacy 
of 120lm/W mounted near the user s temple, to indirectly illuminate the retina through the tis­sue. The 
image of the illuminated retina is projected out through the eye, where it is captured by a camera for 
a real-time live video feed. In our standalone prototypes, which .t in a pair of modi­.ed glasses, we 
provide the user, or an observer, with a live view of the retina. To capture wide .eld-of-view retinal 
panoramas, our other prototypes use a tethered camera and software for real-time processing and image 
stitching. Our software exploits binocular coupling to gaze-lock the eyes, such that the rotation of 
the test eye can be computationally controlled through stimulus patterns shown to the display eye (see 
Fig. 1). Potential Impact and Discussion Many of the leading diseases, on a global scale, of both the 
eye and the body manifest on and within the substructures of the retinal lining. As an optical system, 
the human eye allows for direct screening using non-invasive imag­ing technology. Unfortunately, todays 
retinal imaging devices are bulky, expensive, and require highly trained ophthalmologists and specialized 
capture conditions. Our system is compact, low-cost, requires no moving parts, and utilizes a novel combination 
of indi­rect diffuse illumination through the side of the eye coupled with, a camera, and a computationally 
controlled display, which requires no special training, can be a self-examination, and enables retinal 
imaging into new form of graphic renderings. With our work, we demonstrate the ability to use computational 
imaging and interactive techniques for low-cost retinal imaging via binocular coupling and indirect illumination. 
Although these are only the .rst steps toward a robust diagnostic tool, the positive feedback we have 
received from ophthamalogists indicates great promise and many future opportunities to map graphics research 
to this important health task. We hope our work will inspire further graphics and user interaction research 
for modeling human anatomy with low-cost devices that will eventually impact global health in remote 
parts of the world. References ABR ` AMOFF, M., GARVIN, M., AND SONKA, M. 2010. Retinal imaging and image 
analysis. Biomedical Engineering, IEEE Reviews in 3, 169 208. KEELER, C. 1997. 150 years since babbage 
s ophthalmoscope. Archives of ophthal­mology 115, 11, 1456. VAN TRIGT, A. 1852. De oogspiegel. Nederlandisch 
Lancet, third series, Utrecht 1853, 417 509. YATES, PAUL ANDREW; TRAN, K., 2011. Hand-held portable fundus 
camera for screening photography, March. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los 
Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343100</article_id>
		<sort_key>550</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>41</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Relativistic ultrafast rendering using time-of-flight imaging]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343100</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343100</url>
		<abstract>
			<par><![CDATA[<p>We capture ultrafast movies of light in motion and synthesize physically valid visualizations. The effective exposure time for each frame is under two picoseconds (ps). Capturing a 2D video with this time resolution is highly challenging, given the low signal-to-noise ratio (SNR) associated with ultrafast exposures, as well as the absence of 2D cameras that operate at this time scale. We re-purpose modern imaging hardware to record an average of ultrafast repeatable events that are synchronized to a streak tube, and we introduce reconstruction methods to visualize propagation of light pulses through macroscopic scenes.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[relativistic effects]]></kw>
			<kw><![CDATA[streak sensor]]></kw>
			<kw><![CDATA[time-resolved imaging]]></kw>
			<kw><![CDATA[ultrafast optics]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737785</person_id>
				<author_profile_id><![CDATA[81490676712]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andreas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Velten]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab and Morgridge Institute, University of Wisconsin-Madison]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737787</person_id>
				<author_profile_id><![CDATA[81504685197]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Di]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab and Tsinghua University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737788</person_id>
				<author_profile_id><![CDATA[81503679670]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Adrian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jarabo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Zaragoza]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737789</person_id>
				<author_profile_id><![CDATA[81448601683]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Belen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Masia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Zaragoza]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737790</person_id>
				<author_profile_id><![CDATA[81504688659]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barsi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737791</person_id>
				<author_profile_id><![CDATA[81487645512]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Everett]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lawson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737792</person_id>
				<author_profile_id><![CDATA[81504685293]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Chinmaya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Joshi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab and College of Engineering, Pune]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737793</person_id>
				<author_profile_id><![CDATA[81100022708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Diego]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gutierrez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Zaragoza]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737794</person_id>
				<author_profile_id><![CDATA[81488671861]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Moungi]]></first_name>
				<middle_name><![CDATA[G.]]></middle_name>
				<last_name><![CDATA[Bawendi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737786</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[raskar@media.mit.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2024205</ref_obj_id>
				<ref_obj_pid>2070781</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Naik, N., Zhao, S., Velten, A., Raskar, R., and Bala, K. 2011. Single view reflectance capture using multiplexed scattering and time-of-flight imaging. <i>ACM Trans. Graph. 30</i> (Dec.), 171:1--171:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Velten, A., Willwacher, T., Gupta, O., Veeraraghavan, A., Bawendi, M. G., and Raskar, R. 2012. Recovering three-dimensional shape around a corner using ultrafast time-of-flight imaging. <i>Nature Communications 3</i>, 745.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2354820</ref_obj_id>
				<ref_obj_pid>2354409</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wu, D., O'Toole, M., Velten, A., Agrawal, A., and Raskar, R. 2012. Decomposing global light transport using time of flight imaging. In <i>Computer Vision and Pattern Recognition</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Relativistic Ultrafast Rendering Using Time-of-Flight Imaging Andreas Velten1,2 Di Wu1,3 Adrian Jarabo4 
Belen Masia4 Christopher Barsi1 Everett Lawson1 Chinmaya Joshi1,5 Diego Gutierrez4 Moungi G. Bawendi6 
Ramesh Raskar1* 1MIT Media Lab 2Morgridge Institute,University of Wisconsin-Madison 3Tsinghua University 
4University of Zaragoza 5College of Engineering, Pune 6MIT Dept. of Chemistry   Keywords: time-resolved 
imaging, ultrafast optics, relativistic ef­fects, streak sensor Introduction and Overview We capture 
ultrafast movies of light in motion and synthesize physically valid visualizations. The ef­fective exposure 
time for each frame is under two picoseconds (ps). Capturing a 2D video with this time resolution is 
highly challeng­ing, given the low signal-to-noise ratio (SNR) associated with ultra­fast exposures, 
as well as the absence of 2D cameras that operate at this time scale. We re-purpose modern imaging hardware 
to record an average of ultrafast repeatable events that are synchronized to a streak tube, and we introduce 
reconstruction methods to visualize propagation of light pulses through macroscopic scenes. Capturing 
2D movies with ps resolution, we observe complex light transport effects, including multibounce scattering, 
mirror re.ec­tions, and subsurface scattering. We notice that recorded time in­stances, i.e., camera 
times are different from the events actual times at the scene location, i.e., world times. We introduce 
the notion of time warp between these space-time coordinate systems, and rewarp the space-time movie 
for a different receiver perspec­tive, including relativistic. Approach We use 50 fs long pulses from 
a mode-locked Ti:Sapphire laser at a center wavelength of 795 nm and a 75 MHz repetition rate. The pulse 
is focused onto a (Lambertian) diffuser to create a virtual point source, illuminating the entire scene 
with a spherical pulse. All pulses are statistically identical, so we av­erage many recordings to achieve 
high SNR. The detector is fast streak sensor (Hamamatsu C5680), which is synchronized to the il­lumination 
by splitting off a portion of the beam and directing it onto a fast photodetector connected to the camera. 
The camera, which has an x-t resolution of 672×512, samples over a window of about 1 ns (i.e. less than 
2 ps per sample) records and averages the light scattered by 4.5· 108 pulses for a horizontal single 
line of the scene. A scanning mirror sweeps the line of view of the cam­era through the scene. The system 
integrates light for 6 seconds for *raskar@media.mit.edu each movie scan line. We choose this integration 
time to optimize SNR and minimize system vibrations due to fast motor and mirror movement. Alternatively 
one could use a brighter laser or increase camera gain. Results and Discussion With our capturing of 
time-resolved table-top scenes, we must drop the typical assumption that the speed of light is in.nite. 
Thus, we can examine, with much greater detail, many optical phenomena. For example, subsurface scattering 
can be computationally separated from surface scattering through dis­tinguishing different time scales 
[Wu et al. 2012]. Similar studies allow for novel methods of material acquisition [Naik et al. 2011] 
and looking around corners [Velten et al. 2012]. Further, because time-resolved information is related 
to depth, three-dimensional depth maps can be calculated, especially in cases where conven­tional stereo 
might incur problems, i.e., multi-valued pixel depths (as occurs when imaging transparent/translucent 
objects). Novel visualizations of these phenomena will be presented. Interestingly, arrival times of 
light rays depend on the camera po­sition, so we may distinguish two types of reference frames. The .rst 
is the camera time, which is the measured light propaga­tion (Figure 1, center). The second is the world 
time, i.e., the true propagation path lengths/times. By including relativistic ef­fects, we transform 
from one frame to another, and synthesize new movies from a different camera perspective. References 
NAIK, N., ZHAO, S., VELTEN, A., RASKAR, R., AND BALA, K. 2011. Single view re.ectance capture using multiplexed 
scattering and time-of-.ight imaging. ACM Trans. Graph. 30 (Dec.), 171:1 171:10. VELTEN, A., WILLWACHER, 
T., GUPTA, O., VEERARAGHAVAN, A., BAWENDI, M. G., AND RASKAR, R. 2012. Recovering three-dimensional shape 
around a corner using ultrafast time-of-.ight imaging. Nature Communications 3, 745. WU, D., O TOOLE, 
M., VELTEN, A., AGRAWAL, A., AND RASKAR, R. 2012. Decomposing global light transport using time of .ight 
imaging. In Computer Vision and Pattern Recognition. Copyright is held by the author / owner(s). SIGGRAPH 
2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343101</article_id>
		<sort_key>560</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>42</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Compressive light field photography]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343101</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343101</url>
		<abstract>
			<par><![CDATA[<p>Light field cameras, e.g. [Lytro 2012; Veeraraghavan et al. 2007], have ushered a new direction in photography allowing consumers to synthesize photographs with novel viewpoints or varying focus after the actual recording. Unfortunately, current light field camera designs impose a fixed tradeoff between spatial and angular resolution --- spatial resolution is reduced to capture angular light variation on the sensor. We introduce a principled computational framework and a new camera design to acquire and reconstruct light fields at full spatial and angular resolution from a single exposure. Our framework introduces a high-dimensional sparse basis for light fields learned from millions of light fields patches. The same optimization procedure also allows for the synthesis of optimal mask patterns that are mounted at a slight offset in front of the sensor and optically attenuate the light field before it is recorded by the sensor. Finally, a weighted compressive sensing-style reconstruction is performed to recover the light field. We demonstrate, in theory and with simulations, how our compressive approach to light field photography outperforms state-of-art techniques.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737795</person_id>
				<author_profile_id><![CDATA[81504688520]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kshitij]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marwah]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737796</person_id>
				<author_profile_id><![CDATA[81335499586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wetzstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737797</person_id>
				<author_profile_id><![CDATA[81300333501]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ashok]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Veeraraghavan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rice University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737798</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[E. Candes and Y. Eldar and D. Needell and P. Randall. 20010. Compressed sensing with coherent and redundant dictionaries. <i>Harmonic Analysis</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lytro, I., 2012. Lytro Light Field Camera.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1553463</ref_obj_id>
				<ref_obj_pid>1553374</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Mairal, J., Bach, F., Ponce, J., and Sapiro, G. 2009. Online dictionary learning for sparse coding. <i>International Conference on Machine Learning</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276463</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Veeraraghavan, A., Raskar, R., Agarwal, A., Mohan, A., and Tumblin, J. 2007. Dappled photography: Mask enhanced cameras for heterodyned light fields and coded aperture refocusing. <i>ACM Siggraph</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Compressive Light Field Photography Kshitij Marwah1 Gordon Wetzstein1 Ashok Veeraraghavan2 Ramesh Raskar1 
1MIT Media Lab 2Rice University Figure 1: Compressive light .eld acquisition using joint optical light 
coding and compressive computational reconstruction. Using machine learning techniques, we extract the 
essence of natural light .elds from archives and store them in overcomplete dictionaries (left). The 
same optimization procedure also generates optimal attenuation mask patterns that are physically mounted, 
at a slight offset, in front of a camera sensor (center left). These masks create a dappled pseudo-random 
pattern in captured photographs (center). From a precomputed dictionary and a single sensor image, we 
then reconstruct a full-resolution light .eld that contains slightly different viewpoints of a scene 
(right) and allows a photograph to be refocused after the fact (see supplement). Introduction and Overview 
Light .eld cameras, e.g. [Lytro 2012; Veeraraghavan et al. 2007], have ushered a new direction in photography 
allowing consumers to synthesize photographs with novel viewpoints or varying focus after the actual 
recording. Un­fortunately, current light .eld camera designs impose a .xed trade­off between spatial 
and angular resolution spatial resolution is reduced to capture angular light variation on the sensor. 
We in­troduce a principled computational framework and a new camera design to acquire and reconstruct 
light .elds at full spatial and an­gular resolution from a single exposure. Our framework introduces 
a high-dimensional sparse basis for light .elds learned from mil­lions of light .elds patches. The same 
optimization procedure also allows for the synthesis of optimal mask patterns that are mounted at a slight 
offset in front of the sensor and optically attenuate the light .eld before it is recorded by the sensor. 
Finally, a weighted compressive sensing-style reconstruction is performed to recover the light .eld. 
We demonstrate, in theory and with simulations, how our compressive approach to light .eld photography 
outper­forms state-of-art techniques. Sparse Coding of Light Fields Light .elds can be thought of as 
a collection of slightly different perspectives of the same scene that vary over the size of the camera 
aperture. As these viewpoints have a very narrow baseline, they contain a large amount of redundancy. 
This redundancy can be computationally exploited using a variety of different bases commonly used for 
redundant or sparse coding, including data-independent bases such as Fourier transforms, con­tourlets, 
bandelets, and Hadamard codes as well as data-dependent functions such as principal components or overcomplete 
dictionar­ies. In the signal processing literature, learned overcomplete dic­tionaries are commonly considered 
the best approach for compres­sive sensing [Mairal et al. 2009]. We consider millions of light .eld patches 
from various light .eld archives that each encode the essence of depth variations and occlusions in natural 
scenes. This basis is about 100× overcomplete in which each light .eld patch is represented as a linear 
combination of a few dictionary elements. Compressive Reconstruction By placing a mask on the sen­sor 
we optically modulate the light rays incident on the sensor; the mask creates a dappled pattern in the 
sensor image. The pattern of modulation is learned simultaneously with the dictionary to be the most 
incoherent physically probable sensing matrix for capture. To reconstructing the light .eld back we employ 
a weighted L1­minimization (see [E. Candes and Y. Eldar and D. Needell and P. Randall 20010] for more 
details) on the dappled sensor image as: arg min a 1 (1) a s.t. Isensor - FM Da 2 = , where FM is the 
measurement matrix modeling the attenuation mask and angular integration of the sensor, Isensor is the 
captured sensor image, D is the overcomplete dictionary, and a is the un­known vector with sparse coef.cients. 
The light .eld is represented as the combination of a sparse set of coef.cients and corresponding elements 
in the dictionary Da. Results and Discussion We learned an overcomplete dictionary for patch size 16 
× 16 in space and 5 × 5 viewpoints from synthetic light .elds rendered with a raytracer and also reparameterized 
light .elds from the Stanford Light Field Archive. Our dictionary con­tains about 100,000 elements with 
each patch being represented in no more than ten coef.cients. We tested our reconstruction algo­rithm 
using a new light .eld that was not part of the training dataset, which is simulated to be captured with 
the learned mask pattern in front of the sensor (Fig. 1). The sensor image is then used to recover the 
light .eld, which includes multiple viewpoints and can be used to refocus a photograph, at a higher resolution 
than conventional methods, after it is captured (see supplement). Our approach has the potential to signi.cantly 
improve the resolu­tion of next-generation computational cameras, which are already emerging on the consumer 
market. References E. CANDES AND Y. ELDAR AND D. NEEDELL AND P. RANDALL. 20010. Com­pressed sensing with 
coherent and redundant dictionaries. Harmonic Analysis. LYTRO, I., 2012. Lytro Light Field Camera. MAIRAL, 
J., BACH, F., PONCE, J., AND SAPIRO, G. 2009. Online dictionary learn­ing for sparse coding. International 
Conference on Machine Learning. VEERARAGHAVAN, A., RASKAR, R., AGARWAL, A., MOHAN, A., AND TUMBLIN, J. 
2007. Dappled photography: Mask enhanced cameras for heterodyned light .elds and coded aperture refocusing. 
ACM Siggraph. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 
5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2343102</section_id>
		<sort_key>570</sort_key>
		<section_seq_no>15</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Priming the pipe]]></section_title>
		<section_page_from>15</section_page_from>
	<article_rec>
		<article_id>2343103</article_id>
		<sort_key>580</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>43</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Crom]]></title>
		<subtitle><![CDATA[massively parallel, CPU/GPU hybrid computation platform for visual effects]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343103</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343103</url>
		<abstract>
			<par><![CDATA[<p>Crom (Core Rhythm Operating Machine) was designed to provide a flexible, extensible platform on which to develop our next generation of visual effects tools. At its heart were three key design decisions: utilize a strongly-typed dependency graph where the functional pieces are responsible for producing/manipulating a single value type, and the connective tissue handles necessary type conversions; separate functionality and value storage so a single functional piece can operate on disparate tasks without losing the advantage of a persistent cache; and make the application extensible in both functionality and interface through all levels of production development.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737799</person_id>
				<author_profile_id><![CDATA[81100618982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cournia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[cournia@rhythm.com]]></email_address>
			</au>
			<au>
				<person_id>P3737800</person_id>
				<author_profile_id><![CDATA[81504685474]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bradley]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Smith]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[basmith@rhythm.com]]></email_address>
			</au>
			<au>
				<person_id>P3737801</person_id>
				<author_profile_id><![CDATA[81328490602]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Spitzak]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[spitzak@rhythm.com]]></email_address>
			</au>
			<au>
				<person_id>P3737802</person_id>
				<author_profile_id><![CDATA[81504687919]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Casey]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vanover]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[cvanover@rhythm.com]]></email_address>
			</au>
			<au>
				<person_id>P3737803</person_id>
				<author_profile_id><![CDATA[81100029745]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hans]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rijpkema]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hans@rhythm.com]]></email_address>
			</au>
			<au>
				<person_id>P3737804</person_id>
				<author_profile_id><![CDATA[81504685711]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Josh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tomlinson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[josht@rhythm.com]]></email_address>
			</au>
			<au>
				<person_id>P3737805</person_id>
				<author_profile_id><![CDATA[81504686717]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Nathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Litke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[nlitke@rhythm.com]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Crom -Massively Parallel, CPU/GPU Hybrid Computation Platform for Visual Effects Nathan Cournia Bradley 
Smith Bill Spitzak Rhythm &#38; Hues Rhythm &#38; Hues Rhythm &#38; Hues cournia@rhythm.com basmith@rhythm.com 
spitzak@rhythm.com  Casey Vanover Hans Rijpkema Rhythm &#38; Hues Rhythm &#38; Hues cvanover@rhythm.com 
hans@rhythm.com  1 Introduction Crom (Core Rhythm Operating Machine) was designed to provide a .exible, 
extensible platform on which to develop our next gener­ation of visual effects tools. At its heart were 
three key design de­cisions: utilize a strongly-typed dependency graph where the func­tional pieces are 
responsible for producing/manipulating a single value type, and the connective tissue handles necessary 
type con­versions; separate functionality and value storage so a single func­tional piece can operate 
on disparate tasks without losing the ad­vantage of a persistent cache; and make the application extensible 
in both functionality and interface through all levels of production development. 2 Dependency Graph 
 The core of Crom is a dependency graph divided into three primary pieces: nodes, plugs, and contexts. 
Contexts describe the domain of the computation (frame number, tile number, stereo eye, etc), nodes compute 
property values, and plugs provide transportation of said values. This division provides for two of our 
primary conceits: The contexts allow for a stateless design where nodes are passed, then promptly forget, 
the scope of their computation; and plugs provide a translation mechanism which both simpli.es node design 
and re­duces the required number of nodes by eliminating permutations of similar functions. 3 Interface 
 Like the dependency graph, the user interface is designed for .exi­bility. Rather than provide customizations, 
or a selection of layouts, Crom provides a library of panels which the user may dock, resize, and .oat 
as he/she deems necessary. This .exibility even extends to the manipulation of individual parameters 
where users may often toggle between a variety of widgets (or, if they re technically savvy enough, write 
their own). The interface is also decoupled from the dependency graph. Com­putations are performed asynchronously 
with the values displayed only when ready. This allows the interface to display expensive calculations 
without the loss of responsiveness. It also has the ca­pacity to provide easy transition to a cloud computing 
paradigm in situations where that might prove useful. 4 Hybrid GPU/CPU Compositor Although Crom was 
not written for a particular application, its .rst production use is as a compositor. Where possible, 
nodes leverage on the power of the GPU passing down not image data, but shader functions that can be 
compiled to process individual pixels. This al­lows computation to be deferred until actually required, 
and allows the resulting node tree to be effectively optimized prior to Josh Tomlinson Nathan Litke Rhythm 
&#38; Hues Rhythm &#38; Hues josht@rhythm.com nlitke@rhythm.com  Figure 1: Using Crom as a compositor 
 computation. The result is a node graph that effectively performs as a visual computing language. Where 
operation on the CPU is necessary (or preferable), plugs handle texture to buffer conversions so data 
is passed between the two devices seamlessly. Where no GPU is present, the application uses OpenCL to 
shift all functions to CPU-only processing. 5 Extensibility Crom is designed to be an open platform. 
Panels, and node and plug interfaces can all be written in Python as can the nodes them­selves. Users 
can generate macros which bundle together a network of nodes into a single tool, with the power to create 
and customize the interface. Those tools can then be uploaded to our app store, Anvil, and installed 
on a per-person or per-show basis. The user-level of development has found particular usage in the compositor. 
Because Crom optimizes the result of the node graph, there s very little performance difference between 
a compiled node and a macro providing the same functionality. As a result, we re able to provide a relatively 
small selection of core nodes, and have users create more speci.c functionality out of them. This allows 
the compositor to be easier to maintain and operate in much the same way reduced instruction set computing 
does. 6 Conclusion Crom is an application informed by many years of visual effects experience. At its 
core, it s fast, highly parallelized, and memory ef.cient. On its surface, it s extremely .exible. It 
s a tool designed to evolve through usage and leverage on every resource available both in terms of computation 
and development. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, 
August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343104</article_id>
		<sort_key>590</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>44</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Efficient large-scale hybrid fluid simulation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343104</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343104</url>
		<abstract>
			<par><![CDATA[<p>State-of-the-art methods for fluid simulation, including velocity-based grid methods and smoothed particle hydrodynamics [Bridson and M&#252;ller-Fischer 2007], require a detail vs. domain size tradeoff. As a result, scenes with large spatial scales are restricted to coarse detail under the restriction of limited computational resources. The elliptic problems solved for incompressibility projection in these simulations are <i>bandwidth-bound</i>, since domains of interest are not cache resident on current generation hardware. As a result, limited optimization is possible, and interactive performance is not possible for medium-large scenes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737806</person_id>
				<author_profile_id><![CDATA[81448592792]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Abhinav]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Golas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737807</person_id>
				<author_profile_id><![CDATA[81100232335]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Rahul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Narain]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737808</person_id>
				<author_profile_id><![CDATA[81342510602]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sewall]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737809</person_id>
				<author_profile_id><![CDATA[81481650071]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Pavel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Krajcevski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737810</person_id>
				<author_profile_id><![CDATA[81452602436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1281681</ref_obj_id>
				<ref_obj_pid>1281500</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bridson, R., and M&#252;ller-Fischer, M. 2007. Fluid simulation: Siggraph 2007 course notes. In <i>ACM SIGGRAPH 2007 courses</i>, ACM, SIGGRAPH '07, 1--81.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Cottet, G. H., and Koumoutsakos, P. D. 1998. <i>Vortex Methods: Theory and Practice</i>. Cambridge University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Ef.cient Large-Scale Hybrid Fluid Simulation Abhinav Golas1 Rahul Narain2 Jason Sewall3 Pavel Krajcevski1 
Ming Lin1 1University of North Carolina at Chapel Hill, 2 University of California, Berkeley, 3 Intel 
Corporation 1 Introduction State-of-the-art methods for .uid simulation, including velocity­based grid 
methods and smoothed particle hydrodynamics [Bridson and M¨ uller-Fischer 2007], require a detail vs. 
domain size tradeoff. As a result, scenes with large spatial scales are restricted to coarse detail under 
the restriction of limited computational resources. The elliptic problems solved for incompressibility 
projection in these simulations are bandwidth-bound, since domains of interest are not cache resident 
on current generation hardware. As a result, limited optimization is possible, and interactive performance 
is not possible for medium-large scenes. An alternate approach is to model .uid vorticity, i.e. the curl 
of the velocity .eld using vortex singularities, which can be compactly represented using particle systems 
[Cottet and Koumoutsakos 1998]. These methods are (1) free of numerical dissipation, which can be signi.cant 
with grid-based methods, (2) offer computational savings for scenes with unbounded .uid, and (3) have 
better compute to I/O ratios as well as trivially parallel algorithms. However, robust mod­eling of free-surfaces 
or ef.cient modeling of deformable obstacles with two-way coupling using vorticity methods is challenging. 
We present an ef.cient vortex particle implementation capable of inter­active smoke simulations, and 
a novel hybrid algorithm coupling these methods with existing grid methods for robust and ef.cient modeling 
of obstacles and free-surfaces. 2 Ef.cient Vortex Particle Simulation A vortex particle simulation is 
de.ned by a particle system where each particle carries a vorticity ., and induces a velocity computed 
using the Biot-Savart law, the velocity at any point in space being the sum of all induced velocities. 
However, the Biot-Savart kernel is singular, and therefore unstable, hence a smoothed version of this 
kernel is used in practice. We use the Rosenhead-Moore kernel, which uses polynomial smoothing controlled 
by a smoothing radius. More details about vortex particle methods can be obtained from [Cottet and Koumoutsakos 
1998]. Interactive vortex particle simulation can be obtained using a careful combination of data layout, 
optimal algorithms, and ef.cient uti­lization of computational resources. The particle system underlying 
our simulation is laid out as a struct-of-arrays, making it suitable for streaming operations. There 
are 3 major computational kernels, convection, strength exchange, and obstacle avoidance. For convec­tion, 
naive summation is faster for small-medium scenes, while for large scenes (> 40000 particles), a hierarchical 
method is used. Strength exchange models viscous and shear components of the Navier Stokes equations 
by pair-wise exchange of vorticity. This is accelerated by the underlying spatial hierarchy used for 
convection. To model obstacles, we use the panel method, where vortex elements are placed on the obstacle 
mesh, and their strengths are determined as a solution to a linear system enforcing zero normal .ux through 
the mesh. For rigid obstacles, this linear system can be precomputed, resulting in computational savings. 
 3 Hybrid Fluid Simulation 2-way coupled free-surfaces or deformable objects are not possible in this 
formulation since pressure boundary conditions are non-trivial. To remedy this, we couple vortex methods 
with existing grid-based .uid simulations with domain decomposition, where such surfaces Figure 1: (top) 
Flooding a city, yellow surface denotes the vortex domain, remaining .uid simulated using grid methods 
(bottom) 4­core computation time, with inset. Note: > 2000fps performance for < 1600 particles are modeled 
using grids, while the rest of the scene uses vortex meth­ods, as illustrated in 1(top). A consistent 
incompressible velocity .eld across domain boundaries is obtained by a novel iterative cou­pling algorithm. 
Velocity from each domain is used as a boundary condition for other overlapping domains, as an incompressibility 
projection step for grids, or as a .ux matching panel method for vortex methods. Advection of vorticity 
into the vortex domain is accounted for by creation of vortex particles in the overlap region. Vortex 
particles are also allowed to go through grid domains where they can be used for vorticity con.nement. 
This is an improvement over existing con.nement approaches which use noise to generate vorticity, since 
.uid vorticity is used to maintain detail. 4 Results Using our hybrid algorithm, free surfaces can be 
modeled with vortex methods, and deformable obstacles can be modeled with an O(n 2) algorithm, as opposed 
to the naive O(n 6) algorithm, n being the number of mesh faces. Our approach maintains .uid detail, 
and gives speedups ranging from 2X (water) to 1000X and beyond (smoke) as compared to comparable grid 
simulations. Our vortex particle implementation can simulate smoke in game-like scenarios at real-time 
rates, using 500-1000 vortex particles instead of much larger grids. References BRIDSON, R., AND M¨ 
ULLER-FISCHER, M. 2007. Fluid simulation: Siggraph 2007 course notes. In ACM SIGGRAPH 2007 courses, ACM, 
SIGGRAPH 07, 1 81. COTTET, G. H., AND KOUMOUTSAKOS, P. D. 1998. Vortex Methods: Theory and Practice. 
Cambridge University Press. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, 
August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2343105</section_id>
		<sort_key>600</sort_key>
		<section_seq_no>16</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Effects omelet]]></section_title>
		<section_page_from>16</section_page_from>
	<article_rec>
		<article_id>2343106</article_id>
		<sort_key>610</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>45</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Wrath of the Titans]]></title>
		<subtitle><![CDATA[complex models with voxel greeble]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343106</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343106</url>
		<abstract>
			<par><![CDATA[<p>Our work on Wrath Of The Titans featured an environment on a massive scale, much of which would be destroyed or interacted with by an equally massive Titan. Shots contained multiple models with hundreds of millions of polygons each, which then needed to be shattered, whilst retaining all their detail, in an artdirectable fashion. Techniques involving voxelised geometry and Ptex displacements were devised. These methods allowed for the procedural creation and reconstruction of very heavy models, without loss of detail, that could still be modified in a sculpting package after the procedural step.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737811</person_id>
				<author_profile_id><![CDATA[81421601551]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seddon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Method Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737812</person_id>
				<author_profile_id><![CDATA[81504688592]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zaveckas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Method Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737813</person_id>
				<author_profile_id><![CDATA[81504685368]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kirk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Method Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Wrath Of The Titans -Complex Models With Voxel Greeble Daniel Seddon, Tomas Zaveckas, James Kirk Method 
Studios 1 Introduction Our work on Wrath Of The Titans featured an environment on a massive scale, much 
of which would be destroyed or interacted with by an equally massive Titan. Shots contained multiple 
models with hundreds of millions of polygons each, which then needed to be shattered, whilst retaining 
all their detail, in an art­directable fashion. Techniques involving voxelised geometry and Ptex displacements 
were devised. These methods allowed for the procedural creation and reconstruction of very heavy models, 
without loss of detail, that could still be modified in a sculpting package after the procedural step. 
 Figure 1: Stills of detailed environments from Wrath of the Titans 2 SDF Volumes Initially, high resolution 
sculpts were made, usually of several million polygons and up to about eight million. These models were 
then voxelised at resolutions high enough to capture all the detail. A low-resolution SDF representation 
of each model was also generated. These SDF volumes are insufficient for capturing fine surface detail, 
but will be used as a starting point from which to project high-resolution detail. 3 Projecting To Ptex 
Because storing a high resolution version of the model in memory would be prohibitive and would involve 
the equivalent of billions, if not trillions, of voxels, the model will be built by dividing the hi-res 
volume into tiled cells to be processed independently. The lo-res SDF's bounds are divided up into containers, 
from which the faces of a low-res quad mesh are partitioned. For each container, a high resolution volume 
slice is instantiated and used to project its detail onto the container s faces. The high detail volumes 
are projected on to the low resolution surface by progressively subdividing the surface and snapping 
to the isocontour. This ensures a good surface distribution and guides the high resolution surface onto 
the high resolution SDF. Figure 3. Projection stages, from low to high resolution surface. If done crudely 
by moving along the SDF gradient, this method can result in missed surface detail. Thin protruding regions 
are ignored because sinks in the SDF create discontinuities in the gradient field, causing a jump from 
one point on the isocontour to another on the other side of the feature. A more elegant solution is to 
march along the isocontour from the vertices of the previous iteration until an approximate mid-point 
is reached.  The output from the tiling process is a set of high resolution chunks of the subdivided 
mesh. These can be combined together to create a very high resolution surface that captures all of the 
detail of the instanced high resolution volumes. It forms one continuous surface, derived from the low 
resolution input mesh. This data is now baked down to a Ptex displacement map, convenient because it 
does not require parametrization of the procedural mesh. From there, the lo-res mesh and Ptex can be 
take in to sculpting packages and further edited. Copyright is held by the author / owner(s). SIGGRAPH 
2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343107</article_id>
		<sort_key>620</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>46</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Wrath of the Titans]]></title>
		<subtitle><![CDATA[creating CG lava with advected sculpts]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343107</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343107</url>
		<abstract>
			<par><![CDATA[<p>One of the many challenges on Wrath Of the Titans was to have controllable CG lava emerge from one of the heroes' arms and advance forward through a vast stone chamber, increasing in volume as it went. We first simulated the lava, building a system to emulate the subtleties of the lava's movement, and then advected surface detail along with the fluid motion.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737814</person_id>
				<author_profile_id><![CDATA[81421601551]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seddon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Method Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737815</person_id>
				<author_profile_id><![CDATA[81504683930]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[LeTarte]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Method Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737816</person_id>
				<author_profile_id><![CDATA[81504685368]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kirk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Method Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Wrath Of The Titans -Creating CG Lava With Advected Sculpts Daniel Seddon, Daniel LeTarte, James Kirk 
Method Studios 1 Introduction One of the many challenges on Wrath Of the Titans was to have controllable 
CG lava emerge from one of the heroes arms and advance forward through a vast stone chamber, increasing 
in volume as it went. We first simulated the lava, building a system to emulate the subtleties of the 
lava s movement, and then advected surface detail along with the fluid motion. We rely a lot on Ptex 
for the storage of sculpted vector displacements this seemed ideal for the lava, where creating UV coordinates 
was complex and time consuming. To support Ptex, we created a surfacing algorithm based around marching 
cubes, but instead using octahedrons, guaranteeing that all faces would be quads (recommended for Ptex). 
 Figure 1: Results of our sculpt advection method. 2 Simulation Simulations were designed with the necessity 
of outputting a persistent scalar surface field and a velocity field that was detailed enough to exhibit 
fluid motion. To achieve this, the lava was simulated with both level-set and particle based solvers 
depending on the level of control required. In addition to high viscosity and surface tension properties, 
areas of flow that strayed too far from artist defined paths received speed and temperature penalties, 
which resulted in direct modification to the simulation's collision field. Adjustments to the collision 
field mid-simulation yielded unique emergent behavior, as lava could melt through rock or cool and harden 
to form barriers along its route. Figure 2: Editing the collision field based on lava temperature. 
3 Surfacing Marching Octohedrons The final result of our fluid simulations was an SDF (signed distance 
field) used to build a mesh. Typically, a marching cubes algorithm is employed for this, but we implemented 
a similar method based around octahedrons. Octahedrons have the advantage that all of the valances are 
four where faces meet. This means that cutting an octahedron always results in faces that can be built 
from quads, suitable for sculpting with Ptex.  4 Advecting Sculpts Once we created a surface, our aim 
was to sculpt it to give the appearance of lava. We chose one or more keyframes and added surface detail 
in 3D Coat, a sculpting package which includes Ptex support. Using Ptex removed the need to create UV 
coordinates on complex procedural surfaces. Moving the sculpt with the lava was done by advecting the 
mesh itself back through the simulation. As a result of the surfacing, every frame would have a unique 
mesh with different topology. Each of those meshes would be advected backwards through each of the previous 
frames. To do this efficiently, all meshes for all frames were kept in memory. The SDFs were then read 
in back to front, surfacing and advecting each mesh as they went. Once each mesh was advected back to 
the keyframe, the position of each vertex at this frame was recorded as a rest position that could be 
used as a spatial lookup in to the sculpt at that frame. Each mesh would be pushed along the velocity 
field of the current frame then pushed down on to the isocontour of the SDF of the previous frame, to 
the nearest point on the fluid surface. This is important as we are advecting surface detail, not a procedural 
texture, so we want the advection to remain on the surface. Advecting back for every frame in this way 
means you are working out the origin of each point on the keyframe many points can map back to a single 
origin, as they often do. As a result of this our approach deals with issues like branching and the creation 
of new surface area. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, 
August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343108</article_id>
		<sort_key>630</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>47</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Multiresolution radiosity caching for global illumination in movies]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343108</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343108</url>
		<abstract>
			<par><![CDATA[<p>We describe a multiresolution radiosity caching method that enables efficient computation of global illumination (GI) in a single pass in complex CG movie production scenes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737817</person_id>
				<author_profile_id><![CDATA[81504682633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Per]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Christensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737818</person_id>
				<author_profile_id><![CDATA[81504687620]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Harker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737819</person_id>
				<author_profile_id><![CDATA[81504686170]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shade]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737820</person_id>
				<author_profile_id><![CDATA[81100295748]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Brenden]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schubert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737821</person_id>
				<author_profile_id><![CDATA[81504687992]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Dana]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Batali]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Christensen, P., Fong, J., Laur, D., and Batali, D. 2006. Ray tracing for the movie 'Cars'. In <i>Proc. IEEE Symposium on Interactive Ray Tracing 2006</i>, 1--6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Christensen, P. 2008. Point-based approximate color bleeding. Tech. Rep. 08-01, Pixar Animation Studios.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Fajardo, M., 2010. Ray tracing solution for film production rendering. In {K&#345;iv&#225;nek et al. 2010}.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[K&#345;iv&#225;nek, J., et al. 2010. Global illumination across industries. <i>SIGGRAPH 2010 Course Notes</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015748</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Tabellion, E., and Lamorlette, A. 2004. An approximate global illumination system for computer generated films. In <i>Proc. SIGGRAPH 2004</i>, 469--476.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Multiresolution Radiosity Caching for Global Illumination in Movies c Figure 1: Global illumination 
images from Monsters U. These images render more than 30× faster with radiosity caching. &#38;#169; Disney/Pixar. 
1 Introduction We describe a multiresolution radiosity caching method that en­ables ef.cient computation 
of global illumination (GI) in a single pass in complex CG movie production scenes. For ray-traced GI 
in scenes with complex geometry and shaders, the bottleneck is not the raw ray tracing (acceleration 
structure traversal and ray hit tests), but the evaluation of displacement, light source, and surface 
shaders at the ray hit points. We reduce this time by dividing the shader calculations into view-dependent 
and view-independent components, and caching the view-independent part (radiosity) needed for GI. During 
distribution ray tracing these radiosities are computed on demand and reused many times. The cache contains 
multiple resolutions of the radiosity on the surface patches in the scene; the resolution for each cache 
lookup is selected using ray differentials. Our GI method is implemented in PRMan and used for interactive 
material/lighting design and .nal rendering at several studios. 2 Global Illumination in Movies The 
most widely used GI methods in CG movies and special ef­fects are distribution ray tracing, path tracing, 
and point-based GI [K.riv´ anek et al. 2010]. Tabellion and Lamorlette [2004] compute direct illumination 
and store it as texture maps, and then use distribution ray tracing to compute 1-bounce GI. Path tracing 
[Fajardo 2010] is a brute-force single-pass method that provides fast feedback during interactive lighting 
design. Its main disadvantages are slow convergence to a noise-free .nal image and unsuitability for 
caching of shading re­sults (each shading result is very noisy so cannot be reused without introducing 
signi.cant bias). Point-based GI [Christensen 2008] is fast and noise-free and requires few shader evaluations, 
but due to its multipass nature, it is not ideal for interactive design. Our caching method combines 
the best traits: it is single-pass (suit­able for interactive design), needs few shader evaluations due 
to caching, handles multiple bounces, converges fast, and has .xed cache size and no .le I/O. 3 The 
Multiresolution Radiosity Cache The radiosity cache contains surface patch radiosities at three dif­ferent 
resolutions. The .ne resolution contains radiosities for all the micropolygon vertices of a patch; the 
medium and coarse reso­lutions contain a subset of those. The implementation is similar to our multiresolution 
tessellation cache [Christensen et al. 2006]. We use ray differentials to select the cache resolution 
for a ray hit. Lookups in the .ne cache are coherent; lookups in the medium and coarse caches are less 
coherent, but the capacity of those caches is higher since each entry is smaller. Hence, high cache hit 
rates are obtained for all resolutions. If the cache does not contain an en­try for the requested patch/side/resolution/diffusedepth, 
the view­independent part of the surface shader is run (along with the dis­placement and light source 
shaders) at the patch vertices, and the resulting radiosities are stored. Then the radiosity values near 
the hit point are interpolated. 4 Global Illumination Examples Figure 1 shows GI in three movie scenes 
with motion blur. The im­ages are 1920×1080 pixels and were rendered using 4 threads on a 2.9 GHz machine 
with 12 GB memory. The direct illumination consists of 2 lights (sun and dome) for the exterior scene 
and 20 lights for the interior scenes. There are 13, 64, and 15 million shad­ing points, respectively. 
The indirect illumination is rendered with 256 1024 diffuse rays from each shading point. The radiosity 
cache size is set to 256 MB; peak total memory is 16, 11 and 6 GB. The render times are 1.5, 4.0, and 
1.1 hours. Without radiosity caching the render times are 33, 41, and 94 times longer. 5 Other Caching 
Applications Multiresolution caches are also used to store shader opacity results for faster ray-traced 
shadows, ambient occlusion, and volume ex­tinction, and to store irradiance for ray-traced subsurface 
scattering. More details at: graphics.pixar.com/library/RadiosityCaching. Two videos illustrate interactive 
changes of viewpoint, illumina­tion, and shader parameters while rendering GI and subsurface scat­tering 
with progressive ray tracing. References CHRISTENSEN, P., FONG, J., LAUR, D., AND BATALI, D. 2006. Ray 
tracing for the movie Cars . In Proc. IEEE Symposium on Interactive Ray Tracing 2006, 1 6. CHRISTENSEN, 
P. 2008. Point-based approximate color bleeding. Tech. Rep. 08-01, Pixar Animation Studios. FAJARDO, 
M., 2010. Ray tracing solution for .lm production rendering. In [K.riv´anek et al. 2010]. K.ANEK, J., 
ET AL. 2010. Global illumination across industries. SIGGRAPH 2010 RIV ´ Course Notes.  TABELLION, E., 
AND LAMORLETTE, A. 2004. An approximate global illumination system for computer generated .lms. In Proc. 
SIGGRAPH 2004, 469 476. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, 
August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2343109</section_id>
		<sort_key>640</sort_key>
		<section_seq_no>17</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Image playground]]></section_title>
		<section_page_from>17</section_page_from>
	<article_rec>
		<article_id>2343110</article_id>
		<sort_key>650</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>48</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Crowd sourcing memory colors for image enhancement]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343110</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343110</url>
		<abstract>
			<par><![CDATA[<p>Memory colors refer to those colors recalled in association with familiar objects [Hering 1961]. The deficiency with existing research in this area is that a) screen memory colors have not been rigorously established and b) existing studies do not include extensive human judgements when evaluating image edits based on memory colors. We first perform a context-free perceptual experiment to establish the overall distributions of screen memory colors for three pervasive objects (skin, sky and grass). Then, we use a context-based experiment to locate the most representative memory colors. Finally, we show a simple, yet effective, application using representative memory colors to enhance digital images.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737822</person_id>
				<author_profile_id><![CDATA[81504687043]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Su]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xue]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yale University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[su.xue@yale.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737823</person_id>
				<author_profile_id><![CDATA[81500658478]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ann]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McNamara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ann@viz.tamu.edu]]></email_address>
			</au>
			<au>
				<person_id>P3737824</person_id>
				<author_profile_id><![CDATA[81100255828]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Holly]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rushmeier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yale University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737825</person_id>
				<author_profile_id><![CDATA[81100369597]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Julie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dorsey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yale University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bartleson, C. J. 1960. Memory colors of familiar objects. <i>J. Opt. Soc. Am. 50</i>, 1 (Jan), 73--77.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hering, E. 1961. Principles of a new theory of the color sense. <i>In R. C. Teevan &amp; R. C. Birney (Eds.) Color vision: An enduring problem in psychology: Selected readings</i>, 28--39.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Crowd Sourcing Memory Colors For Image Enhancement Su Xue*1 , Ann McNamara 2 , Holly Rushmeier1, Julie 
Dorsey1 1Department of Computer Science, Yale University, USA, 2Department of Visualization, Texas A&#38;M 
University, USA 1 Introduction Memory colors refer to those colors recalled in association with fa­miliar 
objects [Hering 1961]. The de.ciency with existing research in this area is that a) screen memory colors 
have not been rigorously established and b) existing studies do not include extensive human judgements 
when evaluating image edits based on memory colors. We .rst perform a context-free perceptual experiment 
to establish the overall distributions of screen memory colors for three pervasive objects (skin, sky 
and grass). Then, we use a context-based experi­ment to locate the most representative memory colors. 
Finally, we show a simple, yet effective, application using representative mem­ory colors to enhance 
digital images. 2 Establishing and Applying Memory Colors We repeated the memory color experiment of 
[Bartleson 1960] us­ ing digitized color chips based on the Lu * v * color gamut over a uniform neutral 
gray background, participants were crowd sourced. Lu * v * values were converted to sRGB for display. 
No attempt to detect or adjust for the type, size or settings of the monitors of the participants was 
made. This was deliberate to enable discovery of the most general representative memory colors. Figure 
1 left, shows the results. We then use three 2D Gaussian functions to approxi­mate these elliptically 
shaped distributions, Figure 1 (right). Due to the nature of crowd sourcing, a wide range of participants, 
cul­tures, monitors, viewing conditions, and etc, are sampled. Despite all the potential variations in 
the stimuli, we .nd that the results are very close to those from existing psychophysical experiments, 
which were executed under controlled settings. This is a nice re­sult as it validates the usage of crowd 
sourcing techniques for color evaluation. We then take natural images that include regions of skin, sky 
and grass, and generate a set of stimuli by shifting the colors of individual regions toward a few candidate 
colors (based on the memory colors established in our .rst experiment). Then, we ask viewers to rate 
these manipulated images, which contain different combinations of candidate colors. The ratings reveal 
hu­mans preferences for the joint distribution of three memory colors in the context of natural images. 
In this manner we thereby locate the representative memory colors based on these preferences. As before, 
we use crowd sourcing to elicit the judgements on stimuli, while keeping results as general as possible. 
Participants rate im­ages based only on the quality of the color reproduction, ignoring all other image 
elements such as content, composition etc. The gen­erally high positive correlations demonstrate that, 
despite different *e-mail:su.xue@yale.edu e-mail:ann@viz.tamu.edu Figure 1: Left: Responses from crowd 
sourced workers: red crosses for skin, blue for sky, &#38; black for grass. Right: Fitted Gaus­sian distributions 
for three memory colors on u * v * color space. image contexts, the preferences to memory colors are 
rather consis­tent. Application: Given a raw photo as input, people often seek a sim­ple solution to 
making the component colors more pleasing. Color can be manipulated in many ways, the domain of possibilities 
is overwhelming making this a challenging task for non-expert users. Notably, the representative memory 
colors, which we located us­ing human preference via context-free and context-based experi­ments, serve 
promisingly as candidate standards for color repro­duction. For images containing regions of memory objects, 
arbi­trary color correction is converted to 1D color shifting: moving col­ors of memory objects toward 
(or away from) a target, that is, the representative memory color. In conclusion, we have introduced 
a simple but effective, technique based on this observation. We found, through a third experiment, that 
several scenarios exist (for example Figure 2), in which color shifts can greatly enhance the perceived 
color reproduction of images. (please see supplemental information for more examples). Figure 2: Left: 
Original. Right: Our Technique. In this case, par­ticipants preferred the corrected image which shifted 
skin regions toward the representative memory color values References BARTLESON, C. J. 1960. Memory 
colors of familiar objects. J. Opt. Soc. Am. 50, 1 (Jan), 73 77. HERING, E. 1961. Principles of a new 
theory of the color sense. In R. C. Teevan &#38; R. C. Birney (Eds.) Color vision: An enduring problem 
in psychology: Selected readings, 28 39. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los 
Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343111</article_id>
		<sort_key>660</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>49</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Calligraphic cutting]]></title>
		<subtitle><![CDATA[extreme image resizing with cuts in continuous domain]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343111</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343111</url>
		<abstract>
			<par><![CDATA[<p>Seam carving [Avidan and Shamir 2007; Rubinstein et al. 2009] revolutionized the way we think about image resizing by demonstrating that it is possible to obtain significant changes in image sizes with changes in proximity relationships, which we call topological properties of an image. Seam carving can change the size of an image by progressively carving out (or carving in) seams, which are monotonically connected paths of low-energy pixels crossing an image from top to bottom, or from left to right. Unfortunately, it quickly became obvious that seam carving creates geometric discontinuities once low-energy regions start to diminish. As a result, improvements and alternative approaches have been suggested to minimize discontinuities.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737826</person_id>
				<author_profile_id><![CDATA[81504685409]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Youyou]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3737827</person_id>
				<author_profile_id><![CDATA[81100124987]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ergun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akleman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1276390</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Avidan, S., and Shamir, A. 2007. Seam carving for content-aware image resizing. <i>ACM Trans. Graph. 26</i> (July), 10.1--10.8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531329</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Rubinstein, M., Shamir, A., and Avidan, S. 2009. Multi-operator media retargeting. <i>ACM Trans. Graph 28</i> (July), 23:1--23:11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Calligraphic Cutting: Extreme Image Resizing with Cuts in Continuous Domain Youyou Wang Ergun Akleman 
Department of Computer Science Visualization Department Texas A&#38;M University Texas A&#38;M University 
 Original Image 33% scaling Seam carving (SC) Improved SC Our method Figure 1: Comparison of our method 
with scaling and seam carving after resizing an image with no low-energy regions. Scaling does not cause 
discontinuities but does change curvatures and slopes. Seam carving (SC) causes severe discontinuities 
under extreme resizing. Improved seam carving (ISC) does not eliminate most of these discontinuities. 
Our calligraphic cutting (CC) can resize the image by preserving geometric properties of the original 
image without causing severe discontinuities. The method removes high-energy regions e.ectively by changing 
topological properties. Note, for instance, the pointed arch above the main entrance door, which moved 
down to keep its shape and revealed the background part of the facade automatically. Seam carving [Avidan 
and Shamir 2007; Rubinstein et al. 2009] revolutionized the way we think about image resiz­ing by demonstrating 
that it is possible to obtain signi.cant changes in image sizes with changes in proximity relation­ships, 
which we call topological properties of an image. Seam carving can change the size of an image by progressively 
carving out (or carving in) seams, which are monotonically connected paths of low-energy pixels crossing 
an image from top to bottom, or from left to right. Unfortunately, it quickly became obvious that seam 
carving creates geometric discon­tinuities once low-energy regions start to diminish. As a result, improvements 
and alternative approaches have been suggested to minimize discontinuities. In this work, we show that 
by reformulating the concept of seam carving in continuous domain, it is theoretically pos­sible to eliminate 
most of the distortions and discontinu­ities caused by seam carving. Our theoretical approach does not 
require any user interaction, any importance or saliency map, or any complicated energy function. We 
simply use the original energy function of seam carving. The paper has three theoretical contributions: 
(1) generalization of seams to calligraphic cuts, (2) identi.cation of the conditions to preserve tangent 
continuity, and (3) cutting through mini­mum and maximum points to resize shapes. The calligraphic cuts 
are monotonically connected paths like seams. Nevertheless, calligraphic cuts have one signi.cant advantage 
over seams. The discretized versions of calli­graphic cuts can include a set of pixels that form horizon­tal 
lines for top-to-bottom cuts or vertical lines for left-to­right cuts. Therefore, they can follow the 
contours of shapes and can e.ectively reach and remove all possible low-energy regions. Seams do not 
have this ability and eventually re­move high-energy regions even when there exist low-energy regions. 
Thus, discontinuities caused by seam carving can be avoided by calligraphic cutting. Our method also 
preserve the original G1 i.e. tangent continuities. We show that avoiding only low-energy regions is 
necessary but not su.cient since doing so can only pre­serve G0 continuity. G1 continuity preservation 
requires an additional condition: the calligraphic cuts must be perpen­dicular to the gradient everywhere 
except in minimum and maximum points. The minimum and maximum points in high-energy regions usually have 
very high curvature, and they already look G1 discontinuous in original images the most human-made objects, 
such as roofs of the buildings, usually have such discontinuous maximum points. The cuts that pass through 
such discontinuous points actually pre­serve the visual structure of the original shape. Therefore, exploiting 
this property, we can continue to apply calli­graphic cuts even long after all low-energy regions are 
elim­inated. References Avidan, S., and Shamir, A. 2007. Seam carving for content-aware image resizing. 
ACM Trans. Graph. 26 (July), 10.1 10.8. Rubinstein, M., Shamir, A., and Avidan, S. 2009. Multi­ operator 
media retargeting. ACM Trans. Graph 28 (July), 23:1 23:11. Copyright is held by the author / owner(s). 
SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343112</article_id>
		<sort_key>670</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>50</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Intelligent brush strokes]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343112</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343112</url>
		<abstract>
			<par><![CDATA[<p>We present a fully automatic brush stroke placement algorithm for non-photorealistic rendering leveraging importance sampling techniques. Smarter brush placement enables creation of images using only a handful of well-placed strokes, avoiding the "textured" look common to prior techniques. We focus on the problem of converting source images to painterly renderings, but our technique could be extended to work on video source material or to generate abstract paintings without any source imagery. We leverage genetic search algorithms to evaluate the quality of the final images, guiding the solution through the large space of potential images.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737828</person_id>
				<author_profile_id><![CDATA[81504686796]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wexler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Soda Springs, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[Wex@The11ers.com]]></email_address>
			</au>
			<au>
				<person_id>P3737829</person_id>
				<author_profile_id><![CDATA[81504688115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gilles]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dezeustre]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Oakland, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[Gilles@The11ers.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>280951</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hertzmann, Aaron, 1998. Painterly Rendering with Curved Brush Strokes of Multiple Sizes. In SIGGRAPH'98, <i>Conference Proceedings</i>, 453--460.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258893</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Litwinowicz, Peter, 1997. Processing Images and Video for an Impressionist Effect. <i>In SIGGRAPH '97, Conference Proceedings</i>, 407--414.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Pharr, M., and Humphreys, G.. 2004. Infinite Area Light Source with Importance Sampling. In <i>Pysically Based Rendering: From Theory to Implementation, PBRT Plugins from the Authors</i>, http://pbrt.org/plugins/infinitesample.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2024698</ref_obj_id>
				<ref_obj_pid>2024676</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Zhao, M., and Zhu, S. C., 2011. Customizing Painterly Rendering Styles Using Stroke Processes. In <i>NPAR'11, Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Non-Photrealistic Animation and Rendering</i>, 137--146.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Intelligent Brush Strokes Daniel Wexler Gilles Dezeustre The 11ers The 11ers Soda Springs, CA, USA Oakland, 
CA, USA Wex@The11ers.com Gilles@The11ers.com Figure1. Fromlefttoright:originalphotograph,uniformstratifiedplacement,non-uniformplacement,ProbabilityDensityFunction(PDF) 
fromSobelmagnitudeandfacialdetection,closeupoflayeredstrokeimageshowingretainedeyesandmouthdetail. 1. 
Introduction We present a fully automatic brush stroke placement algorithm for non-photorealistic rendering 
leveraging importance sampling techniques. Smarter brush placement enables creation of images using only 
a handful of well-placed strokes, avoiding the textured look common to prior techniques. We focus on 
the problem of converting source images to painterly renderings, but our technique could be extended 
to work on video source material or to generate abstract paintings without any source imagery. We leverage 
genetic search algorithms to evaluate the quality of the final images, guiding the solution through the 
large space of potential images. 2. Prior Work Our work builds on a large body of layered stroke-based 
rendering (SBR) research. Aaron Hertzmann's work on application techniques and the use of layered difference 
images to guide stroke placement and generation of curved brush strokes are core to our algorithm. Peter 
Litwinowicz work on brush placement and sizing and Zhao's work on detailed image analysis were inspirational. 
Our work builds upon these older SBR algorithms by adding genetic search and non-uniform sample placement 
techniques to place the strokes along key image features. 2.1 Intelligent Stroke Placement Utilizing 
an importance map as a 2D Probability Density Function (PDF), we use the non-uniform sample generation 
technique described in [Pharr &#38; Humphreys], originally developed to place importance samples on area 
light sources for Monte Carlo estimation of global illumination. The detail image is constructed via 
a heuristic combination of facial recognition data, gradient information from Laplacian filters, and 
difference algorithms comparing the painted result to a target image, usually the original source image. 
Non-uniform stroke placement facilitates non­uniform stroke sizing, useful for providing detail in critical 
regions, such as the eyes and mouth. Difference images are computed either after rendering a subset of 
the total number of strokes, comparing the current image against the target, or after rendering all strokes, 
comparing the final image against other final images using a quality metric to guide a genetic search 
algorithm.  3. Results The algorithm utilizes GPGPU techniques and can generate multi-layer images 
with hundreds of thousands of strokes in about a second on an iPad2. Sparse brush placement can be achieved 
to provide a much more hand-drawn look while still retaining full automatic generation as seen in the 
image on the left. References HERTZMANN, AARON, 1998. PAINTERLY RENDERING WITH CURVED BRUSH STROKES 
OF MULTIPLE SIZES. IN SIGGRAPH'98, CONFERENCE PROCEEDINGS, 453 460. LITWINOWICZ, PETER, 1997. PROCESSING 
IMAGES AND VIDEO FOR AN IMPRESSIONIST EFFECT. IN SIGGRAPH '97, CONFERENCE PROCEEDINGS, 407 414. PHARR, 
M., AND HUMPHREYS, G.. 2004. Infinite Area Light Source with Importance Sampling. In Pysically Based 
Rendering: From Theory to Implementation, PBRT Plugins from the Authors, http://pbrt.org/plugins/infinitesample.pdf. 
ZHAO, M., AND ZHU, S.C., 2011. Customizing Painterly Rendering Styles Using Stroke Processes. In NPAR'11, 
Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Non-Photrealistic Animation and Rendering, 
137 - 146. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 
5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343113</article_id>
		<sort_key>680</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>51</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Rich intrinsic image decomposition of outdoor scenes from multiple views]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343045.2343113</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343113</url>
		<abstract>
			<par><![CDATA[<p>Intrinsic images aim at separating an image into reflectance and illumination layers to facilitate analysis or manipulation. Most successful methods rely on user indications [Bousseau et al. 2009], precise geometry, or need multiple images from the same viewpoint and varying lighting to solve this severely ill-posed problem. We propose a method to estimate intrinsic images from multiple views of an outdoor scene at a single time of day without the need for precise geometry and with only a simple manual calibration step.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3737830</person_id>
				<author_profile_id><![CDATA[81467641347]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pierre-Yves]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Laffont]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[REVES/Inria Sophia-Antipolis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pierre-yves.laffont@inria.fr]]></email_address>
			</au>
			<au>
				<person_id>P3737831</person_id>
				<author_profile_id><![CDATA[81314487615]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Adrien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bousseau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[REVES/Inria Sophia-Antipolis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[adrien.bousseau@inria.fr]]></email_address>
			</au>
			<au>
				<person_id>P3737832</person_id>
				<author_profile_id><![CDATA[81100408270]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Drettakis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[REVES/Inria Sophia-Antipolis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[george.drettakis@inria.fr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1618476</ref_obj_id>
				<ref_obj_pid>1618452</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bousseau, A., Paris, S., and Durand, F. 2009. User-assisted intrinsic images. <i>ACM Trans. Graph. 28</i>, 5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Rich Intrinsic Image Decomposition of Outdoor Scenes from Multiple Views Pierre-Yves Laffont, Adrien 
Bousseau, George Drettakis REVES / Inria Sophia-Antipolis*  Figure 1: Starting from multiple views of 
the scene (a), our method decomposes photographs into four intrinsic components the re.ectance (e), 
the illumination due to sun (f), the illumination due to sky (g) and the indirect illumination (h). Each 
intrinsic component can then be manipulated independently for advanced image editing applications (b-d). 
Introduction Intrinsic images aim at separating an image into re.ectance and illumination layers to facilitate 
analysis or manipulation. Most suc­cessful methods rely on user indications [Bousseau et al. 2009], precise 
geometry, or need multiple images from the same viewpoint and varying lighting to solve this severely 
ill-posed problem. We propose a method to estimate intrinsic images from multiple views of an outdoor 
scene at a single time of day without the need for precise geometry and with only a simple manual calibration 
step. We use multi-view stereo to automatically reconstruct a 3D point cloud of the scene. Although this 
point cloud is sparse and incom­plete, we show it provides the necessary information to compute plausible 
sky and indirect illumination at each 3D point. We then introduce an optimization method to estimate 
sun visibility over the point cloud. This algorithm compensates for the lack of accurate geometry and 
allows extraction of precise cast shadows. We .nally propagate the information computed over the sparse 
point cloud to every pixel in the photograph using image-guided propagation. Our method not only separates 
re.ectance and illumination, but also decomposes the illumination into sun, sky and indirect lay­ers. 
This rich decomposition allows novel image manipulations. Our Approach Our method relies on a relatively 
lightweight capture setup composed of a digital camera, a photographer s gray card for calibration, and 
a simple re.ective sphere to capture an environ­ment map. We capture 10-30 pictures from dif­ ferent 
viewpoints in addition to the images to decompose. Geometry-based computation We use structure-from-motion 
and multi-view stereo to reconstruct a cloud of oriented 3D points, and surface reconstruction to obtain 
a proxy geometric model of the scene. The user speci.es the orientation and color of sun and sky through 
a simple calibration step. The geometric proxy is approximate and incomplete, and cannot be directly 
used to estimate the illumination at each pixel. In particu­ *e-mail: {pierre-yves.laffont, adrien.bousseau, 
george.drettakis}@inria.fr lar, it produces inaccurate or even missing cast shadows. However, it can 
give a reasonable approximation for low-frequency lighting components: we estimate sky and indirect illumination 
at recon­structed 3D points by casting rays towards all directions. Rays that intersect the proxy contribute 
to indirect lighting (we use the cap­tured photographs to lookup the outgoing radiance at intersected 
points), while rays that hit the environment map above the horizon contribute to sky lighting. We also 
use the proxy to compute an ini­tial estimate of sun visibility (cast shadows), which we later re.ne. 
Sun visibility estimation We introduce an algorithm to reliably identify points in shadow based on a 
new parameterization of re­.ectance with respect to sun visibility. Our algorithm compensates for the 
lack of accurately reconstructed and complete geometry. We show that the re.ectance of each 3D point 
lies on a candidate curve in color space, once sky and indirect illuminations are estimated. Multiple 
points which share a similar re.ectance generate intersect­ing candidate curves. We use an iterative 
optimization method that reliably estimates the re.ectance and sun visibility at reconstructed points, 
by .nding regions where multiple candidate curves inter­sect. We illustrate this process in the supplementary 
document. Image-based propagation and lighting separation At this stage, the total illumination has been 
estimated at sparse recon­structed 3D points. We propagate it to all pixels of the image to de­compose 
by using an image-guided propagation method [Bousseau et al. 2009], which yields a decomposition into 
re.ectance and total illumination. We further decompose the total illumination into sun, sky and indirect 
illumination, by casting this as a Matting problem and enforcing the illumination values estimated at 
3D points. Results Our algorithm decomposes an image into four layers, which can be then modi.ed independently. 
Fig. 1 shows exam­ ples of changes made possible by our approach, while the video demonstrates how to 
use the decomposition in image editing soft­ware. More results and comparisons with single-image approaches 
are shown in the supplementary document. References BOUSSEAU, A., PARIS, S., AND DURAND, F. 2009. User-assisted 
intrinsic images. ACM Trans. Graph. 28, 5. Copyright is held by the author / owner(s). SIGGRAPH 2012, 
Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
</content>
</proceeding>
