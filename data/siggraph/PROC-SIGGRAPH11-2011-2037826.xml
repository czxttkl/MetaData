<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>08/07/2011</start_date>
		<end_date>08/11/2011</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Vancouver]]></city>
		<state>British Columbia</state>
		<country>Canada</country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>2037826</proc_id>
	<acronym>SIGGRAPH '11</acronym>
	<proc_desc>ACM SIGGRAPH 2011 Talks</proc_desc>
	<conference_number>2011</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-1-4503-0974-5</isbn13>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2011</copyright_year>
	<publication_date>08-07-2011</publication_date>
	<pages>73</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>A broad spectrum of presentations on recent achievements in all areas of computer graphics and interactive techniques, including art, design, animation, visual effects, interactivity, research, and engineering.</p> <p>Talks are a less formal alternative to formal publication. They often highlight the latest developments before publication, present ideas that are still in progress, or showcase how computer graphics and interactive techniques are actually implemented and used, in graphics production or other fields. Talks take you behind the scenes and into the minds of SIGGRAPH 2011 creators.</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<chair_editor>
		<ch_ed>
			<person_id>P2809570</person_id>
			<author_profile_id><![CDATA[81100457004]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[Mark]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Elendt]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[Side Effects Software Inc.]]></affiliation>
			<role><![CDATA[Conference Chair]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
</proceeding_rec>
<content>
	<section>
		<section_id>2037827</section_id>
		<sort_key>10</sort_key>
		<section_seq_no>1</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Pushing production data]]></section_title>
		<section_page_from>1</section_page_from>
	<article_rec>
		<article_id>2037828</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>1</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[PhotoSpace]]></title>
		<subtitle><![CDATA[a vision based approach for digitizing props]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037828</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037828</url>
		<abstract>
			<par><![CDATA[<p>This talk will concentrate on how some of the recent advances in computer graphics and vision fit into a visual effects pipeline and their application for digitizing props. The production of movies often requires digitizing physical properties of various props. At Weta Digital we model, texture, and shade thousands of props in immense detail. We present a vision-based pipeline for digitizing props that requires far less user input than previous methods. This vision based pipeline fully automates the process of creating virtual cameras, reference geometry, and texture maps, which is then used by our artists to create the final assets at a much faster rate.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[modeling]]></kw>
			<kw><![CDATA[photogrametry]]></kw>
			<kw><![CDATA[texturing]]></kw>
			<kw><![CDATA[turntable]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809571</person_id>
				<author_profile_id><![CDATA[81100042831]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pravin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bhat]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Weta Digital]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pbhat@wetafx.co.nz]]></email_address>
			</au>
			<au>
				<person_id>P2809572</person_id>
				<author_profile_id><![CDATA[81466643384]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sebastian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Burke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Weta Digital]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sburke@wetafx.co.nz]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2383889</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bhat, P., Zitnick, C. L., Snavely, N., Agarwala, A., Agrawala, M., Curless, B., Cohen, M., and Kang, S. B. 2007. Using photographs to enhance videos of a static scene. In <i>EGSR 2007</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Furukawa, Y., and Ponce, J., 2010. Patch-based multi-view stereo software. http://grail.cs.washington.edu/software/pmvs.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kazhdan, M. M., 2007. Unconstrained isosurface extraction on arbitrary octrees. http://www.cs.jhu.edu/misha/Code/IsoOctree/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Snavely, N., 2010. Bundler: Structure from motion for unordered image collections. http://phototour.cs.washington.edu/bundler/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PhotoSpace: A Vision Based Approach for Digitizing Props Pravin Bhat* Sebastian Burke Weta Digital 
Weta Digital Keywords: photogrametry, turntable, modeling, texturing 1 Introduction and Motivation Figure 
1: Left image shows one of the 270 photographs of a physical prop captured using a computer-controlled 
turntable. The prop ge­ometry (middle) and texture (right) are extracted automatically from the photographs 
using computer vision techniques and serve as 3D reference for artists creating the production asset. 
This talk will concentrate on how some of the recent advances in computer graphics and vision .t into 
a visual effects pipeline and their application for digitizing props. The production of movies of­ten 
requires digitizingphysical propertiesofvarious props.AtWeta Digital we model, texture, and shade thousands 
of props in immense detail. We present a vision-based pipeline for digitizing props that requiresfar 
less user input than previous methods. This vision based pipeline fully automates the process of creating 
virtual cameras, ref­erence geometry,andtexturemaps,whichisthenusedbyour artists to createthe.nal assetsatamuchfaster 
rate. Our previous in-house approach for digitizing props began with tak­ing reference photographs and 
creating a Maya scene with virtual cameras that match the relative poses at which the physical camera 
photographed the prop. With a few hours of user input a TD would generate virtual cameras corresponding 
to individual photographs using interactive matchmove software. These virtual cameras were then exported 
to a Maya scene with the corresponding photographs attached as image-planes. This served as a multiview 
image refer­ence for the artist when modelling the physical prop. Once a model was created,a3D paint 
packagewas usedto semi-automatically cre­ate a seamless texture map using a painting interface to project 
re­gions from various photographs through corresponding virtual cam­eras. AtWetawealsohavea laserscannerto 
scanthe geometryofa prop. However, this approach does not provide photo-reference aligned to the 3D model, 
which accelerates the creation of prop texture and shader. In addition, complex props can require several 
hours of laser guidance by a human operator to get thorough coverage. Although the dense mesh generated 
by a laser scanner is unsuitable for our rendering pipeline, the mesh serves as an excellent 3D reference 
for the modeler and considerably speeds up the modelling process. Our vision based pipeline for digitizing 
props combines the best fea­turesofthe approaches describedabovebut requiresfar less userin­put. The 
capturingof photographsandextractionof virtual cameras, geometry,and texture is automated. The resulting 
data is provided as reference material inside software packagesfamiliar to artists, thus considerably 
speeding up the creation of the .nal asset. *e-mail: pbhat@wetafx.co.nz e-mail: sburke@wetafx.co.nz 
2 System overview and future work Our vision-based system for digitizing props consists of three parts 
that arebuilt using readilyavailable components. Capture session: The prop is placed on a turntable in 
a studio envi­ronment. Amicroprocessor triggers a camera to takes photographs of the prop at regular 
intervals as it turns the prop 360 degrees. By default the camera captures ninety photographs for one 
revolution of the turntable.To ensurevertical coverageof the prop, the controller can use one to three 
cameras at different vertical heights that pho­tograph the prop at different vertical angles. For a single 
prop, the capture session runs to completion within .fteen minutes. Photogrammetry: Once the photographs 
have been captured, sev­eral vision based tasks are launched on our serverfarm. This usually takes 2-10 
hours depending on the number of photographs captured (90-270) and the complexityof the prop.For each 
photograph, fea­ture points are detected and feature descriptors are extracted. Each photograph is matched 
to its two nearest horizontal neighbors and up to two nearest vertical neighbors, thus generating 2D 
matches across the entire image graph. These 2D correspondences are then processed using Snavely et al.[2010] 
s structure-from-motion algo­ rithm to extract camera parameters for each photograph. The cam­era parameters 
are then used in Furukawa et al.[2010] s multiview stereo algorithm to extract the prop geometry as a 
dense pointcloud of oriented 3D points. The pointcloud is then converted into a wa­ter tight mesh using 
Kazhdan et al.[2007] s octree-based Poisson reconstruction algorithm. Reference generation: The mesh 
generated in the previous step is unsuitable to be used directly in our rendering pipeline. Instead the 
system generates a reference Maya scene that helps a human mod­eller speed up the modelling process. 
The Maya scene contains the meshandcamerasextractedinthepreviousstep, centeredattheori­gin, and oriented 
to be upright. Each photograph is attached as an image-planetothe corresponding cameraintheMaya scene.Asim­ilar 
scene is created for our in-house 3D paint package that allows a texture artist to paint through anyof 
the photographs thus project­ing the selected region through the corresponding camera. An initial seamless 
texture map is automatically generated using the method described in Bhat et al.[2007]. Any residual 
error in the texture map is eliminated by the texture artist using a painting interface. Future work: 
We hope to extend the system to also generate refer­ence shaders for props. During the capture session 
we plan to use a lightstage to ef.ciently vary the scene lighting in a manner that will facilitate automatic 
extraction of the prop s shading properties. References BHAT, P., ZITNICK, C. L., SNAVELY, N., AGARWALA, 
A., AGRAWALA,M.,CURLESS,B.,COHEN,M., ANDKANG,S.B. 2007. Using photographs to enhance videos of a static 
scene. In EGSR 2007. FURUKAWA, Y., AND PONCE, J., 2010. Patch-based multi-view stereo software. http://grail.cs.washington.edu/software/pmvs. 
KAZHDAN, M. M., 2007. Unconstrained isosurface extraction on arbitrary octrees. http://www.cs.jhu.edu/ 
misha/Code/IsoOctree/. SNAVELY,N., 2010. Bundler: Structure from motion for unordered image collections. 
http://phototour.cs.washington.edu/bundler/. Copyright is held by the author / owner(s). SIGGRAPH 2011, 
Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037829</section_id>
		<sort_key>30</sort_key>
		<section_seq_no>2</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Facing hairy production problems]]></section_title>
		<section_page_from>2</section_page_from>
	<article_rec>
		<article_id>2037830</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>2</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Making faces]]></title>
		<subtitle><![CDATA[Eve Online's new portrait rendering]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037830</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037830</url>
		<abstract>
			<par><![CDATA[<p>Eve Online is a single shard MMO with tens of thousands of spaceships battling it out. Recently, its character customization system has seen a major upgrade, providing the players with a high definition avatar rendering system to sculpt, customize and tweak their in-game characters.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809573</person_id>
				<author_profile_id><![CDATA[81488667300]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CCP Games, Reykjavik, Iceland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[bpeers@ccpgames.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{Gritz08} "The Importance Of Being Linear", Larry Gritz et al, <i>GPU Gems 3</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{Hable10} "Uncharted 2: HDR Lighting", Jon Hable, <i>GDC 2010</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{Kelemen01} "A Microfacet Based Coupled Specular Matte BRDF Model with Importance Sampling", Kelement et al, <i>Eurographics 2001</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{D'Eon08} "Advanced Techniques for Realistic Real-time Skin Rendering", Eugene d'Eon, <i>GPU Gems 3</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{Hable09} "Fast Skin Shading", Jon Hable, <i>Shaderx7</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{OpenGl} "Display Lists", OpenGL 1.1 Specification]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{Lauritzen08} "Summed-Area Variance Shadow Maps", Andrew Lauritzen, <i>GPUGems3</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{Chen09} "Lighting Research At Bungie", Chen et al, <i>Siggraph 2009 Course</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1730831</ref_obj_id>
				<ref_obj_pid>1730804</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{Jansen10} "Fourier Opacity Mapping", Jansen et al, <i>I3D 2010</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37435</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{Reeves87} "Rendering Antialiased Shadows With Depth Maps", Reeves et al, <i>Siggraph 1987</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Making FacesEveOnline sNewPortraitRendering Bert Peerse CCPGames,Reykjavik,Icelande bpeers@ccpgames.come 
 1. Introductione e EveOnlineisasingleshardMMOwithtensofthous andsofe spaceshipse battlingitout.Recently,itse 
character e customizatione systemhasseenamajorupgrade,providingtheplay erswithae highe definitionavatarrenderingsystemtosculpt,e 
customizee ande tweaktheirin-gamecharacters.e e e Thise customizationrunsate interactiveratesinthe e 
client,e withe ae staticportraitrenderedusingthesamesystemwith highersettingse attheonlinerenderfarm.e 
 Inthispresentationwesurveythearrayoftrickse andtechniquese usede toe sculpte ande rendere highe qualitye 
skin,e hair,e eyese ande shadows.e Wee breakdowne ae finale rendere intoe itsindi viduale components,showwhatworkedandwhatdidn 
t,andp ointoutae fewenginegems.Evenifthetechniquesarerelati velylowkeyine ordertogetgoodresultsoutofcommodityhardware 
,wehopetoe offerornewtrickseveryoneinaudi ence. onetwotothee  2.Contributionse e Evee Onlineusesanestablishede 
enginee thatwase init iallye gearede towardsrenderingmassivespacebattles.Inthis2 0minutetalke wee giveanoverviewe 
ofe thee changesande additionse ma dee toe introduceavatarintoystem. highqualityrenderingthiss e e  
2.1Elaboratione e Westartwithaquickrecapofproperlinearrender inge [GRITZ08],e filmictonemappinge [HABLE10],andtheskinspecificspecularmodele 
we usede [KELEMEN01]. e Next,e wee discusse ourimplementatione ofe diffusee subs urfacee scatteringe 
[D EON08]e withe ae Poisson-samplede texture-spacee lighte diffusione [HABLE09].e Socallede renderjobse 
automaticallytranslatee renderinge algorithmsexpressedinPythone intoae display-list-likee recordinge 
[OPENGL];thisgivesusveryquickprototypingande iterationerformance. capabilitiesatminimallossinruntimep 
e Subtlefacialanimationrequiresagoodwrinklings ystem.Pythone codemonitorsanimatedbonepositionstoindependen 
tlyactivatee anyof12facialzonesperframe.Anactivezonewi llcallonthee shadere toapplyae normaloffsetande 
skine recoloring e tosimulatee proper skindeformation.e e Turningtoshadowrendering,wesurveyafewtechni 
questhate weree tested,e includinge VarianceShadowMappinge [LAURITZEN08,e CHEN09]e andFourierOpacityMappinge 
[JANSEN10],e beforediscussinge oure PCFsolution.Ae tighteninge ofe thee sceneOBBtoe thee viewe frustumleadstoalowcomplexitybutsatisfyingim 
provementine shadowe quality.e Forhairandglasses,ae screendoore techniquee ise usedshadowse [REEVES87]. 
toemulatetransparente Wrappinguptheshaderpartwithafewsmallbutef fectivetrickse hidinginthehairandtheeyes,weturntoouruse 
ofblendshapese toofaces. improvecustomizabilitytheplayer-sculptedf e Usingmorethan130morphtargetsrulesoutaslider 
-basede approach,e optfore triangle-fieldse e augmentthe soweinstead.Wefacee e withe multiplee 2De trianglee 
meshes,wheree eache vertexe representsapredefinedsetofmorphtargetweights. Astheusere moveso themouse,barycentriccoordinatesareusedt 
interpolateae newsetofblendweights,creatingtheillusionofd irectlysculptinge theface.Asimilarapproachusinganimation-blendt 
reeweightse providestheabilitytoposethefaceandchangeth efaciale expression.Finally,weuseblendshapestopushve 
rticesbehinde obstructinggeometry,offeringtheplayereasy-to-a uthortuckinge options forshirtsandpants. 
  Figuree Playerserver. 1.createdportrait,renderedd onthe  3.Resultse e Usinge thesee techniquese 
wee achievee highe qualitye char actere customizatione one commodityhardware,e usinge DirectX9 .e Bye 
simplye increasingae fewe basicsettingssuche asinpu te texturee resolutionandshadowmapresolution,thesamerend 
eringsysteme worksasarenderfarmproducinghighdefinitionpl ayerportraitse such1. asFiguree e Referencese 
[GRITZ08]e THEe BEINGe LINEAR ,e LARRYe GRITZAL ,e GPU. IMPORTANCEe OFETGems3 e e [HABLE10]e UNCHARTED2:HDRe 
LIGHTING ,e JONe HABLE,e 2010. e GDC[KELEMEN01]e Ae MICROFACETe BASEDe COUPLEDe SPECULARe MATTEe BRDFe 
MODELe WITHe IMPORTANCEe SAMPLING ,e KELEMENTAL ,Eurographics ETe 2001 .e [D EON08]e ADVANCEDe TECHNIQUESe 
REAL-TIMEe SKINe RENDERING ,e EUGENEGems FORe REALISTICD EON,GPU3.d [HABLE09]e FASTe SKINe SHADING ,e 
JONe HABLE,e ShaderX7.d [OPENGL] DISPLAYLISTS ,e 1.1 OPENGLSPECIFICATION [LAURITZEN08]e SUMMED-AREAe 
VARIANCEe SHADOWe MAPS ,e e LAURITZEN, ANDREWGPUGems3.e [CHEN09]e LIGHTINGe RESEARCHe e BUNGIE ,e CHENAL 
,e Siggraph2009Course. ATETe [JANSEN10]e FOURIERe OPACITYe MAPPING ,e JANSENAL ,2010. ETI3D[REEVES87]e 
RENDERINGe ANTIALIASEDe SHADOWSWITHe MAPS ,REEVESAL ,e Siggraph e DEPTHe ET1987.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037831</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>3</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[SpeedFur]]></title>
		<subtitle><![CDATA[a GPU based procedural Hair & Fur modeling system]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037831</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037831</url>
		<abstract>
			<par><![CDATA[<p>Because Fido has a strong focus on character and creature animations, a robust and fast workflow for our Hair and Fur pipeline is essential for our daily work. With our previous off-the-shelf Hair and Fur system we had experienced a lot of problems with storage utilization and network traffic when we pushed it to its limits.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809574</person_id>
				<author_profile_id><![CDATA[81488669482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Vilhelm]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hedberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fido]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[vilhelm.hedberg@fido.se]]></email_address>
			</au>
			<au>
				<person_id>P2809575</person_id>
				<author_profile_id><![CDATA[81466640894]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mattias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lagergren]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fido]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mattias.lagergren@fido.se]]></email_address>
			</au>
			<au>
				<person_id>P2809576</person_id>
				<author_profile_id><![CDATA[81466647680]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Fredrik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lims&#228;ter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fido]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[fredrik.limsater@fido.se]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566627</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kim, T.-Y., and Neumann, U. 2002. Interactive multiresolution hair modeling and editing. In <i>Proceedings of the 29th annual conference on Computer graphics and interactive techniques</i>, ACM, New York, NY, USA, SIGGRAPH '02, 620--629.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882345</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Marschner, S. R., Jensen, H. W., Cammarano, M., Worley, S., and Hanrahan, P. 2003. Light scattering from human hair fibers. In <i>ACM SIGGRAPH 2003 Papers</i>, ACM, New York, NY, USA, SIGGRAPH '03, 780--791.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SpeedFur -A GPU based procedural hair &#38; fur modeling system Vilhelm Hedberg Mattias Lagergren Fredrik 
Lims¨ater Fido * 1 Introduction Because Fido has a strong focus on character and creature anima­tions, 
a robust and fast work.ow for our Hair and Fur pipeline is essential for our daily work. With our previous 
off-the-shelf Hair and Fur system we had experienced a lot of problems with storage utilization and network 
traf.c when we pushed it to its limits. Our goal in creating a new Hair and Fur system was to enable 
our artists to work procedurally in real-time with grooming, shad­ing and rendering and at the same time 
minimize the amount of data that is saved to disk. To ful.ll these criteria we are develop­ing SpeedFur, 
a node-based, procedural hair system integrated as a plug-in for Maya. SpeedFur is capable of rendering 
complex hair styles on both the GPU in real-time and on the CPU for of.ine ren­dering on our render farm, 
and its procedural approach keeps any disk based .les and caches to a minimum. 2 Architecture To give 
our technical artists as much .exibility and creative free­dom as possible we are building SpeedFur as 
a procedural node­based network. All nodes share a common code-base and new fea­tures can easily be added, 
making our system extensible and easy to maintain. The nodes can change the strands in arbitrary ways 
and can even grow new hair strands or remove hair, giving us huge .exibility when developing new nodes. 
For of.ine rendering, SpeedFur is evaluated as a procedural prim­itive in RenderMan which signi.cantly 
reduces the stress on our network infrastructure. Fido currently uses a traditional CPU-based renderfarm 
solution. To eliminate the need for larger investments in specialized hard­ware, SpeedFur is designed 
to run on both the CPU and the GPU. In order to achieve this, we compile the code for both platforms, 
thus ensuring the geometry in the real-time preview is identical to one in the of.ine rendering. 3 Work.ow 
Guide curves are used to setup the main hair layout and control the direction of the hair. The procedural 
node network can then be used to interpolate and style the hair using a combination of our highly specialized 
node types (frizz, clumping, length, noise, etc.). Kim and Neumann [2002] used generalized cylinders 
speci.ed with different functions which change the style at various detail levels. Using our approach, 
we can achieve a similar effect by using spe­cialized nodes for each function, affecting the hair at 
any speci.c *{vilhelm.hedberg, mattias.lagergren, fredrik.limsater}@.do.se point in the node network. 
Each node type exposes a set of parameters and the user can even utilize many of Maya s built in tools, 
e.g. texture painting or dy­namics, to control the result of a speci.c node. One of the major advantages 
of SpeedFur is the ability to give fast and accurate feedback to the artist by evaluating the procedural 
hair network on the GPU. This allows for very fast iterations and en­ables our artists to reach the desired 
result in a fraction of the time previously spent waiting for of.ine renders.  4 GPU Once the guide 
curves are moved to the graphics memory, the node network can be fully evaluated using NVIDIA s CUDA 
API, and then rendered in the viewport using OpenGL without moving any data back and forth to the main 
memory. Thus a very large num­ber of interpolated hair strands can be rendered in real-time as the artist 
is changing the different node parameters. Since OpenGL is used for the preview, we created a GLSL hair 
shader based on the Marschner shading model [Marschner et al. 2003] to create a more realistic preview. 
Sometimes an artist s hair system has more hair then could be cur­rently kept in memory on a commodity 
GPU, but we enabled the user to .lter parts of the hair system so they can view only seg­ments of the 
fur in full detail. Alternatively, the user can control the preview fur density to be lower than the 
render density to be able to view the full hair style but with lower .delity. 5 Results Thanks to SpeedFur 
s procedural approach, we have been able to minimize the impact on our infrastructure when rendering, 
and at the same time reduce the time needed for look development utiliz­ing our GPU accelerated real-time 
preview. SpeedFur s integration with Maya means that we are able to do the shading, lighting, ren­dering 
and editing of hundreds of thousands of shaded hair strands in real-time, in one easy work.ow. The support 
for CPU rendering means that our artists can also keep their existing work.ow using Renderman and that 
setups done in our previous hair and fur system can be carried over to SpeedFur.  References KIM, T.-Y., 
AND NEUMANN, U. 2002. Interactive multiresolution hair modeling and editing. In Proceedings of the 29th 
annual conference on Computer graphics and interactive techniques, ACM, New York, NY, USA, SIGGRAPH 02, 
620 629. MARSCHNER, S. R., JENSEN, H. W., CAMMARANO, M., WORLEY, S., AND HAN- RAHAN, P. 2003. Light scattering 
from human hair .bers. In ACM SIGGRAPH 2003 Papers, ACM, New York, NY, USA, SIGGRAPH 03, 780 791. Copyright 
is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. 
ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037832</article_id>
		<sort_key>60</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>4</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[GPU fluids in production]]></title>
		<subtitle><![CDATA[a compiler approach to parallelism]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037832</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037832</url>
		<abstract>
			<par><![CDATA[<p>Fluid effects in films require the utmost flexibility, from manipulating a small lick of flame to art-directing a huge tidal wave. While fluid solvers are increasingly making use of GPU hardware, one of the biggest challenges is taking advantage of this technology without compromising on either adaptability or performance. We developed the Jet toolset comprised of a high-level language and compiler for structured grids and migrated the grid solver from our proprietary fluid solver, Squirt achieving significant acceleration.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809577</person_id>
				<author_profile_id><![CDATA[81488650461]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bailey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Double Negative]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[drb@dneg.com]]></email_address>
			</au>
			<au>
				<person_id>P2809578</person_id>
				<author_profile_id><![CDATA[81488672204]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Masters]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Double Negative]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[iim@dneg.com]]></email_address>
			</au>
			<au>
				<person_id>P2809579</person_id>
				<author_profile_id><![CDATA[81488644719]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Warner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Double Negative]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mw@dneg.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>956545</ref_obj_id>
				<ref_obj_pid>956417</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Adve, V., and Lattner, C. 2003. LLVA: A Low-level Virtual Instruction Set Architecture. In <i>Proceedings of the 36th annual ACM/IEEE international symposium on Microarchitecture (MICRO-36)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bailey, D., and Masters, I., 2010. GPU fluids in production: Accelerating the pressure projection, July 25. Siggraph Talk.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 GPU Fluids in Production: A Compiler Approach to Parallelism Dan Bailey Ian Masters MattWarner Double 
Negative*  ccc Left to right: Squirt simulations from Sorcerer s Apprentice &#38;#169; 2010 Disney, 
Inception &#38;#169; 2010 Warner Bros, Scott Pilgrim &#38;#169; 2010 Universal Abstract Fluid effects 
in .lms require the utmost .exibility, from manipulat­ing a small lick of .ame to art-directing a huge 
tidal wave. While .uid solvers are increasingly making use of GPU hardware, one of the biggest challenges 
is taking advantage of this technology without compromising on either adaptability or performance. We 
developed the Jet toolset comprised of a high-level language and compiler for structured grids and migrated 
the grid solver from our proprietary .uid solver, Squirt1 achieving signi.cant acceleration. 1 Introduction 
Structured grids are at the heart of our Navier-Stokes .uid solver, howeverprovidingahighlyef.cientGPU 
implementationforthem requires frequent code reuse, complex layers of meta-programming and a fundamentally 
different approach to that of the CPU. In addi­tion, performing sophisticated adjustments to simulation 
data often requiresexportingthedataouttoa3Dpackagesuchas Houdini,as the internals of the solver are trickyto 
change rapidly in line with the demands of production. Our approach to solving this is to use the Jet 
language for express­ing the logic for our structured grids and the Jet compiler to deter­mine the optimum 
indexing strategy and low-level memory man­agement for a target architecture. Separating the application 
logic from the low-level implementation allows for more seamless devel­opmentina highly production-drivenenvironmentas 
artists can as­semble complex work.ows while developers independently re.ne the compiler framework. 
2 The Language Our Jet language is domain-speci.c and non-Turing complete. Un­like much of the active 
research into automatic parallelisation, a bottom-up approach is used, taking the smallest unit of computa­tion 
and scaling it up to a larger system. Programming using this language means it is easy to write code 
that scales well in parallel and hard to write code that doesn tscale well. Fundamental new constructs 
are introduced, such as the colon­bracket focus operator for handling grid offsets relative to the current 
computation voxel. This simpli.es common stencil com­putation and coupled with an otherwisefamiliar syntax 
makes the code easier to understand for both developers and artists. *e-mail:{drb,iim,mw}@dneg.com 1originally 
co-authoredby Marcus Nordenstam and Robert Bridson 3 The Compiler Our Jet compiler isbuilt upon the 
LLVM Compiler Infrastructure [Adve 2003], and uses their intermediate representation (IR) and optimisation 
pass framework as the foundation for applying par­allel transformations. A Bison-based parser and code 
generation phase produces valid parallel-aware LLVM IR, which the trans­lation passes can adapt for sequential 
or parallel execution. The resulting IR is then lowered using the X86 backend for the CPU, or the PTX 
backend for NVidia GPUs. Using LLVM, the Jet compiler generates NVidia PTX instructions directly, obviating 
the need to program intricate grid logic in low-level parallel languages such as CUDA or OpenCL. The 
indexing strategy for parallel computation is as shown in [Bailey2010] and relies on the compiler to 
optimise desired grid ac­cess patterns for memory bandwidth through careful use of shared memory.Workingina 
highly atomic, modularway,the compileris free to perform late-stage and inter-kernel optimisations that 
would just not be feasible with a more .xed compilation cycle. 503 Smoke Simulation Arch Time Speedup 
CPU (GCC 4.1.2) X5570 29m34s 1.0x CPU (Jet 1.3) X5570 14m00s 2.11x GPU (Jet 1.3) FX4800 07m19s 4.04x 
GPU (Jet 1.3) C2050 01m07s 26.7x The single-threaded X5570 CPU solver is twice asfast when us­ing the 
Jet compiler over the original codebase and demonstrates a 26.7x speedup when targeting theNVidia C2050Tesla 
GPU. 4 Conclusion The Jet language and compiler provides a simple, ef.cient way of tackling problems 
involving stencil computations for structured grids.Target-speci.c optimisations are moved upstream from 
com­plex layers of templates and macros to form clean, modular passes. These passes are applied in turn 
to transform each unit of computa­tion to take advantage of features of the requested architecture. 
 References ADVE, V., AND LATTNER, C. 2003. LLVA: A Low-level Vir­tual Instruction Set Architecture. 
In Proceedings of the 36th an­nual ACM/IEEE international symposium on Microarchitecture (MICRO-36). 
BAILEY,D., AND MASTERS,I., 2010. GPU .uidsin production: Accelerating the pressure projection, July 25. 
SIGGRAPHTalk. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, 
Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037833</section_id>
		<sort_key>70</sort_key>
		<section_seq_no>3</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Eye on the road]]></section_title>
		<section_page_from>3</section_page_from>
	<article_rec>
		<article_id>2037834</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>5</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[MotorStorm apocalypse]]></title>
		<subtitle><![CDATA[creating explosive and dynamic urban off road racing]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037834</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037834</url>
		<abstract>
			<par><![CDATA[<p>Evolution has a long history of producing off road racing games. Both the World Rally Championship Series and the first two MotorStorm releases were almost completely off road. More importantly the environments and races were, in all but a few ways, extremely static. The introduction of an entire dynamic events system that covered the range of effects generated by massive earth quakes, hurricanes and almost full-on military battles required a rethink of our process and pipelines (as can be seen in Figure 1).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809580</person_id>
				<author_profile_id><![CDATA[81488651979]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alex]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Perkins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Evolution Studios SCEE]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809581</person_id>
				<author_profile_id><![CDATA[81488669762]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hawson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Evolution Studios SCEE]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 MotorStorm Apocalypse Creating Explosive and Dynamic Urban Off Road Racing \\motorstorm\Art\Marketing\Screens\wallpaper\MotorStorm_Race_20061103_04.jpg 
\\motorstorm2\Art\Marketing\screens\SCEE_box\box_02.jpg \\motorstorm3\Art\screens\MASTER\2010-04-30_Screen_03.jpg 
 Alex Perkins &#38; Dan Hawson Evolution Studios SCEE  Figure 1 : From the barren muddy tracks of MotorStorm 
through the lush environments of Pacific Rift to the dynamic, ever changing races of Apocalypse 1 Introduction 
Evolution has a long history of producing off road racing games. Both the World Rally Championship Series 
and the first two MotorStorm releases were almost completely off road. More importantly the environments 
and races were, in all but a few ways, extremely static. The introduction of an entire dynamic events 
system that covered the range of effects generated by massive earth quakes, hurricanes and almost full-on 
military battles required a rethink of our process and pipelines (as can be seen in Figure 1). These 
requirements influenced not only our rendering solution but also our physics, run-time tools, VFX infrastructure 
and large areas of our asset creation pipeline. Added to this, the game had to fully support stereoscopic 
3D with minimal loss in quality. 2 Stereoscopic 3D At Evolution we, have been amongst the front runners 
for developing 3D games. From our first release, Pacific Rift 3D, we learnt a lot. The decision was made, 
before we d even designed the renderer, to run the 2D version of Apocalypse at 1080p enabling us to have 
3D running almost from the start of production. Among the various concerns for stereoscopic 3D game development, 
we address a particular issues relating to latency and frame tearing. 3 Renderer To solve all the new 
technical and game play challenges, as well as to avoid any static lighting to dynamic lighting discontinuities, 
we changed the renderer from a traditional forward renderer to a pre-lighting approach. We spent time 
investigating a complementary light mapping solution but this was not developed further for various reasons. 
Instead, dynamic lights, several real-time and full screen processes, such as our Volumetric texture 
augmented SSAO and SPU-based MLAA systems were added to this to give us greater flexibility in visuals 
and content creation. To seat all the objects and vehicles into each distinct area within an environment, 
each level had a global lighting solution based on a directional sun, sky and cubemaps (for both specular 
and diffuse). This was blended and combined with areas defined by artist for localised cubemaps. The 
new light probe system that generated the cubemaps required some compromises to get the best results, 
especially at junctions between lighting conditions. The VFX process needed to be upgraded to enable 
large amounts of lit and sorted particles, without hindering frame rate, mixed with full geometry based 
effects to give a rounded realistic feel especially when played in 3D. This required a number of solutions 
varying from an expanded particle system feature set and a substantial increase in SPU workload to bespoke 
effects for individual particle systems. 4 Dynamic Event Creation In conjunction with these render improvements, 
the processes and generation of dynamic content required a new pipeline, especially any driveable geometry 
that could be blown up or could collapse. This was bedded in with new run-time tools that allowed for 
the compositing of animations (whether they were dynamically driven or keyed), audio, effects as well 
as independent playback and scrubbing on platform. Special thanks to : Oli Wright, Steve Humphries, 
Steve Taylor, Andy Seymour, Jack O Neal and Scott Kirkland 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037835</article_id>
		<sort_key>90</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>6</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Interactive hybrid simulation of large-scale traffic]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037835</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037835</url>
		<abstract>
			<par><![CDATA[<p>Automobile traffic is ubiquitous in developed nations and on the rise in developing countries. Traffic simulation techniques for animation, urban planning, and engineering design are of increasing interest and import for analyzing road usage in urban environments and for interactive visualization of virtual cityscapes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809582</person_id>
				<author_profile_id><![CDATA[81342510602]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sewall]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jason.sewall@intel.com]]></email_address>
			</au>
			<au>
				<person_id>P2809583</person_id>
				<author_profile_id><![CDATA[81453657764]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wilkie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UNC Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809584</person_id>
				<author_profile_id><![CDATA[81452602436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UNC Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809585</person_id>
				<author_profile_id><![CDATA[81332496734]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Pradeep]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dubey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1066213</ref_obj_id>
				<ref_obj_pid>1066157</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chen, L., &#214;zsu, M., and Oria, V. 2005. Robust and fast similarity search for moving object trajectories. In <i>ACM SIGMOD</i>, ACM, 491--502. 1]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sewall, J., Wilkie, D., Merrell, P., and Lin, M. C. 2010. Continuum traffic simulation. In <i>Eurographics 2010</i>. 1]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037836</article_id>
		<sort_key>100</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>7</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Impact of subtle gaze direction on short-term spatial information recall]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037836</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037836</url>
		<abstract>
			<par><![CDATA[<p>The ability to direct a viewer's gaze about a scene has important applications in computer graphics and data visualization. The Subtle Gaze Direction (SGD) technique developed by Bailey et al. [2009] provides the ability to guide a viewer's gaze to specific regions of a display. The technique, which combines real-time eye-tracking with subtle image-space modulation, has minimal impact on viewing experience as it does not change the overall appearance of the scene being viewed. Subtlety is achieved by presenting the modulations only to the low-acuity peripheral regions of the field of view so the viewer is never allowed to scrutinize the modulations. The technique has been shown to be quite fast and accurate: viewers typically attend to target regions within 0.5 seconds of the onset of the modulation and the resulting fixations are typically within a single perceptual span of the target. While this shows that the technique is successful at directing gaze, it does not necessarily mean that the viewer fully processed the visual details of those regions or remembered them. To gain a better understanding of the level of visual processing involved, we conducted a study to determine the impact of SGD on short-term spatial information recall.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[gaze direction]]></kw>
			<kw><![CDATA[image-space]]></kw>
			<kw><![CDATA[short-term memory]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809586</person_id>
				<author_profile_id><![CDATA[81100645165]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Reynold]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bailey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rochester Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rjb@cs.rit.edu]]></email_address>
			</au>
			<au>
				<person_id>P2809587</person_id>
				<author_profile_id><![CDATA[81100069081]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ann]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McNamaray]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ann@viz.tamu.edu]]></email_address>
			</au>
			<au>
				<person_id>P2809588</person_id>
				<author_profile_id><![CDATA[81100553787]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Cindy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grimm]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[cmg@cse.wustl.edu]]></email_address>
			</au>
			<au>
				<person_id>P2809589</person_id>
				<author_profile_id><![CDATA[81488665662]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Aaron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Costello]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rochester Institute o Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[atc2436@rit.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1559757</ref_obj_id>
				<ref_obj_pid>1559755</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bailey, R., McNamara, A., Sudarsanam, N., and Grimm, C. 2009. Subtle gaze direction. <i>ACM Trans. Graph. 28</i> (September), 100:1--100:14.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Impact of Subtle Gaze Direction on Short-Term Spatial Information Recall Reynold Bailey* Ann McNamara 
 Aaron Costello Rochester Institute of Technology Texas A&#38;M Rochester Institute o Technology Cindy 
Grimm§ Washington University in St. Louis Abstract The ability to direct a viewer s gaze about a scene 
has important ap­plications in computer graphics and data visualization. The Subtle Gaze Direction (SGD) 
technique developed by Bailey et al. [2009] provides the ability to guide a viewer s gaze to speci.c 
regions of a display. The technique, which combines real-time eye-tracking with subtle image-space modulation, 
has minimal impact on view­ing experience as it does not change the overall appearance of the scene being 
viewed. Subtlety is achieved by presenting the modu­lations only to the low-acuity peripheral regions 
of the .eld of view so the viewer is never allowed to scrutinize the modulations. The technique has been 
shown to be quite fast and accurate: viewers typically attend to target regions within 0.5 seconds of 
the onset of the modulation and the resulting .xations are typically within a single perceptual span 
of the target. While this shows that the tech­nique is successful at directing gaze, it does not necessarily 
mean that the viewer fully processed the visual details of those regions or remembered them. To gain 
a better understanding of the level of visual processing involved, we conducted a study to determine 
the impact of SGD on short-term spatial information recall. CR Categories: I.3.3 [Computer Graphics]: 
Picture/Image Generation Display algorithms; Keywords: Gaze direction, image-space, short-term memory 
Links: DL PDF 1 Approach Participants viewed a randomized sequence of 25 images. Fol­lowing each image, 
they were presented with a blank screen and asked to recall the location of speci.c objects or regions. 
They were instructed to use the mouse to draw the smallest rectangles that bounded each target object 
or region. Their input was analyzed to determine how accurate their short-term spatial recollection was 
in terms of number of targets, location, and shape. 30 participants (4 females, 26 males), between the 
ages of 18 and 35 volunteered to participate in this study. Participants were randomly assigned to one 
of two groups: *e-mail: rjb@cs.rit.edu e-mail: ann@viz.tamu.edu e-mail: atc2436@rit.edu §e-mail: cmg@cse.wustl.edu 
  Figure 1: Photograph of experiment setup. The eye-tracking hard­ware is .xed to the bottom of the 
screen. Static group: 10 participants viewed the images without the use of gaze direction. This group 
served as the control group for the experiment.  Gaze-directed group: 20 participants viewed the images 
with gaze direction turned on. For each image presented to the participants in this group, gaze direction 
was randomly selected to either direct the viewer s gaze away from the cor­rect targets or direct the 
viewer s gaze to the correct target regions of the image. Counterbalancing was used to ensure that every 
image appeared equally often in both gaze-directed conditions.  2 Results Our results show that the 
in.uence of SGD signi.cantly improved accuracy of target count and spatial location recall. In particular, 
we observed the following effects: SGD to correct targets results in a signi.cantly lower counting error 
compared to static viewing  SGD to correct targets results in a signi.cantly lower location error compared 
to static viewing  SGD to incorrect targets results in a signi.cantly higher loca­tion error compared 
to static viewing  SGD has no signi.cant impact on shape error  These results suggest that SGD plays 
a key role in aiding the pri­oritization of spatial attention thereby enabling the visual system to focus 
on key aspects of a scene while disregarding less salient information.  References BAILEY, R., MCNAMARA, 
A., SUDARSANAM, N., AND GRIMM, C. 2009. Subtle gaze direction. ACM Trans. Graph. 28 (Septem­ber), 100:1 
100:14. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, 
August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037837</article_id>
		<sort_key>110</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>8</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Facial cartography]]></title>
		<subtitle><![CDATA[interactive high-resolution scan correspondence]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037837</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037837</url>
		<abstract>
			<par><![CDATA[<p>Blendshape interpolation is one of the most successful techniques for creating emotive digital characters for entertainment and simulation purposes. The quest for ever more realistic digital characters has pushed for higher resolution blendshapes. Many existing correspondence techniques, however, have difficulty establishing correspondences between very detailed blendshapes. To further aggravate the problem, in certain cases (such as pronounced wrinkles) exact 1:1 correspondences are unlikely to be found, and consequently no objectively "correct" solution exists.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809590</person_id>
				<author_profile_id><![CDATA[81448592774]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cyrus]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Wilson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809591</person_id>
				<author_profile_id><![CDATA[81458649961]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Oleg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Alexander]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809592</person_id>
				<author_profile_id><![CDATA[81442617187]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Borom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tunwattanapong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809593</person_id>
				<author_profile_id><![CDATA[81100016992]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Pieter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The College of William & Mary]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809594</person_id>
				<author_profile_id><![CDATA[81385599077]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Abhijeet]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ghosh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809595</person_id>
				<author_profile_id><![CDATA[81440600975]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Jay]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Busch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809596</person_id>
				<author_profile_id><![CDATA[81384595115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Arno]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hartholt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809597</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Facial Cartography: Interactive High-Resolution Scan Correspondence Cyrus A. Wilson Oleg Alexander Borom 
Tunwattanapong Pieter Peers* Abhijeet Ghosh Jay Busch Arno Hartholt Paul Debevec USC Institute for Creative 
Technologies The College of William &#38; Mary* Figure 1: Morph sequence between scans of neutral pose 
and corresponded extreme pose. Low-resolution meshes and high-resolution texture and normal maps are 
blended to give a high-quality result rendered in real-time. Introduction Blendshape interpolation is 
one of the most suc­cessful techniques for creating emotive digital characters for enter­tainment and 
simulation purposes. The quest for ever more realis­tic digital characters has pushed for higher resolution 
blendshapes. Many existing correspondence techniques, however, have dif.culty establishing correspondences 
between very detailed blendshapes. To further aggravate the problem, in certain cases (such as pro­nounced 
wrinkles) exact 1:1 correspondences are unlikely to be found, and consequently no objectively correct 
solution exists. In this work we propose a correspondence technique which allows a trained artist to 
provide guidance during the optimization in order to generate the highest quality blendshape correspondences. 
To our knowledge this is the .rst comprehensive system, which we call Fa­cial Cartography, that allows 
the user to participate in the process. Our method does not rely on markers, but instead leverages geo­metric 
and photometric similarities, and enables interactive user di­rection, to quickly obtain correspondences 
suitable for blendshape animation. Method A key component in our system is an active visage:a proxy for 
representing corresponded blendshapes. While concep­tually similar to a deformable template, a key difference 
is that a deformable template deforms a 3D mesh, while an active visage is constrained to the manifold 
of the non-neutral expression geometry, and hence only deforms the correspondences (this implies no 3D 
error). We have developed a modular optimization framework in which four different forces act on these 
active visages to compute accurate correspondences. These four forces are: 1. image forces: favoring 
correspondences that provide the best alignment of .ne-scale features in the detail maps; 2. internal 
forces: avoiding physically implausible deformations by promoting an as-rigid-as-possible deformation; 
 3. shape forces: guiding the optimization estimate (along the ex­pression geometry manifold) to result 
in a deformed template which more closely resembles a target 3D shape; and 4. user directable forces 
that in conjuction with a GPU imple­mentation of the other forces, enable the user to participate in 
the optimization process, and guide the optimization.  Please refer to the supplemental document for 
a more detailed de­scription. Results While fully-automatic techniques exist, they often fail on dif.cult 
cases (e.g., extreme expressions), requiring one to handle such cases manually. However with a manual 
approach it is im­practical to precisely align .ne-scale details (such as skin pores), as seen in Figure 
2. Alignment of such features is important if high-resolution detail maps are to be blended seamlessly, 
without introducing ghosting artifacts.  Figure 2: Comparison. A texture from a non-neutral expression 
is mapped into the UV space de.ned for the neutral expression (top­left) using three different approaches 
(top row). We assess how well .ne-scale details align by high-pass .ltering the textures (second row) 
and computing the difference between expression and neutral textures after registration (third and fourth 
row). In our approach the artist and computer interact: the artist guides the optimization, as needed, 
through dif.cult or ambiguous situ­ations; and the computation re.nes the optimization on the local scale 
(Figure 2). Figure 1 shows a blend between two expressions which have been corresponded using our technique. 
The correspon­dences obtained with this technique enable us to convincingly blend high-resolution maps. 
See the video for additional examples. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, 
British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037838</section_id>
		<sort_key>120</sort_key>
		<section_seq_no>4</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Tiles and textures and faces oh my!]]></section_title>
		<section_page_from>4</section_page_from>
	<article_rec>
		<article_id>2037839</article_id>
		<sort_key>130</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>9</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Generating displacement from normal map for use in 3D games]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037839</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037839</url>
		<abstract>
			<par><![CDATA[<p>Tessellation is one of the main features of most recent Direct3D 11 API. It enables dicing of incoming geometry on the fly and gives developer control over newly created vertices. If displacement map is available, one can displace those vertices to create rich adaptively generated content in real-time. Until recently just a few games made use of displacement maps. Bump mapping with normal maps is used much wider. Weak point of bump mapping is lack of silhouettes, occlusion, and shadows, which reveals the coarseness of geometry. However, process of converting existing content to use displacement mapping is very long and expensive, and so few game developers risk significant budgets to perform such endeavor without guaranteed investment return. We propose a relatively inexpensive solution of automatic conversion of normal maps to displacement maps.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809598</person_id>
				<author_profile_id><![CDATA[81100598893]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kirill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dmitriev]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809599</person_id>
				<author_profile_id><![CDATA[81488657481]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Evgeny]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Makarov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>507101</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[James F. Blinn. Simulation of wrinkled surfaces. <i>In Proceedings of SIGGRAPH 1978, pp. 286--292</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kaneko, T., et al, Detailed Shape Representation with Parallax Mapping. <i>In Proceedings of ICAT 2001, pp. 205--208</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Microsoft, Inc. 2011. Tessellation Overview, http://msdn.microsoft.com/en-us/library/ff476340(VS.85).aspx]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Generating Displacement from Normal Map for use in 3D Games displacementtoold 2011-02-18 20-46-25-81.bmp 
DisplacementToolD 2011-02-18 20-46-21-38.bmp Kirill Dmitriev          Evgeny Makarov NVIDIA 
          NVIDIA  Original Mesh Displaced Mesh  1. Introduction Tessellation is one of the 
main features of most recent Direct3D 11 API. It enables dicing of incoming geometry on the fly and gives 
developer control over newly created vertices. If displacement map is available, one can displace those 
vertices to create rich adaptively generated content in real-time. Until recently just a few games made 
use of displacement maps. Bump mapping with normal maps is used much wider. Weak point of bump mapping 
is lack of silhouettes, occlusion, and shadows, which reveals the coarseness of geometry. However, process 
of converting existing content to use displacement mapping is very long and expensive, and so few game 
developers risk significant budgets to perform such endeavor without guaranteed investment return. We 
propose a relatively inexpensive solution of automatic conversion of normal maps to displacement maps. 
 2. Exposition Receiving normal map from displacement map is an easy task. Normal at every surface point 
is perpendicular to displacement map gradient. Inverse problem requires integration, which in general 
is never guaranteed to yield precise result because: - The information about surface discontinuities 
is lost in normal map; - Normal maps are usually lossy-compressed;  Because of those problems we do 
not hope to reconstruct displacement exactly, but instead target visually-plausible result. First step 
of conversion is creating of Depth Difference Map (DDM). This is float2 texture map where X channel shows 
how much does the depth change when traversing a pixel in horizontal direction and Y channel shows the 
same for vertical direction. 1D example of DDM from normal map computation is shown below.      
Figure 1. Depth difference is signed value computed from normal map  In order to compute displacement 
of one texel in respect to another, one has to integrate DDM along the curve connecting those texels. 
Any curve can be chosen in principle. We use straight lines. DDM, as well as the normal map it is produced 
from, contains significant errors. To suppress the error and get nicely looking result, we need some 
heuristic assumptions. We are making two: - Integral of displacement map should be 0; - Whatever integration 
paths we chose, they should not prioritize one texel over another or treat one integration direction 
over another;  The resulting algorithm executes the same procedure for every texel and works completely 
on the GPU. Starting from the current sample position we trace a number of 2D rays, uniformly distributed 
over 360 degrees. We integrate DDM along each ray and find approximate depth integral (denoted as AvgD) 
inside the circle, assuming that depth we started from was zero. Now to make the integral zero, we need 
to displace central texel by AvgD. This value is recorded to displacement map. Conversion process can 
be done offline. In general we found radiuses of 250 texels with 500 directions to be enough for most 
textures being commonly used. If normal map consists of several pieces (atlas), those pieces should 
not affect each other, and generated displacement map should be continuous across atlas boundaries to 
avoid cracks in displaced geometry. We address those issues by breaking integration process on the boundaries 
and performing extra steps to fix the seams. References JAMES F. BLINN. Simulation of wrinkled surfaces. 
In Proceedings of SIGGRAPH 1978, pp. 286-292 KANEKO, T., ET AL, Detailed Shape Representation with Parallax 
Mapping. In Proceedings of ICAT 2001, pp. 205-208 MICROSOFT, INC. 2011. Tessellation Overview, http://msdn.microsoft.com/en-us/library/ff476340(VS.85).aspx 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037840</article_id>
		<sort_key>140</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>10</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Per-face texture mapping for real-time rendering]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037840</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037840</url>
		<abstract>
			<par><![CDATA[<p>Texture unwrapping is a nearly ubiquitous step in texture mapping, and serves two important purposes: (1) it indicates where to sample texture across each face and (2) it allows artistic freedom to "trade" texels from low importance areas to high importance areas. Unfortunately, texture unwrapping is time-consuming and automated tools generally produce inferior results to those of a skilled artist. Additionally, even the best unwraps suffer from seams, which cause lighting discontinuities at best and surface discontinuities at worst.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809600</person_id>
				<author_profile_id><![CDATA[81488652119]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McDonald]]></last_name>
				<suffix><![CDATA[Jr]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jmcdonald@nvidia.com]]></email_address>
			</au>
			<au>
				<person_id>P2809601</person_id>
				<author_profile_id><![CDATA[81332491776]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Brent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Burley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[Brent.Burley@disney.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2383478</ref_obj_id>
				<ref_obj_pid>2383465</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Burley, B., and Lacewell, D. 2008. Ptex: Per-Face Texture Mapping for Production Rendering. In <i>Eurographics Symposium on Rendering 2008</i>, 1155--1164.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Microsoft, Inc. 2011. Semantics (DirectX HLSL), http://msdn.microsoft.com/en-us/library/bb509647%28v=vs.85%29.aspx]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Per-Face Texture Mapping for Real-time Rendering John McDonald, Jr NVIDIA Corporation jmcdonald@nvidia.com 
 Brent Burley Walt Disney Animation Studios Brent.Burley@disney.com 1. Introduction Texture unwrapping 
is a nearly ubiquitous step in texture mapping, and serves two important purposes: (1) it indicates where 
to sample texture across each face and (2) it allows artistic freedom to .trade. texels from low importance 
areas to high importance areas. Unfortunately, texture unwrapping is time-consuming and automated tools 
generally produce inferior results to those of a skilled artist. Additionally, even the best unwraps 
suffer from seams, which cause lighting discontinuities at best and surface discontinuities at worst. 
 Ptex, proposed in [Burley and Lacewell 2008], assigns a separate texture per face, skipping the texture 
unwrap entirely. The technique uses adjacency data to filter across faces, removing visible seams. We 
propose a technique to render Ptex datasets intended for offline rendering at interactive framerates 
using commodity Direct3D 11 capable hardware. 2. Algorithm As with offline Ptex, meshes must be quad-based, 
textures must be power-of-two sized (though not necessarily square), and patch adjacency data must be 
provided. The proposed real-time Ptex algorithm involves a preprocess step as well as straightforward 
shader modifications to texture lookups. Textures are first grouped by aspect ratio. Within each group, 
textures are sorted by decreasing size and assigned monotonically increasing IDs in this sorted order. 
Textures are stored in a manual mip-chain using one Texture2DArray per unique resolution, and each array 
is indexed by texture ID. Data from adjacent patches are sampled into a fixed size border around each 
texture. The border size can be chosen according to desired filter quality. A .patch constant. structure 
stores information about each patch's texture data: base array ID, texture ID, and maximum mip-level. 
 During shading, we: 1. Use SV_PrimitiveID [Microsoft 2011] to index into an array of patch constant 
structures to determine the ptex parameter set of the current patch. 2. Compute the desired mip-level 
and select the corresponding texture array. 3. Adjust SV_DomainLocation [Microsoft 2011] to account 
for the fixed size border. The scale and offset are pre-computed per unique texture resolution and are 
accessed from the shader. 4. Sample the specified texture array entry using desired hardware texture 
filter. 5. Repeat steps 2-4 once more if mip-map interpolation is desired.    Figure 1. Dinosaur 
with each face textured from unique textures. 3. Advantages  The texture unwrap step is skipped entirely. 
Texture seams will be invisible along edges and at vertices with a valence of four. Additionally, each 
face can have a texture size completely independent of its neighbors and this can be changed at any point 
in a model s lifetime without having to go through another expensive unwrap operation. Unlike most unwrapping 
schemes, no texture space is wasted, and arbitrary filter widths may be used without sourcing invalid 
texture data. 4. Results We ve applied this method of rendering to several test meshes, including the 
dinosaur pictured above. The dinosaur has 5,812 faces each individually textured. The working set of 
80 mega-texels renders in less than one half of one millisecond on a GTX 460 1 Gb. References BURLEY, 
B., AND LACEWELL, D. 2008. Ptex: Per-Face Texture Mapping for Production Rendering. In Eurographics Symposium 
on Rendering 2008, 1155 1164. MICROSOFT, INC. 2011. Semantics (DirectX HLSL), http://msdn.microsoft.com/en-us/library/bb509647%28v=vs.85%29.aspx 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037841</article_id>
		<sort_key>150</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>11</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Spherical skinning with dual quaternions and QTangents]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037841</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037841</url>
		<abstract>
			<par><![CDATA[<p>Building upon previous work with Dual Quaternion skinning [KAVAN 2008], we will introduce a new format to represent tangent frames.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809602</person_id>
				<author_profile_id><![CDATA[81488672362]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ivo]]></first_name>
				<middle_name><![CDATA[Zoltan]]></middle_name>
				<last_name><![CDATA[Frey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Crytek Research & Development, Frankfurt am Main, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ivof@crytek.com]]></email_address>
			</au>
			<au>
				<person_id>P2809603</person_id>
				<author_profile_id><![CDATA[81488669688]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ivo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Herzeg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Crytek Research & Development, Frankfurt am Main, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ivo@crytek.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1409627</ref_obj_id>
				<ref_obj_pid>1409625</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kavan, L., Collins, S., Zara, J., and O'Sullivan, C. 2008. Geometric Skinning with Approximate Dual Quaternion Blending. In <i>ACM Trans. Graph</i>., vol. 27, ACM, 105.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037842</section_id>
		<sort_key>160</sort_key>
		<section_seq_no>5</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Changing dimensions]]></section_title>
		<section_page_from>5</section_page_from>
	<article_rec>
		<article_id>2037843</article_id>
		<sort_key>170</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>12</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Design and realization of stereoscopic 3D for Disney classics]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037843</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037843</url>
		<abstract>
			<par><![CDATA[<p>The stereoscopic conversions of Disney's <i>The Lion King</i> and <i>Beauty and the Beast</i> required development of novel techniques to art direct, edit, and visualize 3D depth. Producing stereo without geometric models meant the creation of an extensive toolset to create and validate plausible depth for each pixel on screen.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809604</person_id>
				<author_profile_id><![CDATA[81488667981]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Katie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tucker-Fico]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809605</person_id>
				<author_profile_id><![CDATA[81442598713]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Evan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goldberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809606</person_id>
				<author_profile_id><![CDATA[81488669811]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koneval]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809607</person_id>
				<author_profile_id><![CDATA[81442592544]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Dale]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mayeda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809608</person_id>
				<author_profile_id><![CDATA[81442601792]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neuman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809609</person_id>
				<author_profile_id><![CDATA[81488669547]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Olun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Riley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809610</person_id>
				<author_profile_id><![CDATA[81488670067]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schnittker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Williams, L. 1991. Shading in two dimensions. <i>Proceedings of Graphics Interface 91</i>, 143--151.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Design and Realization of Stereoscopic 3D for Disney Classics Katie Tucker-Fico Evan Goldberg Kevin 
Koneval Dale Mayeda Robert Neuman Olun Riley Matthew Schnittker Figure 1:  (a) Initial stereographer 
markups. (b) depthUI interface. (c) Normalized depiction of pixel offsets with hit planes. The stereoscopic 
conversions of Disney s The Lion King and Beauty and the Beast required development of novel techniques 
to art direct, edit, and visualize 3D depth. Producing stereo without geometric models meant the creation 
of an extensive toolset to create and validate plausible depth for each pixel on screen. 1 Markup Process 
and DepthUI The conversion process starts with a stereographer. Each shot goes through a markup session 
(Figure 1a) in which inter-ocular offsets are defined for the scene s elements. In our system, zero parallax 
denotes screen depth, positive values are in front of the screen, and negative are behind. The offsets 
vary from shot to shot and can even be animated. Proper choice of offset values, both within a shot and 
between shots, is crucial. The final stereoscopic images follow a depth script that enhances the story 
and emotion of the film. Audience comfort is another factor. The created depth should represent plausible 
geometry for the characters, props, and sets, and it should avoid scenarios that cause eye fatigue. Scenes 
are issued to artists with each animated element on its own layer (i.e. a virtual diorama). The layers 
can be placed in front of or behind each other at different depths to produce the desired stereoscopic 
composition. Offset values from the markup session enter our stereoscopic pipeline via an interface called 
depthUI (Figure 1b). This is the control center for all depth information in the scene, as well as a 
convenient means to view and control depth continuity across a sequence. Layer depth is entered as horizontal 
offsets. Using trigonometric conversions based on a pair of virtual cameras, we can express layer information 
both as pixel offsets or z-positions. Since our stereo conversion pipeline lacks geometry, this ability 
to convert between units helps artists keep plausible depth by expressing object distances as they might 
exist in a physical environment. If the scene requires more or less overall depth, global max and min 
controls can be adjusted, while individual layers update via direct proportions. 2 Environments, Characters, 
Effects Artists begin with environment layers, segmenting as necessary to convey the markup depth. Proprietary 
tools include depth primitives, such as quads, radials, and joints. They are combined to sculpt the initial 
depths and shapes. Textural detail from thebackgrounds is then used to enhance subtle characteristics 
ofform, adding tactile and immersive qualities to the environment. Characters require a great deal of 
artistry and care. They needbelievable internal volumes, solid contact with the environment, and interaction 
with other characters and props. Most begin with abase inflation [Williams, 1991] to apply an overall 
roundness. Then iterative steps of pushing and pulling with our gradientprimitives are used to sculpt 
life into our Disney characters. Volumetric effects such as snowstorms, rain, and dust are among the 
most challenging artwork to convert. Our technique blends thefinal depth maps with animated fractal patterns. 
This allows effects elements to contour characters and environments while volumetrically pushing away 
from the surfaces. 3 Visualization Using stereoscopic monitors, an artist s binocular vision can detect 
the existence of depth conflicts, but software visualization tools are necessary to profile exact pixel 
offsets. We developed such a tool built around normalized grayscale representations of the scene s depth 
values (Figure 1c). On every frame, the visualization process receives four inputs per sorted composition 
max,di layer Li={Ii,Mi,di min}, i![1,...,n]: Ii - RGBA image for each layer of the composition. Mi - 
artist-generated depth map of normalized, per-pixel offsets. dimax - closest offset value of the layer. 
dimin - furthest offset value of the layer (i.e. horizontal shift). min+Mp max-di The depth of each 
pixel dpi in a layer is di i(di min). We take the minimum dimin and the maximum dimax over all layers 
to determine the entire scene s depth range and convert dpi into normalized visualization space (dpi)'. 
We then use the painter s algorithm to iterate through the layers and determine which (dpi)' to represent 
at each pixel in the visualization. The resulting image is a normalized grayscale of the entire scene 
s depth. Finally, an inverse normalization converts values back to offsets. The artist can input a specific 
depth to profile, and the tool appropriately colors each pixel that intersects this hit plane. 4 Conclusion 
We have described a paradigm for interactively creating and visualizing stereoscopic 3D in the absence 
of geometric models. Our techniques are generic to all compositing packages, and they are applicable 
to a variety of pipelines, including live action stereo conversion and full CG productions. References 
WILLIAMS, L. 1991. Shading in two dimensions. Proceedings of Graphics Interface 91, 143-151. Copyright 
is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. 
ISBN 978-1-4503-0921-9/11/0008   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037844</article_id>
		<sort_key>180</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>13</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[StereoFX]]></title>
		<subtitle><![CDATA[survey of the main stereo film-making techniques]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037844</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037844</url>
		<abstract>
			<par><![CDATA[<p>The unprecedented popularity of <i>Avatar</i> has pushed the entire movie industry to rapidly evolve to be able to produce "3D" movies. In the past year three main techniques of producing them have been used each with their advantages and flaws. We review them using as examples movies MPC has been involved in: <i>Pirates of the Caribbean: On Stranger Tides, The Chronicles of Narnia: The Voyage of the Dawn Treader and Harry Potter and the Deathly Hallows: Part 2</i></p> <p>In the following presentation we will discuss the principles of each technique, their pros and cons and the changes they impose on previous workflow and mindsets.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809611</person_id>
				<author_profile_id><![CDATA[81466647316]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Damien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fagnou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[damien-f@moving-picture.com]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 StereoFX: survey of the main stereo .lm-making techniques Damien Fagnou* MPC Abstract The unprecedented 
popularity of Avatar has pushed the entire movie industry to rapidly evolve to be able to produce 3D 
movies. In the past year three main techniques of producing them have been used each with their advantages 
and .aws. We review them us­ing as examples movies MPC has been involved in : Pirates of the Caribbean: 
On Stranger Tides, The Chronicles of Narnia: The Voyage of the Dawn Treader and Harry Potter and the 
Deathly Hal­lows: Part 2 In the following presentation we will discuss the principles of each technique, 
their pros and cons and the changes they impose on pre­vious work.ow and mindsets. Native Stereo To 
illustrate native stereo we look at speci.c examples of work done on Pirates of the Caribbean: On Stranger 
Tides. Figure 1: Side by side c&#38;#169;2011 Walt Disney Pictures Native Stereo involves .lming with 
two cameras (one for each eye). Cameras are mounted together using specially built rigs that ensure proper 
alignment and synchronisation between each camera. Hav­ing to deal with two sequences of images per shot 
brings several complications to the VFX process. It is very hard to acquire completely identical (apart 
from horizontal shift) sets of images. Even with the latest and most advanced stereo camera rigs slight 
misalignments or distortion can be present. As in most rigs the slave camera .lms though a mirror, there 
is often color shifts between the two views. Using Ocula we were able to re-align and color match the 
plates. We also adjusted the pipeline around this new process to make it very transparent to the compositors. 
Once those differences are recti.ed the task at hand is still quite large, as many of the processes have 
to be done twice (one for each view). This is the case for matchmove, paint and rotoscoping but with 
the help of modern software it can be made simpler. Having to manage two points of view creates new challenges 
for compositing and rendering. For this, we leveraged Nuke and Pixar RenderMan to simplify the task, 
and adjusted our pipeline accord­ingly. *e-mail: damien-f@moving-picture.com 2 Post Conversion We call 
post conversion the process of creating a stereo version of a mono .lm. Other terms for this process 
are dimensionalization and 2D to 3D . This technique was used to create the 3D version of The Chronicles 
of Narnia: The Voyage of the Dawn Treader. Creation of the second view almost always requires rotoscoping 
a large portion of the image to isolate the parts that need to go at different depths. In general, very 
accurate detail is needed Using solely rotoscoping and image shifting can create a cardboard cut-out 
effect. In those cases a geometry reconstruction of the scene using matchmove and camera modeling is 
necessary. The scene is then rendered using projections. Another possible approach is to create depth 
maps for each object and combine them. Each method has situations to which it is better suited than the 
other. Whether we choose depth shifting or projection, occluded areas are revealed which need to be painted. 
This is one of the most time consuming tasks of the post conversion process. Using custom edge reconstruction 
or temporal algorithms we can reduce the manual painting needed. 3 Mixed Stereo c Figure 2: Roto and 
Projections &#38;#169;2011 Warner Bros. Pictures For the last of the Harry Potter series Harry Potter 
and the Deathly Hallows: Part 2 the Mixed Stereo technique was used. Mixed Stereo represents the hybrid 
approach. The goal is to use the two previously described techniques where they are most appropriate. 
This method has gained a lot of popularity in the last year as it can give results that are similar to 
Native Stereo without some of the cost and .lming complications. Scheduling Mixed Stereo is hard due 
to the much larger number of tasks and their interdependencies. But using a parallel approach where the 
.lm plates get processed at the same time as the visual effects are developed and incorporated we can 
improve conversion quality while keeping tight deadlines. The geometric 2D to 3D technique is particularly 
useful in mixed stereo projects and we made heavy use of VFX assets to speed up the conversion process 
while improving the overall quality of the output. Most of the CG elements were rendered using stereo 
cam­eras, therefore achieving optimal quality. Copyright is held by the author / owner(s). SIGGRAPH 2011, 
Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037845</article_id>
		<sort_key>190</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>14</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Developing tools for 2D/3D conversion of Japanese animations]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037845</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037845</url>
		<abstract>
			<par><![CDATA[<p>2D/3D conversion is a process where 3D information is added to an originally 2D flat content. The content may be of three kinds: live-action movies, 3DCG movies, and animated cartoon. In this talk we will describe a set of in-house tools to efficiently convert from Japanese hand-drawn animation to stereoscopic animation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809612</person_id>
				<author_profile_id><![CDATA[81316490655]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Salvati]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[OLM Digital, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809613</person_id>
				<author_profile_id><![CDATA[81448596218]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Miki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kinoshita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[OLM Digital, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809614</person_id>
				<author_profile_id><![CDATA[81335492772]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yosuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Katsura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[OLM Digital, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809615</person_id>
				<author_profile_id><![CDATA[81100085512]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Anjyo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[OLM Digital, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809616</person_id>
				<author_profile_id><![CDATA[81100506136]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Tatsuo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yotsukura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[OLM Digital, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809617</person_id>
				<author_profile_id><![CDATA[81320495833]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Hiroshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Uchibori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[OLM Digital, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1598064</ref_obj_id>
				<ref_obj_pid>1597990</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Goldberg, E. 2009. Medial axis techniques for stereoscopic extraction. In <i>SIGGRAPH 2009: Talks</i>, ACM, New York, NY, USA, SIGGRAPH '09, 74:1--74:1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Developing Tools for 2D/3D Conversion of Japanese Animations Marc Salvati Yosuke Katsura Tatsuo Yotsukura 
Miki Kinoshita Ken Anjyo Hiroshi Uchibori OLM Digital, Inc.  Figure 1: From left to right: single eye, 
depth, anaglyph rendering. Inazuma Eleven the Movie (c)L5/FCIE M 2010 1 Japanese Animation to Stereoscopy 
2D/3D conversion is a process where 3D information is added to an originally 2D .at content. The content 
may be of three kinds: live-action movies, 3DCG movies, and animated cartoon. In this talk we will describe 
a set of in-house tools to ef.ciently convert from Japanese hand-drawn animation to stereoscopic animation. 
Typical conversion process usually involves depth map or 3D model generation. Depth map can be use to 
apply some displace effects on pixel while 3D model can be used to project original art work and re-render 
it with two cameras. As for 3DCG movies and live-action movies, we can retrieve 3D information using 
motion continuity and time coherency (lights, shadows, shape retrieval, tracking...). On the other hand, 
animated cartoon conversion is not straightfor­ward. In animated cartoon, a piece of a fantastic and 
unreal world has been drawn in 2D, on a piece of paper by an artist. The col­ors are usually .at, the 
shapes are exaggerated, and the motion is not continuous. Color information (like color saturation), 
time co­herency may not be used easily to convert from 2D to 3D. Moreover, in the case of Japanese animation, 
for budget reasons, every single drawing is repeated 2-3 times (or more) in a 24 frame per second movie. 
Animation is usually minimal, colors are almost always .at, and the drawings have really few details. 
Even though, those restrictions give to Japanese animation a speci.c style, they make it really dif.cult 
to apply common automatic 2D/3D conver­sion. That s why we developed a set of tools to help designers 
in this mostly non-automatic conversion process. The .rst movie cre­ated using those tools was screened 
in theater : Inazuma Eleven the Movie (2010). 2 The Tools In the case of Inazuma Eleven the Movie, we 
used only 3D back­ground, which halved the problem of rotoscoping and the problem of painting holes after 
displacement. For all CG elements that we rendered, we decided to output depth instead of pair of pictures. 
It uni.es the work.ow and makes it simpler to adjust stereoscopic feeling. We then needed to develop 
tools for three major tasks: ro­toscoping, depth generation and depth. 2.1 Rotoscoping One color is usually 
associated to one part of a character or an ob­ject. Then .at colors of Japanese animation can be used 
to extract parts information. In most compositing package, color keying fea­ture is available, but to 
isolate one color, it means keying all other colors! That s why we developed a simple tool to keep a 
set of colors. We also added the possibility of analyzing connection be­tween colors to automatically 
retrieve shadow colors too. If the color is used in different part of one character, we may get a set 
of separated parts. To separate it in various layers, we use some region recognition and tracking method. 
 2.2 Depth Generation To make it easier to give depth to the various objects, we devel­oped a point gradation 
and path gradation tool. Given a set of points/paths and corresponding colors, gray levels for depth, 
we generate a depth map using scattered data interpolation. We added some tracking features to avoid 
manual tracking of points/paths dur­ing animation. 2.3 Depth Adjustment Once each object depth has been 
created, we need to assemble ev­erything to create the depth for the whole scene. The assembly can be 
a tedious task, so we enable the designer to set each layer depth range with distance number, and we 
then rescale the depth automatically to meet those distance constraints. We then can set a global depth 
range for the scene, but we cannot ensure precise char­acter position relative to the background. We 
then added a color constraint tool where the user can enforce seamless and continuous depth appearance. 
 3 Conclusion and Future Works We developed a whole set of general tools to make it easier to con­vert 
animation to stereoscopic animation. The tools are general and may be used even in non-stereoscopic context. 
We thought about developing some medial axis generation tools like seen re­cently[Goldberg 2009]. The 
speed and the parameter tuning of our implementation were not satisfying production needs. Still, design­ers 
felt that it would be a great tool if we improve it. We also thought about adding some magic selection 
tool to make rotoscope easier. The user would ideally place some point, and according to some adjacency 
condition, a whole part of the character (like head, or arm) would be selected. The current implementation 
of our tools has speed issues, but we hope to alleviate those limitations in the near future. References 
GOLDBERG, E. 2009. Medial axis techniques for stereoscopic extraction. In SIGGRAPH 2009: Talks, ACM, 
New York, NY, USA, SIGGRAPH 09, 74:1 74:1. Copyright is held by the author / owner(s). SIGGRAPH 2011, 
Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037846</article_id>
		<sort_key>200</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>15</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Processing.js]]></title>
		<subtitle><![CDATA[sketching with &lt;canvas&gt;]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037846</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037846</url>
		<abstract>
			<par><![CDATA[<p>The Processing language [Casey Reas 2007], first introduced by Ben Fry and Casey Reas in 2001, is a simple, elegant language for data visualization that is already being used by artists, educators and commercial media groups to produce rich graphical content called sketches. Because Processing is implemented in Java, delivering Processing sketches via a web page requires the user to install a Java plug-in. Processing.js, in comparison, is an open source, cross browser JavaScript port of the Processing language; it translate Processing sketches into JavaScript using the &lt;canvas&gt; element for rendering. No additional plug-ins are required to view a Processing sketch delivered with Processing.js. Furthermore, Processing.js is much more than just a Processing parser written in JavaScript: it enables the embedding of other web technologies into Processing sketches and vice versa. Processing.js seamlessly integrates web technologies with the Processing language to provide an excellent framework for rich multimedia web applications.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809618</person_id>
				<author_profile_id><![CDATA[81392618002]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andor]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Salga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seneca College, Toronto, ON, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[asalga@learn.senecac.on.ca]]></email_address>
			</au>
			<au>
				<person_id>P2809619</person_id>
				<author_profile_id><![CDATA[81488669152]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hodgin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seneca College, Toronto, ON, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[dhodgin@learn.senecac.on.ca]]></email_address>
			</au>
			<au>
				<person_id>P2809620</person_id>
				<author_profile_id><![CDATA[81488673129]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Anna]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sobiepanek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seneca College, Toronto, ON, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[anna.sobiepanek@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P2809621</person_id>
				<author_profile_id><![CDATA[81488668686]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Downe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seneca College, Toronto, ON, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sdowne@learn.senecac.on.ca]]></email_address>
			</au>
			<au>
				<person_id>P2809622</person_id>
				<author_profile_id><![CDATA[81488669401]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Mickael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Medel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seneca College, Toronto, ON, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mnmedel@learn.senecac.on.ca]]></email_address>
			</au>
			<au>
				<person_id>P2809623</person_id>
				<author_profile_id><![CDATA[81392614183]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Catherine]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Leung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seneca College, Toronto, ON, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[catherine.leung@senecac.on.ca]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1296181</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Casey Reas, Ben Fry, J. M. 2007. <i>Processing: a programming handbook for visual designers and artists</i>. MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Processing.js: Sketching with <canvas> Andor Salga Daniel Hodgin Anna Sobiepanek Seneca College Seneca 
College Seneca College Toronto, ON, Canada Toronto, ON, Canada Toronto, ON, Canada asalga@learn.senecac.on.ca 
dhodgin@learn.senecac.on.ca anna.sobiepanek@gmail.com Scott Downe Mickael Medel Catherine Leung Seneca 
College Seneca College Seneca College Toronto, ON, Canada Toronto, ON, Canada Toronto, ON, Canada sdowne@learn.senecac.on.ca 
mnmedel@learn.senecac.on.ca catherine.leung@senecac.on.ca 1 Introduction The Processing language [Casey 
Reas 2007], .rst introduced by Ben Fry and Casey Reas in 2001, is a simple, elegant language for data 
visualization that is already being used by artists, educators and commercial media groups to produce 
rich graphical content called sketches. Because Processing is implemented in Java, delivering Processing 
sketches via a web page requires the user to install a Java plug-in. Processing.js, in comparison, is 
an open source, cross browser JavaScript port of the Processing language; it translate Pro­cessing sketches 
into JavaScript using the <canvas> element for rendering. No additional plug-ins are required to view 
a Processing sketch delivered with Processing.js. Furthermore, Processing.js is much more than just a 
Processing parser written in JavaScript: it enables the embedding of other web technologies into Processing 
sketches and vice versa. Processing.js seamlessly integrates web technologies with the Processing language 
to provide an excellent framework for rich multimedia web applications. John Resig released a proof of 
concept to show how the <canvas> element could be used to render Processing sketches in 2007 as an open 
source project named Processing.js. While it was able to ren­der the 2D sketches found on the Processing 
website, it was not optimized and far from complete. In 2009, the authors of this paper took up the task 
to bring Processing.js to parity with Processing. The 3D functionalities, which had not existed in the 
original Pro­cessing.js demonstration, were implemented. Numerous bugs were .xed and Processing.js was 
re-engineered to be more reliable and ef.cient. In addition to contributing to the project s code base, 
the authors also organized the developer community by managing the work.ow, test systems, issue tracker, 
and repositories. In Novem­ber 2010, Processing.js 1.0 was released. 2 From Processing to Processing.js 
At the time the original Processing language was developed, Java was expected to become the lingua franca 
of web programming, while JavaScript was a client-side scripting language for small tasks. As the web 
has matured, JavaScript has become the na­tive programming language of the web. While many miscon­ceptions 
regarding its speed and capabilities persist, recent ad­vances in JavaScript engine technology have greatly 
improved its performance. In particular, speed increases from its just-in-time (JIT) compilers and use 
of hardware acceleration for graphics help make it fast enough to support real-time interactive multi­media 
web applications. Furthermore, current versions of Firefox and Chrome, along with upcoming versions of 
Opera and Safari browsers, all include implementations of WebGL, a hardware accel­erated JavaScript API 
based on OpenGL ES 2.0. While the initial version of Processing.js was limited to Processing s 2D functions, 
the current version now takes advantage of WebGL to implement Processing s 3D functionality. Processing.js 
works by translating Processing code found in a web page into JavaScript and renders it using the <canvas> 
element on the page. Any JavaScript code encountered is left intact and exe­cuted as JavaScript. This 
method allows for the translation of exist­ing Processing sketches and the injection of JavaScript into 
Process­ing sketches without explicit labelling. Processing.js can correctly render sketches written 
in pure Processing while also easily han­dling sketches that make use of common JavaScript libraries 
such as jQuery. It also enables the sketch to easily interact with other elements of the web page or 
pull in resources from web services such as Twitter, Google Maps, and Flickr, without any plug-ins or 
additional software. Another important feature of Processing.js is that it masks the dif­ferences between 
browsers. Web standards are often loosely de­.ned and variations can exist, not only between different 
browsers but even between versions of the same browser on different plat­forms. The behaviour of something 
as simple as key events can vary widely between browsers and platforms. When creating inter­active content 
on the web, these small differences can create huge problems for the developers. Processing.js solves 
this problem and others like it by behaving the same way regardless of the browser or platform. This 
standardization makes applications developed with Processing.js highly portable, letting developers reach 
the widest possible audience with the least effort. 3 Conclusion Processing.js seamlessly integrates 
elements of a web page, web services, and interactive graphics. It was written to take advantage of modern 
browser features such as an optimized JavaScript engine and hardware accelerated 3D graphics, handling 
events in a consis­tent manner between different browsers, and making it easier for web developers to 
create content for a wide audience. All of these features make Processing.js an ideal platform for rich 
multimedia applications on the web. 4 Acknowledgements We want to thank: David Humphrey, John Resig, 
Alistair Mac-Donald, Corban Brook Riley, Yuri Delendik, Matthew Lam, Do­minic Baranski, Mike Kamermans, 
Tom Brown and the Process­ing.js community for all their work on the project. A special thanks to Marius 
Watz for letting us use his beautiful sketch ab­stract01js. Also thanks to Evan Weaver, Mike Hoye, Dawn 
Mercer, and NSERC CCIP for their support. References CASEY REAS, BEN FRY, J. M. 2007. Processing: a 
programming handbook for visual designers and artists. MIT Press. Copyright is held by the author / owner(s). 
SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037847</section_id>
		<sort_key>210</sort_key>
		<section_seq_no>6</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Let there be light]]></section_title>
		<section_page_from>6</section_page_from>
	<article_rec>
		<article_id>2037848</article_id>
		<sort_key>220</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>16</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Rango]]></title>
		<subtitle><![CDATA[a case of lighting and compositing a CG animated feature in an FX-oriented facility]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037848</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037848</url>
		<abstract>
			<par><![CDATA[<p>At Industrial Light + Magic we have a rich history of doing effects for features, but when Gore Verbinski (with whom we did the "Pirates of the Caribbean" films) approached us in the middle of 2008 to make an entire CG animated feature, we had to face a lot of new challenges.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809624</person_id>
				<author_profile_id><![CDATA[81488670325]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Leandro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Estebecorena]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light+Magic, San Francisco, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809625</person_id>
				<author_profile_id><![CDATA[81488664952]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Nelson]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sepulveda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light+Magic, San Francisco, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809626</person_id>
				<author_profile_id><![CDATA[81320494876]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sprout]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light+Magic, San Francisco, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Rango: A case of Lighting and Compositing a CG animatedfeature in an FX-oriented facility Leandro Estebecorena 
CG supervisor - Industrial Light+Magic San Francisco, CA, USA 1. Introduction At Industrial Light + 
Magic we have a rich history of doing effectsfor features, but when Gore Verbinski (with whom we did 
the Pirates of the Caribbean films) approached us in the middle of2008 to make an entire CG animated 
feature, we had to face a lot ofnew challenges. 2. Materials From early on the show, we setup a material 
library and ranturntables with all the importance-based materials in the library tostudy how the materials 
worked with occlusion and shadows.In Rango (as in no show we've done before at ILM), everythingvisible 
in frame (every single object and prop) had to be created (either locally or by 3rd party vendors) Besides 
the 60 main characters in the movie, there were approximately 1000 propsneeded. To ease the task of look 
dev for these props, some basicprocedural displacement materials were also provided in the library.We 
also tested the materials under different environments and used these turntables to quickly detect any 
visual artifacts or problemsevery time a new set of shaders was rolled out.  3. Lighting In every single 
show that we work on we always could extract cluesfor lighting from the plates or references from set, 
but "Rango" wasa totally new situation, although this lack of references was also anopportunity for trying 
new things.The light rigs for each sequence were setup the in the most simpleway - initially a key, ambient 
and a reflection lights. This convention was followed for all the exterior sequences and some of theinteriors. 
Instead of 360 degrees environments captured on set (aswe do in conventional shows), for the ambient 
and reflection lightswe used maps created from renders of the virtual set provided bythe Digital Matte 
department. Such maps were in some cases not stills but sequences generated from the center of the moving 
objectsfor more accurate representation of reflection/refraction (like in thecase of the moving bottle 
in the "Hawk Escape" sequence)In the setup, the Key light was a customized version of the distantlight 
shader constrained via a switcheable constrain to the skydome.We had a way to clone the distant key light 
on a character basiswhich allowed us for better interaction and improved deep shadows.In this setup you 
could rotate the whole rig to make the most generalchanges, or just rotate the key and just the cloned 
lights wouldfollow, or (for the more specific tweaks) you could move just thecloned lights around the 
character(s).Except for the light setup in the "campFire" and "SubterraneanJourney" sequences (which 
were using multiple area lights at thecenter of the fire) all of the others exteriors are using the samesimple 
format light rig of Distant Key, ambient and reflection lights. The light rig had two main switches: 
one to activate a sun reflectioncard (to provide for a softer sun reflection - mimicking the sun halovisible 
in "real life" reflections) and another switch to turn on theblur of the shadows based on distance. Now 
if you go outside in a shiny day and you cast a shadow withyour hand onto a piece of paper, you can see 
how the blurriness of Nelson Sepulveda Compositing supervisor - Industrial Light+Magic San Francisco, 
CA, USA Kevin SproutCG supervisor - Industrial Light+MagicSan Francisco, CA, USA the shadows' edge increases 
as you raise your hand above thesurface. That fuzzy outer edge of the shadow is called penumbraand it 
increases based on the distance to the casting point. As it turnsout, the width of the penumbra divided 
by its distance from yourhand is always a constant fraction of about 1/112. This is theangular diameter 
of the sun measured in radians. We implemented a switch to turn ON this softness based on distance for 
the shadows in the key light. And although we started with a value that visuallyimitates this "real life" 
results, in some cases we had to diverge fromwhat was "real" to follow artistic directions from the client 
(i.e. theshadows' softness in "Jake" sequence is somewhat exaggerated in anattempt to pursue the mood 
that the director wanted to establish)The simple key/env light setup was augmented with indirect bounce 
global illumination and was a starting point for preflight/sequencelighting. Full sequences could be 
launched and checked in this wayto set the general keylight direction and get all the shots workingtechnically. 
Then when a lighting TD was assigned, they could addspecific fill and rim and bounce card beauty lighting 
to final the shot.We wanted to render our hero characters separately from thebackground, so they could 
be tweaked and polished in the comp togive every frame a hand - crafted treatment. This expanded the 
compositing department's responsibilities and increased renderingrequirements, since we had to render 
a character by itself while stillincluding all of the shadows and bounced light from the surroundingcharacters 
and set, which were now invisible to the camera.  4. Compositing Originally planned as basic A over 
B, the composites quickly grew into a much more detailed process. The increased complexity madecompositing 
more akin to working on a feature film. The directorrequired specific beauty lighting for each shot, 
choreographed and lens accurate depth of field (all done in the composite by a customtool), varied atmospheric 
and photographic effects, (light aberrations, correct anamorphic flaring, heat ripple, camera jitter)Additional 
elements such as dust, foot prints, water, dirt and moredetailed levels of character integration became 
increasingly necessary. For this we had to quickly and efficiently change ourwork methodology to adjust 
to the much larger demand on the compcrew. We created customized generic script, changed the way we generate 
and handle files, and built multiple tools both inside and outof the comp package. 5. Summary Overall, 
this show was very unforgiving - it demanded us to question things that we usually can tolerate and consider 
acceptable in mostof the shows- but which turned out to be unbearable due to he amount of shots involved 
on this movie. We simplified andstandarized the light setup in the show, and totally reorganize theway 
we handle the material library. Also in the comp side, multiplenew techniques were used to create filmic/photo 
real images and streamline the comp generics. The Rango lighting and compositingteam managed to complete 
1550 shots (with a total of 36 compositorsand 56 lighting TDs at our peak). Copyright is held by the 
author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037849</article_id>
		<sort_key>230</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>17</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Ocean mission on <i>Cars 2</i>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037849</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037849</url>
		<abstract>
			<par><![CDATA[<p>On a mission to Dr. Z's lair in the middle of the ocean, secret agent Finn McMissile must face the harsh environment of a stormy ocean.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809627</person_id>
				<author_profile_id><![CDATA[81100420080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alexis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Angelidis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[silex@pixar.com]]></email_address>
			</au>
			<au>
				<person_id>P2809628</person_id>
				<author_profile_id><![CDATA[81548004968]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Josh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Anon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[josha@pixar.com]]></email_address>
			</au>
			<au>
				<person_id>P2809629</person_id>
				<author_profile_id><![CDATA[81335488473]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gary]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bruins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[bruins@pixar.com]]></email_address>
			</au>
			<au>
				<person_id>P2809630</person_id>
				<author_profile_id><![CDATA[81335496472]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reisch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jreisch@pixar.com]]></email_address>
			</au>
			<au>
				<person_id>P2809631</person_id>
				<author_profile_id><![CDATA[81320496117]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Esdras]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Varagnolo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[esdras@pixar.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1401114</ref_obj_id>
				<ref_obj_pid>1401032</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Scheepers, F., and Angelidis, A. 2008. Atmos: a system for building volume shaders. In <i>ACMSIGGRAPH 2008 talks</i>, ACM, SIGGRAPH '08, 64:1--64:1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tessendorf, J., 2004. Simulating ocean surface. ACM SIGGRAPH 2004 Course, January.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Ocean Mission on Cars 2 Alexis Angelidis * Josh Anon Gary Bruins Jon Reisch § Esdras Varagnolo ¶ Pixar 
Animation Studios On a mission to Dr. Z s lair in the middle of the ocean, secret agent Finn McMissile 
must face the harsh environment of a stormy ocean. Early in the development of the .lm there was no 
interaction with ocean water. As production progressed the ocean became an im­portant element of the 
fast paced action sequences, and the night lighting illustrated in Figure 1 demanded an expansion of 
our tools. We discuss our challenges and solutions across multiple disciplines in addressing the technical 
challenges. Figure 1: A shot from Cars 2.c@Disney / Pixar. All rights reserved. 1 Water Surface and 
Interaction To represent the stormy water surface geometry we implemented J. Tessendorf s wave system 
[Tessendorf 2004], with a mechanism to split ranges of wave frequency. This allowed layout artists to 
pro­vide a base ocean made of low frequency waves that would later be detailed in a seamless looking 
way. We implemented a procedural self-intersection healing mechanism that allows waves to look as cuspy 
as visually required. To avoid the memory cost of a .nely tessellated mesh capturing the tiny detail 
of the waves, the geome­try that was not represented by polygons was rendered as displace­ments. In most 
shots we used three to four wave trains to span the range of visible waves and avoid repetition. To layout 
the shots, we established categories of shooting situations and built equivalent rigs on the wave system. 
In some shots, we needed to put the camera on a second boat, de.ned as a virtual boat with its own size 
and position. In other shots we needed a camera operator on the deck of the main boat. To account for 
the operator s delayed reaction to the waves, instead of constraining a camera to the main boat, we created 
a second virtual boat with identical size and motion parameters as the main boat but with offsets to 
the wave reaction parameters. We also discovered that we needed to modify the camera constraint mechanism 
to provide control over side to side motion. Feeling seasick was a subjective indicator that the parameters 
had been pushed too far and we needed to back off. Simulating both the ocean water and the splash together 
would have been prohibitive. We decoupled the ocean from the splash with a frequency .lter applied locally 
to remove the high and medium frequencies that would then be simulated. For ocean integration such as 
boat splashes and spray, we used Houdini s .uid .ip solver. * silex@pixar.com josha@pixar.com bruins@pixar.com 
§jreisch@pixar.com ¶esdras@pixar.com  2 Volumetric Lighting Approach One of the challenges in lighting 
the ocean came when we decided to have refractions of the water below the surface. The primary goal was 
to describe depth, thickness, murkiness and subtle color shifts. We later expanded on that idea by including 
underwater explosions, props and sets with strong directional light sources. This enhanced the sense 
of scale, mystery and action the story was calling for. There were three main bene.ts of integrating 
the volumetric light­ing approach with our surface lighting tools. First, we were able to reuse existing 
color and volume light shapers. Second, the crew was familiar with them and the learning curve was not 
steep. Third, lighters were able to customize depth, color, intensity and coverage on a per shot basis, 
with the utmost level of control, achieving in the end what would become the lighting vision for the 
sequence. To reduce the overwhelming computation of illuminating a partic­ipating medium on either side 
of the water surface, several accel­erations needed to be implemented, as shown in Figure 2. To illu­minate 
the thin volumetric ambient fog, we polygonized at render time an implicit surface that de.ned a region 
as small as possible for rendering the volume (1). To attenuate light as depth increased, we .lled the 
ocean below the surface with a volumetric shader to model the build up of opacity (4). To accelerate 
the distortion of re­.ections and refractions of volumetric lights, we reduced the num­ber of calls to 
the volume shader by storing the volume data in a volume cache (2),(3). The fog density was modeled with 
a pro­grammable volume shader [Scheepers and Angelidis 2008]. For fast moving headlights, we used the 
volume primitive of PRMan 15.0 that provides motion blur derived from a space constrained to the light 
source. Figure 2: (1) Direct light computed by raymarching in an implicit surface. (2)(3) Volume refraction/re.ection 
calculated in a shader cache. (4) Depth attenuation calculated with ocean volume stored in a deep shadow 
map. c @Disney / Pixar. All rights reserved. References SCHEEPERS, F., AND ANGELIDIS, A. 2008. Atmos: 
a system for building volume shaders. In ACM SIGGRAPH 2008 talks, ACM, SIGGRAPH 08, 64:1 64:1. TESSENDORF, 
J., 2004. Simulating ocean surface. ACM SIG-GRAPH 2004 Course, January. Copyright is held by the author 
/ owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037850</article_id>
		<sort_key>240</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>18</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Untangling hair rendering at Disney]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037850</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037850</url>
		<abstract>
			<par><![CDATA[<p>Lighting computer generated hair is always a challenge. When that hair is 70 feet of long, beautiful, blond locks and belongs to the most famous "long haired" character in Western literature, that challenge becomes daunting. Introduce a stylized art direction that fuses the look of traditionally animated classics like Cinderella and Sleeping Beauty with the tangible realism that marks today's CG animation, and the daunting challenge becomes dubious as well. Mix in a sophisticated, physically based hair shader for good measure and this would describe the rendering process on "Tangled".</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809632</person_id>
				<author_profile_id><![CDATA[81100574361]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lewis]]></first_name>
				<middle_name><![CDATA[N.]]></middle_name>
				<last_name><![CDATA[Siegel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[lewis.siegel@disney.com]]></email_address>
			</au>
			<au>
				<person_id>P2809633</person_id>
				<author_profile_id><![CDATA[81488653599]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ryan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Duncan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ryan.duncan@disney.com]]></email_address>
			</au>
			<au>
				<person_id>P2809634</person_id>
				<author_profile_id><![CDATA[81320495035]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Springfield]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[chris.springfield@disney.com]]></email_address>
			</au>
			<au>
				<person_id>P2809635</person_id>
				<author_profile_id><![CDATA[81488670589]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Mitchell]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Snary]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mitchell.snary@disney.com]]></email_address>
			</au>
			<au>
				<person_id>P2809636</person_id>
				<author_profile_id><![CDATA[81488646625]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michelle]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Robinson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[michelle.l.robinson@disney.com]]></email_address>
			</au>
			<au>
				<person_id>P2809637</person_id>
				<author_profile_id><![CDATA[81474643213]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Ramon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Montoya-Vozmediano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dreamworks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ramon.montoya@dreamworks.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1778793</ref_obj_id>
				<ref_obj_pid>1833349</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Sadeghi, I., Pritchett, H., Jensen, H. W., and Tamstorf, R. 2010. An Artist Friendly Hair Shading System. In <i>SIGGRAPH '10: ACM SIGGRAPH 2010 Papers</i>, ACM New York, NY, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Petrovic, L., Henne, M., and Anderson, J. 2005. Volumetric Methods for Simulation and Rendering of Hair. Pixar Technical Memo #06-08, Pixar Animation Studios.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037851</section_id>
		<sort_key>250</sort_key>
		<section_seq_no>7</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Out of core]]></section_title>
		<section_page_from>7</section_page_from>
	<article_rec>
		<article_id>2037852</article_id>
		<sort_key>260</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>19</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Google Body]]></title>
		<subtitle><![CDATA[3D human anatomy in the browser]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037852</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037852</url>
		<abstract>
			<par><![CDATA[<p>Google Body gives any user access to 3D anatomy information typically reserved for physicians and medical students. The user can peel away and add back layers of anatomy, rotate and zoom, select entities such as muscles and nerves, and search. Direct links to any view of the male or female model -- with an optional user-supplied annotation -- can be forwarded to friends, family, or physicians. And it all works from a browser, smartphone, or tablet.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809638</person_id>
				<author_profile_id><![CDATA[81488665078]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Arthur]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Blume]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Google, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809639</person_id>
				<author_profile_id><![CDATA[81541362256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Won]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Google, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809640</person_id>
				<author_profile_id><![CDATA[81488644270]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kogan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Google, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809641</person_id>
				<author_profile_id><![CDATA[81322498148]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Vangelis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kokkevis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Google, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809642</person_id>
				<author_profile_id><![CDATA[81537594056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Nico]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Google, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809643</person_id>
				<author_profile_id><![CDATA[81488669583]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Rachel]]></first_name>
				<middle_name><![CDATA[Weinstein]]></middle_name>
				<last_name><![CDATA[Petterson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Google, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809644</person_id>
				<author_profile_id><![CDATA[81488673016]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Roni]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zeiger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Google, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Google Body: 3D Human Anatomy in the Browser Arthur Blume, Won Chun, David Kogan, Vangelis Kokkevis, 
Nico Weber, Rachel Weinstein Petterson, Roni Zeiger* Google, Inc. 1 Introduction Google Body gives any 
user access to 3D anatomy information typically reserved for physicians and medical students. The user 
can peel away and add back layers of anatomy, rotate and zoom, select entities such as muscles and nerves, 
and search. Direct links to any view of the male or female model with an optional user-supplied annotation 
 can be forwarded to friends, family, or physicians. And it all works from a browser, smartphone, or 
tablet. 2 Graphics and Data Transmission Google Body works without additional plug-ins or extensions 
inside a WebGL-enabled browser. At the time of this writing, the stable versions of Chrome and Firefox 
4 provide WebGL. One of our challenges was to ef.ciently transmit data from the server to the client. 
The transmission needs to be small and decodable through Javascript. A single model (male or female) 
includes 1.4 million triangles with 8 attributes per vertex. This represents 52MB of data using a naive 
approach of JSON with GZIP far more content than any other non-procedurally generated WebGL application 
at the time of writing. Instead, we use a compressed binary format that is quick to decode in the browser 
and only requires 9MB. 3 User Interface Our user interface has two primary components: manipulating 
the camera around the body and directly interacting with the body and its anatomic entities. For camera 
manipulation, we initially decided on a simple mode of 3D interaction so that users don t become disoriented. 
We use a capsule method of navigation that allows the user to rotate around the central axis of the body, 
pan up and down, zoom in and out along the camera s z-axis, and look at the top/bottom of the body as 
it pans past the head/feet. A slider increases or decreases transparency of the layers of the body. If 
moved from top to bottom, the user smoothly removes successive layers from the body. In addition, the 
user can manipulate the transparency of individual layers via per-layer sliders. Google Body also incorporates 
Google s Instant Search feature. As a user begins to type a search term in the search box, the camera 
automatically focuses on the most likely term (see Figure 1). Google Body displays a list of possible 
matches to the query and scrolling through those also moves the camera. When a user doesn t know the 
name of what they are looking for, but knows where it is, they can navigate and click on a body part 
to see its label. The rest of the body fades to highlight the selected entity. Multiple body parts can 
be grouped in one selection. Once a user has manipulated his view and/or selected entities, he can save 
and share this information simply using the url which stores the model s state in a compact fashion. 
* contact: http://goo.gl/mlrWf Figure 1: As a user types the search term de Google Body automatically 
zooms to the current best completion and highlights it by fading out other entities. Other possible completions 
are shown in the drop down list. 4 Social Impact Google Body provides a mechanism for people to discuss 
questions and learn about anatomy. Initial feedback is especially positive from students and teachers, 
from elementary school to medical school; and also from patients, .tness trainers, and other health professionals 
interested in Google Body as a patient education tool. To encourage such social interactions, we added 
an annotation feature to Google Body. When annotations are enabled, the user can select a point on the 
body and add a note. This note is then included with the url which can be sent full or through a short 
url of the form http://goo.gl/xxxx. 5 Conclusion Google Body provides an intuitive and widely accessible 
3D context for learning and discussing human anatomy. Whether on the computer at home or on a phone at 
the doctor s of.ce, there s a new way to look inside yourself. Acknowledgements Rodolfo Arauz, David 
Bau, Henry Bridge, Jeremy Faller, Summer Fetterman, Jason Freidenfelds, Thor Lewis, JL Needham, Sterling 
Stein, Gregg Tavares, Steve Vinter Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, 
British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037853</article_id>
		<sort_key>270</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>20</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Interactive indirect illumination using voxel-based cone tracing]]></title>
		<subtitle><![CDATA[an insight]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037853</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037853</url>
		<abstract>
			<par><![CDATA[<p>Indirect illumination is an important element for realistic image synthesis, but its computation is expensive and highly dependent on the complexity of the scene and of the BRDF of the surfaces involved. While off-line computation and pre-baking can be acceptable for some cases, many applications (games, simulators, etc.) require real-time or interactive approaches to evaluate indirect illumination. We present a novel algorithm to compute indirect lighting in real-time that avoids costly precomputation steps and is not restricted to low frequency illumination. It is based on a hierarchical voxel octree representation generated and updated on-the-fly from a regular scene mesh coupled with an approximate voxel cone tracing that allows a fast estimation of the visibility and incoming energy. Our approach can manage two light bounces for both Lambertian and Glossy materials at interactive framerates (25-70FPS). It exhibits an almost scene-independent performance and allows for fully dynamic content thanks to an interactive octree voxelization scheme. In addition, we demonstrate that our <i>voxel cone tracing</i> can be used to efficiently estimate Ambient Occlusion.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809645</person_id>
				<author_profile_id><![CDATA[81421599021]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cyril]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Crassin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Grenoble Univ, INRIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809646</person_id>
				<author_profile_id><![CDATA[81100506206]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fabrice]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neyret]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CNRS, Grenoble Univ, INRIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809647</person_id>
				<author_profile_id><![CDATA[81100048438]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Miguel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sainz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809648</person_id>
				<author_profile_id><![CDATA[81414609426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Simon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Green]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809649</person_id>
				<author_profile_id><![CDATA[81310501633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Elmar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eisemann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Telecom ParisTech]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1507152</ref_obj_id>
				<ref_obj_pid>1507149</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Crassin, C., Neyret, F., Lefebvre, S., and Eisemann, E. 2009. Gigavoxels: Ray-guided streaming for efficient and detailed voxel rendering. In <i>ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games (I3D)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Indirect Illumination Using Voxel-Based Cone Tracing : An Insight Cyril Crassin Fabrice 
Neyret Miguel Sainz Simon Green Elmar Eisemann Grenoble Univ, INRIA CNRS, Grenoble Univ, INRIA NVIDIA 
NVIDIA Telecom ParisTech Figure 1: Real-time indirect illumination (25-70 fps on a GTX480): Our approach 
supports diffuse and glossy light bounces on complex scenes. We rely on a voxel-based hierarchical structure 
to ensure ef.cient integration of 2-bounce illumination. (Right scene courtesy of G. M. Leal Llaguno) 
Abstract Indirect illumination is an important element for realistic image syn­thesis, but its computation 
is expensive and highly dependent on the complexity of the scene and of the BRDF of the surfaces involved. 
While off-line computation and pre-baking can be acceptable for some cases, many applications (games, 
simulators, etc.) require real-time or interactive approaches to evaluate indirect illumination. We present 
a novel algorithm to compute indirect lighting in real-time that avoids costly precomputation steps and 
is not restricted to low frequency il­lumination. It is based on a hierarchical voxel octree representation 
generated and updated on-the-.y from a regular scene mesh coupled with an approximate voxel cone tracing 
that allows a fast estimation of the visibility and incoming energy. Our approach can manage two light 
bounces for both Lambertian and Glossy materials at interactive framerates (25-70FPS). It exhibits an 
almost scene-independent per­formance and allows for fully dynamic content thanks to an interactive octree 
voxelization scheme. In addition, we demonstrate that our voxel cone tracing can be used to ef.ciently 
estimate Ambient Occlusion. 1 Algorithm overview Our approach is based on a three-steps algorithm. We 
.rst inject in­coming radiance (energy and direction) from dynamic light sources into the sparse voxel 
octree hierarchy, by rasterizing scene meshes and splatting a photon for each visible surface fragment. 
In a second step, we .lter the values in the higher levels of the octree (mipmap). This is done ef.ciently 
in parallel by relying on screen-space quad­tree analysis. Finally, we render the scene from the camera. 
For each visible surface fragment, we combine the direct and indirect illumi­nation. We employ an approximate 
cone tracing to perform a .nal gathering, sending out a few cones over the hemisphere to collect il­lumination 
distributed in the octree. Typically for Phong-like BRDF, a few large cones ( 5) estimate the diffuse 
energy coming from the scene, while a tight cone in the re.ected direction with respect to the viewpoint 
captures the specular component. The aperture of this cone is derived from the specular exponent of the 
material, allowing us to compute glossy re.ections.  2 Dynamic sparse voxel octree structure The core 
of our approach is built upon a pre-.ltered hierarchical voxel version of the scene geometry. For ef.ciency, 
this representation is stored in the form of a compact pointer-based sparse voxel octree in the spirit 
of [Crassin et al. 2009]. To adapt to the speci.cities of the scenes, we use small 33 bricks with values 
located in octree-node cor­ners. This allows us to employ hardware interpolation, while using less than 
half the amount of memory compared to previous solutions. This structure exhibits an almost scene-independent 
performance. It is updated dynamically thanks to a fast GPU based mesh voxelization and octree building 
system. This system handle dynamic updates of the structure, allowing for animated objects and dynamic 
modi.ca­tions of the environment. 3 Pre-integrated Voxel Cone Tracing Our approach approximates the 
re­sult of the visibility, energy and NDF estimation for a bundle of rays in a cone using only a single 
ray and our .ltered (mipmapped) voxel structure. The idea is to perform volume integration steps along 
the cone axis with lookups in our hierarchical representation at the LOD corresponding to the local cone 
radius. During this step, we use quadrilinear interpolation to ensure a smooth LOD variation, similarly 
to anti-aliasing .ltering [Crassin et al. 2009]. Our voxel shading convolves the BRDF, the NDF, the distribution 
of light direc­tions and the span of the view cone, all considered as Gaussian lobes. These lobes are 
reconstructed from direction distributions stored in a compact way as non-normalized vectors in the structure 
 4 Anisotropic Pre-Integration In order to get higher quality visibility estimation and to limit leak­ing 
of light when low resolution mipmap levels are used, we propose to rely on an anisotropic pre-integration 
of voxel values stored in a direction-dependent way in the structure. We use the 6 main directions and 
values are reconstructed by linear interpolation of the 3 closest di­rections. It provides a better approximation 
of the volume rendering integral, at the cost of 1.75x the storage requirement and a slightly slower 
sampling.  References CRASSIN, C., NEYRET, F., LEFEBVRE, S., AND EISEMANN, E. 2009. Gigavoxels : Ray-guided 
streaming for ef.cient and detailed voxel rendering. In ACM SIGGRAPH Symposium on Interactive 3D Graphics 
and Games (I3D). Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, 
Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037854</article_id>
		<sort_key>280</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>21</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Out-of-core GPU ray tracing of complex scenes]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037854</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037854</url>
		<abstract>
			<par><![CDATA[<p>Increased demand for global illumination, image based-lighting and simplified workflow have pushed raytracing into mainstream. Many rendering and simulation algorithms that were considered strictly offline are becoming more interactive on massively parallel GPUs. Unfortunately, the amount of available memory on modern GPUs is relatively small. Scenes for feature film rendering and visualization have large geometric complexity and can easily contain millions of polygons and a large number of texture maps and other data attributes. In this talk, we describe a general purpose out-of-core ray tracing engine for the GPU where we address data management, ray-intersection and shading. We utilize a GPU data cache that enables efficient access of out-of-core data. We develop a novel ray intersection algorithm built around acceleration structure that brings needed data on demand using page-swapping. We further reduce memory usage by using a simple geometry quantization. The ray tracing engine is used to implement a variety of rendering and light transport algorithms.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809650</person_id>
				<author_profile_id><![CDATA[81485656950]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kirill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Garanzha]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KIAM RAS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809651</person_id>
				<author_profile_id><![CDATA[81485658732]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alexander]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bely]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[FUGU]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809652</person_id>
				<author_profile_id><![CDATA[81100604694]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Simon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Premoze]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KIAM RAS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809653</person_id>
				<author_profile_id><![CDATA[81477642873]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Vladimir]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Galaktionov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KIAM RAS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Out-of-core GPU Ray Tracing of Complex Scenes Kirill Garanzha (KIAM RAS) Alexander Bely (FUGU) Simon 
Premoze Vladimir Galaktionov (KIAM RAS)  Abstract Increased demand for global illumination, image based-lighting 
and simplified workflow have pushed raytracing into mainstream. Many rendering and simulation algorithms 
that were considered strictly offline are becoming more interactive on massively parallel GPUs. Unfortunately, 
the amount of available memory on modern GPUs is relatively small. Scenes for feature film rendering 
and visualization have large geometric complexity and can easily contain millions of polygons and a large 
number of texture maps and other data attributes. In this talk, we describe a general purpose out-of-core 
ray tracing engine for the GPU where we address data management, ray-intersection and shading. We utilize 
a GPU data cache that enables efficient access of out-of-core data. We develop a novel ray intersection 
algorithm built around acceleration structure that brings needed data on demand using page-swapping. 
We further reduce memory usage by using a simple geometry quantization. The ray tracing engine is used 
to implement a variety of rendering and light transport algorithms. Data Management Modern GPUs have 
enormous computational power (1 Tflops) and large memory bandwidth (150 Gb/s). Relatively small amount 
of memory and a lack of virtual memory system forced us to design a GPU data cache that increases the 
virtual GPU memory size by an order of magnitude. An application that works on a large dataset creates 
a GPU data manager and processes data using request and process data kernel() in the following loop: 
void out_of_core_data_processing() { available_new_data = 1 while( available_new_data ) { // Process 
in-core data // Request out-of-core data request_and_process_data_kernel(data); // Bring missing data 
into GPU memory available_new_data = swap_requested_pages(gpu_data_manager); } } We first make a data 
request to the data manager. If some desired data blocks are out-of-core, we mark them as requested. 
The data manager brings requested data blocks into the GPU memory (swap_requested_pages()). On subsequent 
loop iterations, needed data is in the GPU memory and the kernel can do computation. This process of 
requesting, computing and transferring data is repeated until no more out-of-core data is requested by 
the application. The virtual data manager allows us to access large data sets and arbitrary arrays. Out-of-Core 
Intersection Efficient raytracing requires an acceleration data structure to compute ray intersections 
with the scene geometry. Since the scene does not necessarily fit into the GPU memory, we create a multi 
level Bounding Volume Hierarchy. We implement a very fast BVH builder on the GPU that produces high quality 
BVHs suitable for fast ray tracing of out-of-core geometry and fully animated scenes. We traverse this 
acceleration structure knowing that requested geometry may not be available in memory. The intersection 
algorithm successfully hides data transfer latency by performing intersections on in-core geometry. We 
also use a simple geometry quantization scheme to reduce geometry size and further reduce expensive memory 
transfer size (description is available in supplemental paper). Shading We support programmable shaders: 
ray generation, materials and light transport. Sophisticated shading and global illumination computation 
may require an arbitrary amount of data. We use the GPU data cache to access large number of textures 
and geometry attributes (normals, texture coords, etc ). The shaders can generate arbitrary number of 
new rays to compute lighting and shadowing. We organize rays in a ray queue that helps with improving 
ray coherency, hides latency (overlap computation with data transfers) and provides simple ordering and 
synchronization mechanism. Shaders submit rays to a global ray queue from which rays are consumed by 
the out-of-core ray-intersector. We use a large ray queue (32M rays) to increase the rendering pipeline 
throughput. The implementation of texture on demand (Peachey) is almost completed and we can potentially 
support large textures which are stored on the disc (some data is available in supplemental document). 
The work in progress is implementing Metropolis Light Transport algorithm where large number of rays 
may be active simultaneously. Summary We have implemented a general purpose out-of-core ray tracer for 
rendering complex scenes on the GPU. Figure 1 shows several frames from a Boeing 777 animation sequence. 
The model has 360 million polygons. Using a single Nvidia GTX 480 graphics card, it takes 15 seconds 
to render a full converged final frame (1024x768) with global illumination (path tracing, three bounces, 
100 progressive image iterations). Path tracing demonstrates a stress test for GPU Virtual memory manager 
because path tracing can be formulated as the number of sparse memory access to the scene geometry. 
    Figure 1: Frames from Boeing 777 animation sequence. It takes 15 seconds to render a fully converged 
final frame with global illumination on a single NVIDIA GTX 480 GPU. The final image accumulates 100 
progressive path traceing refinements (i.e. 100 Monte Carlo samples / pixel are computed in 15 seconds). 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037855</article_id>
		<sort_key>290</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>22</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Developing the interactive dynamic natural world of "From Dust"]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037855</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037855</url>
		<abstract>
			<par><![CDATA[<p>&Lt; From Dust &Gt; is a new game for Playstation 3, Xbox 360 and PC developed at Ubisoft Montpellier Studio. It features a fully dynamic world, where sand, water, lava and vegetation are constantly evolving and ruled by a realistic simulation.</p> <p>While processing some mostly static terrain environments in video games is a well established subject, "From Dust" takes its unique set of challenges in the highly dynamic nature of the environment.</p> <p>With our approach, it is possible to create a game that takes place in a fully procedural environment ruled by a complex simulation for current generation's consoles.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809654</person_id>
				<author_profile_id><![CDATA[81488670925]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ronan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ubisoft Montpellier Studio, Montpellier, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rbel@ubisoft.fr]]></email_address>
			</au>
			<au>
				<person_id>P2809655</person_id>
				<author_profile_id><![CDATA[81488673154]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Beno&#238;t]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vimont]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ubisoft Montpellier Studio, Montpellier, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[bvimont@ubisoft.fr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bel, R., and Chahi, E. 2010. Creating a high-performance simulation: an interactive dynamic natural world to play with. In <i>Game Developer Conference 2010</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Developpingo fDust theinteractivedynamicnaturalworld From RonanBenoîtVimont Bele e UbisoftMontpellierStudioe 
UbisoftMontpellierStudioe Montpellier,Montpellier,Francee Francee rbel@ubisoft.fre bvimont@ubisoft.fre 
 (a)e (b)isdoneonu e turnsinto Vegetationevolvesusingcellularautomatons.FluidsimulationCPUthenenhancedsingGPU.(c)Waterhazewhencontacting 
hotlava. e Abstractm Objectscanbebuffers,streams,textures,vertexb uffersordisplaye e lists.e Computeshaderjobscane 
oftenrune concurrent ly,e enablinge «Dust»isanewgamePlaystation3,Xbox 360ande thefullusageofupto8processors,andruninpa 
rallelwiththe FromforPCdevelopede atUbisoftMontpellierStudio.e Ite feature se afullye GPU processing. 
dynamice world,e wheree sand,e water,lavaandvegetati one aree m constantlyruledbyrealisticsimula tion.3. 
evolvingandaLow-levelOptimizationsm e e Whileprocessingsometerrainmentsinvideoe adata-drivenapproach,deal 
mostlystaticenviron Usingwehadtowitha lotoffeaturese gamesisawellestablishedsubject, FromDust ta kesitsuniquee 
suchasinstructionlatencies,in-orderexecutions talls,256bitse setnatureheSIMDporting,e cache-misses,cachedmemoryaliasing, 
e write­ ofchallengesinthehighlydynamicoft environment.e e m combining,e memoryprefetching,local-storagelimita 
tions,e lock­Withourapproach,itispossibletocreateagamee thattakesplacefreemulti-threading,ng. e hyper-threading,loadbalanci 
inafullyproceduralenvironmentruledbyacomple xsimulatione e forEvenifthese3platformsdifferent,youe cansharesome 
currentgeneration sconsoles.e arequitee methodologiesacrossthemtoreachahighlevelofe performances,e 1.Introductionm 
onlyoneversionofthecode. usinge e Theworldismadeofagridcomprisingseverallaye rsthatstoree Wehowwethis,stepbystep. 
willexplaindidtheinformationaboutrock,sand,fluidsandvegeta tionforeache e cell.areusedtoupdatethesee 
differente Cellularautomatonslayers.4.Resultsm e Forexample,Navier-Stokesequationsareusedtoco mputethee 
Thegamerunsat30fpsonconsolesand60fpsona modernPC,e motionoffluidsandupdatethefluidlayer.Differ entrulesaree 
usingalltheavailableCPUs.Evenwiththesimulat orrunning,e usede todeterminee ife vegetatione shouldspread,burn 
,e live,e die,e lotsofresourcesarestillbeingavailableforthe restofthegame.e e bloomandupdatethe orgrow,vegetationlayer.e 
e e Weaandcodedesignsuitab leused nowhavesetofmethodstobeAfullupdateofthesimulatedworldisrequiredat eachframenoe 
notonlyforphysics,butalsoforrendering,AI,a nimation...e matterthecomplexityoftheworld,andasthegame playdependse 
adaptede tomulti-threadede architectures,withconce ptse shareablee onthesimulation,theupdateisnotonlyt 
othevisibleparte betweenandGPU limitedCPUprogrammers.e ofthemap.e m e Referencesm Toachievethishugeamountofdataprocessing,wee 
iterativelye e enhancee ourcodee andmethodologies,toe usee attheir e bestthee 3e [1]e BEL,e R.,e ANDe 
CHAHI,e E.e 2010.e Creatinge ae high-performancee target platforms. simulation:aninteractivedynamicnaturalworldto 
playwith. e Ine Game. DeveloperConference2010 e 2.CPUComputeshadersm e e Allthesimulationandrenderingcodeisdesigneda 
sSIMDCPUe computee shaders[1]e andrunse nativelye one PS3e SPUs.e Thesee shaderse usesomee ofthee objectse 
ase inputse ande produ cee somee outputsorbythe whichwillbeeitherusedbyothershaderse GPU.e e Copyright 
is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. 
ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037856</section_id>
		<sort_key>300</sort_key>
		<section_seq_no>8</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Building blocks]]></section_title>
		<section_page_from>8</section_page_from>
	<article_rec>
		<article_id>2037857</article_id>
		<sort_key>310</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>23</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[KinectFusion]]></title>
		<subtitle><![CDATA[real-time dynamic 3D surface reconstruction and interaction]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037857</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037857</url>
		<abstract>
			<par><![CDATA[<p>We present KinectFusion, a system that takes live depth data from a moving Kinect camera and in real-time creates high-quality, geometrically accurate, 3D models. Our system allows a user holding a Kinect camera to move quickly within any indoor space, and rapidly scan and create a fused 3D model of the whole room and its contents within seconds. Even small motions, caused for example by camera shake, lead to new viewpoints of the scene and thus refinements of the 3D model, similar to the effect of image super-resolution. As the camera is moved closer to objects in the scene more detail can be added to the acquired 3D model.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809656</person_id>
				<author_profile_id><![CDATA[81328488768]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shahram]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Izadi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shahrami@microsoft.com]]></email_address>
			</au>
			<au>
				<person_id>P2809658</person_id>
				<author_profile_id><![CDATA[81333490377]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Newcombe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research and Imperial College Londo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rnewcombe@doc.ic.ac.uk]]></email_address>
			</au>
			<au>
				<person_id>P2809659</person_id>
				<author_profile_id><![CDATA[81460641092]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[b-davidk@microsoft.com]]></email_address>
			</au>
			<au>
				<person_id>P2809660</person_id>
				<author_profile_id><![CDATA[81309495440]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Otmar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hilliges]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[otmarh@microsoft.com]]></email_address>
			</au>
			<au>
				<person_id>P2809661</person_id>
				<author_profile_id><![CDATA[81388595328]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Molyneaux]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[a-davmo@microsoft.com]]></email_address>
			</au>
			<au>
				<person_id>P2809662</person_id>
				<author_profile_id><![CDATA[81320490166]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hodges]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shodges@microsoft.com]]></email_address>
			</au>
			<au>
				<person_id>P2809663</person_id>
				<author_profile_id><![CDATA[81339510339]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Pushmeet]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kohli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pkohli@microsoft.com]]></email_address>
			</au>
			<au>
				<person_id>P2809664</person_id>
				<author_profile_id><![CDATA[81100400357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Jamie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shotton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jamiesho@microsoft.com]]></email_address>
			</au>
			<au>
				<person_id>P2809665</person_id>
				<author_profile_id><![CDATA[81100056204]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Davison]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Imperial College Londo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ajd@doc.ic.ac.uk]]></email_address>
			</au>
			<au>
				<person_id>P2809657</person_id>
				<author_profile_id><![CDATA[81100376259]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fitzgibbon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[awfg@microsoft.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>132022</ref_obj_id>
				<ref_obj_pid>132013</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Besl, P., and McKay, N. 1992. A method for registration of 3D shapes. 239--256.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Curless, B., and Levoy, M. 1996. A volumetric method for building complex models from range images. In <i>ACM Transactions on Graphics (SIGGRAPH)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 KinectFusion: Real-Time Dynamic 3D Surface Reconstruction and Interaction Shahram Izadi1, Richard A. 
Newcombe1,2, David Kim1, Otmar Hilliges1, David Molyneaux1 , Steve Hodges1, Pushmeet Kohli1, Jamie Shotton1, 
Andrew J. Davison2, Andrew Fitzgibbon1 1 Microsoft Research* , 2Imperial College London Figure 1: We 
introduce a new system for acquiring high-quality, geometrically precise 3D models of an entire room 
rapidly using a single moving Kinect camera. The system generates high-quality models from noisy Kinect 
data in real-time. An entire room or smaller objects can be reconstructed in seconds (top sequences). 
We demonstrate a number of compelling new interactive possibilities such as multi-touch on any arbitrarily 
shaped surface (bottom left sequence); real-time rigid body physics simulated on a dynamic reconstructed 
model (bottom middle sequence); and rapid segmentation and tracking of objects within the model (bottom 
right sequence). 1 Introduction We present KinectFusion, a system that takes live depth data from a moving 
Kinect camera and in real-time creates high-quality, ge­ometrically accurate, 3D models. Our system allows 
a user hold­ing a Kinect camera to move quickly within any indoor space, and rapidly scan and create 
a fused 3D model of the whole room and its contents within seconds. Even small motions, caused for exam­ple 
by camera shake, lead to new viewpoints of the scene and thus re.nements of the 3D model, similar to 
the effect of image super­resolution. As the camera is moved closer to objects in the scene more detail 
can be added to the acquired 3D model. To achieve this, our system continually tracks the 6DOF pose of 
the camera and rapidly builds a representation of the geometry of arbi­trary surfaces. Novel GPU-based 
implementations for both camera tracking and surface reconstruction allow us to run at interactive real-time 
rates that have not previously been demonstrated. We de.ne new instantiations of two well known graphics 
algorithms designed speci.cally for parallelizable GPGPU hardware. 2 KinectFusion The main system pipeline 
.rst takes the live depth map from Kinect and converts from image coordinates into 3D points and normals 
in the coordinate space of the camera. Next the tracking phase computes a rigid 6DOF transform that closely 
aligns the current oriented points with the previous frames, using a novel GPU im­plementation of the 
ICP algorithm [Besl and McKay 1992]. This de.nes a relative rigid transform from the previous camera 
pose to the current. These transforms are incrementally applied to a global transform that de.nes the 
global pose of the camera. Given the *{shahrami,b-davidk,otmarh,a-davmo,shodges,pkohli,jamiesho,awf}@microsoft.com 
{rnewcombe, ajd}@doc.ic.ac.uk global pose of the camera, points and normals are converted into global 
coordinates, and a single consistent 3D model is updated. Instead of simply fusing point clouds, we reconstruct 
surfaces based on a novel GPU-based implementation of volumetric truncated signed distance functions 
[Curless and Levoy 1996]. Voxels within the volume are updated based on our globally converted measure­ments. 
Each voxel stores a running average of its distance to the assumed position of a physical surface. Finally 
we use GPU accel­erated raycasting to render a view of the volume and the 3D surfaces it contains given 
the current global pose of the camera. The view of the volume equates to a synthetic depth map, which 
can be used as a less noisy more globally consistent base or reference frame for the next iteration of 
ICP tracking. This allows us to track by comparing the current live depth map with our less noisy raycasted 
view of the model, as opposed to using only the live depth maps frame-to-frame. The system can reconstruct 
a scene within seconds and enables in­teractive possibilities including: extending multi-touch interactions 
to any arbitrarily shaped reconstructed surface; advanced features for augmented reality; real-time physics 
that are simulated live on the dynamic model; and novel methods for segmentation and track­ing of scanned 
objects. We also present extensions to our GPU­based tracking algorithm to distinguish scene motion from 
camera motion thus dealing with dynamic scenes, in particular ones where users are interacting. See Figure 
1 and accompanying video for examples.  References BESL, P., AND MCKAY, N. 1992. A method for registration 
of 3D shapes. 239 256. CURLESS, B., AND LEVOY, M. 1996. A volumetric method for building complex models 
from range images. In ACM Transac­tions on Graphics (SIGGRAPH). Copyright is held by the author / owner(s). 
SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037858</article_id>
		<sort_key>320</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>24</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[SBL mesh filter]]></title>
		<subtitle><![CDATA[fast separable approximation of bilateral mesh filtering]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037858</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037858</url>
		<abstract>
			<par><![CDATA[<p>Bilateral mesh filtering (BL) is a simple and powerful feature-preserving filtering operator which allows to smooth or remove noise from surface meshes while preserving important features in a non-iterative way. However, to be effective, such a filter may require quite a large support size, inducing slow processing when applied on high resolution meshes. In this paper, we propose a separable approximation of BL (SBL) based on a local decomposition of the bi-dimensional filter into a product of two one-dimensional ones. The main problem here is to find meaningful directions at every point to orient the two one-dimensional filters. Our solution exploits the minimum and maximum curvature directions at each point and demonstrates a significant speed-up on meshes ranging from thousands to millions of elements, enabling feature-preserving filtering with large support size in a variety of practical scenarii (Fig. 1).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809666</person_id>
				<author_profile_id><![CDATA[81488672957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Guillaume]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vialaneix]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[EDF R&D - Telecom ParisTech - CNRS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809667</person_id>
				<author_profile_id><![CDATA[81309487978]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tamy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boubekeur]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Telecom ParisTech - CNRS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>882367</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Jones, T. R., and Durand, F. 2003. Non-iterative, feature-preserving mesh smoothing. <i>ACM Transactions on Graphics</i> 22, 943--949.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Pham, T., and Van Vliet, L. 2005. Separable bilateral filtering for fast video preprocessing. In <i>IEEE ICME 2005</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SBL Mesh Filter: Fast Separable Approximation of Bilateral Mesh Filtering Guillaume Vialaneix Tamy 
Boubekeur EDF R&#38;D -Telecom ParisTech -CNRS Telecom ParisTech -CNRS propose a separable approximation 
of BL (SBL) based on a local Figure 1: From left to right : the original model, the BL denoising method 
(1sec), our SBL technique (0.6sec), the per-vertex error between the two denoised version. Introduction. 
Bilateral mesh .ltering (BL) is a simple and pow­erful feature-preserving .ltering operator which allows 
to smooth or remove noise from surface meshes while preserving important features in a non-iterative 
way. However, to be effective, such a .lter may require quite a large support size, inducing slow pro­cessing 
when applied on high resolution meshes. In this paper, we in the case of Gaussian .ltering, such a decomposition 
leads only to an approximate feature-preserving .lter [Pham and van Vliet 2005]. Thus, we compute .ltered 
minimum and maximum curvature direc­tions {ui, vi} at every vertex pi of M to de.ne locally the .lter­ing 
direction for our two passes. We de.ne two orthogonal planes v = {ni, ui} and .i = {ni, vi}, one for 
each curvature direc­tion and both intersecting along the vertex normal (Fig. 2). These iFn .ju i.u 
decomposition of the bi-dimensional .lter into a product of two planes intersect the neighboring faces 
and offer a straightforward one-dimensional ones. The main problem here is to .nd mean-predicate to collect 
the restricted set of faces for each pass, de.ned ingful directions at every point to orient the two 
one-dimensional as the intersection of the full ball-neighborhood Ni with the planes nh .lters. Our 
solution exploits the minimum and maximum curvature u . Finally, our SBL approx­ = Ø NFj .Ni | imation 
consist in the combination of a two restricted BL .ltering: = i directions at each point and demonstrates 
a signi.cant speed-up on meshes ranging from thousands to millions of elements, enabling feature-preserving 
.ltering with large support size in a variety of practical scenarii (Fig. 1). Separable Bilateral Filtering. 
Considering a noisy object, a sim­ple low-pass .lter helps reducing the noise, but it considers only 
the distances in space of neighboring samples to weight their con­tributions in a (local) combination. 
Consequently, noise is .ltered out, but feature lines are proportionally blurred. The idea of bi­lateral 
.ltering is to introduce a second weighting term based on the difference in range between object samples 
to weight their rel­ative contribution. We focus here on Jones and Durand formula­tion [Jones and Durand 
2003] of the BL. They de.ne a range space for meshes by the mean of predictions computed as the projection 
of the vertex onto the tangent plane of its neighbors : considering a mesh M, and one of its vertices 
pi, then for each of the faces Fj in its neighborhood Ni, we can compute the projection Pcj (pi) of pi 
on the plane de.ned by the center cj and the smoothed normal nj of Fj . Using a spatial kernel Gss and 
a range kernel Gsr , a vertex pi is .ltered according to the neighboring faces set Ni by BLNi (pi)=Gss 
(dij )Gsr (hij ) cj (1) Fj.Ni with dij = ||pi - cj || and hij = ||pi -Pcj (pi)||. The key idea of our 
approximation model is to speed-up the BL .lter by reduc­ing the size of Ni while still covering the 
same support size. Our approximation works in two passes: in the .rst pass we collect a set of neighboring 
faces restricted to one tangent direction on the surface and then .lter the vertex using this reduced 
set only. This .rst pass is applied to all vertices. In the second pass, we .lter the output of the .rst 
pass using an orthogonal tangent direction. This approach is inspired by the classical separable Gaussian 
.lter for images. Note however that while the exact solution is reproduced Copyright is held by the author 
/ owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
 pass, which are aligned with curvature directions. SBL(pi)= BLNu BLNv (pi) BLNi (pi) (2) ii Results. 
Our SBL approximation model is twice or more faster with typical setting, while preserving a RMS error 
below 0.1% of the mesh size when compared to the exact .lter. We show that our choice based on curvature 
directions is superior to other ones such as random or regular local frames, providing results which 
are visually indistringuishable from the exact BL .lter. References JONES, T. R., AND DURAND, F. 2003. 
Non-iterative, feature-preserving mesh smoothing. ACM Transactions on Graphics 22, 943 949. PHAM, T., 
AND VAN VLIET, L. 2005. Separable bilateral .ltering for fast video preprocessing. In IEEE ICME 2005. 
 iiFigure2: Twolocalplanes and..v u for two restricted sets of neighborhing faces (orange), one for each 
(green), allow to query 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037859</article_id>
		<sort_key>330</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>25</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Band decomposition of 2-manifold meshes for physical construction of large structures]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037859</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037859</url>
		<abstract>
			<par><![CDATA[<p>With the design and construction of more and more unusually shaped buildings, the computer graphics community has started to explore new methods to reduce the cost of the physical construction for large shapes. Most of currently suggested methods focus on reduction of the number of differently shaped components to reduce fabrication cost. In this work, we focus on physical construction using developable components such as thin metals or thick papers. In practice, for developable surfaces fabrication is economical even if each component is different. Such developable components can be manufactured fairly inexpensively by cutting large sheets of thin metals or thin paper using laser-cutters, which are now widely available.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809668</person_id>
				<author_profile_id><![CDATA[81100014535]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Qing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xing]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809669</person_id>
				<author_profile_id><![CDATA[81488670486]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gabriel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Esquivel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809670</person_id>
				<author_profile_id><![CDATA[81100124987]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ergun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akleman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809671</person_id>
				<author_profile_id><![CDATA[81408598676]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jianer]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809672</person_id>
				<author_profile_id><![CDATA[81342496250]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gross]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Columbia University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Band Decomposition of 2-Manifold Meshes For Physical Construction of Large Structures Qing Xing, Gabriel 
Esquivel, Ergun Akleman &#38; Jianer Chen Jonathan Gross Texas A&#38;M University Columbia University 
 Figure 1: This large sculpture of Bunny is constructed with laser cut poster-board papers assembled 
with brass fasteners. With the design and construction of more and more unusu­ally shaped buildings, 
the computer graphics community has started to explore new methods to reduce the cost of the physical 
construction for large shapes. Most of currently suggested methods focus on reduction of the number of 
dif­ferently shaped components to reduce fabrication cost. In this work, we focus on physical construction 
using devel­opable components such as thin metals or thick papers. In practice, for developable surfaces 
fabrication is economical even if each component is di.erent. Such developable com­ponents can be manufactured 
fairly inexpensively by cutting large sheets of thin metals or thin paper using laser-cutters, which 
are now widely available. (a) (b) (c) Figure 2: Construction elements for bunny. (a) is an exam­ple 
of vertex component that is cut with laser cutter, (b) shows elements of vertex component and (c) shows 
the pro­cess of assembling vertex components with fasteners. We observe that one of the biggest expenses 
for construc­tion of large shapes comes from handling and assembling the large number components. This 
problem is like putting pieces of a large puzzle together. However, unlike puzzles we do not want construction 
process to be challenging. In­stead, we want to simplify the construction process in such a way that 
the components can be assembled with a minimum instruction by the construction workers who may not have 
extensive experience. In this work, we introduce an approach to automatically cre­ate such easily assembled 
developable components from any given manifold mesh. Our approach is based on classical Graph Rotation 
Systems (GRS). Each developable compo­nent, which we call vertex component, is a physical equiva­lent 
of a rotation at the vertex v of a graph G. Each vertex component is a star shaped polygon that physically 
corre­sponds to the cyclic permutation of the edge-ends incident on v (See Figure 2(a)). We engrave edge-numbers 
with laser­cutters directly on edge-ends of vertex components to sim­plify .nding corresponding edge 
ends. When we print edge­numbers, we actually de.ne a collection of rotations, one for each vertex in 
G. This is formally called a pure rotation system of a graph. The fundamental He.ter-Edmunds theorem 
of GRS asserts that there is a bijective correspondence between the set of pure rotation systems of a 
graph and the set of equivalence classes of embeddings of the graph in the orientable surfaces. As a 
direct consequence of the theorem, to assemble the structure all construction workers have to do is to 
attach the corresponding edge-ends of vertex components. Once all the components are attached to each 
other, the whole structure will correctly be assembled. Gauss-Bonnet theorem, moreover, asserts that 
the total Gaussian curvature of a surface is the Euler characteristics times 2p. Since the structure 
is made up only developable components, Gaussian curvature is zero everywhere on the solid parts. The 
Gaussian curvature happens only in empty regions and that are determined uniquely. Since, we cor­rectly 
form Gaussian curvature of holes, the structures will always be raised and formed 3-space. We also develop 
strategies to simplify .nding corresponding pieces among a large number of vertex components. Us­ing 
this approach, Architecture students have constructed a large version of Stanford Bunny (see Figure 1) 
in a design and fabrication course in College of Architecture. The costs of poster-board papers and fasteners 
were very minimal, less than $100. We are currently working on to construct even larger shapes using 
stronger materials. We are also planning to use the structures obtained by this approach as molds to 
cast large plaster or cement sculptures. This work partially supported by the National Science Foun­dation 
under Grant No. NSF-CCF-0917288. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, 
British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>National Science Foundation</funding_agency>
			<grant_numbers>
				<grant_number>NSF-CCF-0917288</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2037860</article_id>
		<sort_key>340</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>26</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Pattern mapping with quad-pattern-coverable quad-meshes]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037860</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037860</url>
		<abstract>
			<par><![CDATA[<p>Pattern mapping (i.e. texturing arbitrary surfaces with repetitive patterns [Soler et al. 2002]) is a particularly useful texture mapping technique, since repeating a pattern reduces the memory cost, by mapping the same texture image to all faces. Moreover, pattern mapping does not require painting a texture image for each surface or generating a global texture map on each surface. Pattern mapping can potentially provide natural-looking materials, such as stone, wood, or marble, as well as human-made materials, such as wallpapers or repeating tiles. One of the main challenges arising when mapping patterns to arbitrary polyhedral meshes is to avoid texture discontinuities caused by singularities non-4-valent vertices in the quad-meshes. These discontinuities can appear at seams along the edges, which can be visually distracting. Unfortunately, it is not always possible to avoid non-4-valent vertices, since 4-regular quadmeshes exist only for genus-1 surfaces.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809673</person_id>
				<author_profile_id><![CDATA[81488672137]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shi-Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809674</person_id>
				<author_profile_id><![CDATA[81100014535]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Qing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xing]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809675</person_id>
				<author_profile_id><![CDATA[81100124987]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ergun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akleman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809676</person_id>
				<author_profile_id><![CDATA[81342491147]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jianer]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809677</person_id>
				<author_profile_id><![CDATA[81342496250]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gross]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Columbia University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566635</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Soler, C., Cani, M. P., and Angelidis, A. 2002. Hierarchical pattern mapping. <i>Proceedings of ACM SIGGRAPH 2001 21</i>, 4, 673--680.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Pattern Mapping with Quad-Pattern-Coverable Quad-Meshes Shi-Yu, Qing Xing, Ergun Akleman &#38; Jianer 
Chen Jonathan Gross Texas A&#38;M University Columbia University (a) Escher-like pattern -anisotropic/periodic 
(b) twill-weave pattern -anisotropic/periodic (c) wood pattern -anisotropic/aperiodic Figure 1: Comparison 
of covering a genus-7 quad-pattern-coverable mesh with periodic and aperiodic tiles. Pattern mapping 
(i.e. texturing arbitrary surfaces with repetitive patterns [Soler et al. 2002]) is a particularly use­ful 
texture mapping technique, since repeating a pattern re­duces the memory cost, by mapping the same texture 
im­age to all faces. Moreover, pattern mapping does not re­quire painting a texture image for each surface 
or generat­ing a global texture map on each surface. Pattern mapping can potentially provide natural-looking 
materials, such as stone, wood, or marble, as well as human-made materials, such as wallpapers or repeating 
tiles. One of the main chal­lenges arising when mapping patterns to arbitrary polyhe­dral meshes is to 
avoid texture discontinuities caused by sin­gularities non-4-valent vertices in the quad-meshes. These 
discontinuities can appear at seams along the edges, which can be visually distracting. Unfortunately, 
it is not always possible to avoid non-4-valent vertices, since 4-regular quad­meshes exist only for 
genus-1 surfaces. (a) (b) (c) Figure 2: (a) a genus-4 quad-pattern-coverable mesh; (b) the same mesh 
after Catmull-Clark subdivision; (c) Wang tiles (wood pattern) superimposed on the quad-mesh. In this 
paper, we show that for any surface of positive genus, there exist quad-meshes that do not cause texture 
discon­tinuities. Using such quad-meshes, which we call quad­pattern-coverable meshes (abbr. QPC), it 
is possible to seam­lessly cover a surface of positive genus periodically with plane symmetric tiles 
or aperiodically with Wang tiles. Fig­ure 2 shows an example of a QPC mesh that is covered ape­riodically 
with four Wang tiles that give a wood texture. Figure 1 provides a visual comparison of periodic and 
ape­riodic patterns covered a QPC mesh. The periodic patterns that cover QPC meshes can easily be animated, 
since cyclic translations of wallpaper patterns are also wallpaper pat­terns. In this work, we establish 
su.cient conditions for a mesh to be classi.ed as QPC. Our results imply that a quad-mesh is not QPC 
if the valence of at least one vertex is not divisible by 4. This observation implies, in turn, that 
there exists no genus-0 QPC mesh, since a genus-0 quad-mesh always has some vertices with valences smaller 
than 4. For positive genus surfaces, by way of contrast, there exist a wide variety of mesh structures 
that can satisfy the su.ciency conditions. We have also developed algorithms for construction of QPC 
meshes based on combinatorial voltage graph theory. It is preferable to reduce the large multiples to 
valence 8, the smallest non-trivial multiple of 4. We introduce an op­eration that can transform the 
4k-valent vertices in a mesh into 8-valent vertices, while preserving the QPC property, regardless of 
surface genus. We also introduce another QPC preservation operation that can move such high valence ver­tices 
to any desired position to avoid texture distortions. Catmull-Clark subdivision preserves quad-pattern­coverability. 
It is possible, therefore, to cover any surface of positive genus with iteratively .ner versions of a 
given texture. Catmull-Clark subdivision is also useful in creating smooth models. In our examples such 
as Figure 1, starting with very coarse QPC meshes, we obtain smooth versions by Catmull-Clark subdivision. 
Instead of re-texturing the mesh, we bi-linearly interpolate texture coordinates. The result is equivalent 
to using curved quads, as shown in Figure 2. This work partially supported by the National Science Foun­dation 
under Grant No. NSF-CCF-0917288. References Soler, C., Cani, M. P., and Angelidis, A. 2002. Hierar­ 
chical pattern mapping. Proceedings of ACM SIGGRAPH 2001 21, 4, 673 680. Copyright is held by the author 
/ owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>National Science Foundation</funding_agency>
			<grant_numbers>
				<grant_number>NSF-CCF-0917288</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	</section>
	<section>
		<section_id>2037861</section_id>
		<sort_key>350</sort_key>
		<section_seq_no>9</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Walk the line]]></section_title>
		<section_page_from>9</section_page_from>
	<article_rec>
		<article_id>2037862</article_id>
		<sort_key>360</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>27</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Motion comics]]></title>
		<subtitle><![CDATA[visualization, browsing and searching of human motion data]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037862</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037862</url>
		<abstract>
			<par><![CDATA[<p>The use of motion capture data is becoming popular not only for graphics researcher and developers in industry but for common graphics application users. Recently large motion databases are publicly available on the web [CMU-Graphics-Lab 2003]. <i>Second Life</i> [2003] allows its users to animate their virtual characters by using their own collection of motion data. Even with the proliferation of motion data, searching a desired fragment of motion data from a large data repository is still challenging because of the lack of appropriate user interfaces.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809678</person_id>
				<author_profile_id><![CDATA[81452599435]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Myung]]></first_name>
				<middle_name><![CDATA[Geol]]></middle_name>
				<last_name><![CDATA[Choi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[JST ERATO IGARASHI Design Interface]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809679</person_id>
				<author_profile_id><![CDATA[81488644609]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kyungyong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809680</person_id>
				<author_profile_id><![CDATA[81100429553]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jehee]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809681</person_id>
				<author_profile_id><![CDATA[81100209891]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[JST ERATO IGARASHI Design Interface and University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809682</person_id>
				<author_profile_id><![CDATA[81100444444]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Takeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Igarashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[JST ERATO IGARASHI Design Interface and The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1073246</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Assa, J., Caspi, Y., and Cohen-Or, D. 2005. Action synopsis: pose selection and illustration. <i>ACM Transactions on Graphics (SIGGRAPH 2005)</i>, 667--676.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[CMU-Graphics-Lab, 2003. CMU motion capture database, http://mocap.cs.cmu.edu/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Linden-Lab, 2003. Second life, http://secondlife.com.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Motion Comics: Visualization, Browsing and Searching of Human Motion Data Myung Geol Choi1 Kyungyong 
Yang2 Jehee Lee2 Jun Mitani1,3 Takeo Igarashi1,4 JST ERATO IGARASHI Design Interface1 Seoul National 
University2 University of Tsukuba3 The University of Tokyo4 1 Introduction The use of motion capture 
data is becoming popular not only for graphics researcher and developers in industry but for common graphics 
application users. Recently large motion databases are publicly available on the web [CMU-Graphics-Lab 
2003]. Second Life [2003] allows its users to animate their virtual characters by using their own collection 
of motion data. Even with the prolifer­ation of motion data, searching a desired fragment of motion data 
from a large data repository is still challenging because of the lack of appropriate user interfaces. 
A brief description or keyword annotated by the one who acquired motion data might be helpful to some 
extent. However, text anno­tation could be subjective and often insuf.cient for describing the content, 
style, context, and nuance of human motion. One auto­mated solution is to pick out and display a number 
of distinctive frames of motion data [Assa et al. 2005]. This approach ef.ciently visualize signi.cant 
moments of motion data but has some limita­tion to explain overall story of the motion. In addition, 
it would be dif.cult to provide a proper search method or interface for the end-users. We introduce a 
comic-based interface for visualizing, browsing and searching human motion data. A comic is a medium 
which deliv­ers a story with a sequence of images. The power of comics is in its ability to convey dynamic 
and temporal information in an ab­stract form. A small collection of comic-style images can express much 
information, such as spatial movements, passage of time, the interaction between characters, and their 
emotional states. Speci.­cally, we aim two goals: One is visualizing motion capture data in a comic style 
and the other is searching motion data using comic­style sketch queries (see Figure 1). 2 Generating 
Motion Comics The comic-style visualization of motion data, called motion comics, generates a sequence 
of comic panels to capture the key moments of motion data. In human motion, the velocity (and kinetic 
energy) of joints decrease in a moment, when they change the direction of their movement or stop the 
motion. Therefore, the motion between two neighboring local minimums of the speed would be monotonic 
in most cases that can be easily inferred from two keyframes at both ends. Thus, we separate the motion 
at these points. Each motion segment constructs each panel of the motion comics. First we draw the keyframe 
(the last frame) of a segment in the corresponding panel then add cartoonish signs that explain more 
details of the mo­tion where it is necessary. For the joints whose maximum kinetic energy exceed prede.ned 
threshold, we draw a thin line or multiple lines along the trajectory of the joints to help viewers understand­ing 
of the motion speed . For the camera of each panel, we select the viewing angle that maximizes the spatial 
variance of important components (body joints at the key pose and the sampled points of trajectory and 
speed lines) in the motion segment on the 2D plane. Finally, for more compact representation, we detect 
a series of pan­els repeating similar motion (e.g. locomotion) and leave one panel among them. (b) Moton 
Data (c) User Figure 1: Motion comics for visualizing motion data and sketch­based motion query. Motion 
comic (a) is generated from the motion data (b), which can be retrieved by incomplete user sketches (c). 
 3 Searching Motion Comics Sketch-based motion query and comic-style motion visualization are inverse 
problems to each other. Two problems are mutually re­inforcing each other because the important features 
of comic-style visualization, such as stick .gures, trajectory/speed lines, and di­viding panels, can 
be also used for sketching queries. The unique aspect of our context-based query interface is that it 
allows partial drawings to be used to search fullbody motion data. The user may focus on sketching of 
his/her interests, for example gaze direction, and leave the other parts unspeci.ed, for example without 
arms and legs (see upper-right in Figure 1). We compute a feature vector for the user s input sketch 
and compare it against a feature vector computed for a panel in motion comics. If the input sketch is 
incomplete, the system enumerates all possi­ble joint label assignments and computes a feature vector 
for each assignment. The system then generates an equivalent set of feature vectors for each panel in 
motion comics. Our feature vector consists of two components, one for bone orientations and one for trajectory 
patterns. The system takes one or more comic panels drawn by the user and returns a sequence of comic 
panels from the database that matches to the user input. 4 Conclusion To demonstrate the effectiveness 
of our approach, we built a col­lection of motion comics from more than one hundred motion data capturing 
a diversity of human behaviors. In our simple user test of the sketch-based search, the participants 
mostly found out their desired motions with only simple and incomplete posture inputs. References ASSA, 
J., CASPI, Y., AND COHEN-OR, D. 2005. Action synopsis: pose selection and illustration. ACM Transactions 
on Graphics (SIGGRAPH 2005), 667 676. CMU-GRAPHICS-LAB, 2003. CMU motion capture database,. LINDEN-LAB, 
2003. Second life,. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, 
Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037863</article_id>
		<sort_key>370</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>28</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Parameterizing animated lines for stylized rendering]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037863</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037863</url>
		<abstract>
			<par><![CDATA[<p>We describe a method to parameterize lines generated from animated 3D models in the context of animated line drawings. Cartoons and mechanical illustrations are popular subjects of non-photorealistic drawings and are often generated from 3D models. Adding texture to the lines, for instance to depict brush strokes or dashed lines, enables greater expressiveness, e.g. to distinguish between visible and hidden lines. However, dynamic visibility events and the evolving shape of the lines raise issues that have been only partially explored so far. In this paper, we assume that the entire 3D animation is known ahead of time, as is typically the case for feature animations and off-line rendering. At the core of our method is a geometric formulation of the problem as a parameterization of the space-time surface swept by a 2D line during the animation. First, we build this surface by extracting lines in each frame. We demonstrate our approach with silhouette lines. Then, we locate visibility events that would create discontinuities and propagate them through time. They decompose the surface into charts with a disc topology. We parameterize each chart via a least-squares approach that reflects the specific requirements of line drawing. This step results in a texture atlas of the space-time surface which defines the parameterization for each line. We show that by adjusting a few weights in the least-squares energy, the artist can obtain an artifact-free animated motion in a variety of typical non-photorealistic styles such as painterly strokes and technical line drawing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809683</person_id>
				<author_profile_id><![CDATA[81488649854]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Buchholz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Telecom ParisTech -- CNRS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809684</person_id>
				<author_profile_id><![CDATA[81309487978]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tamy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boubekeur]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Telecom ParisTech -- CNRS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809685</person_id>
				<author_profile_id><![CDATA[81488671777]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Noura]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Faraj]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Telecom ParisTech -- CNRS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809686</person_id>
				<author_profile_id><![CDATA[81310501633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Elmar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eisemann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Telecom ParisTech -- CNRS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809687</person_id>
				<author_profile_id><![CDATA[81440595240]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Sylvain]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Paris]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Adobe]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>882355</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kalnins, R. D., Davidson, P. L., Markosian, L., and Finkelstein, A. 2003. Coherent stylized silhouettes. <i>ACM Transactions on Graphics 22</i>, 3 (July), 856--861.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Parameterizing Animated Lines for Stylized Rendering Bert Buchholz Tamy Boubekeur Noura Faraj Elmar 
Eisemann Sylvain Paris Telecom ParisTech CNRS Adobe Figure 1: Our algorithm builds a temporally consistent 
parameterization for lines extracted from an animated 3D scene. Left: Example sequence of two metaballs 
joining and parting again, creating the depicted space-time surface including split lines. Lines are 
then parame­terized and textured. Right: Further results. 1 Overview We describe a method to parameterize 
lines generated from ani­mated 3D models in the context of animated line drawings. Car­toons and mechanical 
illustrations are popular subjects of non­photorealistic drawings and are often generated from 3D models. 
Adding texture to the lines, for instance to depict brush strokes or dashed lines, enables greater expressiveness, 
e.g. to distinguish be­tween visible and hidden lines. However, dynamic visibility events and the evolving 
shape of the lines raise issues that have been only partially explored so far. In this paper, we assume 
that the entire 3D animation is known ahead of time, as is typically the case for fea­ture animations 
and off-line rendering. At the core of our method is a geometric formulation of the problem as a parameterization 
of the space-time surface swept by a 2D line during the animation. First, we build this surface by extracting 
lines in each frame. We demon­strate our approach with silhouette lines. Then, we locate visibility events 
that would create discontinuities and propagate them through time. They decompose the surface into charts 
with a disc topology. We parameterize each chart via a least-squares approach that re­.ects the speci.c 
requirements of line drawing. This step results in a texture atlas of the space-time surface which de.nes 
the parame­terization for each line. We show that by adjusting a few weights in the least-squares energy, 
the artist can obtain an artifact-free an­imated motion in a variety of typical non-photorealistic styles 
such as painterly strokes and technical line drawing. On the contrary to previous approaches [2003], 
we do not seek for real time processing but rather for high quality output from a known animated sequence. 
Therefore, we introduce a global space-time approach to the line parameterizaton problem. 2 Spatio-Temporal 
Analysis Our algorithm takes as input a possibly animated 3D model and a sequence of camera viewpoints. 
We assume that the model has a temporally consistent 1-to-1 correspondance, that is, for any vertex of 
the model, we can compute its trajectory during the animation. Our objective is to parameterize 2D lines 
generated by typical line drawing methods such as contours and silhouettes. In this work, we seek for 
a parameterization of the lines that is temporally consistent. First, the lines are extracted independently 
for each frame of the an­imation before being grouped as plausible corresponding lines from frame to 
frame. Second, the so de.ned 2D+t model takes the form of a space-time surface which acts as our underlying 
decomposition model for the rest of the algorithm. Third, lines may split or merge during the animation 
due to occlusion/visibility events. We handle these events by reusing the same cut for several discontinuities 
to avoid over-segmenting the lines. With our space-time formulation, the cuts are geodesic lines on the 
space-time surface and we handle discontinuities by decompos­ing the space-time surface into charts with 
a disc topology. In the following step, these space-time charts are parameterized indepen­dently optionally 
under user control to provide time-coherent parametric lines. 3 Parameterization During the animation, 
a single open line sweeps a portion of the space-time surface resulting in charts with disc topology. 
We seek for a temporally coherent 1D+t parameterization of the lines by means of a 2D parameterizaton 
of each chart. The two main con­straints are the temporal and spatial coherence of the parameteriza­tion. 
Intuitively, temporal coherence corresponds to the presevation of the parameterization of a vertex of 
a line while it moves through space (i.e., sticking the parameterization to the line in 3D). Spa­tial 
coherence keeps the parameterization coherent on the projected lines (i.e., in image space). Additionally, 
we aim at preserving a [0, 1] parameterization of each line. In general, these three constraints cannot 
be satis.ed simultane­ously. Therefore, we formulate these constraints as energies and minimize their 
combination using a least-squares approach result­ing in a trade-off which can be intuitively controlled 
by setting a few weights. Finally, the parameterized lines can be used to achieve a coherent texturing 
on the lines during the animation and the under­lying chart decomposition of the space-time surface allows 
users to edit space-time lines easily (e.g., selecting a speci.c texture for a speci.c animated line). 
 References KALNINS, R. D., DAVIDSON, P. L., MARKOSIAN, L., AND FINKELSTEIN, A. 2003. Coherent stylized 
silhouettes. ACM Transactions on Graphics 22, 3 (July), 856 861. Copyright is held by the author / owner(s). 
SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037864</article_id>
		<sort_key>380</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>29</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Multiperspective rendering for anime-like exaggeration of joint models]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037864</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037864</url>
		<abstract>
			<par><![CDATA[<p>In this paper, representing view-dependent deformation of the perspective effect with a multi-perspective approach is discussed. Traditional hand-drawn animation often constructs a more flexible and impressive depiction than that of computer graphics. Among deformations in handmade drawing, anime has traditionally used deformed depictions that are used to exaggerate the effect of perspective in order to exaggerate the posture of a figure into stylish and dramatic distortions. However, in terms of 3D geometry, the depiction may contain a few contradictions between each part of the body and the background.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[exaggeration]]></kw>
			<kw><![CDATA[multiperspective rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809688</person_id>
				<author_profile_id><![CDATA[81319502900]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Utsugi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kei.utsugi.nz@hitachi.com]]></email_address>
			</au>
			<au>
				<person_id>P2809689</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[naemura@nae-lab.org]]></email_address>
			</au>
			<au>
				<person_id>P2809690</person_id>
				<author_profile_id><![CDATA[81331496069]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takafumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koike]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hitachi Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[takafumi.koike.kj@hitachi.com]]></email_address>
			</au>
			<au>
				<person_id>P2809691</person_id>
				<author_profile_id><![CDATA[81317499090]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Oikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hitachi Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[michio.oikawa.ky@hitachi.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2383542</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Yu, J., and Mcmillan, L. 2004. A framwork for multiperspective rendering. <i>Eurographics Symposium on Rendering</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Figure 1: en from which the corresponding joint segment Mn Keywords: multiperspective rendering, exaggeration 
In this paper, representing view-dependent deformation of the per­spective effect with a multi-perspective 
approach is discussed. Tra­ditional hand-drawn animation often constructs a more .exible and impressive 
depiction than that of computer graphics. Among de­formations in handmade drawing, anime has traditionally 
used de­formed depictions that are used to exaggerate the effect of perspec­tive in order to exaggerate 
the posture of a .gure into stylish and dramatic distortions. However, in terms of 3D geometry, the de­piction 
may contain a few contradictions between each part of the body and the background. We introduce a method 
for the anime-like exaggeration of the ap­pearance of perspective. Our novel method achieves an energetic 
depiction when rendering animation by using multiperspective ren­dering. Multiperspective rendering is 
a non-photorealistic render­ing method that combines what is seen from several viewpoints into a single 
image and enables the rendering of anomalous pictures [Yu and McMillan 2004]. Our contribution with this 
rendering method is the realtime and interactive coordination of viewpoints special­ized for .gure models. 
We focused on a systematic arrangement of viewpoints specialized to depict the exaggerated rendering 
of an anime-like .gure model with articulated joints so that these view­points represent exaggerations 
similar to traditional freehand draw­ing in anime. For this purpose, we constructed a tree structure 
that we call a viewpoint hierarchy (VH). Here, The VH is isomorphic to a .g­ure s joint hierarchy, and 
its nodes contain the viewpoints for each joint part necessary for multiple perspectives (Figure 1-(c)). 
VH dynamically arranges each viewpoint between the viewpoint s par­ent node viewpoint and a control point 
embedded in a .gure model (Figure 2). In a user test, users were asked to adjust exaggeration parameters 
of .gures with a camera motion. As a result, most users preferred partially exaggerated images with closer 
pseudo viewpoints than that of correct distance. (Figure 3) *e-mail: kei.utsugi.nz@hitachi.com e-mail:naemura@nae-lab.org 
e-mail:takafumi.koike.kj@hitachi.com §e-mail:michio.oikawa.ky@hitachi.com Figure 2: View- Figure 3: 
Example of a user preferred parameter. Acknowledgements Part of this study is sponsored by the National 
Institute of Informa­tion and Communications Technology (NICT). References YU, J., AND MCMILLAN, L. 
2004. A framwork for multiperspec­tive rendering. Eurographics Symposium on Rendering. Copyright is held 
by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 
978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037865</article_id>
		<sort_key>390</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>30</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Learning to classify human object sketches]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037865</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037865</url>
		<abstract>
			<par><![CDATA[<p>We present ongoing work on object category recognition from binary human outline sketches. We first define a novel set of 187 "sketchable" object categories by extracting the labels of the most frequent objects in the LabelMe dataset. In a large-scale experiment, we then gather a dataset of over 5,500 human sketches, evenly distributed over all categories. We show that by training multi-class support vector machines on this dataset, we can classify novel sketches with high accuracy. We demonstrate this in an inter-active sketching application that progressively updates its category prediction as users add more strokes to a sketch.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809692</person_id>
				<author_profile_id><![CDATA[81336488955]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mathias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eitz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Berlin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[m.eitz@tu-berlin.de]]></email_address>
			</au>
			<au>
				<person_id>P2809693</person_id>
				<author_profile_id><![CDATA[81100345426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hays]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hays@cs.brown.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2053424</ref_obj_id>
				<ref_obj_pid>2053035</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Eitz, M., Hildebrand, K., Boubekeur, T., and Alexa, M. 2011. Sketch-based image retrieval: benchmark and bag-offeatures descriptors. <i>IEEE Trans. Vis. Comp. Graph.</i>. Preprints.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>946751</ref_obj_id>
				<ref_obj_pid>946247</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sivic, J., and Zisserman, A. 2003. Video Google: A Text Retrieval Approach to Object Matching in Videos. In <i>IEEE ICCV</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Learning to classify human object sketches Mathias Eitz* James Hays TU Berlin Brown University Figure 
1: Left: sample sketches from two categories, right: averages of 30 sketches per category. Abstract We 
present ongoing work on object category recognition from bi­nary human outline sketches. We .rst de.ne 
a novel set of 187 sketchable object categories by extracting the labels of the most frequent objects 
in the LabelMe dataset. In a large-scale exper­iment, we then gather a dataset of over 5,500 human sketches, 
evenly distributed over all categories. We show that by training multi-class support vector machines 
on this dataset, we can classify novel sketches with high accuracy. We demonstrate this in an inter­active 
sketching application that progressively updates its category prediction as users add more strokes to 
a sketch. 1 Introduction Sketching is a common means of visual communication often used for conveying 
rough visual ideas as in architectural drawings, de­sign studies, comics, or movie storyboards. There 
exists signif­icant prior research on retrieving images or 3d models based on sketches. The assumption 
in all of these works is that, in some well-engineered feature space, sketched objects will resemble 
their real-world counterparts. But this fundamental assumption is often violated most humans are not 
faithful artists. Instead people use shared, iconic representations of objects (e.g. stick .gures) or 
they make dramatic simpli.cations or exaggerations. Because the rela­tionship between sketched and real 
objects is so abstract, to recog­nize sketched objects one must learn from a training database of real 
sketches. Because people represent the same object using dif­fering degrees of realism and distinct drawing 
styles (see Fig. 1, left), we believe that a successful approach can only be based on a dataset that 
provides a suf.ciently dense sampling of that space, i.e. we need a large training dataset of sketches. 
Although both shape and proportions of a sketched object may be far from that of the corresponding real 
object, and at the same time sketches are an im­poverished visual representation, humans are amazingly 
accurate at interpreting such sketches. In this paper we demonstrate our ongo­ing work on trying to teach 
computers classify sketched objects just as humans do effortlessly. 2 Dataset &#38; Classi.cation To 
classify sketches we address four main tasks: 1) de.ning a set of object categories ideally those would 
represent the most common objects in our world; 2) creating a dataset of sketches with diverse samples 
for each category; 3) de.ning low-level features for rep­resenting the sketches and .nally 4) training 
classi.ers from our *e-mail: m.eitz@tu-berlin.de e-mail: hays@cs.brown.edu dataset such that we can accurately 
recognize novel sketches. We have de.ned a list of common object categories by computing the 1,000 most 
frequent objects from the LabelMe dataset and collaps­ing semantically similar categories. This resulted 
in 187 object cat­egories, mainly containing common objects such as airplane, house, cup, and horse. 
In a large-scale user experiment, we asked humans to sketch such objects given only the category name. 
We instructed them to a) draw sketches that would be clearly recognizable to other humans as belonging 
to a given category, b) use outlines only and c) avoid context around the actual object. Currently, the 
dataset contains 30 sketches in each category for a total of about 5,500 sketches. We have performed 
this experiment using Ama­zon Mechanical Turk. For learning on this dataset we perform two main steps: 
we employ a bag-of-features approach [Sivic and Zis­ serman 2003] and use SIFT-like descriptors to represent 
sketches as histograms of visual words [Eitz et al. 2011]. We train one-vs­ all classi.ers using support 
vector machines with RBF kernels. We .nd the best model by performing grid search over the parameters 
space of the SVM and use 5-fold cross-validation to avoid over­.tting. The best-performing model achieves 
an accuracy of about 37%. This is a very reasonable result considering that chance lies at about 0.54%. 
We additionally demonstrate the subjectively very good performance of the resulting model in an interactive 
applica­tion that progressively visualizes classi.cation results as the user adds strokes to a sketch 
(please see the accompanying video). 3 Conclusion We have demonstrated that given a large dataset of 
sketches rea­sonable classi.cation rates can be achieved, limited primarily (we believe) by the bag-of-features 
representation which does not en­code any spatial information. Clearly, constructing better features, 
and extending and analyzing the dataset are promising areas for future work. Finally, the large dataset 
in itself (which we plan to provide as a free resource) as well as the semantic sketch classi.ca­tion 
will be highly bene.cial for applications such as sketch-based image and 3D model retrieval.  References 
EITZ, M., HILDEBRAND, K., BOUBEKEUR, T., AND ALEXA, M. 2011. Sketch-based image retrieval: benchmark 
and bag-of­features descriptors. IEEE Trans. Vis. Comp. Graph.. Preprints. SIVIC, J., AND ZISSERMAN, 
A. 2003. Video Google: A Text Re­trieval Approach to Object Matching in Videos. In IEEE ICCV. Copyright 
is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. 
ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037866</section_id>
		<sort_key>400</sort_key>
		<section_seq_no>10</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[1000 points of light]]></section_title>
		<section_page_from>10</section_page_from>
	<article_rec>
		<article_id>2037867</article_id>
		<sort_key>410</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>31</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Tokyo race lighting for <i>Cars 2</i>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037867</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037867</url>
		<abstract>
			<par><![CDATA[<p>Tokyo at night is one of the world's most beautiful locations. The architecture combined with a massive quantity of illuminated signs creates a visual complexity rarely seen. The challenge was to recreate the emotional impact and excitement of downtown Tokyo but maintain the visual style of Cars 2. Additionally, this lighting setup had to work over a large number of locations and camera angles. This led to thinking about the setup as a lighting environment, and not just an angle specific rig. To break up the complexity of this visual problem, each component of the lighting was focused on individually.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809694</person_id>
				<author_profile_id><![CDATA[81328489039]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mitch]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kopelman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Tokyo Race Lighting for Cars 2 Mitch Kopelman Pixar Animation Studios Tokyo at night is one of the 
world s most beautiful locations. The architecture combined with a massive quantity of illuminated signs 
creates a visual complexity rarely seen. The challenge was to recre­ate the emotional impact and excitement 
of downtown Tokyo but maintain the visual style of Cars 2. Additionally, this lighting setup had to work 
over a large number of locations and camera angles. This led to thinking about the setup as a lighting 
environment, and not just an angle speci.c rig. To break up the complexity of this visual problem, each 
component of the lighting was focused on in­dividually. c Figure 1: Tokyo Race Lighting for Cars 2. 
@Disney / Pixar. All rights reserved. Figure 2: Rainbow Bridge Lighting for Cars 2.c @Disney / Pixar. 
All rights reserved. sign re.ections. The buildings themselves were fairly dim and they have a convenient 
rectangular shape, so using a texture based ap­proach to simulate their re.ections was accurate enough. 
Using a texture based approach also allowed for the baking of the diffuse irradiance into the texture, 
which would have been too expensive to calculate with ray-tracing. The race track lights also used a 
texture based approach, stringing 100 s of texture cards along the bottom of the actual lamps. 1 Di.use 
Lighting in the Tokyo Race In the Tokyo Race there are three major sources of diffuse light; the lights 
illuminating the track proper, the illuminated signs hanging on the buildings and the headlights of the 
racer s themselves. Each of these needed a unique approach. The track and racers are illu­minated by 
a series of overhead lights on the sides of the track. For this, line lights (an analytic light created 
with a line segment) were implemented. These line lights have the perfect shape to mimic the throw of 
hundreds of .xtures all in a row. Shadowing these lights proved the more complicated issue. Many variations 
of ray­traced and mapped shadows were implemented before the correct balance of render time vs. aesthetic 
desire was achieved. The sec­ond largest in.uence on the set s diffuse lighting was the illumi­nated 
signs. There are 100 s of signs hand placed throughout the set and they needed to have a .exible and 
accurate system of il­lumination. Surprisingly, a ray-traced solution using point based irradiance provided 
a great simulation of these signs. Lastly, the headlights of the racers themselves were a large contributor 
of dif­fuse illumination. These used a traditional approach of pointlights combined with shadow maps 
and shapers (barns and slides). 2 Re.ection in the Tokyo Race Re.ection was the most complex and expensive 
portion of the setup to build and render. The major components were the track lights, illuminated signs, 
and the re.ection of the buildings into each other and the racers. The illuminated signs were the most 
important vi­sual feature of the re.ections. Mimicking the approach for the dif­fuse component, ray-tracing 
was used to accurately simulate the Figure 3: Back Alley Lighting for Cars 2.c @Disney / Pixar. All 
rights reserved. 3 Optimization With several thousand lights and a few dozen Tokyo city blocks full 
of geometry, the full Tokyo setup was fairly un-renderable. Careful optimization was needed to make sure 
each shot had the lights it needed but nothing more. The setup was built in a compartmen­talized fashion, 
so that entire sections could be easily deactivated when not on screen. Additionally, in many cases, 
the lights them­selves were built procedurally. This allowed the lighting setup to quickly adapt when 
the race course was modi.ed and allowed dif­ferent sections of the city to be built up quickly. Perhaps 
most im­portantly, the same general techniques were used throughout the city. The names of the lights 
and geometry might change, but the underlying approaches for the illumination was always consistent. 
This allowed the lighting artists to quickly ramp up in the differ­ent sections of the sequence and provided 
visual continuity for the sequence as a whole. Copyright is held by the author / owner(s). SIGGRAPH 2011, 
Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037868</article_id>
		<sort_key>420</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>32</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Megamind]]></title>
		<subtitle><![CDATA[lighting metro city at night]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037868</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037868</url>
		<abstract>
			<par><![CDATA[<p>In the Dreamworks Animation film Megamind, our main visual goal for the sequence where Roxanne is kidnapped by Titan, is for the audience to experience first hand the grave danger that she is going through. To achieve this goal, we needed to make sure that everybody could relate to what a city looks like. Given that the lighting is such an important visual component of what defines a city at night, we paid special attention to the lighting to ensure it was tangible and believable.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809695</person_id>
				<author_profile_id><![CDATA[81488672110]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jimmy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maidens]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jimmy.maidens@dreamworks.com]]></email_address>
			</au>
			<au>
				<person_id>P2809696</person_id>
				<author_profile_id><![CDATA[81458648127]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Philippe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Denis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[philippe.denis@dreamworks.com]]></email_address>
			</au>
			<au>
				<person_id>P2809697</person_id>
				<author_profile_id><![CDATA[81488672262]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gianni]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aliotti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 MEGAMIND Lighting Metro City at Night Jimmy Maidens Philippe Denis* Gianni Aliotti DreamWorks Animation 
 1 Introduction In the Dreamworks Animation film Megamind, our main visual goal for the sequence where 
Roxanne is kidnapped by Titan, is for the audience to experience first hand the grave danger that she 
is going through. To achieve this goal, we needed to make sure that everybody could relate to what a 
city looks like. Given that the lighting is such an important visual component of what defines a city 
at night, we paid special attention to the lighting to ensure it was tangible and believable. 2 Visual 
Challenges In studying the look of a city at night, three observations can be made. The first is that 
buildings are mostly defined by their interior lighting (fig.1). The second is that cities are defined 
by their street's lighting (fig.2). The third is that the lighting in the streets is fairly soft due 
to the multitude of lamppost and light sources. With traditional techniques such as spot lights, we were 
facing the many light problem. Additionally we needed to come up with a soft lighting solution which 
presented us with yet another challenge. Figure 1: Interior lighting Figure 2: City streets.  3 Lighting 
Approach To solve the complexity of building interiors, a shader was created that mapped a cubic environment 
map, representing the interior of a room, onto the window geometry with the appropriate perspective. 
This solution offered great visual results, good control and was very efficient from a rendering standpoint. 
However, efficiently lighting the exteriors of the buildings and the city streets presented a larger 
challenge. A full global illumination approach was unsuccessful due to the size and complexity of the 
set. A simpler approach, combining several lightweight passes (occlusion, noise, and gradients) was not 
satisfying visually. The best result came from choosing a hybrid solution. Different components of the 
lighting were split into separate Point Base Global Illumination files (PBGI) cached on disks. Each one 
of these files contained different types of data (occlusion, static direct lighting, and dynamic direct 
lighting). The passes were then combined together at shading time (fig.3). This allowed us to create 
a fairly standardized lighting rig shared throughout the sequence while providing the creative control 
required. We were also able to split the city point cloud file into separate neighborhoods files using 
our procedural city system further allowing a level of detail management and optimization. email: jimmy.maidens@dreamworks.com 
*email: philippe.denis@dreamworks.com Point cloud Ambient occlusion glow  Street lights Composited 
layers Figure 3  4 Compositing The most impactful lighting passes were the PBGI-based ones such as: 
sky fill, street fill, car light contribution, car lights, and street lights. These provided the bulk 
of the look. Various mattes (Z, Y, cars, buildings, and occlusion) were used to affect specific areas 
of the city. Separate reflection and ambient passes were used to control the look of the buildings' glass 
and interior lighting that was so prevalent in our reference. Using these various passes, real-time interactivity 
was achieved while still allowing the VFX Supervisor and Art Director to direct a very shot-specific 
look (fig.4).  Compositing pass. Figure 5  5 Conclusion Achieving a believable cityscape at night was 
our main goal and the biggest challenge was to achieve a result that would not only work from afar, but 
also up close. Additionally, the solution needed to be fairly interactive and fully customizable from 
one shot to another. Deliver finals in a reasonable amount of time and yet keep control of the creative 
side was our main constraint. Our in­house PBGI lighting tool and the ability to procedurally divide 
up the city into neighborhood, combined with a lightweight compositing approach, helped us achieve our 
lofty goal and deliver Metro City at Night on time. email: gianni.aliotti@dreamworks.com Copyright is 
held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. 
ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037869</article_id>
		<sort_key>430</sort_key>
		<display_label>Article No.</display_label>
		<pages>2</pages>
		<display_no>33</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Deferred shading techniques using frostbite in "Battlefield 3" and "Need for Speed the Run"]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037869</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037869</url>
		<abstract>
			<par><![CDATA[<p>Deferred shading algorithms offer games the ability to render many light sources in real-time, however traditional approaches to implementing deferred shading on the GPU are fill-rate intensive and constrain light complexity to maintain real time performance.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809698</person_id>
				<author_profile_id><![CDATA[81488651718]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alex]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ferrier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Black Box (Electronic Arts)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[aferrier@ea.com]]></email_address>
			</au>
			<au>
				<person_id>P2809699</person_id>
				<author_profile_id><![CDATA[81488641748]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Coffin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DICE (Electronic Arts)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[christina.coffin@dice.se]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Deferred Shading Techniques using Frostbite in Battlefield 3 and "Need for Speed The Run C:\Users\aferrier\AppData\Local\Microsoft\Windows\Temporary 
Internet Files\Content.Outlook\G6DUI70L\image2.bmp Alex Ferrier     Christina Coffin Black Box (Electronic 
Arts)     DICE (Electronic Arts) aferrier@ea.com  christina.coffin@dice.se  1. Introduction Deferred 
shading algorithms offer games the ability to render many light sources in real-time, however traditional 
approaches to implementing deferred shading on the GPU are fill-rate intensive and constrain light complexity 
to maintain real time performance. For Battlefield 3 and Need for Speed The Run we wanted to pursue 
a new art direction with a massive increase in the amount of dynamic lighting without compromising on 
frame-rate or scene complexity found in our previous games. To achieve this, we implemented a tile based 
lighting architecture that improves light culling, reduces bandwidth, and enables shading to execute 
in parallel with other GPU work.  2. PS3 Rendering Frame Breakdown For our deferred shading implementation 
on the Playstation 3, we shade in parallel with the main rendering, using the SPUs as a co-processor 
to the GPU: We split rendering of a frame into three stages: . The g-buffer and z-buffer laydown itself, 
which must be completed before lighting. . Intermediate rendering passes that do not depend on the results 
of lighting and thus can be computed independently, e.g. forward-rendered environment maps, virtual texturing, 
particle buffer rendering, and SSAO. . Final compositing operations that depend on lighting results 
e.g. bloom and tone-mapping.  The rendering passes in the second stage take the GPU approximately 7 
milliseconds to complete. During this time, we use a parallel tile-based algorithm running on 5 SPUs 
to deferred shade the g-buffer and z-buffer with the local lights in the scene. Lights are classified 
by screen area, and those larger than 16-pixel radius are rasterized by SPU tasks into 32x16 pixel tiles 
of screen. The accumulated lighting results are then transferred back to GPU memory and the GPU and SPUs 
synchronized before the final stage begins. Small, typically distant, lights are still rasterized on 
the GPU as the fill cost of their small bounding primitives is lower than the overhead of many small 
lights intersecting tiles. 3. Tile-Based Classification for Outdoor Lighting After computation of local 
lights on SPU, we also render sunlight, deferred environment map application, and shadow-casting local 
spotlights on the GPU using a tile-based architecture. In addition to shadow maps, most objects in our 
scene receive one of several real-time dynamic environment maps, and these environment maps are applied 
in the deferred pass to preserve HDR values at 16-bit precision. Current console graphics hardware is 
inefficient at branching around texture reads on a per-pixel level, which makes sampling from multiple 
environment maps in the deferred pass too slow for production use. To avoid branches, during the SPU 
local lighting task we perform tile classification on the g-buffer and z-buffer to determine which textures 
can contribute to the image within each 32x16 pixel tile. Our outdoor lighting is then computed on the 
GPU, using a shader permutation selected per-tile by the bitmask of maps classified as affecting geometry 
in that tile. As a result, the GPU does not have to branch on texture reads, and efficiently samples 
only those textures contributing to each individual tile. 4. A Hybrid CPU/GPU implementation for Xbox360 
For our Xbox360 implementation, a multi-stage shader analyzes the GBuffer and Z+Stencil data into a 40x23 
pixel MRT image, each pixel maps to a 32x32 pixel source image tile. The encoded tile information contains 
min/max depth extents, specular albedo, and material type residency information which is used to optimize 
light culling and shader permutation selection for each 32x32 pixel image tile. After the GPU encodes 
the information, the GPU triggers CPU jobs that read the encoded pixel data to generate command buffers 
and vertex buffer data for the GPU in order to optimally shade each tiled area depending on its material 
screen coverage. While the CPU job is running, the GPU continues rendering shadowmaps and particles 
to avoid the GPU stalling and minimize latency. Once the CPU job is completed, the generated vertex and 
command buffer data is passed to the GPU to complete the required shading work. 5. Conclusion By reconstructing 
our lighting algorithms, and maximizing the amount of work we can do on the GPU in parallel with CPU/SPU 
work, we are able to achieve an entirely new visual look for the franchise whilst preserving geometric 
complexity in our scenes. Additionally, by implementing a reference environment on high-end PCs first, 
we allowed our artists to iterate and tune content to achieve our visual targets early in the project. 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037870</section_id>
		<sort_key>440</sort_key>
		<section_seq_no>11</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Fur and feathers]]></section_title>
		<section_page_from>11</section_page_from>
	<article_rec>
		<article_id>2037871</article_id>
		<sort_key>450</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>34</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Quill]]></title>
		<subtitle><![CDATA[birds of a feather tool]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037871</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037871</url>
		<abstract>
			<par><![CDATA[<p><i>Legend of the Guardians: The Owls of Ga'Hoole (LotG)</i> features over sixty distinctive, art-directed, hyper-realistic feathered characters. We developed a procedural feathering pipeline, <i>Quill</i>, to efficiently realise more than fifteen unique bird species required by the story. This toolset allows a procedural representation of feathers to be developed by surfacing artists, augmented with automatic deintersection, animation, character effects and dynamics and rendered with extensive level-of-detail support.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809700</person_id>
				<author_profile_id><![CDATA[81317494706]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heckenberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Animal Logic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[danielh@al.com.au]]></email_address>
			</au>
			<au>
				<person_id>P2809701</person_id>
				<author_profile_id><![CDATA[81488649267]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Damien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gray]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Animal Logic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[damieng@al.com.au]]></email_address>
			</au>
			<au>
				<person_id>P2809702</person_id>
				<author_profile_id><![CDATA[81488657358]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bryan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Smith]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Animal Logic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[bryans@al.com.au]]></email_address>
			</au>
			<au>
				<person_id>P2809703</person_id>
				<author_profile_id><![CDATA[81488642141]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wills]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Animal Logic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jonathanw@al.com.au]]></email_address>
			</au>
			<au>
				<person_id>P2809704</person_id>
				<author_profile_id><![CDATA[81488664951]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bone]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Animal Logic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[bone@al.com.au]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Quill: Birds of a Feather Tool Daniel Heckenberg Damien Gray Bryan Smith Jonathan Wills Chris Bone Animal 
Logic*  c Figure 1: A cast of owls. &#38;#169;Warner Bros. All rights reserved. Abstract Legend of 
the Guardians: The Owls of Ga Hoole (LotG) features over sixty distinctive, art-directed, hyper-realistic 
feathered charac­ters. We developed a procedural feathering pipeline, Quill, to ef.­ciently realise more 
than .fteen unique bird species required by the story. This toolset allows a procedural representation 
of feathers to be developed by surfacing artists, augmented with automatic dein­tersection, animation, 
character effects and dynamics and rendered with extensive level-of-detail support. 1 Character Design 
LotG depicts a world of owls, requiring many individual charac­ters to be clearly distinguishable, directable 
and emote in a broad repertoire of performance including dialogue, combat and .ight. Characters were 
art-directed with a realistic aesthetic (see .gs 1,2) requiring their feathers to be fully modeled, articulated 
and simu­lated. Almost every shot of the .lm included a number of birds, from subtle close-ups to wide, 
kinetic crowd scenes.  Figure 2: Character designs such as Ezylryb (left) were captured with an interactive 
toolset (center) to produce rich rendered output (right). c &#38;#169;Warner Bros. All rights reserved. 
 2 Pipeline Quill feathers are available in every department in the production pipeline. Our proprietary 
node-based, procedural animation and simulation engine ALF was extended to support modeling, surfac­ing, 
animation, effects and rendering of feathers. Procedural feath­ * {danielh,damieng,bryans,jonathanw,bone}@al.com.au 
 ers may be controlled and visualised throughout our pipeline in XSI, Maya and PRMan. 3 Grooming Typical 
grooms contain upwards of 20,000 feathers of which 500 to 1000 would be individually con.gured. Through 
interactive ma­nipulators in Maya, artists directly control many aspects of a para­metric feather primitive 
such as outline shape, length, density and shape of barbs and also deformations such as twist, fold and 
curl. Featherparameters and orientation are interpolated across the bird s surface to other passive feathers 
with modulation from maps and noise. Groom poses may be de.ned for extremes of movement (e.g. open .ight 
vs closed perched wings) and emotional performance (e.g. prominent ear feathers). 4 Feather Construction 
The feather primitive produces both .at geometry (a subdivision mesh or curves, depending on level-of-detail) 
and a simple 2d lat­tice for deformation. Lattices are treated as proxy meshes for au­tomatic feather 
deintersection and simulation. Deintersection is achieved by generating an implicit normal .eld from 
all of the inter­polated lattices and then reconstructing each lattice to be orthogonal to the normal 
.eld. 5 Animation and Dynamics Feathers are animated primarily through rig-based control and de­formation 
of the surface mesh and auxilliary meshes such as wing membranes. Animators may also key the blend weights 
of feather poses and the kinematics of hero feathers. Character effects are per­formed on the deformation 
lattices through procedural effects and soft-body simulations which are then applied to the feather geome­try 
for preview and rendering. 6 Rendering Extensive level-of-detail and procedural data management is re­quired 
to ef.ciently render close shots of feathers with millions of individual barbs as well as crowd shots 
with hundreds of feathered characters. The procedural ALF node-graph provides support for deferred evaluation 
of graph components and level-of-detail allow­ing each feather to be rendered from a simple quadrilateral 
to thou­sands of curves. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British 
Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037872</article_id>
		<sort_key>460</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>35</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Dynamic, penetration-free feathers in <i>Rango</i>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037872</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037872</url>
		<abstract>
			<par><![CDATA[<p>A typical workflow for a CGI character with feathers involves the manual placement of primary feathers with a guide curve to represent the shaft. Adherence to character design and visual development is extremely important, and the process of instancing and guiding feather geometry along these curves, without interpenetrations, is very challenging. <i>Rango</i> required many hero-level avian characters to attain fine detail in their rest pose and to maintain that look throughout animation, breathing shapes, and the application of wind, momentum, and inertia.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[collision-detection]]></kw>
			<kw><![CDATA[feathers]]></kw>
			<kw><![CDATA[simulation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809705</person_id>
				<author_profile_id><![CDATA[81488670386]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Bowline]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light and Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sbowline@ilm.com]]></email_address>
			</au>
			<au>
				<person_id>P2809706</person_id>
				<author_profile_id><![CDATA[81320490688]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zoran]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ka&#269;i&#263;-Alesi&#263;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light and Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[zoran@ilm.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1516523</ref_obj_id>
				<ref_obj_pid>1516522</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Weber, A. J., and Gornowicz, G. 2009. Collision-free construction of animated feathers using implicit constraint surfaces. <i>ACM Transaction on Graphics 28</i>, 2 (April).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Dynamic, Penetration-Free Feathers in Rango StephenD.Bowline * ZoranKa.c-Alesi´ ci´c IndustrialLight 
andMagic IndustrialLight andMagic  Figure 1: Theredtailhawk,with4800primaryfeathers,andthemariachiwithoveronethousand, 
fromRango.Each characterwasprocessed with our simple,dynamicsbasedpenetration avoidance work.ow. Keywords: 
Collision-Detection,Feathers,Simulation 1 Introduction A typical work.ow for a CGI character with feathers 
involves the manual placement of primary feathers with a guide curve to rep­resent the shaft. Adherence 
to character design and visual devel­opment is extremely important, and the process of instancing and 
guidingfeathergeometry along thesecurves,without interpenetra­tions, is very challenging. Rango required 
many hero-level avian characters toattain.nedetailintheirrestposeand tomaintainthat lookthroughout animation,breathing 
shapes,andtheapplicationof wind, momentum, and inertia. We present a simulation based technique that 
preserves the artist s visual development as closely as possible while providing a collision-free statefor 
subsequent animation and simulation. 2 Taking advantage of the dynamics pipeline Avianfeathersare layered 
uponeach otherwithsomedegreeof or­der and overlap. Our system allows the digital artist to concentrate 
on theoverallgrooming of thefeathers, theirlayered look,and the silhouetteofthecharacterwithout manuallyadjusting 
andpainstak­ingly removingpenetrations. The artist startsbyplacing a curve on theskinofthecharacterand 
adjustingthepathofthecurve torepre­senttheshapeofthefeatheralong itsshaft.Rectangulargeometry is instanced 
along this shaft with several artist controlled parame­ters like width, twist, and hang. Hang refers 
to how closely the instanced geometry follows the curve as it relates to the surface normal at the point 
of origin. The rectangular strips achieve their feathered look with various textures at render time. 
Our method uses two sets of instanced feathers. The .rst is a tar­geted set of feathers that despite 
many interpenetrations with other feathers achieves an artistic standard(seeFigure2a). The second sethas 
identicalfeatherroots,but thefeathersare instanced asnar­rower strips with less hang, that is, a shape 
that is more normal to the surface(seeFigure2b). This second set of almostporcupine­like feathers serves 
as an initial, penetration free state for a cloth sim. This is a departure from other, more complicated 
methods [Weber and Gornowicz 2009]. Our next step is to rig the second * e-mail: sbowline@ilm.com e-mail: 
zoran@ilm.com (a) Withpenetrations (b) Start of sim Figure 2: Targeted look and initial state for simulation. 
 set offeathersfor a cloth simulation with no activebodyforces. The base of each cloth strip is constrained 
to the surface and the strips con.guredtocollideagainst each other.The .rst set offeathers, the target 
look, is established as a vertex-by-vertex force-based posi­tion constraint on the cloth solve. Under 
simulation, the cloth strips willbeguided towards their targetpositions in the .rstset,but the collision 
solver will prevent interpenetrations while preserving the artist s work as closely aspossible. A variety 
ofdifferent schemes can be used for the collision avoidance, such as triangle/triangle or edge/edgebased 
repulsions. This method serves as a crucial .rst step towards penetration free character animation of 
feathered creatures. To be free of penetra­tionswhilethecharacterbodyisunderanimation, thefeatherscould 
undergo additional simulation passes. This allows realistic sliding andfrictioneffectsofdifferent layersoffeathersagainst 
each other. Or, if the feather instancer is temporally aware, the feathers could begeneratedpenetration-free 
in theshaderusingthispre-computed rest pose as a starting point. This technique is highly scriptable, 
can become part of a feathered character setup, and saves scores of artist hours shaping and adjusting 
feathers to avoid penetrations everytime the character model is altered or updated. Our simple solution 
integrates well with a character pipeline that alreadyinvolvessimulationandprovidesfeathers that arefreefrom 
intersection with complicated or concave body geometry, props, and clothing. References WEBER, A. J., 
AND GORNOWICZ, G. 2009. Collision-free construction of animated feathers using implicit constraint surfaces. 
ACM Transaction on Graphics 28,2 (April). Copyright is held by the author / owner(s). SIGGRAPH 2011, 
Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037873</article_id>
		<sort_key>470</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>36</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Accurate contact resolution for interpolated hairs]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037873</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037873</url>
		<abstract>
			<par><![CDATA[<p>The motion of hair is generally smooth---nearby hair strands move in a similar manner. This allows to compute the motion on a subset of the hairs which is often referred to as the <i>guide hairs</i>. The motion of the rest of the hairs is derived through interpolation, these hairs are referred to as <i>interpolated hairs</i>.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[collision detection]]></kw>
			<kw><![CDATA[hair rendering]]></kw>
			<kw><![CDATA[hair simulation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809707</person_id>
				<author_profile_id><![CDATA[81100201570]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goldenthal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light & Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rgoldenthal@ilm.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1276438</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Goldenthal, R., Harmon, D., Fattal, R., Bercovier, M., and Grinspun, E. 2007. Efficient simulation of inextensible cloth. <i>ACM Transactions on Graphics 26</i>, 3 (July), 49:1--49:7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1667252</ref_obj_id>
				<ref_obj_pid>1667239</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Verma, V., Schneider, P., Mitchell, J., Castano, I., Peters, J., and Ni, T. 2009. Efficient substitutes for subdivision surfaces. In <i>ACM SIGGRAPH Class Notes, August, 2009</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Accurate Contact Resolution for Interpolated Hairs * RonyGoldenthal IndustrialLight&#38;Magic  Figure 
1: InRango,manyfurrycharacterswearclothes.Wedeveloped anaccuratecontacthandling method that .xespenetrationsbetween 
hairandCatmull-Clark surfaces,inorder topreventfurfrompenetratingthrough clothing. Keywords: CollisionDetection,HairSimulation,HairRendering 
1 Introduction Themotionofhairisgenerallysmooth nearbyhairstrandsmove in a similar manner. This allows 
to compute the motion on a subset ofthehairswhichisoftenreferredtoas the guidehairs. The motion of the 
rest of the hairs is derived through interpolation, these hairs are referred to as interpolatedhairs. 
Contact resolution is one of the most expensive stages of hair sim­ulation,hence,restricting contact 
resolution to theguidehairspro­vides a substantial performance gain. In some cases, the visual re­sult 
isacceptable,and wecanget away withoutperforming contact resolution for the interpolated hairs. Unfortunately, 
this is not al­ways the case. In Rango, multiple characters are wearing clothes on top of their furrybodies(Fig. 
1). Having evenasinglehair pop through the clothing is visually distracting and cannot be permitted. 
Simulat­ing theentiresetofhairsjust to.xpenetrationsisnotpracticalfor several reasons. First, it is extremely 
expensive, the number of in­terpolatedhairsisoftenathousand timeslargerthanthenumberof guide hairs. Second, 
it works against the pipeline, during the sim­ulation phase the interpolated hairs do not exists yet. 
And .nally, collisiondetectionduringphysical simulation is oftendone using an approximated polygonal 
geometry, and not the rendered Catmull­Clark(CC) representation;Evensmalldifferencesbetween thesur­facerepresentationsmightlead 
tomissingpenetrations. In this talk we present a new approach for .xing penetrations of interpolated 
hairs into skin, clothing and other objects. The main challenges are accuracy, ef.ciency, and temporal 
coherence. The main contribution are performing collision detection against the smooth surface representation,performing 
continuous collisionde­tection along the hair s growth direction, and and resolving colli­sions on eachframe 
separately. The resulting methodology was used extensively in Rango, it was ef.cient enough so render 
time was not increased substantially, and it was accurate enough so that no post-.xing was required due 
to missedpenetrations. * e-mail: rgoldenthal@ilm.com  2 Overview Surface representation We use the methoddescribed 
in[Verma et al. 2009] to convert the CC surfaces to Bezier representation. This allows for explicit surface 
representation amenable to treat­mentby accurate intersectionqueries. Collision detection We use a hierarchical 
data structure to obtain a list of hair vs. Bezier patch candidates which we test for col­lisions. Since 
the current pair might be penetrating, we start from theroot of thehairand grow thehairalong itsparametricspace 
to­wards the tip. We search for continuous hair-surface intersections along theparametricshapeof thecurve. 
WeuseNewton smethod tocompute thehair-surface intersection. Shape preservation Fixing penetrations without 
shape preserva­tion leads to severe visual artifacts. We take a simple approach to preserve shape that 
replaces the need to employ a full hair simu­lation. After .xing the penetrations we apply the fast-projection 
methoddescribedin[Goldenthal etal.2007] tocorrectlengthvi­olations. We iterate between contact resolution 
and applying inex­tensibilityuntilbotharesatis.ed.Formost cases,few iterationsare required. Parallel 
evaluation and temporal coherence The interpolated hairsaregeneratedprocedurallyduringrender timeand 
we.xpen­etrations during this phase. Therefore it is critical to resolve the penetrations for each frame 
independently. This restriction has one potential drawback, we might get temporal incoherence arti­facts: 
hair deformation might be non-smooth between consecutive frames.We try reducethisartifactbybiasingtheresult 
towardsthe direction in which the hair grows. While not perfect1, it was good enough to not necessitate 
a serialized alternative. References GOLDENTHAL, R., HARMON, D., FATTAL, R., BERCOVIER, M., AND GRINSPUN, 
E.2007.Ef.cientsimulationofinextensible cloth. ACMTransactions onGraphics 26,3 (July),49:1 49:7. VERMA, 
V., SCHNEIDER, P., MITCHELL, J., CASTANO, I., PETERS, J., AND NI, T. 2009. Ef.cient substitutes for subdivision 
surfaces. In ACMSIGGRAPHClass Notes,August,2009. 1Thisheuristicis notuseful when thehairis straight 
anditpenetrates the surface at a right angle. Copyright is held by the author / owner(s). SIGGRAPH 2011, 
Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037874</section_id>
		<sort_key>480</sort_key>
		<section_seq_no>12</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Mixed grill]]></section_title>
		<section_page_from>12</section_page_from>
	<article_rec>
		<article_id>2037875</article_id>
		<sort_key>490</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>37</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Animation workflow in KILLZONE3#8482;]]></title>
		<subtitle><![CDATA[a fast facial retargeting system for game characters]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037875</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037875</url>
		<abstract>
			<par><![CDATA[<p>Applying facial motion capture data on a virtual actor is always a tedious task. The main problem is how to deal with different face conformations without losing too much of the original dynamic performance range. With this talk, I would like to explain a technique that helped us deliver more than 30 different faces over 70 minutes of Cutscenes for the AAA PS3 exclusive title KILLZONE3#8482;.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809708</person_id>
				<author_profile_id><![CDATA[81320487648]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrea]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Arghinenti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Guerrilla Games]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[andrea.arghinenti@guerrilla-games.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[G. Faigin, <i>The Artist's Complete Guide to Facial Expressions</i>; F. Parke, K. Waters, <i>Computer Facial Animation</i>; A. Menache, <i>Understanding Motion Capture</i>; I. S. Pandzic, R. Forchheimer, <i>MPEG-4 Facial Animation</i>; P. Ekman, W. Friesen, <i>Facial Action Coding System</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Animation Workflow in KILLZONE3 : A fast facial retargeting system for game characters Andrea Arghinenti 
Guerrilla Games  Figure 1: KILLZONE3 &#38;#169; Guerrilla Games - Sony Computer Entertainment Europe 
 Introduction Applying facial motion capture data on a virtual actor is always a tedious task. The main 
problem is how to deal with different face conformations without losing too much of the original dynamic 
performance range. With this talk, I would like to explain a technique that helped us deliver more than 
30 different faces over 70 minutes of Cutscenes for the AAA PS3 exclusive title KILLZONE3 . Approach 
A direct mapping approach has been wrapped into a workflow and into a very motion capture-friendly facial 
rig. We start from a grid-like pattern of markers placed over the actor's face in order to capture the 
surface as a whole, rather than as individual facial features. The data, after having been tracked and 
stabilized, is loaded directly onto the rig. A calibration process then takes place, which consists of 
measuring and storing on the rig itself all distance information between the markers and the virtual 
correspondent locators placed on our CG face. The result is an offset mapping, driving the mesh directly. 
The process is not destructive, and every marker's influence can be easily scaled down to 0 (or set to 
a negative value). On top of this, a muscle system, an expression layer, and a series of phonemes clips 
can be used to fine tune and/or characterize the original performance. This is an additive system of 
which every component adds itself on top of the others. Muscles, expressions and phonemes are poses stored 
in the rig. Every pose can be also a mini-clip triggered by an animatable attribute. It is possible to 
tweak every single pose per character, and load each one separately from the rig creation process. Poses 
are connected using a matrix mapping approach of the type: Mk = OMk +. m PKl . Vl l =1 where Mk is the 
final position of the retargeted marker k plus the post-animation; OMk is the offset distance calculated 
by the calibration process; l is the total amount of poses; PK is the k component of the stored pose; 
and V is its coefficient. The muscle system has been generated from motion capture data as well. Starting 
from a range of motion, positional information has been extracted for individual areas of the face. The 
muscle poses follow the FACS convention system. Conclusions A direct mapping approach with a proper 
post­animation layer and a fast and non-destructive calibration process gave us very solid and consistent 
results. Each performance as a whole was properly mapped onto all our heads, resulting in right mood 
changes and fidelity of motion. As a result the overall expressivity of our characters improved dramatically 
from the previous game.  References G. FAIGIN, The Artist's Complete Guide to Facial Expressions; F. 
PARKE, K. WATERS, Computer Facial Animation; A. MENACHE, Understanding Motion Capture; I. S. PANDZIC, 
R. FORCHHEIMER, MPEG-4 Facial Animation; P. EKMAN, W. FRIESEN, Facial Action Coding System. e-mail: 
andrea.arghinenti@guerrilla-games.com Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, 
British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037876</article_id>
		<sort_key>500</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>38</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Adaptive importance sampling for multi-ray gathering]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037876</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037876</url>
		<abstract>
			<par><![CDATA[<p>We present an adaptive noise reduction technique for integrating incident radiance at a fixed position. We use a Russian-roulette-based importance sampler to reshape the directional probability density of future rays in a batch, based on an <i>affinity map</i> that incorporates ratings of evaluated rays, provided by the rendering engine. Our method is unbiased, has low overhead, requires no precomputation, and works in concert with other importance sampling schemes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809709</person_id>
				<author_profile_id><![CDATA[81100390217]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ivan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neulander]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1854996</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Pharr, M., and Humphreys, G. 2010. <i>Physically Based Rendering, Second Edition: From Theory To Implementation</i>, 2nd ed. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>927297</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Veach, E. 1998. <i>Robust monte carlo methods for light transport simulation</i>. PhD thesis, Stanford, CA, USA. AAI9837162.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Adaptive Importance Sampling for Multi-Ray Gathering Ivan Neulander Rhythm&#38;Hues Studios We present 
an adaptive noise reduction technique for integrating incident radiance at a .xed position. We use a 
Russian-roulette-based importance sampler to reshape the directional probability density of futureraysinabatch, 
basedonan af.nity map that incorporates ratings of evaluated rays, provided by the rendering engine. 
Our method is unbiased, has low overhead, requires no precomputation, and works in concert with other 
importance sampling schemes. Figure 1: Single-bounce diffuse re.ection of mapped environment, with traditional 
importance sampling (left) and adaptive importance sampling (right). Ray counts differbut render times 
match. Motivation Our original motivation was to reduce shadow noise on surfaces dif­fusely lit by in.nite 
area lights. We use the PH importance sam­pler [Pharr and Humphreys 2010], which selects bright pixels 
from a latitude-longitude environment map in logarithmic time (based on tex­turewidth)usingapairofexplicitcdfs. 
Whileveryfast,thissampler is unaware of intervening geometry so it may repeatedly send rays into dark 
surfaces or outside the BSDF lobe, producing noise in occluded regions. Blending the PH sampler with 
a BSDF-based sampler using multiple importance sampling (MIS)[Veach 1998]failsto adequately solve this 
problem. We alternatively considered adjusting the PH cdfs adaptively basedonevaluated rays.However,the 
costof these updates becomes prohibitive beyond low texture resolutions. We ultimately opted for a rejection-based 
approach, drawing extra samples and discarding some of them to achieve the desired ray count with a modi.ed 
directional density. This method is general enough to work with our existing importance samplers, including 
MIS, allowing the BSDF, lighting, and occlusion to all guide ray selection. Implementation Our technique 
involves the stochastic rejection of rays based on an af.nity map, described below. In addition, we may 
unconditionally reject rays thatfall outside the BSDF support. Toavoid introducing bias, we reject samples 
using Russian roulette, as described by [Veach 1998]. This can cause extra zero-contribution rays to 
be generated, exceeding the user-requested ray count. The sampler reports this zero counttothe renderer,allowingitto 
incrementtheeffectiveray totalby which the summed ray color is ultimately divided. Stochastic rejection 
causes the total number of samples drawn per gather batch to .uctuate, precluding the use of low-discrepancy 
se­quences that requirea .xed sample count for proper strati.cation.We have found the multidimensional 
Halton sequence to work reasonably well for this and other forms of adaptive sampling, despite its higher 
discrepancycompared to .xed-size sequences. The A.nity Map Foreachbatchofgatherrayswe constructandqueryamulti-resolution 
latitude-longitude mapping to represent directional af.nity for future rays.We de.neaf.nity(.,f ) as 
the conditional probability of accept­ing a ray in the direction (.,f ) given its selection by the sampler. 
The af.nity map is continually updated based on evaluated rays, which are rated by the renderer. This 
rating compares a ray s actual incident radiance with the sampler s estimate of it (e.g. assuming unoccluded 
area lights). A mip-map pyramid of 3-channel textures comprise the af.nity map. The .rst two channels 
store total weighted af.nity and total weight, while the third storesa counter thatfacilitates rapidly 
clearing the af.nity map for the next ray batch. Ateachbatchpixelsare clearedtoanaverage weightedaf.nityof1, 
requiring subsequent lowratings to effect future rejections. This initial biastoward1preventslow-con.denceaf.nityvalues 
derived fromlow sample counts from causing improper sample rejections. Each time a ray is rated with 
a value r = 0, all textures in the af.nity map are modi.ed at that pixel as follows: w = 1/af.nity(. 
,f) map(. ,f)[0] += rw map(. ,f)[1] += w (1) To compute af.nity(.,f ) for a given ray, we point-sample 
all resolutions of the af.nity map at (.,f), starting with the high­est. The .rst nonempty pixel determines 
the tentative result at = map(. ,f)[0]/map(.,f )[1]. We account for pixel size by rescaling the tentative 
result to amin + at(1- amin) as the .nal af.nity, where amin = 1/(tol· height2 ) for user tolerance tol. 
This prevents large map pixels from causing signi.cant rejection, despite their potentially low average 
af.nity. (Our formula for amin is not based on a pixel s solid angle because polar pixels subtend large 
latitude angles despite their low solid angles, so they are no safer to use than equatorial pixels.) 
 Figure 2: Ashadow cast from an environment map by a dark sphere, renderedwith64,128,256,512 rays/pixel.Theuppersubimagesshow 
theaf.nitymap (left)andray samples (right)fora single chosenpixel. Red dots mark occluded ray directions, 
green dots unoccluded. Future Work Our methodworks best whena large numberofgather rays are sam­pled 
from the same point. Reusing an existing af.nity map at subse­quent nearbygather batches (with some distance-based 
penalty) could permit more aggressive rejection early on. Also, the use of pixel .l­tering and less-distorting 
texture parametrizations for the af.nity map should allow for better rejection decisions. References 
PHARR, M., AND HUMPHREYS, G. 2010. Physically Based Ren­dering, Second Edition: From TheoryTo Implementation, 
2nd ed. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA. VEACH, E. 1998. Robust monte carlo methods 
for light transport simulation. PhD thesis, Stanford, CA, USA. AAI9837162. Copyright is held by the author 
/ owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037877</article_id>
		<sort_key>510</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>39</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[High-resolution relightable buildings from photographs]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037877</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037877</url>
		<abstract>
			<par><![CDATA[<p>We present a quasi-automatic image-based building reconstruction system that recovers fully relightable models by approximating both albedo and sufficient textured surface detail to reproduce complex self-shadowing effects. Albedo and surface geometric detail are recovered though an exemplar-based transfer approach. We focus on a simple data capture, inexpensive equipment and automatic processes. This system provides perceptually high-quality models approximating well the visual appearance of the relit building.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809710</person_id>
				<author_profile_id><![CDATA[81365597937]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Francho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Melendez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Manchester]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809711</person_id>
				<author_profile_id><![CDATA[81490660922]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mashhuda]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Glencross]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Manchester]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809712</person_id>
				<author_profile_id><![CDATA[81100633003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gregory]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Ward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dolby Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809713</person_id>
				<author_profile_id><![CDATA[81100561115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Roger]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Hubbold]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Manchester]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1360658</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Glencross, M., Ward, G. J., Jay, C., Liu, J., Melendez, F., and Hubbold, R. 2008. A perceptually validated model for surface depth hallucination. <i>ACM SIGGRAPH 27</i>, 3, 59:1--59:8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Melendez, F., Glencross, M., Ward, G. J., and Hubbold, R. J. 2011. Relightable Buildings from Images. <i>EG 2011 - Areas Papers</i>, 33--40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 High-Resolution Relightable Buildings from Photographs Francho Melendez Mashhuda Glencross Gregory 
J. Ward Roger J. Hubbold The University of Manchester The University of Manchester Dolby Canada The University 
of Manchester  Figure 1: (Left) Input Data: 10 of the 40 photographs taken for the wide-baseline sequence, 
and Material Exemplars. (Middle and Right) Model rendered under novel view-points and lighting conditions. 
1 Introduction We present a quasi-automatic image-based building reconstruction system that recovers 
fully relightable models by approximating both albedo and suf.cient textured surface detail to reproduce 
com­plex self-shadowing effects. Albedo and surface geometric detail are recovered though an exemplar-based 
transfer approach. We fo­cus on a simple data capture, inexpensive equipment and automatic processes. 
This system provides perceptually high-quality models approximating well the visual appearance of the 
relit building. 2 System Structure Employing a standard uncalibrated digital SLR camera, we capture 
two types of image data under diffuse lighting conditions as input to our model recovery pipeline; a 
hand-held wide-baseline sequence, and textured material exemplar image data consisting of a .ash and 
a no-.ash fronto-parallel view of a representative material. The wide-base line sequence is used to recover 
a low-resolution model that recovers the global structure of the building and to reconstruct a texture 
map. Exemplars capture the material properties in acces­sible areas. We transfer the albedo and high-frequency 
geometric detail from these exemplars to the full model. The system pipeline is divided into the following 
steps: Gross-scale geometry: We use automatic Multi-view Stereo techniques to recover the global structure 
using the wide­baseline sequence.  Multi-view texture mosaicing: An automatic texture recon­struction 
algorithm uses Markov Random Fields to optimally combine the multi-view data into a combined texture 
map.  Material Exemplars: Flash/No-.ash image pairs are used as samples of the different materials present 
in the building fac¸ade. Surface depth hallucination [Glencross et al. 2008] estimates albedo and surface 
detail for these exemplars.  Albedo and Surface Detail Transfer: We transfer surface detail and albedo 
from the exemplars to the texture mo­saic using Histogram Matching resulting in a per-texel depth map 
[Melendez et al. 2011].  Geometry Fusion: A frequency-based method is used to combine both geometries 
resulting in a high-resolution model.  The process is automatic apart from simple cleaning of the gross 
scale model. This consists of removing excess geometry, and a user guided segmentation process of the 
texture map to assign exemplars with different regions in the model. 3 Exemplar Based Transfer Approach 
The main contribution of this work is the idea of approximating the material properties and surface detail 
of a model by transfer­ring them from a series of exemplars. This can be potentially in­cluded in any 
reconstruction pipeline to add high-frequency detail and albedo, transferred from exemplars to the recovered 
texture. 4 Results and Conclusion Figure 2 shows the plausible appearance recovered with our system 
by comparing side-by-side a photograph 1 with a rendering of the model under approximately matched lighting 
conditions. Figure 2: (Left) Photograph, (Right) Synthetic Rendering. This system provides a low-cost 
way to acquire highly detailed re­lightable models though a simple capture process and with modest user 
interaction, appropriate for a range of visualization and enter­tainment applications. These models represent 
a great improvement over the traditional low resolution plus texture models, and allows us to reproduce 
self-shadowing effects which have been shown to be important to provide perceptual plausibility. References 
GLENCROSS, M., WARD, G. J., JAY, C., LIU, J., MELENDEZ, F., AND HUBBOLD, R. 2008. A perceptually validated 
model for surface depth hallucination. ACM SIGGRAPH 27, 3, 59:1 59:8. MELENDEZ, F., GLENCROSS, M., WARD, 
G. J., AND HUBBOLD, R. J. 2011. Relightable Buildings from Images. EG 2011 -Areas Papers, 33 40. 1http://www.nicolaconforto.com/erasmus/img/york20080927 
 Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 
7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037878</section_id>
		<sort_key>520</sort_key>
		<section_seq_no>13</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Directing destruction]]></section_title>
		<section_page_from>13</section_page_from>
	<article_rec>
		<article_id>2037879</article_id>
		<sort_key>530</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>40</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Kali]]></title>
		<subtitle><![CDATA[high quality FEM destruction in Zack Snyder's Sucker Punch]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037879</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037879</url>
		<abstract>
			<par><![CDATA[<p>MPC introduces Kali, a finite element based simulation toolkit for large scale and detailed destruction, developed in collaboration with Pixelux. The toolkit is described with examples from Zack Snyder's <i>Sucker Punch</i>.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809714</person_id>
				<author_profile_id><![CDATA[81488653119]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ben]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cole]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ben-c@moving-picture.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1837066</ref_obj_id>
				<ref_obj_pid>1837026</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Meeres-Young, G., Ricklefs, H., and Tovell, R. 2010. Managing thousands of assets for the prince of persia city of alamut. In <i>ACM SIGGRAPH 2010 Talks, ACM</i>, New York, NY, USA, SIGGRAPH '10, 30:1--30:1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1599492</ref_obj_id>
				<ref_obj_pid>1599470</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Parker, E. G., and O'Brien, J. F. 2009. Real-time deformation and fracture in a game environment. In <i>Proceedings of the 2009 ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, ACM, New York, NY, USA, SCA '09, 165--175.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Kali: High Quality FEM Destruction in Zack Snyder s Sucker Punch Ben Cole* MPC Abstract MPC introduces 
Kali, a .nite element based simulation toolkit for large scale and detailed destruction, developed in 
collaboration with Pixelux. The toolkitis described withexamples from ZackSnyder s Sucker Punch. 1 Introduction 
Zack Snyder s Sucker Punch required us to deliver 50 large scale fully CG destruction shots, featuring 
close-up, slow-motion de­structionofbrick, stone,woodand metal.ConventionalVFX toolk­its use Rigid Bodies 
for modelling and simulation of destruction. These tools make it hard to simulate .exing, tearing and 
breaking ina realistic and controllableway.With Kali, wehave replaced the Rigid Body solver with a Finite 
Element solver. We worked with the development team from Pixelux to take their DMMtechnology, created 
for realtime videogame applications[Parker and O Brien 2009],and reapplyitto visualeffects. Modi.cations 
were required tothesoftwareandanewpipelinewas establishedtodealwithVFX production demands. Figure 1: 
Ground destruction in Sucker Punch. 2 Usage Kali uses a tetrahedral .nite element representation of 
geometry. We have chosen to use a cage based approach, separating the ren­dered geometry from geometry 
used for simulation. This allows artists to de.ne simulations as theywish, adding fracture detail and 
altering the shapes of collision objects independently of the ren­dered geometry. Simulation detail can 
be changed shot to shot, with more tetrahedra in areas where attention will be focused. Asingle tetrahedral 
mesh can map to many pieces of render geometry, or vice versa. Render geometry is chopped at render-time 
against the last frame of simulation, so changes to the model simply require a re-chop, not a re-sim 
or re-cache. Likewise, changes to the simulation do not require remodelling. This leads to ef.ciencygains 
as there is far less cyclic inter-dependency between our modelling, lighting and simulation teams. This 
approach is general and will work with simulations and animations created with other tools, too. Kali 
has an event system that allows TDs to add features to the system as needed. We ve created a library 
of standard events, e.g., *e-mail: ben-c@moving-picture.com for pinning, driving, and controlling material 
toughness. Every tet­mesh has one or more materials assigned to it, controlling .exiblity, brittleness, 
resilience, etc. Awide range of material types can be de.ned. Material properties can be changed during 
simulation us­ing the event system. In general, simulations are run on thefarmin batches withvaried parameter 
ranges. More than one cache can be used with a single renderable asset, allowing different parts to be 
simulated separately, which is useful for larger environments, or for shot continuity. On Sucker Punch 
we simulated the destruction of the interior and exterior of a Japanese style pagoda. The pagoda exterior 
had ap­proximately 10.7 millionfaces. The interiorwas approximately11 million. We simulated destruction 
of the roof, .oor, walls, pillars, doors and furnishings. Individual pillars were simulated with up to 
a million tetrahedra, which were then used to chop the renderable geometry. Chopping took between1minute 
and1hour. Simulation times were1or2hoursforaverage setupsto about20 hoursforthe most complex simulations. 
There is a simple system for editing caches after simulation, that allows vertices or regions to be moved 
around to tidy up simula­tions or add speci.c behaviour. Caches can also be used as sources for particle 
emission. The system knows how and when speci.c re­gions broke apart, which is useful for calculating 
realistic emission for secondary simulations. 3 Modelling and Rendering The modeling department does 
not need to know how the model willbeusedin simulation,aslongastheyfollowasetofbasicrules. Objects mustbe 
closedandfaces musthavearea. These restrictions do not dramatically limit geometry that can be used with 
the system. For rendering,Kali simulation assets can literallybe dropped-inin place of standard geometry 
assets and rendered using the same set ups [Meeres-Young et al. 2010]. All that is needed in addition 
is a shader for newly created interiorfaces. 4 Summary The work we undertook on Sucker Punch using our 
Kali system demonstrates that a .nite element approach to large scale and de­tailed destructionworkseffectivelyinaproduction 
pipeline. Perfor­mance is good and results are visually interesting and high-quality. The system has 
also been used on several other VFX projects, in­cluding Harry Potter and the Deathly Hallows (Part Two) 
and X-Men: First Class.  References MEERES-YOUNG, G., RICKLEFS, H., AND TOVELL, R. 2010. Managing thousands 
of assets for the prince of persia city of alamut. In ACM SIGGRAPH 2010 Talks,ACM, NewYork, NY, USA, 
SIGGRAPH 10, 30:1 30:1. PARKER,E.G., AND O BRIEN,J.F. 2009. Real-time deforma­tionand fractureinagameenvironment. 
In Proceedings of the 2009 ACM SIGGRAPH/Eurographics Symposium on Computer Animation,ACM,NewYork,NY, 
USA, SCA 09, 165 175. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, 
Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037880</article_id>
		<sort_key>540</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>41</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Directing hair motion on <i>Tangled</i>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037880</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037880</url>
		<abstract>
			<par><![CDATA[<p>State-of-the-art simulation techniques can produce compelling and natural hair motion. In creating Disney's feature film <i>Tangled</i>, physically plausible motion was important but just one foundational component. The story contains an unprecedented amount of interaction of the hair with the characters, as well as a high level of art-direction. Through 2D "drawovers" the artistic vision was conveyed in an often very detailed way - how the hair should move, and what poses and silhouettes it should hit. Simulation alone is not sufficient when such a high degree of direction and interaction is required. We describe a hybrid approach leveraging the power of custom hair dynamics with the artistic control of key-framed animation that was key to the success of directing hair motion on <i>Tangled</i>.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809715</person_id>
				<author_profile_id><![CDATA[81341496532]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Maryann]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Simmons]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809716</person_id>
				<author_profile_id><![CDATA[81450592780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kelly]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809717</person_id>
				<author_profile_id><![CDATA[81466648668]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hidetaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yosumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809718</person_id>
				<author_profile_id><![CDATA[81466643671]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hubert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Leo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809719</person_id>
				<author_profile_id><![CDATA[81466644111]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Xinmin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Directing Hair Motion on Tangled Maryann Simmons Kelly Ward Hidetaka Yosumi Hubert Leo Xinmin Zhao Walt 
Disney Animation Studios  (a) 2D Drawover. (b) Posing with hair rig. 1 Introduction State-of-the-art 
simulation techniques can produce compelling and natural hair motion. In creating Disney s feature .lm 
Tangled, physically plausible motion was important but just one foundational component. The story contains 
an unprecedented amount of inter­action of the hair with the characters, as well as a high level of art-direction. 
Through 2D drawovers the artistic vision was con­veyed in an often very detailed way -how the hair should 
move, and what poses and silhouettes it should hit. Simulation alone is not suf.cient when such a high 
degree of direction and interaction is required. We describe a hybrid approach leveraging the power of 
custom hair dynamics with the artistic control of key-framed anima­tion that was key to the success of 
directing hair motion on Tangled.  2 Approaches Animation rigs give direct control over the placement 
of the hair and are most suitable to use when motion is not physically-based and/or dif.cult or impossible 
to hit through simulation. This ap­proach supports fast iterations, but does not scale well due to the 
amount of manual input required, making it only feasible for use on a small number of representative 
curves. In addition, through rig controls alone, it is very time-consuming to produce animation that 
faithfully preserves physical properties of the hair necessary to produce natural, collision-free motion. 
In contrast, simulation can handle independent and coherent motion of thousands of curves, producing 
natural and highly complex motion. Such dynamics, however only provide implicit control and therefore 
are dif.cult to direct. In addition it is a computationally expensive approach-lim­iting the number of 
iterations practical. Thus we introduce a system of interleaving the best of both approaches on a shot-by-shot 
basis. Animation Approach: Rig-based key-framed animation is used to produce hair mo­tion in shots calling 
for non-physically-based motion and/or a high level of art­direction. The specialized hair rig contains 
controls for moving the hair as a whole as well as ten break-up controls that allow sub-groups of hair 
to be pulled out and manipulated indepen­dently from the remaining hair volume. The rig contains IK, 
FK, twist, and pinch operations supported at the global or on the sub­group level; the number and distribution 
of these operation con­trollers can be altered per shot as necessary. This two-tiered hair rig is used 
for both posing the initial positions and for creating key­framed animation of the hair. Simulation Approach: 
At the other end of the spectrum are simulation-driven shots where physics primarily controls the mo­tion. 
By animating external forces (constraints, collision objects, (c) Sim for settling/collision. (d) Final 
render. and force .elds), the hair motion can be in.uenced to a desired out­come. Finer level control 
is provided through the manipulation of simulation properties via override sets and curve maps. These 
maps provide localized, per-vertex control over any constraint or property in the simulation, e.g. mass, 
stiffness, etc., as well as a mechanism to locally animate these properties within one simulation pass. 
 3 Interleaving Animation and Simulation Hair shots are .rst categorized as passive, animation-or simulation­driven. 
Passive shots do not require any art direction or special interac­tion. These shots are .rst batch processed 
using a default simulation setup. The results of this pass are evaluated and the accepted pas­sive shots 
are sent to the downstream department. The remaining shots are divided into two sections. For those that 
share a common characteristic (e.g. wet hair), a special case simulation setup is cre­ated and applied 
to each shot in that group through another round of batch processing. Remaining ungrouped shots are re-classi.ed 
as animation-or simulation-driven and passed on for shot-speci.c processing. This batch processing iterates 
until all shots in the ini­tial grouping are approved or re-classi.ed. The majority of non-passive highly-directed 
shots involve multi­ple interleaved iterations of animation and simulation, though one generally dominates. 
For example, shots where the hair undergoes extreme action as a whole are animation-driven, whereas if 
the hair motion is organic, but still needs to hit a speci.c silhouette, the shot is simulation-driven. 
Different portions of the hair may be classi.ed and processed sepa­rately and the solutions blended, 
or combined using simulation tar­geting. Rig-animated shots undergo a simulation iteration by cre­ating 
targeting forces to pull the simulated curves towards the ani­mated ones. This simulation layer enhances 
the motion by adding .ne-scale details dif.cult or impossible to achieve by hand. The process of creating 
the key frame rig-driven poses itself is layered with simulation: the rig drives the curves directly-an 
interactive interface driven by the simulation in turn can be used to drive the rig. In this way dynamics 
and explicit control are interleaved seam­lessly. For simulation-driven shots, the motion can be in.uenced 
post-simulation by direct manipulation of the rig or other deform­ers, which ride along with the simulated 
hairs; this allows high-level editing of the hair without re-running a time-consuming simulation. The 
presented classi.cation and interleaved animation/simulation work.ow made it possible to not only achieve 
the desired mo­tions with suf.cient quality, but also to do it ef.ciently enough to meet the practical 
reality of production scheduling a signi.cant achievement given the almost 500 art-directed hair shots. 
All images are c@Disney Enterprises, Inc. Copyright is held by the author / owner(s). SIGGRAPH 2011, 
Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037881</article_id>
		<sort_key>550</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>42</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Tangled choreographing destruction]]></title>
		<subtitle><![CDATA[art directing a dam break]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037881</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037881</url>
		<abstract>
			<par><![CDATA[<p>Much like the character animation in Tangled, the goal of the dam break sequence was to bring classic Disney 2D sensibilities to CG effects. Hand-drawn effects animation in films such as Pinocchio and Fantasia served as inspiration. The water shapes drawn in these films were very stylized yet conveyed recognizable forms of nature (see e.g. figure 2). The concept was to emulate these shapes and then enhance them with the modern benefits of CG rendering such as ray traced reflections and ambient occlusion. We developed processes that allowed the combination of physical simulations, articulate sculpting of form and specific manipulation of timing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809720</person_id>
				<author_profile_id><![CDATA[81488671538]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brett]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boggs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809721</person_id>
				<author_profile_id><![CDATA[81488666288]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Lawrence]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809722</person_id>
				<author_profile_id><![CDATA[81466648514]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaschalk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809723</person_id>
				<author_profile_id><![CDATA[81100351513]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Selle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Tangled Choreographing Destruction: Art Directing a Dam Break Brett Boggs Lawrence Chai Michael Kaschalk 
Andrew Selle Walt Disney Animation Studios 1 Introduction Much like the character animation in Tangled, 
the goal of the dam break sequence was to bring classic Disney 2D sensibilities to CG effects. Hand-drawn 
effects animation in .lms such as Pinocchio and Fantasia served as inspiration. The water shapes drawn 
in these .lms were very stylized yet conveyed recognizable forms of na­ture (see e.g. .gure 2). The concept 
was to emulate these shapes and then enhance them with the modern bene.ts of CG rendering such as ray 
traced re.ections and ambient occlusion. We developed processes that allowed the combination of physical 
simulations, ar­ticulate sculpting of form and speci.c manipulation of timing. 2 Fast Exploration The 
design process for this sequence began with a close collabo­ration with the layout team. An effects animator 
was tasked with providing animatics containing rough .uid simulations to establish timing and scale. 
Fast turnarounds of a few days per pass gave the directors the ability to play with the sequence freely. 
Early vi­sualization of potential problems, creative opportunities, and even additional shots were born 
out of this collaboration. One such ex­ample is the ride .lm style shot that gives the audience the feeling 
of plunging down the .ooded cavern. 3 Simulation and Sculpting A particle level set liquid simulator 
created initial shapes and par­ticle passes. Using the animatic as a base, high resolution simula­tions 
were run that matched the established timing (see .gure 1). Remapping the timing of the simulation gave 
the artist complete control over anticipation, energy, and scale. In some instances, sev­eral simulations 
were run separately and merged together before building the outer surface of the water so that very speci.c 
beats could be hit on exact frames. Shapes were then re.ned using Hou­dini s procedural deformation tools 
to enhance the form. This al­lowed the team to further art direct the simulation by sculpting the .uid 
into the desired shapes. Additional foam and spray particle elements were layered to add realism and 
stylization. Figure 2: An concept sketch of the director s desired water style (by Ted Kierscey). c 
&#38;#169;Disney. 4 Integration A major challenge was maintaining a sense of large scale while still 
stylizing the look. This required keeping the water .ow speed slow enough to feel large. On the rendering 
side, we found minimizing and localizing mirror re.ectivity was also necessary. Several layers of procedural 
spray and mist tied the base .uid shapes with the environment. We used exaggerated contrast shading on 
the foam levels to further create a classic look. 5 Conclusion Natural elements such as water in an 
animated feature bene.t greatly from applying the same twelve principles of animation that are consistently 
followed for character animation. As an animation studio, exaggeration and stylization of effects and 
environments are thus very important. This is especially dif.cult to accomplish while using physically 
based techniques such as .uid simulations. We be­lieve we have taken steps toward better stylization 
in CGI on Tan­gled, but plan to go further in the future.  Copyright is held by the author / owner(s). 
SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037882</section_id>
		<sort_key>560</sort_key>
		<section_seq_no>14</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Crowds]]></section_title>
		<section_page_from>14</section_page_from>
	<article_rec>
		<article_id>2037883</article_id>
		<sort_key>570</sort_key>
		<display_label>Article No.</display_label>
		<pages>2</pages>
		<display_no>43</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Crowds on Cars 2]]></title>
		<page_from>1</page_from>
		<page_to>2</page_to>
		<doi_number>10.1145/2037826.2037883</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037883</url>
		<abstract>
			<par><![CDATA[<p>We implemented a simple custom data format not unlike Maya's PDC particle data container. Specifically, we store per-frame data about root transforms, body rotations, wheel steering and roll angles, brake light intensity, and optionally a pointer to a desired animation clip to layer on top. A header stores a list of agents in each simulation file, their character variants, the superset of all animation clips requested in the simulation, etc.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809724</person_id>
				<author_profile_id><![CDATA[81488655364]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Trent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Crow]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809725</person_id>
				<author_profile_id><![CDATA[81100644162]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gustafson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809726</person_id>
				<author_profile_id><![CDATA[81488640792]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lorenzen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809727</person_id>
				<author_profile_id><![CDATA[81488668546]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jake]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Merrell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809728</person_id>
				<author_profile_id><![CDATA[81442604025]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Bob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moyer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[bmoyer@pixar.com]]></email_address>
			</au>
			<au>
				<person_id>P2809729</person_id>
				<author_profile_id><![CDATA[81466646830]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Northrup]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Crowds on Cars 2 Trent Crow Stephen Gustafson Michael Lorenzen Jake Merrell Bob Moyer* J.D. Northrup 
Pixar Animation Studios 1 Uni.ed Crowd Data Format We implemented a simple custom data format not unlike 
Maya s PDC particle data container. Speci.cally, we store per-frame data about root transforms, body 
rotations, wheel steering and roll an­gles, brake light intensity, and optionally a pointer to a desired 
ani­mation clip to layer on top. A header stores a list of agents in each simulation .le, their character 
variants, the superset of all animation clips requested in the simulation, etc. We integrated this format 
with Massive, Houdini, and Marionette our in-house animation system with a series of plug-ins and scripts. 
 Figure 1: Houdini (top) and Massive (bottom) editing the same lay­out data.  2 Constraint-Based Posing 
One of the more unusual aspects of our pipeline was that we struck a balance between pre-posing animation 
into caches versus storing live animation for the entire crowd. In traditional instancer-based crowds, 
all animation clips are cached into per-frame geometry, and then sequenced at appropriate locations for 
the crowds. By contrast, with dynamic pose engines (like Massive s standard rendering plug­ins), per-frame 
joint angles are simulated and then posed for each crowd character on each frame of the .nal shots. This 
can be useful for simulations needing IK goal modi.cations or pose-blending, but * e-mail: bmoyer@pixar.com 
 in the case of playing back discrete keyframe-animated clips it can be inef.cient. Our system stores 
animation data on a small pool of off-screen char­acters called actors (typically 10 to 60), which is 
accessed via a static, in-memory array of pose information by the larger crowd of agents . We have two 
modes for this: one where each of these actors represent a single animation clip, and another where each 
actor has an entire sequence of animation applied via a .nite state machine system. For the former case, 
simulated crowd agents pick which animation clip to play back for a given frame, and look up the pose 
information from the appropriate actor. This mode was use­ful for tightly-choreographed shots where agents 
chose their acting based on events in a simulation. In the second mode, crowd agents randomly pick a 
given actor to follow, and mimic their pose information, with a randomized frame offset. This was most 
useful for more ambient crowds, where we wanted to easily try out a variety of keep-alive animation without 
much speci.c choreography. 3 Agent Behavior We also implemented a collection of higher-order operators 
on top of Massive s fuzzy nodes, including signal decay functions to serve as memory , accumulator/decumulators 
for integrating accelera­tion signals into velocity signals, temporal hysteresis to help with commitment 
to actions, and .ip-.ops to implement simple stateful operations in the relatively stateless fuzzy environment. 
We also used a signal processing-based system developed during Wall-E to approximate damped harmonic 
motion. Taking a cue from gradient-domain image-processing techniques, we found blending control systems 
operating in the velocity and acceleration domains provided more robust set of behaviors than using either 
domain alone. We found it elegant that this algorithmic approach helps with motion hitches in much the 
same way as it allows for minimizing discontinuities in an image gradient. 4 Render Metrics Shrinkwraps 
signi.cantly increased our ability to render crowds. This effect was most noticeable for large crowds 
or long distances, since cost falls with pixel coverage of each model. For a test shot with a basic lighting 
setup: models rib gen (h:mm:ss) rib gen memory prman (h:mm:ss) prman memory (close) 200 real 0:29:39 
1.0 GB 1:45:22 1.9 GB (close) 200 shrinkwrap 0:00:31 0.15 GB 0:37:48 0.6 GB (long) 200 real 0:31:10 1.0 
GB 0:59:24 3.8 GB (long) 200 shrinkwrap 0:00:37 0.15 GB 0:00:51 61 MB Copyright is held by the author 
/ owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  5 Limitations The use of shrinkwraps was not ideal for models with concavi­ties or transparency. Side 
mirrors of many cars were the main ar­eas we noticed visible artifacts. Heavy displacement can also cre­ate 
render-time artifacts from lighting and texture .ltering, but for background characters we found these 
acceptable. We also avoided some forms of ray-tracing of crowds in particular, self-re.ection. Our displacement-shaded 
mouths weren t shaped the same as our full mouths because of the way we parameterized the front end of 
the various car types. Because we essentially collapsed the entire model hierarchy down to a box for 
the body, and four cylinders for the wheels, per-gprim lighting was not possible. In addition, as is 
the case with any type of LOD scheme, maintenance and preparation of our crowd mod­els entailed some 
overhead for the characters department. Finally, animation was limited to rigid transforms of the body 
and wheels, along with facial details. 6 Intended Audience This talk is targeted to anyone interested 
in implementing or using a production crowds pipeline, especially for .lm work.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037884</section_id>
		<sort_key>580</sort_key>
		<section_seq_no>15</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Show me the pixels]]></section_title>
		<section_page_from>15</section_page_from>
	<article_rec>
		<article_id>2037885</article_id>
		<sort_key>590</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>44</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Slow art with a trillion frames per second camera]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037885</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037885</url>
		<abstract>
			<par><![CDATA[<p>How will the world look with a one trillion frame per second camera? Although such a camera does not exist today, we converted high end research equipment to produce conventional movies at 0.5 trillion (5&#183; 10<sup>11</sup>) frames per second, with light moving barely 0.6 mm in each frame. Our camera has the game changing ability to capture objects moving at the speed of light. Inspired by the classic high speed photography art of Harold Edgerton [Kayafas and Edgerton 1987] we use this camera to capture movies of several scenes.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[computational photography]]></kw>
			<kw><![CDATA[time of flight camera]]></kw>
			<kw><![CDATA[video capture]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809730</person_id>
				<author_profile_id><![CDATA[81490676712]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andreas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Velten]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809731</person_id>
				<author_profile_id><![CDATA[81487645512]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Everett]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lawson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809732</person_id>
				<author_profile_id><![CDATA[81488669467]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bardagjy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809733</person_id>
				<author_profile_id><![CDATA[81488671861]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Moungi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bawendi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809734</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[raskar@media.mit.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CameraCulture, 2010. Femto photography: http://web.media.mit.edu/raskar/femto/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Goda, K., Tsia, K. K., and Jalali, B. 2009. Serial time-encoded amplified imaging for real-time observation of fast dynamic phenomena. <i>Nature 458</i>, 1145--1149.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kayafas, G., and Edgerton, H. 1987. <i>Stopping Time: The Photographs of Harold Edgerton</i>. Harry N Abrams.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Slow Art with a Trillion Frames Per Second Camera Andreas Velten1, Everett Lawson1, Andrew Bardagjy1, 
Moungi Bawendi2, Ramesh Raskar1* MIT Media Lab1, MIT Department of Chemistry2  Figure 1: A candle pulse 
being hit by a laser pulse. This sequence of images taken from the movie included in our submission reveals 
the beauty of processes happening at the speed of light. The light pulse enters the scene through a pinhole 
on the left and strikes the side of the candle. Details like wick of the candle are visible. In the attached 
movie you will also note light bouncing off several mounts in the scene such as the pinhole on the left 
and the white wall in the left behind the candle. Abstract How will the world look with a one trillion 
frame per second cam­era? Although such a camera does not exist today, we converted high end research 
equipment to produce conventional movies at 0.5 trillion (5· 1011) frames per second, with light moving 
barely 0.6 mm in each frame. Our camera has the game changing abil­ity to capture objects moving at the 
speed of light. Inspired by the classic high speed photography art of Harold Edgerton [Kayafas and Edgerton 
1987] we use this camera to capture movies of sev­eral scenes. Keywords: computational photography, time 
of .ight camera, video capture Introduction and Motivation Visualizing processes at ever higher speeds 
is of of high interest in scienti.c applications as well as in art and entertainment applications. Harold 
Edgerton dedicated a lifetime of work to this effort producing stunning and powerful works of art. In 
this tradition we extend our high speed streak cam­era and use it in an unconventional way to capture 
common two dimensional movies at extremely high speeds, visualizing a world unfamiliar to human intuition 
and visual perception and creating a new kind of art. While regular high speed video cameras can only 
capture at several thousand frames per second, recently a two di­mensional camera using a femtosecond 
laser capable of a frame rate of 6,100,000 frames per second has been demonstrated [Goda et al. 2009]. 
Time-of-.ight light .eld analysis offers a variety of new possibili­ties to computer graphics and computer 
vision. Among other things it becomes possible to analyze sub-surface scattering and to look around the 
corner [CameraCulture 2010]. At the heart of this ef­fort is an imaging system capable of illuminating 
and capturing at speeds that make the travel time of light from the source, through the scene and to 
the detector accessible. Here we present a modi­.cation of our setup, that produces two dimensional movies 
with a time resolution of about two picoseconds (500,000,000,000 frames per second). Technical Approach 
and Results Our setup consists of a Ti:Sapphire laser emitting 50 fs pulses at a repetition rate of 75 
MHz. The scene is placed on a stage that can be adjusted in height. A pinhole attached to the left side 
of this stage directs the laser beam onto the scene. The camera is a Hamamatsu C5680 streak camera that 
captures a horizontal line on the scene at a .xed *e-mail: raskar@media.mit.edu Figure 2: Top Left: 
Part of our setup. The laser beam enters from the bottom left, the object in this case is diluted milk, 
the camera objective is visible on the bottom left. Top Right: Frame of a movie of a tomato. Bottom: 
Movie frame of a sugar crystal. height with two picosecond time resolution. The images produced by the 
camera are two dimensional 512x672 pixel images, with one dimension corresponding to time and one to 
the captured line in space. To cover the second spacial dimension we move the scene and light source 
vertically. A complete movie is combined from a large number of single images. During the capture time 
of up to several hours the laser and camera have to be kept in calibration by reference signals built 
into our optical setup. All captured videos are attached to this submission. Single frames of selected 
videos are shown in Figures and the teaser images. While the main purpose of this work is to demonstrate 
the beauty of ultra-fast optical processes and provide an intuitive window into a world otherwise only 
seen through numbers and graphs, it also provides important insight in the possibilities of the emerging 
.eld of femto photography.  References CAMERACULTURE, 2010. Femto photography: http://web.media.mit.edu/ 
raskar/femto/. GODA, K., TSIA, K. K., AND JALALI, B. 2009. Serial time-encoded ampli.ed imaging for real-time 
observation of fast dynamic phenomena. Nature 458, 1145 1149. KAYAFAS, G., AND EDGERTON, H. 1987. Stopping 
Time: The Photographs of Harold Edgerton. Harry N Abrams. Copyright is held by the author / owner(s). 
SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037886</article_id>
		<sort_key>600</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>45</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Display pixel caching]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037886</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037886</url>
		<abstract>
			<par><![CDATA[<p>A variety of standard video modes that stretch or zoom lower resolution video content linearly to take full advantage of large screen sizes have been implemented in TV sets. When content and screen aspect ratios differ, format proportions may be compromised, video content may be clipped, or screen regions may remain unused. Newer techniques, such as video retargeting and video upsampling, rescale individual video frames and can potentially match them to the display resolution and aspect ratio. However, none of these methods can display simultaneously more than is contained in a single frame.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809735</person_id>
				<author_profile_id><![CDATA[81488672490]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Clemens]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Birklbauer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[clemens.birklbauer@jku.at]]></email_address>
			</au>
			<au>
				<person_id>P2809736</person_id>
				<author_profile_id><![CDATA[81488655525]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tianlun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tianlun.liu@jku.at]]></email_address>
			</au>
			<au>
				<person_id>P2809737</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[oliver.bimber@jku.at]]></email_address>
			</au>
			<au>
				<person_id>P2809738</person_id>
				<author_profile_id><![CDATA[81421597701]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Max]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grosse]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[max.grosse@uni-weimar.de]]></email_address>
			</au>
			<au>
				<person_id>P2809739</person_id>
				<author_profile_id><![CDATA[81100631719]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Anselm]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grundh&#246;fer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[anselm.grundhofer@uni-weimar.de]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Display Pixel Caching Clemens Birklbauer, Tianlun Liu, Oliver Bimber Max Grosse, Anselm Grundh¨ofer 
Johannes Kepler University Linz * Bauhaus-University Weimar Figure 1: Results of DPC for 4:3 content 
on a 16:9 screen after several frames of leftward camera motion while empty cache regions (left and right 
borders) are left blank (a-d). Result of DPC for a cinemascope format and downward camera motion while 
empty cache regions (bottom border) are pre-.lled with a smooth color transition (f). Result of DPC for 
a 16:9 format and forward camera motion (h). Frames a,e, and g are the original frames while frames b,c,d,f, 
and h were computed by DPC. The original frames are always unmodi.ed and embedded in the center of the 
DPC output. 1 Introduction and Motivation A variety of standard video modes that stretch or zoom lower 
res­olution video content linearly to take full advantage of large screen sizes have been implemented 
in TV sets. When content and screen aspect ratios differ, format proportions may be compromised, video 
content may be clipped, or screen regions may remain unused. Newer techniques, such as video retargeting 
and video upsampling, rescale individual video frames and can potentially match them to the display resolution 
and aspect ratio. However, none of these methods can display simultaneously more than is contained in 
a single frame. 2 Our Approach With display pixel caching (DPC), we take a completely different approach. 
Instead of zooming, stretching, or retargeting individ­ual frames, we merge the motion information from 
many subse­quent frames to generate high-resolution panoramas in an ad-hoc and fully automatic manner. 
Thus, we cache pixels in border re­gions as long as they are visually reasonable. In contrast to conven­tional 
video mosaicing, however, the challenges to DPC are achiev­ing real-time rates for high-resolution input 
content, and ensuring spatial and temporal consistency in complex local and global video motion patterns. 
The DPC video processing pipeline can be summarized as follows: Motion patterns of input video frames 
are analyzed and segmented into motion layers. The different motion layers are warped and ac­cumulated 
to .ll border regions (i.e., the display cache). Border regions that remain empty can optionally be initialized 
(e.g., by a smooth extrapolation). Uncertain cache content that accumulates vivid registration errors 
is identi.ed and is temporally faded out. If shot transitions are detected the cache content undergoes 
the same transition as the original frames. Subsequent cache states are tem­porally smoothed and .nally 
displayed together with the original *{.rstname.lastname}@jku.at {.rstname.lastname}@uni-weimar.de Figure 
2: User preferences for various video modes over the orig­inal (unmodi.ed) content: The scores range 
from 1 (strong prefer­ence) to 6 (low preference). The bar chart displays, average, me­dian, the lower, 
and the upper quartiles. For retargeting, we used Rubinstein s multi-operator media retargeting with 
face detection. frames. The result is a high-resolution panorama that successively .lls the empty screen 
borders while leaving the original frames un­touched, rather than a framed video as in common video modes. 
 3 Results DPC achieves real-time rates for high-resolution video content (e.g., 50fps for PAL videos 
displayed on a 720p screen, 29fps for PAL when displayed on a 1080p screen, and 26fps for 720p dis­played 
on a 1080p screen) while processing complex motion pat­terns fully automatically. We compared DPC to 
related video modes in the context of a user evaluation with 59 subjects. Thereby, different options 
for ini­tializing empty cache regions were also tested (see supplementary videos): no initialization 
leaves empty cache regions blank, full ini­tialization smears cache edges towards screen edges, progressive 
clipping initializes only the regions between original cache content and its outermost extent, conservative 
clipping cuts cache content at its inner limit. We found, that DPC is preferred most when straight and 
screen-aligned cache edges are preserved. Copyright is held by the author / owner(s). SIGGRAPH 2011, 
Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037887</article_id>
		<sort_key>610</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>46</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Device-independent imaging system for high-fidelity colors]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037887</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037887</url>
		<abstract>
			<par><![CDATA[<p>An imaging system, in general, consists of a capturing system (i.e., a camera), a signal transmission process, and an output device (display and/or printing systems). When capturing a picture, colors are captured by an imaging sensor on a camera, whose spectral responses are, in general, highly different from the color matching functions of CIE 1931. Such cameras show non-colorimetric response and provides inaccurate color information of a captured object. Additionally, when storing data, different digital cameras generate different signals in red, green, and blue (RGB), whose reproduction is highly dependent each device. Then, a display device applies its own color conversion based on RGB format. As a result, no colors are correctly captured and reproduced. A severe miscommunication happens between capturing and displaying systems as far as using device-dependent imaging systems. In order to really reproduce images of high-fidelity colors on a display, it is essential for the imaging system to adopt colorimetric method not only in displaying and but also in capturing images. Therefore, we propose a device-independent imaging system which can accurately capture colors and faithfully reproduce colors.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809740</person_id>
				<author_profile_id><![CDATA[81490642709]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kazunari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tomizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SHARP Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809741</person_id>
				<author_profile_id><![CDATA[81365592580]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Akiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SHARP Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809742</person_id>
				<author_profile_id><![CDATA[81490677945]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Makoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hasegawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SHARP Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809743</person_id>
				<author_profile_id><![CDATA[81466641193]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yasuhiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SHARP Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809744</person_id>
				<author_profile_id><![CDATA[81430651940]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Shinichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Katoh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ikegami Tsushinki]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809745</person_id>
				<author_profile_id><![CDATA[81488649164]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Kenichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ikegami Tsushinki]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809746</person_id>
				<author_profile_id><![CDATA[81488672973]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Osamu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ozawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ikegami Tsushinki]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809747</person_id>
				<author_profile_id><![CDATA[81490655524]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Yoshifumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shimodaira]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Shizuoka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Device-Independent Imaging System for High-Fidelity Colors Kazunari Tomizawa, Akiko Yoshida, Makoto 
Hasegawa, Yasuhiro Yoshida SHARP Corporation Shinichi Katoh, Kenichi Nishizawa, Osamu Ozawa Ikegami Tsushinki 
  (a) (b) (c) (d) Figure 1: (a) Device-independent imaging system working at real-time. Multi-Primary 
Color (MPC) display system (right) and a comparable RGB-based display (left) are located with our capturing 
system (center). (b) Capturing system to take the whole visible spectrum. (c) Representation on an MPC 
display system. (d) Representation on a comparable conventional display. Note that the cyan parts of 
Morpho are accurately captured and faithfully reproduced by our imaging system. 1 Problem Statement An 
imaging system, in general, consists of a capturing system (i.e., a camera), a signal transmission process, 
and an output device (dis­play and/or printing systems). When capturing a picture, colors are captured 
by an imaging sensor on a camera, whose spectral responses are, in general, highly different from the 
color match­ing functions of CIE 1931. Such cameras show non-colorimetric response and provides inaccurate 
color information of a captured object. Additionally, when storing data, different digital cameras generate 
different signals in red, green, and blue (RGB), whose re­production is highly dependent each device. 
Then, a display device applies its own color conversion based on RGB format. As a result, no colors are 
correctly captured and reproduced. A severe miscom­munication happens between capturing and displaying 
systems as far as using device-dependent imaging systems. In order to really reproduce images of high-.delity 
colors on a display, it is essential for the imaging system to adopt colorimetric method not only in 
displaying and but also in capturing images. Therefore, we propose a device-independent imaging system 
which can accurately capture colors and faithfully reproduce colors.  2 Capturing System There are two 
major ways to capture colors accurately: employing a camera with a single sensor whose sensitivities 
satisfy Luther-Ives conditionor using a multi-spectral camera. We chose to de­velop a camera with three 
sensors and three .lters which captures all signals from different bands of spectral distribution simultane­ously 
like conventional digital cameras.This camera has the equiv­alent spectral distribution to the color 
matching functions and is in a handheld size. Monochrome CMOS image sensors are mounted with the resolution 
of 3.2 mega-pixels at 12 bits/pixel. The camera is adopted color spectral sensitivities of three .lters 
which satisfy the Luther-Ives condition.In the end, matrix calculation is applied to convert the spectral 
sensitivity functions to the tristimulus values of an object. The least square error method is used to 
obtain the matrix in order to closely .t the spectral sensitivity curves of the CIE color matching functions. 
As a result, our capturing system can take the whole range of the visible spectrum. Accuracy of capturing 
colors of the camera is examined by comparing the data taken by our cam­era and by a spectral colorimeter 
for MacBeth Color Checker (24 colors). The color difference between measured and captured data is quite 
small at the average of .E =0.27 in CIE L*a*b*. High accuracy is achieved in capturing colors by our 
camera system.  3 Display System One of the multi-primary color (MPC) display systems, QuintPixel, was 
presented in 2010.While conventional liquid crystal displays (LCD) are assembled with three primary colors: 
RGB. QuintPixel employs additional Yellow and Cyan primaries in addition to RGB. The primary goal of 
QuintPixel was to accurately reproduce the real-surface colors with high ef.ciency and, in the end, it 
achieved over 99% reproduction of the real-surface colors. Besides its wide color gamut, some other bene.ts 
of MPC display systems are al­ready known based on MPC s characteristic of color reproduction redundancy. 
For our new device-independent imaging system, we employ QuintPixel display system with six sub-pixels 
(the area for red is doubled) for the resolution of 1920 × 1080 pixels in 60-inch size. 4 Device-Independent 
Imaging System Now, there is a device-independent imaging system consisting of a capturing system, signal 
transmission, and a display system. Our capturing system provides output signals in one of the device­independent 
formats, XYZ, via CameraLink with the resolution of 1920 × 1080 pixels for 30Hz. After doubling the video 
frequency from 30 to 60Hz, QuintPixel converts the XYZ signals to its own in­put signals in red, green, 
blue, yellow, and cyan and reproduces im­ages. A comparable conventional display applies a gamut-mapping 
and reproduces images within RGB-structured color gamut. Ad­ditionally, our imaging system can be working 
at real-time. As shown in Figure 1, if some colors are located out of the RGB­based color gamut (e.g., 
sRGB), such colors are clipped onto the gamut boundary and cannot be reproduced on conventional RGB display 
while QuintPixel display system can accurately reproduce those colors. Overall, our device-independent 
imaging system can accurately capture and faithfully reproduce colors. Acknowledgments We would like 
to express our gratitude to Mr. Makoto Katoh at PaPaLaB Ltd. for his support on developing our camera 
prototype. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, 
August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037888</section_id>
		<sort_key>620</sort_key>
		<section_seq_no>16</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Hiding complexity]]></section_title>
		<section_page_from>16</section_page_from>
	<article_rec>
		<article_id>2037889</article_id>
		<sort_key>630</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>47</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Occlusion culling in Alan Wake]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037889</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037889</url>
		<abstract>
			<par><![CDATA[<p>The combination of large outdoor environments and dynamic, shadow casting light sources posed a rendering performance challenge Remedy had to tackle during the production of Alan Wake.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809748</person_id>
				<author_profile_id><![CDATA[81481647381]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Silvennoinen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Umbra Software]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ari@umbrasoftware.com]]></email_address>
			</au>
			<au>
				<person_id>P2809749</person_id>
				<author_profile_id><![CDATA[81488642232]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Teppo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Soininen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Umbra Software]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[teppo@umbasoftware.com]]></email_address>
			</au>
			<au>
				<person_id>P2809750</person_id>
				<author_profile_id><![CDATA[81488649167]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Markus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[M&#228;ki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Remedy Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[markus@remedygames.com]]></email_address>
			</au>
			<au>
				<person_id>P2809751</person_id>
				<author_profile_id><![CDATA[81488670106]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Olli]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tervo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Remedy Enternainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[olli.tervo@remedygames.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1032322</ref_obj_id>
				<ref_obj_pid>1032273</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Aila, T., and Miettinen, V. 2004. dpvs: An occlusion culling system for massive dynamic environments. <i>IEEE Computer Graphics and Applications 24, 2</i>, 86--97.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bittner, J., Wimmer, M., Piringer, H., and Purgathofer, W. 2004. Coherent hierarchical culling: Hardware occlusion queries made useful. <i>Computer Graphics Forum 23</i>, 3, 615--624.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1944759</ref_obj_id>
				<ref_obj_pid>1944745</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bittner, J., Mattausch, O., Silvennoinen, A., and Wimmer, M. 2011. Shadow caster culling for efficient shadow mapping. In <i>Symposium on Interactive 3D Graphics and Games</i>, ACM, New York, NY, USA, I3D '11, 81--88.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383920</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Guthe, M., Bal&#225;zs, &#193;., and Klein, R. 2006. Near optimal hierarchical culling: Performance driven use of hardware occlusion queries. In <i>Eurographics Symposium on Rendering 2006</i>, The Eurographics Association, T. Akenine-M&#246;ller and W. Heidrich, Eds.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Occlusion Culling in Alan Wake Ari Silvennoinen* Teppo Soininen Markus M¨aki Olli Tervo§ Umbra Software 
Umbra Software Remedy Entertainment Remedy Enternainment Figure 1: A typical outdoor scene in Alan Wake 
(courtesy of Remedy Entertainment). 1 Introduction The combination of large outdoor environments and 
dynamic, shadow casting light sources posed a rendering performance chal­lenge Remedy had to tackle during 
the production of Alan Wake. This talk presents the evolution of the occlusion culling system used in 
Alan Wake from early concepts and manually placed portals to a fully automatic online occlusion culling 
solution developed to­gether with Umbra Software. In addition to using this system to cull invisible 
objects and light sources from the camera s point of view, Alan Wake uses the visi­bility information 
in a novel way to ef.ciently cull shadow casters from the light s point of view to speed up shadow map 
generation. This extension uses a mask of potential shadow receivers to cull shadow casters using a hierarchical 
occlusion culling algorithm. We will demonstrate the speedup in rendering performance in Alan Wake and 
conclude the talk with a discussion of the trade-offs be­tween hardware and software based occlusion 
culling systems. 2 Culling Algorithms For solving visibility from the camera s point of view, Alan Wake 
used a hierarchical hardware based occlusion query algorithm [Bit­ tner et al. 2004], [Guthe et al. 2006]. 
The advantages of using a hardware based culling algorithm outweighed the content pipeline issues associated 
with using a typical software occlusion system based on either precomputed potentially visible sets (PVS) 
or artist generated occlusion geometry and software rasterizer [Aila and Mi­ ettinen 2004]. Occlusion 
culling minimized the overdraw and the bottleneck shifted to shadow map generation. In order to speed 
up shadow map rendering, Alan Wake used a pro­totypical implementation of a shadow caster culling algorithm 
for ef.cient shadow mapping [Bittner et al. 2011]. A simple approxi­ mation to the receiver mask is obtained 
by spanning a light space scissor rectangle for each shadow cascade based on camera visibil­ity. This 
light space AABB can be also used for accurate shadow frustum culling and it automatically culls empty 
cascades. *e-mail: ari@umbrasoftware.com e-mail: teppo@umbasoftware.com e-mail: markus@remedygames.com 
§e-mail: olli.tervo@remedygames.com  3 Conclusion The main bene.t of a hardware based occlusion system 
is that it does not require any modi.cations to the game content or the con­tent pipeline, whereas typical 
software based approaches require customized occlusion models. This can lead to increased content generation 
times and the dependencies between renderable models and their respective occlusion models can be tedious 
to maintain. There are basically two major drawbacks in using a hardware based solution. First, a GPU 
assisted solution requires the application to synchronize CPU and GPU in order to know which draw calls 
need to be submitted to the GPU. Second, processing the hardware occlusion queries consumes valuable 
GPU time, which can be an issue on some target platforms. The choice between a hardware and software 
based occlusion sys­tem requires a careful thought and is dependent on the application and target platform. 
However, we expect that the content pipeline issues related to software based systems will be solved 
in the near future, making it a more viable option in a wider array of scenarios. References AILA, T., 
AND MIETTINEN, V. 2004. dpvs: An occlusion culling system for massive dynamic environments. IEEE Computer 
Graphics and Applications 24, 2, 86 97. BITTNER, J., WIMMER, M., PIRINGER, H., AND PURGATH-OFER, W. 2004. 
Coherent hierarchical culling: Hardware oc­clusion queries made useful. Computer Graphics Forum 23, 3, 
615 624. BITTNER, J., MATTAUSCH, O., SILVENNOINEN, A., AND WIM-MER, M. 2011. Shadow caster culling for 
ef.cient shadow map­ping. In Symposium on Interactive 3D Graphics and Games, ACM, New York, NY, USA, 
I3D 11, 81 88. ´hierarchical culling: Performance driven use of hardware occlu­sion queries. In Eurographics 
Symposium on Rendering 2006, The Eurographics Association, T. Akenine-M¨oller and W. Hei­drich, Eds. 
GUTHE, M., BAL ´A., AND KLEIN, R. 2006. Near optimal AZS, Copyright is held by the author / owner(s). 
SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037890</article_id>
		<sort_key>640</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>48</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Increasing scene complexity]]></title>
		<subtitle><![CDATA[distributed vectorized view culling]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037890</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037890</url>
		<abstract>
			<par><![CDATA[<p>Interactive Entertainment requires increasingly complex scenes at high performance. The apex of the Rendering pipeline is the Culling System which filters entities based on visibility and optimizes the load on the GPU. Culling Systems have standardized around spatial trees to produce a visible set of entities which are inefficient on current console hardware.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809752</person_id>
				<author_profile_id><![CDATA[81488668992]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Routledge]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts Black Box]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[andyr@ea.com]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Increasing Scene Complexity -Distributed Vectorized View Culling Andrew RoutledgeElectronic Arts Black 
Box andyr@ea.com 1. Introduction Interactive Entertainment requires increasingly complex scenes athigh 
performance. The apex of the Rendering pipeline is the Culling System which filters entities based on 
visibility and optimizes the load on the GPU. Culling Systems have standardized around spatial trees 
to produce a visible set of entities which are inefficient on current console hardware. 2. Outline 2.1 
The Problem. In order to increase visual complexity and improve performance,the culling system needed 
to be rebuilt. Expensive fragmentedmemory reads needed to be reduced and culling needed to be aconcurrent 
process. 2.3 Solution The solution was to move away from spatial trees with fragmented memory accesses 
and move to a flattened, localized,high density data representation and using console Vector units,linearly 
process the data stream and evaluate entity visibility. Thistechnique significantly reduces memory access 
times, allows forpre-fetching of data into memory unit caches and using the Vectorunit instructions to 
accept/reject entities 4 entities at a time. With linear high density data, portions of the data can 
be sliced upand batched into concurrent job tasks yielding higher even higherperformance.  3. Implementation 
3.1 Views Typical AAA titles have 10-15 unique views to produce a varietyof advanced rendering effects. 
Each view requires culling and foreach additional view the overall cost of the culling increaseswhich 
in turn decreases scene complexity. A typical view setup may include 1xPlayer View, 3xOrthagraphicCascade 
Shadow, 6x Environment Map reflection views, anddependent on genre additional views could include a RearviewMirror, 
multiple Parabolic Reflection and Spotlight Shadow views. 3.2 Vectorized Job In the pipeline preprocess 
a spatial subsection of entities, typicallyencapsulating 200m2, is collected and flat packed into a highdensity 
linear data stream containing series of entity reference id,transform and bounding box information. The 
culling system, determines all visible spatial subsections,generates a series of job tasks and slices 
the data streams intosliced batches that are passed as inputs into a vectorized cullingtask for visibility 
testing. The culling job outputs a list of visibleentities that is subsequently passed onto the Rendering 
Engine forprocessing.  3.3 Culling Methods For maximum performance, dependent on view projection theculling 
method can be tailored to the dataset typical in a givenview. Typically for a PlayerView a perspective 
Frustum Culling isperformed, however for Orthographic and Parabloid projections it is possible to optimize 
the culling method. For Orthographic aviewport clip method can be applied and for a Parabloid View a 
radial distance and single hemisphere plane method is best. In addition to projection optimal methods, 
occlusion planes andpixel size rejection methods were implemented. Pixel size rejection removes all entities 
that when projected cover less pixelspace than a given threshold. Occlusion planes are 3 dimensionalfinite 
planes that are placed in a scene to occlude entities behind them. 4. Conclusion This solution improved 
our visual complexity whilst significantlyimproving the performance of the Culling System on targetconsoles. 
This was important addition to the product as we couldincrease our scene complexity and add additional 
advanced rendering features whilst reducing the impact on CPU performance. Copyright is held by the author 
/ owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037891</article_id>
		<sort_key>650</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>49</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Practical occlusion culling in KILLZONE 3]]></title>
		<subtitle><![CDATA[Will Vale - second intention limited - contract R&#38;D for Guerrilla BV]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037891</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037891</url>
		<abstract>
			<par><![CDATA[<p>In KILLZONE 2, we used a geometric occlusion system with zones linked by portals and manually-placed blocker geometry. This was difficult to edit and had unpredictable performance characteristics.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Practical occlusion culling in KILLZONE 3 D:\Work\Guerrilla\KZ3\KIN\Assets\Game_Assets\Screenshots\ShotPS3_0344.png 
../../../Guerrilla/KZ3/KIN/Assets/Game_Assets/Screenshots/ShotPS3_0345.png  Will Vale Second Intention 
Limited contract R&#38;D for Guerrilla BV In KILLZONE 2, we used a geometric occlusion system with 
zones linked by portals and manually-placed blocker geometry. This was difficult to edit and had unpredictable 
performance characteristics. For KILLZONE 3, we wanted to produce a largely automatic occlusion system 
requiring less artist involvement. There are three main aspects to this work: . We generate good quality 
occluders by automatically selecting suitable physics mesh data during level export. This is augmented 
by a system of artist tags for overriding the heuristics, and we also allow artists to place simple occluders 
manually to give the best possible results in critical areas. . At runtime, we produce an occlusion 
buffer using software rendering. We find occluder geometry in the view frustum, and rasterise it to a 
relatively large (360p) depth buffer. This is conservatively down-sampled and compressed for storage 
in main memory. . We then use the occlusion buffer to accelerate scene graph traversal and cull away 
occluded objects. We test candidates using a sequence of three tests constant time sphere rejection 
for smaller objects, rasterisation of OBB diagonals, and finally rasterisation of front-facing OBB quads. 
This gives us relatively fast rejection for visible objects, while the accurate rasterisation test reduces 
the number of false positives, and saves RSX time.  Key advantages to this scheme are: . Speed we 
typically render 5K occluder triangles and test 2-3K objects using 10-20% of one SPU (with lower elapsed 
time due to heavy parallelism) . Scalability performance is predictably linear in occluders rendered, 
and linear in tests performed. . SPU only we do not spend PPU or RSX time on this system, and unlike 
traditional GPU-based occlusion culling there is no complex pipeline to manage. This also allows us to 
cull objects very early and avoid all pipeline costs for them.   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037892</article_id>
		<sort_key>660</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>50</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[High quality previewing of shading and lighting for Killzone3]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037892</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037892</url>
		<abstract>
			<par><![CDATA[<p>Current generation games are pushing real-time computer graphics to the limits of their potential, getting closer to cinematic quality everyday. With the increase in quality, it has become crucial to preview the visual appearance of the assets with the highest fidelity possible. For Killzone3, after realizing that existing software couldn't reach anywhere near the quality that we were looking for, we decided to try something new. We embedded our game rendering engine inside Autodesk Maya, our main content creation tool, achieving interactive previews with an unprecedented mix of flexibility and quality.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809753</person_id>
				<author_profile_id><![CDATA[81320489890]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Francesco]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Giordana]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Guerrilla Games -- SCEE]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Highandlightingfor Killzone3 qualitypreviewingofshading Francesco Giordanae Guerrilla- GamesSCEEe Figuree 
Mayafor 1:Screencapturesfrome 1. Introduction Currente generationgamese arepushinge real-timee comp utere 
graphicse tothee limitse ofe theirpotential,gettinge closere toe cinematicqualityeveryday.Withtheincreaseinqu 
ality,ite hasbecomecrucialtopreviewthevisualappearance ofthee assetswiththehighestfidelitypossible.ForKill 
zone3,aftere realizingthatexistingsoftwarecouldn treachany whereneare thee qualitythatwewerelookingfor,e 
wee decidedto e trye somethinge new.e e embeddedoure renderingengin e Wegameinsidee Autodeske Maya,e 
ourmaine contente creatione too l,e achievinginteractivepreviewswithanunprecedente dmixofe flexibility 
andquality.e 2. TheFramework WedevelopedaMaya-centeredframework,whereoura ssetse aree renderedusingourcustomOpenGLrenderingengi 
ne.e Theree aretworenderingmodes:ae directone,e basede one customhardwareshadernodesinastandardMayavie 
wport;e andadeferredone,basedonacustomdeferredrend ererthate completelyoverridesMaya srenderingpipeline.The 
shadinge isdonethroughCGshadersdeducedfromalivetrav ersalofe theMaya sshadinggraph,grantingusgreatflexibi 
lityinthee definitionmaterials. ofthe 2.1 Hardwareshadernodes Throughe thee Mayae API,e wecreatede acustome 
hardwaree shadere node,e which,connectedtoe anyshadinggraphe insidee Maya,willoverrideitsrenderingoperationsinthe 
viewport.e Eachchangeinthenetworkwilltriggeratraversal ofthee graph,throughwhichwebuildtheCGsourcecodefo 
rareale timeshadertobeusedforthepreview.Theresulti ngshadere isthenusedincombinationwiththevertexandind 
exarrayse providede byMayae torenderthee geometryusinge stand arde OpenGLUpbeattheetimefor calls.to8lightscanusedsam 
thise renderingmode.Additionally,e weimplementede c ustome preandpost-renderoperationstoperformsceneana 
lysisthate can greatlyincreaseperformance.e KILLZONE3&#38;#169;SonyCEE GuerrillaGames­ 2.2renderingengine 
Integrateddeferrede Trulyuniquetoourframeworkisourcustomrendere r,basede one ourgame se deferredrenderinge 
engine,e whichtake se overe theviewp This entirerenderingpipelineinsidetheMayaort.enginee hasitse ownscenee 
graphande representationf ore alle objectse inthee scene.e Beinge adeferredrenderer,we e cane previewanynumberoflights,withtheirrespective 
real-timee shadows,e andalsoe applyae fullrangee ofpost-proces singe effects.Ingeneral,anythingthatcanberenderede 
insidethee gamecanbepreviewedinsideMaya,withaqualitya lmoste identicaltothefinalin-gameresult.Thankstoso 
merecente improvementsMayaAPI,aretonder inthewenowablere one tope ofthee sceneanye elementse fromthee 
standardMay ae renderinge pipeline,e likee manipulators,lightrepres entations,e wireframes,andsimilar.Therenderingenginealsoe 
featuresae seriesofveryusefuldebuggingoptions,likethev isualizatione oftheindividualbuffersusedthedeferredren 
dering. for 2.3 Keepingitinsync DuringtheMayasession,wekeeptrackofallobjec tscreatede anddeletedbytheuserviaacomplexsystemofcal 
lbacks.e Wee buildae secondrepresentatione ofe thee sceneinme mory,e readablebyourengine,andweupdateiteverytime 
thereisae userinteraction.Themainobjectsthatneedtobee syncedaree meshesandlights:themeshgeometrycomesdirectly 
frome thee indexande e e while e the vertexarrayse providedbyMaya,lightse gete convertedintoe CGe shaderse 
similarlytoe t hee geometry shaders. 3. Conclusions Thee developmentofthise frameworkhaspresentedman 
ye challenges,e giventhatwepushede Mayaintoae brande newe territory,e butthee resulthasproventobeae verye 
s uccessfule improvementtoourpipeline.Theartistsarenowca pableofe workingonanyassetwithaveryhighqualityprevi 
ew;thise hase considerablycutdowniteratione timesfore alle a rte teams,e becominganessentialtoolespeciallyforlighterse 
andshadere artists.Futureworkwillbemainlyfocusedonswit chingtoae 64-bitarchitecturetocenes.e allowpreviewingofbiggers 
Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 
7 11, 2011. ISBN 978-1-4503-0921-9/11/0008   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037893</section_id>
		<sort_key>670</sort_key>
		<section_seq_no>17</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Smokin' fluids]]></section_title>
		<section_page_from>17</section_page_from>
	<article_rec>
		<article_id>2037894</article_id>
		<sort_key>680</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>51</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[DB+Grid]]></title>
		<subtitle><![CDATA[a novel dynamic blocked grid for sparse high-resolution volumes and level sets]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037894</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037894</url>
		<abstract>
			<par><![CDATA[<p>We have developed a new data structure for the efficient representation of sparse, time-varying volumetric data discretized on a 3D grid. Our "DB+Grid", so named because it is a &lt;u&gt;D&lt;/u&gt;ynamic &lt;u&gt;B&lt;/u&gt;locked &lt;u&gt;G&lt;/u&gt;rid that shares several characteristics with B+Trees (typically employed in databases and file systems), exploits the spatial coherency of time-varying data to separately and compactly encode data values and grid topology. DB+Grid allows for cache-coherent and fast data access into sparse, 3D grids of very high resolution, exceeding millions of grid points in each dimension. Additionally, DB+Grid is very general, since it imposes neither topology restrictions on the sparsity of the volumetric data nor access patterns when the data are inserted, retrieved or deleted. This is in contrast to most existing sparse volumetric data structures, which either assume static data (<i>i.e.</i> values and topology) or fixed data topology (<i>e.g.</i> manifold surfaces) and require specific access patterns to avoid slow random access. Since DB+Grid is a hierarchical data structure, it also facilitates adaptive grid sampling, and the inherent acceleration structure leads to fast algorithms that are well-suited for simulations. As such, DB+Grid has proven useful for several applications that call for very large, sparse, animated volumes, <i>e.g.</i> level sets and fluid simulations. In this talk we will compare DB+Grid with existing state-of-the-art dynamic sparse data structures and showcase applications from the visual effects industry.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809754</person_id>
				<author_profile_id><![CDATA[81324492244]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Museth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation, SKG]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 DB+Grid: A Novel Dynamic Blocked Grid For Sparse High-Resolution Volumes and Level Sets Ken Museth, 
DreamWorks Animation, SKG Figure 1: Enright benchmark test on high-resolution DB+Grid. Divergence-free 
advection of the Armadillo at an effective resolution of 111 40963. Far-left: Close-up of dynamic 43 
blocks. Frames show level sets at time= 0, 8 , 4 , 2 ,1 period. Notice how the initial (time=0) and last 
(time=1) frames appear almost identical, indicating low mass-loss due to numerical dissipation. This 
is a consequence of the high resolution. Introduction We have developed a new data structure for the 
ef.cient represen­tation of sparse, time-varying volumetric data discretized on a 3D grid. Our DB+Grid 
, so named because it is a Dynamic Blocked Grid that shares several characteristics with B+Trees (typically 
em­ployed in databases and .le systems), exploits the spatial coherency of time-varying data to separately 
and compactly encode data val­ues and grid topology. DB+Grid allows for cache-coherent and fast data 
access into sparse, 3D grids of very high resolution, exceeding millions of grid points in each dimension. 
Additionally, DB+Grid is very general, since it imposes neither topology restrictions on the sparsity 
of the volumetric data nor access patterns when the data are inserted, retrieved or deleted. This is 
in contrast to most ex­isting sparse volumetric data structures, which either assume static data (i.e. 
values and topology) or .xed data topology (e.g. manifold surfaces) and require speci.c access patterns 
to avoid slow random access. Since DB+Grid is a hierarchical data structure, it also facili­tates adaptive 
grid sampling, and the inherent acceleration structure leads to fast algorithms that are well-suited 
for simulations. As such, DB+Grid has proven useful for several applications that call for very large, 
sparse, animated volumes, e.g. level sets and .uid simulations. In this talk we will compare DB+Grid 
with existing state-of-the-art dynamic sparse data structures and showcase appli­cations from the visual 
effects industry. Highlights DB+Grid is a sparse and dynamic volumetric data-structure with the following 
characteristic features: Dynamic. Unlike most sparse volumetric data structures DB+Grid is developed 
for both dynamic topology and values typical for time­dependent numerical simulations and animated volumes. 
This re­quires ef.cient implementation of sparse .nite-difference iterators, topology morphology (e.g. 
dilation) and rebuild algorithms as well as temporal value buffers for cache-coherent numerical integration. 
Compact. A dynamic and hierarchical allocation of blocks leads to a compact sparse data-structure that 
allows for extremely high grid resolutions. To further reduce the memory-footprint we present an ef.cient, 
lossless compression technique that can be employed for both on-line and off-line storage. To further 
reduce the off-line footprint we support a combination of bit-quantization and standard compression schemes. 
General topology. Unlike most existing dynamic data structures for narrow-band level sets ours can effectively 
represent sparse volume data with arbitrary dynamic topology. This implies that DB+Grid can be use as 
a generic volumetric data structure as op­pose to merely supporting dynamic level set applications. Fast 
and .exible data access. DB+Grid supports fast constant­time random data lookup, insertion and deletion. 
DB+Grid also offers fast (constant-time) sequential stencil access iterators which are essential for 
ef.cient simulations employing .nite-difference schemes. Spatially coherent access-patterns even have 
an amortized computational complexity that is independent of the depth of the underlying B+tree employed 
by the DB+Grid. Ef.cient Algorithms. Our hierarchical blocking approach offers several bene.ts, including; 
cache-coherence, inherent bounding­volume acceleration and fast per-block (vs per-voxel) operations. 
It also lends itself to several hierarchical optimization techniques re­sulting in improved computational 
performance, e.g. near real-time boolean operations and ef.cient multi-threading schemes. Adaptive Grid. 
Unlike most existing narrow-band level set data structures DB+Grid is hierarchical and can store values 
at adaptive resolution, by encoding data at all levels of the underlying B+tree. While obviously no as 
adaptive as octrees, this feature is very useful for applications like ray-marching and collision detection 
of sparse volumes, e.g. narrow-band level sets. Con.gurable. By design DB+Grid is highly con.gurable 
in terms of tree-depth, fan-out factors and block dimensions, even wrt. the 3 coordinate axis. This allows 
the grid to be tailored to speci.c appli­cations in order to optimize factors like memory foot-prints, 
cache utilization, random vs. sequential access times and grid adaptively. Out-Of-The-Box. The domain 
of DB+Grid is virtually unbounded in the sense that it can dynamically and randomly expand and con­tract 
in all eight coordinate directions without requiring a full and expensive re-allocation and deep copy. 
This is an especially de­sirable property for grids with dynamic topology. The support for negative grid 
indexing has also proved convenient for many practi­cal applications. Out-of-core. DB+Grid supports simple 
out-of-core streaming. More speci.cally, we can reduce the in-core memory footprint by storing grid values 
out-of-core and only keep the grid topology in­core. Values are then loaded on demand e.g. during ray-tracing. 
Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 
7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037895</article_id>
		<sort_key>690</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>52</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Implicit FEM and fluid coupling on GPU for interactive multiphysics simulation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037895</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037895</url>
		<abstract>
			<par><![CDATA[<p>We present a method to implement on the GPU an implicit FEM solver which is fast and stable enough to handle interactions and collisions. We combine this method with GPU-based fluids [Zhang et al. 2008] and collision detection [Allard et al. 2010] to achieve interactive multiphysics simulations entirely running on the GPU.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809755</person_id>
				<author_profile_id><![CDATA[81488660884]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J&#233;r&#233;mie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Allard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA and University of Lille]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809756</person_id>
				<author_profile_id><![CDATA[81440622142]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hadrien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Courtecuisse]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA and University of Lille]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809757</person_id>
				<author_profile_id><![CDATA[81100210102]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Fran&#231;ois]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Faure]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA and University of Grenoble and LJK -- CNRS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1778819</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Allard, J., Faure, F., Courtecuisse, H., Falipou, F., Duriez, C., and Kry, P. G. 2010. Volume contact constraints at arbitrary resolution. <i>ACM Trans. Graph. 29</i>, 4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Allard, J., Courtecuisse, H., and Faure, F. 2011. Implicit FEM solver on GPU for interactive deformation simulation. In <i>GPU Computing Gems Jade Edition</i>. Elsevier, ch. 21. to appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1552468</ref_obj_id>
				<ref_obj_pid>1552466</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Buatois, L., Caumon, G., and L&#233;vy, B. 2009. Concurrent number cruncher - a GPU implementation of a general sparse linear solver. <i>Int J Parallel Emerg. Distrib. Syst. 24</i>, 3, 205--223.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1426989</ref_obj_id>
				<ref_obj_pid>1426984</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Comas, O., Taylor, Z., Allard, J., Ourselin, S., Cotin, S., and Passenger, J. 2008. Efficient nonlinear FEM for soft tissue modelling and its GPU implementation. In <i>ISBMS</i>, 28--39.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Kr&#252;ger, J., and Westermann, R. 2005. A GPU frame-work for solving systems of linear equations. In <i>GPU Gems 2</i>. Addison-Wesley, ch. 44, 703--718.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Mendoza, C., and Garcia, M. 2010. Soft bodies using finite elements. In <i>Game Physics Pearls</i>. A. K. Peters, ch. 10, 217--250.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Tonge, R., Wyatt, B., and Nicholson, N. 2010. PhysX GPU rigid bodies in Batman: Arkham Asylum. In <i>Game Programming Gems 8</i>. Cengage, ch. 7, 590--601.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2386433</ref_obj_id>
				<ref_obj_pid>2386410</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Zhang, Y., Solenthaler, B., and Pajarola, R. 2008. Adaptive sampling and rendering of fluids on the GPU. In <i>Proc. of Symp. on Point-Based Graph.</i>, 137--146.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Implicit FEM and Fluid Coupling on GPU for Interactive Multiphysics Simulation J´er´emie Allard1,2 
Hadrien Courtecuisse1,2 Franc¸ois Faure3,1,4 1INRIA 2University of Lille 3University of Grenoble 4LJK 
 CNRS  Figure 1: We combine GPU methods for detailed deformable objects (left), image-based collisions 
(middle), and SPH .uids to simulate a two-way .uid-deformable coupling (right) at interactive rates. 
Abstract We present a method to implement on the GPU an implicit FEM solver which is fast and stable 
enough to handle interactions and collisions. We combine this method with GPU-based .uids [Zhang et al. 
2008] and collision detection [Allard et al. 2010] to achieve interactive multiphysics simulations entirely 
running on the GPU. 1 Introduction The Finite Element Method (FEM) is widely used to simu­late deformable 
materials in mechanical simulations. Recently, co-rotational linear FEM was successfully applied in interactive 
games [Mendoza and Garcia 2010] and medical simulations, how­ever CPU methods are limited to coarse meshes 
due to performance constraints. GPU-based FEM methods have been proposed [Comas et al. 2008], but they 
apply explicit time integration and thus require prohibitively small time-steps. We overcome these limitations 
by implementing an implicit time integration scheme on the GPU [Al­lard et al. 2011]. GPU collision handling 
methods for rigid [Tonge et al. 2010] and deformable bodies [Allard et al. 2010] are available, yet an 
interactive simulation of two-way .uid-deformable coupling has remained an open problem. 2 Methods Implicit 
FEM Solver on GPU To implement implicit FEM, we rely on an iterative Conjugate Gradient (CG) solver. 
However, in contrast to existing GPU-based sparse solvers [Kr¨uger and West­ermann 2005; Buatois et al. 
2009], we do not explicitly build the system matrix, but instead parallelize the required matrix-vector 
products directly on the original mesh. This considerably reduces the number of operations required, 
and more importantly the con­sumed bandwidth, enabling the method to be fast enough for in­teractive 
simulations of soft bodies. The parallelization, detailed in [Allard et al. 2011], relies on .rst computing 
the contribution of mesh elements using one thread per tetrahedron, followed by a parallel gather to 
accumulate contributions at vertices. Further opti­mizations include mesh ordering, compact data structures, 
memory layout, and changing sequences of operations to reduce synchro­nization points. Fluid Coupling 
using Image-based Collisions An image­based collision method [Allard et al. 2010] has been proposed to 
handle complex deformable objects. It computes intersection vol­ume gradients which are discretized on 
pixels and accumulated on vertices. To handle solid-.uid coupling, we extend this approach to compute 
additional pixels directly from the SPH .uid density .eld using ray-tracing. When intersections are detected, 
contribu­tions are accumulated to the .uid particles based on their relative contribution as given by 
the SPH kernels evaluated at the pixel. As ray-tracing can be expensive, an important optimization is 
to test rays only when an existing pixel is within the .uid, which requires only a simple evaluation 
of the SPH density .eld. 3 Results Our CUDA-based FEM solver is able to simulate a deformable ob­ject 
with 45k tetrahedral elements (Fig. 1 left) at 212 FPS on a Nvidia GeForce GTX 480, 18× faster than our 
most optimized se­quential implementation on an Intel Core i7 975 3.33GHz CPU. All timings exclude rendering 
as the cost of this step can vary greatly depending on the desired visual quality and complexity. The 
.uid simulation (Fig. 1 right) demonstrates two-way coupling between the .uid and a soft cup, achieving 
25 FPS using 2k FEM elements and 32k SPH particles on a GeForce GTX 280 GPU.  References ALLARD, J., 
FAURE,F., COURTECUISSE, H., FALIPOU,F., DURIEZ, C., AND KRY, P. G. 2010. Volume contact constraints at 
arbitrary resolution. ACM Trans. Graph. 29,4. ALLARD, J., COURTECUISSE, H., AND FAURE, F. 2011. Implicit 
FEM solver on GPU for interactive deformation simulation. In GPU Computing Gems Jade Edition. Elsevier, 
ch. 21. to appear. BUATOIS, L., CAUMON, G., AND LEVY´, B. 2009. Concurrent number cruncher -a GPU implementation 
of a general sparse linear solver. Int J Parallel Emerg. Distrib. Syst. 24, 3, 205 223. COMAS, O., TAYLOR, 
Z., ALLARD, J., OURSELIN, S., COTIN, S., AND PASSENGER, J. 2008. Ef.cient nonlinear FEM for soft tissue 
modelling and its GPU implementation. In ISBMS, 28 39. KRUGER¨ , J., AND WESTERMANN, R. 2005. A GPU frame­work 
for solving systems of linear equations. In GPU Gems 2. Addison-Wesley, ch. 44, 703 718. MENDOZA, C., 
AND GARCIA, M. 2010. Soft bodies using .nite elements. In Game Physics Pearls. A.K. Peters, ch. 10, 217 
250. TONGE, R., WYATT, B., AND NICHOLSON, N. 2010. PhysX GPU rigid bodies in Batman: Arkham Asylum. In 
Game Pro­gramming Gems 8. Cengage, ch. 7, 590 601. ZHANG,Y., SOLENTHALER, B., AND PAJAROLA, R. 2008. 
Adap­tive sampling and rendering of .uids on the GPU. In Proc. of Symp. on Point-Based Graph., 137 146. 
Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 
7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037896</article_id>
		<sort_key>700</sort_key>
		<display_label>Article No.</display_label>
		<pages>2</pages>
		<display_no>53</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Correcting low frequency impulses in distributed simulations]]></title>
		<page_from>1</page_from>
		<page_to>2</page_to>
		<doi_number>10.1145/2037826.2037896</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037896</url>
		<abstract>
			<par><![CDATA[<p>Level set fluid simulations are a common approach for simulating liquids. Their regular grid sampling make it straightforward to apply incompressibility constraints through a velocity projection. However, the high resolutions needed for fine detail result in large memory requirements and long computation times. Distributing liquid simulations across multiple machines can address these problems.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809758</person_id>
				<author_profile_id><![CDATA[81488670649]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jeff]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lait]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Side Effects Software Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jlait@sidefx.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1141959</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Irving, G., Guendelman, E., Losasso, F., and Fedkiw, R. 2006. Efficient simulation of large bodies of water by coupling two and three dimensional techniques. <i>ACM Trans. Graph. 25</i> (July), 805--811.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Correcting Low Frequency Impulses in Distributed Simulations Jeff Lait* Side Effects Software Inc. 
 (a) One machine, 93 min/frame (b) Frame delayed, 16 min/frame 1 Introduction Level set .uid simulations 
are a common approach for simulat­ing liquids. Their regular grid sampling make it straightforward to 
apply incompressibility constraints through a velocity projec­tion. However, the high resolutions needed 
for .ne detail result in large memory requirements and long computation times. Dis­tributing liquid simulations 
across multiple machines can address these problems. Incompressibility is an intrinsically global constraint. 
[Irving et al. 2006] exchange boundary data during each iteration of the projec­tion, but this requires 
large data transfers for 3D .uids. The ap­proach we take closely matches that of [Lentine et al. 2010]; 
our contribution is to use a padded grid that avoids the need for a sepa­rate surface pressure solve 
to preserve detail. We divide the .uid simulation box into equal-sized slices, one per machine. Because 
most stages of a .uid simulation only require local knowledge of the simulation grids, we pad our slices 
by a band of voxels. The size of the band can be computed from a CFL­like condition. After each time 
step, neighboring slices exchange their padded data. Our reference test is designed to focus on a simple 
.uid impulse. We divide a 4003 .uid box by quadrants into four slices and then drop a slice-centered 
sphere of liquid into a standing pool.  2 Previous Method We originally used a frame delayed method 
that solved the projec­tion locally on each slice. Closed boundary conditions were used to prevent the 
.uid from draining into adjacent slices. Boundary cells velocities were overwritten every frame by the 
padding exchange. Each slice thus reacted to a frame delayed version of its neighbors. The frame delayed 
boundaries introduce two artifacts: they delay the transfer of .uid impulses and introduce a re.ection 
at the inter­nal slice boundaries. Our padding ensured we accurately represent the neighborhood of every 
voxel, so the naturally more local in na­ture high frequency .uid detail would be preserved. Low frequency 
detail, on the other hand, has to propagate across the entire .uid in a single time step. This approach 
is effective for smoke and .re simulations where the error is often masked by artist induced turbulence 
in the system. Liquid simulations are more problematic. Although turbulent and * e-mail: jlait@sidefx.com 
 (c) FLIP update, 11 min/frame (d) Border update, 17 min/frame fast moving .uid can produce usable results 
by breaking up the error, when low frequency impulses dominate the system, the result is clearly erroneous. 
 3 New Method We adopted a multi-resolution technique to address the low fre­quency errors in liquids. 
Each slice is reduced to quarter resolution and exchanged with the other machines. A full size, low resolution, 
version of the simulation can then be built and quickly solved. 3.1 FLIP Update We .rst coupled the low 
and high resolution simulations with a FLIP update. We computed the change in velocity caused by the 
low resolution solve and added it directly to our high resolution velocity. This delta update informed 
the high resolution simulation of the global state and avoided diffusion of .ne detail. Consequently, 
low frequency errors were eliminated by the FLIP update. Computation time for each slice was also reduced 
because most of the work for the high resolution pressure projection was already done. However, we also 
lost high resolution detail -the low resolution simulation would incorrectly cancel out surface waves 
that fell below its sampling frequency. 3.2 Border Update Since the low resolution correction factor 
was damping the detail, our next approach was to minimize its use. [Lentine et al. 2010] note the key 
step was to update only the .ne cells that lie on coarse boundaries. In the projection stage, this implied 
that the only im­portant velocity values to update were those on the slice bound­aries. Correcting closed 
boundary conditions with the low resolu­tion deltas provided the high resolution projection a close approx­imation 
of the values we expected it to have computed if it had access to the full simulation. Our padding of 
the slices caused the localized error due to the low resolution approximation to be erased in the border 
exchange stage.  References IRVING, G., GUENDELMAN, E., LOSASSO, F., AND FEDKIW, R. 2006. Ef.cient simulation 
of large bodies of water by coupling two and three dimensional techniques. ACM Trans. Graph. 25 (July), 
805 811. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, 
August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 LENTINE, M., ZHENG, W., AND FEDKIW, R. 2010. A novel 
algorithm for incompressible .ow using only a coarse grid pro­jection. ACM Trans. Graph. 29 (July), 114:1 
114:9.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037897</section_id>
		<sort_key>710</sort_key>
		<section_seq_no>18</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Volume and rendering]]></section_title>
		<section_page_from>18</section_page_from>
	<article_rec>
		<article_id>2037898</article_id>
		<sort_key>720</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>54</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Gaussian quadrature for photon beams in Tangled]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037898</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037898</url>
		<abstract>
			<par><![CDATA[<p>We implemented the recent Photon Beams [Jarosz et al. 2011] algorithm in Photorealistic RenderMan to efficiently render artistically-directed volumetric lighting effects for the feature-length animated movie <i>Tangled</i> [Nowrouzezahrai et al. 2011]. Photon beams generalize volumetric photon mapping by storing the entire path of a photon instead of just the scattering location. Conceptually, each photon beam represents a truncated conical beam of light through the medium. Jarosz et. al formulated several ways to compute a radiance estimate from photon beams, promoting the so-called Beam x Beam 1D estimate which interprets each beam as a flat axial billboard.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809759</person_id>
				<author_profile_id><![CDATA[81487643872]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jared]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Johnson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios and University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809760</person_id>
				<author_profile_id><![CDATA[81466647309]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dylan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lacewell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809761</person_id>
				<author_profile_id><![CDATA[81100351513]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Selle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809762</person_id>
				<author_profile_id><![CDATA[81100389194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Wojciech]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jarosz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research Z&#252;rich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1899409</ref_obj_id>
				<ref_obj_pid>1899404</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Jarosz, W., Nowrouzezahrai, D., Sadeghi, I., and Jensen, H. W. 2011. A comprehensive theory of volumetric radiance estimation using photon points and beams. <i>ACM Transactions on Graphics 30</i> (January), 5:1--5:19.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964924</ref_obj_id>
				<ref_obj_pid>2010324</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Nowrouzezahrai, D., Johnson, J., Selle, A., Lacewell, D., Kaschalk, M., and Jarosz, W. 2011. A programmable system for artistic volumetric lighting. <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Pegoraro, V., and Parker, S. G. 2009. An Analytical Solution to Single Scattering in Homogeneous Participating Media. <i>Computer Graphics Forum (Proceedings of the 30th Eurographics Conference) 28</i>, 2, 329--335.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Stroud, A. H., and Secrest, D. 1966. <i>Gaussian quadrature formulas</i>. Prentice-Hall series in automatic computation. Prentice-Hall, Englewood Cliffs, NJ.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Gaussian Quadrature for Photon Beams in Tangled Jared M. Johnson1,2 Dylan Lacewell1 Andrew Selle1 Wojciech 
Jarosz3 1Walt Disney Animation Studios 2University of Central Florida urich 3Disney Research Z¨  Figure 
1: Photon Beams used in the movie Tangled. (left) Photon beams only. (right) Final composited image. 
1 Introduction We implemented the recent Photon Beams [Jarosz et al. 2011] algo­rithm in Photorealistic 
RenderMan to ef.ciently render artistically­directed volumetric lighting effects for the feature-length 
animated movie Tangled [Nowrouzezahrai et al. 2011]. Photon beams gen­eralize volumetric photon mapping 
by storing the entire path of a photon instead of just the scattering location. Conceptually, each photon 
beam represents a truncated conical beam of light through the medium. Jarosz et. al formulated several 
ways to compute a radiance estimate from photon beams, promoting the so-called Beam × Beam 1D estimate 
which interprets each beam as a .at axial billboard. In the original formulation, the radiance along 
a beam has an expo­nential falloff, physically-based on the scattering properties of the medium. Fortunately, 
the billboard representation easily handles ar­bitrary, artist-controlled fall-off functions along beams, 
making it an ideal candidate for artistic volumetric effects. Though the billboard representation gives 
the correct approach in the limit, it can some­times produce artifacts. When the beams are aimed at and 
contain the camera a .at billboard is a poor approximation for the conical photon beam. To avoid this 
problem, we use the Beam × Beam 2D estimate which treats each beams as a conical frustum with a .nite 
cross-section. In this case, to compute the beam s contribution, we need to consider the integral along 
each camera ray through a beam. While there are analytic solutions to this path integral for physically­based 
single-scattering [Pegoraro and Parker 2009], we need to allow for arbitrary, non-physical fall-off functions 
for artistic control. With the knowledge that most fall-off functions de.ned by our artists would be 
polynomial-smooth, we use Gaussian Quadrature [Stroud and Secrest 1966] to accurately and ef.ciently 
estimate the lighting contribution of these camera-containing beams. Our numerical approach allows for 
robust artistic control over beam appearance, while reducing the number of lighting samples compared 
to other numerical approaches with no loss of accuracy. 2 Gaussian Quadrature Like the more commonly-used 
Riemann sum, Gaussian Quadrature is a method for approximating a de.nite integral. It has greater accuracy 
for the same number of sample points as a Riemann sum because sample locations are weighted and placed 
in speci.c loca­tions to solve for an exact result with n-samples for polynomials of degree 2n - 1. For 
a prescribed number of sample points, Gaussian Quadrature computes an integral in the canonical domain 
[-1, 1] using a pre­computed set of positions xi and corresponding weights wi: Z 1 n X f(x)dx wif (xi). 
(1) -1 i=1 An integral over an arbitrary interval [a, b] is computed using a simple transformation of 
the samples points and weights: Z bn « X b - ab - aa + b f(x)dx wifxi + . (2) a 2 22 i=1 3 Photon Beams 
in PRMan We implemented photon beams in PRMan using linear RiCurves with two control points. When the 
camera is contained within a beam, RiCurves result in visual artifacts. We therefore replace these RiCurves 
with a screen-space quad that covers the entire screen, to ensure that every pixel receives a shading 
contribution. For each shading point on the quad we do a ray/cone intersection, .nding the ray interval 
enclosed in the beam volume. We map our desired number of sample points to this ray interval, scaling 
sample weights using Equation 2. We then calculate the lighting at these points with our artist-de.ned 
function, and then compute the weighted summation. In practice, at most six lighting sample points were 
suf.cient to provide a good estimate. A Riemann sum of equivalent accuracy would require more than twice 
as many sample points. 4 Results The keyhole shot from the .lm Tangled (Figure 1) used photon beams 
in PhotoRealistic Renderman to represent light shining through a keyhole in participating media. Our 
artists used a one­dimensional color texture to design a color change as the beams extinguish in the 
media. This color change was multiplied by physically-based attenuation. As the camera passes through 
photon beams, our Gaussian quadrature estimate on full-screen quads blends seamlessly with the RiCurve 
estimate, accurately rendering the camera inside photon beams. All of this is done within the PRMan pipeline, 
maintaining proper motion-blur, depth-of-.eld effects, and rendering correctly in stereoscopic 3D. References 
JAROSZ, W., NOWROUZEZAHRAI, D., SADEGHI, I., AND JENSEN, H. W. 2011. A comprehensive theory of volumetric 
radiance estimation using photon points and beams. ACM Transactions on Graphics 30 (January), 5:1 5:19. 
NOWROUZEZAHRAI, D., JOHNSON, J., SELLE, A., LACEWELL, D., KASCHALK, M., AND JAROSZ, W. 2011. A programmable 
system for artistic volumetric lighting. ACM Transactions on Graphics (Proceedings of SIGGRAPH). PEGORARO, 
V., AND PARKER, S. G. 2009. An Analytical Solution to Single Scattering in Homogeneous Participating 
Media. Computer Graphics Forum (Proceedings of the 30th Eurographics Conference) 28, 2, 329 335. STROUD, 
A. H., AND SECREST, D. 1966. Gaussian quadrature formulas. Prentice-Hall series in automatic computation. 
Prentice-Hall, Englewood Cliffs, NJ. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, 
British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037899</article_id>
		<sort_key>730</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>55</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Importance sampling of area lights in participating media]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037899</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037899</url>
		<abstract>
			<par><![CDATA[<p>We focus on the problem of single scattering in homogeneous volumes and develop a new importance sampling technique that avoids the singularity near point light sources. We then generalize our method to area lights of arbitrary shapes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809763</person_id>
				<author_profile_id><![CDATA[81100127436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kulla]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809764</person_id>
				<author_profile_id><![CDATA[81488655123]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marcos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fajardo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Solid Angle]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>218498</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Veach, E., and Guibas, L. J. 1995. Optimally combining sampling techniques for monte carlo rendering. In <i>Proceedings of the 22nd annual conference on Computer graphics and interactive techniques</i>, ACM, New York, NY, USA, SIGGRAPH '95, 419--428.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1866199</ref_obj_id>
				<ref_obj_pid>1882261</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Yue, Y., Iwasaki, K., Chen, B.-Y., Dobashi, Y., and Nishita, T. 2010. Unbiased, adaptive stochastic sampling for rendering inhomogeneous participating media. <i>ACM Trans. Graph. 29</i> (December), 177:1--177:8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Importance Sampling of Area Lights in Participating Media Christopher Kulla Marcos Fajardo Sony Pictures 
Imageworks Solid Angle (a) (b) (c) (d) (e) Figure 1: Point light source with density sampling (a) vs. 
our method (b). Textured area light with density sampling (c), our method (d), and our method with MIS 
(e). All images use 16 light samples per pixel. We focus on the problem of single scattering in homogeneous 
vol­umes and develop a new importance sampling technique that avoids the singularity near point light 
sources. We then generalize our method to area lights of arbitrary shapes. Previous work in unbiased 
volume rendering [Yue et al. 2010] has concentrated on ef.cient importance sampling of the transmission 
term. In homogeneous media this term is non-zero and smoothly varying, thus the light sources are a much 
greater source of noise. Importance sampling for point light sources The volume ren­dering equation for 
a single point light in a homogeneous medium can be expressed as: b v F -st (t+.+ D2 +t2 ) L(x, .)=sse 
dt (1) D2 + t2 a Figure 2 describes the involved parameters. Note that to simplify the notation, we 
re-parameterize t so that the origin is the orthog­onal projection of the light onto the ray. This change 
modi.es the integration bounds a and b (which can be negative now) and adds an extra term . which is 
the distance between the real origin and the new one. Figure 2: Equi-angular sampling con.guration 
Designing a PDF proportional to the 1/r2 term, we obtain the fol­lowing normalized equation and sampling 
function (. in [0,1)): D pdf(t)= (2) (.b - .a)(D2 +t2) () t(.)= Dtan(1- .).a + ..b(3) Equation 3 reveals 
that this technique makes equal angle steps along the ray. We thus refer to this technique as equi-angular 
sampling. Importance sampling for area lights For area lights, the equa­tion becomes more complex, as 
we have a nested integral over the surface of the light. Most notably, the distance D which was con­stant 
before is now a function of the position sampled from the light. We could make a simplifying assumption 
and choose an arbitrary point like the center of the light to apply the previous equation. While this 
approximation works well for small light sources, it fails as the light becomes larger. The edges of 
the light source become noisier as the D2 term in the PDF starts to dominate. Our key insight is that 
we can distribute the error from the non­constant D by simply using the sample point we will estimate 
ra­diance from as our center for equations 2 and 3. While this does not solve the potential singularity 
caused by sometimes choosing a sampling center too close to the ray, the error is now uniformly distributed 
over the surface of the light, even in challenging cases like stretched rectangular lights, or textured 
area lights. This remaining noise can be masked by applying multiple impor­tance sampling [Veach and 
Guibas 1995] between the area light sur­face and the phase function. As our method has concentrated most 
high variance noise close to the light s surface, MIS can be much more effective than with other line 
sampling distributions (see sup­plemental material for examples). Results Figure 1 shows some sample 
renderings performed with and without our method. Our new sampling equations substantially reduce variance. 
As our method is a generic sampling technique, it can be used to accelerate several light transport algorithms, 
includ­ing path tracing and bidirectional path tracing. References VEACH, E., AND GUIBAS, L. J. 1995. 
Optimally combining sam­pling techniques for monte carlo rendering. In Proceedings of the 22nd annual 
conference on Computer graphics and interac­tive techniques, ACM, New York, NY, USA, SIGGRAPH 95, 419 
428. YUE, Y., IWASAKI, K., CHEN, B.-Y., DOBASHI, Y., AND NISHITA, T. 2010. Unbiased, adaptive stochastic 
sampling for rendering inhomogeneous participating media. ACM Trans. Graph. 29 (December), 177:1 177:8. 
Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 
7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037900</article_id>
		<sort_key>740</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>56</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Decoupled ray-marching of heterogeneous participating media]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037900</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037900</url>
		<abstract>
			<par><![CDATA[<p>We describe the integration of heterogeneous participating media into our production ray tracer. We specifically focus on the problem of computing the lighting without incurring a quadratic number of shader evaluations. The main difficulty in rendering volumes comes from the inability to analytically importance sample the transmission term:</p> <p>[EQUATION]</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809765</person_id>
				<author_profile_id><![CDATA[81100127436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kulla]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>344958</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lokovic, T., and Veach, E. 2000. Deep shadow maps. In <i>Proceedings of the 27th annual conference on Computer graphics and interactive techniques</i>, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, SIGGRAPH '00, 385--392.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74359</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Perlin, K., and Hoffert, E. M. 1989. Hypertexture. <i>SIGGRAPH Comput. Graph. 23</i> (July), 253--262.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1866199</ref_obj_id>
				<ref_obj_pid>1882261</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Yue, Y., Iwasaki, K., Chen, B.-Y., Dobashi, Y., and Nishita, T. 2010. Unbiased, adaptive stochastic sampling for rendering inhomogeneous participating media. <i>ACM Trans. Graph. 29</i> (December), 177:1--177:8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Decoupled Ray-marching of Heterogeneous Participating Media ChristopherKulla SonyPicturesImageworks 
 (a) Brute force ray marching, 5h (b) Decoupled ray marching, 4min (c) Indirect lighting, 10min (d) 
With re.ections, 2min Figure 1: Ray traced heterogeneous volume densities (roughly 100 steps used in 
all images) We describe the integration of heterogeneous participating media into ourproduction raytracer. 
We speci.callyfocus on theproblem of computing the lighting without incurring a quadratic number of shader 
evaluations. The maindif.cultyin rendering volumes comes from the inability to analytically importance 
sample the transmis­sion term: |. -..s............ ..t....(1) The classicalray-marching algorithm[Perlin 
andHoffert1989]has a long history of use in production rendering due to its simplicity. The volume equation 
is solved by marching down the ray in .xed or adaptive steps, evaluating the lighting at each sample. 
Unfortu­nately all shadow rays must themselves be ray marched, leading to aquadratic number of shader 
evaluations. The typical solution to this problem has been to employ light caching techniques such asdeep-shadow 
maps[Lokovic andVeach 2000].Unfortunately thesetechniques requiremultiplepasses,and are prone to many 
sources of artifacts. In contrast, our method re­duces the quadratic complexity without giving up the 
bene.ts of accurate ray tracedlighting. Ray-marching Revisited Webeginby observing thatforhomo­geneous 
media, unbiased solutions are practical. This is due to the fact that equation 1 simpli.es to: .-...s....which 
can be both evaluated and sampled analytically, leading to simple Monte Carlo solutions. These bene.ts 
remain even if we con.ne the homoge­neous media to a region of space, only the normalization constants 
change on the associatedprobability density function(PDF). Our key insight is to observe that we can 
reformulate the task of ray-marching as transforming an unknown, spatially varying vol­umeinto alistofpiecewisehomogeneous 
segments. Thisgives us access to inexpensive analytical formulas for evaluating and sam­pling equation 
1 at arbitrarypoints. Ouralgorithmbeginsby marchingdowntheray in .xed(oradap­tive) steps. We pick a random 
sample point within the segment of each step and run the volume shader. The volume properties .. and 
..are recordedinto an arrayfrom which we canbuild apiece­wise linear PDF proportional to ...t...t.. We 
can then perform lightsampling outside the ray-marchingloop(hence decoupled)by choosingpositions alongthe 
ray usingthisPDF.For eachlight sam­ple we trace shadow rays to the lights, which in turn runs the .rst 
halfof our algorithm to estimate the transmissionfactor(traditional ray marching). As the lighting calculations 
are decoupled, we can also evaluate indirectlighting recursively at a reduced cost. Time Complexity If 
we assume that ray-marching requires ....shader evaluations per ray, the traditional ray marching al­gorithmperforms 
... ..shader evaluations. Our method only re­quires.......evaluations(.being the number oflight sam­ples) 
which is a substantial reduction when .... In fact we typically use ...as we need to trace many primary 
rays for smooth anti-aliasing and motionblur. Bias Naturally our method introduces some bias, however 
it is explicitly controlled by the step size parameter and can be made arbitrarily small. In fact, as 
the step size approaches the Nyquist rate ofthe volume, our algorithm converges on aground truthresult. 
In ourimplementation, the shader controlshow manyray-marching samplesarerequired withintheboundsit occupies 
along theray to allow for volumes of different frequency contents to be mixed in the same scene. We alsopoint 
out that thehomogeneous limit case is handled without bias as a single ray-marching sample captures the 
volumeproperties exactly. In contrast to unbiasedintegration schemes[Yue et al.2010] that use rejection 
sampling to invert equation 1, we do not require any knowledge of upper bounds on volume properties which 
is essen­tialtodeal withprocedural and out-of-core volume representations. Moreover, since our PDF is 
proportional to ...t...t.instead of ..t., it does not waste any samples on regions that do not scatter 
light. References LOKOVIC, T., AND VEACH,E. 2000. Deep shadow maps. In Pro­ceedings of the 27th annual 
conference on Computer graphics and interactive techniques, ACM Press/Addison-Wesley Pub­lishingCo.,NewYork,NY,USA,SIGGRAPH 
00,385 392. PERLIN, K., AND HOFFERT, E. M. 1989. Hypertexture. SIG-GRAPH Comput. Graph. 23 (July),253 
262. YUE, Y., IWASAKI, K., CHEN, B.-Y., DOBASHI, Y., AND NISHITA, T. 2010. Unbiased, adaptive stochastic 
sampling for rendering inhomogeneous participating media. ACM Trans. Graph. 29 (December), 177:1 177:8. 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037901</article_id>
		<sort_key>750</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>57</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Demand-driven volume rendering of terascale EM data]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037901</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037901</url>
		<abstract>
			<par><![CDATA[<p>In neuroscience, a very promising bottom-up approach to understanding how the brain works is built on acquiring and analyzing electron microscopy (EM) scans of brain tissue, an area known as <i>Connectomics</i>. This results in volume data of extremely high resolution of 3--5nm per pixel and 25--50nm slice thickness, overall leading to data sizes of many terabytes [Jeong et al. 2010]. To support the work of neurobiologists, interactive exploration and analysis of such volumes requires novel visual computing systems because the requirements differ from those of current systems in several key aspects. In this talk, we describe the system that we are working on to enable neuroscientists to interactively roam terascale EM volumes and support their analysis. A major design principle was to avoid the standard approach of pre-computing a 3D multi-resolution hierarchy such as an octree. Data acquisition proceeds from 2D image tile to 2D image tile, where not only the slices along the z axis are scanned independently, but each slice is itself acquired as many smaller image tiles. These images tiles need to be aligned and stitched, and neurobiologists also want to be able to combine different resolutions used for scanning different regions, without re-sampling everything to a single global resolution. Therefore, we focus on working directly with a stream of individual 2D image tiles, instead of a 3D volume that usually is assumed to exist in its entirety for visualization. We perform interactive volume rendering of a "virtual" volume, where the corresponding physical storage is only represented and populated in a sparse manner with 2D instead of 3D image data on the fly during rendering. Furthermore, these 2D image tiles can be of different resolution, scale, and orientation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809766</person_id>
				<author_profile_id><![CDATA[81339490397]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Johanna]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Beyer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAUST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809767</person_id>
				<author_profile_id><![CDATA[81100644757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Markus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hadwiger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAUST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809768</person_id>
				<author_profile_id><![CDATA[81100058208]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Won-Ki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jeong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809769</person_id>
				<author_profile_id><![CDATA[81100199891]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hanspeter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pfister]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809770</person_id>
				<author_profile_id><![CDATA[81384602018]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jeff]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lichtman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1507152</ref_obj_id>
				<ref_obj_pid>1507149</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Crassin, C., Neyret, F., Lefebvre, S., and Eisemann, E. 2009. GigaVoxels: Ray-Guided Streaming for Efficient and Detailed Voxel Rendering. In <i>Proceedings of 2009 Symposium on Interactive 3D Graphics and Games</i>, 15--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1394367</ref_obj_id>
				<ref_obj_pid>1394332</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Gobbetti, E., Marton, F., and Guitan, J. 2008. A Single-pass GPU Ray Casting Framework for Interactive Out-of-core Rendering of Massive Volumetric Datasets. <i>The Visual Computer 24</i>, 7, 797--806.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1803964</ref_obj_id>
				<ref_obj_pid>1803928</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Jeong, W.-K., Beyer, J., Hadwiger, M., Blue, R., Law, C., Vasquez, A., Reid, C., Lichtman, J., and Pfister, H. 2010. SSECRETT and Neuro-Trace: Interactive Visualization and Analysis Tools for Large-Scale Neuroscience Datasets. <i>IEEE Computer Graphics and Applications 30</i>, 3, 58--70.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Demand-Driven Volume Rendering of Terascale EM Data Johanna Beyer, Markus Hadwiger Won-Ki Jeong, Hanspeter 
P.ster, Jeff Lichtman KAUST Harvard University 1 Introduction In neuroscience, a very promising bottom-up 
approach to under­standing how the brain works is built on acquiring and analyzing electron microscopy 
(EM) scans of brain tissue, an area known as Connectomics. This results in volume data of extremely high 
reso­lution of 3-5nm per pixel and 25-50nm slice thickness, overall lead­ing to data sizes of many terabytes 
[Jeong et al. 2010]. To support the work of neurobiologists, interactive exploration and analysis of 
such volumes requires novel visual computing systems because the requirements differ from those of current 
systems in several key as­pects. In this talk, we describe the system that we are working on to enable 
neuroscientists to interactively roam terascale EM volumes and support their analysis. A major design 
principle was to avoid the standard approach of pre-computing a 3D multi-resolution hier­archy such as 
an octree. Data acquisition proceeds from 2D im­age tile to 2D image tile, where not only the slices 
along the z axis are scanned independently, but each slice is itself acquired as many smaller image tiles. 
These images tiles need to be aligned and stitched, and neurobiologists also want to be able to combine 
different resolutions used for scanning different regions, without re-sampling everything to a single 
global resolution. Therefore, we focus on working directly with a stream of individual 2D image tiles, 
instead of a 3D volume that usually is assumed to exist in its entirety for visualization. We perform 
interactive volume rendering of a virtual volume, where the corresponding physical storage is only represented 
and populated in a sparse manner with 2D instead of 3D image data on the .y during rendering. Furthermore, 
these 2D image tiles can be of different resolution, scale, and orientation. 2 Demand-Driven Volume 
Rendering In order to enable 3D volume rendering in the described scenario, we use a GPU ray-caster that 
renders a virtual octree . The phys­ical data contained in this octree are only computed on demand, driven 
by the actual visibility and the current screen-projection size of the volume data. These parameters 
are detected on the .y during ray-casting and drive the actual data reconstruction from a stream of 2D 
image tiles, which is performed by a .exible demand-driven data back-end that runs decoupled from the 
ray-caster in a sepa­rate background thread. The back-end can also accommodate data produced by continuously 
scanning microscopes in a progressive fashion while rendering. The ray-caster drives data reconstruction 
in a display-aware fashion, by detecting cache misses of virtual oc­tree nodes that are required for 
front-to-back traversal but are not yet resident in the GPU cache. This is illustrated in Figure 1. If 
a cache miss cannot be satis.ed from the larger cache in CPU mem­ory, the physical data of matching resolution 
are reconstructed on the .y in the background, while rendering proceeds at interactive rates. Determining 
the octree nodes for reconstruction in this man­ner signi.cantly bounds the actual cache working set 
required for volume rendering because even high display resolutions are much smaller than our data sets. 
In this way, the required resolution for reconstruction in the data back-end is determined directly by 
the output display and is independent of the volume resolution itself. To achieve interactive frame rates, 
we use a multi-level paged vir­tual memory management system for reconstructed octree nodes as well as 
image tiles, both in GPU memory as well as in CPU mem­ory. In between these cache levels, we page data 
on demand. In addition to rendering a virtual octree that is populated only on de­mand, another major 
difference to other octree volume rendering Figure 1: Demand-driven reconstruction for sparsely populating 
the virtual octree from individual 2D image tiles during rendering. systems [Gobbetti et al. 2008; Crassin 
et al. 2009] is that we use a simpler and faster scheme for volume traversal, which builds on arbitrary 
direct access to the full virtual octree in virtual memory. During ray-casting, all sample positions 
are computed in virtual volume space, which are translated on the .y through a two-level page table hierarchy. 
Only a very small top-level page directory is fully resident in GPU memory. The current working set of 
the 2nd­level page table resides in a small page table cache, whose entries further refer to physical 
octree nodes, only a small subset of which is resident in the physical cache texture. Performing look-ups 
in this page table hierarchy is extremely fast when spatial coherence along rays is exploited. Whenever 
a required octree node is not physically present, the ray-caster generates a cache miss. We use a small 
size of 323 physical voxels for each octree node, which achieves very good culling and update granularity. 
However, our system still scales to very large volumes because of the page table hierarchy. For example, 
we can render a 1TB volume with a top­level page table of 48 KB, a 2nd-level page table cache of 32 MB, 
and a physical texture cache with sizes from 1 GB on a single GPU to a total combined size of several 
gigabytes on multiple GPUs. We currently render three large EM volumes acquired by the Har­vard Center 
for Brain Science: mouse cortex (21, 494 × 25, 790 × 1, 850 = 955 GB), and two mouse hippocampus volumes 
(43 GB and 92 GB, respectively). Rendering is fully interactive and hides the latency of data reconstruction, 
because the ray-caster can sub­stitute missing data with data of lower resolution and reconstruction 
proceeds simultaneously in the background. In the background, getting a 3D block of size 512 × 512 × 
32 from the demand-driven data back-end can be as fast as 30ms when the required 2D image tiles are already 
in memory, but may take up to a few seconds if all images still have to be read from disk. However, interactive 
roam­ing is always possible, which is crucial for neurobiologists. We are currently focusing on reducing 
the latency in the back-end itself. References CRASSIN, C., NEYRET, F., LEFEBVRE, S., AND EISEMANN, 
E. 2009. GigaVoxels : Ray-Guided Streaming for Ef.cient and Detailed Voxel Rendering. In Proceedings 
of 2009 Symposium on Interactive 3D Graphics and Games, 15 22. GOBBETTI, E., MARTON,F., AND GUITAN,J.2008.ASingle-passGPURayCasting 
Framework for Interactive Out-of-core Rendering of Massive Volumetric Datasets. The Visual Computer 24, 
7, 797 806. JEONG, W.-K., BEYER, J., HADWIGER, M., BLUE, R., LAW, C., VASQUEZ, A., REID, C., LICHTMAN, 
J., AND PFISTER, H. 2010. SSECRETT and Neuro-Trace: Interactive Visualization and Analysis Tools for 
Large-Scale Neuroscience Datasets. IEEE Computer Graphics and Applications 30, 3, 58 70. Copyright is 
held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. 
ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037902</section_id>
		<sort_key>760</sort_key>
		<section_seq_no>19</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Heads or tails]]></section_title>
		<section_page_from>19</section_page_from>
	<article_rec>
		<article_id>2037903</article_id>
		<sort_key>770</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>58</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Optimized local blendshape mapping for facial motion retargeting]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037903</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037903</url>
		<abstract>
			<par><![CDATA[<p>One of the popular methods for facial motion retargeting is local blendshape mapping [Pighin and Lewis 2006], where each local facial region is controlled by a tracked feature (for example, a vertex in motion capture data). To map a target motion input onto blendshapes, a pose set is chosen for each facial region with minimal retargeting error. However, since the best pose set for each region is chosen independently, the solution likely has unorganized pose sets across the face regions, as shown in Figure 1(b). Therefore, even though every pose set matches the local features, the retargeting result is not guaranteed to be spatially smooth. In addition, previous methods ignored temporal coherence which is key for jitter-free results.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809771</person_id>
				<author_profile_id><![CDATA[81100447998]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wan-Chun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809772</person_id>
				<author_profile_id><![CDATA[81421597317]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Graham]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fyffe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809773</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1185842</ref_obj_id>
				<ref_obj_pid>1185657</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Pighin, F., and Lewis, J. P. 2006. Facial motion retargeting. In <i>SIGGRAPH Courses</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>779352</ref_obj_id>
				<ref_obj_pid>779343</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Yedidia, J., Freeman, W., and Weiss, Y. 2003. <i>Understanding Belief Propagation and Its Generalizations</i>. ch. 8, 239--236]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Optimized Local Blendshape Mapping for Facial Motion Retargeting Wan-Chun Ma Graham Fy.e Paul Debevec 
USC Institute for Creative Technologies  (a) (b) (c) (d) Figure 1: Optimized Local Blendshape Mapping. 
(a) Retargeted result from the previous technique. (b) A visualization of the pose sets that are used 
in (a). Each color represents a di.erent pose set. (c) Retargeted result from the proposed technique. 
(d) A visualization of the pose sets that are used in (c). 1 Concept One of the popular methods for facial 
motion retargeting is local blendshape mapping [Pighin and Lewis 2006], where each local facial region 
is controlled by a tracked feature (for example, a vertex in motion capture data). To map a target motion 
input onto blendshapes, a pose set is chosen for each facial region with minimal retargeting error. However, 
since the best pose set for each region is chosen independently, the solution likely has unorganized 
pose sets across the face regions, as shown in Figure 1(b). Therefore, even though every pose set matches 
the local features, the retargeting result is not guaranteed to be spatially smooth. In addition, previous 
methods ignored temporal coherence which is key for jitter-free results. In order to deal with these 
problems, we consider the facial motion retargeting algorithm as an optimization problem which takes 
the following criteria into consideration: 1. The number of poses that are (fully or partially) used 
to represent the current shape should be minimal. 2. The pose sets of all tracked features should vary 
smoothly across both spatial and temporal domains. 3. The retargeting error should be as small as possible. 
  2 Optimization We formulate the criteria as the following cost function, which we solve using belief 
propagation [Yedidia et al. 2003]: D min F(T )=Ri(ti)+ ktS(ti,ti)+ ksS(ti,tj ). T i.V {i,j}.E V and 
E are the set of vertices and edges respectively. T = {ti|1 = i = nv} is a con.guration of facial deforma­tion; 
ti describes a pose set that is associated with a vertex vi. kt and ks are weighting factor of temporal 
and spatial smoothness. We assume that local deformation around a vertex can be represented by a small 
number of blendshape poses. Therefore, we de.ne each pose set as a vector of three i ii pose indices 
ti =(p0,p 1,p 2). Given a pose set, each local patch can be approximated as a convex linear combination 
of corresponding local patches . i(ti) from the three poses in the set. We set p i 0 be always neutral 
pose, and therefore np-1 there are C2 possible pose sets where np is the number of blendshape poses. 
We then regularize the pose sets using the following terms. The retargeting error term: Ri is the sum 
of distances between all local patch vertices of the target shape .i around vi and the reconstructed 
shape . i(ti) based on a pose set ti: Ri(ti)= .i - . i(ti) 2 We compute the convex linear combination 
weights which minimizes Ri by quadratic programming. We use the re­sulting weights later on to blend 
the .nal result. D The smoothness terms: S(ti,ti) describes the temporal cost of assigning a pose set 
ti to vi. It is the Hamming distance between the current pose set ti and the pose set D t computed in 
the previous frame. Similarly, the spatial i smoothness term S(ti,tj ) is the Hamming distance between 
the two pose sets ti and tj of adjacent features vi and vj . 3 Conclusion Our technique is able to deliver 
better visual quality over traditional local blendshape mapping methods. It computes facial motion retargeting 
for blendshapes without any prior knowledge of facial segmentation, which is required for most of the 
blendshape retargeting methods. The number of blendshape poses that are used for rendering a frame is 
re­duced hence computing resources is saved. References Pighin, F., and Lewis, J. P. 2006. Facial motion 
retar­geting. In SIGGRAPH Courses. Yedidia, J., Freeman, W., and Weiss, Y. 2003. Under­ standing Belief 
Propagation and Its Generalizations. ch. 8, 239 236. Copyright is held by the author / owner(s). SIGGRAPH 
2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037904</section_id>
		<sort_key>780</sort_key>
		<section_seq_no>20</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Speed of light]]></section_title>
		<section_page_from>20</section_page_from>
	<article_rec>
		<article_id>2037905</article_id>
		<sort_key>790</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>59</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Runtime implementation of modular radiance transfer]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037905</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037905</url>
		<abstract>
			<par><![CDATA[<p>Real-time rendering of indirect lighting significantly enhances the sense of realism in video games. Unfortunately, previously including such effects often required time consuming scene dependent precomputation and heavy runtime computations unsuitable for low-end devices, such as mobile phones or game consoles. Modular Radiance Transfer (MRT) [Loos et al. 2011] is a recent technique that computes approximate direct-to-indirect transfer [Ha&#353;an et al. 2006; Kontkanen et al. 2006; Lehtinen et al. 2008] by warping and combining light transport, in real-time, from a small library of simple shapes. This talk focusses on implementation issues of the MRT technical paper, including how our run time is designed to scale across many different platforms, from iPhones to modern GPUs.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809774</person_id>
				<author_profile_id><![CDATA[81456623904]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brad]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Loos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809775</person_id>
				<author_profile_id><![CDATA[81444606031]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Lakulish]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Antani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UNC Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809776</person_id>
				<author_profile_id><![CDATA[81490663992]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kenny]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitchell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Interactive Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809777</person_id>
				<author_profile_id><![CDATA[81319498527]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Derek]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nowrouzezahrai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research Z&#252;rich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809778</person_id>
				<author_profile_id><![CDATA[81100389194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Wojciech]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jarosz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research Z&#252;rich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809779</person_id>
				<author_profile_id><![CDATA[81100524617]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Peter-Pike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sloan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Interactive Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1141998</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ha&#353;an, M., Pellacini, F., and Bala, K. 2006. Direct-to-indirect transfer for cinematic relighting. <i>ACM Trans. Graph.</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383915</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kontkanen, J., Turquin, E., Holzschuch, N., and Sillion, F. 2006. Wavelet radiance transport for interactive indirect lighting. In <i>EGSR</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360636</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lehtinen, J., Zwicker, M., Turquin, E., Kontkanen, J., Durand, F., Sillion, F. X., and Aila, T. 2008. A meshless hierarchical representation for light transport. <i>TOG</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Loos, B., Antani, L., Mitchell, K., Nowrouzezahrai, D., Jarosz, W., and Sloan, P.-P. 2011. Modular radiance transfer. <i>Disney Technical Report</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Runtime Implementation of Modular Radiance Transfer Brad Loos1 Lakulish Antani2 Kenny Mitchell3 Derek 
Nowrouzezahrai4 Wojciech Jarosz4 Peter-Pike Sloan3 1University of Utah 2UNC Chapel Hill 3Disney Interactive 
Studios 4Disney Research Z¨urich Figure 1: Indirect light computed for a 19 block maze-like cave scene 
constructed completely at run-time. Our algorithm can model color bleeding and indirect light from surfaces 
with detailed normal variation and real-time performance. Timings are on an NVidia 480 GTX. 1 Introduction 
Real-time rendering of indirect lighting signi.cantly enhances the sense of realism in video games. Unfortunately, 
previously includ­ing such effects often required time consuming scene dependent precomputation and heavy 
runtime computations unsuitable for low­end devices, such as mobile phones or game consoles. Modular 
Radiance Transfer (MRT) [Loos et al. 2011] is a recent technique that computes approximate direct-to-indirect 
transfer [Hasan et al. . 2006; Kontkanen et al. 2006; Lehtinen et al. 2008] by warping and combining 
light transport, in real-time, from a small library of sim­ple shapes. This talk focusses on implementation 
issues of the MRT technical paper, including how our run time is designed to scale across many different 
platforms, from iPhones to modern GPUs. 2 Modular Radiance Transfer Overview MRT is a novel technique 
for approximate direct-to-indirect trans­fer. The key contributions are the use of a lighting prior to 
reduce the dimension/entropy of the one-bounce light transport operator, approximating a scene with simpler 
shapes, and ef.ciently com­puting indirect lighting on these simpler shapes by expressing both self transfer 
and propagation between shapes using carefully con­structed reduced-dimensional spaces. We achieve high 
performance, across a wide range of platforms, with dynamically generated scenes (see [Loos et al. 2011] 
for more details). We compute indirect lighting as lind = Ub b + Ur r , where b = Td.b ld are the spectral 
indirect lighting coef.cients, and r = Tb.r b are reduced light.eld response coef.cients. The Td.b operator 
maps direct light (ld) to reduced indirect light coef.cients and Tb.r maps indirect light at all the 
scene blocks to reduced dimensional light.elds at the interfaces of these blocks. Ub is a compact basis 
for indirect light inside a shape, and Ur is a compact basis for mapping light .owing through a light.eld 
to response from a shape to its neighbors. 3 Run Time Algorithm Overview The core run time steps for 
MRT are as follows: 1. Compute direct light at a reduced resolution in each block (ld), 2. Compute per-block 
spectral coef.cients (b = Td.b ld), 3. Compute indirect light within a block into a lightmap (Ub b), 
 4. Compute response coef.cients at interfaces (r = Tb.r b),  5. Blend response from external blocks 
into a lightmap (Ur r), 6. Pad the lightmap texture to avoid texture mapping artifacts, 7. Render scene 
using the dynamic lightmaps of indirect lighting, 8. [optional] Compute indirect light volume (from 
b s and r s), 9. [optional] Render dynamic objects the volume lighting.  We describe two run times: 
a DX11 version entirely on the GPU, and a CPU version for low-end platforms (iPhones and iPads). 4 Implementation 
Details There are several ways to map the computation to the GPU, in­volving trade-offs in the number 
of passes, data layout in memory, execution kernels scheduling, etc. On low-end devices the GPUs are 
underpowered (e.g., a single unshadowed point-light evaluation costs 25ms on the iPad) and often missing 
important features such as high precision texture formats. This motivated our CPU-focused implementation 
for those platforms. The differences between GPU and CPU architectures lead to differ­ent solutions. 
In particular, we transformed the GPU data layout used on higher-end platforms to a CPU friendly one, 
optimizing for better cache usage and proper exploitation of CPU vector instructions. 5 Conclusion MRT 
is a compelling, real-time approximate global illumination technique. This talk discusses implementation 
details that allow the algorithm to scale across different hardware platforms with wildly varying capabilities. 
We believe some of the techniques employed by MRT may be useful to developers facing similar scaling 
challenges. References HA.SAN, M., PELLACINI, F., AND BALA, K. 2006. Direct-to-indirect transfer for 
cinematic relighting. ACM Trans. Graph.. KONTKANEN, J., TURQUIN, E., HOLZSCHUCH, N., AND SILLION, F. 
2006. Wavelet radiance transport for interactive indirect lighting. In EGSR. LEHTINEN, J., ZWICKER, M., 
TURQUIN, E., KONTKANEN, J., DU-RAND, F., SILLION, F. X., AND AILA, T. 2008. A meshless hierar­chical 
representation for light transport. TOG. LOOS, B., ANTANI, L., MITCHELL, K., NOWROUZEZAHRAI, D., JAROSZ, 
W., AND SLOAN, P.-P. 2011. Modular radiance transfer. Disney Technical Report. Copyright is held by the 
author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037906</article_id>
		<sort_key>800</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>60</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Next generation image based lighting using HDR video]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037906</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037906</url>
		<abstract>
			<par><![CDATA[<p>We present an overview of our recently developed systems pipeline for capture, reconstruction, modeling and rendering of real world scenes based on state-of-the-art high dynamic range video (HDRV). The reconstructed scene representation allows for photo-realistic Image Based Lighting (IBL) in complex environments with strong spatial variations in the illumination. The pipeline comprises the following essential steps:</p> <p><b>1.) Capture</b> - The scene capture is based on a 4MPixel global shutter HDRV camera with a dynamic range of more than 24 f-stops at 30 fps. The HDR output stream is stored as individual un-compressed frames for maximum flexibility. A scene is usually captured using a combination of panoramic light probe sequences [1], and sequences with a smaller field of view to maximize the resolution at regions of special interest in the scene. The panoramic sequences ensure full angular coverage at each position and guarantee that the information required for IBL is captured. The position and orientation of the camera is tracked during capture.</p> <p><b>2.) Scene recovery</b> - Taking one or more HDRV sequences as input, a geometric proxy model of the scene is built using a semi-automatic approach. First, traditional computer vision algorithms such as structure from motion [2] and Manhattan world stereo [3] are used. If necessary, the recovered model is then modified using an interaction scheme based on visualizations of a volumetric representation of the scene radiance computed from the input HDRV sequence. The HDR nature of this volume also enables robust extraction of direct light sources and other high intensity regions in the scene.</p> <p><b>3.) Radiance processing</b> - When the scene proxy geometry has been recovered, the radiance data captured in the HDRV sequences are re-projected onto the surfaces and the recovered light sources. Since most surface points have been imaged from a large number of directions, it is possible to reconstruct view dependent texture maps at the proxy geometries. These 4D data sets describe a combination of detailed geometry that has not been recovered and the radiance reflected from the underlying real surfaces. The view dependent textures are then processed and compactly stored in an adaptive data structure.</p> <p><b>4.) Rendering</b> - Once the geometric and radiometric scene information has been recovered, it is possible to place virtual objects into the real scene and create photo-realistic renderings as illustrated above. The extracted light sources enable efficient sampling and rendering times that are fully comparable to that of traditional virtual computer graphics light sources. No previously described method is capable of capturing and reproducing the angular and spatial variation in the scene illumination in comparable detail.</p> <p>We believe that the rapid development of high quality HDRV systems will soon have a large impact on both computer vision and graphics. Following this trend, we are developing theory and algorithms for efficient processing HDRV sequences and using the abundance of radiance data that is going to be available.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809780</person_id>
				<author_profile_id><![CDATA[81100120690]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jonas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Unger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Link&#246;ping University, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809781</person_id>
				<author_profile_id><![CDATA[81320490266]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Stefan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gustavson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Link&#246;ping University, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809782</person_id>
				<author_profile_id><![CDATA[81488647582]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Joel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kronander]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Link&#246;ping University, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809783</person_id>
				<author_profile_id><![CDATA[81485650017]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Per]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Larsson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Link&#246;ping University, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809784</person_id>
				<author_profile_id><![CDATA[81448595537]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Gerhard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bonnet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SpheronVR, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809785</person_id>
				<author_profile_id><![CDATA[81488659528]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Gunnar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaiser]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SpheronVR, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>280864</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[P. Debevec: <i>Rendering synthetic objects into real scenes: bridging traditional and image-based graphics with global illumination and high dynamic range photography</i>. In SIGGRAPH 98 Papers, ACM, New York, USA, 189--198, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>685629</ref_obj_id>
				<ref_obj_pid>646271</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[B. Triggs, P. Mclauchlan, R. Hartley, and A. Fitzgibbon: <i>Bundle adjustment -- a modern synthesis, Vision Algorithms: Theory and Practice</i>, LNCS, 298--375, Springer Verlag, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Y. Furukawa, B. Curless, S. M. Seitz and R. Szeliski: <i>Manhattan world stereo</i>, IEEE Conference on Computer Vision and Pattern Recognition, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Next Generation Image Based Lighting using HDR Video Jonas Unger, Stefan Gustavson, Joel Kronander and 
Per Larsson, Linköping University, Sweden Gerhard Bonnet and Gunnar Kaiser, SpheronVR, Germany  Figure 
1: Left: synthetic scene rendered with traditional image based lighting, using a single representative 
light probe for illumination. Right: the same scene, rendered with scene data extracted from tens of 
thousands of light probes to accurately capture the complex spatial variation in the illumination. We 
present an overview of our recently developed systems pipeline for capture, reconstruction, modeling 
and rendering of real world scenes based on state-of-the-art high dynamic range video (HDRV). The reconstructed 
scene representa­tion allows for photo-realistic Image Based Lighting (IBL) in complex environments with 
strong spatial variations in the illumination. The pipeline comprises the following essential steps: 
1.) Capture -The scene capture is based on a 4MPixel global shutter HDRV camera with a dynamic range 
of more than 24 f-stops at 30 fps. The HDR output stream is stored as individual un-compressed frames 
for maximum flexibility. A scene is usually captured using a combination of pano­ramic light probe sequences 
[1], and sequences with a smaller field of view to maximize the resolution at regions of special interest 
in the scene. The panoramic sequences ensure full angular coverage at each position and guarantee that 
the information required for IBL is captured. The posi­tion and orientation of the camera is tracked 
during capture. 2.) Scene recovery -Taking one or more HDRV sequences as input, a geometric proxy model 
of the scene is built using a semi-automatic approach. First, traditional computer vi­sion algorithms 
such as structure from motion [2] and Man­hattan world stereo [3] are used. If necessary, the reco­vered 
model is then modified using an interaction scheme based on visualizations of a volumetric representation 
of the scene radiance computed from the input HDRV sequence. The HDR nature of this volume also enables 
robust extrac­tion of direct light sources and other high intensity regions in the scene. 3.) Radiance 
processing -When the scene proxy geo­metry has been recovered, the radiance data captured in the HDRV 
sequences are re-projected onto the surfaces and the recovered light sources. Since most surface points 
have been imaged from a large number of directions, it is possible to reconstruct view dependent texture 
maps at the proxy geometries. These 4D data sets describe a combination of detailed geometry that has 
not been recovered and the radiance reflected from the underlying real surfaces. The view dependent textures 
are then processed and compactly stored in an adaptive data structure. 4.) Rendering -Once the geometric 
and radiometric scene information has been recovered, it is possible to place virtual objects into the 
real scene and create photo-realistic renderings as illustrated above. The extracted light sources enable 
efficient sampling and rendering times that are fully comparable to that of traditional virtual computer 
graphics light sources. No previously described method is capable of capturing and reproducing the angular 
and spatial variation in the scene illumination in comparable detail. We believe that the rapid development 
of high quality HDRV systems will soon have a large impact on both computer vision and graphics. Following 
this trend, we are developing theory and algorithms for efficient processing HDRV sequences and using 
the abundance of radiance data that is going to be available. References [1] P. Debevec: Rendering synthetic 
objects into real scenes: bridging traditional and image-based graphics with global illumination and 
high dynamic range photography. In SIGGRAPH 98 Papers, ACM, New York, USA, 189 198, 1998. [2] B. Triggs, 
P. Mclauchlan, R. Hartley, and A. Fitzgibbon: Bundle adjustment a modern synthesis, Vision Algorithms: 
Theory and Practice, LNCS, 298-375, Springer Verlag, 2000. [3] Y. Furukawa, B. Curless, S. M. Seitz and 
R. Szeliski: Manhattan world stereo, IEEE Conference on Computer Vision and Pattern Recognition, 2009. 
 Figure 2: A scene is captured using as a set of HDRV sequences and processed into a scene representation 
including a proxy geometry and scene radiance information. This enables photo-realistic rendering of 
virtual objects placed into real scenes exhibiting complex variations in the lighting environment. Copyright 
is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. 
ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037907</article_id>
		<sort_key>810</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>61</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Triple depth culling]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037907</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037907</url>
		<abstract>
			<par><![CDATA[<p>Virtual worlds feature increasing geometric and shading complexities, resulting in a constant need for effective solutions to avoid rendering objects invisible for the viewer. This observation is particularly true in the context of real-time rendering of highly occluded environments such as urban areas, landscapes or indoor scenes. This problem has been intensively researched in the past decades, resulting in numerous optimizations building upon the well-known Z-buffer technique. Among them, extensions of graphics hardware such as early Z-culling [Morein 2000] efficiently avoid shading most of invisible fragments. However, this technique is not applicable when the fragment shader discards fragments or modifies their depth value, or if alpha testing is enabled [nVidia 2008].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809786</person_id>
				<author_profile_id><![CDATA[81100029219]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jean-Eudes]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marvie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technicolor Research & Innovation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jean-eudes.marvie@technicolor.com]]></email_address>
			</au>
			<au>
				<person_id>P2809787</person_id>
				<author_profile_id><![CDATA[81100339836]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gautron]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technicolor Research & Innovation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pascal.gautron@technicolor.com]]></email_address>
			</au>
			<au>
				<person_id>P2809788</person_id>
				<author_profile_id><![CDATA[81481650476]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ga&#235;l]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sourimant]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technicolor Research & Innovation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[gael.sourimant@technicolor.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Morein, S. 2000. ATI Radeon Hyper-Z technology. In <i>ACM SIGGRAPH/Eurographics workshop on graphics hardware</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[nVidia. 2008. GPU programming guide version for GeForce 8 and later GPUs.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Triple Depth Culling Jean-Eudes Marvie Pascal Gautron Ga¨el Sourimant {jean-eudes.marvie, pascal.gautron, 
gael.sourimant}@technicolor.com Technicolor Research &#38; Innovation Figure 1: Triple Depth Culling 
introduces controllable early depth testing for highly complex scenes with arbitrary shader operations. 
Virtual worlds feature increasing geometric and shading complexi­ties, resulting in a constant need for 
effective solutions to avoid ren­dering objects invisible for the viewer. This observation is partic­ularly 
true in the context of real-time rendering of highly occluded environments such as urban areas, landscapes 
or indoor scenes. This problem has been intensively researched in the past decades, resulting in numerous 
optimizations building upon the well-known Z-buffer technique. Among them, extensions of graphics hardware 
such as early Z-culling [Morein 2000] ef.ciently avoid shading most of invisible fragments. However, 
this technique is not applica­ble when the fragment shader discards fragments or modi.es their depth 
value, or if alpha testing is enabled [nVidia 2008]. We introduce Triple Depth culling for fast and controllable 
per­pixel visibility at the fragment shading stage using multiple depth buffers. Based on alternate rendering 
of object batches, our method effectively avoids the shading of hidden fragments in a single pass, hence 
reducing the overall rendering costs. Our approach pro­vides an effective control on how culling is performed 
prior to shading, regardless of potential discard or alpha testing operations. Triple Depth culling is 
also complementary with the existing culling stages of graphics hardware, making our method easily integrable 
as an additional stage of the graphics pipeline. 1 Triple Depth Culling Our algorithm builds upon the 
Z-Buffer approach, in which a fragment is kept or discarded after shading by testing its depth against 
the current depth stored in the depth buffer. Early Z-culling performs this elimination beforehand, but 
involves restrictions on shading operations, or requires a depth-only prepass to populate the depth buffer 
[nVidia 2008]. Conversely, our method works in a single pass by introducing a depth culling step between 
the early Z-culling and the shading of fragments (Figure 2). Starting with a roughly depth-sorted list 
of objects batches, a .rst batch is rendered into a classical RGBA buffer Z1 and a depth buffer. For 
each fragment we store its depth into the alpha chan­nel of Z1 (or an additional render target). Then, 
upon rendering of a second batch of objects into Z2, each fragment undergoes the following steps: its 
depth is .rst compared to the corresponding al­pha value in Z1. If the fragment is occluded by the .rst 
batch of objects, its output is simply discarded. Otherwise, the fragment is shaded and stored. The remainder 
of the object batches is then ren­dered using alternatively Z1 and Z2 as render targets. Note that as 
the objects are sorted according to the viewing distance the batches tend to spread over the entire image 
space, increasing the screen coverage of each batch and hence the ef.ciency of our method. An overhead 
of our method lies in the alternation of render buffers, which tends to generate pipeline stalls. We 
amortize the cost by adjusting the size of the object batches. This size can be adjusted Figure 2: Triple 
Depth culling is an additional, programmable depth culling step at the fragment shader stage. While rendering 
into Z2, the culling is performed using the partial depth informa­tion available in Z1 (step 1). Z1 and 
Z2 are then swapped, hence maintaining information in both buffers (step 2). either manually or automatically 
using a simple convergence based on the render time of the last frame. While the overhead does not completely 
vanish, the savings are signi.cant especially in scenes containing many objects. Our technique allows 
the shader to determine whether the shading must be carried out, depending on its depth output. The expen­sive 
shading is then performed only if the fragment is determined as visible. Note that this visibility determination 
is approximate: as each of the additional depth buffers holds only a part of the ren­dered fragments, 
some fragments may be erroneously considered as visible. While this results in unnecessary computations, 
this also ensures the conservativeness of our visibility algorithm and does not introduce artifacts as 
the corresponding fragments eventually get discarded after shading by classical depth testing. 2 Results 
Our method has been implemented within fragment shaders on an nVidia GeForce GTX480. The presented scenes 
contains 15K and 50K objects with complex shader operations rendered at a resolu­tion of 1280×720. Compared 
to classical Z-buffering, Triple Depth culling provides performance increases of 8 to 50% using batches 
of 50 objects, while remaining applicable in any context. We be­lieve further performance could be achieved 
by implementing our approach in an additional stage in future graphics hardware for pro­grammable fragment 
elimination, potentially taking advantage of the hierarchical representation of depth buffers. Scene 
# tri # obj Z Early Triple Depth Forest 70M 50K 90 ms N/A 44.5 ms Asteroids 35M 15K 215 ms N/A 198 ms 
 References MOREIN, S. 2000. ATI Radeon Hyper-Z technology. In ACM SIGGRAPH/Eurographics workshop on 
graphics hardware. NVIDIA. 2008. GPU programming guide version for GeForce 8 and later GPUs. Copyright 
is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. 
ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037908</article_id>
		<sort_key>820</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>62</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Non-uniform motion deblurring for camera shakes using image registration]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037908</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037908</url>
		<abstract>
			<par><![CDATA[<p>This paper presents a novel blind motion deblurring method for dealing with non-uniform blurs caused by camera shakes. While there are recent works for non-uniform motion deblurring [Whyte et al. 2010; Joshi et al. 2010], those approaches either limit the freedom of camera motions or require special hardware. Our method is based on a novel representation of motion blurs, which models the blur effects using a set of homographies [Tai et al. to appear]. This representation can fully describe the motions of camera shakes in 3D world, which cause non-uniform motion blurs. Our main contribution is the blind motion deblurring algorithm associated with this representation. We solve the ill-posed non-uniform point spread function (PSF) estimation problem by transforming it into a well-posed image registration problem that estimates homographies. To improve the robustness of our method, we use two input images for deblurring. Our method is experimented with both synthetic and real world examples, and produces superior deblurring results compared to previous methods.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809789</person_id>
				<author_profile_id><![CDATA[81448593617]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sunghyun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[POSTECH]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sodomau@postech.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P2809790</person_id>
				<author_profile_id><![CDATA[81488663916]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hojin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[POSTECH]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[nucL23@postech.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P2809791</person_id>
				<author_profile_id><![CDATA[81100311445]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yu-Wing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yuwing@cs.kaist.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P2809792</person_id>
				<author_profile_id><![CDATA[81409594327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Seungyong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[POSTECH]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[leesy@postech.ac.kr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>964604</ref_obj_id>
				<ref_obj_pid>964568</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baker, S., and Matthews, I. 2004. Lucas-kanade 20 years on: A unifying framework. <i>International Journal of Computer Vision (IJCV) 56</i>, 3, 221--255.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778767</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Joshi, N., Kang, S. B., Zitnick, L., and Szeliski, R. 2010. Image deblurring with inertial measurement sensors. <i>ACM Trans. Graphics 29</i>, 3, 30:1--30:9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2006990</ref_obj_id>
				<ref_obj_pid>2006853</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Tai, Y.-W., P. Tan, and Brown, M. to appear. Richardson-lucy deblurring for scenes under a projective motion path. <i>IEEE Trans. Pattern Analysis Machine Intelligence</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Whyte, O., Sivic, J., Zisserman, A., and Ponce, J. 2010. Non-uniform deblurring for shaken images. In <i>Proc. CVPR 2010</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Non-uniform Motion Deblurring for Camera Shakes using Image Registration * Sunghyun Cho Hojin Cho POSTECH 
POSTECH sodomau@postech.ac.kr nucL23@postech.ac.kr  Figure 1: Non-uniform motion deblurring. Top row: 
Input images. Bottom row: Our non-uniform deblurring result and a uniform de­blurring result. 1 Introduction 
This paper presents a novel blind motion deblurring method for dealing with non-uniform blurs caused 
by camera shakes. While there are recent works for non-uniform motion deblurring [Whyte et al. 2010; 
Joshi et al. 2010], those approaches either limit the free­ dom of camera motions or require special 
hardware. Our method is based on a novel representation of motion blurs, which models the blur effects 
using a set of homographies [Tai et al. to appear]. This representation can fully describe the motions 
of camera shakes in 3D world, which cause non-uniform motion blurs. Our main contri­bution is the blind 
motion deblurring algorithm associated with this representation. We solve the ill-posed non-uniform point 
spread function (PSF) estimation problem by transforming it into a well­posed image registration problem 
that estimates homographies. To improve the robustness of our method, we use two input images for deblurring. 
Our method is experimented with both synthetic and real world examples, and produces superior deblurring 
results com­pared to previous methods. 2 Non-uniform Motion Deblurring We represent non-uniform motion 
blurs caused by camera shakes using b =wiPil + n, (1) i where b is a blurred input image, l is the 
latent image to be esti­mated, and n is additional image noise, all in a vector representa­tion [Tai 
et al. to appear]. Pi denotes a homography that corre­sponds to a changed camera pose, and produces a 
projective trans­form of image l. wi is a weight representing the proportion of time n spent by the 
camera in that pose, and i wi =1. *This work was supported by the Industrial Strategic Technology Devel­opment 
Program of MKE/MCST/KEIT (KI001820, Development of Com­putational Photography Technologies for Image 
and Video Contents). Yu-Wing Tai Seungyong Lee KAIST POSTECH yuwing@cs.kaist.ac.kr leesy@postech.ac.kr 
 Let P and w be the sets of homographies {Pi} and weights {wi}, respectively. Our non-uniform PSF estimation 
involves computing P using image registration and updating weights w using a least­square solution in 
an iterative optimization process. To compute P, we .rst rearrange the order of homographies in Eq. (1) 
and obtain b -wj Pj l = wiPil + n, (2) j] =i where the left-hand side is the residual image for the 
homographiy Pi. Using Eq. (2), estimating each Pi is reduced to an image registration problem that minimizes 
the difference between wiPil n and b - j]=i wj Pj l, where wj and Pj are results from the previous iteration. 
This problem can be effectively solved using a Lucas-Kanade image registration method [Baker and Matthews 
2004]. In other words, we do not directly solve the ill-posed PSF estimation problem, but the well-posed 
image registration problem. After updating the homographies, we compute w that minimizes ra ||b - Lw||2, 
where L = P1lP2l ··· Ppl and p is the number of homographies. Once (P, w) is computed, we obtain an intermediate 
latent image l with .xed (P, w) using the deconvolution method in [Tai et al. to appear] with a sparsity 
prior. At the next iteration, (P, w) is newly computed using the updated latent image l. This iterative 
process progressively re.nes the PSF parameters (P, w) and the latent image l. To avoid poor local minima 
and to increase the stability of the algorithm, we adopt a multi-scale approach using a Gaussian pyramid. 
At the coarsest level, we use Pi = I and wi =1/p for initial homographies and weights. For the latent 
image l at the coarsest level, the downsampled version of one input image provides the initial value 
for the other to avoid the trivial delta function solution in PSF estimation.  3 Experimental Results 
Fig. 1 shows an example of our non-uniform deblurring method and its comparison to uniform motion deblurring. 
More deblurring results are provided in the supplementary material, and demonstrate that our method estimates 
and removes real camera motion blurs better than previous methods.  References BAKER, S., AND MATTHEWS, 
I. 2004. Lucas-kanade 20 years on: A unifying framework. International Journal of Computer Vision (IJCV) 
56, 3, 221 255. JOSHI, N., KANG, S. B., ZITNICK, L., AND SZELISKI, R. 2010. Image deblurring with inertial 
measurement sensors. ACM Trans. Graphics 29, 3, 30:1 30:9. TAI, Y.-W., P.TAN, AND BROWN, M. to appear. 
Richardson­lucy deblurring for scenes under a projective motion path. IEEE Trans. Pattern Analysis Machine 
Intelligence. WHYTE, O., SIVIC, J., ZISSERMAN, A., AND PONCE, J. 2010. Non-uniform deblurring for shaken 
images. In Proc. CVPR 2010. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British 
Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037909</section_id>
		<sort_key>830</sort_key>
		<section_seq_no>21</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Capture and construction]]></section_title>
		<section_page_from>21</section_page_from>
	<article_rec>
		<article_id>2037910</article_id>
		<sort_key>840</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>63</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Data-driven bird simulation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037910</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037910</url>
		<abstract>
			<par><![CDATA[<p>Natural motion of living creatures such as human and animals has generated widespread interest in computer animation field. Many film and game industries want to present these virtual creatures on their products and exhibit natural and realistic motions as much as possible. Among them, flying animals such as birds have been particularly focused on because of their special condition moving in flight. Because they move in the sky with its wings and their motions are affected by subtle air flow, the principle for generating flying behavior has to be completely different from that for creating biped human's or quadruped animal's locomotion behavior.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809793</person_id>
				<author_profile_id><![CDATA[81474690898]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eunjung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ju]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809794</person_id>
				<author_profile_id><![CDATA[81453633257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Byungkuk]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Choi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Korea Advanced Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809795</person_id>
				<author_profile_id><![CDATA[81100607730]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Junyong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Noh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Korea Advanced Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809796</person_id>
				<author_profile_id><![CDATA[81100429553]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jehee]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1781155</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lee, Y., Kim, S., and Lee, J. 2010. Data-driven biped control. <i>ACM Trans. Graph. 29</i> (July), 129:1--129:8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276511</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sok, K. W., Kim, M., and Lee, J. 2007. Simulating biped behaviors from human motion data. <i>ACM Trans. Graph. 26</i> (July).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882360</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wu, J.-C., and Popovi&#263;, Z. 2003. Realistic modeling of bird flight animations. <i>ACM Transactions on Graphics 22</i>, 3, 888--895.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Data-driven Bird Simulation EunjungJu1 ByungkukChoi2 JunyongNoh2 JeheeLee1 SeoulNationalUniversity1 
KoreaAdvancedInstitute ofScience andTechnology2 Figure1:A sequenceof apigeon s .ying motion 1 Introduction 
Natural motion of living creatures such as human and animals has generated widespread interest in computer 
animation .eld. Many .lm and game industries want to present these virtual creatures on their products 
and exhibit natural and realistic motions as much as possible.Amongthem, .yinganimalssuch asbirdshavebeenpar­ticularly 
focused on because of their special condition moving in .ight. Because they move in the sky with its 
wings and their mo­tions are affected by subtle air .ow, the principle for generating .yingbehaviorhas 
tobecompletelydifferentfrom thatforcreating bipedhuman sorquadruped animal s locomotionbehavior. Wu andPopovi´ 
c[2003] generated a bird character .ying along a givenpath. Theymanuallyformulated andparameterized wingbeat 
motions of a bird by naive observations and simulated its .ight in physically plausible manner. Although 
the result motions seemed to be physically correct, they could not guarantee reality of those motions 
due to its manual parameterization. In the case of biped behavior s simulation, there are some efforts 
to make a physically simulated controllerbased on motion capturedata[Sok et al.2007; Leeetal.2010].Thedata-drivencontroller 
took realhuman smo­tion as a reference data and made it possible to generate both nat­ural andphysicallyplausible 
locomotionwithout cumbersomepro­cedure. In this work, we propose a novel method to physically simulate 
.ying motions imitating real bird .ight. For reproducing realis­tic .ight, we capture a pigeon s .ying 
and parameterize a wing­beat motion from acquired data. Because we parameterize a bird s wingbeat using 
real motion automatically, we canget a convincing formulation representing wingbeat motion easily. Our 
.ight con­trollerbased onreal wingbeatdataallowsus togeneratephysically plausible as well as apparently 
realisticbirdbehaviors. 2 Our Approach To acquire realistic bird .ight data, we captured a pigeon s 
behav­iors using both a Vicon optical motion capture system and high speed cameras. We attached 14 markers 
on wing joints and body parts includingahead,a trunk and a tail,and aViconoptical system with 28 cameras 
tracked these markers at 240 frames/second. This marker-based capture system played a role of recording 
wingbeat motions expressed by each joint mainly. Also we used four high speed cameras with a frame rate 
of 1000 fps for observing subtle motion offeathers closely. lablejoints(shoulder,elbow,wrist,and tail).Afteracquiringthree­dimensionalmarkerpositionsfrommotioncapturesystem, 
thecap­tured data is analyzed to each DOF s angles of controllable joints. As a wingbeat motion is a 
cyclic behavior, the analyzed angle val­ues also exhibit cyclic patterns. The wingbeat is represented 
by a kind of sinusoidal curve,however, eachDOFhas adifferent ampli­tude, phase and distinct details which 
are dif.cult to design manu­ally. By using these captured data, we can formulate more realis­tic wingbeat 
motions easily. When we simulate bird .ights, a nat­ural deformation of feathers functions as re.ecting 
aerodynamics correctly as well as demonstrating bird motions more realistically. However, because feathers 
have extremely light and soft property, it is hard to capture a feather s deformation using a marker-based 
capture system. High speed cameras with higher frame rates than a Vicon capture system record detail 
motions of feathers and al­lowustoobserveandthenre.ect itsdeformationsinaerodynamics simulationphase. 
Inourexperiments,asimulationsystemconsistsoftwoparts;track­ing the reference wingbeat motions using Proportional 
Derivative (PD) control approach and applying air resistance using simpli­.ed aerodynamics[Wu andPopovi´c2003]. 
The simplePD control method works well for tracking cyclic wingbeats on-the-.y at run­time. Simpli.ed 
aerodynamics represents drag and lift forces ap­pliedtoa.yingbirdproperlyand enables itsfeatherstoexhibitde­formationssuchasbending 
andtwisting naturally.Wecangenerate an animation of.ight motion whichis simulated at runtime without 
acomplicatedformulationprocessand a time-consuming optimiza­tion phase. Despite simulating regular and 
symmetry wingbeats repetitively,abird charactersometimes losesabalanceby accumu­lation ofphysical simulation 
errors. Our simplefeedback controller allows the bird to maintain its balance in real-time and go straight 
ahead consistently. Currently we are working on designing a dy­namic controller which enables us to control 
speed and directions of .ight motions and create various bird behaviors such as taking off, landing and 
an aerobatic .ight. References LEE, Y., KIM, S., AND LEE, J. 2010. Data-driven biped control. ACM Trans. 
Graph. 29 (July),129:1 129:8. SOK, K. W., KIM, M., AND LEE, J. 2007. Simulating biped be­haviorsfromhuman 
motiondata. ACM Trans. Graph. 26 (July). WU,J.-C., AND POPOVIC´,Z. 2003. Realisticmodeling ofbird .ight 
animations. ACM Transactions on Graphics 22, 3, 888 895. Our bird character has one root joint and four 
kinds of control- Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, 
Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037911</article_id>
		<sort_key>850</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>64</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Designing with constraints parametric BIM]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037911</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037911</url>
		<abstract>
			<par><![CDATA[<p>While usually associated with the back-end of the design process (implementation), building information modeling (BIM) could also redefine the way design ideas are generated by bridging formal creativity with design and technological innovation. This is achieved through a close integration of generative tools with parametric capabilities and intelligent database-enriched digital objects.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809797</person_id>
				<author_profile_id><![CDATA[81328491221]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrzej]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zarzycki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New Jersey Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[andrzej.zarzycki@njit.edu]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Designing with constraints: parametric BIM Andrzej Zarzycki New Jersey Institute of Technology andrzej.zarzycki@njit.edu 
 1 Introduction While usually associated with the back-end of the design process (implementation), building 
information modeling (BIM) could also redefine the way design ideas are generated by bridging formal 
cre­ativity with design and technological innovation. This is achieved through a close integration of 
generative tools with parametric ca­pabilities and intelligent database-enriched digital objects. Presently, 
BIM based tools lack significant generative design mod­ules, and thus become peripheral within creative 
process. This de­ficiency reflects the difficulty of reconciling the generative lateral modes of creativity 
with the didactic hierarchical problem solv­ing. At the same time, general use, generative design software 
lacks the database dimension and material-based knowledge associated with its digital models. Architects 
may be able to develop inter­esting designs, however, it is impossible to verify if these designs correspond 
to anything physically constructible, nor they can be associated with a particular scale or material 
characteristics. This discontinuity in the creative process between generative and imple­ment-ive design 
stages exemplify significant limitation of digital tool. To bridge this gap, this talk investigates generative 
qualities of the BIM platform through a relatively narrow but potent set of examples of parametrically 
controlled constructional details. 2 Approach Specifically, this talk discusses the integration of building 
sci­ences with everyday architectural professional practice. It proposes a design methodology that starts 
with a construction detail, and pursues designs that naturally emerge out of the assembly of dis­cussed 
components. While this is a long-practiced approach, this study broadens this method by considering a 
broader set of design solutions resulting from parametric alterations and alternations of original components. 
The final design project emerges through a series of explorations with fragments informing the entirety 
of the architectural design solution: fragments that are representative of the overall design. It is 
conceptually and metaphorically analogous to a fractal relationship, where a component implies an overall 
structure. For appropriate precedent, we investigated contemporary designs representing established practices, 
which naturally translated into parametric and BIM thinking. Projects by Nicholas Grimshaw, Nor­man Foster, 
and Santiago Calatrava were just a few of the designs that fit well into the class methodology and were 
relatively easy to handle using digital tools. In selecting projects and particular assembly components 
or construction details, students were asked to study this precedent, model partial assemblies and test 
them as a three-dimensional BIM models. When choosing examples for their explorations, students were 
asked to consider the open-endedness of their particular designs and an ability to develop meaningful 
variations. Through these studies students learned about the spatial coordination of various element 
and system components, their interconnectivity and inter­dependencies. Students were able to manipulate 
and experiment with parametric components and to follow interactively through the design alteration. 
Later, in the second part of the project, stu­dents explored the parametric possibilities of BIM models. 
[figure 1] Three-dimensional, parametrically resolved architectural details served as speculative, idea-generating 
devices for design. Students were expected to demonstrate the creative possibilities of their BIM models 
and to document their parametric explorations [figures 2] through images, digital models, and a text 
narrative (final report). 3 Broader Discussion With parametric analysis, designers can immediately trace 
the de­sign changes and see how they impact other components in the assembly. Combining or nesting parametric 
components not only allow for an ease of modeling and a greater flexibility, but also allows understanding 
how individual changes impact an overall design. Once a single parameter was changed in an overall, often 
complex, assembly of individual components, students we able to trace the propagation of changes throughout 
the database-model and immediately evaluate consequences of this particular change. Also, they could 
propose new designs through interactive manipu­lations of parameters and see changes propagated through 
the en­tire system assembly. This dual use of parametric digital models for understanding of a significant 
architectural precedence (con­struction knowledge building) and for speculative explorations of possible 
design propositions allows for greater integration of final designs. Furthermore, in parametrically defined 
BIM environ­ments, architects can explore designs that are native to the world of construction that do 
not have to be translated or reinvented as a result of the progression from a conceptual idea to a real 
product. [figure 3]  Figure 2. Analyzing parametrically-driven behaviors of element as­semblies. Fully 
detailed truss at roof condition with parametric control of truss members and slab thickness. Remaining 
geometry follows spatial transformations of the truss and slab.  Figure 1. Parametric variations of 
the roof/deck structure Figure 3. Parametric design explorations  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037912</article_id>
		<sort_key>860</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>65</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Form-making with special effect tools]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037912</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037912</url>
		<abstract>
			<par><![CDATA[<p>Recent developments in digital design have brought new tectonic freedom into architecture. These innovative forms break away from the traditional or modernistic framework. In turn, they often relate to contemporary developments in other design disciplines, such as product design. Emerging tectonic trends, combined with research into new material and fabrication technologies, make it possible to purse imaginative designs that were not possible a decade ago. While digital tools allow for a broader architectural reading, resulting in innovative and unique designs and new expectations regarding space and form, these emerging designs often exist exclusively as visual propositions, deprived of a deeper structural, constructional, or functional logic.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809798</person_id>
				<author_profile_id><![CDATA[81328491221]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrzej]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zarzycki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New Jersey Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[andrzej.zarzycki@njit.edu]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Form-making with special effect tools Andrzej Zarzycki New Jersey Institute of Technology andrzej.zarzycki@njit.edu 
 1 Introduction Recent developments in digital design have brought new tectonic freedom into architecture. 
These innovative forms break away from the traditional or modernistic framework. In turn, they often 
relate to contemporary de­velopments in other design disciplines, such as product design. Emerging tectonic 
trends, combined with research into new material and fabrication technologies, make it possible to purse 
imaginative designs that were not possible a decade ago. While digital tools allow for a broader architectural 
reading, resulting in innovative and unique designs and new expectations regarding space and form, these 
emerging designs often exist exclusively as visual propositions, deprived of a deeper structural, constructional, 
or functional logic. Similarly, the proliferation of structural analysis software has helped engi­neers 
to calculate sophisticated structural models and understand the intri­cacies of complex tectonics. However, 
this ability seldom translates back into an architectural form or empowers the design process. Consequently, 
these two parallel developments, while promising in their individual ca­pabilities, fall short in the 
synergizing of an architectural form. Although attempts are being made to mitigate this separation with 
the use of building information modeling (BIM) software, these solutions still operate within classical, 
architectural-versus-structural paradigms and do not address the visual thinking dimension of building 
simulations together with design evaluation criteria. This sketch presentation discusses the integration 
of tectonic architectural studies with structural analyses and building performance simulations from 
an academic perspective, where the conceptual or visceral under­standing of structures a general and 
qualitative point of view is more important than quantitative and numeric. The ability to visualize structural 
performance stress, tension, shear and interactively study the impact of these forces on an architectural 
form, results in an integrated design process. Furthermore, these interactive simulations translate into 
a visually inspired, virtual hands-on experience for students and young practitioners by helping them 
to develop an intuitive knowledge of architecture. Finally, this presentation focuses on the strategies 
for generative design validation with the use of digital simulations, particularly dynamics-based modeling 
tools. Specifically, I focus on tools that employ rigid/soft body dynamics, such as cloth simulations, 
forward and inverse kinematics (FK/IK), and particle interactions. This approach was used in a classroom 
set­ting and on research projects as an alternative to other methodologies, such as genetic algorithms 
(GA). 2 Dynamic objects and digital simulations as a design opportunity The gap is narrowing between 
generative design tools, which are often used to pursue exclusively formal gestures, and BIM tools. Generative 
tools start considering the performance as well as the material behaviors of a form, while BIM tools 
define architecture as a parametric, spatially resolved object that can be freely manipulated and explored. 
This mutual convergence between generative and BIM tools is particularly effective in a scale of design 
components, where individual elements and properties can be parametrically interrelated. Both approaches 
also establish an active link between an object (component) and the entire system (whole), mak­ing it 
possible to manipulate individual design characteristics. While each software environment achieves this 
in a different way, the ability to inter­relate a fragment with the entire design is common for both 
environments: generative dynamics and parametric BIM. For example a rigged, IK bone system can demonstrate 
behavior similar to that of a parametrically controlled composite beam-column. [Fig. 1, 2] Both are defined 
by degrees of freedom and controlled by a set of con­straints. These constraints can simplify a number 
and variety of individual subcomponents, while a parametric definition of the overall design can result 
in multiplicity of design implementations. While there is still a need to develop effective ways to bridge 
these two digital design environments, the strategies for forming this connection emerge with parametric 
simulations and dynamics playing key roles. Con­sequently, dynamics-based simulations not only create 
an opportunity for design validation but also form a natural stepping stone toward parametri­cally defined 
architectural models (details) that could be utilized through­out the entire design process. 3 Closing 
Thoughts This simulation based interactive approach shifts the designer s focus from the visualization 
of buildings or data to the visualization of physical pro­cesses and behaviors. The benefit of approach 
is to give designers a good-enough approximation of a model s behavior, as well as to set an overall 
design context and parameters. In the design process, particularly in the early formative stages, good-enough 
precision is satisfactory. Consequently, through the use of dynamics based software a promising di­rection 
of generative architectural designs emerges. An architectural form not only can be analyzed based on 
its structural performance, but also de­rived through the process of structural simulations.  Figure 
2. Parametrically defined structural member.  Figure 1: Composite beam-column member.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037913</article_id>
		<sort_key>870</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>66</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Building and animating cobwebs for antique sets]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037913</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037913</url>
		<abstract>
			<par><![CDATA[<p>When working with antique sets, cobwebs are one of the essential elements that set up the right atmosphere. The complex nature of cobwebs makes them difficult to build by hand. We want to utilize the accuracy of simulated effects, the speed and predictability of procedural modeling and the beauty of hand modeling. This talk describes the mixed pipeline that we created, which was first introduced in DreamWorks Animation's film <i>Shrek Forever After</i> and Halloween TV special <i>Monsters vs Aliens: Mutant Pumpkins from Outer Space</i> and is used in several upcoming projects.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809799</person_id>
				<author_profile_id><![CDATA[81488655700]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fangwei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[fangwei.lee@pdi.dreamworks.com]]></email_address>
			</au>
			<au>
				<person_id>P2809800</person_id>
				<author_profile_id><![CDATA[81488672757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alex]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ongaro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809801</person_id>
				<author_profile_id><![CDATA[81488641080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Domin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809802</person_id>
				<author_profile_id><![CDATA[81488659231]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[August]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Meredith]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Building and Animating Cobwebs for Antique Sets Fangwei Lee* Alex Ongaro Domin Lee August Meredith DreamWorks 
Animation Introduction When working with antique sets, cobwebs are one of the essential elements that 
set up the right atmosphere. The complex nature of cobwebs makes them difficult to build by hand. We 
want to utilize the accuracy of simulated effects, the speed and predictability of procedural modeling 
and the beauty of hand modeling. This talk describes the mixed pipeline that we created, which was first 
introduced in DreamWorks Animation's film Shrek Forever After and Halloween TV special Monsters vs Aliens: 
Mutant Pumpkins from Outer Space and is used in several upcoming projects. Before applying the new solution, 
our first approach was to use standard surface texturing. Unfortunately, there are three issues with 
this technique: first, surface textures do not create enough depth and parallax. Although this can be 
solved by creating more overlapping layers, painting these layers is tedious. Second, while hand-modeled 
webs are artistically beautiful, they do not necessarily form correct physics catenary and any small 
biases can break the realism that we are trying to achieve. Third, UV stretching becomes a problem with 
complex surfaces. To solve the above issues of modeling antique sets, we created this system which speeds 
up modeling and animating with procedural operators and light-weighted simulations.  Physics for Cobwebs 
Cobwebs are webs built by spiders that have collected dust. Although spiders spin webs at regular intervals, 
those old webs are built, destroyed, re-built, and expanded for years. Those chaotic natural events give 
the webs irregular and fractal structures. Thus the silky spider threads covered by dust and droplets 
produce the anisotropic and fuzzy texture. The Structure The continuous build-and-destroy cycle creates 
a self-similar structure which can be defined recursively. We discovered that recursively creating line 
segments from two points on two line segments produces cobweb-like structure. For L0 is the set of initial 
segments: L0={S 0 ,S 1 ,S 2...} LL. XY ,where X ,Y . L n. 1 = nn  The Statics Observationally, a hanging 
string forms a catenary-the theoretical shape of a hanging flexible chain or cable when supported at 
its ends and acted upon by a uniform gravitational force (its own weight) and in equilibrium. The curve 
has a U shape that is similar in appearance to the parabola: xa x/ a-x /a y =acosh . .= .e .e . a 2 Because 
a parabola can be calculated much faster than catenary, we use the parabolic equation for speed and simplicity. 
y =ax2 .bx.c Because a catenary in 3D space is actually on a 2D plane we can convert two 3D points into 
a vertical plane space: A(0,0), B(u,v). Given gravity factor g, we obtain: g 2 v - u g 4 a= b= ,c=0 4 
, u yield: g 2 v - u g 4 2. y = xx 4 u and we use this new equation to approximate catenary curves. 
The Dynamics Considering cobwebs as connected threads, the dynamics can be solved by cloth physics. Basic 
cloth simulations consist of structure, shear, and bend springs. Because spider threads have minimal 
thickness, the webs have no shear springs and very weak bend springs.  Modeling Modelers start with 
a clean prop without any web. First, they define the basic web structure quickly and intuitively with 
line segments, which form the base curves (figure 1). Second, we set up constraints to adhere the curves 
to the prop and connect to each other. The modelers place pin locators on sets to fix the web points 
in space and snap multiple points together to outline the rough structure. (figure 2) After finishing 
the design, we run a procedure that converts base curves into thin polygon meshes. Cloth simulations 
are run with force settings on the resulting polygon web and polygon edges are extracted as newly deformed 
curves. This procedure deforms our original straight curves, and creates new curves in a physics-based 
form. (figure 3)   Knitting Procedure The simulated base curves are used as our infrastructure on 
which details build. We select curves to knit together and execute the knitting procedure to generate 
filled curves. New curves are generated recursively; we choose two random points on two random base curves 
or filled curves, and create new filled curves to connect them (figure 4). We use parabola equations 
to approximate catenary curves. Given the positions of two points, number of segments, and gravity factor, 
the procedure calculates theoretical positions and moves control points on the filled curves (figure 
5). After modelers are done with the cobweb design, we apply a final pass of the knitting procedure to 
create the detailed curves which represent the finest details for our cobwebs (figure 6). figure 4: 
Knit filled curves between figure 5: Offset curve points with figure 6: Recursively create detailed base 
curves. catenary equations to apply curves to bring more complexity to deterministic gravity effect. 
the web.  Animation Cobweb animation is done in Maya with its native dynamic fields and cloth physics. 
To optimize the process, only the thin polygon extruded from the base curves are used in the simulations. 
Because filled curves are still connected to base curves via construction history, forces applied on 
the base curves, move the filled curves with them as if they are simulated as well. Using the extruded 
polygon as a cloth object, we define basic atmospheric environment with wind and turbulence fields (figure 
8). And then we animate the constraints to detach specific components of the base curves at desired time 
during simulation.   The Texturing and Shading Generally, cobwebs are composed of two different kinds 
of materials-silky spider threads, and a fuzzy layer created by dust clinging to the threads. For rendering 
threads, we loft geometry tubes from the curves and apply an anisotropic material. During the modeling 
phases, the modeler can also make cobweb patches by lofting between two curves. Those patch surfaces 
are then painted by texture artists to add another layer of artistic control and to create finer details 
that are too expensive or require specific art direction for the knitting approach.  Implementation 
To produce the cobweb, we divide the system into five parts: a modeling toolset that allows us to freely 
model the web without restriction; a cobweb simulation engine; a procedural geometry shader to create 
fine details, which are otherwise tedious to do by hand; an animation tool; and a shader network. This 
approach is compatible with our standard geometry-based studio pipeline. *email: fangwei.lee@pdi.dreamworks.com 
 In Production One FX artist first defined the look and behavior for the show, and then modeling department 
used the toolset to build the cobweb models. The modelers also lofted a few shapes between the biggest 
areas of the webs to make NURBS patches that the surfacing department uses to apply textures resembling 
the fine cobweb detail. These curves and patches are then installed to the model library and picked up 
automatically for any shot in production. Copyright is held by the author / owner(s). SIGGRAPH 2011, 
Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037914</section_id>
		<sort_key>880</sort_key>
		<section_seq_no>22</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Light my fire]]></section_title>
		<section_page_from>22</section_page_from>
	<article_rec>
		<article_id>2037915</article_id>
		<sort_key>890</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>67</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Simulating massive dust in <i>Megamind</i>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037915</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037915</url>
		<abstract>
			<par><![CDATA[<p>In Dreamworks Animation's <i>Megamind</i>, the production faced a challenge in creating a shot where the top of a skyscraper is torn off and hurled down a city street, destroying portions of the city and creating a massive dust cloud. Development for this shot inspired a new fluid simulation framework with significant improvements in four areas: speed, visual quality, setup flexibility and artistic control. In this talk we describe the simulation algorithm and the framework that allowed it to be used in our effects pipeline.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809803</person_id>
				<author_profile_id><![CDATA[81488672035]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Koen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vroeijenstijn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[koen.vroeijenstijn@dreamworks.com]]></email_address>
			</au>
			<au>
				<person_id>P2809804</person_id>
				<author_profile_id><![CDATA[81548023786]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ronald]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Henderson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ron.henderson@dreamworks.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1401764</ref_obj_id>
				<ref_obj_pid>1401696</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Selle, A., Fedkiw, R. Kim, B., Liu, Y. and Rossignac, J. 2008. An unconditionally stable MacCormack method. J. Sci. Comput. 35, 350--371.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Simulating Massive Dust in Megamind Koen Vroeijenstijn Ronald D. Henderson DreamWorks Animation  Figure 
1: Massive dust cloud animated using our .uid simulation framework at a resolution of 1200 × 195 × 590, 
which runs in 52 seconds per frame on a single workstation. This is a volume render of the simulated 
density .eld (front view and top view). In Dreamworks Animation s Megamind, the production faced a challenge 
in creating a shot where the top of a skyscraper is torn off and hurled down a city street, destroying 
portions of the city and creating a massive dust cloud. Development for this shot inspired a new .uid 
simulation framework with signi.cant improvements in four areas: speed, visual quality, setup .exibility 
and artistic control. In this talk we describe the simulation algorithm and the framework that allowed 
it to be used in our effects pipeline. Fast, Direct Simulation Our simulation algorithm is based on integrating 
the incom­pressible Navier-Stokes equations, but with several modi.ca­tions to classic approaches. We 
represent all solution variables (velocity, pressure, density, etc.) as discrete volumes sampled on a 
regular, non-staggered grid. In addition to standard buoy­ancy and external forcing terms, we also include 
an explicit divergence control .eld which is incorporated into the pres­sure solve. We use a MacCormack 
method with a local limiter for advection [Selle et al. 2008] and treat all other terms im­plicitly, 
taking advantage of a Fast Helmholtz Solver for all elliptic systems. The resulting algorithm is unconditionally 
stable, requires constant solve time per frame, and both mem­ory and computational time scale close to 
linearly with grid size. All stages of the solver pipeline have a high theoretical parallel ef.ciency. 
There are several advantages to this approach. Unlike .rst­order upwind methods, the MacCormack method 
introduces predominantly dispersive rather than diffusive errors, which is far more forgiving when simulating 
turbulence. Results gen­erally demonstrate very little numerical diffusion. By includ­ing physical diffusion, 
we explicitly control a small-scale cut­off and achieve excellent correspondence between high-and low-resolution 
simulations. Simulations can be tweaked at a lower resolution, running at almost interactive speeds, 
know­ing the high resolution version will match the overall motion. Fast solve times allowed us to run 
direct simulations at nearly one voxel per pixel without resorting to advection techniques to add detail. 
 Everything is a Volume All inputs to the simulation framework are represented as discrete volumes, including 
an arbitrary number of dynamic scalars, forces, and source terms. Collision data is represented as a 
velocity .eld and voxel mask. A simulation can be cre­ated using existing tools and techniques to populate 
volume buffers. Since most artists are comfortable generating, ma­nipulating and analyzing volumes, a 
lot of the guesswork is Figure 2: Visualation of the input collision volume for the simulation environment. 
Each voxel is repesented as a single point. removed from the simulation setup. Execution can be rep­resented 
as a node graph where manipulation of the internal solver state is mixed with other volume operations. 
This en­courages artistic exploration since the complete dynamic state can be manipulated to sculpt the 
simulation in creative ways as it runs. Putting It All Together In the Megamind setup we used only three 
volume inputs to control the simulations: density, collision, and divergence. Density sources were created 
from particle simulations start­ing at the contact line between the tower and city street, and then rasterized 
into volumes. Deforming and moving geome­try was rasterized to create collision volumes. In addition 
to coupling the solver with the animation environment, we show that manipulating the collision mask can 
be an effective way to drive additional turbulence that is superior in both look and computational cost 
to just adding noise to the simulation. By setting positive divergence in areas with higher density, 
we were able to create violent expansion in speci.c areas of the dust cloud. In addition to a breakdown 
for the tower destruc­tion, we provide implementation details for how we created an interactive setup 
and simulation environment and demonstrate scene con.gurations typical of other production shots. References 
SELLE, A., FEDKIW, R. KIM, B., LIU, Y. AND ROSSIGNAC, J. 2008. An unconditionally stable MacCormack method. 
J. Sci. Comput. 35, 350-371. {koen.vroeijenstijn, ron.henderson}@dreamworks.com Copyright is held by 
the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037916</article_id>
		<sort_key>900</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>68</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA["MegaMind"]]></title>
		<subtitle><![CDATA[fire, smoke and data]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037916</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037916</url>
		<abstract>
			<par><![CDATA[<p>Exploding a super hero in DreamWorks Animation's "MegaMind" was a large scale task. The detail and scale of the explosion was to be massive and the collaboration between effects artists unprecedented. This created several technical hurdles and eventually several innovations such as: a fluid clamping plugin, a volume splatting tool adjustable by noise, and a collaborative caching tool.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809805</person_id>
				<author_profile_id><![CDATA[81488663709]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Krzysztof]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rost]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[krzysztof.rost@dreamoworks.com]]></email_address>
			</au>
			<au>
				<person_id>P2809806</person_id>
				<author_profile_id><![CDATA[81319492707]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hart]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[greg.hart@dreamworks.com]]></email_address>
			</au>
			<au>
				<person_id>P2809807</person_id>
				<author_profile_id><![CDATA[81100505245]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peterson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[scott.peterson@dreamworks.com]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 MEGAMIND  : FIRE, SMOKE AND DATA Krzysztof Rost, Greg Hart, Scott Peterson DreamWorks Animation Introduction 
Exploding a super hero in DreamWorks Animation's MegaMind was a large scale task. The detail and scale 
of the explosion was to be massive and the collaboration between effects artists unprecedented. This 
created several technical hurdles and eventually several innovations such as: a fluid clamping plugin, 
a volume splatting tool adjustable by noise, and a collaborative caching tool. Dependency At the start 
we identified the list of required elements: fire, dust, smoke, debris trails, shock wave, splashes, 
water interaction and a light beam. The production schedule required us to create a smooth work­flow 
between seven effects artists working at the same time on shared data. Our goal was to design a clear 
plan for communicating responsibility, not only between effects artists but also between the lighter 
and compositor. Fire and smoke Figure 1: Final rendered images of the explosion We developed several 
tools to help us achieve greater detail in our Maya fluid simulations and greater control over our iterations. 
The first notable development was a Maya plugin which allowed for manipulations of the fluid. The plugin's 
main feature was to clamp user­specified fluid fields such as density, temperature, fuel, or velocity 
at user­specified rate. Our implementation used a noise field to mask out areas for clamping to be applied. 
The noise field could be animated over time to add a variant as to how values are affected. Another function 
that was implemented was temperature erosion. The result was that a significant amount of perceived detail 
could be added. Also, more control was attained over what would have otherwise been an unstable simulation. 
The second development was another Maya plugin which allowed stamping fluid attributes into a fluid container 
using particles by simple splatting at the simulation time. By coupling a noise function with the plugin 
, we were able to transfer information from each particle using noise as a mask into the fluid grid, 
and as a result achieve a better distribution. Fig(2). Figure 2: on left distribution using noise, on 
the right direct particle transfer We also designed a particle clustering technique to further control 
the shape and motion of the explosion. In a nut shell, the clustered particles were staggered with an 
offset along the parent velocity vector and at the same time rotating around the same axis. This method 
helped to create a sense of a rolling fire ball. Fig(3­left).  Figure 4: smoke, fireball and tendrils 
For the smoke and ash layer, the simulation conditions were kept the same, only subtly modifing density, 
temperature, and fuel fields. Only density was exported and rendered. Fig(4­left). Lastly, we added layers 
such as debris, water splashes, ripples, volume lighting and used extensive compositing to further increase 
visual richness. Fig(5). Figure 5: Final composite Data A new level of collaboration tools were necessary 
to coordinate between the seven FX artists involved. The observatory explosion required generating massive 
amounts of particles, volumes, curves, geometry, and deep image data. Our goal was to work as efficiently 
as possible by generating heavy data only once. To achieve this level of collaboration, we designed a 
pipeline tool which uses another artist's cached data when it exists and generates it when it doesn't. 
A publishing paradigm allowed artists to decide when and what to share with others. By systematically 
sharing cached data, artists could stay in sync, save on disk space and reduce the number of times the 
same data is created. Credits Krzysztof Rost, Greg Hart, David Lipton, Scott Peterson, Devon Penny, 
Kyle Maxwell, Tobin Jones, Shaun Graham, John Allwine, Markus Burki, Gianni Aliotti, Abhik Pramanik, 
Eli Bocek­Rivele email: krzysztof.rost@dreamoworks.com email: greg.hart@dreamworks.com email: david.lipton@dreamworks.com 
 email: scott.peterson@dreamworks.com Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, 
British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037917</article_id>
		<sort_key>910</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>69</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Volumetric effects in a Snap]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037917</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037917</url>
		<abstract>
			<par><![CDATA[<p>Volumetric effects have become a mainstay of the visual effects community over the past decade. With many third-party solutions available, what are the benefits and challenges of creating a proprietary system? We outline <i>Snap</i>, Animal Logic's grid-based simulation and volumetric rendering framework used in <i>Legend of the Guardians: The Owls of Ga'Hoole (LotG)</i> and <i>Sucker Punch (SP)</i>.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809808</person_id>
				<author_profile_id><![CDATA[81488653656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Joseph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hegarty]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Animal Logic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[joeyh@al.com.au]]></email_address>
			</au>
			<au>
				<person_id>P2809809</person_id>
				<author_profile_id><![CDATA[81488669536]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Denis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Teplyashin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Animal Logic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[denist@al.com.au]]></email_address>
			</au>
			<au>
				<person_id>P2809810</person_id>
				<author_profile_id><![CDATA[81488669941]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Georges]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Animal Logic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[peterg@al.com.au]]></email_address>
			</au>
			<au>
				<person_id>P2809811</person_id>
				<author_profile_id><![CDATA[81317494706]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heckenberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Animal Logic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[danielh@al.com.au]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Balay, S., Brown, J., Buschelman, K., Gropp, W. D., Kaushik, D., Knepley, M. G., McInnes, L. C., Smith, B. F., and Zhang, H., 2011. PETSc Web page. http://www.mcs.anl.gov/petsc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360649</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kim, T., Th&#252;rey, N., James, D., and Gross, M. 2008. Wavelet turbulence for fluid simulation. <i>ACM Trans. Graph. 27</i> (August), 50:1--50:6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Volumetric Effects in a Snap Joseph Hegarty Denis Teplyashin Peter Georges Daniel Heckenberg Animal 
Logic*  c Figure 1: Atmospherics and .uid simulation were used extensively in LotG. &#38;#169;Warner 
Bros. All rights reserved. Abstract Volumetric effects have become a mainstay of the visual effects community 
over the past decade. With many third-party solutions available, what are the bene.ts and challenges 
of creating a propri­etary system? We outline Snap, Animal Logic s grid-based simu­lation and volumetric 
rendering framework used in Legend of the Guardians: The Owls of Ga Hoole (LotG) and Sucker Punch (SP). 
1 Production Motivation and Experiences When assessing the effects requirements for LotG it was clear 
that existing in-house and third-party .uid simulation systems would not meet the scale and turnaround 
requirements for a number of important sequences, such as the climactic bush .re sequence (see .g. 1). 
Additionally, as the sky would be the stage for much of the .lm s action, ef.cient, directable procedural 
atmospheric ef­fects would be required. Development began with the relatively simple problem of creating 
procedural volume modeling tools for cloud and fog effects along with the necessary preview and rendering 
infrastructure. Snap was then augmented with disk-based caching and a modular set of algo­rithms for 
large-scale grid-based .uid simulation. 2 Overview Snap is a plugin for ALF, Animal Logic s proprietary 
multi­threaded node-based computation framework. ALF can be used standalone or embedded inside Maya (among 
other environments). Snap simulation rigs may be created using the Maya Hypergraph, with realtime feedback 
through the Maya viewport. Production quality simulations may be executed locally or on the render farm, 
distributed across any number of machines while maintaining a uni.ed solve. Simulation caches and procedural 
rigs can then be rendered using PRMan (another environment where ALF can be embedded). 3 Bene.ts The 
node-based nature of Snap, compared to a monolithic black­box solution, allows a great deal of .exibility 
when creating rigs. * e-mail: {joeyh,denist,peterg,danielh}@al.com.au Many production requests were ful.lled 
simply with a combination of existing nodes, without requiring additional development. Having complete 
control over the code-base allows for rapid turn­around of bug .xes and new features, e.g. support for 
wavelet tur­bulence [Kim et al. 2008] was added through the addition of a single new node in combination 
with several existing nodes. There are a number of third-party libraries available which can greatly 
accelerate the process of creating a distributed grid-based simulation framework. For LotG and SP the 
distributed simulations made use of PETSc [Balay et al. 2011] to manage the decomposi­tion of the simulation 
and perform the incompressible solve. 4 Challenges Computational Fluid Dynamics, including the subset 
of techniques normallyused toproduce computer generated imagery, require con­siderable domain speci.c 
knowledge. Also, while creating a simple .uid simulation prototype is relatively straight-forward, creating 
a production ready solution which provides the user with enough cre­ative tools takes time. If ALF did 
not already exist, it is questionable if the decision to create Snap would have been made. 5 Next Steps 
Our experience with Snap in its .rst set of productions has been that the ability to modify and extend 
any and all aspects of the frame­work is invaluable. Integration with other ALF-based tools is an area 
of active development, e.g. Snap interoperability with Nexus, our particle-based simulation framework. 
Only by having complete ownership of a code-base are such opportunities available.  References BALAY, 
S., BROWN, J., BUSCHELMAN, K., GROPP, W. D., KAUSHIK, D., KNEPLEY, M. G., MCINNES, L. C., SMITH, B. F., 
AND ZHANG, H., 2011. PETSc Web page. http://www.mcs.anl.gov/petsc. KIM, T., TH¨N., JAMES, D., AND GROSS, 
M. UREY, 2008. Wavelet turbulence for .uid simulation. ACM Trans. Graph. 27 (August), 50:1 50:6. Copyright 
is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. 
ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037918</article_id>
		<sort_key>920</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>70</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Fluid dynamics and lighting implementation in PixelJunk Shooter 2]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037826.2037918</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037918</url>
		<abstract>
			<par><![CDATA[<p>PixelJunk Shooter 2 is a video game for the Playstation 3 in which the player must navigate a spaceship through a two dimensional environment, solving physics-based puzzles and rescuing survivors to progress. A key element of the game is the use of realtime fluid dynamics to simulate a range of materials (such as solids, lava, water and gases) and their interactions with both each other and the surrounding environment. In this paper we will present the algorithms used to achieve this in realtime utilizing the parallel processing elements of the PS3, and discuss techniques developed for extending these to the efficient calculation of lighting and visibility occlusion information.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809812</person_id>
				<author_profile_id><![CDATA[81322497550]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jaymin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kessler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Q-Games, Kyoto, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jaymin@q-games.com]]></email_address>
			</au>
			<au>
				<person_id>P2809813</person_id>
				<author_profile_id><![CDATA[81488670603]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Paolo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Carabaich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Q-Games, Kyoto, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[paolo@q-games.com]]></email_address>
			</au>
			<au>
				<person_id>P2809814</person_id>
				<author_profile_id><![CDATA[81488650389]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kinoshita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Q-Games, Kyoto, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[naoki@q-games.com]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fluid Dynamics and Lighting Implementation in PixelJunk Shooter 2 Jaymin Kessler Paolo Carabaich Naoki 
Kinoshita Q-Games Q-Games Q-Games Kyoto, Japan Kyoto, Japan Kyoto, Japan jaymin@q-games.com paolo@q-games.com 
naoki@q-games.com ABSTRACT PixelJunkShooter2is a videogameforthePlaystation3in which the player must 
navigate a spaceship through a two dimensional environment, solvingphysics-basedpuzzles and rescuing 
survivors to progress. A key element of the game is the use of realtime .uid dynamics to simulate a range 
of materials (such as solids, lava, water and gases) and their interactions with both each other and 
the surrounding envi­ronment. In this paper we will present the algorithms used to achieve this in realtime 
utilizing the parallel processing elements of the PS3, and discuss techniques developed for extending 
these to the e.cient calculation of lighting and visibility occlusion information. 1. INTRODUCTION Realistic 
.uid simulation can be computationally expensive. Because in many models .ow is assumed to be incompress­ible, 
particle-based .uid simulations rely on maintaining a constant distance between particles to achieve 
.uid-like be­havior. If in addition the particles have di.erent types, masses, transferrable heat, and 
reactions to other particle types, simulation becomes more computationally intensive. Our goal was to 
create a stylized .uid simulation that be­haves realistically even in user-modi.ed levels, while still 
maintaining 60 frames per second at a resolution of 1080p. Considering ourtargethardware, weplaced strong 
emphasis onalgorithmsthatparallelize well and catertothestrengths of the SPUs.  2. FLUID SIMULATION 
Fluid particles in PixelJunk Shooter 2 use a biased radius that expands and contracts in response to 
both the number and type of surroundingparticles. A repulsiveforcepropor­tional to the number of particles 
within a particle s radius + bias is added between particles. This helps keep .uid particles apart, and 
their interactions smooth and .owing. Di.erent combinations ofparticletypeshavedi.erentforce­of-repulsion 
values, reactions to other particle types, and thermal transfer values. To model thermal transfer between 
particles, we apply the same calculations used to resolve the repulsive forces between them to the relative 
temperature values. 2.1 distance .eld-based collision For collision between particles and the level 
geometry, we have the SPUs compute a static distance .eld at load-time and a dynamic distance .eld every 
frame, merging the re­sults. The dynamic distance .eld is necessary for destruc­tible or changeable parts 
of the level. Distance .eld-based collision s primary bene.t is that after calculating the .eld, determining 
not only if a collision has occurred, but also distance from the closest wall becomes a single lookup. 
The distance .eld also plays an important role in lighting, which will be discussed further below.  
3. RENDERING PixelJunk Shooter has a distinctive style that partly comes from the rendering of the .uids. 
Groups of particles need to be rendered as a smooth-.owing .uid .eld with a well­de.ned boundary. We 
achieved the e.ect by .rst rendering .uid particles to an o.screen bu.er using a luminance tex­ture, 
blurring the o.screen bu.er, scaling up using bilinear .ltering, and using the resulting brightness to 
color the liq­uid. Asmooth stepfunctionis used, with twothresholdsfor water and surface. Tohelp depict 
motion, eachparticlegets assigned a random RG value, with special colors used when RG is {0.5, 0.5}. 
Other e.ects adding to the unique style ofShooter2includeturbulence-likefoam andbubblee.ects when water 
suddenly changes direction, and refraction ef­fects for objects under water or above strong heat sources 
such as lava. 4. REAL-TIME LIGHTING New in PixelJunk Shooter 2 were dark stages that required special 
lighting. These stages have player ship spotlights, point lights from in-game objects, and wall lights 
that make wall edges glow. Up to 40 lights can be active at once with radii ranging from 1/8th screen 
size to fullscreen, and all lights exhibitaccurate occlusion and attenuation against .u­ids and objects. 
We use the SPU s SIMD instructions to process 16 pixels at a time, calculating the length of an en­tire 
ray segment all at once. Unrolling allowed us to have four rays in .ight at at the same time. 5. ADDITIONAL 
AUTHORS Additionalauthors: DylanCuthbert(Q-Games, email: dylan@q-games.com)andBenCarter(Q-Games, email: 
ben@q-games.com). Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, 
Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
</content>
</proceeding>
