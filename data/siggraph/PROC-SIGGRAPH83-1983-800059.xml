<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>07-25-1983</start_date>
		<end_date>07-29-1983</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Detroit]]></city>
		<state>Michigan</state>
		<country>USA</country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>800059</proc_id>
	<acronym>SIGGRAPH '83</acronym>
	<proc_desc>Proceedings of the 10th annual conference</proc_desc>
	<conference_number></conference_number>
	<proc_class>conference</proc_class>
	<proc_title>Computer graphics and interactive techniques</proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn>0-89791-109-1</isbn>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>1983</copyright_year>
	<publication_date>07-25-1983</publication_date>
	<pages>420</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source>ACM Order No. 428830</other_source>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node>I.3</cat_node>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<ccs2012>
		<concept>
			<concept_id>0.10010147.10010371</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010371</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
			<concept_significance>500</concept_significance>
		</concept>
	</ccs2012>
	<general_terms>
		<gt>Theory</gt>
	</general_terms>
	<chair_editor>
		<ch_ed>
			<person_id>P225388</person_id>
			<author_profile_id><![CDATA[81100035891]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[Peter]]></first_name>
			<middle_name><![CDATA[P.]]></middle_name>
			<last_name><![CDATA[Tanner]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[]]></affiliation>
			<role><![CDATA[Editor]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>1983</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>801126</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Pyramidal parametrics]]></title>
		<page_from>1</page_from>
		<page_to>11</page_to>
		<doi_number>10.1145/800059.801126</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801126</url>
		<abstract>
			<par><![CDATA[<p>The mapping of images onto surfaces may substantially increase the realism and information content of computer-generated imagery. The projection of a flat source image onto a curved surface may involve sampling difficulties, however, which are compounded as the view of the surface changes. As the projected scale of the surface increases, interpolation between the original samples of the source image is necessary; as the scale is reduced, approximation of multiple samples in the source is required. Thus a constantly changing sampling window of view-dependent shape must traverse the source image.</p> <p>To reduce the computation implied by these requirements, a set of prefiltered source images may be created. This approach can be applied to particular advantage in animation, where a large number of frames using the same source image must be generated. This paper advances a &#8220;pyramidal parametric&#8221; prefiltering and sampling geometry which minimizes aliasing effects and assures continuity within and between target images.</p> <p>Although the mapping of texture onto surfaces is an excellent example of the process and provided the original motivation for its development, pyramidal parametric data structures admit of wider application. The aliasing of not only surface texture, but also highlights and even the surface representations themselves, may be minimized by pyramidal parametric means.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Antialiasing]]></kw>
			<kw><![CDATA[Illumination models]]></kw>
			<kw><![CDATA[Modeling]]></kw>
			<kw><![CDATA[Pyramidal data structures]]></kw>
			<kw><![CDATA[Reflectance mapping]]></kw>
			<kw><![CDATA[Texture mapping]]></kw>
			<kw><![CDATA[Visible surface algorithms]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31101708</person_id>
				<author_profile_id><![CDATA[81100005753]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lance]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Williams]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Laboratory, New York Institute of Technology, Old Westbury, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>360353</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Blinn, J., and Newell, M., "Texture and Reflection on Computer Generated Images," CACM, Vol. 19, #10, Oct. 1976, pp. 542-547.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>906584</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bui-Tuong Phong, "Illumination for Computer Generated Pictures," PhD. dissertation, Department of Computer Science, University of Utah, December 1978.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907952</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Crow, F.C., "The Aliasing Problem in Computer Synthesized Shaded Images," PhD. dissertation, Department of Computer Science, University of Utah, Tech. Report UTEC-CSc-76-015, March 1976.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807383</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Dungan, W., Stenger, A., and Sutty, G., "Texture Tile Considerations for Raster Graphics," SIGGRAPH 1978 Proceedings, Vol. 12, #3, August 1978.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>362281</ref_obj_id>
				<ref_obj_pid>362258</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Eastman, Charles M., "Representations for Space Planning," CACM, Vol. 13,#4, April 1970.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807507</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Feibush, E.A., Levoy, M., and Cook, R.L., "Synthetic Texturing Using Digital Filters," Computer Graphics, Vol. 14, July, 1980.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Hanrahan, Pat, private communication, 1983.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Heckbert, Paul, "Texture Mapping Polygons in Perspective," NYIT Computer Graphics Lab Tech. Memo #13, April, 1983.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Klinger, A., and Dyer, C.R., "Experiments on Picture Representation Using Regular Decomposition," Computer Graphics and Image Processing, #5, March, 1976.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Knowlton, K., "Progressive Transmission of Gray-Scale and Binary Pictures by Simple, Efficient, and Lossless Encoding Schemes," Proceedings of the IEEE, Vol. 68, #7, July 1980, pp. 885-896.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Meagher, D., "Octree Encoding: A New Technique for the Representation, Manipulation, and Display of Arbitrary 3D Objects by Computer," IPL-TR-80-111, Image Processing Lab, Electrical and Systems Engineering Dept., Rensselaer Polytechnic Institute, October 1980.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578446</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Tanimoto, S.L., and Klinger, A., Structured Computer Vision, Academic Press, New York, 1980.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Tanimoto, S.L., and Pavlidis, T., "A Hierarchical Data Structure for Picture Processing," Computer Graphics and Image Processing, Vol. 4, #2, June 1975.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Tanimoto, S.L., "Image Processing with Gross Information First," Computer Graphics and Image Processing 9, 1979.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Warnock, J.E., "A Hidden-Line Algorithm for Halftone Picture Representation," Department of Computer Science, University of Utah, TR 4-15, 1969.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Williams, L., "Pyramidal Parametrics," SIGGRAPH tutorial notes, "Advanced Image Synthesis," 1981.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Yau, M.M., and Srihari, S.N., "Recursive Generation of Hierarchical Data Structures for Multidimensional Digital Images," Proceedings of the IEEE Computer Society Conference on Pattern Recognition and Image Processing, August 1981.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Pyramidal Parametrics Lance Williams Computer Graphics Laboratory New York Institute of Technology 
Old Westbury, New York Abstract The mapping of images onto surfaces may substantially increase the 
realism and information content of computer-generated imagery. The projection of a flat source image 
onto a curved surface may involve sampling difficulties, however, which are compounded as the view of 
the surface changes. As the projected scale of the surface increases, interpolation between the original 
samples of the source image is necessary; as the scale is reduced, approximation of multiple samples 
in the source is required. Thus a constantly changing sampling window of view-dependent shape must traverse 
the source image. To reduce the computation implied by these requirements, a set of prefiltered source 
images may be created. This approach can be applied to particular advantage in animation, where a large 
number of frames using the same source image must be generated. This paper advances a "pyramidal parametric" 
pre- filtering and sampling geometry which minimizes aliasing effects and assures continuity within 
and between target images. Although the mapping of texture onto surfaces is an excellent example of 
the process and provided the original motiva- tion for its development, pyramidal parametric data structures 
admit of wider application. The aliasing of not only surface texture, but also highlights and even the 
surface representations them- selves, may be minimized by pyramidal parametric means. General Terms: 
Algorithms. Keywords and Phrases: Antialiasing, Illumination Models, Modeling, Pyramidal Data Structures, 
Reflectance Mapping, Tex- ture Mapping, Visible Surface Algorithms. C R Categories: 1.3.3 [Computer 
Graphics]: Picture/Image Generation--~ algo- rithms; 1.3.5 [Computer Graphlc~: Compu- tational Geometry 
and Object Modeling-- curve, surface, solid and object represen- tations, geometric algorithms, languages 
and systems; 1.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-- color, shading, shadowing, 
and texture. Permission to copy without ~e all or part of this material is granted provided that the 
copies are not made or distributed ~r direct commercial advantage, the ACM copyright notice and the title 
of the &#38;#169; ACM 0-89791-109-1/83/007/0001 $00.75 ~. Pyramidal Data Structures Pyramidal data structures 
may be based on various subdivisions: binary trees, quad trees, oct trees, or n- dimensional hierarchies 
[17]. The common feature of these structures is a succes- sion of levels which vary the resolution at 
which the data is represented. The decomposition of an image by two-dimensional binary subdivision was 
a pioneering strategy in computer graphics for visible surface determination [15]. The approach was essentially 
a synthesis- by-analysis: the image plane was subdi- vided into quadrants recursively until analysis 
of a subsection showed that sur- face ordering was sufficiently simple to permit rendering. Such subdivision 
and analysis has been subsequently adopted to generate spatial data structures [5], which have been used 
to represent images [9] both for pattern recognition [13] and for transmission [i0], [14]. In the field 
of computer graphics, such data structures have been adopted for texture mapping [4], [16], and generalized 
to represent objects in space [Ii]. The application of pyramidal data to image storage and transmission 
may permit significant compression of the data to be stored or transmitted. This is so because highly 
detailed features may be localized within an otherwise low-frequency image, permitting the sampling rate 
to be reduced for large sections of the image. Besides permitting bandwidth compression, the representation 
orders data in such a way that the general character of images may be recalled or transmitted before 
the specific details. Pattern recognition and classifica- tion often require the comparison of a candidate 
image against a set of canonical patterns. This is an operation the expense of which increases as the 
square of the resolution at which it is per- formed. The use of pyramidal data struc- tures in pattern 
recognition and classifi- cation permits the comparison of the gross features of two-dimensional functions 
preliminary to the minute particulars; a good general reference on this application is [12]. publication 
and its date appear, and notice is given that copying is by permission of the Association for Computing 
Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. In computer 
graphics, pyramidal tex- ture maps may be used to perform arbitrary mappings of a function with minimal 
alias- ing artifacts and reduced computation. Once again, images may be represented at different spatial 
bandwidths. The concern is that inappropriate resolution misrepresents the data; that is, sampling high-resolution 
data at larger sample intervals invites aliasing. 2. Parametric Interpolation By a pyramidal parametric 
data struc- ture, we will mean simply a pyramidal structure with both intra-and inter-level interpolation. 
Consider the case of an image represented as a two-dimensional array of samples. Interpolation is neces- 
sary to produce a continuous function of two parameters, U and V. If, in addition, a third parameter 
(call it D) moves us up and down a hierarchy of corresponding two-dimensional functions, with interpola- 
tion between (or among) the levels of the pyramid providing continuity, the struc- ture is pyramidal 
parametric. ~he practical distinction between such a structure and an ordinary interpo- lant over an 
n-dimensional array of sam- ples is that the number of samples representing each level of the pyramid 
may be different. ~. Mip Mapping "Mip" mapping is a particular format for two-dimensional parametric 
functions, which, along with its associated address- ing scheme, has been used successfully to bandlimit 
texture mapping at New York Institute of Technology since 1979. The acronym "mip" is from the Latin phrase 
"multum in parvo," meaning "many things in a small place." Mip mapping supplements bilinear interpolation 
of pixel values in the texture map (which may be used to smoothly translate and magnify the tex- ture) 
with interpolation between prefil- tered versions of the map (which may be used to compress many pixels 
into a small place). In this latter capacity, mip offers much greater speed than texturing algorithms 
which perform explicit convolu- tion over an area in the texture map for each pixel rendered [I], [6]. 
 Mip owes its speed in compressing texture to two factors. First, a fair amount of filtering of the original 
tex- ture takes place when the mip map is first created. Second, subsequent filtering is approximated 
by blending different levels of the mip map. This means that all filters are approximated by linearly 
interpolating a set of square box filters, the sides of which are powers-of-two pix- els in length. Thus, 
mapping entails a fixed overhead, which is independent of the area filtered to compute a sample. G 
Figure (i) Structure of a Color Mip Map Smaller and smaller images diminish into the upper left corner 
of the map. Each of the images is averaged down from its larger predecessor. (Below:) Mip maps are indexed 
by three coordinates: U, V, and D. U and V are spatial coordi- nates of the map; D is the variable used 
to index, and interpolate between, th~ different levels of the pyramid. V V V ~LL Figure (I) illustrates 
the memory organization of a color mip map. The image is separated into its red, green, and blue components 
(R, G, and B in the diagram). Successively filtered and down- sampled versions of each component are 
instanced above and to the left of the originals, in a series of smaller and smaller images, each half 
the linear dimension (and a quarter the number of    4. Hi@blight Antialiasin@ As small or highly 
curved objects move across a raster, their surface nor- mals may beat erratically with the sam- pling 
grid. This causes the shading values to flash annoyingly in motion sequences, a symptom of illumination 
aliasing. The surface normals essentially point-sample the illumination function. Figure (12) illustrates 
samples of the surface normals of a set of parallel cylinders. The cylinders in the diagram are depicted 
as if from the edge of the image plane; the regularly-spaced vertical line segments are the samples along 
a sin- gle axis. The arrows at the sample points indicate the directions of the surface normals. Depending 
on the shading formula invoked, there may be very high contrast between samples where the normal is nearly 
parallel to the sample axis, and samples where the normal points directly at the observer's eye. Figure 
(12) 4) The shading function depends not only on the shape of the surface, but its light reflection 
properties (characterized by the shading formula), the position of the light source, and the position 
of the observer's eye. Hanrahan [7] expresses it in honest Greek: Ixly~(E,N,L) ~(u,v)0(x,y) dxdy where 
the normal, N, the light sources, L, and the eye, E, are vectors which may each be functions of U and 
V, and the limits of integration are the X, Y boundaries of the pixel. Figure (13) illustrates highlight 
aliasing on a perfectly flat surface. The viewing conventions of the diagram are the same as in Figure 
(12). "L" is the direc- tion vector of the light source; the sur- face is a polygon at an angle to the 
image plane; the dotted bump is a graph of the reflected light, characteristic of a Figure (13) .-",.. 
/ : i'. is Figure (14) :i ' \ ! i~ specular surface reflection function. The highlight indicated by 
the bump falls entirely between the samples. (Note that this is only possible on a flat surface if either 
the eye or the light is local, a point in space rather than simply a direc- tion vector. Some boring 
shading formulae exclude the possibility of highlight aliasing on polygons by requiring all flat surfaces 
to be flat in shading.) A first attempt to overcome the limi- tations of point-sampling the illumination 
function is to integrate the function over the projected area represented by each sample point. This 
approach is illus- trated in Figure (14). The brackets at each sample represent the area of the sur- 
face over which the illumination function is integrated. This procedure is analo- gous to area-averaging 
of sampled edges or texture [3]. In order to generalize this approach to curved surfaces, the "sample 
interval" over which illumination is integrated must be modified according to the local curva- ture 
of the surface at a sample. In Fig- ure (15), the area of a surface represented by a pixel has been 
projected onto a curved surface. The solid angle over which illumination must be integrated is approximated 
by the volume enclosed by the normals at the pixel corners. The distribution of light within this volume 
will sum to an estimate of the diffuse reflection over the pixel. If the surface exhibits undulations 
at the pixel level, however, aliasing will result. Figure (15)  32 x 32 64 x 64 Figures (20-23) Different 
resolution meshes. 5. Levels of Detail in Surface Represen- tation In addition to bandlimiting texture 
and illumination functions for mapping onto a surface, pyramidal parametrics may be used to limit the 
level of detail with which the surface itself is represented. The goal is to represent an object for 
graphic display as economically as its projection on the image plane permits, without boiling and sparkling 
aliasing artifacts as the projection changes. The expense of computing and shading each pixel dominates 
the cost of many algorithms for rendering higher-order sur- faces. For meshes of polygons or patch control 
points which project onto a small portion of the image, however, the vertex (or control-point) expense 
dominates. In these situations it is desirable to reduce the number of points used to represent the object. 
 A pyramidal parametric data structure the components of which are spatial coor- dinates (the X-Y-Z of 
the vertices of a rectangular mesh, for example, as opposed to the R-G-B of a texture or illumination 
map) provides a continuously-variable fil- tered instance of the surface for sampling at any desired 
degree of resolution. Figures (20) through (23) illustrate a simple surface based on a human face model 
developed by Fred Parke at the University of Utah. As the sampling den- sity varies, so does the filtering 
of the surface. These faces are filtered and sampled by the same methods previously discussed for texture 
and reflectance maps. Pyramidal parametric representa- tions such as these appear promising for reducing 
aliasing effects as well as sys- tematically sampling very large data bases over a wide range of scales 
and viewing angles. 6. Conclusions Pyramidal data structures are of pro- ven value in image analysis 
and have interesting application to image bandwidth compression and transmission. "Pyramidal parametrics," 
pyramidal data structures with intra- and inter-level interpolation, are here proposed for use in image 
syn- thesis. By continuously varying the detail with which data are resolved, pyramidal parametrics provide 
economical approximate solutions to filtering prob- lems in mapping texture and illumination onto surfaces, 
and preliminary experiments suggest they may provide flexible surface representations as well. 7. Acknowledgments 
 I would like to acknowledge Ed Cat- mull, the first (to my knowledge) to apply multiple prefiltered 
images to texture mapping: the method was applied to the bicubic patches in his thesis, although it was 
not described. Credit is also due Tom Duff, who wrote both recursive and scan- order routines for creating 
mip maps which preserved numerical precision over all map instances; Dick Lundin, who wrote the first 
assembly-coded mip map accessing routines; Ephraim Cohen, who wrote the second; Rick Ace, who translated 
Ephraim's PDP-II versions for the VAX assembler; Paul Heckbert, for refining and speeding up both creation 
and accessing routines, and investigating various estimates of "D"; Michael Chou, for implementing highlight 
antialiasing and high-resolution reflectance mapping on quadric surfaces. I owe special thanks to Jules 
Bloomenthal, Michael Chou, Pat Hanrahan, and Paul Heckbert for critical reading and numerous helpful 
suggestions in the course of preparing this text. Photographic sup- port was provided by Michael Lehman. 
 8. References [1] Blinn, J., and Newell, M., "Texture and Reflection on Computer Generated Images," 
CACM, Vol. 19, #i0, Oct. 1976, pp. 542-547. [2] Bui-Tuong Phong, "Illumination for Computer Generated 
Pictures," PhD. dissertation, Department of Computer Science, University of Utah, December 1978. [3] 
Crow, F.C., "The Aliasing Problem in Computer Synthesized Shaded Images," PhD. dissertation, Department 
of Com- puter Science, University of Utah, Tech. Report UTEC-CSc-76-015, March 1976. [4] Dungan, W., 
Stenger, A., and Sutty, G., "Texture Tile Considerations for Raster Graphics," SIGGRAPH 1978 Proceedings, 
Vol. 12, #3, August 1978. [5] Eastman, Charles M., "Representations for Space Planning," CACM, Vol. 
13, #4, April 1970. [6] Feibush, E.A., Levoy, M., and Cook, R.L., "Synthetic Texturing Using Digital 
Filters," Computer Graphics, Vol. 14, July, 1980. [7] Hanrahan, Pat, private communication, 1983. [8] 
Heckbert, Paul, "Texture Mapping Polygons in Perspective," NYIT Com- puter Graphics Lab Tech. Memo #13, 
April, 1983. [9] Klinger, A., and Dyer, C.R., "Experi- ments on Picture Representation Using Regular 
Decomposition," Computer Graphics and Image Processing, #5, March, 1976. [i0] Knowlton, K., "Progressive 
Transmis- sion of Gray-Scale and Binary Pic- tures by Simple, Efficient, and Loss- less Encoding Schemes," 
Proceedings of the IEEE, Vol. 68, #7, July 1980, pp. 885-896. [ii] Meagher, D., "Octree Encoding: A 
New Technique for the Representation, Manipulation, and Display of Arbi- trary 3D Objects by Computer," 
IPL- TR-80-111, Image Processing Lab, Electrical and Systems Engineering Dept., Rensselaer Polytechnic 
Insti- tute, October 1980. [12] Tanimoto, S.L., and Klinger, A., Structured Computer Vision, Academic 
Press, New York, 1980. [13] Tanimoto, S.L., and Pavlidis, T., "A Hierarchical Data Structure for Pic- 
ture Processing," Computer Graphics and Image Processing, Vol. 4, #2, June 1975. [14] Tanimoto, S.L., 
"Image Processing with Gross Information First," Com- puter Graphics and Image Processing 9, 1979. [15] 
Warnock, J.E., "A Hidden-Line Algo- rithm for Halftone Picture Represen- tation," Department of Computer 
Sci- ence, University of Utah, TR 4-15, 1969. [16] Williams, L., "Pyramidal Parametrics," SIGGRAPH tutorial 
notes, "Advanced Image Synthesis," 1981. [17] Yau, M.M., and Srihari, S.N., "Recur- sive Generation 
of Hierarchical Data Structures for Multidimensional Digi- tal Images," Proceedings of the IEEE Computer 
Society Conference on Pat- tern Recognition and Image Process- ing, August 1981. 11 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801127</article_id>
		<sort_key>13</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Lighting controls for synthetic images]]></title>
		<page_from>13</page_from>
		<page_to>21</page_to>
		<doi_number>10.1145/800059.801127</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801127</url>
		<abstract>
			<par><![CDATA[<p>The traditional point source light used for synthetic image generation provides no adjustments other than position, and this severely limits the effects that can be achieved. It can be difficult to highlight particular features of an object. This paper describes new lighting controls and techniques which can be used to optimize the lighting for synthetic images. These controls are based in part on observations of the lights used in the studio of a professional photographer. Controls include a light direction to aim a light at a particular area of an object, light concentration to simulate spotlights or floodlights, and flaps or cones to restrict the path of a light. Another control relates to the color of the light source and its effect on the perceived color of specular highlights. An object color can be blended with a color for the highlights to simulate different materials, paints, or lighting conditions. This can be accomplished dynamically while the image is displayed, using the video lookup table to approximate the specular color shift</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Light controls]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P64268</person_id>
				<author_profile_id><![CDATA[81100641681]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Warn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, General Motors Research Laboratories, Warren, Michigan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807403</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Atherton, Peter, Weiler, Kevin, and Greenberg, Donald, "Polygon Shadow Generation," SIGGRAPH 1978 Proceedings, Computer Graphics, Volume 12, Number 3, pp. 275-281.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bass, Daniel H., "Using the Video Lookup Table for Reflectivity Calculations: Specific Techniques and Graphic Results," Computer Graphics and Image Processing, Volume 17, pp. 249-261, 1981.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Beatty, J. C., Booth, K. S., and Matthies, L. H., "Revisiting Watkins Algorithm," CMCSS, University of Waterloo, Ontario, Canada, 1981.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360353</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Blinn, J. F., and Newell, Martin E., "Texture and Reflection in Computer Generated Images," Communications of the ACM, Volume 19, Number 10, pp. 542-547, October, 1976.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563893</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Blinn, J. F., "Models of Light Reflection for Computer Synthesized Pictures," SIGGRAPH 1977 Proceedings, Computer Graphics, Volume 11, Number 2, pp. 192-198.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>507101</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Blinn, J. F., "Simulation of Wrinkled Surfaces," SIGGRAPH 1978 Proceedings, Computer Graphics, Volume 12, Number 3, pp. 286-292.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801255</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Blinn, J. F., "Light Reflection Functions for Simulation of Clouds and Dusty Surfaces," SIGGRAPH 1982 Proceedings, Computer Graphics, Volume 16, Number 3, pp. 21-29.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Boyse, John W. and Gilchrist, Jack E., "GMSolid: Interactive Modeling for Design and Analysis of Solids," IEEE Computer Graphics and Applications, Volume 2, Number 2, pp. 27-40, March 1982.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806819</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Cook, Robert L. and Torrance, Kenneth E., "A Reflectance Model for Computer Graphics," SIGGRAPH 1981 Proceedings, Computer Graphics, Volume 15, Number 3, pp. 307-316.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563901</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Crow, Franklin C., "Shadow Algorithms for Computer Graphics, " SIGGRAPH 1977 Proceedings, Computer Graphics, Volume 11, Number 2, pp. 242-248.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806801</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Dill, John C., "An Application of Color Graphics to the Display of Surface Curvature," SIGGRAPH 1981 Proceedings, Computer Graphics, Volume 15, Number 3, pp. 153-161.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Joyce, J. D. and Oliver, N. N., "REGIS - A Relational Information System with Graphics and Statistics," Proceedings of the AFIPS National Computer Conference 1976, Volume 45, pp. 839-844.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807438</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Kay, Donald Scott, and Greenberg, Donald, "Transparency for Computer Synthesized Images," SIGGRAPH 1979 Proceedings, Volume 13, Number 2, pp. 158-164.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Oliver, N. N., "COLORSTRESS - A System for Superimposing Stress Patterns in Color on Three Dimensional Structures," General Motors Research Publication GMR-3712, November, 1981.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360839</ref_obj_id>
				<ref_obj_pid>360825</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Phong, B. T., "Illumination for Computer Generated Pictures," Communications of the ACM, Volume 18, Number 6, pp. 311-317, June 1975.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806818</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Potmesil, Michael, and Chakravarty, Indranil, "A Lens and Aperature Camera Model for Synthetic Image Generation", SIGGRAPH 1981 Proceedings, Computer Graphics, Volume 15, Number 3, pp. 297-305.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>810236</ref_obj_id>
				<ref_obj_pid>800179</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Newell, Martin E., and Blinn, James F., "The Progression of Realism in Computer Generated Images," ACM 77, Proceedings of the Annual Conference, pp. 444-448.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Newman, William M. and Sproull, Robert F., Principles of Interactive Computer Graphics, Second Edition, McGraw-Hill, Chapter 25, 1979.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356626</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I. E., Sproull R. F., and Schumacker, R. A., "A Characterization of Ten Hidden Surface Algorithms," ACM Computing Surveys, Volume 6, Number l, pp. 1-55, March 1974.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Warn, David R., "VDAM - A Virtual Data Access Manager for Computer Aided Design," Proceedings of the ACM Workshop on Databases for Interactive Design, pp. 104-111, September 1975.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Watkins, G. S., "A Real-Time Visible Surface Algorithm," Computer Science Department, University of Utah, UTECH-CSC-70-101, June 1970.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Whitted, Turner, "An Improved Illumination Model for Shaded Display," Communications of the ACM, Volume 23, Number 6, pp. 343-349, June 1980.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 LIGHTING CONTROLS FOR SYNTHETIC IMAGES David R. Warn Computer Science Department General Motors Research 
Laboratories Warren, Michigan 48090 ABSTRACT The traditional point source light used for synthetic 
image generation provides no adjustments other than position, and this severely limits the effects that 
can be achieved. It can be difficult to highlight particular features of an object. This paper describes 
new lighting controls and techniques which can be used to optimize the lighting for synthetic images. 
These controls are based in part on observations of the lights used in the studio of a professional photographer. 
Controls include a light direction to aim a light at a particular area of an object, light concen- tration 
to simulate spotlights or floodlights, and flaps or cones to restrict the path of a light. Another control 
relates to the color of the light source and its effect on the perceived color of specular highlights. 
An object color can be blended with a color for the highlights to simulate different materials, paints, 
or lighting conditions. This can be accomplished dynamically while the image is displayed, using the 
video lookup table to approximate the specular color shift. CR Categories and Subject Descriptors: 1.3.3 
[Computer Graphics]: Picture/Image Generation - display algorithms; 1.3.7 [Computer Graphics]: Three-Dimensional 
Graphics and Realism - color, shading, shadowing, and texture; Additional Key Words and Phrases: light 
controls INTRODUCTION The lighting model and controls described in this paper are part of GMR AUTOCOLOR, 
a system developed at the General Motors Research Laboratories for automatic generation of color synthetic 
images. The major application for this Permission to copy without fee all or part of this material is 
granted provided that the copies are not made or distributed for direct commercial advantage, the ACM 
copyright notice and the title of the publication and its date appear, and notice is given that copying 
is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. &#38;#169; ACM 0-89791-109-1/83/007/0013 $00.75 capability within GM 
is design evaluation, to view a realistic image prior to or in place of building a physical model in 
wood or clay. It can be used both to display automotive body surfaces to aid in their aesthetic evaluation 
and to display automotive parts constructed with the GMSolid [8] modeling system to aid in visualizing 
complex geometric shapes. GMR AUTOCOLOR can also be used to display color coded scalar values such as 
stress [14], pressure, or curvature [ii] on the surface of an object. The geometric shape of an object 
is approximated by a mesh of three or four sided planar polygons. The polygonal representation was selected 
both for its availability within GM and for its efficiency. Polygons can be grouped to form parts. For 
a car body there might be separate parts for the door, hood, roof, fender, etc. Each part is assigned 
a surface type such as body or glass, and each type of surface has associated color and reflectance properties. 
The entire data structure is stored in tables using the GMR developed REGIS interactive relational data 
base [12]. The polygonal mesh is defined by two tables: a node table containing the Cartesian coordinates 
of the node points and a polygon table containing the node numbers to be connected to form each polygon. 
Auxiliary tables containing the part, surface, and lighting parameters can be supplied as input or created 
automatically by the system as they are needed. The tables are stored in virtual memory files [20] and 
are all directly addressable during image generation. This data organization is very efficient because 
it eliminates all I/O other than paging. GMR AUTOCOLOR is a completely interactive system using menus 
and screen selection to define parameters and options, select a viewing orientation, or mix a color. 
A Watkins based scan line algorithm [3,21] is used for hidden surface elimination to determine which 
point on the object is visible at each pixel location. This algorithm is well suited for the polygonal 
representation. See [19] for a survey and comparison of other hidden surface algorithms. The system 
is implemented in PL/I and runs on a VAX 11/780 computer under VMS. A RAMTEK 9300 is used to display 
the generated color images. Computer Graphics Volume 17, Number 3 July 1983 THE LIGHTING MODEL  The 
lighting model determines the intensity of the reflected light that reaches the eye from a given point 
on the object. It takes into account the reflectance properties of the surface as well as the physics 
of light reflection. This lighting computation is performed scan line by scan line as the hidden surface 
algorithm determines which portion of a polygon is visible. A great deal of research has been reported 
on lighting models for synthetic images. See [17] for a good summary of early work with historical perspective 
and Newman and Sproull [18] for a general discussion of the issues involved. The Phong [15] model was 
the first to achieve realistic highlights using interpolation of surface normals and an approximation 
of specular reflection. This model has been extended and modified by others to incorporate improved highlights 
[5] and global scene illumination (multiple reflections) [22]. Other related work affecting the realism 
of synthetic images includes shadows [i,i0], texture [4,6], transparency [13], clouds [7], a camera model 
[16], and surface reflection color [9]. The lighting model used in GMR AUTOCOLOR is based on the original 
Phong approach. The extensions which are described in this paper relate to providing controls and adjustments 
on individual lights and not to the particular method used to model light reflection. They could be applied 
to any lighting model. Using the Phong model to determine the intensity contribution from each light 
and with the additional controls described in the following section, it is possible to generate very 
realistic images of automotive body surfaces and parts. To provide a framework for discussing these extensions, 
the Phong lighting model will first be reviewed. The geometry of light reflection is illustrated in 
Figure i for a light illuminating a point P on the surface of an object. Normal Reflected LIGHT Ray 
EYE Figure 1. Geometry of Light Reflection  The positions of the EYE and each of the lights are specified 
by the user. The angle of incidence I is the angle between the ray from the light source and the surface 
normal at the point P. The angle of reflection R is equal to the angle of incidence. The direction of 
the surface normal vector is computed from the node normals using bilinear interpolation. The intensity 
contribution for each light is modeled as the sum of diffuse and specular components multiplied by the 
intensity of the light source as shown in Figure 2. Diffuse reflection is light which is reflected uniformly 
in all directions. It depends only on the position of the light source, not on the position of the viewer 
and is modeled as the cosine of the angle of incidence, obeying Lambert's law. Specu- lar reflection 
makes the object appear shiny by generating highlights in the mirror direction, taking into account the 
position of the viewer and the surface finish. The effect of specular reflection is approximated by a 
cosine raised to a power. This value will peak when the reflected ray points directly at the EYE so that 
the angle E is zero. It will fall off either gradually or quickly depending on the value of the exponent 
N which corresponds to the glossiness of the surface. Dull surfaces have a small value of N and shiny 
surfaces have a high value. light(j) = intensity(j)*(diffuse + specular) where: diffuse = D(cos(I)) 
 specular = S(cosN(E)) intensity(j) = intensity of light j D is the diffuse coefficient, S is the 
specular coefficient, N is the specular exponent, I is the angle of incidence, E is the angle between 
the reflected ray and the ray to the eye. Figure 2. Phong Intensity Computation for each Light  D, 
S, and N are the reflectance properties associated with each type of surface to be displayed. The coefficients 
D and S control the proportion of diffuse and specular reflection. For a more accurate rendering, the 
specular coefficient S should be a function of the angle of incidence I as well as the type of surface 
 (material) [18]. The values of cos(I) and cos(E) can be easily computed using the dot product of normalized 
unit vectors [18]. The intensity must be calculated for each light source and every point on the object 
which corresponds to a visible pixel in the image. The intensity at each point is the sum of the contributions 
from all lights plus an ambient term. The ambient intensity indicates the general level of illumination 
for the entire scene. The method used for computing these intensities is the first departure from the 
basic Phong model. The light intensity at each pixel is computed as an unscaled floating point value 
in a virtual picture array. If an object is composed of multiple surfaces, the surface number being displayed 
at each pixel is stored in an auxiliary picture sized array. As the image generation pro- ceeds, the 
maximum and minimum unscaled intensity values for the entire image and for each surface type are determined. 
The unscaled intensities are then mapped into integer intensities for the display. This rescaling process 
can be based on the unsealed intensity range for each surface or for the entire object. This approach 
places no restrictions on the range of values for the diffuse and specular coefficients D and S or on 
the number and intensity of light sources. It also permits experimentation with nonlinear mappings of 
the unscaled values. LIGHT CONTROLS The traditional point source light used for synthetic image generation 
provides no controls or adjustments other than position, and this severely limits the effects that 
can be achieved. The light controls described in this section were developed for GMR AUTOCOLOR after 
observing the lighting used to photograph a car in a studio~ Cars are not photographed with a bare lO0 
watt light bulb (the analogy of a point source light). Many lights are used and each is carefully adjusted 
to" produce a very specific highlight. The light controls described below are implemented as modifiers 
of the intensity value determined at each point by the Phong lighting model and there- fore could be 
applied to any other lighting model. Light Direction and Concentration The two most basic lighting 
controls are direction and concentration. These controls make it possible to aim the light and to adjust 
the distribution over an area. Point source lights are very difficult to position in order to produce 
highlights in a particular area of an object, and they can cause unwanted highlights in other areas. 
Each time a light is moved, the entire image must be recomputed which takes i to 5 minutes, depending 
on the number of polygons and the area of the screen covered by visible surfaces. This makes experimentation 
with lighting very tedious. Light direction and concentration controls make it possible to isolate the 
effect of a light to a particular area and achieve a desired highlight more quickly. This is not because 
the lighting model computations are any faster, but because the lights behave in a more "natural" way. 
A different approach to make lights easier to adjust is suggested in [2], using the lookup table to store 
surface normal information to permit dynamic lighting changes. On real lights, direction and concentration 
are produced by reflectors, lenses, and housings. It would be possible to model these components directly, 
but that would introduce considerable overhead to the lighting computation. A simpler approach is to 
model the overall effect of such a system rather than the individual causes. A directed light can be 
modeled mathematically as the light emitted by a single point specular reflecting surface illuminated 
by a hypothetical point source light. Think of the point labelled "Light" in Figure 3 as a surface which 
reflects light onto the object. The normal orientation of this single point surface is controlled by 
the light direction vector. A hypothetical point source light located along this vector illuminates the 
reflector surface which, in turn, reflects  light onto the object. LIGHT Normal Reflected Ray EYE Direction 
P Figure 3. Light Direction  We can compute the intensity of the reflected light at a point P on the 
object using the model in Figure 2 and treating P as the eye position. Because the hypothetical point 
source light illuminating the reflector (but nothing else) is located on the surface normal, the angle 
of incidence is 0 and the reflected ray coincides with the light direction vector. The angle between 
the reflected ray and the ray to the eye (the point P) is then the angle labelled L in Figure 3, between 
the light direction and the ray from the light position to the point P. Since we are considering the 
light to be a specular reflecting surface, we will use a diffuse coefficient of 0 and a specular coefficient 
of 1. The intensity at the point P from the directed light source is then: intensity(j)*cosC(L)  where 
C is the specular exponent of the light reflector surface. The exponent C provides control over the concentration 
of the light. By increasing the value of C, the light becomes more concentrated around the primary direction. 
This can be used to simulate the effect of a spot light or a flood light. The light can be aimed by adjusting 
the orientation of the light direction vector. The above expression for the directed light intensity 
at a point can then be used to compute the intensity seen at the actual eye position from that point, 
again using the model in Figure 2. The intensity contribution from a directed light can then be expressed 
as: (cosC(L))*light(j)  for cos(L) > 0 and light(j) computed as shown in Figure 2. Notice that when 
the concentration exponent C = 0 the directional multiplier becomes 1 and we have a point source light. 
 The effect of directed light can be seen in Figure 4. Figure 4.A shows a scene without directed lights. 
There is one point source light located about half the height of the pyramid above the plane. Specular 
reflection causes a highlight on the plane, which may not be desired. In Figure 4.B the same light has 
been aimed at the side of the pyramid, and the concentration exponent C is set to 1 for a floodlight 
effect. This eliminates the bright spot on the plane. In Figure 4.C the concentration exponent C has 
been increased to 40 for a spotlight effect. Obviously, additional lights are needed to provide general 
illumination in this scene. The user can position and aim the lights using the interactive display shown 
in Figure 5. Three views of the object are shown: top, side, and  front. Each view shows the eye position 
and view direction as well as the position and direction of each light. Point source lights would be 
shown with only a cross at the light position. To change any of these locations, the user selects the 
current position in one of the three views and then selects the new position in that view. By changing 
a position or direction in two of the three views, all three coordinates can be quickly changed without 
having to enter numeric coordinate values. By moving the eye position or view direction, the view orientation 
can easily be changed. A small mesh image of the current view can be generated by selecting DISPLAY VIEW. 
This can be used to quickly verify that the desired orientation has been achieved. RESCALE can be selected 
to move closer or farther away. This control automatical- ly rescales the three views based on the maximum 
coordinate values to leave a reasonable amount of space on all sides. ANGLE is used to select a rotation 
angle for the horizontal axis of the image plane. Figure 6 shows the image generated for a 1983 Chevrolet 
Camaro using the lights and view from Figure 5. The polygonal mesh was generated from the actual design 
data base for the vehicle. The mesh has 9619 node points, 8136 polygons, and 17940 distinct edges. Figure 
7 shows the same image with a single point source light located at the eye. Figure 8 shows the effect 
of each of the lights individually. Light Flaps and Cones Two additional light controls can be used 
to generate special effects by limiting the area illuminated by a light. Studio lights are equipped with 
large flaps called barndoors which can be used to restrict the path of a light. They can emphasize a 
crease line by lining up the flap with the crease to put all the light above or below the line. Light 
parameters provide flaps in the X, Y, and Z directions to restrict light to a particular range of object 
coordinates. A minimum and maximum value can be specified for each coordinate, and a separate switch 
controls whether the flap is on or off. Maximum Z Flap ....................................~f LIGHT 
/Object  The implementation of flaps is really quite simple. If any of the flap switches are on for 
a particular light, the object coordinates of the point being displayed are compared with the flap maximum 
and minimum values. If the point lies within the range, the lighting model is evaluated for that light. 
Otherwise, that light has no effect. Z flaps are illustrated in Figure 9. The light has no effect below 
the Z minimum flap or above the Z maximum flap. This implementation could easily be extended to provide 
general flaps which are not parallel to the coordinate axes. It is possible to use flaps to produce 
effects which have no physical counterpart, for example, to drop an invisible curtain beyond which a 
light will have no effect. Using an X or Y maximum flap, the light would not illuminate any portion of 
an object which extended beyond the flap setting. Another light control is a variable sized cone surrounding 
the light direction. This feature is not found on studio lights, but it can be used to produce a sharply 
delineated spotlight (as opposed to the tapering off which occurs using the concentration exponent) or 
to isolate a light on a single object in a scene composed of several objects. The cone size is specified 
by a cone angle, A, about the light direction as shown in Figure i0. The light has no effect if the angle 
L between the light direction ray and the ray to the point P on the surface is greater than the cone 
angle A. LIGHT Light pDirection Figure 10. Light Cone Angle The cone can be easily implemented as 
an extension of the light direction. Cos(L) is already being computed so it is only necessary to test 
if cos(L) < = cos(A) to determine if the point lies within the cone. If the point is outside the cone, 
the intensity contribution is zero, and no further evaluation of the lighting model is required for that 
light. All of these controls --direction, concentration, flaps, and cones --can be used together or 
separately to produce the desired effect on the image. They are provided as lighting model param- eters 
and can be selected from a menu by the user. Minimum Z Flap I Figure 9. Light Flaps   Light Source 
Color Blending At SIGGRAPH 81 Cook and Torrence [9] described a model for surface reflection. Their 
paper discusses in detail the importance of modeling the color of the specular reflection to realistically 
portray different materials. They show that non-homogeneous materials such as painted objects and plastics 
have specular and diffuse components that do not have the same color. For metals, the diffuse component 
is negligible and the color of the specular component is determined by the light source and the reflectance 
properties of the metal. The color blending technique described below and implemented in GMR AUTOCOLOR 
uses the video looku~ table to control the color of specular reflection. A user can dynamically blend 
an object color with a color for the specular highlights to simulate different materials or paints under 
variable lighting conditions. It is important to note that this method does not compute or predict the 
color of the specular component. It does provide a means of generating realistic effects without adding 
overhead to the lighting model. The key to this approach is the method used to map the intensity values 
into video lookup table entries. The image intensities computed by the lighting model are mapped into 
separate sequential ranges of video lookup table entries for each surface type as shown in Figure ii. 
The value stored in the frame buffer is then logically equivalent to intensity for each surface type. 
By replacing the video lookup table values in one of the ranges, the corresponding surface color can 
be changed without recomputing the image. This approach requires a parallel lookup table which defines 
all three color components from a single value in the frame buffer. It cannot be used with three independent 
eight bit lookup tables. Red Green Blue 0 ..ooooo..ooooo...o. Minimum Intensity Color Values for 
Intensity ..... Surface Type i Maximum Intensity ...of.............. Color Values for Surface Type 
2 ...............coo. Frame Buffer 640 x 480 l0 Bit Values 1023 Video Lookup Table 24 Bit Entries 
Figure ll. Intensity Mapping Two colors are specified for each surface type, one for the diffuse component 
and the other for the specular component. As noted in [9], the color of the specular component may be 
the color of the material, the color of the light source, or a color derived from the light source and 
the material. Fox example, simulating plastic requires a colored diffuse component which is added to 
a white specular component. The relative amount of diffuse and specular intensity determines the resulting 
color. If we assume that each intensity level in the image corresponds to exactly one combination of 
diffuse and specular intensities, then we can store a fixed set of colors in the lookup table so that 
each intensity is associated with a particular combination of the diffuse and specular colors. This is 
an obvious simplification since many different combinations of diffuse and specular intensities could 
produce the same total intensity, but it works quite well. The lookup table color values for a surface 
are obtained by combining the diffuse and specular colors. A blend intensity level determines the point 
at which the color begins to shift from the diffuse color to the specular color. All intensities below 
the blend intensity are assumed to be purely diffuse. In the intensity range from the blend intensity 
to the maximum intensity the color shifts from diffuse to specular as shown in Figure 12. The blending 
is done by determining the number of intensity levels N in the range and then reducing the diffuse color 
contribution and increasing the specular contribution by I/N for each intensity increment. Video Lookup 
Table Range for One Surface Type Specular 1 Diffuse Color ~ I Minimum Maximum Intensity Blend Intensity 
Intensity Figure 12. Color Blending This blending method assumes that the highest intensity values 
for a surface correspond to specular reflection and that as the intensity reaches the maximum, thespecular 
color dominates. The brightest points are assumed to be i00% specular. Since the diffuse component never 
really goes away, another control should be added to adjust the percentage of the specular color at the 
maximum intensity. Even with the simplifica- tions and assumptions inherent in this approxima- tion, 
the resulting images appear realistic.  The colors of the diffuse and specular components for each surface 
type are assumed to be known. Interactive color mixing permits the user to experiment with different 
color combinations without recomputing the image. The frame buffer image intensities are computed in 
a virtual memory array by the lighting model. This array is simply retransmitted to the display after 
color mixing is completed. The interactive display used for mixing and blending colors for a surface 
is shown in Figure 13. Here the diffuse and specular colors are labelled object and light source for 
the benefit of users. The object (diffuse) color is displayed in the lower portion of the screen with 
all intensity values shown in the area to the left of the color bars. By selecting proportions on the 
red, green and blue bars, any color can be mixed and immediately displayed in the area to the left. The 
white bar can be used to quickly increase or decrease all three color components equally to generate 
tints or shades. The light source (specular) color is displayed similarly in the upper portion of the 
screen. Here it is shown as white but any color can be mixed. The horizontal bar in the middle of the 
display is used to select the blend intensity and shows the resulting color shift. Intensity values increase 
from left to right across the bar and the color shifts from the object color to the light source color 
in the intensity range to the right of the blend intensity. For a given combination of object and light 
source colors, the user can interactively adjust the blend intensity and watch the image change dynamically 
on the screen to achieve a desired effect. This feature is illustrated in Figure 14. The vertical bar 
indicates intensity with the highest intensity at the top and lowest at the bottom. By selecting a new 
blend intensity on this bar, the image is redisplayed instantly with the new light blend. The sequence 
in Figure 15 shows the effects that can be generated using the same colors without recomputing the image. 
Only the body color is being changed. Figure 15.A uses only the object color. As the blend intensity 
point is reduced, the light source color is introduced at lower intensities as shown in Figure 15.B 
and 15.C. With the blend intensity at the lowest point in Figure 15.C, it gives the appearance of metallic 
paint. CONCLUSIONS  The addition of controls on lights makes it possible to produce synthetic images 
with optimized highlights and special effects. Because they behave more like the actual lights used in 
studio photography, controlled lights are easier for users to position and adjust. With directed lights, 
undesirable side effects can be eliminated and a particular highlight can be obtained much faster than 
with point source lights. Adjustable concentration provides control over the distribu- tion of light 
from an isolated spotlight to a general floodlight. The flap and cone angle controls provide special 
effects similar to studio lighting. Interactive, dynamic control over object and light source color blending 
makes it possible to simulate different materials or paints. Not all of these controls are used with 
every image, but by providing them as an integral part of the lighting model, they are available when 
the need arises. ACKNOWLEDGEMENTS This work has been accomplished in cooperation with GM Design Staff. 
I would particularly like to thank John Tuckfield and Tom Boswell for providing the polygonal mesh data 
for the Camaro using the GM Corporate Graphic System.  REFERENCES i. Atherton, Peter, Weiler, Kevin, 
and Greenberg, Donald, "Polygon Shadow Generation," SIGGRAPB 1978 Proceedings, Computer Graphics, Volume 
12, Number 3, pp.275-281. 2. Bass, Daniel H., "Using the Video Lookup Table for Reflectivity Calculations: 
Specific Techniques and Graphic Results," Computer Graphics and Image Processing, Volume 17, pp. 249-261, 
1981.  3. Beatty, J. C., Booth, K. S., and Matthies, L. H., "Revisiting Watkins Algorithm," CMCSS, University 
of Waterloo, Ontario, Canada, 1981  4. Blinn, J. F., and Newell, Martin E., "Texture and Reflection 
in Computer Generated Images," Communications of the ACM, Volume 19, Number i0, pp. 542-547, October, 
1976.  5. Blinn, J. F., "Models of Light Reflection for Computer Synthesized Pictures," SIGGRAPH 1977 
Proceedings, Computer Graphics, Volume Ii, Number 2, pp. 192-198.  6. Blinn, J. F., "Simulation of Wrinkled 
Sur- faces," SIGGRAPH 1978 Proceedings, Computer Graphics, Volume 12, Number 3, pp.286-292.  7. Blinn, 
J. F., "Light Reflection Functions for Simulation of Clouds and Dusty Surfaces," SIGGRAPH 1982 Proceedings, 
Computer Graphics, Volume 16, Number 3, pp.21-29.  8. Boyse, John W. and Gilchrist, Jack E., "GMSolid: 
Interactive Modeling for Design and Analysis of Solids," IEEE Computer Graphics  and Applications, 
Volume 2, Number 2, pp. 27-40, March 1982. 9. Cook, Robert L. and Torrance, Kenneth E., "A Reflectance 
Model for Computer Graphics," SIGGRAPH 1981 Proceedings, Computer Graphics, Volume 15, Number 3, PP. 
307-316. i0. Crow, Franklin C., "Shadow Algorithms for Computer Graphics," SIGGRAPH 1977 Proceedings, 
Computer Graphics, Volume ll, Number 2, pp.242-248. ii. Dill, John C., "An Application of Color Graphics 
to the Display of Surface Curvature," SIGGRAPH 1981 Proceedings, Computer Graphics, Volume 15, Number 
3, PP. 153-161. 12. Joyce, J. D. and Oliver, N. N., "REGIS -A Relational Information System with Graphics 
and Statistics," Proceedings of the AFIPS National Computer Conference 1976, Volume 45,  pp. 839-~44. 
 13. Kay, Donald Scott, and Greenberg, Donald, "Transparency for Computer Synthesized Images," SIGGRAPH 
1979 Proceedings, Volume 13, Number 2, pp. 15~-164.   REFERENCES  14. Oliver, N. N., "COLORSTRESS 
-A System for Superimposing Stress Patterns in Color on Three Dimensional Structures," General Motors 
Research Publication GMR-3712, November, 1981.  15. Phong, B. T., "Illumination for Computer Generated 
Pictures," Communications of the ACM, Volume 18, Number 6, pp. 311-317, June 1975.  16. Potmesil, Michael, 
and Chakravarty, Indranil, "A Lens and Aperature Camera Model for Synthetic Image GenerationS, SIGGRAPH 
1981 Proceedings, Computer Graphics, Volume 15, Number 3, PP. 297-305.  17. Newell, Martin E., and Blinn, 
James F., "The Progression of Realism in Computer Generated Images," ACM 77, Proceedings of the Annual 
Conference, pp. 444-448.  18. Newman, William M. and Sproull, Robert F., Principles of Interactive Computer 
Graphics, Second Edition, McGraw-Hill, Chapter 25, 1979.  19. Sutherland, I. E., Sproull R. F., and 
  Schumacker, R. A., "A Characterization of Ten Hidden Surface Algorithms," ACM Computing Surveys, 
Volume 6, Number l, pp. 1-55, March 1974. 20. Warn, David R., "VDAM -A Virtual Data Access Manager 
for Computer Aided Design," Proceedings of the ACM Workshop on Databases for Interactive Design, pp. 
I04-ii1, September 1975.  21. Watkins, G. S., "A Real-Time Visible Surface  Algorithm," Computer Science 
Department, University of Utah, UTECH-CSC-70-101, June 1970.  22. Whitted, Turner, "An Improved Illumination 
Model for Shaded Display," Communications of the ACM, Volume 23, Number b, pp. 343-349, June 1980.  
  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801128</article_id>
		<sort_key>23</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Artificial texturing]]></title>
		<subtitle><![CDATA[An aid to surface visualization]]></subtitle>
		<page_from>23</page_from>
		<page_to>29</page_to>
		<doi_number>10.1145/800059.801128</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801128</url>
		<abstract>
			<par><![CDATA[<p>Texture is an important surface characteristic which provides a great deal of information about the nature of a surface, and several computationally complex algorithms have been implemented to replicate realistic textures in computer shaded images. Perceptual psychologists have recognized the importance of surface texture as a cue to space perception, and have attempted to delineate which factors provide primary shape information. A rendering algorithm is presented which uses these factors to produce a texture specifically designed to aid visualization. Since the texture is not attempting to replicate an actual texture pattern, it is called &#8220;artificial&#8221; texturing. This algorithm avoids the computational limitations of other methods because this &#8220;artificial&#8221; approach does not require complex mappings from a defined texture space to the transformed image to be rendered. A simple filtering method is presented to avoid unacceptable aliasing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39083061</person_id>
				<author_profile_id><![CDATA[81100265872]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dino]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schweitzer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, University of Utah, Salt Lake City, Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>360353</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Blinn, James F. and Newell, Martin E. "Texture and Reflection in Computer Generated Images", Communications of the ACM, Vol. 19, No. 10, October 1976, pp. 542-547.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>507101</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Blinn, J. F. "Simulation of Wrinkled Surfaces", Computer Graphics, Vol. 12, No. 3, August 1978, pp. 286-292.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Braunstein, M. L. Depth Perception through Motion, Academic Press, New York, 1976.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Catmull, E. "A Subdivision Algorithm for Computer Display of Curved Surfaces", Tech. Report No. UTEC-CSc-74-133, University of Utah, December 1974.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807507</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Feibush, E. A., Levoy, M., and Cook, R. L. "Synthetic Texturing using Digital Filters", Computer Graphics, Vol. 14, No. 3, July 1980, pp. 294-301.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>6684</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Foley, J. D. and Van Dam, A. Fundamentals of Interactive Computer Graphics, Addison-Wesley, Reading, Mass., 1982.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Gibson, J. J. The Perception of the Visual World, The Riverside Press, Cambridge, Mass, 1950.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Haber, Ralph N. and Henderson, Maurice. The Psychology of Visual Perception, 2nd ed., Holt, Rhinehart and Winston, New York, 1980.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Newman, William F. and Sproull, Robert F. Principles of Interactive Computer Programming, 2nd ed., McGraw-Hill, New York, 1979.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801252</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Norton, A., Rockwood, A. P., and Skolmoski, P. T. "Clamping: A Method of Antialiasing Textured Surfaces by Bandwidth Limiting in Object Space", Computer Graphics, Vol. 16, No. 3, July 1982, pp. 1-8.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Phillips, R. J. "Stationary Visual Texture and the Estimation of Slant Angle", Quarterly Journal of Experimental Psychology, Vol., No. 22, 1970, pp. 389-397.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Artificial Texturing: An Aid to Surface Visualization Dino Schweitzer Computer Science Department University 
of Utah Salt Lake City, Utah 84112 Abstract Texture is an important surface characteristic which provides 
a great deal of information about the nature of a surface, and several computationally complex algorithms 
have been implemented to replicate realistic textures in computer shaded images. Perceptual psychologists 
have recognized the importance of surface texture as a cue to space perception, and have attempted to 
delineate which factors provide primary shape information. A rendering algorithm is presented which uses 
these factors to produce a texture specifically designed to aid visualization. Since the texture is not 
attempting to replicate an actual texture pattern, it is called "artificial" texturing. This algorithm 
avoids the computational limitations of other methods because this "artificial" approach does not require 
complex mappings from a defined texture space to the transformed image to be rendered. A simple filtering 
method is presented to avoid unacceptable aliasing. CR Categories and Subject Descriptors: 1.3.3 [Computer 
Graphics]: Picture/Image Generation -display algorithms; 1.3.7 [Computer Graphics]: Three-Dimensional 
Graphics and Realism -color, shading, shadowing and texture General Terms: Algorithms This work was 
supported in part by the National Science Foundation (MCS-8004116 and MCS-8121750), the U.S. Army Research 
Office (DAAG29-81K-0111 and DAAG29-82-K-0176), and the Office of Naval Research(N00014-82-K-0351). Permission 
to copy without fee all or part of this material is granted providedthat the copies are not made or distributed 
for direct commercialadvantage, the ACM copyright notice and the title of the publicationand its date 
appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy otherwise, or to republish, requires a fee and/or specific permission. &#38;#169; ACM 0-89791-109-1/83/007/0023 
$00.75  1. Introduction Almost all physical surfaces contain some microstructure or texture visible 
to the human eye. This texture gives us information such as the type of material which composes the object 
and the relative smoothness or coarseness of the surface. Computer generated images which portray such 
texture provide the same types of information and appear extremely realistic. Unfortunately, the algorithms 
for generating such images are generally time consuming and require knowledge of how to map the texture 
onto the surface. Perceptual psychologists have analyzed a different type of information provided by 
texture: that of space perception [7] [3] [8]. The microstructure of a surface provides, a somewhat regular 
pattern for visualization. The changes in this pattern, or texture gradient, give strong cues as to the 
orientation and depth of the surface. Extensive studies and experiments have shown that there are three 
characteristics of texture which provide this perceptual information: size, shape, and density. Changes 
in these components due to standard perspective and projective transformations provide knowledge about 
surface depth and changes in the orientation of the surface. To avoid confusion in terminology, "texture" 
in this paper will not refer to the intuitive meaning of changes in the normal direction due to the surface 
microstructure. Rather, it is the pattern on the surface which is of interest. This pattern may be caused 
by different colors as in a tiled floor, or different intensities because of changing normals such as 
the skin of an orange. As will be discussed, it is the changes in this pattern which provide a basis 
for perception. This paper describes a texturing technique which approximates the texture changes caused 
by distance from the viewer and orientation of a surface without attempting to exactly render a realistic 
texture. This approach supplies several of the same visualization cues as provided by exact texture rendering 
algorithms without the complexity of generating a realistic texture. Thus, "artificial" texturing is 
a means for providing an inexpensive aid to visualizing the shape of a shaded surface. 2. Background 
To understand the objectives and advantages of this approach, some background will he provided on the 
role of textures in perception and conventional algorithms to simulate such texture. 2.1. Texture in 
Perception Visual perception has been studied by different groups over many centuries. Early artists 
were interested in making paintings appear realistic, and thus developed the concepts of perspective, 
shading, and several other "cues" which could be used to imply depth. Physiologists and psychologists 
have studied vision and perception in an attempt to understand the workings of the eye and the brain. 
The results of these studies have been applied to computer generated images producing very realistic 
results. The classic paradox in the study of perception is how one perceives depth from a two--dimensional 
retinal image. Cue theory attempts to explain this dilemma by suggesting that it is additional pieces 
of information, or cues, which when combined with the flat image provide the depth knowledge. Traditional 
primary cues were attributed to the physiological effects necessary to focus on an object while the cues 
used by painters such as perspective, occlusion, and relative size were termed secondary[3]. More recently, 
texture and movement have been studied as important visual cues [7]. Surface texture provides a fairly 
regular pattern over a large surface. For example, the size of individual blades of grass in a field 
may be random, but taken as a whole, creates a constant textural pattern. There are also textures which 
are created regularly such as bricks in a wall or a checkerboard. As a surface is slanted away from the 
eye the texture pattern gradually changes due to perspective and projection. This gradual change is called 
the texture gradient [8]. Perception researchers isolate three separate gradients which influence perception: 
texture size, shape, and density. Several experiments have shown that each of these gradients strongly 
influence the perception of surface depth and slant [3]. Figure 2-1 is a simple example of a textured 
surface, and illustrates the effects of texture size, shape, and density to perception. Figure 2-1: Simple 
texture  2.2. Conventional Algorithms In an ever increasing struggle for realism, computer graphics 
researchers have developed different approaches to modeling surface texture. Catmull suggested one of 
the earliest techniques for mapping a defined texture onto subdivided patches [4]. His rendering algorithm 
subdivided parametric patches down to the pixel level for display. As a patch was subdivided, the parametric 
boundaries of each subpatch were carried along down to the pixel. Once at the pixel level, the (u,v) 
coordinates of the pixel corners specified an area in a texture definition array defined in (u,v) space. 
The color of the pixel was determined by averaging the values in this texture definition area. Blinn 
and Newell took Catmull's technique a step further by introducing a more sophisticated filtering method 
[1]. The quadrilateral formed in texture definition space by the (u,v) corners of the pixel was used 
as the base for a square pyramid to weight the texture values. Their paper also discussed different techniques 
for creating texture patterns such as (u,v) functions, hand drawings, and scanned photographs. Blinn 
also suggested a more novel approach to creating actual bumps in the surface ma normal perturbation [2]. 
In this method, a perturbation function was mapped onto the surface to modify the normal vectors giving 
the illusion of wrinkles. This technique produced extremely realistic images, but at about twice the 
time of standard texture mapping. Because of the amount of detail involved, textured images are extremely 
vulnerable to aliasing. More recent approaches have concentrated on the filtering aspect of the texture 
mapping [5] [10]. In each approach to creating texture the same two general functions must be performed: 
some sort of mapping is required from texture space to image space, and filtering is necessary to create 
acceptable images. Both of these increase the computation time of image rendering. Additionally, the 
mapping procedure is extremely data texture dependent, and is not always easy to define. For example, 
if one had a collection of polygons representing a surface and wanted to simulate bricks, it is not obvious 
how to map each polygons' coodinates to the brick texture definition. Although both of these functions 
are necessary to create realistic texturing, a simpler approach can be used to obtain the perceptual 
benefits of texture: artificial texturing.  3. Artificial Texturing Artificial texturing is a means 
of applying a pattern to a surface in order to obtain the perceptual benefits of texture gradients. The 
goal of this approach is not to produce realistic textured images, but rather to aid the visualization 
of rendered surfaces. No knowledge of the type of object space data is required, but it is assumed that 
depth and normal information is available at each pixel as is required for z-buffer hidden surface rendering 
with normal interpolation for shading. Each of the perceptual characteristics of texture discussed earlier 
will be presented with a look at the information required to generate each. 3.1. Texture Size Psychological 
experiments have shown that the texture pattern most effective in aiding perception is one of equally 
sized elements positioned on the surface at a regular density [3]. An example would be a set of equally 
spaced grid lines. As mentioned earlier, the problem with projecting such lines on an arbitrarily warped 
surface is that some mapping knowledge is required to know how such lines continue from surface element 
to surface element. However, a type of pattern which avoids this problem is one composed of individual 
texture elements, or "texels", such as a pattern of disjoint squares on a piece of cloth. The same texture 
gradients apply to individual texels: their changing size, shape, and density. Along with the advantage 
of not requiring mapping knowledge, the use of individual texels also lends itself to a simplifying assumption: 
that each texel is only a function of the center of the individual element. Although this assumption 
does not imitate realism, it allows for a significant decrease in computation time. This simplification 
will be used in the calculation of all three texture gradients. The size of each individual texel is 
strictly a function of the depth of the element from the projecting plane. The idea of perspective projections 
is well understood in the field of computer graphics and has been addressed by many authors [6]. Using 
the same simplifying assumption as previously stated, the size of the texel can be determined as a function 
of the depth of the center of the element using a standard perspective transformation. 3.2. Texture 
Shape When equally shaped texels are used, the shape gradient is a strong cue as to the surface orientation 
[11]. The shape will be a function of the surface orientation over the texel as well as the orientation 
of the texel itself. For example, square elements will not necessarily be aligned with all edges parallel, 
and would appear unnatural if so forced. To avoid this problem, symmetric texels are used in this algorithm, 
specifically circles, or polka dots. The shape as a function of surface orientation can be simplified 
using the same assumption as before, that it is strictly based on the orientation of the element's center. 
This will not result in exact realism since individual texels will not follow the surface curvature over 
the area of the element. However, as a visualization aid, this assumption allows for a simple texture 
generating method which produces perceptually helpful gradients. The shape of individual polka dots on 
the surface can be determined by projecting a circle perpendicular to the normal at the circle's center 
onto the display screen. Figure 3-1 shows a graphic description of this projection. The projection of 
the circle will be an ellipse whose eccentricity depends on the direction of the normal.  3.3. Texture 
Density Another texture gradient providing perceptual information is the density gradient. When the texture 
pattern has regular density, the changes in the Normal: 1 0 0 Normal: 0 0 -1 Normal: 1 0 -1 Normal: 
1 1-1 Figure 3-1: Circle projections for different normals distance between elements, or density gradient, 
on the textured surface give clues to the depth and orientation of points on the surface. Researchers 
have distinguished two types of density gradients, compression and convergence [3]. Compression is a 
decrease in distance between texels due to the perspective transformation of elements at a greater depth 
from the viewer. Convergence is a result of elements being projected closer together when the surface 
is at a slant to the viewer. Thus, compression is a function of depth while convergence is based on normal 
information. As in the other two gradients, the approximation of using only the element's center simplifies 
the texture density calculations at any given point on the surface. Algorithmically, the density information 
is required to determine where to put each texel. If only a density change is desired in either the horizontal 
or vertical direction, a simple solution is possible. The normal information available at each pixel 
during rendering can be used to estimate the surface distance between pixel centers. For a given density, 
the rendering process can determine where the next element goes by accumulating the distances across 
a scan line. Similarly, images could be scanned in a vertical fashion to create a vertical density change. 
Although either of these density techniques may provide perceptive information, the images they produce 
appear awkward and off balance. To produce density information in both directions requires more detailed 
computation than simple horizontal or vertical distances. One solution is to approximate areas covered 
by each pixel's boundaries rather than distances between pixel centers. For a given normal at the center 
of each pixel, an approximation can be derived to the area bounded by the projection of the pixel boundaries 
onto the surface. This approximation can be modified by a perspective factor based on the depth of the 
surface at the pixel center. For a specified texture density, the placement of texels can be determined 
by summing surrounding pixel areas until enough area is found to place one element. Because the area 
calculation encompasses both depth and normal information, both compression and convergence density results. 
Figure 3-2 illustrates the difference in images using the variable density calculation.  Once individual 
pixel areas are found, it is necessary to group them into cohesive sections of equal area. This implementation 
uses a simple approach to this grouping by building an "area binary tree" from the individual areas. 
At the top level is the bounding box for the entire image. This image is di~cided into two subareas by 
a horizontal or vertical line with approximately equal areas on either side of the line. Each of these 
subareas are further subdivided into two equal segments, and so forth down to some specified level. At 
each level n, 2 n rectangular sections are defined of approximately equal areas. The direction of division 
for each area is determined by the longest side of the bounding box in an attempt to keep the subareas 
relatively square. The area center of each rectangle can be determined by weighting the pixel locations 
within the rectangle by their individual areas. This area center is stored as a texel location for the 
density defined by this level. Figure 5-2 shows a level of bounding boxes and texel centers for the surface 
in figure 5-1. Using these texel centers, figure 5-3 shows the resulting textured image. The z-buffer 
input file is preprocessed to build an "area binary tree" of several levels (usually around 6). The resulting 
tree consists solely of texel locations for each level and becomes an input to the artificial texture 
rendering program. The user specifies a desired density level, and the appropriate level of the tree 
is loaded which specifies texel locations to be projected. Figure 5-4 shows the difference between static 
and variable density at different levels of the area binary tree. The upper two cylinders have a static 
density (texel centers equally spaced in x and y directions in screen space.) The lower cylinders were 
textured using different levels of the area binary tree, and do not demonstrate the "gaps" along the 
edges apparent in the upper cylinders.  5. Conclusion Artificial texturing provides simple visualization 
enhancement at a low computation cost. Figures 5-1 and 5-3 are an example of the information available 
using artificial texturing. The lower left portion of the shaded image in figure 5-1 contains very little 
shading information to help interpret the shape of the surface in that area. It appears from the shading 
to be relatively flat. The artificially textured image, figure 5-3, gives a visual impression of the 
curvature of the surface. The information provided can be compared to the "hedgehog" technique (displaying 
normal vectors attached to the surface)[9]. In fact, projected polka dots can be viewed as sort of "flattened 
hedgehogs". However, they provide a much more natural means of visualizing shape information since textures 
are interpreted in our everyday visual experiences. Also, additional information, such as depth, is more 
readily interpreted from texture gradients. One enhancement to artificial texturing which was found to 
be effective is to combine hedgehogs and texture. Projected polka dots are ambiguous in that they can 
represent one of two possible normals. By overlaying hedgehogs, the ambiguity is reconciled. Figures 
5-5 through 5--8 show a warped surface with hedgehogs, texture, and both. Although the combined picture 
may not appear natural, it is easier to interpret than strictly hedgehogs and resolves the ambiguity 
of the texture elements. Because of the number of assumptions and simplifications made, textures are 
sometimes generated which do not appear realistic. For example, on a surface with a sharp corner, if 
a texel is centered close to the edge of the corner, it will not "bend" around the corner as would seem 
natural. Another example is that because of the simple means of generating regions of equal surface area, 
densities at times will not look aligned exactly as they should. However, as stated at the outset of 
this paper, the goal is not extreme realism, but to simulate the texture gradients which provide strong 
perceptive information. To this goal, artificial texturing provides a fast, inexpensive approach to enhance 
surface visualization. References 1. BlinP_, James F. and Newell, Martin E. "Texture and Reflection 
in Computer Generated Images", Communications of the ACM, Vol. 19, No. 10, October 1976, pp. 542-547. 
 2. Blinn, J. F. "Simulation of Wrinkled Surfaces", Computer Graphics, Vol. 12, No. 3, August 1978, pp. 
286-292. 3. Braunstein, M. L. Depth Perception through Motion, Academic Press, New York, 1976. 4. Catmull, 
E. "A Subdivision Algorithm for Computer Display of Curved Surfaces", Tech. Report No. UTEC-CSc-74-133, 
University of Utah, December 1974. 5. Feibush, E. A., Levoy, M., and Cook, R. L. "Synthetic Texturing 
using Digital Filters", Computer Graphics, Vol. 14, No. 3, July 1980, pp. 294-301. 6. Foley, J. D. and 
Van Dam, A. Fundamentals of Interactive Computer Graphics, Addison-Wesley, Reading, Mass., 1982. 7. 
Gibson, J. J.. The Perception of the Visual World, The Riverside Press, Cambridge, Mass, 1950. 8. Haber, 
Ralph N. and Henderson, Maurice. The Psychology of Visual Perception, 2nd ed., Holt, Rhinehart and Winston, 
New York, 1980. 9. Newman, William F. and Sproull, Robert F.  Principles of Interactive Computer Programming, 
2nd ed., McGraw-Hill, New York, 1979. 10. Norton, A., Rockwood, A. P., and Skolmoski, P. T. "Clamping: 
A Method of Antialiasing Textured Surfaces by Bandwidth Limiting in Object Space", Computer Graphics, 
Vol. 16, No. 3, July 1982, pp. 1-8. 11. Phillips, R. J. "Stationary Visual Texture and the Estimation 
of Slant Angle", Quarterly Journal of Experimental Psychology, Vol., No. 22, 1970, pp. 389-397.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801129</article_id>
		<sort_key>31</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Computer graphics in higher education (Panel Session)]]></title>
		<page_from>31</page_from>
		<page_to>33</page_to>
		<doi_number>10.1145/800059.801129</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801129</url>
		<categories>
			<primary_category>
				<cat_node>I.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>K.3.2</cat_node>
				<descriptor>Curriculum</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003457.10003527.10003530</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education->Model curricula</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P132223</person_id>
				<author_profile_id><![CDATA[81100302796]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Foley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The George Washington University]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P328902</person_id>
				<author_profile_id><![CDATA[81100437656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Al]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bork]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The George Washington University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39074150</person_id>
				<author_profile_id><![CDATA[81332491279]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Maxine]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brown]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The George Washington University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14077526</person_id>
				<author_profile_id><![CDATA[81332509024]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Robin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[King]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The George Washington University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P18590</person_id>
				<author_profile_id><![CDATA[81452592989]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Andries]]></first_name>
				<middle_name><![CDATA[van]]></middle_name>
				<last_name><![CDATA[Dam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The George Washington University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332511</person_id>
				<author_profile_id><![CDATA[81100251533]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wozny]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The George Washington University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PANEL COMPUTER GRAPHICS IN HIGHER EDUCATION CHAIR: James D. Foley, The George Washington University 
PANELISTS: A1 Bork, University of California - Irvine Maxine Brown, Digital Productions Robin King, Sheridan 
College Andries van Dam, Brown University Mike Wozny, Rensselaer Polytechnic Institute CHAIRMAN'S INTRODUCTION: 
 Computer graphics is becoming an integral part of many educational curricula in col- leges and universities. 
Four individuals who have developed successful curricula will describe both the form of the curri- cula 
as well as the process of building the curricula, so that others who wish to undertake such activities 
can benefit from their experience. Panelists will address two areas. The first is how computer graphics 
integrates into the curriculum: how graphics is used, how often it is used, objectives for its use. 
The second area is a retrospec- tive on how the program was developed: the resources needed, the administrative 
structures put in place to support the development, the delivery of graphics- oriented instruction, the 
overall develop- ment process, problems encountered, and lessons for those who would repeat the process. 
 Five areas are represented on the panel: science education, graphics design educa- tion, engineering 
education, computer sci- ence education, and SIGGRAPH. Professor Alfred Bork of the University of California, 
Irvine, is Professor of Phy- sics, Professor of Information and Com- puter Science, and Director of the 
Educa- tional Technology Center. The Center's work concentrates on personal computers in education. 
Projects are proceeding in public libraries, middle schools, and universities. Bork was one of the four 
American speakers to the World Conference on Computers in Education. He was a co- director and keynote 
speaker at a NATO Advanced Study Institute. He was a con- sultant to the United Kingdom National Development 
Program in Computer Aided Learning. He is on the editorial board of Computers and Art and THE Journal 
-Tech- nolo@ical Horizons for Education. He served as chair of the Speclal Interest Group on Computer 
Uses in Education of the Association for Computing Machinery. He is Physics Series Editor for CONDUIT. 
 The Educational Technology Center is involved in research and development of highly interactive graphics 
computer-based learning material. Professor Bork will describe briefly (i) a beginning physics quarter 
for science-engineering majors; and (2) scientific literacy material for non-science students. This material 
was developed using a production system which has been under development for many years at the Educational 
Technology Center. The facets of the system are shown in S.A.D.T. diagrams. The system begins with very 
competent teachers and completely separates pedagogical design from techni- cal implementation, as with 
current text- book production. For the past several years the implementation language has been UCSD Pascal. 
Textual and graphic software has been developed to ease the task. Mr. Robin King is coordinator of the 
one- year program in computer graphics at Sheridan College of Applied Art and Tech- nology, Toronto, 
Canada. He has been at Sheridan since 1971 as a full-time faculty member in the School of Visual Arts, 
and has just recently developed their computer graphics program. Born and educated in England, Mr. King 
previously did research work in both chemistry and physics. His M.A. is in interdisciplinary studies, 
with his thesis work focusing on the Application of General Systems Theory to an Analysis of the Creative 
Process in Visual Art. Mr. King will discuss some departures from traditional educational methods which 
have been taken in order to provide a balance between the need to establish a theoreti- cal and practical 
groundwork and a strong desire to stress the importance of main- taining personal visual integrity. The 
most difficult obstacles encountered appeared during the initial transition from traditional skills and 
working methods to productive application of graphics technology as a tool for solving visual problems. 
 A wide variety of graduate students with different visual arts specialities were chosen in order to 
provide a wide range of 31 expertise and to encourage exchange and collaboration. Several were trained 
on a variety of commercial turnkey graphics systems such as the Dicomed D39, Geni- graphics 100C, and 
Norpack IPS2 and then, during the academic year, employed to teach a small group of workshops on a rotating 
basis. In addition to technical training, each student contracts a self- directed research program. A 
full-time program director and a programmer are the only full time staff, providing direction and continuity. 
Part-time specialists (an animator, a graphic designer and a videotext specialist) are available on a 
regular basis for individual consultation, and a production seminar focuses on group problems on a weekly 
basis. A large number of local, national and interna- tional guest speakers and lecturers are then used 
as a resource chosen by staff and students to match the particular needs and interests of the program 
participants and the group as a whole. Michael Wozny is a director of the Center for Interactive Computer 
Graph,s at Rensselaer Polytechnic Institute, in Troy New York. As an engineering educator, he feels 
his primary goal is to provide stu- dents with a solid grounding in fundamen- tals and an ability to 
use them for pro- ductive results. Computer graphics plays an important role in both areas. The inherent 
motivational aspects of a graph- ics display, coupled with appropriate interactive software strengthen 
students' understanding of concepts and improve their ability to make engineering judg- ments. Industry's 
widespread acceptance of computer graphics, integrated with CAD/CAM software, for increasing an engineer's 
productivity also means that most graduates will use this technology and methodology throughout their 
careers. The RPI Center for Interactive Computer Graphics was originally established to address such 
graphics related educational issues. Today, five years later, the Center has evolved into a comprehensive 
instructional and research program, comprising of significant industrial and NSF research support, extensive 
graphics equipment, and a strong research and operations staff. The instructional laboratory, which now 
represents about a third of the Center's activities, handles more than 1,500 engineering students in 
48 courses each semester. During the Fall, 1982 semester, for example, the faculty and students spent 
approximately 19,000 terminal hours in front of graphics displays working on classroom assignments and 
materials. The success of the Center is based on many factors, including: (i) strong high-level administrative 
backing; (2) significant faculty interest and a commitment to attract faculty; (3) a major initial investment 
in hardware; (4) a charter to promote and coordinate instruction across the entire School of Engineering; 
and (5) a strong industrially sponsored research program which trickles ideas down into, and shares certain 
equipment with, the instructional program. Andries van Dam is Chairman of the Depart- ment of Computer 
Science at Brown Univer- sity where he has been teaching since 1965. His research has concerned software 
engineering in general and computer graph- ics, text processing, mini and micro- computers and microprogramming 
in particu- lar. He has recently been working on the design of "computer books" based on color graphics 
displays, both for teaching pur- poses and for use by maintenance and repair technicians instead of paper 
docu- mentation. Van Dam received the B.Sc. degree from Swarthmore College in 1960 and the MS and PhD 
from the University of Pennsylvania in 1963 and 1966, respec- tively. A member of Sigma Xi IEEE Com- 
puter Society, and ACM, he helped to found and was an editor of Computer Graphics and Information Processing 
for the last decade, and has now become an editor of ACM's new Transactions o_nn Graphics. He was also 
the co-founder of ACM's SIGGRAPH Society for Computer Graphics in 1967. The book Fundamentals of Interactive 
Com- puter Graphics, co-authored with J. D. Foley, was published by Addison-Wesley in December of 1981. 
 Brown University has started an ambitious program to put networks of high resolu- tion, graphics-based 
professional wOrksta- tions across the entire campus for teach- ing and research. A report will be given 
on the first year's operational experience of an experimental teaching lab with a ring of twenty Apollo 
computers used in teaching introductory programming and com- puter science. The experiment and the experimental 
software and courseware will be very briefly described. Maxine Brown is Secretary of SIGGRAPH and is 
director of documentation at Digital Productions, an animation studio special- izing in computer imaging 
and simulation. She was formerly with ISSCO Graphics, working in the documentation, development and marketing 
areas. She has also worked at Hewlett-Packard, in both their research and development groups and in marketing. 
Maxine holds an M.S.E. degree in Computer Science from the University of Pennsyl- vania. 32 Ms. Brown 
will describe how SIGGRAPH serves as a forum for the promotion and dissemination of current computer 
graphics research, technologies and applications. The latest computer graphics achievements are communicated 
to members through publi- cations, slide sets, fiche, video tapes and the annual conference. The SIGGRAPH 
conference includes technical presenta- tions, tutorials and seminars, a large exhibition of hardware 
and software pro- ducts, a formal art exhibit, and extensive film and video presentations. SIGGRAPH 
encourages active participation from academia and industry to develop course curricula, prepare audio/visual 
materials, compile information source books and further its educational goals. An Education Committee 
is currently being organized to compile, develop and distri- bute high quality written and visual materials 
describing fundamental computer graphics concepts. Counselling informa- tion, detailing educational institutions, 
career requirements and application areas, is being prepared to inform students, counsellors, artists, 
technicians, and computer professionals of opportunities in the computer graphics field. Other activities 
are under discussion and will be given greater consideration when the Education Committee is in full 
operation. SIGGRAPH has a strong commitment to its membership to provide educational materi- als and 
programs...to heighten awareness and encourage interest...and to further the enhancement of the exploding 
computer graphics industry. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801130</article_id>
		<sort_key>35</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Towards a comprehensive user interface management system]]></title>
		<page_from>35</page_from>
		<page_to>42</page_to>
		<doi_number>10.1145/800059.801130</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801130</url>
		<abstract>
			<par><![CDATA[<p>A UIMS developed at the University of Toronto is presented. The system has two main components. The first is a set of tools to support the design and implementation of interactive graphics programs. The second is a run-time support package which handles interactions between the system and the user (things such as hit detection, event detection, screen updates, and procedure invocation), and provides facilities for logging user interactions for later protocol analysis.</p> <p>The design/implementation tool is a preprocessor, called MENULAY, which permits the applications programmer to use interactive graphics techniques to design graphics menus and their functionality. The output of this preprocessor is high-level code which can be compiled with application-specific routines. User interactions with the resulting executable module are then handled by the run-time support package. The presentation works through an example from design to execution in a step-by-step manner.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Dialogue design and specification]]></kw>
			<kw><![CDATA[Dialogue run-time support.]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>User interface management systems (UIMS)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.6.1</cat_node>
				<descriptor>Systems development</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003457.10003490.10003491.10003496</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Management of computing and information systems->Project and people management->Systems development</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003129.10010885</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interactive systems and tools->User interface management systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
			<gt>Management</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39069884</person_id>
				<author_profile_id><![CDATA[81452616426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[W.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Buxton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Systems Research Group, University of Toronto, Toronto, Ontario, Canada M5S 1A4]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P180888</person_id>
				<author_profile_id><![CDATA[81100353768]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[M.]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Lamb]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Systems Research Group, University of Toronto, Toronto, Ontario, Canada M5S 1A4]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31039391</person_id>
				<author_profile_id><![CDATA[81100372584]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[D.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sherman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Systems Research Group, University of Toronto, Toronto, Ontario, Canada M5S 1A4]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39079141</person_id>
				<author_profile_id><![CDATA[81337493638]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[K.]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Smith]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Systems Research Group, University of Toronto, Toronto, Ontario, Canada M5S 1A4]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Buxton, W., Fogels, A., Fedorkow, G., Sasaki, L. &amp; Smith, K. C. (1978). An introduction to the SSSP Digital Synthesizer. Computer Music Journal2(4), 28-38.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Buxton, W., Patel, S., Reeves, W. &amp; Baecker, R. (1982). Objed and the Design of Timbral Resources. Computer Music Journal6(2), 32-44.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Buxton, W., Sniderman, R., Reeves, W., Patel, S. &amp; Baecker, R. (1978). The Evolution of the SSSP Score Editing Tools. Computer Music Journal3(4), 14-25.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Buxton, W. &amp; Sniderman, R. (1980). Iteration and the Design of the Human-Computer Interface. Proceedings of the 13th Annual Meeting of the Human Factors Association of Canada, pp 72-81.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Deutsh, L &amp; Taft, E. A. (1980). Requirements for an Experimental Programming Environment. Technical Report CSL-80-10, XEROX PARC.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801268</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Kasik, D. (1982). A User Interface Management System. Computer Graphics,16(3), 90-106.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Kuzmich, N. (in preparation). Melody Manipulations. Music Dept., Faculty of Education, University of Toronto.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988589</ref_obj_id>
				<ref_obj_pid>988584</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Olson, D. (1983). Automatic Generation of Interactive Systems, Computer Graphics17(1), 53-57.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Reeves, W. (1975). A Device-Independent Interactive Graphics Package M.Sc. Thesis, Dept. of Computer Science, University of Toronto.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358572</ref_obj_id>
				<ref_obj_pid>358557</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Swartout, W &amp; Balzer, R. (1982). An Inevitable Intertwining of Specification and Implementation. Communications of the ACM25(7), 438-440.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801267</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Wong, Peter C.S., and Eric R. Reid (1982). FLAIR - User Interface Dialog Design Tool, Computer Graphics,16(3), 87-98.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 TOWARDS A COMPREHENSIVE USER INTERFACE MANAGEMENT SYSTEM W. Buxton, M. R. Lamb, D. Sherman &#38; K. 
C. Smith Computer Systems Research Group University of Toronto Toronto, Ontario Canada MBS 1A4 ABSTRACT 
A UIMS developed at the University of Toronto Is presented. The system has two maln components. The first 
is a set of tools to support the deslgn and implementation of Interac- tive graphics programs. The second 
Is a run-time support package which handles Interactlons between the system and the user (thlngs such 
as hit detection, event detection, screen updates, and procedure invocation), and provides facilities 
for logging user interactions for later protocol analysis. The deslgn/Implementation tool Is a preprocessor, 
called MENULAY, which permits the applications programmer to use interactive graphics technlques to design 
graphics menus and their functionality. The output of this preprocessor Is high-level code which can 
be compiled with application-specific routines. User Interactions with the resulting exe- cutable module 
are then handled by the run-time support package. The presentation works through an example from design 
to execution In a step-by-step manner. CR Categories and Subject Descriptors.- D.2.2 [Software] Tools 
and Techniques -User Interfaces; H.1.2 [Information Systems] User/Machine Systems -Human Information 
Pro- cessing; 1.3.4 [Computer Graphics] Graphics Utilities -Software Support; 1.3.6 [Computer Graphics] 
Methodology and Techniques -Interactive Techniques. General Terms: Human-Computer Dialogue, Interaction 
Management. Additional Keywords and Phrases: Dialogue Deslgn and Specification, and Dialogue Run-Time 
Support. I. INTRODUCTION Traditionally, Interactive graphical programs have been writ- ten using conventional 
programming languages, low-level tools, and often ad hoc techniques. The cost of doing so has been time, 
frustration, and quality of end product. Permission to copy without fee all or part of this material 
is granted provided that the copies are not made or distributed for direct commercial advantage, the 
ACM copyright notice and the title of the publication and its date appear, and notice is given that copying 
is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. &#38;#169; ACM 0-89791-109-1/83/007/0035 $00.75 A user interface management 
system (UIMS) is Intended to reduce this cost. The objective Is to free the applications programmer from 
low-level details so as to be able to con- centrate on higher applications-specific aspects of the user 
interface. A UIMS typically consists of a package of tools which support the implementation, debugging 
and evaluation of interactive human-computer dialogues. An analogy can be drawn between a UIMS and a 
data base management system (DBMS). The DBMS mediates between programmer and data, enforcing a consistency 
of technique among all programmers In accessing that data. It provides portability because only the lowest-level 
DBMS routines are hardware-dependent. Similarly, the UIMS mediates between the applications programmer 
and the input events, encouraging a consistency both of graphical, layout representation and of input 
processing mechanisms. 1 This paper presents a UIMS developed at the Computer Sys- tems Research Group, 
University of Toronto. The system comprises a set of tools for designing and implementing menu-based 
dialogues and for providing run-time support for more general event-driven interactive programs. The 
module for designing/implementing menu-based dialogues has the property of permitting hand-written programs 
to be integrated with code automatically synthesized from the programmer's specifications. 1.1 The Need 
for a UIMS Our current understanding of human-computer Interaction Is extremely limited. We cannot sit 
down and design an inter- face for an application and know, a priori, how well it will perform. We must 
accept st the outset, therefore, the inevitable intertwining of specification, design, and imple- mentation 
(Swartout &#38; Balzer, 1982). Design then becomes an iterative process, each iteration consisting of 
three phases: design, implementation and evaluation (Buxton &#38; Sniderman, 1980). The motivation to 
develop improved tools can therefore be seen as a desire to increase the number of iterations that we 
can afford to pass through in this loop. The hoped-for consequence will be an improvement in the quality 
of the user interfaces which we produce. The UIMS presented herein aids In the design end imple-mentation 
phases by providing the following set of tools: a graphics package; sketch editors; a standardized graphics 
communications protocol; table-driven run-time support of the user dialogue; and, most recently, a graphical 
layout and interaction specification preprocessor which generates C language programs. Underway are the 
development ot~ interaction analysis tools to aid in the evaluation phase of the cycle. 1. This Is not 
to Imply that data-base management Is Independent of the user Interface. Rather, the tools applied to 
each of these free the applications programmer so that their mutual Influence can be better taken Into 
consideration. Each of the modules of the existing system has limitations. However, when viewed together, 
the various pieces begin to shed some light on what would comprise a comprehensive set of tools for the 
user interface designer. In presenting our experience in a longitudinal way, it is hoped that we will 
make some contribution towards the realization of such a comprehensive set of tools.  1.2 U of T UIMS 
Structure In its current state, our UIMS consists of two main modules. The first Is a preprocessor which 
enables the program designer, using interactive graphics techniques, to design and specify the graphical 
layout and functionality of menu- based user interfaces. It is with this module that the appli- cations 
programmer establishes the relationships between what the user sees, does, and hears, and the application- 
specific semantics underlying the program being imple-mented. The second main module is the run-time 
support package. It handles things such as event and hit detection, procedure invocation, and updating 
the display -all according to the schema specified using the preprocessor. An important aspect of this 
implementation is that the code executed by the run-time module can be generated automatically (com- 
plete with comments) from the data specified to the prepro- cessor. The applicability of the current 
implementation of the preprocessor is restricted to menu-based interaction. Within this domain, however, 
it is quite flexible, and sup- ports various types of displays (colour or monochrome) and sound output. 
The run-time support package is more gen- eral and does not depend upon the preprocessor. It Is designed 
for supporting event-driven software and e variety of input devices. These two packages do not stand 
alone. They are just two components in a more general "kit" of prototyping tools to support Iterative 
and experimental programming (Deutsh &#38; Taft, 1980). In discussing these tools, we find it useful 
to distinguish between genera/ design tools and programmers' packaged units. The general design tools 
facilitate the specification of graphical information and functional relationships, the writ- ing of 
application-specific code and the debugging of suc- cessive versions of applications programs. Many of 
these tools are standard components in modern operating sys-tems. Besides the two modules mentioned above, 
our 2 inventory of such tools includes: (1) the UNIX operating system, with its highly sophisticated 
command interpreter, C language compiler, hierarchical file system and many utili- ties; (2) the GPAC 
device-independent graphics package (Reeves, 1976); (;3) routines to save and retrieve pictures stored 
as standard format metafiles; (4) text editors; (5) graphical sketch editors; and (B) interactive symbolic 
debuggers. Programmers' packaged units are ready-made modules which can be integrated into applications 
software. They are documented and tested building-blocks. They are gen- erally packaged versions of blocks 
of code which are observed to be reccuring and of general utility. Our library of such modules, therefore, 
is continually expanding. Representative of such packaged units are: (1)specialized iconic cursors which 
can be used as meaningful tracking symbols; (2) directory windows which permit graphical user reference 
to any file of a given type; (3) a graphical poten- tiometer with a "knob" which the user can "slide" 
up and down to change the value of a program parameter; (4) a graphical piano keyboard which audibly 
plays the notes pointed to; (5) routines to manipulate lightbuttons within the above menu-driven system 
(e.g, to flasih deactivate and highlight individual menu items); and (6) an audio support package, which 
drives a digital sound synthesizer (Buxton, Fogels, Fedorkow, Sasaki &#38; Smith, 1078). Many of these 
2. UNIX is a trademark of Bell Laboratories. modules will be Illustrated In examples which follow. 2. 
EVOLUTION OF OUR UIMS Our UIMS has evolved over several years, In parallel with the development of innovative 
software for graphics applica- tions, it is implemented in the C programming language on a PDP-11/45, 
which was acquired in 1975 and runs UNIX ver- sion 6. First came GPAC, a device-independent graphics 
package (Reeves, I g75), onto which successively higher levels were built. Interaction techniques, specialized 
cursors and direc- tory windows followed, in 1976-78. As part of the Structure Sound Synthesis Project, 
the table-driven run-time support package of the UIMS was put together in 19zg. Finally, in 1982, MENULAY, 
the graphical menu layout and function specification preprocessor was built. 2.1 Table-driven run-time 
support system With the development of a number of menu-based interac- tive programs in the late lgTO's, 
it became apparent that the user interfaces all had a large number of common features, but that each 
programmer implemented them with a "personal" touch. So personal, in fact, that the code was difficult 
for others to understand or support. Accordingly, we began to develop a more uniform methodology which 
not only captured the best ideas of these various approaches, but also made them available in a fully-documented 
and well-supported environment. The basis of our new metho-dology was an externally-controlled, table-driven 
system which supported event-driven interactions. The program- mer supplied the system with the applications-specific 
rou- tines and Information about what types of events were active when, and what and how such events 
were to effect the program's behaviour. At run-time, then, responses to user interactions (such as event 
and hit detection, display updates, and procedure invocation) were handled by this package. The present 
package, an extension of that developed in lgTg, is primarily oriented towards screen and menu-based 
systems. It is capable of supporting event-driven interac- tions that go well beyond simple light-button 
selection (examples are: Buxton, Sniderman, Reeves, Patel &#38; Baecker, 1978; and Buxton, Patel, Reeves 
&#38; Baecker, 1082). The tables used by the system have nine user-specified fields per entry. These 
fields include: the name of a user-specified procedure which is to be invoked upon the detection of a 
specified input event; the input event which is to trigger that procedure; fields to contain (variable) 
parameters of that procedure; the x and y hit area for procedures triggered by pointing events; any text 
(or the name of a graphics file containing a picture) to be displayed; the x and y co-ordinates of the 
item to be displayed; the item's size and colour. The system also supports dummy table entries for setting 
various parameters, such as whether subsequent hit-areas are to have boxes drawn around them or not. 
As well, it pro- vides for functions to be invoked in special cases arising in menu-based systems: (1) 
if the user misses all of the light- buttons, (2) when entering or (3) when exiting from the menu. Finally,the 
package provides easy-to-use routines for setting-up and switching among the menus, or "staes" of the 
application-defined system. While this package was of value, it was still a tedious task to specify the 
data which made up the tables. This was especially true during the early stages of development, when 
the graphic design, functionality, and syntax of the user interface were continually being revised due 
to experi- mentation. ConSequently, we were motivated to develop another tool which would facilitate 
the specification and modification of these data. The result was the preproces- sor, MENULAY, described 
in the next section. 3. MENULAY AND MAKEMENU 3.1 Concept The preprocessor which serves as the front end 
of our UIMS is known as MENULAY. The package is designed to enable the user interface designer (who is 
not necessarily the applications programmer) to specify rapidly and natur- ally the graphical and functiona! 
relationships within and among the displays making up a menu-based system. Specifications made using 
the package are converted into the C programming language and compiled through the use of a companion 
program MAKEMENU. The resulting code can be linked with application-specific routines. MENULAY enables 
the designer to define user interfaces which are made up of networks of menus. These may be structured 
in a hierarchic manner, or in an arbitrary fashion. Furthermore, the method of interacting with these 
menus is open, and up to the designer. A prime objective of the tool is to minimize the bias imposed 
by the path of least resis- tance, which may favour one interaction technique over another. MENULAY is 
a product of itself. It therefore gives the user interface designer a feel for the nature of the interaction 
sequences being specified, while at the same time indicates the range of tools available. 3.2 Functional 
Flow The entire sequence as set out in Figure 1 can be per- formed by the Jcreale =Lctures (DRAt/ or 
HAFWIT)I   MEN~ULAY $1' Jchanae ~.ze~ end coLourt, a~mLmn funclLont~ $ 6Lore tn menu spectfLceL~on 
ftLel $ MAKEMENU creale C prosrams for I table-driven menu svsleni $ ~om~te and runJ Figure 1. A typical 
Sequence for Constructing an Interac-tive Dialogue applications designer in an interactive graphics environ-ment. 
A graphics tablet and four-button puck are used for all input to MENULAY (except for such character-oriented 
input as the typing of the names of application-specific functions tcr be called upon hit detection). 
The MAKEMENU program creation and compilation options are also specified graphically. 3.3 Automatic Code 
Generation MENULAY's elegance lies in the automatic conversion into C language programs of the graphical 
interface specified by the applications designer. This conversion is performed by MENULAY's companion 
program, MAKEMENU. The programs generated from MENULAY's compact specification files are syntactically 
correct, complete and internally documented with liberal comments. They are designed for compilation 
with the same table-driven support system on which MENU- LAY itself is built. Because these programs 
are in C, how- ever, they can be adapted when required by using a text editor. Where necessary, these 
changes can be recorded automatically, and rep#ated whenever the menu specifica- tion files are changed 
~. Where the applications designer specifies names of application-specific functions, the programs generated 
by MAKEMENU contain unresolved external references ("hooks"). By writing functions with the required 
names and referencing the appropriate file names when MAKEMENU is called, the applications programmer 
can add any amount of application-specific programming to the layout and sequence Information specified 
by the designer. The two sets of code (computer-generated and programmer-authored) are completely compatible, 
and can share global variable names, external function references, and so on. Work is presently underway 
on a decompiler which will be able to reverse the MAKEMENU process. This decompiler, called UNEMEKAM, 
will convert a C program which makes use of the table-driven menu software into a menu specification 
file which can be edited graphically using MENULAY. 3.4 Command Hierarchy MENULAY has one main command 
level, containing seven basic commands. These are layout, size, colour, function, get save, tryout and 
exit These are illustrated in Figure 2. ILayouL s~ze ~w Please potnt Lo the burette ~tlt IcoLour to 
add a drop of' acid  Ifuncl Lor~ clear Re~, ILrvouL exll saved [h ftLe "tlE~r=LLoh'. Figure 2. MENULAY 
Commands Layout allows the designer to create textual items (e.g., light buttons) on the screen; add 
pictures either taken from the system library of graphical icons or created by the user with a sketch 
editor; change the position of any item; and delete items from the screen (see Figure 3). Size enables 
the designer to change the scale of any item on the screen, either by "sliding the knob" on a graphical 
potentiometer or by typing in a scale factor, as shown in 3. Hand-coding changes to MAKEMENU output 
obviously causes problems In "unmaking" menus. Using the facility Is a concession to reality= the state-of-the-art 
does not yet permit us to make a totally comprehensive UIMS. Figure 4. Co/our allows one to specify 
(or change) the colour of any item on the end-user's screen (see Figure 5). This is independent of the 
device on which the designer Is using MENULAY; the high-resolution display we most commonly use, for 
example, does not support colour graphics. LAYOUT OONE [] Figure 3. MENULAY - Laying Out the Display 
Graphics r~ dI I Figure 4. MENULAY -Changing the Size of an Item (the stand) with a Graphical Potentiometer 
Function enables the designer to specify what will happen when the end-user points to light buttons. 
The light but- tons may be text or pictures. The designer may also specify a function to be invoked if 
the user activates an input event without pointing to any one light button, in addition, it is possible 
to specify functions to be called when the menu being designed is entered and exited. This is shown in 
Figure 6. The function names may be taken from MENULAY's library of utility functions or be written by 
the applications ~---BLUE > Figure 5. MENULAY -Assigning Colours to the Items on the Screen programmer 
before compilation of the program. The writ- ing of these functions is done entirely Independently of 
(and either before or after) the creation of the menu specification file with MENULAY. Function also 
allows programs of up to 50 characters -such as short print statements -to be typed directly into MENULAY. 
MAKEMENU takes care of creating a new function name for the function table to enable the code typed in 
by the user to be loaded. PLease po~nf, to the burette to add a drop of" sold S [] T I DostLion cursor 
and LYoe FuncLton name Figure 6. MENULAY - Assigning Function Names to the Active Items (Light-Buttons) 
Get (and save) allow the user to retrieve from (and store into) a menu specification file the details 
of the layout and functional relationships which the designer has specified. The menu specification file 
is extremely com-pact and is thus a very efficient storage format. Each item of the menu screen is referenced 
by x and y co- ordinates, hit area, size, colour, function, text (If a tex- tual item) or standard graphics 
format file name (if a graphical icon). Tryout gives the user the opportunity to invoke rnakemenu, the 
code generation program, by specifying graphically the options and files he wishes to access (see Figure 
7). OPTIONS + [Graphic Wonderl IS~iwrlt (coLour}l F-~ I PILOTstm J COMPUTER-GENERATED COMMAND STRING 
F~ makemenu t Ltra~ ;.on D Figure7. MENULAY -Generating 'C' Code to Implement Specified Interface (by 
Invoking MAKEMENU) Once the program has been compiled, tryout then runs It for the user. Exit is used 
to exit the program. The user is given a new menu with "YES" and "NO" options to confirm that he does 
indeed wish to exit. MENULAY has levels of use: "novice" and "expert". In novice mode, only the basic 
commands (as set out above) are accessible. All input Is done through one button on the cursor puck, 
and instructions are given at various points in the program. In expert mode, the other three buttons 
on the puck can be used to perform special functions (e.g., displaying a grid while positioning items; 
flipping from one menus of pictures or colours to another; assigning a func- tion name which is identical 
to the text in the menu item), and fewer Instructional diagnostics are displayed. The sys- tem maintains 
on disk a profile for each user (a file called "userpro"), initially tagging each person as "novice" 
and upgrading them based on experience with the system and on specific request. 4 From any point within 
MENULAY (except during layout, where typing text causes the creation of light buttons containing the 
text typed), the user can access the UNIX shell (com- mand interpreter) by typing at the terminal. (When 
this hap- pens, the MENULAY screen fades and the user's scroller is reset to the full screen until the 
user returns to MENULAY.) This means that the full range of terminal-based commands can be accessed instantly 
without leaving MENULAY. For example, any calculations which the designer wishes to make in the course 
of the graphical layout specification can be made by invoking the on-line desk calculator. Similarly, 
the applications designer who is also a programmer may compose a function by invoking the text editor. 
Or the designer may send the programmer comments about the implementation by electronic mail before there's 
a chance to forget them.  3.6 Applications MENULAY has already been used to construct the user interface 
for each of the following programs: (1) MENULAY itself; (2) the DRAW sketch editor, which is used to 
gen- erate light-button graphics; (3) a computer-assisted instruction (CAI) pro_gram which teaches children 
about birds and their nestsb(see Figure 8); ISELECT-A-BEAKI iced-creek Ins * Lop;hi n t L-pu~pooe Lel~ 
;ns L~secL-ca Lch ink P~'obLhS rruct-elttns hammePtns PQLr~E Lg ~.he beck you nnt, your b;,d Lo f~lvl. 
If yOt~ ttlHl~* ilmrl ~,01fomtttlft, pc.Lnk to EXPLA~[N. Figure 8. Beak Selection Sequence from 'Land 
Birds and Their Nests' and (4) a graphical piano keyboard and musical notation edi- tor (Kuzmich, In 
preparation: see Figure g). c ,Ad jo-# d "~ J I NOTATING I ~ Figure 9. 'Melody Manipulations' music 
notation editor and teaching tool It can be u~sed to assemble, In minutes, sequences of frames for CAI 
in virtually any area of instruction where graphics is helpful. It has also proven useful for laying 
out 5. "Land Birds and Their Nests", designed by Young Naturalist Foundation, Toronto, Canada, and programmed 
by CSRG (In press), figures such as Figure 1 of this paper. Appendix I presents a walk-through of a CAI 
example. It is notable that since graphical Icons for the user's screen are referenced by file name, 
these pictures can be changed without even having to recompile the applications program. Thus, a program 
could be fully constructed by an applica- tions designer who is not a graphic artist, and an artist brought 
in later to revise the pictures. 4. UNIFORMITY AND PORTABILITY It is important that a UIMS be portable 
In a number of senses. The system described satisfies these criteria theoretically and we are in the 
process of proving its porta- bility in practice. 4.1 Output Device Independence At the most primitive 
level, a user interface management system must support a number of different output devices with different 
characteristics. In the case of MENULAY and MAKEMENU, for example, the device independence is achieved 
by using the GPAC graphics package with such varied devices as a high-resolution vector display with 
1B intensity levels and a low-resolution 16-colour raster display. The run-time support package (without 
MENULAY as of yet) has also been made to run on various alphanumeric terminals. 4.2 Input Device Independence 
The UIMS must also be able to support alternative input techniques. The primary input device used in 
MENULAY at present is a graphics tablet with a four-button=puck and the typewriter keyboard. A set of 
Allison slidersUare also used, but are only available through pre-programmed pack- ages. Other pointing 
devices (such as mice, light-pens, or touch-screens) could be used in place of the tablet with the provision 
of the appropriate GPAC device driver. The run-time support package, however, can be driven by virtually 
any event-generating device that has a GPAC driver. 4.3 Language Independence At a further level of 
portability, the UIMS should be struc- tured to facilitate the ability to generate code in different 
programming languages. The output of MENULAY is a metacode which is translated into high-level language 
by MAKEMENU. To output code In a different language would involve rewriting this program and providing 
run-time sup- port in a compatible format. 4.4 Machine Independence At a higher degree of portability 
is the capacity to transfer a system such as MENULAY either to a more powerful machine than the PDP-11/45 
(e.g., a VAX-11/ZBO) or a less powerful one (e.g. an APPLE microcomputer). As has been noted, MENULAY 
and MAKEMENU are written in C (a standardized language which is relatively portable) and use GPAC, a 
device-independent graphics package (which is itself written in C). Provided the necessary hardware drivers 
are available, GPAC and thus MENULAY/MAKEMENU could be transferred at reasonable cost to any system which 
will support UNIX and C. 4.5 Applications Program Portability In contrast with MENULAY, the applications 
programs gen- erated by MAKEMENU can be ported even to systems which do not support UNIX. With a cross-compiler 
and a basic graphics package, for example, applications programs such as computer-assisted instruction 
frames could be compiled to- run on many microcomputers. 6. This device Is a continuous belt slider. 
It is a treadmll( with a g by 1.5 cm surface exposed which Is used as a motion-sensitive Input device. 
The rnechanlcal section was deveEoped by Allison Research Inc., 2.817 Erlca Place, Nashville, Tenn. 37204. 
The electronics used here were developed In house, 5. COMPARISON WITH OTHER SYSTEMS Other UIMSs do exist, 
and have had an influence on the evo- lution of our system. The most distinguishing feature of MENULAY 
is Its natural way of Integrating graphical design specification with human-written applications programming. 
By way of comparison, we review briefly three systems: TRW's FLAIR; Olsen's automatic code generation 
design; and Kasik's TIGER. 45.1 FLAIR FLAIR (Functional Language Articulated Interactive Resource) (Wong 
&#38; Reid, lg82), Is a user interface dialogue design tool which enables a system designer to construct 
graphically a user dialogue for an applications program. It is largely driven by voice Input and incorporates 
text picture construction and editing (at the graphical primitive level) as well as dynamic frame layout. 
Its high-level features include the ability to define and control a menu hierarchy, graph and map generation, 
an on-line calculator and rela- tional data base access for graphical entity storage and retrieval. FLAIR 
is more advanced than MENULAY in its use of multiple input techniques and in its ability to permit the 
applications designer to specify a wide range of end user Interactions. However, we are in the process 
of extending MENULAY's capabilities to permit the specification of a much wider range of user interactions. 
FLAIR contains a powerful set of internal utilities, but appears to be rather limiting in its Integration 
with application-specific code. FLAIR is a language and package unto itself, with no apparent "hooks" 
into other programming languages. This suggests that if the FLAIR "language" does not permit the applications 
programmer to program a certain algorithm conveniently, then that algorithm will be inacces- sible. MENULAY, 
on the other hand, creates menu specifica- tion files that are converted into fully-documented C pro- 
grams which, as noted earlier, are automatically integrated with any code the programmer may have written 
for the specific application. =5.20lsen's Model for Automatic Code Generation Olsen (1082) describes 
research into the automatic genera- tion of interactive graphical systems to facilitate faster and cheaper 
generation of interactive user interfaces. This work has not yet progressed beyond the design stage. 
Olsen points out the useful distinction between these design of the application program interface and 
thes writ-ing of the program itself. He observes that it is the design aspect of the program creation 
which is suited to automatic program generation. This is because of the high cost in time and effort 
of hand-coding and the increased reliability of automatically generated software. Olsen envisages the 
use of Pascal procedure definitions for the characterization of interactive commands in the applica- 
tions program. We feel that MENULAY is a significant improvement over this idea in that the command menus 
and interaction relationships are specified in the very way in which the end user will interact with 
the applications pro- gram, I.e. by pointing. Olsen does not address the possibility of having the specification 
technique use the same devices and interfaces as those the end-user will ultimately face. 5.3 TIGER Kasik 
(lg82), describes a UIMS which, like our UIMS, takes care of the bookkeeping associated with screen layout, 
interrupt handling and the definition of interactive dialogue sequences. This UIMS, called TIGER, has 
as Its core the language TICCL, which permits the applications programmer to concentrate on the logical 
functions which he wishes to perform rather than the physical, low-level steps which must be taken to 
accomplish the task. TICCL can be used to describe algorithms which combine graphical primitives in response 
to user interactions as well as to define user interaction sequences. TICCL code operates at a higher 
level than the Pascal code which is used for the non-graphical portion of the applications pro- gramming. 
TICCL is useful as a mechanism for specifying user interac- tion at a higher level than is otherwise 
available to Its designers. Such a language combined with a higher level package such as MENULAY would 
permit even more flexibil- ity in user interface prototyping. While TIGER does not currently Incorporate 
a module comparable to MENULAY, TICCL Is a powerful language, and could support such a tool. To the extent 
TICCL is used for constructing graphical primi- tives from user interactions, it is more advanced than 
our table-driven menu system for which MENULAY acts as preprocesor. On the other hand, we feel that our 
program- mers' packaged units, together with the flexibility of GPAC and its integration into programmers' 
C code, provide a use- ful alternative set of interaction response tools. 6. FUTURE DIRECTIONS 6.1 Protocol 
Analysis The recording of sequential data about end-user Interac-tions is essential to the evaluation 
of the interaction tech- niques used in an applications program. With a menu-driven system based on cursor 
and tablet, this data consists of a time-stamped record of each user input, recording the x and y tablet 
co-ordinates of the cursor and the input event which was activated. We are in the process of developing 
tools for the analysis of this data, stored in a so-called "dribble file". As part of the process of 
developing the interaction sequences for an applications program, the designer will ask an end-user to 
spend a session with the program. Afterwards the new tools will facilitate the analysis of the "dribble 
file" for that session in a number of ways. First, they will allow the designer to "play back" the user 
interactions in real time, so as to get a feel for the flow of the user-computer dialogue. Second, they 
will draw for the designer a "spiderweb" which superimposes graphically all of the hand motions of the 
user in his interaction with the program (see Figure 10). The spi- derweb makes it easy to spot the Figure 
10. Tracing of Coordinates of Events Recording a User's Session points at which the user is being forced 
to repeatedly make hand motions that are uncomfortable or excessively long. To function properly, the 
dribble file data must be recorded at every user input. This implies that the recording be done at the 
level of event recognition, and that the information always be available unless the programmer has specified 
otherwise. To do so, however, requires special support in the event-detection mechanism of the graphics 
package used. Furthermore, the event recognition routines them-selves should be able to function in a 
mode which permits input from sources other than the physical input devices. The stored data files should 
be able to be used as the source of input events.  6.2 Window Management MENULAY presently generates 
code which operates within a single window on the screen. While multiple frames or levels within a program 
can be created very easily, multiple win-dows can not. It is intended to expand the capabilities of the 
table-driven menu system (and therefore of MENULAY) to permit the designation of multiple windows by 
the applications designer. Windows would have attributed to them specific cursor tracking symbols, background 
colours, and input events. As at present, specific light buttons (text or graph- ical) will be Iocatable 
at any place within a particular win- dow. 7. ACKNOWLEDGEMENTS MENULAY and MAKEMENU are the highest level 
of a user interface management system which is built on years of work at the Computer Systems Research 
Group. We ack- nowledge with thanks the contributions of Ron Baecker and Leslie Mezel, former directors 
of the Dynamic Graphics Pro- ject; Bill Reeves, author of the GPAC graphics package; and other major 
contributors to our inventory of graphical tools and techniques: Tom Duff, Greg Hill, Tom Horsley, Sanand 
Patel, Rob Pike, David Tilbrook, Mike Tilson and Martin Tuori. We also gratefully acknowledge the helpful 
comments made by the referees and by Dave Kasik. Interactive graphics research at the Computer Systems 
Research Group has been funded for many years by the National Sciences and Engineering Research Council, 
and more recently by the Social Sciences and Humanities Research Council. 8. REFERENCES Buxton, W., Fogels, 
A., Fedorkow, G., Sasaki, L. &#38; Smith, K. C. (lg78). An Introduction to the SSSP Digital Syn-thesizer. 
Computer Music Journal 2(4), 28 - 38. Buxton, W., Patel, S., Reeves, W. &#38; Baecker, R. (1982). Objed 
and the Design of Timbral Resources. Com-puter Music Journal 6(2), 32 - 44. Buxton, W., Sniderman, R., 
Reeves, W., Patel, S. &#38; Baecker, R. (lg78). The Evolution of the SSSP Score Editing Tools. Computer 
Music Journal 3(4), 14 - 25. Buxton, W. &#38; Sniderman, R. (1080). Iteration and the Design of the Human-Computer 
Interface. Proceedings of the 13th Annual Meeting of the Human Factors Asso- ciation of Canada, pp 72 
- 81. Deutsh, L &#38; Taft, E. A. (1980). Requirements for an Experi- mental Programming Environment. 
Technical Report CSL-80-I O, XEROX PARC. Kasik, D. (1982). A User Interface Management System. Computer 
Graphics, 16(3), 90 -106. Kuzmich, N. (in preparation). Melody Manipulations. Music Dept., Faculty of 
Education, Unlverslty of Toronto. Olson, D. (1983). Automatic Generation of Interactive Sys- tems, Computer 
Graphics 17(1), ~53 -/57. Reeves, W. (1975). A Device-Independent Interactive PLease point, to the bureLt.e 
 Graphics Package M.Sc. Thesis, Dept. of Computer Science, University of Toronto. ~.o add a drop of" 
acLd Swartout, W &#38; Balzer, R. (1982). An Inevitable Intertwining of Specification and Implementation. 
Communica-tions of the ACM 25(7), 438 -440. Wong, Peter C.S., and Eric R. Reid (1082). FLAIR -User Interface 
Dialog Design Tool, Computer Graphics, 16(3), 87 -98. APPENDIX 1: A Walkthrough of a CAI application 
 The following is a brief account of an applications designer's use of MENULAY to create a lesson to 
help teach chemistry titration. The time taken in this instance was less than ten minutes. The designer 
begins by typing "draw" to invoke the DRAW program and then uses the graphics tablet to input free- hand 
pictures of e burette, a beaker and a stand. Each pic- ture is scaled down in size by pointing to the 
command "SIZE" and then sliding the "knob" on the displayed poten- tiometer (like that in Figure 4). 
The "knob" is slid by posi- tioning the cursor over it and holding down the main button on the cursor 
puck while sliding it up or down. Each picture is stored in a disk file (by pointing to the command "SAVE", 
and typing in or pointing to the file name). Next, MENULAY is invoked (by selecting "EXIT" and then "MENULAY" 
from a new menu), whereupon an explanation is displayed together with MENULAY's command menu. The user 
selects "LAYOUT" and sees the newly created pictures in a menu at the bottom of the screen. Selection 
of the burette causes a copy of it to be tracked as the cursor. It is anchored in the work area by pressing 
or releasing the main button on the cursor puck (see Figure 3). The same is done to the beaker and the 
stand. Typing at the keyboard causes that text to be displayed at the current cursor posi- tion. Any 
item in the work area (whether text or graphics) can be repositioned by pointing to it and dragging it 
to a new position, again anchoring it either by releasing the button or, if it was released immediately 
upon pointing, by pressing the button again. To change the scale of any item, the user selects "SIZE" 
in the main menu (displayed in Figure 2), selects the item, and then changes its size, again with a graphical 
potentiometer (see Figure 4). To set or change the colour of any item, the user selects "COLOUR" in the 
main menu and chooses a tint from the menu at the bottom of the screen. This tint is tracked (as shown 
in Figure 5) until another tint is selected. Any items pointed to are assigned the currently tracked 
colour. The colour of each item is displayed next to the Item if the hardware device does not support 
colour graphics. The designer specifies that the function named clrip is to be invoked when the end-user 
points to the burette (by selecting "FUNCTION", selecting the burette, and typing "drip": see Figure 
6). The entire set of interface specifica- tions is now stored in a file (by pointing to "SAVE" and typing 
a file name. Finally, the user selects "TRYOUT" and then chooses Spiwrit (a colour raster screen) as 
the display device (see Figure 7) and references "drip.c", an appLication-specific C source file. This 
causes the interaction specifications to be automatically converted into C language programs which are 
compiled and linked with the application-specific code. The resulting binary file is then executed (see 
Figure 11). Y Wow! It chanBed coLour! Figure 11. Running the Titration Simulation Here Is the source 
code for the drip routine - the only code which the programmer had to write: #Include "/uO/dave/mast 
er/menuglobal.h" #define beaker plcture('1~eaker"); Int drops O; drlPO type("Added a drop of liquid..."); 
sound(DRIPPING); drops = drops, + 1; if(drops == 3) type('~Now! It changed ook>url"); resetoolour(beaker,PINK); 
sound(BUZZING);    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801131</article_id>
		<sort_key>43</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[SYNGRAPH]]></title>
		<subtitle><![CDATA[A graphical user interface generator]]></subtitle>
		<page_from>43</page_from>
		<page_to>50</page_to>
		<doi_number>10.1145/800059.801131</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801131</url>
		<abstract>
			<par><![CDATA[<p>The SYNGRAPH system automatically generates graphical user interfaces. It generates interactive Pascal programs from a description of the input language's grammar. From the grammar it deduces information about how to manage both physical and simulated devices, and how prompting and echoing are performed. Input errors are detected, and can be corrected using automatically provided rubout and cancel features. The natural integration of application specific semantics into the system is also shown including appropriate semantic recovery from input errors.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[user interface management]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Graphical user interfaces (GUI)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>D.2.5</cat_node>
				<descriptor>Error handling and recovery</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10011007.10011074.10011092.10011691</concept_id>
				<concept_desc>CCS->Software and its engineering->Software creation and management->Software development techniques->Error handling and recovery</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010865</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Graphical user interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P329693</person_id>
				<author_profile_id><![CDATA[81410595121]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[R]]></middle_name>
				<last_name><![CDATA[Olsen]]></last_name>
				<suffix><![CDATA[Jr.]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Arizona State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P76689</person_id>
				<author_profile_id><![CDATA[81100061154]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Elizabeth]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Dempsey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Arizona State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>801801</ref_obj_id>
				<ref_obj_pid>800049</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bleser, T. and Foley, J. "Towards Specifying and Evaluating the Human Factors of User-Computer Interfaces." Proceedings: Human Factors in Computer Systems, (March 1982).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807504</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hanau, Paul R. and Lenorovitz, David R. "Prototyping and Simulation tools for User/Computer Dialogue Design." Computer Graphics 14,3 (Aug 1980).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801765</ref_obj_id>
				<ref_obj_pid>800049</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Feldman, M. and Rogers, G. "Toward the Design and Development of Style-Independent Interactive Systems." Proceedings: Human Factors in Computer Systems, (March 1982).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>6684</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Foley, James D. and Van Dam, Andries. Fundamentals of Interactive Computer Graphics. Addison-Wesley Publishing Company, (1982).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801802</ref_obj_id>
				<ref_obj_pid>800049</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Jacob, R. "Using Formal Specifications in the Design of a Human-Computer Interface." Proceedings: Human Factors in Computer Systems, (March 1982).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801268</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Kasik, David J. "A User Interface Management System." Computer Graphics 16. 3 (July 1982).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Newman, William M. "A System for Interactive Graphical Programming." Spring Joint Computer Conference, (1968).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Newman, William M. and Sproull, Robert F. Principles of Interactive Computer Graphics. New York: McGraw-Hill, (1979).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801257</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Rosenthal, David S. H., Michener, James C., Pfaff, Gunther, Kessener, Rens and Sabin, Malcolm. "Detailed Semantics of Graphics Input Devices." Computer Graphics 16. 3 (July 1982).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Sutherland, Ivan E. "Computer Graphics: Ten Unsolved Problems." Datamation 12,5 (May 1966).]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>802551</ref_obj_id>
				<ref_obj_pid>800078</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Wasserman, A. I. "User Software Engineering and the Design of Interactive Systems." Proceedings of the Fifth International Conference on Software Engineering, (March 1981).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SYk~APH: A Graphical User Interface Generator Dan R. Olsen Jr. Elizabeth P. Dempsey Computer Science 
Department Arizona State University Abstract The SYNGRAPH system automatically generates graphical 
user interfaces. It generates interactive Pascal programs from a description of the input language's 
grammar. From the granmar it deduces information about how to manage both phy- sical and simulated devices, 
and how prompting and echoing are performed. Input errors are detected, and can be corrected using automatically 
provided rubout and cancel features. The natural integration of application specific seman- tics into 
the system is also shown includ- ing appropriate semantic recovery from input errors. CR Categories 
and Subject Descrip- tors: 1.3.4 Computer Graphics: Graphics Utilities -software support; I3.6 Ccm- 
puter Graphics Methodology and Techniques -interaction techniques Additional Keywords: user interface 
management i. INTRODUCTION The SYNGRAPH (SYNtax directed GRAPH- ics) user interface generator was designed 
and implemented as part of the AHI (Automated Human Interfaces) project at Arizona State University. 
The purpose of the AHI project is to generate graphical user interface programs from a description of 
the user interface. As computing hardware becfmes cheaper an increasing amount of emphasis has been 
placed on making software systems interact directly with human users. A major Permission to copy without 
fee all or part of this material is granted provided that the copies are not made or distributed for 
direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, 
and notice is given that copying is by permission of the Association for Computing Machinery. To copy 
otherwise, or to republish, requires a fee and/or specific permission. stumbling block, however, is the 
high cost of producing a quality user interface even if cne knows exactly how such an interface should 
be designed. It has been the goal of our research to produce tools which will aut~te the production of 
such human interface software and thus make quality human interfaces more economical. SYN- GRAPH is the 
first such tool which has been actually implemented on the project. Formal languages and automata have 
long been a part of user interface design. One of the earliest of these was Newman's Reaction Handler 
[Newm68]. More recently a great deal of work has been done in using formal languages to characterize 
and analyze user interfaces [BlesS2, JacoB2, WassSl]. Our specification language most closely resembles 
that of Bleser and Foley [Bles82]. Our research, however, has con- centrated on the use of formal languages 
for implementation rather than specifica- tion. Hanau and Lenorovitz [Hana80] have already shown the 
use of parser generation techniques for implementing prototypes of user interfaces. Our work goes beycnd 
simply using the gra.mar to perform the input parsing. We perform additicnal analysis of the granmar 
to support other user interface management functions. In addition, our work is closely related to the 
TIGER system [Kasi82] which uses menu trees instead of gra~ars as its control structure. The fact that 
a user interface is expressed grammatically rather than scat- tered throughout the program makes automatic 
analysis of the interface possi- ble. ~meng the features that are automati- cally supplied by this analysis 
are menu management, simulated device management, prQmpting, echoing, error handling, back- ing up over 
erroneous inputs and canceling a sequence of interactive inputs to return to some known home state. 
&#38;#169; ACM 0-89791-109-1/83/007/0043 $00.75 The key to this analysis lies in the parser autcmatos 
generated from the gram- mar. This automaton contains complete information about the syntax of the user 
interface. In order to perform much of this analysis it is necessary to stratify the user interface into 
"levels of interaction". Levels of interaction are in some sense the gl~hal modes or states of the generated 
user interface. From a user's point of view a level of interac- tion is characterized by the set of cam- 
mands that can be performed within it. From the interface specifier's point of view, levels of interaction 
are character- ized by certain non-terminals within the grammar. This additional information allows interactive 
resources to be allo- cated based not only on the current state but on the resource needs of a given 
level of interaction. One side effect of such automation is that the application no longer has expli- 
cit control over the interactive devices and their management. In the current implementation all lexical 
input tech- niques are predefined by the system. This loss of explicit control over input dev- ices is 
in some sense analogous to the loss of control over specific registers when one moves from assembly to 
a high- level language. It is hoped however that, as with compilers, these techniques will improve to 
the point that their other advantages will outweigh the loss of con- trol in all but very specialized 
applica- tions. It is also hq~ed that, as with high-level languages, the use of automatic user interface 
management will lead to a more disciplined and consistent approach to human interface design. As described 
by Foley and Van Dam [Fole82], a user interface is deccmposed into ccnceptual, semantic, syntactic and 
lexical levels. A ~ interface specification fits naturally into this model. The ccnceptual level guides 
imple- mentation rather than being part of it and is therefore only indirectly part of such a specification. 
The interaction semantics of SYNGRAPH are defined in a fashion simi- lar to that used by Feldman [Feld82]. 
The semantics are viewed as a set of pro- cedures and data types written in the implementaticn language. 
It is important to the semantic interface that a language supporting user-defined data types be used 
as the implementation language so that the semantic dojects can be modeled using data types suitable 
to the application. The syntactic level is where the greatest con- tributicns of ~ are made. The syn- 
 tax is described using an extended version of B~ which includes the names of logical input tokens, 
semantic actions and infor- maticn about how the user interface is organized. The lexical level binds 
the logical token names to actual input dev- ices and techniques. The remainder of this paper will dis- 
cuss how a user interface is expressed to SYNGRAPH and what services are performed automatically by the 
system. These will be discussed in terms of the lexical and syn- tactic specifications and it will be 
shown how the semantic interface is integrated with the syntax. The techniques for actu- ally generating 
the interface are the sub- ject of another paper and will not be dis- cussed here. 2. Lexical SPecifications 
 In the lexical portion of the specif- icaticn, various input devices and tech- niques are bound to logical 
token na~es. These token names will then be used in the syntactic specificatian. There is no con- ceptual 
limit to the number of such tokens that can be created. The mapping of this potentially infinite token 
set to a finite set of interactive resources is handled syntactically. It should be noted that these 
tokens represent individual logical input devices. More complex input methods can be built up as non-terminals 
syntactic specification. in the 2.1. Device Configuration Even though the user can specify input devices 
without limit there is a physical limit to the interactive resources available. This includes screen 
space as well as physical buttons and knobs. It will be shown in the next sec- ticn how syntactic knowledge 
is used to organize and allocate these resources. Wherever there are insufficient physical devices to 
accommodate the specification, virtual devices are automatically pro- vided. The (me exception to the 
unlimited number of logical devices is that only (me locator is allowed. This is not an inherent restriction 
but is rather a limi- tation of the i~plementaticm. 2.2. Device Properties All primitive input devices 
supported by the system are characterized in a fashion similar to that proposed by Rcsenthal et. al. 
[Rcse82]. Each primi- tive has the following properties:  The type of value that it returns.  Whether 
it is an event or sanpled input.  The display actian required when it is enabled.  The pram~t required 
when it is a legal input in the current parse state.  The acknowledgment required when it has been 
selected.  What effect it has on the choice of parse transitions.   Note that all of the above prqperties 
are predefined and managed autamatically by the system without need for programmer intervention. In SYNGRAPH 
a device is enabled when it is eligible for use within the current level of interaction. A device can 
be enabled without being a legal input in the current parse state  2.3. ~ ~nputs There are seven primitive 
input dev- ices supported by ~ . They are: menu items, the locator, valuators, function buttons, keys, 
characters, and picks. Menu items are defined simply by giv- ing a name without any further specifica- 
tiou. All such tokens are autcmatically organized into menus as defined by the syntax specification. 
If desired an icon can be specified to be used in place of the textual name of the menu item. The name 
of a menu item or its ican is displayed in the menu space whenever the item is enabled. Whenever that 
token would be valid as the next input, the menu item is highlighted. The enabling and highlighting are 
performed under syntactic control. A token can also be associated with a functiou buttou ntm~er. If 
no such button exists then the token is automatically converted to a menu item. This allows various physical 
device confiqurations to be handled automatically. At present ouly one logical button can be bound to 
a given physical button. In future implementations this restriction will be lifted in all cases except 
where two or more logical buttons, which are mapped to the same physical button, are simultaneously acceptable 
inputs in some parse state. The difference between key and char- acter tokens is t_hat a key token refers 
to a specific character and returns no value, whereas a character token refers to any character and returns 
the character as its value. The key token's primary purpose is to cunstruct keywords and delimiters to 
control the parse. The purpose of a char- acter token is to input textual material.  The valuator token 
is a sampled dev- ice that does not control the parsing actions. That is, it has no effect an which transition 
should be taken frun a given parse state. It simply inquires as to the current value of the device's 
meas- ure. As with buttons, if there are insuf- ficient physical valuators then simulated oues are created. 
The creation and organi- zation of simulated valuators is syntacti- cally controlled in a fashiou similar 
to menus. 2.4. Pick tokens are a special case in that the type of value that they return is not predefined. 
The notion that a pick token returns an integer pick identifier is really not suitable for effectively 
characterizing an interaction. Instead a pick token returns a value of a particular user-defined type 
[BlesS2]. Unique pick tokens are defined in the lexical specifi- cation for each type of object that 
can be selected fram the application's display space. This means that the graphical cut- put routines 
return a pointer to a data value, along with an indication of its type, instead of a simple integer. 
The SYNGRAPH system automatically generates a routine for each pick type. The applica- tion can then 
use such routines to specify pick values when displaying images. This use of typed picking instead of 
generic identifiers not ouly improves the interaction specification but also eases the semantic interface. 
In addition, it also helps solve same picking ambiguity problems [Suth66]. When an object is selected 
which is part of a hierarchy of objects, the intended level of the hierar- chy is ~bigucus. The syntactic 
knowledge of the expected type of object, coupled with the type information in the pick definition, can 
remove this ambiguity in most cases. In those cases where it cannot be solved, the system will automatically 
query the user for the intended object. Pick tokens are also somewhat diffi- cult to classify in terms 
of their event or sampled nature. The logical cancept of a pick token is modeled after a light pen which 
is normally viewed as an event dev- ice. In many cases, however, logical pick devices actually use a 
tablet or other locator. Sampled devices are not suitable for controlling parse transitions because they 
do not specify when the transition should be taken. Pick tokens, unlike valuator or locator tokens, have 
a role in cantrolling the parse transitiuns. This is contrary to their sampled nature. The use of sampled 
input devices to control parser actions requires the association of a trigger with the device's measure 
process. This association is defined syntactically as will be shown later. 2.~. Example  The following 
example shows the asso- ciation of logical token names with their input specifications. The use of "m" 
and "d" in the icon definition signifies mov- ing to a point and drawing a line to a point. TERMINALS 
 ice locator; pick_point button i; scale valuator R 0.0,i0.0; angle valuator I 0, 360; draw; move; 
 new__house; new_window icon m(0,0) d(l.0,0) d(l.0,1.0) d(0,1.0) d(0,0) m(0.5,0) d(0.5,1.0) m(0,0.5) 
d(l.0,0.5) ; house pick house_type; window pick window_type; Figure 1 Sample token declarations In 
the above example the token I/)C refers to the locator and PICK_POINT the stylus tip switch. The SCALE 
and ANGLE tokens are real and integer valuators. Since no phy- sical valuator is specified they will 
both be simulated. The tokens DR/~, MOVE and Ig~/_H(XISE are all menu items that will be  represented 
by their textual names. The NE~_WINIX)W token will be represented in menus as a picture of a window. 
The HOUSE and WIIg30W tokens each specify that a pic- ture of the specified type is to be picked. 3. 
Syntactic Specification The syntactic specification not only specifies the sequence of tokens which 
constitute valid input streams but also the organization and arrangement of their prompts and echos. 
Standard techniques from compiler construction are available for handling the valid sequencing of inputs. 
These techniques must be extended for interactive purposes to allow organi- zation of prompting, echoing 
and device simulation. In addition, the error han- dling strategies used in cc~pilers are inadequate 
in an interactive environment. The special prc~)lems caused by sampled devices must also be handled. 
 3.1. Grammar The syntax of the dialogue is expressed in terms of a modified BNF which uses the logical 
token names defined in the lexical specification as well as non- terminals declared in the granmar. The 
use  of non-terminals allows either a bottom-~p design where primitive tokens are col- lected together 
to form more complex input sequences or a top-d(mrn approach where the user interface is decomposed into 
succes- sively less complex sub-dialogues. The hierarchical arrangement of non-terminals within a grammar 
fits naturally the kinds of hierarchies found in many interactive systems. The granmar used by SYNGRAI~ 
allows for cpti(mal phrases, repeating phrases zero or more times (curly braces {}) as well as alternation 
(vertical bar I ). These extensicns beyond BNF allow non- terminals to be used to organize the dialogue 
rather than as a recursive con- trol structure. cmnd ::= move ( house I window ) pick_point { ice pick_point 
} Figure 2 Sample syntax specification  The above example is not the actual syntax used by SYNGRAPH 
because it lacks the semantic interface information. 3.2. Levels of Inter~Gtion ~s menticned before 
it is the syntax processing which controls the management of display and input resources. In complex 
interactions the logical resources required are much larger than those actu- ally available. It is therefore 
necessary to decompose the user interface into smaller sub-dialogues. The resources are then allocated 
within each such sub- dialogue. The appropriate choice of such an organization must be application depen- 
dent so that functionally similar activi- ties can be grouped together. This cannot therefore be done 
automatically. This organization is expressed by designating certain non-terminals as defining new lev- 
els of interaction. A level of interaction extends from the nan-terminal which is specified as its head 
through all ncn-terminals invoked by it either directly or indirectly which are not new levels themselves. 
When a new level of interaction is entered all of the tokens which are used within that level became 
enabled. In the case of menu items, simulated buttons and simulated valuators, this means that they are 
allocated screen space and displayed. A level therefore defines a specific coufiguraticn of vir- tual 
input devices. The following examples show how a change of level effects menus. interaction [newlevel] 
: := edit_~mode I draw_mode edit_jmode [newlevel] ::= EDIT ( CH;R~E I INSERT I DELETE ) draw_mode ::= 
~ ( LINE I CIRCLE ) Figure 3 Sample use of levels menu for interaction = ED~ LINE CIRCLE menu for 
edit_mode = CH/R~E INSERT DELETE Figure 4 Menu sets for levels of interaction  Note that the menu set 
for the IN I~CYION level included the starting tokens for both edit_mode and draw_mode because those 
tokens are necessary to be able to enter those non-terminals. LINE and CIRCLE are also included because 
draw_mode is not a new level. Note however that C~ANGE, INSERT and DELETE are not included since they 
are exclusively part of EDIT_MODE which is a level of interaction of its own. At present the allocation 
and organi- zation of menus and virtual devices is performed automatically by SYNGRAPH and is outside 
of the user's control. The place- ment of these objects on the screen can be defined by tables generated 
by SYNGRAPH. In the future there will be the capability of interactively repositioning these items simply 
by editing these tables. This will allow the human factors of the user inter- face robe fine-tunedwithout 
altering its functionality or correctness. 2.3. Prcn~tinc Most of the prompting of the user is performed 
automatically by SYNGRAPH. This is done by examining the set of input tokens that are acceptable for 
a given state. Menu tokens and simulated valua- tors are highlighted and function buttons and physical 
valuators are lighted or labeled (if possible) whenever they are acceptable as the next input. Feedback 
is also performed automatically when a par- ticular input has been selected. In addition to the automatic 
prompt- ing, it is possible to add additional help and prompting information to the grammar. Help items 
can be added as special transi- tions which do not effect the parse. These transitions, when taken, cause 
a message to be placed on top of a help stack. When- ever a new non-terminal is entered, the top of the 
help stack is marked. When the non-terminal has been ccmpletely recog- nized the help stack is popped 
down to the mark. The top message on the help stack is continuously displayed by the system. /%dditicnal 
levels of help can be dotained by a special H~.P menu item which steps the user up through the help stack. 
 The management of help information on a stack all(~s an interactive user access at all times to information 
about the glo- bal environment. Many times, in complex or deeply nested interactions, the user will lose 
track of where he is. The constant availability of this help stack can help him relocate himself. The 
fact that this service is performed automatically means that it is not dependent on the progra, mer to 
insure that it is available in all cir- czmstances. ~.4. Error Correction Error detection is relatively 
easy in a table driven input parser. It is simply a matter of reporting an error whenever an input is 
found for which there is no valid transition. The interactive handling of errors is much easier than 
in a compiler because the erroneous input is simply ignored and the user is prompted for a correct entry. 
In a generated interface this is always handled correctly and con- sistently.  3.4.1. Graphical Rubout 
 A special problem arises in interac- tions when a legal input is entered by the  user but the user 
is not pleased with it. In textual interactions it is possible to rubcut or backup over undesired inputs. 
This is usually done by deferring any pro- cessing on an input line until a carriage return is entered. 
Because no processing has been done, the rubout is simply a matter of removing the undesired charac- 
ters from the input line. In a graphical interaction there are two problems with performing rubouts. 
The first is that there is no concept of a current line and no generally accepted terminator. The second 
is that even though the executicn of a command can be deferred, the processing of prompts and echos must 
proceed concurrently with the input process.  Rubout processing can be performed either by deferring 
semantic processing of a ccnmand until closure is reached or by sane method of undoing the semantics 
of the input to be removed. In SYNGRAPH both methods are used. Because S"/NGRAP~ con- trols all of the 
prcmpting, menu manage- ment and echoing all of these functions are undone when a rubout is requested. 
The applicaticn semantics however are not con- trolled by SYNGRAFH and cannot be undone. These actions 
are deferred until closure is reached. Because there is no concept of an end of line to specify when 
closure is reached, we use certain non-terminals in the grara~ar to specify complete ccnmands. Certain 
non-terminals in the syntactic specification are identified as rubout non-terminals. When the parsing 
process enters such a non-terminal, the input process begins saving all of the inputs. None of the application 
semantic actions are performed during this phase. The prunpting and syntactic feedback, how- ever, are 
performed using the information in the automaton which defines the user interface. When a rubout is 
encountered, the last input is removed fram the saved inputs and the parse is restored to the state that 
it was in before the input was entered. When the parse state is restored, the prompting and feedback 
are also restored to the appropriate status for that state. Rubout actions cannot be per- formed past 
the beginning of the current rubout non-terminal. Rubout processing also extends to all nan-terminals 
that appear in the production of a rubout non- terminal. Closure occurs when a complete input string 
for the rubout non-terminal has been recognized. When an input is entered which indicates that the rubout 
nun- terminal has been campletely recognized, a semantic parse of the saved inputs is exe- cuted without 
performing any of the prcmpting or echoing. This evaluates all of the deferred semantic actions.  
  3.4.2. Returnina to a Known State In very involved interactions a user may, an occasion, get lost 
or otherwise determine that he would like to dispose of some porticn of what has been entered and return 
to some known state. For this pur- pose a CANCEL operation is provided. Can- cel operations, to be effective, 
must be enterable at any point in the input and behave correctly when entered. They also frequently require 
a more gld~al recovery than can be appropriately handled by rubout. The CANCEL operation n~st there- 
fore provide for semantic recovery.  In order to acccmplish this semantic recovery, the applicatian 
must be notified that CANCEL processing is being performed so that appropriate steps can be taken to 
undo the semantic acticms. In order to handle the CANCEL actiou each non-terminal has an ESCAPE production, 
in additian to its normal parse productian. Whenever CANCEL is entered, the parse Jamediately branches 
to the beginning of the ESCAPE preducticn for the current non-terminal and begins executing whatever 
acticns are found there. When the ESCAPE production is campleted, the parse ter- minates the non-terminal 
and returns to where the ncn-terminal was called from. The ESCAPE productian for the non-terminal that 
was returned to is then initiated. This process continues until a action is encountered in an ESCAPE 
produc- tion. The HANDLED action indicates that the ~ has been completely processed. When the ESCAPE 
production in which HAN- DLED appears is campletely recognized, the parse can continue normally. The 
following example, although some- what simplistic, will show how the CANCEL action would be performed. 
Given the gram- mar and input stream in figure 5, the parse would perform the actian to undo_object and 
return to the camand non-terminal. Because the CANCEL has not yet been handled it would perform the action 
undo_command, mark the C~%NCEL as handled and return to IN'I~CI'ION at the point indicated in figure 
6. interaction : := { cc~sand } QUIT command : := MOVE object ~O loc IDR/~ ESCAPE ::= undo_command) 
HANDLED object : := H(~SE pick_house I WINDOW pick_window I DOOR pick__door ESCAPE : := undo_object) 
 input stream MOVE HOUSE CANCEL Figure 5 Processing of CANCEL operatians  interaction ::= { ccmmand 
} CElT ^ Figure 6 Parse state after performing CANCEL  ~.~. Pickina As stated previously, a pick 
token has a specific data type associated with it. When a pick input is performed all pick identifiers 
which can possibly be associated with the pick operation are collected. Multiple identifiers can either 
occur because of a picture hierar- chy or because the picking mechanism is inexact, as when a locator 
with gravity fields [Newm79] is used. Each of these pick identifiers is compared against the legal transitions 
for the current parse state. Any pick identifier whose data type dces not~atch one of the pick transitions 
is eliminated. If more than one such iden- tifier remains, then the system autcmati- cally prompts the 
interactive user to resolve this ambiguity. ~.~. Sa,~lina Sampled devices such as locators and valuators 
require two kinds of additional information. The first is that a trigger must be associated with them 
to specify when the sampling is to take place. Second there must be in most cases, a discrete input such 
as a pick or button to act as a selector to differentiate a state transi- tion on a sampled device from 
other tran- sitions. The association of sampled devices with their triggers and selectors (which may 
be the same device) as well as associating triggers with pick devices is defined syntactically. In the 
following grammar segment the trigger event for picking a house is the key H, for a window it is the 
key W and for simply inputing a point it is the pick_point button. cmnd ::= ( house 'H' I window 'W' 
 J ioc pick_point)  For trigger associaticns pick and sampled devices are handled in a similar fashion. 
The trigger (s) for an instance of a sampled token are the set of event devices that follow the sampled 
token in the parse string. As the example shows, this association of a trigger event with a pick input 
token also allaws the use of syntactic centext to further resolve any ambiguities in picking. Such an 
a~nbiguity would occur between all three alternatives when pointing at a window which is part of the 
house. The association of selectors with sampled tokens requires a special second pass in order to calculate 
the parse tran- sitions. The first pass propagates event devices through the grammar to associate triggers 
with sampled and pick devices. The second pass propagates picks and their associated triggers through 
the granmar to act as selectors for sampled tokens.  3.7. Summary of Syntax At the syntactic level 
the SYNGRAPH system deduces a substantial amount of information from the granm~r. The prompt- ing of 
legal input tokens is autamatically derived from the state machine. The ack- nowledging of inputs is 
handled by the supplied devices. A general organization of dialogues into levels allows menu and simulated 
device setup to be performed autc~atically by examining the granmar. A rubout facility is automatically 
provided with automatic prompting even though the semantics have been deferred until clo- sure. A CANCEL 
operation which returns the interaction to same known state allows for explicit semantic recovery. 4. 
Semantics The SYNGRAPH system is written in PASCAL and the interaction semantics are handled using PASCAL 
statements. Each non-terminal is generated as a recursive PASCAL procedure whose control structure is 
the syntax tables produced fram the grammar. Each non-terminal in the grammar can have a parameter string 
and a header string associated with it. The parameter string must be a PASCAL parameter specifi- cation 
and the header string a set of PAS- CAL declarations. This allows complete flexibility in the type and 
organization of the semantic attributes. Within the production for the non-terminal, semantics are expressed 
either as symbol attributes or semantic actions. Figure 7 Trigger Specification  Each non-terminal 
in the granmmr can be viewed as a PASCAL procedure whose con- trol structure is the syntax of the input 
grammar. Each such non-terminal can have parameters and local declarations. Inter- spersed in the production, 
PASCAL state- ments can be inserted to perform semantic actions. When the parse reaches the state where 
such an action appears, it is exe- cuted. Symbol attributes are associated with both non-terminals and 
terminals as PASCAL argument lists. This procedure view of semantics which is clearly tied to the implementa- 
tion language makes it much easier for the average progranlner to produce a complete practical system. 
Even when rubout is present the deferring semantic actions until closure is masked from the applica- 
tion progra, m~r leaving only the problem of extracting and processing information from the inputs. It 
has been felt frrm the outset that the semantics of the user interface must be a natural extension of 
the specification rather than an awkward add-~n. 5. Conclusions The SYNGRAPH system has shown how a 
granmmtic specification can be used to automatically generate graphical interac- tions. SYNGRAPH has 
demonstrated the auto- mation of prompting, menu and device management as well as error detection and 
handling. Inference of resource alloca- tions for menus and virtual devices using the specification grammar 
have been demon- strated using levels of interaction to partition the user interface. A rubout facility 
has been automatically generated which allows prompting and feedback while deferring actual semantic 
actions until closure is reached. A CANCEL operation is provided which allows semantic recovery to a 
known state, lastly the semantic actions of the interface have been integrated with the input specification 
in a manner which is consistent with the implementation language. The generation of these features automatically 
from the specification should reduce the cost of i~plementing interactive systems as well as greatly 
enhance the ability to modify them. This modifiability will ease the process of iterat ively producing 
a user-friendly interface. In additi~, because rubout, cancel and error detection are all per- formed 
automatically they will be much more reliable and consistent than most hand-coded systems. References 
 Bleser, T. and Foley, J. "Towards Specifying and Evaluating the Htm~n Factors of User-Computer Interfaces." 
Proceedings: H~an Factors in Computer Systems, (March 1982). Hanau, Paul R. and Lenorovitz, David R. 
"Prototyping and Simulation tools for User/Ccmputer Dialogue Design." Computer Graphics 14,3 (~g 1980). 
 Feldman, M. and Rogers, G. "Toward the Design and Development of Style- Independent Interactive Systems." 
Proceedings: Human Factors in Computer Systems, (March 1982). Foley, James D. and Van Dam, ~ndries. 
Fundamentals of Interactive Computer Graphics. ~%ddison-Wesley Publishing Company, (1982). Jacob, R. 
"Using Formal Specifications in the Design of a Human-Ccmputer Interface." Proceedings: Human Factors 
in Computer Systems, (March 1982). Kasik, David J. "A User Interface Management System." Computer Graphics 
 16. 3 (July 1982). Newman, William M. "ASystem for Interactive Graphical Programming." Spring Joint 
Computer Conference, (1968). Newman, William M. and Sproull, Robert F. Principles of Interactive Computer 
Graphics. New York: McGraw-Hill, (1979). Rasenthal, David S. H., Michener, James C., Pfaff, Gunther, 
Kessener, Rens and Sabin, Malcolm. "Detailed Semantics of Graphics Input Devices." Computer Graphics 
16. 3 (July 1982). Sutherland, Ivan E. "Computer Graphics: Ten Unsolved Problems." Datamation 12, 5 
(May 1966). Wasserman, A. I. "User Software Engineering and the Design of Interactive Systems." Proceedings 
of the Fifth International Conference on Software Engineering, (March 1981).  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801132</article_id>
		<sort_key>51</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[A graphics editor for benesh movement notation]]></title>
		<page_from>51</page_from>
		<page_to>62</page_to>
		<doi_number>10.1145/800059.801132</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801132</url>
		<abstract>
			<par><![CDATA[<p>This paper describes an interactive computerized editor for <italic>Benesh Movement Notation</italic> that aids in the preparation of dance scores on a medium resolution colour display. Benesh Movement Notation is a two-dimensional system for recording human movement in three dimensions of space which has been successfully used in the preparation of scores for a wide repertoire of dances. The preparation and revision of Benesh scores is a lengthy and error-prone process which interactive editing techniques can greatly facilitate. We describe the current state and future extensions of a prototype editing system in which all user interaction is based on a menu-driven scheme using a graphics tablet. Extensive visual cues including iconic trackers and semantic information are provided at all times. Both user and system initiated dialogues are supported. This system has served as a testbed for a variety of man-machine interaction studies, allowing us to transfer the wealth of experience with interactive text editing to a related task: the preparation of dance scores. The project has given us a better understanding of the issues involved in implementing an effective user-interface.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Dance]]></kw>
			<kw><![CDATA[Menu-driven]]></kw>
			<kw><![CDATA[Movement notation]]></kw>
			<kw><![CDATA[System-initiated dialogue]]></kw>
			<kw><![CDATA[User-initiated dialogue]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Graphics editors</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Application packages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Performing arts (e.g., dance, music)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P27436</person_id>
				<author_profile_id><![CDATA[81100278543]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Baldev]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Singh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Schlumberger-Doll Research, Old Quarry Road - P.O. Box 307, Ridgefield, Conn]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P142871</person_id>
				<author_profile_id><![CDATA[81100324790]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Beatty]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Laboratory, University of Waterloo, Waterloo, Ontario N2L 3G1]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31095042</person_id>
				<author_profile_id><![CDATA[81100373007]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Rhonda]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ryman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Dance, University of Waterloo, Waterloo, Ontario N2L 3G1]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ARNHEIM, R. Visual Thinking. University of California Press, Berkeley, Calif., 1969.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807491</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BADLER, N.I., J. O'ROURKE, and B. KAUFMAN. Special problems in human movement simulation. Computer Graphics 14, 3 (July 1980), 189-197.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BENESH, R. and J. BENESH. Reading Dance, The Birth of Choreology. Souvenir Press (E&amp;A) Ltd., 1977.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BOURNE, L.E. and B.R. EKSTRAND. Psychology: Its Principles and Meanings (3rd edition). Holt, Rinehart and Winston, 1979.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BRESLIN, P., and J.C. BEATTY. A powerful interface to a high performance raster graphics system. Tech. Rep. CS-82-45, Computer Graphics Laboratory, Department of Computer Science, University of Waterloo, Dec. 1982.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[BROWN, M.D. and S.W. SMOLIAR. Preparing dance notation scores with a computer. Computers and Graphics 3, 1 (1978), 1-7.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[CALVERT, T.W., J. CHAPMAN, and A. PATLA. The simulation of human movement. Proceedings Graphics Interface '82, NCGA of Canada, Toronto, Ontario (1982), 227-234.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[CAUSLEY, M. An Introduction to Benesh Movement Notation, Man Parrish, London, 1967.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[DREYFUSS, H. Symbol Sourcebook. McGraw-Hill Book Company, New York, 1972.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[ESHKOL, N. Right Angled Curves. Movement Notation Society, 1975. (For the Research Centre for Movement Notation, Faculty of Arts, Tel Aviv University, Israel)]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[FEDAK, J. F. An initial design specification of a syntactic analyzer for Labanotation. Movement Project Report No. 10, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, Pa., Jan. 1978.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[FOLEY, J.D. The design and implementation of user-computer interfaces. Siggraph '82 Tutorial Notes 7, ACM, New York, N.Y., July 1982.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[GOMBRICH, E.H. The Visual Image. Scientific American 227, 3 (Sep. 1972), 82-96.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[GRATER, A. private correspondence. Institute of Choreology, London, England, 1982.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>664096</ref_obj_id>
				<ref_obj_pid>645679</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[HERBISON-EVANS, D. A human movement language for computer animation. Language Design and Programming Methodology, ed. Jeffrey M. Tobias, (Sep. 1979), 117-128. (Proceedings of the Symposium on Language Design and Programming Methodology, Sydney, Australia)]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[HUTCHINSON, A. Labanotation (3rd Edition). New York: Theatre Arts, 1977.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[LABAN, R. #Laban's Principles of Dance and Movement Notation (2nd Edition). Macdonald &amp; Evans Ltd., London, 1975.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[LANSDOWN, J. The computer in choreography. IEEE Computer 11, 8 (Aug. 1978), 19-30.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801283</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[MACKAY, S.A., R.E. SAYRE, and M.J. POTEL. 3D Galatea: Entry of three-dimensional moving points from multiple perspective views. Computer Graphics 16, 3 (July 1982).]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>574884</ref_obj_id>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[MARTIN, J. Design of Man-Machine Dialogues. Englewood Cliffs, N.J., 1973.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[MCNAIR, B. A language for notating human movement. M.Sc. Thesis, Basser Department of Computer Science, Sydney University, 1979.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[MILLER, G. The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychology Review 63, 2 (March 1956), 81-97.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[MILLS, M.I. Cognitive schemata and the design of graphics displays. Proceedings Graphics Interface '82, NCGA of Canada, Toronto, Ontario, (May, 1982), 3-12.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[NEWMAN, W.M. and R.F. SPROULL. Principles of Interactive Computer Graphics (2nd edition). McGraw-Hill Book Company, 1979.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[PERRYNOWSKI, M.R. A Physiological Model for the Solution of Individual Muscle Forces During Normal Human Walking. Ph.D. Thesis, Department of Kinesiology, Simon Fraser University, July 1982.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[SAVAGE, G.J., J.M. OFFICER, and G. McDOUGALL. Computer graphics simulation of body movement language. Proceedings 6th Man-Computer Communications Conference, (1979), 209-217.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[SEALEY, D. Computers and Labanotation. Proceedings of the Twelfth Biennial Conference, Columbus, Ohio, International Council of Kinetography Laban, (Aug. 1981), 126-127.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[SINGH, B., J.C. BEATTY, K.S. BOOTH and R. RYMAN. A graphics editor for Benesh movement notation. Tech. Rep. CS-82-41, Computer Graphics Laboratory, Department of Computer Science, University of Waterloo, Waterloo, Ontario, Dec. 1982.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[SMOLIAR, S.W. and L. WEBER. Using the computer for a semantic representation of Labanotation. Computing in the Humanities, Proceedings of the Third International Conference on Computing in the Humanities, eds. Serge Lusignan and John S. North, The University of Waterloo Press, Waterloo, Canada, (Aug. 1977), 253-261.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[TILBROOK, D, A newspaper page layout system. M.Sc. Thesis, Department of Computer Science, University of Toronto, Toronto, Ontario, 1976.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[TOWNSEND, M.A., M. IZAK, and R.W. JACKSON. Total motion knee goniometry. J. Biomechanics 10, 3 (1977), 183-193.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 17, Number 3 July 1983 A Graphics Editor for Benesh Movement Notation Baldev 
Singh Schlumberger-Doll Research Old Quarry Road -P.O. Box 307 Ridgefield, Conn 06877 (203) 431-5000 
 John C Beatty Kellogg S. Booth Computer Graphics Laboratory University of Waterloo Waterloo, Ontario 
N2L 3G1 (519) 886-1351 Rhonda Ryman Department of Dance University of Waterloo Waterloo, Ontario N2L 
3G1 (519) 885-1211 ext 3665 This paper describes an interactive computerized editor for Benesh Movement 
Notation that aids in the preparation of dance scores on a medium resolution colour display. Benesh Movement 
Notation is a two-dimensional system for recording human movement in three dimensions of space which 
has been successfully used in the prepara- tion of scores for a wide repertoire of dances. The preparation 
and revision of Benesh scores is a lengthy and error-prone process which interactive editing techniques 
can greatly facilitate. We describe the current state and future extensions of a prototype editing system 
in which all user interaction is based on a menu-driven scheme using a graphics tablet. Extensive visual 
cues including iconic trackers and semantic information are provided at all times. Both user and system 
initiated dialogues are supported. This system has served as a testbed for a variety of man-machine interaction 
studies, allowing us to transfer the wealth of experience with interactive text editing to a related 
task: the preparation of dance scores. The project has given us a better understanding of the issues 
involved in implementing an effec- tive user-interface. CR Categories and Subject Descriptors: 1.3.4 
[Computer Graphics]: Graphics Utilities --application packages; 1.3.4 [Computer Graphics]: Methodology 
and Techniques --ergonomics; interaction techniques; J.5 [Arts and Humanities] --arts, fine and performing 
General Terms: Design, Human Factors Additional Key Words and Phrases: Dance, menu-driven, movement notation, 
system-initiated dialogue, user-initiated dialogue Permission to copy without fee all or part of this 
material is granted provided that the copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the publication and its date appear, and notice is given that 
copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, 
requires a fee and/or specific permission. &#38;#169; ACM 0-89791-109-1/83/007/0051 $00.75 1. INTRODUCTION 
Today we take for granted the existence of a suitable notation in almost all fields of human study. Notation 
is the primary means by which inventors communicate with those who will implement their ideas. Without 
a concise and precise notation system, the world as we know it would be unimaginable. Our knowledge of 
the past and present would be limited by memory and prone to error in oral transmission. Any comparison 
of past and present works would be virtually impossible, limiting the depth and breadth of our knowledge 
to what we can carry in our minds. The field of human movement exhibits these limita-tions, but has no 
universal notation system. The problems are especially apparent in ballet --a classical style of expressive 
dance. Most ballets from the past have been completely lost, leaving behind only memories. The losses 
have been enormous even among the most recent ballets. This problem has been widely recognized by profes- 
sionals in the area and a number of notation systems have been developed to capture human movement. Some 
of these mark the steps of each dancer with lines superim- posed over a sketch of a floor plan, while 
others use small skeleton figures drawn in dance positions. Still others adapt musical notation, using 
the conventional signs to represent space instead of pitch. There have been many individualized systems 
too --some very ingenious. But most fail to be practical under rigorous professional condi- tions. Among 
the few that have enjoyed substantial interest are Labanotation, developed by Rudolf Laban in 1928 [16,17], 
Eshkol-Wachmann notation devised by Noa Eshkol and Abraham Wachmann during 1951-56 [10], and Benesh notation, 
a system invented by Rudolf Benesh in 1947 [3,8]. Human movement is very complex; movement nota- tion 
systems are inherently complicated and difficult to master. However, we can use a computer to provide 
tools that simplify the use of these notations. Such tools have been developed at the University of Pennsylvania 
[2,6,11,29], the University of Waterloo [26], the University of Iowa [27], Simon Fraser University [7], 
Sydney Univer- sity [15,21] and elsewhere [18]. With the exception of the Sydney University project and 
Lansdown's work, which use Benesh notation, almost all of these projects deal exclusively with the interpretation 
of Labanotation. The Computer Graphics Laboratory and the Dance Group at the University of Waterloo, 
in consultation with the National Ballet of Canada, are currently investigating the computerized editing 
of Benesh Movement Notation. Our aim is to provide state-of-the-art computer tools for creating, editing, 
and verifying records of human move-ment, similar to the tools commonly available for interac- tive text 
editing. This paper describes the design and implementation of a graphical editor for a subset of Benesh 
Movement Notation. Professional choreologists have tested a prototype system based on these ideas. Our 
primary interest here is in describing the techniques for man-machine interaction. We view this work 
as a signifi- cant testbed for a number of ideas, some new, incorporat- ing a systematic methodology 
for man-computer dialogue. The interactive tools developed for the Benesh editor have been successfully 
integrated into a VLSI layout editor and an interactive Beta-spline surface modeling system. 2. PROBLEM 
DESCRIPTION Many movement notations seem similar to musical notations at first glance. Both serve as 
a communication medium between composers and performers. However, they are used differently. Musicians 
learn a composition from musical manuscripts, whereas dancers learn their parts directly from the choreographer 
(a dance composer) through verbal and visual communication; dance requires direct interaction between 
the choreographer and the danc- ers. In music, notation is the means by which a performer realizes the 
creator's conception of a composition [6]. In contrast, movement notations are mainly used to record 
the finished product for historical preservation. They can also be used for comparative analysis and 
reconstruction at a later date, without the choreographer's presence or the need to repeat the entire 
creative process. While musical notations are well standardized, dance notations are still evolving. 
The process of recording dance using Benesh Move- ment Notation begins with a choreologist, a specialist 
in Benesh notation, who attends rehearsals and makes rough notes which are refined gradually during subsequent 
rehearsals. This score is later checked for errors in the movements recorded and in the notation's usage. 
Once the score has been checked, it is put into finished form by an autographer, who composes and inscribes 
each page by hand. Upon completion of this process the score is ready for printing. During rehearsals 
dancers learn complete movements: all parts of the body move simultaneously. However, the notation must 
record changes in each body part individu- ally. If there are many dancers learning complex move-ments, 
it becomes difficult to capture all the details. The recording process is limited by the speed at which 
the choreologist can write. Therefore the choreologist usually notes only the key positions, filling 
in the details at later rehearsals. In a large ballet company, where many hours each day are devoted 
to teaching new steps, the choreolo- gist works with different groups of dancers throughout a ten to 
twelve hour period. This makes filling in details after rehearsal virtually impossible. The task becomes 
even more difficult when the choreographer decides to add or drop parts of the dance or revise steps 
and sections. The choreologist has to struggle constantly with the organi- zation of notes, filling in 
the missing details during cleanup rehearsals conducted after all the dancers have learned their parts. 
Under these conditions the choreologist is often unable to work on the final draft of the score as the 
rough notes accumulate. There is often not enough time until after the dance is in performance. Then 
many hours must be spent laboriously copying the rough notes, laying out the pages and refining the notation. 
It should be apparent that the basic problem facing a choreologist is one of information overload which 
worsens from rehearsal to rehearsal. Improvements to this process might involve a faster means for recording 
movements (ideally at the speed of performance), an efficient mechan- ism for storing and retrieving 
this information for editing or verification, or a means for producing the finished score in hardcopy 
form. One possible approach would be to store the rough score in a computer system by scanning it with 
a video input device at the end of each rehearsal. The time between rehearsals could be devoted to editing 
and verify- ing this information through a graphical editor and produc- ing medium quality hard-copy 
output that could be used for further refinements at subsequent rehearsals. Alterna-tively, movement 
information could be directly entered into a computer system using goniometers [7,31] or three- dimensional 
imaging techniques [19,25], although this is difficult in a rehearsal situation. The inverse process 
of producing movement from notation has proven to be a very useful tool for verifying the recorded movement. 
Labanotation has been used to animate Calvert's stick-figure model and Badler's Bubble-Man, while Herbison-Evans 
used Benesh notation to ani- mate Sausage-Woman. Although substantial progress has been made in recording 
and analyzing movements, tools for editing scores, especially in Benesh Movement Notation, are virtu- 
ally non-existent. Much more research is needed to make editing movement scores as easy and convenient 
as editing text. The work presented here deals with the issues involved in editing dance scores based 
on the Benesh Movement Notation system. Another Benesh editor is being developed at Sydney University 
[15]. Earlier editors based on Labanotation have been designed at the Univer- sity of Pennsylvania [6] 
and at the University of Iowa [27].  Computer Graphics Volume 17, Number 3 July 1983 3. BENESH MOVEMENT 
NOTATION Benesh Movement Notation records movements using marks on a matrix representing a human figure. 
The five line music stave serves conveniently as a matrix dividing the human body at the feet, knees, 
waist, top of the shoulder and top of the head (see Figure 1). Two horizon- tal dashed leger lines are 
added to cover all possible body positions. A leger line above the stave lines represents the maximum 
reach of the hands when the arms are fully extended upwards. Locomotor transitions such as jumps, slides, 
turns and skimming movements are shown by lines drawn between the first stave line and a leger line below 
it. Because the span of horizontally extended legs is approxi- mately equal to the body height with the 
arms fully extended upward, all possible body positions can be enclosed in a square. Therefore the stave 
is divided into square frames, each representing a body position [14]. All this makes the notation visual 
and avoids the need for a large number of signs. Leger lines q Top of 1 Top of the llne 5 line 4 line 
3 line line q Fig. 1. The Benesh stave. NOTE: The doubly hashed region is the overlap for hands and feet 
Movements are recorded as they would be observed from behind the performer. Positions are recorded by 
not- ing the projection of the four body extremities (hands and feet) and the bends (in knees and elbows) 
onto the cardinal coronal plane (see Figure 2). This projection is not a true projection of the. whole 
body figure. Rather, it is the pro- jection of body segments in relation to the body centre line onto 
the corresponding areas on or between the stave lines. The projections provide two-dimensional information. 
For three-dimensional information it is necessary to know whether a limb is in front of or behind the 
cardinal coronal plane, which is considered to be as thick as the body. This is recorded using the following 
basic signs for the four extremities: level (with the cardinal coronal plane) in front (of the cardinal 
coronal plane) behind (the cardinal coronal plane) SUPERIOR J ANTERIOR Cardinal Sagittal plan~ POSTERIOR 
Cardinal Transverse plane Cardinal Coronal plane 1 INFERIOR Fig. 2. Human body in anatomic position. 
It is not necessary to state how far in front or how far behind the cardinal coronal plane the extremities 
lie. Fig-ure 3 illustrates the use of basic signs. Only one position is physically possible as the limbs 
are of fixed length, although there is the possibility of bending the limbs. The basic signs for bends 
(in knee or elbow) are: I level (with the cardinal coronal plane) in front (of the cardinal coronal plane) 
X behind (the cardinal coronal plane) The signs for all four limbs are identical and thus do not identify 
the limb (right leg, left arm, etc.) to which they refer. This information is inferred from the location 
of the sign. Figure 1 shows the domains for the four body limbs. Whenever a limb sign moves out of its 
domain it is lightly crossed with a diagonal stroke known as the cross-over (see Figure 3). There are 
two types of crossovers: a lateral crossover in which an extremity or bend is posi-tioned to the other 
side of the body; and a vertical cross-over in which a foot or bent knee is positioned above the waist 
line, or a hand or bent elbow moves below the thigh. The crossover signs are: lateral crossover ~ vertical 
crossover head upper torso I; + crossover 4,,,,,,-I I ...... _A._ ..... ...... i ...... I I I I I 
I II I I i I ...... I ....... ...... / ...... Fig. 3. Use of basic Signs. The head is represented by 
a straight line drawn between the fourth and the fifth stave lines. The bottom end of this head line 
is normally fixed to the intersection point of the fourth stave line with the body centre line while 
the top end always touches the fifth stave line. The angle between this head line and the body centre 
line specifies the degree of tilt. A shorter straight line drawn perpendicular to the head line represents 
the chin position. The length and location of this chin line indicates the degree of turn in the head. 
The intersection point between the head and chin lines specifies the extent of backward or forward bend 
in the head (see Figure 4). Intersection above this midpoint represents a backward bend and inter- section 
below the midpoint indicates a forward bend. lowered erect raised "l/4-turned .... inclined .... lowered" 
right right right ---r ....... T ....   i i i il'J --I i .---1 ....... L___ Fig. 4. Various head positions. 
The third stave line represents the waist. Therefore all body bends and turns from the waist up (upper 
torso) are recorded between the third and fourth stave lines. The signs used and their manipulation are 
identical to those used for the head. Hip (pelvic) movements are similarly recorded between the second 
and the third stave lines. Figure 3 illustrates the method for recording these main body parts. In Benesh 
one typically records only the changes in posture; constant data is not shown, but is inferred from previous 
frames. Furthermore, positions for main body parts (head, torso and hips) are recorded only if they are 
not in or returning to their normal anatomic position. 4. THE BENESH EDITOR The basic unit of information 
in Benesh Movement Notation is the frame. Unlike a line of text, which is made up of symbols (chai'acters) 
written at discrete points in a one-dimensional space, the Benesh frame records body information by means 
of signs in a continuous two-dimensional rectangular space. Benesh frames are ordered sequentially within 
a score, but the positioning of signs within a frame is more complex than the placement of characters 
in a line of text. Three levels of basic operations are necessary for edit- ing a text document: editing 
lines, editing words within a line and editing characters within a word. Similarly, the editing operations 
required for Benesh scores are manipula- tion of an entire frame or a set of frames, manipulation of 
sign positions within a frame and construction of the signs themselves. A complete score corresponds 
to an entire text document. High level operations such as print, copy and archive, are analogous to those 
for text. The techniques for implementing text editors are well developed and a number of very good editors 
such as Ed, Emacs, Ex, Fred, Qed, Vi and the Xerox Star editor are available. The Benesh editor described 
in this paper uses many techniques derived from the analogy between editing text documents and Benesh 
scores. As in most text editors the primary data structure used here is a doubly linked list, in which 
each node represents a Benesh frame. Each node contains complete body information for a frame even though 
only changes are displayed. Benesh notation uses a basic set of pre-defined signs. Other signs are built 
from a combination of these basic signs, for example, by adding a crossover. Still others, like main 
body signs, are constructed whenever necessary. This last approach is used for highly variable signs. 
An exam- ple is the sign which captures the degree of tilt, turn and backward or forward bend of the 
head. In this case it would be very difficult to provide a fixed set of predefined signs covering all 
possible combinations. Thus a new sign is constructed whenever a new head position must be speci- fied. 
A moving display window is superimposed on the linked list structure of frames. At any given time, the 
edi- tor displays all the frames within the current display win- dow. Other frames are displayed by moving 
the window over the desired frames in the linked list. The window displays the score as it would normally 
be written on a piece of paper, except that the first frame in the window always shows the complete body 
position for readability, irrespective of the frame immediately preceding it in the score. In addition, 
the editor provides a working frame in which all frames are first constructed before posting them to 
the display window. The editor is based on a menu-driven scheme and pro- vides abort and undo facilities 
for error recovery. Abort cancels the current action and backs up to the next higher level in the menu 
hierarchy. Thus a sequence of abort requests will eventually return the editor to its initial state. 
Two types of undo requests are supported by the edi- tor. The first undoes the previous step in a sequence 
of actions comprising an editing operation, while the second undoes the entire last editing operation 
that was success-fully completed. In both cases undo acts like a toggle switch; a second application 
undoes the last undo, thus res- toring the original action.  5. USER INTERFACE The most important single 
consideration in designing any computer system, hardware or software, is the design of the interface 
between computer and user. It is the most visible aspect of the editor and the only channel of communication 
between the system and the choreologist. The success of a system is entirely dependent on the suc-cess 
of its user interface. However, user interfaces are the most difficult and the least understood part 
of interactive systems. Our design process was highly iterative and thus very time consuming. The interactive 
techniques used by the Benesh editor have evolved through a careful task analysis and prototyp- ing process. 
A choreologist was closely observed at work under rehearsal conditions and later interviewed. The results 
of this study formed the problem description described earlier. Careful analysis of user behaviour and 
a trace of interactions during editing sessions helped to iso- late problem areas and devise alternate 
techniques. The resulting design adheres to a small set of principles that have been adapted to make 
the editing environment fami- liar and friendly to the user, to simplify the man-machine interaction, 
and to unify all editing functions so that any experience gained in one situation can be easily applied 
to similar situations. In designing the user interface, two pri- mary issues of concern were: an interaction 
language, the medium through which the user expresses commands to the system, and a display representation, 
showing the system's state and the options currently available to the user. The first is expressed as 
actions applied to the input dev- ices, while the latter is expressed through the output dev- ices. This 
section discusses these principles. 5.1. Hardware and Software Before describing the user interface 
of the Benesh edi- tor, several essential characteristics of the hardware and software used should be 
pointed out. Without these it would have been difficult to design an interface such as the present one. 
The editor uses an Ikonas RDS 3000 frame buffer attached to a colour display monitor and a Summagraph-ics 
Bit Pad tablet with a puck. The frame buffer's bit planes are divided into two sets called the foreground 
and background, as shown in Figure 5. The background planes contain static display information, such 
as the menus. Each item of static information is written into the frame buffer memory using a different 
seven-bit colour value. The item can then be made visible or invisible by manipulating colour lookup 
table entries. To display an item written in the background planes, the corresponding colour lookup table 
entry is set to a colour different from the background colour. The foreground plane contains dynamic 
information. A microprocessor based support package is used to display all foreground information [5]. 
This package is organized around a segmented display file which is interpreted by a high speed bit-slice 
microprocessor. It provides facilities to create, delete, move, or pick segments and to make them visible 
or invisible. A write-mask is used to access only the single fore- ground bit-plane and an auto-clear 
feature in the display hardware causes that bit-plane to be erased after every display cycle. This combination 
of clearing and re-writing provides the illusion of dragging a symbol as its display  When all relevant 
'information is visible, the display eases the load on short-term memory by acting like a "visual cache". 
Thinking becomes easier and more productive. A well designed dialogue can actually improve the quality 
of user thinking. The "conversational nature of a system-initiated dialogue-implies a truly interactive 
form of com- munication. It enables the system to give instantaneous positive or negative feedback to 
the user in response to his previous action, in addition to prompting fhe user for the next step. By 
contrast, user-initiated dialogues are generally easier to implement and more efficient in executing 
a par- ticular task once the user has gained some experience. They are usually more flexible, and shortcuts 
are often available for performing a particular task. The ordering of dialogue steps is not fixed and 
this can be more efficient for an experienced user, although the opposite is true for a casual user or 
novice. User-initiated dialogues also place an additional burden on the user's memory. Command Set In 
designing the man-machine dialogue for the Benesh editor, the user's problem space has been broken into 
tasks which are in turn partitioned into two different forms of dialogue. Commands like abort, undo, 
and help (which is not yet implemented) can be invoked at any time and have a user-initiated dialogue. 
The commands in this group have been restricted to a very small number and are struc- tured to fit the 
user's normal way of thinking and working, thus reducing the burden on short term memory. These commands 
can be invoked at any time during an editing session by pressing the puck buttons shown in Figure 7. 
The assignment of these commands to the puck buttons is always displayed on the screen as an aid to the 
novice or occasional user. The remaining commands are grouped appropriately within the system-initiated 
dialogue and implemented using menus. Buttons allow a very convenient and efficient way of specifying 
functions, but they distract the user, causing a shift of attention from the display to the buttons. 
With experience a user learns the button locations and activates them without looking, just as a touch 
typist no longer looks at the typewriter keys but instead concentrates on the manuscript. This is especially 
true if there are only a few buttons to remember. The puck used for the prototype editor (see Figure 
7) has four buttons that are located close together in a dia-mond pattern. Each button has been colour 
coded for easy recognition. However, the user doesn't normally look at these buttons, but instead uses 
tactile memory to locate them. In our experience users have consistently had trou-ble selecting the appropriate 
button for actions other than the do function (yellow button). The colour coding is obvi- ously of no 
help. The problem has to do with the structure of the hand and the layout of the puck buttons. Our finger 
tips move comfortably in the vertical direction but not in the horizontal direction. Thus the user has 
no prob- lem selecting the do and undo buttons, but has difficulty in selecting the help and abort buttons. 
These observa-tions suggest that pucks be designed with a single button, or at most two vertically positioned 
buttons per finger. Many mice and some tablet pucks have taken this into consideration. button) ;n button) 
; buttor~ Help (white button) Fig. 7. Tablet puck. A menu can have a very large number of options. The 
discussion in the previous section argued that short-term memory is the most important level utilized 
in cons- cious thought. Miller's experiments [22] show that the capacity of our short-term memory is 
limited to "the magi- cal number seven plus or minus two" for absolute judge- ment of unidimensional 
stimuli. He explains that this capacity can be increased by increasing the dimensions of the stimuli. 
However, the increase in capacity is not linear. Instead it asymptotically approaches a fixed limit as 
the dimensionality of the stimulus is increased [22]. One way to increase this capacity is by grouping 
[4]: thus telephone numbers are divided into groups to help in remembering them. Similarly, commands 
for manipulating frames have been sub-divided into five groups according to their function, each group 
containing a maximum of five commands. Figure 8 illustrates this grouping within the root menu, which 
is the menu for manipulating frames. Text versus Iconic Labels for Menu items Natural language is an 
excellent medium for com-municating propositions intended for people. A picture can also be thought of 
as conveying a proposition, but the problem with a picture is that it often conveys a large number of 
propositions [13]. Without some constraining context it is difficult to know the intended proposition. 
Mills explains that the problem of multiple meanings in a picture has its roots in the very nature of 
the human con-ceptual apparatus, which uses a finite set of general cogni- tive frameworks or schemata 
to form definite descriptions of specific inputs [23]. Even in purely visual perception, the same input 
can give rise to many different descriptions by drawing on different combinations of schemas depend- 
ing on changes in context. This flexibility can create prob- lems for the use of icons if they are not 
designed properly. .... (----73 , J  Quit I ip ,J I De-orc~ve i i Displayed only if the finished 
score contains at least one frame \ , I ,I[Add Frome I I= :J Displayed only if the save buffer is not 
empty Displayed if either side of the display window contains some score Fig. 8. Root menu. Fig. 9. 
Body menu. The design of iconic representations is a specialized area of study that draws on many other 
disciplines, such as art and psychology. A good icon needs simultaneously to be a symbol --representing 
something else by convention or association, and a picture --a visual representation. It must use abstract 
graphical elements to highlight generic qualities, yet must be recognizable as a visual representa- tion 
of a specific kind of object, situation or event. Poor icon design will inhibit communication rather 
than help. Dreyfuss presents a collection of standard symbols used internationally [9]. These symbols 
can be easily adopted for icons in man-machine dialogues. Translating a verbal statement into an equivalent 
pic- torial description is often not easy. In fact, it is difficult to form pictorial propositions about 
past, future and condi- tional events or to form logical chains of inference [23]. However, in an appropriate 
combination pictures and text can function together in directing the user towards the intended meaning 
of a message. These ideas have been used in labeling menu items for the Benesh editor and in defining 
the iconic cursors explained in a later section. Abbreviated words from the Benesh terminology have been 
adopted to label most menu items. Benesh signs have been used whenever available. The body menu in Figure 
9, showing the body limbs and main body parts, is an excellent example of pictorial label- ing of menus. 
This combination of icons and abbreviated text labels is intended to constrain label meanings to avoid 
ambiguity. Dynamic versus Static Menus Menus that are always displayed at a fixed location are known 
as static menus. The user becomes accustomed to their display locations and habitually looks for and 
expects them to appear at the same position. This pro-vides visual continuity and maintains a sense of 
"place" in the interaction dialogue. A drawback of static menus is that the user has to move his eyes 
(and his hand for con-trolling the puck) from their current position on the display screen to the location 
at which the menu is displayed. In contrast, a dynamic menu appears at the user's current position on 
the display screen, thus avoiding additional eye or hand movements. The assumption is that the user's 
centre of attention is near the tracker, his current position on the display screen. Thus a dynamic menu 
minimizes overall hand and eye movement. Static menus allow the use of tactile memory to pre-position 
the tablet puck, while dynamic menus minimize visual search [12]. Dynamic menus are more flexible; they 
can overlay existing data and so can be displayed anywhere on the display screen. Static menus use fixed 
display regions that cannot be used for displaying any other data. The total number of static menus that 
can be displayed is thus lim-ited by the available area on the display. An important feature of static 
menus is their natural display of the current system state. The Benesh editor uses both types of menus. 
All sign menus are dynamic, minimizing hand and eye movement for sign selection. The remaining menus 
are static, highlighting the closure [12] of each operation by requir- ing the user to return to the 
menu display area. The lay- out of the static menus is also designed to minimize hand and eye movements 
and to show the system state, as explained below. 5.3. Display Representation The display is the most 
visible part of any system and must be carefully designed to be both appealing and infor- mative. It 
should always reflect the user's conceptual model and should hide all implementation details. Our design 
goal was to minimize the short-term memory required for system operation, to relieve it for more impor- 
tant user tasks. This is achieved by displaying all the information necessary for the user to perform 
a task at any given time. The editor provides this information by immediate feedback to user requests, 
by displaying the sys- tem state at all times, and by always displaying the current status of the finished 
score as well as that of the frame being composed. This section describes the methods used for displaying 
this information. Feedback Feedback is an essential part of any conversation, whether with a machine 
or with a person. In a normal conversation several forms of feedback are exchanged automatically without 
any conscious action by the partici- pants. This feedback includes gesture, body language, facial expression 
and eye contact. Foley explains the psychological blocks that occur in the absence of adequate feedback 
[12]. Typical symptoms are boredom, panic, frustration, and confusion. Immediate feedback not only avoids 
these psychological blocks but also provides con-tinuity in the dialogue. Originally the editor provided 
feedback at the lexical, syntactic and semantic levels. Lexical feedback consists of changing the cursor 
icon to the icons shown in Figure 10, depending on the puck button pushed. Syntactic feedback differs 
depending on the item being selected. Selecting a frame results in a change in its background colour. 
Select-ing a sign from a sign menu replaces the cursor icon with the selected sign, as shown in Figure 
11. Semantic feed-back is provided by displaying the results of the selected operation. This feedback 
hierarchy offers several advantages. Lexical feedback provides a placebo whenever the system is running 
slowly. However, this placebo can be annoying under normal system operation. When the transition from 
the lexical to the syntactic level takes a very short time, the lexical feedback turns into an irritating 
flash. These flashes are especially afinoying for the do button, the most frequently used. To avoid this 
effect, lexical feedback for the do button has been suppressed and for consistency all lexical feedback 
was removed. Thus the current version of the editor provides feedback at the syntactic and semantic levels 
only. This has proven satisfactory for most opera- tions. + 00 Undo Abort   Help Fig. 10. Icons for 
lexical feedback. I t I ~////////////'~///"~ l ~/////////////~//~A u i --~ ~///////////////,/////~ 
I b I ,~/~/'~//////////[~ selecting frame [ OptionLobem ~ ~L~Jption Labe selecting menu option O,,m 
f  selecting a symbol Fig. 11. Syntactic feedback. Some form of placebo is still necessary for time-consuming 
operations such as archiving and dearchiving finished scores. All editor operations were analyzed and 
a placebo is provided for such operations. The placebo used is a Buddha icon adopted from Newswhole [30], 
as shown in Figure 12.  System is busy Fig. 12. Buddha icon. System State A very important aspect 
of the display representation is showing the system state at all times. Most systems reserve an area 
of the display screen for displaying this information. However, there are some drawbacks to this approach. 
During an interactive dialogue, the user's centre of attention is generally focussed on the object that 
he is manipulating. Anything appearing outside his centre of attention, especially outside his peripheral 
field of vision, will not register. Therefore the problem with displaying the system state in a fixed 
location is that if this location falls outside the user's centre of attention but within his peripheral 
field of vision, it will distract his attention. The user's curiousity will move his centre of attention 
to the location where the system state is displayed. After reading the state information, the user will 
return his attention to the original position. This eye movement every time the system state is changed 
can be very tiring and very annoy- ing, especially if the system state changes quite frequently. A better 
way to present this information is to display it within the user's centre of attention, thus avoiding 
eye movement and providing visual continuity. The system state referred to here is not the internal program 
state, but the current mode of the editor within the user's conceptual model. Selecting an option in 
a non- sign menu inverts the option label and all other options are switched off. As the user moves along 
the menu hierarchy selecting menu options, he leaves behind a trail of reverse video menu labels, the 
most recent of which is always at the centre of attention. The editor also indicates the action in progress 
by changing the tracker icon, as shown in Figure 13. The inverted labels and the iconic tracker together 
define the complete state of the system at a given time. The Current Status of the Score The editor displays 
the finished score in the display window. While editing a frame, the selected frame is left h~ghlighted 
to indicate its context within the score. When adding new frames to the finished score, the editor displays 
the current insertion position, which is updated each time a new frame is added. All frames added during 
the current add frame operation remain highlighted until the end of the operation. This always shows 
the current status of the finished score. + selecting commands I selecting symbols selecting frame Select 
frames selecting insertion position  Where? Select frame selecting edit position Fig. 13. Tracker 
icons. When editing a frame or composing a new frame, the working frame shows three types of information. 
Firstly, a body part may not have been defined yet. Secondly, a body part may have been carried over 
from the previous frame. Thirdly, a body part may have been newly defined for this frame. All body information 
is shown using signs. One way to show the different types of information is to use signs of different 
colour. However, this might imply an ordering of signs by their importance and thus change the semantics 
of the signs. Instead the editor displays this information for each body part by using different back-ground 
colours for the corresponding menu items, as shown in Figure 14. A black background means that the menu 
item has not been selected. Colour association is used for the remaining types of information. If the 
background colour of a menu item is the same as the highlight colour of the last frame added to the finished 
score, this implies that the corresponding sign has been carried over from the previous frame and selection 
of this item will result in replacement of the existing sign. A background colour which is the same as 
that of the working frame implies that the corresponding body part has been newly defined. This colouring 
scheme provides checklists of body informa- tion that has already been defined and that still need to 
be defined. 6. FUTURE EXTENSIONS The major problem in designing interactive computer systems is the development 
of man-machine communica-tion mechanisms suited to the particular task. Most of the design effort for 
the current system has been directed towards the construction of a user interface which is both powerful 
and easy to use. Some additional functions will 3 ~//~JT///(///A r-v-g:ff3 item not specified yet 
item carried over from previous frames item newly specified for this frame Fig. 14. Status information 
display. be incorporated into the system in the near future. These include specification of movement 
lines and enforcement of body model constraints (neither are in the current imple-mentation). Full screen 
display of the score with facilities to scroll backward and forward will also be incorporated. This will 
enable the user to read through a score more easily by examining an entire screenfull at one time. We 
plan to add a means for producing a medium quality copy of the composed score on a 200 dots per inch 
electrostatic printer/plotter. The aim is to eventually pro- duce high-quality copies of the score on 
a digital typesetter for reproduction purposes. Because a Benesh score con-sists of a limited set of 
basic signs, it is possible to define a specialized Benesh type font. Several manufacturers of phototypesetting 
equipment allow for the incorporation of such user-defined fonts into their systems. In the long term 
there is a need for extending the sys- tem to verify body position, to score multiple dancers and to 
detect collisions. Further error checking could be performed in the following manner. Each frame in a 
Benesh score defines what is know by film animators as a key-frame. An animation system with a suitable 
interpola- tion method could be used to calculate the intermediate positions between Benesh frames. The 
sequence of Benesh frames and intermediate frames could then be used to pro- duce a stick-figure animation 
of the Benesh score, enabling us to verify the movement. Another area of research is the translation 
of data between various notation systems. Three dimensional body information calculated from the Benesh 
score could be used for this translation. Similar information is collected by other systems based on 
Labanotation, and the reverse mapping from boi:ly information into notation also exists. For example, 
a project at Simon Fraser University collects body information using goniometers and then maps this data 
into Labanotation scores [7]. 7. CONCLUSION The importance of achieving literacy in dance is very well 
recognized. The notational systems which have been developed are often difficult to master or cumbersome 
to use. The computer assisted editing system described in this paper is designed to facilitate the learning, 
teaching and use of the Benesh Movement Notation. Its particular value to the professional dance community 
lies in the abil- ity to facilitate the production and revision of master scores. Its value to the computer 
graphics community is an increased understanding of the issues involved in the implementation of effective 
user interfaces. ACKNOWLEDGEMENTS It is especially important in a project such as this to elicit the 
evaluations of skilled choreologists concerning the ease and naturalness with which the system can be 
used. We would like to thank Monica Parker (director, Institute of Choreology, London, England), Robyn 
Hughes (then choreologist with the National Ballet of Canada), Wendy Walker (choreologist with the American 
Ballet Theatre), Sandy Caverly Lowery and Joan Mallet of York Univer-sity, and the dancers who attended 
the First International Summer School in Benesh Movement Notation, held at the University of Waterloo 
in August 1982. We would also like to thank the other members of the Computer Graphics Laboratory at 
the University of Waterloo for providing the tools and environment in which this work could be carried 
out. The Ikonas software written by Paul Breslin [5] was especially valuable. Our work was supported 
by the Natural Sciences and Engineering Research Council of Canada, the Province of Ontario under its 
BILD Program, and the University of Waterloo. REFERENCES [1] ARNHEIM, R. Visual Thinking. University 
of California Press, Berkeley, Calif., 1969. [2] BADLER, N.I., J. O'ROURKE, and B. KAUFMAN. Special prob- 
lems in human movement simulation. Computer Graphics 14, 3 (July 1980), 189-197. [3] BENESH, R. and J. 
BENESH. Reading Dance, The Birth of Choreology. Souvenir Press (E&#38;A) Ltd., 1977. [41 BOURNE,L.E. 
and B.R. EKSTRAND. Psychology: Its Principles and Meanings (3rd edition). Holt, Rinehart and Winston, 
1979. [5] BRESLIN,P., and J.C. BEATTY. A powerful interface to a high performance raster graphics system. 
Tech. Rep. CS-82-45, Com- puter Graphics Laboratory, Department of Computer Science, University of Waterloo, 
Dec. 1982. [6] BROWN, M.D. and S.W. SMOLIAR. Preparing dance notation scores with a computer. Computers 
and Graphics 3, 1 (1978), 1-7. [7] CALVERT, T.W., J. CHAPMAN, and A. PATLA. The simulation of human movement. 
Proceedings Graphics Interface "82, NCGA of Canada, Toronto, Ontario (1982), 227-234. [8] CAUSLEY, M. 
An Introduction to Benesh Movement Notation, Man Parrish, London, 1967. [9] DREYFUSS, H. Symbol Sourcebook. 
McGraw-Hill Book Com- pany, New York, 1972. [10] ESHKOL, N. Right Angled Curves. Movement Notation Society, 
1975. (For the Research Centre for Movement Nota- tion, Faculty of Arts, Tel Aviv University, Israel) 
[11] FEDAK, J. F. An initial design specification of a syntactic analyzer for Labanotation. Movement 
Project Report No. 10, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, 
Pa., Jan. 1978. [12] FOLEY, J.D. The design and implementation of user-computer interfaces. Siggraph 
"82 Tutorial Notes 7, ACM, New York, N.Y., July 1982. [13] GOMBRICH, E.H. The Visual Image. Scientific 
American 227, 3 (Sep. 1972), 82-96. [14] GRATER, A. private correspondence. Institute of Choreology, 
London, England, 1982. [15] HERBISON-EVANS, D. A human movement language for com- puter animation. Language 
Design and Programming Methodol- ogy, ed. Jeffrey M. Tobias, (Sep. 1979), 117-128. (Proceedings of the 
Symposium on Language Design and Programming Methodology, Sydney, Australia) [16] HUTCHINSON, A. Labanotation 
(3rd Edition). New York: Theatre Arts, 1977. [17] LABAN, R. Laban's Principles of Dance and Movement 
Nota- tion (2nd Edition). Macdonald &#38; Evans Ltd., London, 1975. [18] LANSDOWN, J. The computer in 
choreography. IEEE Com-puter 1l, 8 (Aug. 1978), 19-30. [19] MACKAY, S.A., R.E. SAYRE, and M.J. POTEL. 
3D Galatea: Entry of three-dimensional moving points from multiple perspec- tive views. Computer Graphics 
16, 3 (July 1982). [20] MARTIN, J. Design of Man-Machine Dialogues. Englewood Cliffs, N.J., 1973. [21] 
MCNAIR, B. A language for notating human movement. M.Sc. Thesis, Basser Department of Computer Science, 
Sydney Univer- sity, 1979. [22] MILLER, G. The magical number seven, plus or minus two: Some limits on 
our capacity for processing information. Psychol-ogy Review 63, 2 (March 1956), 81-97. [23] MILLS, M.I. 
Cognitive schemata and the design of graphics displays. Proceedings Graphics Interface '82, NCGA of Canada, 
"Toronto, Ontario, (May, 1982), 3-12. [24] NEWMAN, W.M. and R.F. SPROULL. Principles of Interactive 
Computer Graphics (2nd edition). McGraw-Hill Book Company, 1979. [25] PERRYNOWSKI, M.R. A Physiological 
Model for the Solution of Individual Muscle Forces During Normal Human Walking. Ph.D. Thesis, Department 
of Kinesiology, Simon Fraser Univer- sity, July 1982. [26] SAVAGE, G.J., J.M. OFFICER, and G. McDOUGALL. 
Computer graphics simulation of body movement language. Proceedings 6th Man-Computer Communications Conference, 
(1979), 209- 217. [27] SEALEY,D. Computers and Labanotation. Proceedings of the Twelfth Biennial Conference, 
Columbus, Ohio, International Council of Kinetography Laban, (Aug. 1981), 126-127. [28] SINGH, B., J.C. 
BEATTY, K.S. BOOTH and R. RYMAN. A graph- ics editor for Benesh movement notation. Tech. Rep. CS-82-41, 
Computer Graphics Laboratory, Department of Computer Sci- ence, University of Waterloo, Waterloo, Ontario, 
Dec. 1982. [29] SMOLIAR,S.W. and L. WEBER. Using the computer for a semantic representation of Labanotation. 
Computing in the Humanities, Proceedings of the Third International Conference on Computing in the Humanities, 
eds. Serge Lusignan and John S. North, The University of Waterloo Press, Waterloo, Canada, (Aug. 1977), 
253-261. [30] TILBROOK, D, A newspaper page layout system. M.Sc. Thesis, Department of Computer Science, 
University of Toronto, Toronto, Ontario, 1976. [31] TOWNSEND, M.A., M. IZAK, and R.W. JACKSON. Total 
motion knee goniometry. J. Biomechanics 10, 3 (1977), 183-193.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801133</article_id>
		<sort_key>63</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Advances in new display technology (Panel Session)]]></title>
		<page_from>63</page_from>
		<page_to>64</page_to>
		<doi_number>10.1145/800059.801133</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801133</url>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31094292</person_id>
				<author_profile_id><![CDATA[81100627264]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sol]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sherr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Westland Electronics]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P330861</person_id>
				<author_profile_id><![CDATA[81546854256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ifay]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P334053</person_id>
				<author_profile_id><![CDATA[81100652561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maloney]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PanelVision]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P225009</person_id>
				<author_profile_id><![CDATA[81100276801]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pleshko]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P330129</person_id>
				<author_profile_id><![CDATA[81100026347]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Elliot]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schlam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Eradcom]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332907</person_id>
				<author_profile_id><![CDATA[81332526316]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seats]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Thomas Electronics]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Sherr, Sol, "Video and Digital Electronic Displays - a User's Guide", John Wiley, N.Y. 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sherr, Sol, "Electronic Displays", John Wiley, N.Y. 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kmetz, A.R. and F. K. willisen, Eds., "Non-Emissive Electroopt ic Displays", Plenum Press, N. Y. 1976.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Kazan, B. Ed., "Advances in Image Pickup and Display", Academic Press, N.Y.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Cakir, A., et al, "Visual Display Terminals", John Wiley, N. Y., 1980.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Morell, A.M., "Color Television Picture Tubes", B. Kazan Ed., Advances in Image Pickup and Display, Supplement i, Academic, N.Y., 1974.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Cornsweet, T.N., "Visual Perception", Academic Press, N.Y., 1970.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Biberman, L.M., Ed., "Perception of Displayed Information", Plenum Press, N.Y., 1973.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Biberman, L.M., and S. Nudelman, Eds., "Photoelectric Imaging Devices", Plenum Press, N.Y., 1971.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Pleshko, P., "AC Plasma Flat Panel Displays", Computer Graphics World, Vol.5, No.7, July 1982, pp.47-48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PANEL IN NEW DISPLAY TECHNOLOGY ADVANCES CHAIR: PANELISTS: Elliot CHAIRMAN'S INTRODUCTION: New 
display technology has made some sig- nificant strides forward in the last few years. Although the 
CRT remains the most prevalent display device, accounting for over 85% of all installations, flat-panel 
matrix displays are becoming an increas- ingly significant factor in this area. The most important advances 
have been in thin film electroluminescence, gas discharge panels, and liquid crystal displays. In addition, 
light emitting diodes have shown improvements in light output and efficiency, and some recent work in 
electrophoretics give some promise that this technology will lead to viable display devices. Also, older 
technologies such as incandescent, vacuum flourescent, and electromechanical are still with us, and continue 
to be useful in many applica- tions, the first where very high luminance is required, the second in such 
applica- tions as automobile dashboards, and the third in various outdoor and large board installations. 
Finally, the CRT, not to be outdone by its rivals, has come up with improvements in performance, and 
with the flat tube versions in form factors that compete with those offered by the matrix displays. All 
of these advances will be discussed by the members of this panel. The participants in this panel are 
all eminently qualified to deal with the vari- ous technologies. Dr. Peter Pleshko of IBM is in charge 
of their plasma panel activity, which has taken on added impor- tance with the recent announcements that 
IBM is offering both the panel and termi- nals using the panel on an OEM basis. Dr. Ifay Chang, who is 
a staff member at the IBM Yorktown Laboratories, has been active in the display field for many years, 
and has contributed numerous articles and papers on advances in display technology to various publications. 
He is Vice President of the Society for Information Display, (SID) and has been Guest Editor for several 
issues of the Proceedings of the SID. Dr. Pleshko has also been a Guest Editor of the Proceedings, is 
a Regional Director of SID, and has been a co-Chairman of the Biennial Display Sol Sherr, Westland 
Electronics Ifay Chang, IBM Corporation Thomas Maloney, PanelVision Peter Pleshko, IBM Corporation Schlam, 
Eradcom Peter Seats, Thomas Electronics Conference. He has contributed many arti- cles to various publications, 
including the seminal one with P. M. Alt on "Scan- ning Limitations of Liquid Crystal Displays". Both 
he and Dr. Chang are Fel- lows of SID. Thomas Maloney is President of Panelvi- sion, a company that 
is in the forefront of developments in liquid crystal displays using thin-film transistors for switching 
matrices. Prior to his joining Panelvi- sion, he was associated with the Burroughs Corporation for many 
years, and was inti- mately involved with the development and production of SelfScan and other display 
products of that company. He is a Fellow of SID, and has been actively involved in that society at 
both the local and national level, as well as being co- chairman of the Biennial Display Confer- ence. 
 Peter Seats is President of Thomas Elec- tronics, one of the few remaining manufac- turers of CRT's 
left in the U.S. He has been active in CRT design and manufacture for many years and has lectured exten- 
sively on the subject at seminars and other forums. He is one of the outstand- ing authorities in the 
world on CRT's and is intimately familiar with all advances in that technology. Dr. Elliot Schlam is 
Chief of the Display Devices Branch at Eradcom, a Fellow of SID, and has been Guest Editor of the SID 
Proceedings. In addition, he has contri- buted important articles on electro- luminescent display devices 
to the IEEE Proceedings and other publications. He has also been an Editor of the Transac- tions on 
Electron Devices, and most recently has been closely associated with the latest advances in thin-film 
electro- luminescence. Finally, Sol, Sherr, the Chairman of this panel, is President of Westland Electron- 
 ics, Ltd., a consulting firm that special- izes in computer graphics and display technology. He is 
a charter Member and Fellow of SID, has written three books on display devices, equipment and systems 
as SS well as numerous articles, and is Editor of the SID Proceedings. Sherr, Sol, "Video and Digital 
Electronic Displays -a User's Guide", John Wiley, N.Y. 1982. Sherr, Sol, "Electronic Displays", John 
Wiley, N.Y. 1979. Kmetz, A.R. and F. K. willisen, Eds., "Non-Emissive Electroopt ic Displays", Ple- 
num Press, N. Y. 1976. Kazan, B. Ed., "Advances in Image Pickup and Display", Academic Press, N.Y. 
 Cakir, A., et al, "Visual Display Termi- nals", John Wiley, N. Y., 1980. Morell, A.M., "Color Television 
Picture Tubes", B. Kazan Ed., Advances in Image Pickup and Display, Supplement i, Academic, N.Y., 
1974. Cornsweet, T.N., "Visual Perception", Academic Press, N.Y., 1970. Biberman, L.M., Ed., "Perception 
of Displayed Information", Plenum Press, N.Y., 1973. Biberman, L.M., and S. Nudelman, Eds., "Photoelectric 
Imaging Devices", Plenum Press, N.Y., 1971. Proceedings of the Society for Information display, all 
issues. SID Symposia Digests, all issues. IEEE Transactions on Electron Devices, selected issues. 
 Biennial Display Conference Record, all issues. PANELISTS' ABSTRACTS Dr. I. F. Chang, IBM Evolutionary 
advances have always been made in various display technologies including CRT and several flat panel displays. 
However, advancements are expected to be accelerated due to the tremendous demand from exploding computer 
applications. The main thrust in display technology development today is to achieve good quality display 
from existing mature technology with emphasis on high resolu- tion, ergonomics, portability and color 
capability. Thus, electronic integration, power, volume and weight reductions and ergonomic design are 
being pursued along with effort in achieving high resolution and color capability at a reasonable cost. 
Advances made in these areas for CRT and several competing display technologies will be the main discussion. 
 Peter Pleshko, IBM AC Plasma Technology is emerging as the leading flat panel technology for large 
screens. Its inherent memory and fast pixel update time allow the screen to be updated fast and the image 
to be displayed without flicker, regardless of size. Monochromatic resolutions of up to 125 lines per 
inch have been demonstrated, which, practically speaking, eliminate "staircasing". Colour has also been 
shown feasible using dc plasma as a source of either ultra-violet radiation or for elec- trons to excite 
phosphors. Pleshko, P., "AC Plasma Flat Panel Displays", Computer Graphics World, Vol.5, No.7, July 
1982, pp.47-48. Criscimanga, T.N., and P. Pleshko, "The AC Plasma Display", Chapter 3 of Voi.40, Topics 
in Applied Physics, Springer-Velag, Berlin, 1980, J.I. Pankove, Editor. (A comprehensive, detailed treatment 
of both the processing technology and electro- optic characterization and behavior of the ac plasma 
display.) Elliot Schlam, Eradcom Thin film electroluminescent display tech- nology offers the advantages 
of high definition computer graphics and fast response gray scale, making it television compatible. These 
capabilities, coupled with very low power consumption, small package size and light weight, create exciting 
potential for this technology. A brief description of operating principles and a live demonstration will 
be presented. Peter Seats, Thomas Electronics A review of CRT developments will be presented with 
emphasis on monochrome and color displays for use with portable com- puters. The progress of flat CRT 
tech- niques will also be discussed. CRT's are also being developed for high resolution (4000 x 4000) 
color slide recording, and their special capabilities will be described. If time permits, CRT develop- 
 ments for improved graphics displays, based on three-color voltage penetration screens and color wheel 
techniques will also be reviewed. 64 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801134</article_id>
		<sort_key>65</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Near real-time shaded display of rigid objects]]></title>
		<page_from>65</page_from>
		<page_to>72</page_to>
		<doi_number>10.1145/800059.801134</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801134</url>
		<abstract>
			<par><![CDATA[<p>Described is a visible surface algorithm and an implementation that generates shaded display of objects with hundreds of polygons rapidly enough for interactive use &#8212; several images per second. The basic algorithm, introduced in [Fuchs, Kedem and Naylor, 1980], is designed to handle rigid objects and scenes by preprocessing the object data base to minimize visibility computation cost. The speed of the algorithm is further enhanced by its simplicity, which allows it to be implemented within the internal graphics processor of a general purpose raster system.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>PP39072004</person_id>
				<author_profile_id><![CDATA[81339500019]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Henry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fuchs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P99648</person_id>
				<author_profile_id><![CDATA[81100592360]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gregory]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Abram]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P330154</person_id>
				<author_profile_id><![CDATA[81100081551]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Grant]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Clark, J.H. and Hannah, M.R. "Distributed Processing in a High-Performance Smart Image Memory", LAMBDA, 4th Quarter (1980), pp. 40-50.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>6684</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Foley, J.D. and Van Dam, A. Fundamentals of Interactive Computer Graphics, Addison Wesley, Reading, Mass. (1982).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807481</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Fuchs, H., Kedem, Z.M., and Naylor, B.F. On Visible Surface Generation by A Priori Tree Structures, Computer Graphics (Proc. SIGGRAPH '80), Vol. 14, No. 3, (July, 1980), pp. 124-133.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Fuchs, H., Poulton, J., Paeth, A., Bell A. Developing Pixel-Planes, A Smart Memory-Based Raster Graphics System, Proceedings, Conference on Advanced Research in VLSI, Cambridge, Mass. January 25-27, 1982.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>909951</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Naylor, B.F. "A Priori Based Techniques for Determining Visibility Priority for 3-D Scenes", Doctoral Dissertation, University of Texas at Dallas (1981).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Newman, W.M. and Sproull, R.F. Principles of Interactive Computer Graphics (2nd. ed.), McGraw-Hill, New York, (1979).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Schachter B.J. Computer Image Generation for Flight Simulation, IEEE Computer Graphics and Applications, Vol. 1, No. 4, (October, 1981).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Schumacker, R.A. "A New Visual System Architecture, Proc. Second Interservice/Industry Training Equipment Conf., Salt Lake City, Utah, (Nov. 1980), pp. 94-101.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Schumacker, R.A., Brand, B., Gilliland, M., and Sharp, W. "Study for Applying Computer Generated Images to Visual Simulation", Tech. Report No. AEHRL-TR-69-14, (AD 700375), US Air Force Human Resources Lab (1969).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356626</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I.E., Sproull, R.F. and R.A. Schumacker, R.A. "A Characterization of Ten Hidden-Surface Algorithms", Computing Surveys, Vol. 6, No. 1 (1974).]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 17, Number 3 July 1983 Near Real-Time Shaded Display of Rigid Objects Henry 
Fuehs Gregory D. Abram Eric D. Grant University of North Carolina at Chapel Hill Abstract Described 
is a visible surface algorithm and an implementation that generates shaded display of objects with hundreds 
of polygons rapidly enough for interactive use --several images per second. The basic algorithm, introduced 
in [Fuchs, Kedem and Naylor, 1980], is designed to handle rigid objects and scenes by preproeessing the 
object data base to minimize visibility computation cost. The speed of the algorithm is further enhanced 
by its simplicity, which allows it to be implemented within the internal graphics processor of a general 
purpose raster system. CR Categories and Subject Descriptors: 1.3.3 [Computer Graphics]: Picture/Image 
Generation Display Algorithms; Viewing Algorithms; 1.3.7 [Computer Graphics]: Three-Dimensional Graphics 
and Realism -Visible Line/Surface Elimination 1. Introduction The generation of realistic, colored images 
of 3D scenes has been a subject of much study for nearly twenty years. Many visible-surface algorithms 
have been developed for a variety of applications and machine environments (see, eg., [Sutherland, Sproull 
and Schumaeker, 1974] or [Foley and Van Dam, 1982]). However, for real-time interactive applications, 
very expensive special purpose hardware is needed. Even if a much lower image generation rate, of perhaps 
one or two per second, is acceptable, we know of no (previously published) algorithm which can accomplish 
this on a general purpose graphics system. It is not surprising that the generation of rendered color 
images is computationally expensive. Not only do the usual transformation, clipping and perspective steps 
need to be performed (the ones independent of the particular type of display or image generation algorithm) 
but visibility and rendering calculations must *This work was supported in part by NSF grant MCS79-02593 
and in part by NIH grant NSIHL 16759-01. Permission to copy without fee all or part of this material 
is granted provided that the copies are not made or distributed for direct commercial advantage, the 
ACM copyright notice and the title of the publication and its date appear, and notice is given that copying 
is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. also be performed for every pixel in the image. General purpose algorithms 
may take several seconds (or much longer) to generate these images because they require either many calculations 
at each pixel or have signi~cant overhead at a higher level in order to minimize the pixel calculations. 
Our own long-term goals are to have real-time 3D images generated in our laboratory, for use in a wide 
range of interactive applications. Although we are designing special purpose equipment for this [Fuchs, 
Poulton, Paeth and Bell, 1982], in the immediate future the images have to be generated with our current 
general purpose graphics system. To generate these images, we are willing to sacrifice update rate, down 
to even one or two per second. Further, we have found that many of our applications (as well as those 
of others) have the significantly simplifying property that the world model changes far less frequently 
than the viewing position [Schumacker, Brand, Gilliland and Sharp, 1969]. Our aim, then, is to cut image 
generation time by taking advantage of this simplification. We do so by by pushing much of the visible-surface 
overhead into a preprocessing phase, thereby greatly reducing the overhead at image-generation time. 
By doing so, we hope to make the generation of realistic, colored images for our class of applications 
fast enough to be useful in an interactive mode. 2. BSP-Tree Basics This section briefly reviews the 
Binary Space Partitioning algorithm (BSP-tree) as introduced in [Fuchs, Kedem and Naylor, 1980],  2.1. 
Motivation [Schumacker, Brand, Gilliland and Sharp, 1969] introduced the notion that the image generation 
can be simplified in situations where the world model changes less frequently than the viewpoint or direction 
of view of the observer. Many applications have this property: a biochemist studying a complex molecule, 
a physician examining an anatomical structure for signs of disease, an architect (or her client) walking 
through a planned house or subdivision, an engineer designing a mechanical part. However, the Schumacker 
approach's dependence on manual intervention in the building of the internal data structure made it difficult 
(and time consuming) to generate new databases, and thus limited its general usefulness. Although the 
current implementation of the BSP-tree algorithm is limited to static world models (since whenever the 
world model changes, the preprocessing data restructuring step must be invoked) we hope to ease this 
restriction in the future. &#38;#169; ACM 0-89791-109-1/83/007/0065 $00.75 In order for the algorithm 
to run fast, the entire BSP-tree should be in local memory. Although this is not an inherent restriction 
in the algorithm, the overall performance would be significantly degraded if parts of the tree had to 
be swapped from backing store. Our implementation (detailed below) uses 8 bit-planes of a 24-bit frame 
buffer to store a BSP-tree data structure with up to approximately 5000 polygons. 2.2. Description of 
the basic alEorithm The algorithm consists of two components: a one-time preprocessing module ("Make-tree") 
that converts the input polygon list into the BSP-trce structure, and  an image-generation module ("Traverse") 
which traverses this structure and generates the polygons in a back-to-front order. (Strictly speaking, 
the order is not back-to-front, but is functionally equivalent to it.)  2.2.1. Building the BSP-tree 
The fundamental notion is one of a separating plane: that is, given a plane in the 3D scene and a viewing 
point, no polygon on the viewpoint side of the plane can be obstructed by any polygon on the far side. 
Of course, if the viewpoint should move to the other side of the plane, the obstruction priorities are 
reversed [Sohumaeker, Brand, GiUiland and Sharp, 1969]. The algorithm uses this simple notion to construct 
a binary tree of polygons from the original polygon list (see Fig. 1). A polygon is selected from the 
list and placed at the root of the tree. Each remaining polygon in the list is tested to determine the 
side of the root polygon in which it lies and is then placed in the appropriate descendent list. Any 
polygon which crosses the plane of the root polygon is split along that plane and each part put in the 
appropriate list (see polygon 5 in fig. 2). This procedure is repeated recursivcly in the following way: 
from each of these descendent lists, a polygon is selected to be the root of that subtrec, and the remaining 
polygons in this list are split by the plane of the root of the new subtree (see polygon 2 fig. 3). / 
/ 5b  / I    ,' / ! / / Figure 1: Top view of scene PROC Make tree (poly_list) returns (BSPjree); 
if (polyJist is empty) return (NULI~TREE) else root := select (poly..~st); backJist := NULL; front_list 
:= NULL; foreach (polygon in polyjist) if (polygon is not the root) if (polygon in front of root) Addlist 
(polygon, front list): else if (polygon is behind root) Addlist (polygon, back list); else Split._poly 
(polygon, root, front_part, backpart); Addiist (front.part, front list): Addlist (back_part, back list); 
return (Combine-tree (Make_tree (front list), root, Make_tree (back list))); END The choice of the 
root polygon strongly influences the size of the tree. In the example illustrated in Figures 1-3, a better 
choice of the initial root would be a polygon other than 3, for example polygon 5. Figure 4 illustrates 
a BSP-tree with an initial choice of polygon 5. Note that the number of polygons in this tree is the 
same as the number in the input polygon list, while in the example of Figure 3, the tree is larger (6 
polygons instead of the original 5). In reasonable-sized scene descriptions, the tree may grow substantially. 
In section 3.1 we discuss strategies to keep the tree small and give results. ([Nayior, 1981] develops 
bounds on the size of the BSP- tree and discusses many other related issues.) 2.2.2. Image generation 
Once the BSP-tree has been constructed, generating an image from any point of view is simple. The tree 
is traversed in a special in-order fashion. At each node of the tree, we determine whether the eye is 
in front of or behind the node polygon. This result determines which subtree will be traversed first. 
The order is always the same: traverse the "other side" subtree, output and paint the node polygon, then 
traverse the "near side" subtree. 3. New Results 3.1. Tree size When this algorithm was first introduced, 
there was concern that. in Many cases, the tree would be significantly larger than the original polygon 
list. Indeed. there were fears that, given a list of N polygons, the BSP-tree may turn out to contain 
N 2 or more polygons! Although no tight bound has been yet been found, from our experience the trees 
derived from most world model databases are less than twice the size of the original %  Figure 2: Figure 
3: front/~ After one level A complete tree. ~kback of recursion. Polygofl 3 chosen as root. Figure 
4: An alternate tree with polygon 5 at root.  We use the heuristic of selecting a root at each step 
whose plane cuts the fewest other polygons in the list. In our first experiments, we made the selection 
after examining every polygon in the list as a candidate for root. We have since found, however, that 
selecting just a few candidates at random from the list (originally suggested by Zvi Kedem) gives nearly 
as good results. These results are shown in table 1, which indicates that near-minimal trees can be found 
by examining only about 5 candidate polygons at each level. The most startling result in this table is 
the example of the "Old Well", (the longstanding symbol of UNC - Chapel Hill). In this highly non-convex 
model of 356 polygons, this heuristic produces a tree which is exactly the same size as the input polygon 
list. 3.2. Simple implementation in a graphics processor: Although it may be clear from section 2.2 
that the algorithm can be simply expressed in a high level language with recursion, what may be less 
obvious is that the image generation component is simple enough to be implemented entirely within a 
programmable graphics processor. Our implementation runs on an Ikonas RDS3000 raster graphics system, 
which has a programmable AM2900-based internal processor. The run-time component consists of 1309 64-bit 
microcode words, of which only 218 words implement the BSP-tree part of the image generation algorithm, 
while the rest (1091 words) implement the transformations, clipping, perspective and polygon painting 
routines that are needed in any 3D image ~eneration system. PROC Traverse tree (tree) /, Traverse an 
input BSP-tree and generate visible- surface image. The function Display handles transformation, clipping, 
perspective division, lighting and rendering. All but the rendering can be done elsewhere if deemed more 
efficient. ,/ if (tree is empty) return else if (eye is in front of tree.root.polygon) Traverse_tree 
(tree.back_descendent); Display (tree.root.polygon); Traverse_tree (tree.front_descendent); else Traverse_tree 
(tree.front_descendent); Display (tree.root.polygon); /* if back-facing polygons are to be considered 
invisible, remove the previous line. */ Traverse_tree (tree.backdescendent); END polygon list; the 
largest found was 2.33 times. It should be noted that the image generation time does not increase by 
nearly this factor of two since it is dominated by pixel-painting time and the total pixel area of a 
model does not change as the polygons within it are split. Table 1 indicates the input polygon list and 
BSP- tree sizes for the various objects illustrated in Figures 6 -10. Size of Output for varying l~Lre 
..,11 e input number of eemdid atcs (and number) polygon list 1 3 5 15 Carotid artery 915 2421 1843 1720 
1633 (Fig. s 6a,b) Old Well (Fig. 8) 356 1125 384 378 356 (low detail) SkullCT (Fig. 9) 988 3341 2361 
2255 2053 Old Well 1000 2426 1304 1176 1032 (high detail) Space Shuttle 418 2092 1201 1095 972 3cubes 
(Fig. I0) 216 402 279 263 240 Ribbon plot 368 1478 929 797 660 Klein bottle 450 1475 1118 1035 990 Robot 
arm 268 975 650 587 440 Treemaking time Figure name for 5 candidates (seconds) Carotid artery 116.3 
Old Well 45.0 (low detail) Old Well 448.2 (high detail) SkulICT 225.8 Space Shuttle 144.2 3cubes 15.4 
Ribbon plot 66.4 Klein bottle 272.7 Robot arm 52.7 Table I: Tree making statistics. The implementation 
is written in C under UNIX on a VAX 11/780. 3.3. System speed Table 2 indicates the time needed to generate 
the images illustrated in Figures 6 - 10. Because some of the speed of this implementation is due to 
a relatively fast graphics processor, it may be useful to analyze where the effieieneies are due to the 
algorithm itself (rather than the processor), to determine the utility of the algorithm independent of 
the processor on which it is implemented. To do this, we compare it with the most similar previous algorithm, 
the widely used Z (or depth) buffer algorithm (see, eg., [Foley and Van Dam, 1962]). The BSP-tree algorithm 
has a fixed per-polygon overhead of the tree traversal; this consists of a) the maintenance of a stack 
of tree return pointers for traversing the tree, and b) performing the inner product at each node to 
determine which way to turn next. The Z buffer, on the other hand, has none of this polygon overhead, 
but has the extra burden of calculating the Z, comparing, and possibly updating Z at each pixel. There 
is then a tradeoff between this per-polygon overhead and the per- pixel overhead, and it appears that 
until the average polygon size becomes very small (a few pixels) that the per-polygon overhead of the 
BSP-tree will be considerably less burdensome than the per-pixel  Number of Image Figure name polygons 
generation rate and number in tree (frames/second) Carotid Artery 1633 0.50 - 0.58 (Fig. 6a, b) SkuI1CT 
(Fig. 9) 2255 0.40 - 0.48 3 atoms (Fig. 7) 1440 1.53 -1.84 Old Well (Fig. 8) 356 1.6 - 2.5 (low detail) 
Space Shuttle 923 1.94- 2.56 3 cubes (Fig. 10) 280 2.46 - 4.81 Old Well 1005 1.05 - 1.58 (high detail) 
 Table 2: Image generation statistics. course, is only valid for applications which can conform to the 
two limitations of the BSP-tree algorithm: a static world model and sufficient local memory to hold the 
BSP-tree. 4. Future Work 4.1. Removing static world model restrictions In the present implementation 
of the algorithm, whenever the world model changes, the entire BSP-tree must be rebuilt. From table 1, 
one can see that it takes a minute or two for this process. We are working on various alternatives to 
allow relaxing this restriction, We are considering the situation in which there is only limited ~hange 
in the world model. One such example might be when the range of motion of the moving objects is known; 
such as airplanes which always fly above the airport and land only on runways, or automobiles which remain 
on the road, or parts of molecules which only move in certain restricted ways, or doors which only swing 
on their hinges. With this knowledge, we might be able to construct a series of convex regions, which 
always contain the moving object. The BSP-tree of the world model could be constructed such that no root 
polygons cuts this region. This causes all the polygons of the moving object to end up in their own subtree, 
which may then be transformed independently from the rest of the scene by using a nested transformation 
matrix at the root of this object's subtree (see Figure 5).  4.2. Anti-Aliasing We are currently experimenting 
with adding anti-aliasing to the image generation using a sub-pixel mask similar to the latest Evans 
and Sutherland digital scene generator, the CT-5 [Sehumacker, 1980]. This technique involves maintaining 
a binary mask of, say, 4x4 subpixels at each pixel. The polygons are painted front-to-back, and are sampled 
at the subpixel resolution, with the binary mask indicating the subpixel areas which have already been 
covered by a polygon. We note that the BSP-tree can generate a front-to-back (equivalent) order of polygons 
simply by reversing the order of the traversal (i.e., instead of far side; node polygon, near side, the 
order becomes near side, node polygon, far side). The contributions of the current polygon to a particular 
pixel's color are determined by the number of subpixels within that pixel of which this polygon is visible. 
This contribution is accumulated in the RGB pixel value in the image frame buffer. Group 1 of buildings, 
. B? Subtree for for car Group 2 Group 2 of buildings, trees, etc. Figures 5a and 5b: non-static scene 
handling  5. Summary and Conclusions We have shown that the BSP-tree visible-surface algorithm generates 
images rapidly enough to be useful in interactive applications and that it can be easily implemented 
in a programmable graphics processor. Further, we have shown that in all cases encountered so far, that 
the tree size stays within reasonable bounds. We are currently using the system to study reconstructed 
surfaces of human arteries and density distributions of organic molecules. Our experience indicates 
that it is a viable (and faster) alternative to the commonly used Z buffer in many situations. Finally, 
we note that this algorithm may be increasingly attractive as new raster graphics hardware systems become 
available. At least one new commercially available system (Megatek 7200 [Foley and Van Dam, 1982]) and 
several experimental designs ([Clark and Hannah, 1980] and [Fuchs, Poulton, Paeth and Bell, 1982]) concentrate 
on fast rendering of lines and polygons. It appears that these systems will most easily and directly 
generate realistic images of 3D scenes from a back-to-front ordered list of polygons. Since many can 
be expected to have transformation and clipping hardware, all that remains to be done in software is 
to generate the ordered list of polygons, something which can be achieved quite handily by the BSP-tree 
algorithm. ,~knowledgements We would like to thank the many people at UNC-CH and elsewhere who have assisted 
us with this research: Gregg Podnar and Yehuda Kalay (CMU), Mike Connally (Yale), James R. Smith (NASA-Johnson 
Space Center), Richard A. Weinberg (Cray Research), and Sandy Bloomberg and Frank Moore (UNC) all for 
kindly allowing us the use of their graphic databases, Gary Bishop (UNC) for programming tools, and Bo 
Strain and Mike Pique (UNC) for photographic assistance. References Clark, J.H. and Hannah, M.R. Distributed 
Processing in a High-Performance Smart Image Memory", LAMBDA, 4th Quarter (1980), pp. 40-50. Foley, J.D. 
and Van Dam, A. 1~nclamentals of Interactive Computer Graphics, Addison Wesley, Reading, Mass. (1962). 
Fuchs, H., Kedem, Z.M., and Naylor, B.F. On Visible Surface Generation by A Priori Tree Structures, 
Computer Graphics (Proc. SIGGRAPH. '80), VoL 14, No. 3, (July, 1980), pp. 124-133. Fuchs, H., Poulton, 
J., Paeth, A., Bell A. Developing Pixel- Planes, A Smart Memory-Based Raster Graphics System, Proceedings, 
Conference on Advanced Research in VLSI, Cambridge, Mass. January 25-27, 1982. Naylor, B.F. "A .Pr/om 
Based Techniques for Determining Visibility Priority for 3-D Scenes", Doctoral Dissertation, University 
of Texas at Dallas (1981). Newman, W.M. and Sproull, R.F. Principles of Interactive Computer Graphics 
(2nd. ed.), McGraw-Hill, New York, (1979). Schachter B.J. Computer Image Generation for Flight Simulation, 
IEEE Computer Graphics and Applications, Vol. 1, No. 4, (October, 1981).. Schumacker, R.A. "A New Visual 
System Architecture, Proc. Second Interservice/Industry Training Equipment Conf., Salt Lake City, Utah, 
(Nov. 1980), pp. 94-101. Schumaeker, R.A., Brand, B., Giililand, M., and Sharp, W. "Study for Applying 
Computer Generated Images to Visual Simulation", Tech. Report No. AEHRL-TR-69-14, (AD 700375), US Air 
Force Human Resources Lab (1969). Sutherland, I.E., Sproull, R.F. and R.A. Schumacker, R.A. A Characterization 
of Ten Hidden-Surface Algorithms", Computing Surveys, VoL 6, No. 1 (1974).    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801135</article_id>
		<sort_key>73</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[A scan-line hidden surface removal procedure for constructive solid geometry]]></title>
		<page_from>73</page_from>
		<page_to>82</page_to>
		<doi_number>10.1145/800059.801135</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801135</url>
		<abstract>
			<par><![CDATA[<p>This paper presents a new methodology for resolving visible surface images of solid models derived from Boolean combinations of volumetric building blocks. The algorithm introduced here is an extension of well-established scan-line hidden surface removal procedures, and it integrates knowledge of a Boolean construction tree in the surface resolution process. Several hidden surface coherence properties are discussed in terms of their possible exploitation in the intricate solid model visualization process. While many of the earlier coherence techniques depend on a polygon environment in which surfaces and volumes do not intersect, the Boolean process can not afford that luxury because it is inherently required to handle intersecting volumes and surfaces. Initial tests indicate that substantial performance improvements over previous methods can be achieved with the algorithm described in this paper, and that these improvements increase as model complexity increases.</p> <p>An underlying philosophy of a dual solid modeling system is proposed in this paper. It suggests that two solid modelers are necessary to successfully satisfy both analytical precision requirements and user interface visualization requirements. The visual solid modeling task addressed in this paper provides greatly improved response capabilities, as compared to other systems, by striving to optimize the constructive solid model (CSG) solid model computations specifically for display purposes.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Computer-aided design]]></kw>
			<kw><![CDATA[Constructive solid geometry]]></kw>
			<kw><![CDATA[Hidden line removal]]></kw>
			<kw><![CDATA[Hidden-surface removal]]></kw>
			<kw><![CDATA[Solid modeling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Hidden line/surface removal</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P332876</person_id>
				<author_profile_id><![CDATA[81100484386]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[R]]></middle_name>
				<last_name><![CDATA[Atherton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[General Electric Company, Corporate Research and Development, Schenectady, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>362739</ref_obj_id>
				<ref_obj_pid>362736</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bouknight, W.J., "A Procedure for Generation of Three-Dimensional Half-Toned Computer Graphics Representations," Comm. ACM, Vol. 13, No. 9, September 1970.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Boyse, J.W. and Gilchrist, J.E., "GMSolid: Interactive Modeling for Design and Analysis of Solids," IEEE Computer Graphics and Applications, Vol. 2, No. 2, March 1982, pp. 86-97.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Brown, C.M., "PADL-2: A Technical Summary," IEEE Computer Graphics and Applications, Vol. 2, No. 2, March 1982, pp. 69-84.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563895</ref_obj_id>
				<ref_obj_pid>965141</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Hamlin, G. and Gear, C.W., "Raster-Scan Hidden Surface Algorithm Techniques," Computer Graphics, Vol. 11, 1977, No. 2, pp. 206-213.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>6684</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Foley, J.D. and VanDam, A., Fundamentals of Interactive Computer Graphics, Addison-Wesley, 1982.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Goldstein, R.A. and Malin, L., "3D Modeling with the Syntha Vision System," First Annual Conference on Computer Graphics in CAD/CAM Systems, MIT, April 1979, pp. 244-247.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Hindel, B., Emulation and Invention, New York University Press, 1981.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806788</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Hubschman, H., and Zucker, S., "Frame-to-Frame Coherence and the Hidden Surface Computation: Constraints for a Convex World," Computer Graphics, Vol. 15, No. 3, August 1981, pp. 45-54.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Myers, W., "An Industrial Perspective on Solid Modeling," IEEE Computer Graphics and Applications, Vol. 2, No. 2, March 1982, pp. 86-97.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Requicha, A.A.G., and Voelcker, H.B., "Solid Modeling: A Historical Summary and Contemporary Assessment," IEEE Computer Graphics and Applications, Vol. 2, No. 2, March 1982, pp. 9-24.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Romney, G.W., Computer Assisted Assembly and Rendering of Solids, Computer Science Department, University of Utah, TR-4-20, 1970.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Roth, S.D., "Ray Casting for Modeling Solids," Computer Graphics and Image Processing, No. 18, 1982, pp. 109-144.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806785</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Sechrest, S., and Greenberg, D.P., "A Visible Polygon Reconstruction Algorithm," Computer Graphics, Vol. 15, No. 3, August 1981, pp. 17-27.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356626</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I.E., Sproull, R.F., and Schumacher, R. A., "A Characterization of Ten Hidden-Surface Algorithms," ACM Computing Surveys, Vol. 6, No. 1, March 1974, pp. 1-55.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Tilove, R.B., "Set Membership Clarification: A Unified Approach to Geometric Intersection Problems," IEEE Transactions on Computers, Vol. C-29, No. 10, October 1980.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Voelcker, H.B., "Algorithms and Applications," Tutorial on Solid Modeling, SIGGRAPH '82 (ACM).]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Watkins, G.S., A Real-Time Visible Surface Algorithm, Computer Science Department, University of Utah, UTECH-CSC-70-101, June 1970.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Wesley, M. A., Lozano-Perez, T., Lieberman, L. I., Lavin, M. A. and Grossman, D. D., "A Geometric Modeling System for Automated Mechanical Assembly," IBM Journal of Research and Development, Vol. 24, No. 1, January 1980.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>802345</ref_obj_id>
				<ref_obj_pid>800073</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Wolfe, R., Fitzgerald, W., and Gracer, F., "Interactive Graphics for Volume Modeling" Proceedings of the IEEE Eighteenth Design Automation Conference, pp. 463-470.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A SCAN-LINE HIDDEN SURFACE REMOVAL PROCEDURE FOR CONSTRUCTIVE SOLID GEOMETRY Peter R. Atherton General 
Electric Company Corporate Research and Development Schenectady, New York 12345 Abstract This paper 
presents a new methodology for resolving visi- ble surface images of solid models derived from Boolean 
combinations of volumetric building blocks. The algorithm introduced here is an extension of well-established 
scan-line hidden surface removal procedures, and it integrates knowledge of a Boolean construction tree 
in the surface reso- lution process. Several hidden surface coherence properties are discussed in terms 
of their possible exploitation in the in- tricate solid model visualization process. While many of the 
earlier coherence techniques depend on a polygon environ- ment in which surfaces and volumes do not intersect, 
the Boolean process can not afford that luxury because it is in- herently required to handle intersecting 
volumes and sur-faces. Initial tests indicate that substantial performance im- provements over previous 
methods can be achieved with the algorithm described in this paper, and that these improve-ments increase 
as model complexity increases. An underlying philosophy of a dual solid modeling system is proposed in 
this paper. It suggests that two solid modelers are necessary to successfully satisfy both analytical 
precision requirements and user interface visualization requirements. The visual solid modeling task 
addressed in this paper pro-vides greatly improved response capabilities, as compared to other systems, 
by striving to optimize the constructive solid model (CSG) solid model computations specifically for 
dis- play purposes. CR Categories: I3.7 [Computer Graphics]: Three-Dimen-sional Graphics and Realism 
-Visible Line/Surface algo-rithm; 13.5 [Computer Graphics]: Computational Geometry and Object Modeling 
--Curve, surface, solid and object representations; 13.3 [Computer Graphics]: Picture/Image Generation 
-Display algorithms; J.6 [Computer Applica-tions]: Computer-aided Engineering -Computer-aided design 
(CAD), Computer-aided manufacture (CAM) Key Words and Phrases: computer graphics, hidden-surface removal, 
hidden line removal, solid modeling, constructive solid geometry, computer-aided design Permission to 
copy without fee all or part of this material is granted provided that the copies are not made or distributed 
for direct commercial advantage, the ACM copyright notice and the title of the publication and its date 
appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy otherwise, or to republish, requires a fee and/or specific permission. &#38;#169; ACM 0-89791-109-1/83/007/0073 
$00.75 1. INTRODUCTION In his book entitled Emulation and Invention [7], Brooke Hindle says of Samuel 
Morse "His great strength remained a quality of mind that permitted him to manipulate mental im- ages 
of three-dimensional telegraph com-ponents as well as complete telegraphic sys-tems, altering them at 
will and projecting vari- ous possibilities for change and development." Few people possess the natural 
capability to mentally envi- sion three-dimensional mechanical systems, particularly from someone else's 
description. Therefore, there is a very great opportunity for the application of computer graphics in 
solid modeling to assist the design, analysis, and manufacturing engineers with their spatial understanding 
of model .struc-tures. In concept, solid models can offer the designer more physically understandable 
building components, and comput- er graphics will afford him visual comprehension of the phys- ical structures 
and the spatial relationships between those building blocks. In contrast to the views of some researchers 
in the solid modeling field [10, page 20], I feel that interac- tive graphics technology has a long way 
to go before it can offer all CAD/CAM users a tool that will provide three-dimensional visual insight 
equaling the natural abilities of someone like Samuel Morse. The application of volumetric Boolean set 
operations to solid objects in order to represent more complex solid objects is commonly referred to 
as constructive solid geometry (CSG). The fundamental building blocks used in CSG are solid objects called 
primitives. Two primitives (for instance, A and B) can be combined to form a new solid object by ap- 
plying one of the volumetric Boolean set operations defined as follows: Union (A U B): that volume of 
space found in either A orB Intersection (A N B): that volume of space found in both A and B Difference 
(A-B): that volume of space found in A but not found in B The systematic application of the Boolean 
set operations is usually controlled by a binary tree data structure commonly referred to as a CSG tree 
(Figure 1). Each internal node of a CSG tree defines the Boolean set operation to be applied to Figure 
1. A CSG tree controls the systematic application of Boolean set operations its two children. The leaves 
of the CSG tree contain infor- mation to indicate what solid primitives are to be used for Boolean processing. 
A CSG model includes both the CSG tree and the associated solid primitives. A new primitive can be created 
by geometrically evaluating the CSG model in or- der to produce a new solid object definition. A library 
of primitives can be built up from both original solid objects m defined by some means other than CSG 
and the solid objects resulting from previous CSG evaluations. For CAD/CAM applications, there are essentially 
two basic uses of a CSG model. In most manufacturing processes, a very precise evaluation of the CSG 
model is re- quired. There is little concern for how much computing time is needed to derive this accurate 
definition as long as it is ac- complished eventually. For example, numerical control en-gineers will 
normally refer to accuracy tolerances in terms of thousandths of an inch and run-times in terms of minutes 
or hours. On the other hand, most interactive visualization tasks such as design or robot programming 
need only enough accuracy and information about the CSG model to make a meaningful image of it. However, 
user interaction does re-quire that images be generated rapidly in order to maximize user productivity. 
In the past, solid modelers have been developed with the intention of solving both the accuracy and interactive 
visualization tasks with essentially the same modeler. It is proposed here that these tasks can be better 
solved by deviating from the single modeler trend and analyzing each of these two tasks separately (Figure 
2). Op-erating in the dual modeler scenario, a designer would create and edit a CSG model by interacting 
with the visual modeler at a design station. The analytical modeler would be invoked on a demand basis 
when exact model solutions are required for high-tolerance applications. The motivation behind this paper 
is to solve the visualization task efficiently, in order to provide informative shaded surface imagery 
of CSG models rapidly enough for effective interactive use. As described in past literature there are 
two general ap- proaches used to generate shaded surface images from CSG models: 1. Geometrically evaluate 
the CSG model in order to define the boundary surface model resulting from the application of volumetric 
Boolean processes to previ-ously defined solid primitives. Subsequently, apply a dPI to s Generate Placemenl 
Preview Create New Inspection Numerical Analysis and Edit of Primitives CSG Result Solid Primitives Control 
CSG Tree Figure 2. The Dual Solid Modeling Scenario hidden surface removal program to the resulting 
solid object bounaary definition [1,4,5,8,13,14,17]. One class of hidden surface removal procedures that 
could be applied to boundary surface models is described in Section 3 of this paper. 2. Mathematically 
fire a ray through each image picture element into an environment composed of transformed primitives. 
For each ray, analyze the one-dimensional Boolean problem enough to resolve the first visible point along 
the ray [2,3,6,9,10,12,16,19]. The applica- tion of ray-firing is closely examined in Section 2. Given 
that the "ultimate" user interface goal is real-time hu- man interaction with the solid model, execution 
speed of the image generation task is a primary concern. In light of that goal, both approaches described 
above involve significant un- necessary computation. The first approach requires that a geometric solution 
to the CSG model be found before a dis- play calculation can be made. As such, a great deal of the computation 
is wasted in resolving CSG results that the hid- den surface removal process will hide. Also, calculation 
of the three-dimensional Boolean tasks (like surface intersec-tions and data base changes) tends to be 
much more com-plex and time consuming than the subsequent hidden surface removal processing even for 
polygon based models or model approximations. Several commercial solid modelers do sup- port polyhedral 
model approximations, but they utilize this time consuming two-step approach in order to generate CSG 
images. The ray-firing approach does simplify the Boolean calculations by operating in a one-dimensional 
space instead of three. It is significant to note that several CSG modelers that can produce a resultant 
boundary surface model from a CSG model use the ray-firing technique to produce shaded surface images 
[2,3]. Nonetheless, ray-firing does tend to be computationally expensive and time consuming in that dis- 
tinct solutions are derived for every picture element. In oth- er words, the general ray-firing approach 
is inefficient be- cause it does not take advantage of coherence properties [12]. Based upon past analyses, 
shaded image generation of higher order surfaces can be made significantly faster by us- ing polygonal 
approximation techniques [5]. In most en-gineering, manufacturing, inspection, and assembly applica- 
tions, polygonal approximation of curved surfaces is either unacceptable in terms of accuracy or impractical 
in terms of the number of polygons necessary for reasonable tolerance limits. For a user display interface, 
however, the accuracy requirements are not nearly as rigorous. In addition, polygo- nal approximation 
computations of solid primitive surfaces can be performed as a preprocessing step to CSG interaction 
and need not be performed during the actual interaction pro- cess. Thus, interactive speed requirements 
seem to dictate that polygonal approximations be used for interactive image generation until superior 
methods are developed. When the entire geometric model has been transformed into a polygonal form, a 
large body of existing hidden surface removal technology can be made available. To date, there is no 
publication known to the author that applies scan-line hid- den surface removal techniques to the problem 
of generating shaded images directly from CSG models. The remainder of this paper will focus on the application 
of polygon-based scan-line techniques for visible surface processing of models composed of Boolean combinations 
of polygonal primitives. The next two sections provide a framework for the CSG scan-line hidden surface 
procedure by describing the funda- mental ray-firing and scan-line hidden surface removal algo- rithms 
that will be applied. Following sections describe a specific CSG scan-line hidden surface procedure in 
detail, and how coherence techniques [14] can be applied to this more complex procedure. The paper concludes 
with a description of a CSG scan-line hidden surface implementa-tion and a comparison to a corresponding 
ray-firing algorithm in terms of execution speed. 2. SOLVING ONE-DIMENSIONAL BOOLEAN TASKS USING RAY-FIRING 
Ray-firing algorithms like those described by Roth [12] and Goldstein and Malin [6] apply a single procedure 
to every point on the viewing screen in order to resolve surface visibility while solving the required 
Boolean operations. This procedure determines the Boolean solution for a one-dimen- sional sample of 
the three-dimensional CSG model. A for- malized classification of one-dimensional Boolean solutions in 
a polygon based environment has been proposed by Tilove [15]. The major significance of the ray-firing 
tech- nique is that it does not require that a complete geometrical boundary surface solution of the 
CSG model be determined a priorL An iterative procedure for resolving the visible sur- face points from 
a CSG model using ray-firing might be structured as follows: For each scan-line For each pixel in the 
scan-line 1. For each surface Determine surface intersections with the visual ray 2. Sort surface 
intersections by Z-depth 3. Resolve the one-dimensional Boolean operations along the visual ray using 
the CSG tree  4. Determine the first visible point along the  ray from the Boolean solution Display 
point Figure 3 illustrates the simplicity of solving Boolean opera- tions in one dimension of the sampled 
space for two primi- tives. In addition to simplicity, this technique offers an at-tractive advantage 
for display --namely, the fact that the Boolean operations need only be solved as far as the first visible 
point. Unfortunately, that neat one-dimensional Boolean solution procedure must be precluded by expensive 
surface intersection and Z-sort computations. Furthermore, all four steps must be executed for every 
point on the screen. Roth does describe a culling procedure to somewhat reduce the number of surfaces 
to be Considered for each pic- ture element. The problem has changed little, however, be- cause the surface 
intersection, Z-sort, Boolean, and visibility computations are still executed in the inner-most program 
loop for every pixel. Roth also describes a sampling/search- ing technique which will locate and display 
silhouette and surface intersection edges without having to analyze each dis- play picture element. Unfortunately, 
that approach is not generalized for shaded surface display in that the higher level of sparseness of 
sampling that leads to speed improvement also leads to a higher potential for informational errors in 
the resulting image. f l Viewer~// Solid Primitive A Solid Primitive B Viewer i B i ,, , , a A 
Image CSG Model II I I t A U B ~J J t A N B a ~ I I II I I t B Figure 3. Ray-firing technique used 
to solve the one- dimensional Boolean problems for two solid  primitives 3. OVERVIEW OF POLYGON SCAN-LINE 
HIDDEN SURFACE ALGORITHMS A classification scheme for hidden surface removal algo- rithms was published 
in 1974 by Sutherland, Sproull and Schumacher [14]. Within this "taxonomy" of algorithms, Sutherland 
et al. classified three algorithms that sampled points in image space and resolved the visible surfaces 
based on depth priority. The three authors referenced by Suther- land et al. in this classification were 
Romney [11], Bouk-night [1], and Watkins [17]. All three methods followed a similar sequence of operations 
that reflect an execution or-dering of television-type raster devices. The four steps are described as 
follows: 1. Polygon edges are sorted by Y-vertex values typically using a bucket sort. For each scan-line, 
the Y-sorted list is utilized to determine which polygons should be considered for each scan-line, and 
where the associated scan-plane intersects each of those polygons. Suther-land et al. refer to the intersection 
of a scan-plane and a polygon as a segment. 2. For each scan-line that crosses one or more polygon surfaces 
in image space, an X-sorted list of related seg- ments is determined in order to establish a sequential 
set of spans. A span is typically a continuous portion of a scan-line from which the visibility of a 
single segmem can be resolved. Romney utilized a bucket sort to determine the X-ordering, whereas Bouknight 
and Wat- kins applied a bubble sort to update an X-sorted list be- tween scan-lines, in order to capitalize 
on scan-line coherence properties. 3. In order to restrict further analysis to within the bounds of 
a span, all segments that enter the span are clipped to the span limits. 4. In most cases, visibility 
is then determined simply by searching for the segment portion that is closest to the viewer. Some additional 
complexity is encountered in the event that two visible segment portions cross within a span [1].  A 
typical scan-line program that utilizes the Y-X-Z order of sorting described above would have an iterative 
structure like the following: 1. For each polygon edge Bucket sort by the Y-vertex values For each scan-line 
For each active polygon Determine segment (intersection of scan-plane and polygon) Sort segments by 
X-vertex values  2. Determine span boundaries For each span 3. Clip active segments to span boundaries 
 4. Resolve segment visibility within span by  searching for the closest segment in Z Display segment 
For each of the algorithms they studied, Sutherland et al. pointed out that the authors made crucial 
decisions in the selection of their sorting procedures and corresponding appli- cation of coherence properties. 
It is the judicious utilization of coherence that reduces the costly sorting computational requirements 
for these visible surface programs. The appli- cation of coherence becomes significantly more complex 
when a visible surface algorithm is extended to solve Boolean operations between solid primitives. Before 
the applications of coherence are dealt with in detail, it is appropriate to first describe a general 
scan-line hidden surface removal pro-cedure for constructive solid geometry. 4. A GENERAL CSG SCAN-LINE 
HIDDEN SURFACE REMOVAL PROCEDURE The hidden surface removal algorithm for CSG models that is to be presented 
utilizes a framework based on essen-tially the same four steps outlined by Sutherland et al. [14] for 
scan-line algorithms as described in the preceding section. The general CSG scan-line algorithm is achieved 
by integrat- ing the efficient one-dimensional Boolean solution technique of the ray-firing approach 
into step (4) of the scan-line itera- live structure. That is, for each span boundary, the one-dimensional 
Boolean problem is solved to the first visi- ble point. Where the same surface is visible throughout 
a span, shading can be interpolated in between. The functional steps for the general CSG hidden surface 
removal procedure are structured as follows: 1. For each polygon edge Bucket sort by the Y-vertex values 
For each scan-line For each active polygon Determine segment (intersection of scan-plane and polygon) 
Sort segments by X-vertex values 2. Determine span boundaries For each span  3. Clip active segments 
to span boundaries 4. Solve one-dimensional boolean operations to the first visible point as specified 
in the CSG tree  If same Z-order of segments at boundaries Then Display segment Else Subdivide span 
at segment intersections and repeat steps (3) and (4) for each new span Some complexity may be added 
where surface intersections take place between span boundaries. Note, however, that the image generation 
process need not have knowledge of those surface intersections that do not affect the resulting im- age. 
A change in surface visibility within a span is detected by a difference in the order of segments encountered 
between the viewer and the first visible point at each span boundary. This order is determined as part 
of the one-dimensional Boolean calculation for both left and right sides of the span boundary. When needed, 
segment intersection decisions are made using this depth order with the understanding that all segments 
that fall within a span will completely traverse that span. After the intersection between two selected 
segments is determined, the X-component of the intersection is used to create another span boundary which 
subdivides the span under consideration. The two new spans formed by this sub- division are processed 
separately in a recursive manner. It is significant to recognize that in most practical cases, polygonal 
intersections are relatively rare occurrences. This characteris- tic is a coherence property which can 
be exploited to improve program efficiency and will be discussed further in the next section of this 
paper. 76 A simple example of this general scan-line procedure for generating visible surface imagery 
of a CSG model containing two primitives is illustrated in Figure 4. The two solid primi- tives found 
within the CSG model are a six-sided box (primitive A) and a four-sided pyramid (primitive B). A scan-plane 
that intersects each primitive and a volume of space common to both is illustrated in Figure 4-A. The 
seven end points of the resulting segments are sorted by their X-components and subsequently used to 
define six spans (Figure 4-B). Up to this point, the operations described are common to the scan-line 
hidden surface re-moval algorithms described earlier. The ray-firing technique is applied to each span 
border in order to evaluate the one-dimensional Boolean problem up to the first visible point. Taken 
in Z-depth order, each seg- ment intersection along the ray is examined to see if a CSG tree solution 
exists. If so, the first visible segment point is 3aved and all surface intersections encountered up 
to that point are maintained in a sorted list. The Boolean process must be solved on both the left and 
right sides of the span boundaries because a change in segment geometry may occur there. However, not 
all of the span boundaries are necessary to complete the processing for a scan-line because some do not 
represent any changes in surface visibility. Span bounda- ries that can be deleted from further consideration 
in the processing of a scan-line without affecting the resulting image exhibit one of the two following 
properties: Primitive A Primitive B I  4-A. Scan-plane intersection with CSG model  4-C. 4-D 4-E 4-F. 
1. Boolean processing on each side of the span boundary resolves no visible points (as a result of subtraction 
or intersection operations). 2. Boolean processing resolves visible points from the same segment on 
both sides of the span boundary.  Figures 4-C through 4-F illustrate the four potential Boolean combinations 
of the two primitives. Pertinent span boundaries are labeled by the order of primitive surface in- tersections 
a visual ray would encounter on each side of the span boundary until the first visible point is found. 
Span boundaries that exhibit one of thc two properties described above have been removed. Span boundaries 
that would be inserted as a result of segment intersections are included. Also illustrated are the outlines 
of the resulting images of each Boolean operation with the scan-plane intersections highlighted. Imagery 
can be produced for each scan-line in two forms. A hidden-line removed image can be produced on a sampled 
pixel basis by rendering each scan-line point that represents a visible segment endpoint or visible segment 
intersection. These points are produced as a result of the span boundary generation and deletion processes 
already described. If color and intensity information is maintained at each of the final span boundaries, 
shading values can be in- terpolated between in order to produce a faceted or smoothly shaded image. 
Examples of imagery produced in this manner can be found in Section 6 of this paper. ~Z  4-B. Initial 
span boundaries AV B A~ B A -B  B -A Figure 4. The CSG scan-line hidden surface removal process for 
two solid primitives Computer Graphics Volume 17, Number 3 July 1983 5. APPLICATION OF COHERENCE TECHNIQUES 
 Several coherence techniques for scan-line hidden surface removal algorithms have been previously published. 
Although the terminology may vary, the goal common to all is to reduce computation, particularly within 
inner program loops. The major categories of coherence techniques are list- ed below..Each of these will 
be examined for applicability to the CSG scan-line visible surface procedure. A. Span Coherence: Only 
one surface is usually visible be- tween initial span boundaries. The exception is neces- sary for intersecting 
surfaces. B. Visible Segment Coherence: Where a segment is known to be visible across span boundaries, 
those boundaries need not be considered further [17]. C. Scan-line Coherence: There are relatively few 
changes between neighboring scan-lines. C.1 Intersections of adjacent scan-planes with a po-tygon model 
produce nearly the same set of seg- ments [1,4,11,14,17]. C.2 Sorted ordering of segments along the X-axis 
tends .to be very similar for neighboring scan-lines [1,4,13,17]. C.3 Sorted ordering of segments along 
the Z-axis tends to be very similar for neighboring scan-lines [4]. C.4 Segment intersections have nearly 
the same X- and Z-components for neighboring scan-lines. D. Face Coherence: The occurrence of intersecting 
sur-faces is assumed to be relatively rare [4]. E. Object Coherence: Within its image space boundaries, 
the visibility status of a solid primitive changes little. E.1 Polygon connectivity can be applied to 
scan-line segments where no intersections exist [4,13]. E.2 Where it is known that specific objects do 
not visibly intersect, silhouette boundaries of those objects can be used to determine visibility. F. 
Boolean Combination Coherence: Simplifications can be devised from particular sequences of Boolean opera-tions. 
G. Frame-to-frame coherence: Image composition changes little between frames that are adjacent in time 
[8]. The coherence properties described in items (A) through (C) are specific to scan-line based algorithms 
whereas the proper- ties represented by items (D) through (G) are more general in nature. Several of 
these techniques were utilized to some extent in the general CSG scan-line algorithm described in the 
last section. Span coherence (A) can be considered as the first improvement over ray-firing algorithms. 
That is, where there is no change in surface visibility throughout a span, Boolean visibility computations 
need be executed only at the span boundaries. Shading can then be interpolated in between. Even when 
surface intersections require subdivision of an initial span, st'ading can be interpolated between the 
new subdivision span boundaries. Visible segment coherence (B) was established by a property that allowed 
for span bounda- ries where no visible point is resolved to be removed from further consideration in 
the processing of a scan-line. Note, however, that for both (A) and (B) to be applied in the CSG scan-line 
procedure, the depth order of all segments encoun- tgred up to the first visible point are necessary. 
Previous scanline hidden-surface algorithms need only be concerned with information relative to the single 
segment closest to the viewer since it would hide the other more distant segments. Some applications 
of scan-line coherence property (C) can easily be added to the general CSG visible surface algorithm. 
Several people discovered that the straight polygon edges al- low for a simplification of the scan-plane/polygon 
intersection calculation on successive scan-lines (C.1). That is, when each edge of a polygon is first 
encountered in the Y-bucket sorted list, the values dx/dy and dz/dy are found. While an edge remains 
active, segment endpoints for successive scan-lines can be determined using simple difference equa- tions. 
Horizontal polygon edges are handled as a special case since they would result in a zero dy value. The 
X-sort scan-line coherence property (C.2) can be applied by main- taining the segment X-order information 
between successive scan-lines. Since the X-sort order generally changes very lit- tle between scan-lines, 
this coherence property can be uti-lized by applying a sorting algorithm that executes quickly on a list 
that is assumed to be nearly sorted. Several hidden surface program developers have found a bubble sort 
effective for maintaining the X-sort list from scan-line to scan-line [1,17]. Extension of other forms 
of scan-line coherence are not so easily applied to the CSG visible surface problem. The Z-sort coherence 
property (C.3) was first implemented by Hamlin and Gear [5] by combining the X- and Z-sort opera- tions 
into a single "stack" procedure. Their execution statis- tics exhibited marginal improvement for geometric 
models that did not allow intersecting polygons. Extensions of the "stack" algorithm to handle intersecting 
polygons would in- crease program complexity, and execution speed could be ad- versely affected. The 
application of a segment intersection scan-line coherence property (C.4) appears to exhibit a simi- lar 
efficiency weakness. For the application of constructive solid geometry, there is a potential for surface 
intersection at any screen location where primitives overlap. As such, the test for polygon intersection 
must be made at each span boundary where the visual ray penetrates more than one solid primitive regardless 
of the results from the previous scan-line. Previous scan-line hidden surface algorithms that utilized 
face coherence (D) did so by providing a preprocessing step that would split up all intersecting polygons 
along their com- mon borders. The major application for this approach would be for geometric models that 
were to be viewed from several positions, but whose internal components would not change relative to 
each other. In direct contradiction to that applica- tion, constructive solid modeling interaction demands 
rela-tive movements between primitives be made with the poten- tial of many surface intersections taking 
place. As Suther-land et al. pointed out [14], there is also a great probability of needlessly calculating 
intersections for portions of the model that would be hidden from view. At least one form of object coherence, 
polygon connec-tivity (E.1), can easily be applied to the CSG hidden surface removal algorithm. For regions 
of image space where only one primitive resides, no Boolean or intersection solutions need be produced 
(Figure 5). For segments with visible points resolved from a single primitive, only the first point on 
each span boundary needs to be found, as in normal visi- ble surface processing. Common span borders 
that resolve no visible points as a result of subtraction of intersection operations can be removed from 
further processing of a ? Figure 5. Object coherence properties can be used to define span boundaries 
where no Boolean operations need be resolved scan-line. Uses of polygon connectivity coherence assume 
the common notion that surfaces bounding a primitive volume do not intersect. There appears to be potential 
for a similar application of polygon connectivity coherence at each level of the Boolean construction 
tree. At this point in time, the author has not found an efficient solution that compen- sates for the 
greatly increased complexity of this task. There are several other potential areas of coherence appli- 
cation that could improve the efficiency of a CSG visible sur- face algorithm. Object coherence has often 
been applied to geometric models comprised of nonintersecting surfaces by determining visibility of primitives 
relative to each other us-ing their visual silhouette border (E.2). Some extensions might be applied 
as special cases to the CSG model environ- ment for nonintersecting primitives. An area not yet pur- 
sued is the application of rules characteristic to Boolean com- binations that might provide some coherence 
benefits (F). An example of this would be where a region of the image space is covered by overlapping 
primitives that take part in only union operations. When this situation is detected in the Boolean tree, 
the visible surface task need only search for portions of the model nearest the viewer. This process 
is analogous to the standard scan-line hidden surface removal procedure (like that described in Section 
3). Perhaps the coherence techniques involving the most complicated application to the CSG visible surface 
problem are those that utilize knowledge maintaine d between image frames (G). This is because the CSG 
interactive editing pro- cess involves surface intersections and geometry changes which are not encountered 
in traditional frame-to-frame coherence applications. Several people have suggested local updating to 
only portions of an image that correspond to geometry changes. This approach is reasonable as long as 
the viewing transformations are consistent. There remain several applications of coherence properties 
yet to be explored in the CSG visible surface task. The problems are significantly more complex than 
the traditional hidden surface removal problem; but the benefits to interac- tive solid modeling may 
result in" significant future payoffs for CAD/CAM applications. 6. IMPLEMENTATION AND COMPARISONS A 
Fortran implementation of the CSG scan-line hidden surface removal procedure described in Section 4 has 
been completed on a VAX 11/780 computer system. The CSG scan-line code is preceded by software that performs 
coordi- nate transformation, viewbox clipping, and perspective-divide operations. A transformation matrix 
can be allocated to each node of the CSG tree for relative scaling and placement of CSG model components. 
The images in Figures 6 and 7 were generated by the CSG scan-line hidden surface removal program at 512 
x 512 resolution. The images in Figure 6 were generated from simple CSG models, each containing the same 
three boxes. The top image in Figure 6 illustrates the relative placement of the three boxes before Boolean 
evaluation. The CSG model for the top image contained a single-node CSG tree for each primitive. The 
other two images represent CSG models with only one CSG tree apiece, utilizing all three primitives. 
New surfaces introduced by subtraction opera-tions are highlighted in blue for better visualization. 
The images in Figure 7 represent the two basic stages of a plastic coffee pot handle mold design, as 
produced using con- structive solid modeling. During the first stage of design, solid primitives are 
either loaded from a library or created using in-house CAD systems. It is significant to point out that 
the coffee pot handle primitive was modeled using bicu- bic surface patches which have been approximated 
by po-lygons. Figure 7 illustrates some basic operations which a designer could use to create the bottom 
half of the mold cav- ity. In the first operation, a truncated cone is subtracted from the handle in 
order to provide space for a screw that will attach the handle to the rest of the coffee pot. This final 
representation of the plastic coffee pot handle is then sub-tracted from a metal block to form the bottom 
half of the mold. In order to provide a basis of comparison in terms of pro- gram efficiency, a ray-firing 
CSG hidden-surface removal program has been implemented. The ray-firing program is a polygon-based implementation 
of the algorithm described in Section 2 of this paper, with extensions to use the culling-box technique 
suggested by Roth [12] in order to reduce computation. The CSG models depicted in Figures 6 and 7 were 
used as test cases and the results are summarized in Table 1. Table 1 EXECUTION TIMING TEST RESULTS 
Number of CPU seconds (Vax 11/780) Image Polygons Ray-Firing CSG Scan-fine Figure 6: [(AuC) --B]-. 18 
65.3 8.8 [(B- (AUC)] 18 64.6 9.1 Figure 7: [Handle -Cone] 1056 650.2 11.7 [Block --(Handle --Cone)] 1062 
1143.6 18.8  7. CONCLUSION A method for displaying visible surfaces of CSG models using a scan-line 
hidden surface removal procedure has been described. Implementations of this procedure demonstrate significant 
efficiency improvements over previous approaches. Initial tests indicate that these efficiency improvements 
in-crease with model complexity. These results indicate that a visual solid modeler based upon the CSG 
scan-line hidden surface removal procedure could be a truly interactive sys- tem. In addition to providing 
a vehicle for these dramatic speed improvements, the fundamental polygon basis of this procedure allows 
it to be a general purpose visualization tool for CSG model editing in that all higher order surfaces 
can be approximated to desired tolerances by polygons. There remain several coherence properties that 
could be further ex- ploited in order to improve execution speed of the CSG hid- den surface removal 
program even more. Further study is also necessary to understand what rendering techniques would provide 
the most information to the user at the least computational expense for the purpose of CSG model crea-tion 
and editing. A. dual solid modeling system which serves to separate the functions of a "visual solid 
modeler" and an "analytic solid modeler" has been proposed. The CSG hidden surface removal algorithm 
presented in this paper is aimed at solving the interactive image generation requirements of the "visual 
solid modeler." A great deal of work rema~'ns in precisely defining the "analytical solid modeler" and 
the interrelation- ships between the analytical and visual solid modelers at a system level.  8. ACKNOWLEDGEMENTS 
This paper incorporates a number of recommendations that were made by readers of earlier drafts. Of particular 
note were the comments made by Kevin Weiler who bathed my first draft with red ink. Recommendations by 
Lon Jones, Virg Lucke and three anonymous SIGGRAPH reviewers led to several clarifications and deletions 
of extraneous verbiage. The final proofreading, typesetting, photography and layout were accomplished 
by General Electric's Graphics Operation. The geometric models used for Figure 7 were provided by Jim 
Beck and John Hinds. I would also like to thank Professor Wozny of Rensselaer Polytechnic Institute who 
is leading me in my Ph.D. effort of which this paper will play a major part. 9. REFERENCES [1] Bouknight, 
W.J., "A Procedure for Generation of Three-Dimensional Half-Toned Computer Graphics Representations," 
Com~ ACM, Vol. 13, No. 9, Sep- tember 1970. [2] Boyse, J.W. and Gilchrist, J.E., "GMSolid: Interactive 
Modeling for Design and Analysis of Solids," IEEE Computer Graphics and Applications, Vol. 2, No. 2, 
March 1982, pp. 86-97. [3] Brown, C.M., "PADL-2: A Technical Summary," 1EEE Computer Graphics and Applications, 
Vol. 2, No. 2, March 1982, pp. 69-84. [4] Hamlin, G. and Gear, C.W., "Raster-Scan Hidden Surface Algorithm 
Techniques," Computer Graphics, Vol. 11, 1977, No. 2, pp. 206-213. [5] Foley, J.D. and VanDam, A., Fundamentals 
oflnterac- tt~,e Computer Graphics, Addison-Wesley, 1982. [6] Goldstein, R.A. and Malin, L., "3D Modeling 
with the Syntha Vision System," First Annual Conference on Computer: Graphics in CAD/CAM Systems, MIT, 
April 1979, pp. 244-247. [7] Hindel, B., Emulation and Invention, New York Univer- sity Press, 1981. 
[8] Hubschman, H., and Zucker, S., "Frame-to-Frame Coherence and the Hidden Surface Computation: Constraints 
for a Convex World," Computer Graphics, Vol. 15, No. 3, August 1981, pp. 45-54. [9] Myers, W., "An Industrial 
Perspective on Solid Modeling," IEEE Computer Graphics and Applications, Vol. 2, No. 2, March 1982, pp. 
86-97. [10] Requicha, A.A.G., and Voelcker, H.B., "Solid Model- ing: A Historical Summary and Contemporary 
Asess- merit," 1EEE Computer Graphics and Applications, Vol. 2, No. 2, March 1982, pp. 9-24. [11] Romney, 
G.W., Computer Assisted Assembly and Rendering of Solids, Computer Science Department, University of 
Utah, TR-4-20, 1970. [12] Roth, S.D., "Ray Casting for Modeling Solids," Com-puter Graphics and Image 
Processing, No. 18, 1982, pp. 109-144. [13] Sechrest, S., and Greenberg, D.P., "A Visible Polygon Reconstruction 
Algorithm," Computer Graphics, Vol. 15, No. 3, August 1981, pp. 17-27. [14] Sutherland, I.E., Sproull, 
R.F., and Schumacher, R.A., "A Characterization of Ten Hidden-Surface Algo-rithms," ACM Computing Surveys, 
Vol. 6, No. 1, March 1974, pp. 1-55. [15] Tilove, R.B., "Set Membership Clarification: A Unified Approach 
to Geometric Intersection Prob-lems," 1EEE Transactions on Computers, Vol. C-29, No.10, October 1980. 
[16] Voelcker, H.B., "Algorithms and Applications," Tu-torial on Solid Modeling, SIGGRAPH '82 (ACM). 
[17] Watkins, G.S., A Real-Time Visible Surface Algorithm, Computer Science Department, University of 
Utah, UTECH-CSC-70-101, June 1970. [18] Wesley, M.A., Lozano-Perez, T., Lieberman, L.I., La- vin, M.A. 
and Grossman, D.D., "A Geometric Modeling System for Automated Mechanical Assem- bly," IBM Journal of 
Research and Development, Vol. 24, No. 1, January 1980. [19] Wolfe, R., Fitzgerald, W., and Gracer, F., 
"Interactive Graphics for Volume Modeling," Procedings of the IEEE Eighteenth Design Automation Conference, 
pp. 463-470. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801136</article_id>
		<sort_key>83</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Ray tracing algebraic surfaces]]></title>
		<page_from>83</page_from>
		<page_to>90</page_to>
		<doi_number>10.1145/800059.801136</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801136</url>
		<abstract>
			<par><![CDATA[<p>Many interesting surfaces can be written as polynomial functions of the spatial coordinates, often of low degree. We present a method based on a ray casting algorithm, extended to work in more than three dimensions, to produce pictures of these surfaces. The method uses a symbolic algebra system to automatically derive the equation of intersection between the ray and the surface and then solves this equation using an exact polynomial root finding algorithm.</p> <p>Included are illustrations of the cusp catastrophe surface, and two unusually shaped quartic surfaces, Kummer's quadruple and Steiner's surface.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Algebraic surfaces]]></kw>
			<kw><![CDATA[Curved surface display]]></kw>
			<kw><![CDATA[Polynomial roots]]></kw>
			<kw><![CDATA[Ray tracing]]></kw>
			<kw><![CDATA[Symbolic algebra]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.5</cat_node>
				<descriptor>Polynomials, methods for</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003739</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Nonlinear equations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003720</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on polynomials</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>PP15033698</person_id>
				<author_profile_id><![CDATA[81100482576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanrahan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Laboratory, New York Institute of Technology, Old Westbury, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1089271</ref_obj_id>
				<ref_obj_pid>1089270</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Arnon, D. S., Automatic analysis of real algebraic curves. SIGSAM 15(4) pp 3-9, 1981.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Barr, A. H., Super quadrics and the angle preserving transformation. IEEE Transactions on Computer Graphics and Applications. 1(1) pp 11-23, 1981.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563893</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Blinn, J. F., Models of light reflection for computer synthesized pictures. SIGGRAPH 77 Proceedings, Computer Graphics 11(2) pp 192-198, 1977.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Blinn, J.F., Notes on geometric modelling. SIGGRAPH 1980 Tutorial.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357310</ref_obj_id>
				<ref_obj_pid>357306</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Blinn, J. F., A generalization of algebraic surface drawing. ACM Transactions on Graphics 1(3) pp 235-236, 1982.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358815</ref_obj_id>
				<ref_obj_pid>358808</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Blinn, J.F., Carpenter, L.C., Lane, J.M., and Whitted, T., Scanline methods for displaying parametrically defined surfaces. CACM, 23(1) pp 23-34, 1980.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Catmull, E., Computer Display of Curved Surfaces., Proc. IEEE Conference on Computer Graphics, Pattern Recognition, and Data Structures, pp 11-17, 1975.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806346</ref_obj_id>
				<ref_obj_pid>800205</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Collins, G. E. and Akritas, A. G., Polynomial real root isolation using Descartes' rule of signs. Proc. 1976 ACM Symposium on Symbolic and Algebraic Computation. pp 272-276, 1976.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Collins, G. E. and Loos, R., Real zeros of polynomials. Computing, Suppl. 4 pp 83-94, 1982.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806819</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Cook, R. L., and Torrance, K. E., A reflectance model for computer graphics. SIGGRAPH 81 Proceedings, Computer Graphics 15(3) pp 307-316, 1981.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Duff, T., The soid and roid manual. New York Institute of Technology. 1980.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Goldstein, E. and Nagle, R., 3D visual simulation. Simulation 16(1) pp 25-31, 1971.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Hudson, R.W.H.T., Kummer's Quartic Surface. Cambridge University Press, 1905.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801287</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Kajiya, J. T., Ray tracing parametric patches SIGGRAPH 82 Proceedings, Computer Graphics 16(3) pp 245-254, 1982.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806818</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Potmesil, M. and Chakravarty, I., A lens and aperture camera model for synthetic image generation. SIGGRAPH 81 Proceedings, Computer Graphics 15(3) pp 297-306, 1981.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Uspensky, J.V., Theory of Equations McGraw-Hill, 1948.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Whitted, T. J., An improved illumination model for shaded display. CACM 23(6) pp 343-349, 1980.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 17, Number 3 July 1983 Ray Tracing Algebraic Surfaces Pat Hanrahan Computer 
Graphics Laboratory New York Institute of Technology Old Westbury, New York Abstract  Many interesting 
surfaces can be written as polynomial functions of the spatial coordinates, often of low degree. We present 
a method based on a ray casting algorithm, extended to work in more than three dimensions, to produce 
pictures of these surfaces. The method uses a symbolic algebra system to automatically derive the equation 
of intersection between the ray and the surface and then solves this equa- tion using an exact polynomial 
root find- ing algorithm. Included are illustrations of the cusp catastrophe surface, and two unusu- 
ally shaped quartic surfaces, Kummer's quadruple and Steiner's surface. General Terms: Alqorithms. 
Ke~words: Algebraic Surfaces, Curved Sur- face Display, Ray Tracing, Symbolic Alge- bra, Polynomial Roots. 
 CR Categories: 1.3.3 [Computer Graphics]: Picture/Image Generation--display algo- rithms; 1.3.5 [Computer 
Graphics]: Compu- tational Geometry and Object Modelling-- curve, surface, solid and object represen- 
tations, geometric algorithms, languages and systems; 1.3.5 [Computer Graphics]: Three-Dimensional Graphics 
and Realism-- visible line/surface algorithms Permission to copy without fee all or part of this material 
is granted provided that the copies are not made or distributed for direct commercial advantage, the 
ACM copyright notice and the title of the publication and its date appear, and notice is given that copying 
is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. &#38;#169; ACM 0-89791-109-1/83/007/0083 $00.75 i. Introduction Underlying 
the most spectacular com- puter generated images is a precise geometric model of the scene. A very pro- 
fitable line of recent research has been the development of display algorithms for a larger range of 
mathematical models. New display algorithms have been imple- mented that are capable of displaying curved 
surfaces, some for parametric sur- face patches, most notably the bicubic patch [Catmull, 1975; Blinn, 
Carpenter, Lane and Whitted, 1980] and implicitly defined surfaces, for example quadrics [Duff, 1980; 
Blinn, 1980], superquadrics [Barr, 1981] and exponential density dis- tributions [Blinn, 1982]. The differences 
in these algorithms are primarily geometric; that is, for each type of sur- face equation different methods 
may be needed to transform the object into the viewing coordinate system, and to deter- mine the relative 
priority between parts of an object or objects so that only the visible surface element will be drawn. 
Once a visible surface patch has been found it is rendered into a frame buffer. This can be done quite 
realistically using the normal of the surface element and a local lighting model [Blinn, 1977; Cook and 
Torrance, 1981]. Ray tracing [Goldstein and Nagle, 1971] is a very powerful general approach to rendering 
surfaces since it reduces the geometric calculations to that of solving for the intersection of a line 
with the surface. This has been done for the case of planar and quadric surfaces [Goldstein and Nagle, 
1971; Potmesil and Chakravarty, 1981; Whitted, 1980] and more recently bicubic surface patches [Whitted, 
1980; Potmesil and Chakravarty, 1981; Kajiya, 1982]. Generally there is no need to transform the surface 
since the view is encoded in the geometry of the light rays. This also has the advantage that rays can 
be recursively split [Whirred, 1980], to form new rays representing specular reflections and transmissions, 
without changing the nature of the calculations.  2. Ray tracing implicit surfaces Although we are 
concerned primarily with algebraic surfaces embedded in n- dimensional space, in this section we will 
outline the steps needed to display any analytic surface by means of a ray trace. Each point in this 
space is represented by a vector, the n-tuple (x, y ..... z) (i) and the surface by the n-I dimensional 
 subset satisfying the equation: F(x,y ..... z) = 0 (2) The normal to the surface is in the direc- 
tion of the gradient of F evaluated at a point on the surface, = (~F, ~F ..... ~F) I (3) The _major 
geometric calculation underlying all ray tracing algorithms is the intersection of a line, which represents 
a particular light beam, with a surface. The line equation can be written in the following parametric 
form: x = x t+x 1 0 y = y t+y 1 0 (4) z = z t+z 1 0 If the parameter t spans the real numbers then 
the line extends infinitely in both directions; if instead we restrict t to take on only positive values 
then we are left with a ray anchored at the point, (x , y ..... z ), and pointing in the 0 0 0 direction 
(x , y .... , z ), For each ray 1 1 1 of light we must find which surface it intersects, if any, and 
for all the possible intersections (there may be more than one) the closest intersection along the path 
of the ray. The intersections can be found by substituting for the x,y,...,z coordinates of the surface 
equa- tion the corresponding ray equations, (4), to yield the new equation in the single variable t, 
 F*(t) = 0 (5) The real zeros, t , t , ..., t , of this  0 1 k equation are simultaneous solutions 
to both the ray and surface equations and therefore intersections. If there are no real solutions then 
the ray does not intersect the surface; if there exists a solution t<0 then that intersection is behind 
the anchor point of the ray and therefore physically does not impede the path of the ray. The first physical 
intersection is the closest point to the anchor and corresponds to the lowest posi- tive real solution 
to the above equation. Given the solution we can determine the coordinates of the point of the intersec- 
tion by the original ray-equation (4). These in turn can then be substituted into equation (3) to find 
the surface normal. Depending on the illumination model, we can either split the incominq ray into additional 
rays and recursively trace them or immediately compute an intensity value using a local shadinq function. 
 To generate a complete frame buffer image a ray is cast at each picture ele- ment, sampling the surface. 
To do this we must determine the location of the image plane in space and the type of projection desired. 
The image plane can be parameterized by: Y = [ R+uU+vV : (u,v) e[-l,l] ] (6)  where the vector R = 
(x ,y ..... z ) r r r is a point on the plane, and the two vec- tors, U = (x ,y ..... z ) u u u and 
 v = (x ,y ..... z ), v v v are two orthogonal vectors contained in that plane. The parameters u and 
v are normalized frame buffer coordinates. The surface is projected onto this plane by passing a ray 
through that image point out into the object space. There are two com- mon methods of doing this (i) 
by forcing all rays to pass through a common vantage ~oint, or (ii) forcing all rays to travel in the 
same direction. The first method is called a perspective projection; the second method a parallel projection. 
 The method outlined above works for arbitrary functions provided we can (i) convert the function F to 
F*, and (ii) solve for the first positive real zero of  F*. The remainder of this paper will describe 
a system that performs both these operations for surfaces defined as polyno- mial functions of the space 
coordinates. ~. Ray tracing algebraic surfaces An algebraic surface are those sur- faces that can be 
written as a finite number of terms involving coordinates raised to a positive integral power. With this 
definition a surface defined using logarithms, exponentials or transcendental functions is not considered 
an algebraic surface. The general form is there- fore:Ill 1 m n i j k Pcx y ..... aijkx y z C7)  
The degree of the surface is equal to the maximum combined degree of the coordi- nates, d=l+m+n. Upon 
substitution of the ray-equation into this equation a univariate polynomial in t is formed: P*(t) = 
~ a.t i (8) i=O i  The line intersection with an algebraic surface has several properties not shared 
by the general analytic surface. First, notice that the maximum number of real roots is equal to the 
degree of the origi- nal polynomial. And, if all the coeffi- cients of the univariate polynomial are 
zero then the equation is always satis- fied, implying that the line lies on the surface. The display 
system we have developed consists of two major parts, a surface compiler and the ray caster/intersector. 
The surface compiler reads the equation of the surface and generates the coefficients of the univariate 
polynomial P*. The same program calculates expressions for the normal to the surface. The output of the 
surface compiler is included in another program which casts the rays to generate the picture. The a~pects 
of hese pro- grams unique to algebraic surfaces are (i) the symbolic processing in the surface compiler 
and (ii) the line intersection calculation in the ray casting program. These are described in the next 
sections. If]The symbol F is used to signify an analytic surface and the symbol P to denote a polynomial 
or algebraic surface. ~.!. Symbolic Methods The surface compiler is a lisp pro- gram which accepts 
as input the equation of the surface, substitutes the ray equa- tions into this equation to form expres- 
sions for the coefficients of the polyno- mial P* and outputs them as a series of statements in the programming 
language C. The symbolic algebra needed to manipulate polynomials in these ways is well under- stood, 
therefore we will only outline the methods used. When the equation is read it is con- verted to the 
equivalent lisp expression (i.e., from infix to prefix). All the arithmetic operators from the original 
surface equation are replaced by operators which perform polynomial arithmetic and all the spatial coordinates 
are replaced with expressions from the right hand side of the ray equation (4) for that variable; these 
equations are linear polynomials in t. The resulting expression for the sur- face is evaluated by symbolically 
combin- ing the polynomials in t. At each step the coefficients of the new polynomials are passed to 
an algebraic simplifier. When all the original arithmetic operators have been evaluated there remains 
a single univariate polynomial in t, the desired P*. Notice that the coefficients are represented by 
expressions containing the variables of the ray, the x and x etc., 0 1 and any other constants which 
were part of the original surface equation. The eventual evaluation of the coef- ficients can be further 
optimized while their equations are still in symbolic form. In particular, the resulting state- ments 
often contain common subexpressions which are repeated several times. The total number of arithmetic 
operations can be substantially reduced by evaluating these subexpressions only once. The optimizer accepts 
a group of statements, and from these creates a list of innermost expressions (that is, terminal nodes 
in the expression tree) which occur more than once. For each repeated subexpression a new statement is 
created which assigns that subexpression to a temporary variable and substitutes the temporary variable 
for the subexpression in the original set of statements. The entire process is then repeated until no 
common subexpressions have been found. Finally from the original surface equation it is possible to 
symbolically differentiate the equation with respect to each spatial coordinate to form the normal to 
the surface. Figure 4 contains the output of the surface compiler for the coefficients of Roman surface 
shown in Figure i. The same figure also shows the final code generated  {  a[4J=y0*y0*z0*z0+z0*z0*x0*x0+x0*x0*y0*y0+x0*y0*z0; 
a[3]=((((yl*y0+y0*yl)*z0*z0)+(y0*y0*(zl*z0+z0*zl)))+((((zl*z0+z0*zl)*x0*x0)+ (z *z *(x *x +x *x )))+((((x 
*x +x *x )*y *y )+(x *x *(y *y +y *y )))+(((x *y +x *y )*z ) +(x0*y0*zl)))))); a[2]=((yl*yl*z0*z0+(((yl*y0+y0*yl)*(zl*z0+z0*zl))+y0*y0*zl*zl))+((zl*zl*x0*x0+ 
 (((z *z +z *z )*(x *x +x *x ))+z *z *x *x ))+((x *x *y *y +(((x *x +x *x )*(y *y +y *y )) +(x0*x0*yl*yl)))+((xl*yl*z0)+((xl*y0+x0*yl)*zl))))); 
a[l]=(((yl*yl*(zl*z0+z0*zl))+((yl*y0+y0*yl)*zl*zl))+(((zl*zl*(xl*x0+x0*xl))+ ((z *z +z *z )*x *x ))+(((x 
*x *(y *y +y *y ))+((x *x +x *x )*y *y ))+(x *y *z )))); a[0]=yl*yl*zl*zl+zl*zl*xl*xl+xl*xl*yl*yl; d=4; 
 } {  double g26 g25'g24 g23'g22 g2 g2 g 9 g 8 g 7 g 6'g 5'g 4 g 3 g 2 g 'g 'g 9 g 8 g 7 g07=y0*y0; 
g08=z0*z0; g09=x0*x0; gl0=x0*y0; gll=yl*y0; gl2=y0*yl; gl3=zl*z0t gl4=z0*zl: gl5=xl*x0t gl6=x0*xl; gl7=xl*y0; 
gl8=x0*yl; gl9=yl*yl; g20=zl*zlt g21=xl*xl; g22=xl*yl; g23=gll+g12; g24=g13+g14; g25=g15+g16; g26=g17+g18; 
 d=4; a[0]=gl9*g20+g20*g21+g21*gl9; a[l]=g19*g24+g23*g20+g20*g25+g24*g21+g21*g23+g25*g19+g22*zl; a[2]=g19*g08+g23*g24+g07*g20+g20*g09+g24*g25+g08*g21+g21*g07+ 
 g25*g23+g09*g19+g22*z0+q26*zl; a[3]=g23*g08+g07*g24+g24*g09+g08*g25+g25*g07+g09*g23+g26*z0+g10*zl; a[4]=g07*g08+g08*g09+g09*g07+gl0*z0; 
 }  Figure 4. The output of the surface compiler for the Steiner surface shown in Figure i. after subexpression 
optimization which in this case reduces the total number of arithmetic operations required by almost 
50 percent.  3.2. Numerical Methods Once the coefficients are determined several methods are used to 
solve for the real roots. We have opted for root finders which uses the properties of polynomials to 
increase the accuracy of the roots. Many of the equations of interest have degree less than 5. For these 
equa- tions there exist analytical solutions requiring the radical of a real number. Linear equations 
can be trivially solved; quadratic equations can be solved by the quadratic formula, requiring a square 
root. A method for solving cubics was discovered by Cardan and a method for solving quartics was discovered 
by his pupil Ferrari.[2] In these cases we gen- erate all the roots, ignore the imaginary roots and negative 
real roots, and then  [2]Other exact methods exist up to higher orders but involve complicated functions. 
It is well known that equations above degree five cannot be solved using only radicals; in fact equations 
of degree 3 also cannot be solved without the use of trigonometric substitution--this is the famous causus 
irreducibilis. finally sort the remaining real roots to find the lowest positive root. We have found 
that these methods are simple, fast and robust. For polynomials whose degree is greater or equal to 
5 we use a hybrid method which is guaranteed to give the exact answer to within some tolerance. It was 
originally described by Uspensky[19483 and then improved and modified by Collins for use in the SAC-2 
computer algebra sys- tem [Collins and Akritas, 1976; see also Collins and Loos, 1982]. This algorithm 
allows one to separate the real roots within the interval, 0 to i, into a set of disjoint intervals each 
containing a sin- gle root. The number of roots within an interval can be calculated using a theorem 
due to Descartes that states that the number of sign variations, that is, tran- sitions in the coefficients 
from negative to positive or vice versa, is equal to the number of positive real roots or less than that 
by an even number. A corollary is, if the number of sign variations is 1 then there exists a single 
positive real root, and if the number of sign variations is then there cannot be any positive real roots. 
To take advantage of this theorem the interval, 0 to i, must be transformed to the positive real axis. 
This is done with the substitution, p*' = (t+l)np*(I/(t+l))  (which amounts to simply reversing the 
order of the coefficients and then expand- ing the polynomial in powers of t+l). If there is more than 
one sign variation we can bisect the standard interval, transform the two subintervals to the standard 
interval and repeat the above test. An elegant theorem due to Collins guarantees that this method isolates 
all the roots. A further remarkable fact is that if the coefficients are integers then all the operations 
described above can be done using only shifts and adds. Since we need only the lowest posi- tive real 
root we can always search the leftmost interval first, switching to the right interval only if no roots 
are con- tained in the first. This allows us to more quickly home in on the isolating interval containing 
the lowest positive root. Also once an interval containing a single root has been found other iterative 
root finding methods can be employed pro- vided they guarantee that the root does not wander.outside 
the interval. Experi- mentally, we have found that the method of re@ula falsi more rapidly converges 
to the root than either simple bisection or con- tinual application of the Collins-Uspensky algorithm. 
 4. Results To test the program we used Steiner's surface 22 22 22 x y +y z +z x +xyz = 0,  the quadruple 
Kummer's surface (x 4 4 4 2 2 2 2 2 2 2 2y2) +y +z +l)-(x +y +z +y z +z x +x = 0,  and the cubic cusp 
catastrophe 3 z +xz+v = 0.  These surfaces were generated at an effective resolution of 1024 x 972 
on a Digital Equipment Corporation VAX 11/780. Since they have degree less than 5 the ray intersection 
can be calculated with either the classic method or the Collins-Uspensky algorithm. On the average a 
full screen image of the quartics took 6 minutes of CPU time with the exact solution and approximately 
25 percent longer with the more general polynomial root finder. 5. Summary and Discussion To reiterate, 
we have shown how to automatically perform the line-surface intersection for a general algebraic sur- 
face resulting in a univariate polynomial, P*, and how tO solve for the first posi- tive real root of 
this polynomial to within some specified tolerance. This root is the intersection of a ray with the sur- 
face and represents the visible surface element. The above methodology is very general placing no restrictions 
on the number of dimensions or the degree of the surface. The symbolic surface compiler has been very 
valuable, especially since we are interested in displaying a variety of surfaces defined by quite different 
equa- tions. To generate a picture of a new surface the user need only provide the equation of the surface 
and the view desired; there is no need to know any of the details of the ray tracing algorithm itself. 
Within a few minutes of entering the equation of a surface a custom program is created which is capable 
of displaying the surface. The surface compiler automatically performs the tedious and error prone task 
of generating the coeffi- cients of the univariate polynomial and the normal to the surface. Since the 
compilation is done once per surface, it is worth the extra time to simplify and optimize the evaluation 
of the various expressions generated. Execu- tion time profiles show that over 90 per- cent of the time 
is spent in solving for roots of the polynomial P*, the substitu- tion time being almost insignificant 
despite the complexity of the expressions. We strongly believe that the symbolic preprocessing significantly 
speeds the execution time of the resulting program but since the programming required to per- form the 
ray substitution at every pixel has approximately the same complexity as that involved in the compiler 
we decided to only implement it as a preprocessor. The importance of the polynomial root finder is very 
graphically illustrated by the catastrophe surfaces which contain sharp singularities susceptible to 
numeri- cal imprecision. The polynomial root finder we have used was originally imple- mented as part 
of the SAC-2 computer alge- bra system and is exact, that is, if the coefficients of the polynomials 
are represented as multiple precision integers the calculated roots are guaranteed to lie within a prespecified 
tolerance of the true roots. Dennis Arnon has used this system to display algebraic curves [Arnon, 1981]. 
Unfortunately, generating a shaded image with multiple precision integers would be quite slow so we opted 
for ordi- nary floating point arithmetic. This introduces some error into the calculation but does not 
seem to effect the pictures we have generated. However, a systematic method of performing the computations 
with integers of fixed size would be very desirable. When tracing a ray of light only the lowest positive 
solution need be calcu- lated. An algorithm employing bisection, such as the Collins-Uspensky algorithm, 
can use this fact to reduce the number of roots calculated and save a significant amount of time. Of 
considerable interest is the development of algorithms for other classes of analytical surfaces. As out- 
lined above with a ray tracing approach the problem reduces to essentially that of finding positive real 
roots of the func- tion F*. General purpose zero finding pro- cedures either are too computationally 
expensive and/or lack the accuracy required. Unlike algebraic functions which have exact solutions other 
classes of functions require heuristics based on the qualitative properties of the function. For example, 
properly selected starting points followed by the application of an iterative root finding algorithm 
will often converge to the true root. Jim Blinn's blobby atoms display algorithm is excellent example 
of careful application of this paradigm. One of the major advantages of ray tracing algorithms is that 
they provide a uniform geometric framework, the line- surface intersection, for displaying a variety 
of different surface types. Perhaps only Z-buffer algorithms are as flexible. Unfortunately this increased 
generality increases the computation time slgnificantly. Therefore, it may be time to seriously consider 
implementing some part of the algorithms in hardware. One approach is to exploit the independence of 
the calculations from pixel to pixel and perform these calculations in parallel, perhaps on a multiprocessor 
architecture. But since the majority of the execution time is spent searching for line-surface intersections 
it might be more advanta- geous to employ special hardware to solve for these intersections. In this 
paper we have shown how this problem can be solved for algebraic surfaces. Jim Kajiya [Kajiya, 1982] 
has also very elegantly shown how the intersections of a piecewise polynomial surfaces with a line can 
also reduced to solving a univariate polynomial equation. A large class of surfaces could therefore be 
displayed if a polynomial root finder was embedded in hardware. The Collins-Uspensky algorithm would 
be a leading candidate for such a machine since it involves very simple operations on the coefficients 
of the polynomial.  6. Acknowledgements I would like to thank Kevin Hunter for sparking my interest 
in display algo- rithms for implicitly defined surfaces. also am greatly indebted to George Col- lins, 
for graciously providing me with his polynomial root finding algorithm, and Scott McCallum for helping 
me through the details. Michael Lehman provided invalu- able photographic assistance. 7. References 
 Arnon, D. S., Automatic analysis of real algebraic curves. SIGSAM 15(4) pp 3-9, 1981. Barr, A. H., 
Super quadrics and the angle preserving transformation. IEEE Transac- tions o_~n Computer Graphics and 
Applica- tions. I(I) pp 11-23, 1981. Blinn, J. F., Models of light reflection for computer synthesized 
pictures. SIG- GRAPH 77 Proceedings, Computer Graphics n-Y~7~ 192-198, 1977. Blinn, J.F., Notes on geometric 
modelling. SIGGRAPH 1980 Tutorial. Blinn, J. F., A generalization of alge- braic surface drawing. ACM 
Transactions on Graphics 1(3) pp 235-236, 1982. Blinn, J.F., Carpenter, L.C., Lane, J.M., and Whitted, 
T., Scanline methods for displaying parametrically defined sur- faces. CACM, 23(1) pp 23-34, 1980. Catmull, 
E., Computer Display of Curved Surfaces., Proc. IEEE Conference on Com- ~uter Graphics, Pattern Recognition, 
and Data Structures, pp 11-17, 1975. Collins, G. E. and Akritas, A. G., Polyno- mial real root isolation 
using Descartes' rule of signs. Proc. 1976 ACM Symposium o_nn S~mbolic and Al~ebraic Computation. pp 
272-276, 1976. Collins, G. E. and Loos, R., Real zeros of polynomials. Computing, Suppl. 4 pp 83- 94, 
1982. Cook, R. L., and Torrance, K. E., A reflectance model for computer graphics. SIGGRAPH 81 Proceedings, 
Computer Graphics 15(3) pp 3-07-316, 1981. Duff, T., The soid and roid manual. New York Institute of 
Technology. 1980. Goldstein, E. and Nagle, R., 3D visual simulation. Simulation 16(1) pp 25-31, 1971. 
 Hudson, R.W.H.T., Kummer's Quartic Sur- face. Cambridge University Press, 1905. Kajiya, J. T., Ray 
tracing parametric patches SIGGRAPH 82 Proceedings, Computer 16(3) pp 245-254, 1982. Potmesil, M. and 
Chakravarty, I., A lens and aperture camera model for synthetic image generation. SIGGRAPH 81 Proceed- 
ings, Computer Graphics 15(3) pp 297-306, 1981. Uspensky, J.V., Theory of Equations McGraw-Hill, 1948. 
 Whitted, T. J., An improved illumination model for shaded display. CACM 23(6) pp 343-349, 1980.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801137</article_id>
		<sort_key>91</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[New techniques for ray tracing procedurally defined objects]]></title>
		<page_from>91</page_from>
		<page_to>102</page_to>
		<doi_number>10.1145/800059.801137</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801137</url>
		<abstract>
			<par><![CDATA[<p>We present new algorithms for efficient ray tracing of three procedurally defined objects: fractal surfaces, prisms, and surfaces of revolution. The fractal surface algorithm performs recursive subdivision adaptively. Subsurfaces which cannot intersect a given ray are culled from further consideration. The prism algorithm transforms the three dimensional ray-surface intersection problem into a two dimensional ray-curve intersection problem, which is solved by the method of strip trees. The surface of revolution algorithm transforms the three dimensional ray-surface intersection problem into a two dimensional curve-curve intersection problem, which again is solved by strip trees.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Fractal surfaces]]></kw>
			<kw><![CDATA[Procedural modelling]]></kw>
			<kw><![CDATA[Raster graphics]]></kw>
			<kw><![CDATA[Ray tracing]]></kw>
			<kw><![CDATA[Stochastic models]]></kw>
			<kw><![CDATA[Strip trees]]></kw>
			<kw><![CDATA[Surfaces of revolution]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Fractals</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P132648</person_id>
				<author_profile_id><![CDATA[81100653012]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Kajiya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California Institute of Technology, Pasadena, Ca.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[APPEL, A. Some Techniques for Shading Machine Renderings of Solids. SJCC (1968) 37-45]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358661</ref_obj_id>
				<ref_obj_pid>358645</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BALLARD, D. H Strip trees: a hierarchical representation for curves. Comm. ACM, 24 (May 1981) 310-321.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>507101</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BLINN, J.F. Simulation of wrinkled surfaces. Computer Graphics 12 (August 1978) 286-292.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BRAID, I.C. Designing with Volumes, Ph.D. Dissertation. Univ. Cambridge, England (1973).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807478</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CARPENTER, L.C. Computer rendering of fractal curves and surfaces. SIGGRAPH80 Conference Proceedings Supplement (August 1980).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[CHOW Y.S., TEICHER, H. Probability Theory: Independence, Interchangeability, Martingales. Springer Verlag, Heidelberg (1978).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[DUDA, R.O. AND HART, P.E. Pattern Classification and Scene Analysis. Wiley-Interscience, New York (1973).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807477</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[FOURNIER, A., FUSSELL, D. Stochastic modelling in computer graphics. SIGGRAPH80 Conference Proceedings Supplement (August 1980).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358553</ref_obj_id>
				<ref_obj_pid>358523</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[FOURNIER, A FUSSELL, D. AND CARPENTER, L. Computer rendering of stochastic models. Comm. ACM, 25 (June 1982) 371-384.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[GOLDSTEIN, E. AND NAGLE, R. 3D visual simulation Simulation 16 (Jan 1971) 25-31.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[MANDELBROT, B. Fractals: Form, Chance, and Dimension. W.H. Freeman, San Francisco(1977).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[MANDELBROT, B. Fractional brownian motions, fractional noises and applications. SIAM Review 10, 4 (October 1968) 422-437.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356833</ref_obj_id>
				<ref_obj_pid>356827</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[REQUICHA, A.A.G. Representations for rigid solids: theory, methods, and systems. ACM Computing Surveys 12 (December 1980) 437-464.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[ROTH, S.D Ray casting for modeling solids. Computer Graphics and Image Processing 18 (1982) 109-144.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807479</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[RUBIN, S. AND WHITTED, T. A three-dimensional representation for fast rendering of complex scenes. Computer Graphics 14 (1980) 110-116.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[WHITTED, T. An improved illumination model for shaded display. Comm. ACM, 23 (June 1980) 343-349.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[WHITTED, T. private communication (1983).]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 NEW TECHNIQUES FOR RAY TRACING PROCEDURALLY DEFINED OBJECTS James T. Kajiya California Institute of 
Technology Pasadena, Ca. 91125 ABSTRACT. We present new algorithms for efficient ray tracing of three 
procedurally defined objects: fractal sur-faces, prisms, and surfaces of revolution. The fractal sur- 
face algorithm performs recursive subdivision adaptively. Subsurfaces which cannot intersect a given 
ray are culled from further consideration. The prism algorithm trans- forms the three dimensional ray-surface 
intersection prob- lem into a two dimensional ray-curve intersection problem, which is solved by the 
method of strip trees. The surface of revolution algorithm transforms the three dimensional ray- surface 
intersection problem into a two dimensional curve- curve intersection problem, which again is solved 
by strip trees. KEYWORDS: computer graphics, raster graphics, ray tracing, fractal surfaces, procedural 
modelling, strip trees, stochastic models, surfaces of revolution. CR CATEGORIES: 1.3.3, 1.3.5, 1.3.7 
1 Introduction Of all synthetic images, those rendered by ray tracing stand above the rest in realism 
(Appe][1], Goldstein and Nagle[10], Whitted[16]). Many have disparaged its use because of its large appetite 
for floating point computation. However --even though ray tracing is conceded to be the slowest of all 
methods for render- ing computer imagery --no other technique has a performance envelope quite as large. 
In ray trac-ing the combined effects of hidden surfaces, shadows, Permission to copy without fee all 
or part of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. &#38;#169; ACM 0-89791-109-1/83/007/0091 
$00.75 reflection, and refraction are handled with a simplicity and elegance unmatched by its competitors. 
This paper is about novel ways of performing the key computational step.in rendering procedural objects 
via ray tracing. We present new ways of computing the intersection between a ray and certain procedurally 
defined objects. The use of procedural objects is not new to ray tracing --Whitted[16] and Rubin and 
Whitted[15] have advocated their use. Indeed, one could make the claim that the natural organization 
for ray tracing programs is one using procedural objects. In section two, we present a method which efficiently 
computes the intersection of a ray with a fractal sur-face. In the course of the development of this 
algo-rithm a number of results useful to other rendering techniques of fractals. We also analyze the 
complexity of the new technqiue. In section three, we give an algorithm for intersecting a ray with a 
surface defined by translating a plane curve orthogonally. We call a surface of this type a prism. In 
section four, we treat surfaces of revolution. The algorithm works well even though the radius curve 
may be defined by thousands of points. Algorithms for efficient rendering the above two ob- jects are 
of immediate importance to CAD/CAM applications. Combined with well known methods for ray tracing Euler 
combinations of primitives (Goldstine and Nagel[10], Roth[14]), the above tech- niques facilitate rendering 
of models described by con- structive solid geometry (CSG). Indeed, most of the objects in CSG are combinations 
of prisms and surfaces of revolution (Braid[4], Requicha[13]), the so called swept volumes. In section 
five we present the results of our algorithms; and in section six, we discuss directions for further 
work. It is likely that the locus of ideas exposed here will be 91 applicable to other types of objects 
exhibiting either a high degree of symmetry or a hierarchical organization or both. These ideas suggest 
a rich panoply of tech- niques applicable to many procedural objects which has heretofore gone unsuspected. 
2 Fractal Surfaces The so-called method of fractals (Fournier, Fussell and Carpenter[0], Mandelbrot[ll]) 
has generated models of startling realism. One would conjecture that applying a ray tracing rendering 
algorithm to objects modelled by this technique would yield very interesting images indeed. Unfortuately, 
practical considerations prevent one from doing this directly. Though a simple algorithm, the fractal 
modelling tech- nique generates models of visual interest through their great geometric complexity. A 
typical fractal surface may consist of polygons whose numbers may reach easily into the six figure range. 
As we shall see below, naive ray tracing is simply too slow to feasibly render a complex fractal surface. 
The method we present here overcomes this problem. It does so by evolving the fractal surface in concert 
with the rendering of it. We generate only those parts of the surface which are likely to intersect the 
ray being traced. In this way, each ray need be compared with only a handful of polygons instead of the 
large collec- tion of polygons making up the entire fractal surface. We compute a certain polytope, viz. 
an extent, which completely encloses the surface ultimately evolved with a high degree of certainty. 
If a ray intersects this ex-tent we must inspect its contents more closely. If not, the extent is pruned 
from further consideration thus saving much computation. We mention that this method is essentially the 
adaptive subdivision technique mentioned in (Fournier, Fussell, and Carpenter[0]) translated to the ray 
tracing context. 2.1 The Recursive Subdivision Method We use the subdivision method for generating frac- 
tals (Carpenter[5], Fournier and Fussell[8], Fournier, Fussell, and Carpenter[9]). In what follows we 
give a brief review of the technique as presented in the references above. The recursive subdivision 
technique proceeds as fol-lows. 1 The surface is modelled as a large number of triangles. The x and y 
coordinates of the triangle ver- tices are set on an isometric grid (See figure 1), while their z-coordinates 
are generated recursively as follows: Once the displacements zi, i = 1,2,3 for a given level of reeursion 
axe determined, generate three new independent random vaxi- ables ~i, i = 1, 2,3 with mean given by the 
equation zj+zk E~ -~ -- 2 and variance where li is the length of the side of a triangle and H is the 
"fractai dimension". That is, zi is the height of the vertex i of a triangle and the ~i is the displacement 
over the bisector of its opposite edge. These axe summed to make the vertices of triangles at the next 
level of reeursion. 2.2 The Rendering Algorithm The rendering algorithm is very simple. Instead of instantiating 
the surface in its entirety, we evolve a piece at a time. There are several advantages to this. In the 
ray tracing method, the intersection computa- tion is the most time consuming step. If we were to fully 
evolve the fractal surface, we would need to determine the intersection point of the current ray with 
every polygon. 2 As a typical scene requires trac- ing on the order of a million rays and a fractal sur-face 
may contain the same order of magnitude of polygons, the intersection computation under the naive method 
would require a trillion ray-polygon intersec- tions. Clearly this is impractical. By evolving only a 
piece of the fractal surface at a time, we can cut down the number of intersections which must be computed. 
The computation must intersect a ray r with a recur- sively defined surface. Suppose that we are able 
to enclose each part of the surface with an extent, viz. a volume which is guaranteed to contain a segment 
of 1We restrict ourselves to the case of triangulaxized surfaces. See Carpenter[5] and Fournier and Fussell[8], 
for other methods. 2One of the reviewers has pointed out that a fractal surface can never be ftdly evolved 
but only approximated to a certain level of detail 92 the fully evolved surface. The fractal surface 
is repre- sented by a tree t of branching ratio four. At each node n of t we associate a pair (p, e) 
where p is a polygon, called a facet, representing the surface at the level of recursion indexed by the 
depth of n, and e is an ex- tent which encloses the surface given by the subtree at n. The leaf nodes 
of the tree correspond to the fully evolved fractal surface. The polygon corresponding to a leaf node 
shall be called a primitive facet. An an object A is said to shadow a node n(p, e) with respect to a 
ray r if the ray intersects A at a point closer to the ray origin than the intersection with e. A node 
n(p, e) is said to be active if its extent e intersects the ray and no primitive facet shadows it. Note 
that an inactive node can never contain the closest intersection of the ray with the fully evolved surface. 
The algorithm maintains a list of active nodes which are to be traced by the current ray r. With each 
node is associated a diatance d from the origin of the ray to the closest intersection with its extent. 
The algorithm proceeds as follows: Choose the closest node and remove it from the active node list. 
Intersect with the four extents el, i ~ 1,..., 4 associated with the children nl, i ~ 1,..., 4 of the 
node ~. If no ei intersects r then there are no new active nodes. Start over. Else, add the nodes whose 
extents el intersect the ray to the active node list. If the new nodes contain primitive facets p~ and 
the facets intersect the ray then cull from the ac- tive node list the nodes shadowed by the closest 
pi. (Simply compare the distance of each node to the distance of the facet intersection point). It may 
be conjectured that a node ought to be rendered inactive whenever it is shadowed by any facet (not necessarily 
primitive). Certainly this holds in the two dimensional case, see figure 2a. But figure 2b shows that 
this is false in the three dimensional case. The facet for node n shadows its neighboring extent but 
it contains no primitive facet doing so. 2.3 Computing Extents The key to the performance of the above 
algorithm lies with the specification of extents. Beyond the necessary conditions for an extent--that 
they enclose the actual surface segment corresponding to a node--there are two additional desiderata 
which extents should satisfy. First, extents should be tight, that is, they should enclose the actual 
surface snugly. Consider the case when extents do not, say when an extent is the whole of space. Then 
no pruning will ever take place: the al- gorithm degenerates to the naive method. As extents become more 
and more snug, opportunity to prune sur- faces arises. Thus tight extents improve the asymptotic complexity 
of the algorithm. The second desideratum for an extent is that it be easily intersected with a ray. That 
is, one should not expend undue amounts of computation in determining whether a ray intersects an extent 
or not. This criterion determines the not insignificant multiplier in the complexity of the algo- rithm. 
By the second criterion, an ideal extent for ray tracing is the sphere. Among all bounding surfaces it 
is the easiest to intersect with a ray. However, it fails with respect to the first criterion. Spheres 
do not contain fractal surfaces very snugly. We have chosen an extent which is shown in figure 3. This 
extent is formed by taking the convex hull of the facet translated in z by a distance +t/. Because of 
its shape we have called this object a cheesecake eztent. Given that t/is of minimum value, this extent 
is relatively tight. Because it is formed from simple polygons, it is easy to determine ray intersections. 
How can we determine the value of ~t? A fractal surface is stochastically defined. Thus it is not possible 
to predict with complete certainty how it will vary. On the other hand, because the statistical properties 
of the surface are well known, we can choose t/ so that there is an overwhelming probability that a cheesecake 
encloses the fully evolved surface. There are two approaches we may take. The first uses the Chebyshev 
inequality (Chow and Teicher[6]): Let X be a random variable with EIX ] < oo and variance e~c. Then P{]X-EX 
I > a} < _ aux , a>O _ a2 Now, if S(z,y) is a generalized Levy surface, the variance of as = s(=,, w) 
- is simply a function of the distance d between the points (xl, Yl) and (x2, Y2). Namely, var(z s) 
= a "vz 93 where V~r is a constant depending on the fractal dimen- sion H, (see corollary 3.4 of Mandelbrot 
and van  Ness[12]). The point of maximum distance from all the vertices of a triangle with sides of 
length I is the center. This dis- tance is ~ If we use the expression for unconditional variance, (it 
is clear that conditioning can only make the variance go down) then the variance is given by: Choosing 
q to be 10a guarantees a .99 chance of the cheesecake enclosing the fully evolved surface. If, as is 
usual, we know the surface follows a Gaussian distribution then one can do much better than the Chebyshev 
inequality. The probability that the cen-ter point extends beyond the cheesecake is then simply twice 
the tail of the distribution. For example, choos- ing q to be 3a gives P ~-- .9974. Carpenter has pointed 
out that if the random numbers ~ are generated via table lookup, then extents may be calculated with 
total certainty (Whitted[17]). 2.4 Analysis of the Algorithm To analyze this algorithm we note that there 
are three cases of ray intersection: 1)the ray does not intersect the top level cheesecake; 2)the ray 
intersects the top level but not the surface; and 3)the ray intersects the surface. If the ray does not 
intersect the top level cheesecake then one intersection computation is sufficient to prune the fractat. 
The number of rays which do not satisfy this criterion is roughly the number of pixels in which the fractal 
is visible (assuming extensive self shadowing does not occur). The number of rays of the second kind 
is small if the extents are at all tight, to be conservative we treat these rays as if they were of the 
third kind. The algorithm choses active nodes to process on the basis of distance. In the best case, 
only those nodes contained in the path from the root to primitive facet are chosen. At each step, new 
nodes are spawned and added to the active node list. Each step computes intersections with 4 cheesecakes 
and may either discard or add the new nodes to the active node list. When the primitive facet is finally 
encountered the entire rest of the list is culled. Thus the number of intersection computations which 
must be done is: 4 log 4 n. The number of node generation steps is the same. Generating each new node 
is relatively cheap compared to the intersection computation. Thus in the best case, the number of intersection 
cal- culations is (numberof visible pixels) X 4 log 4 n In the worst case, the extents are able to do 
no pruning at all. Then we degenerate to the naive case: nm where n is the number of primitive triangles 
and m is the number of rays for the scene. It has been our experience that the average case with real 
data possesses the same asymptotic complexity as the best case. We present some preliminary computa- 
tions indicating why this should be so. In general, however, evaluating the average case complexity is 
problematical. We would like to calculate the probability that a ray striking a triangular facet with 
a given set of angles will strike neighboring cheesecakes first, thus causing un-necessary intersection 
computations to be performed. In the following analysis we assume that ray strike points are distributed 
uniformly across a facet. Let a ray strike a triangular facet with angle 0 to the facet normal. (See 
figure 4). Then it must be a distance less than l~ from the boundary to strike a neighboring cheesecake, 
where l ~ = htan# Now, projecting the ray onto the facet plane gives figure 5. The ratio of the area 
of the entire triangular facet to the area of the shaded region gives the probability that a random ray 
will strike a neighboring cheesecake first. This ratio may be computed by noting that the follow- ing 
equations hold: c = Usin c c a -- -- tan f b =/Icos~b The ratio of the areas is the ratio of a + b 
~o I or Prob -----T(cos ~b + -~-sin ~b)  _ h t~n 0 (cos  + -~- sm ) Where h is determined as the cheesecake 
height of the previous section, say 4a. So the probability of at least an extra intersection computation 
occuring given a certain set of angles for the incoming ray is: [ ~ 2H P(Extra Computation]0,~b}~---4~-~-J 
VHtan8  V~ 2H ~b) / Ze~g hbr  (COS ~ + --3- sin ltriangle H K/2neighbor The probability given a set 
of angles goes down ex-ponentially for each level of recursion of the neighbors. Now, each level of recursion 
halves /neighbor- We sum the geometric series to obtain the expected number of intersection computations 
required. Number of computations ~ g y~ l.e o, n~0 <K~-~ l~ or 2H n~0 = K/neighbor (~1) 2Hn n~0 --K 
(2lnelghbr)2H 2TM --1 The key observation one should make about the above calculation is that the number 
of expected superfluous computations for small incidence angles is very small --much less than for the 
worst case. In fact, the geometric series sums to a small constant making the asymptotic complexity the 
same as for the best case. To find the expected number of superfluous computa- tions we use the theorem 
of total probability. But here the evaluation of the number of computations be- comes problematical. 
The distribution of angles is not a simple uniform distribution. Figure 6 shows that a facet relatively 
far from the ray source sees a distribu- tion of angles which is highly peaked about the view- ing angle. 
When shadowing and reflection occur, the distribution is far from uniform. We have observed in practice 
an average performance whose general character is very good. For example, when the level of recursion 
was increased from 5 to 6 --quadrupling the number of polygons from 1024 to 409{} --the runtime of the 
algorithm increased by some 12%. No effort has been made to cache calculated subtrees. Each ray intersection 
must evolve the surface anew from the top node. Whirred has suggested that cach- ing the subtree computations 
can be a very useful op- timization. (Whitted[17]). 3 Prisms A box is formed by moving a rectangle orthogonally 
in space, and a cylinder is formed by so moving a circle. In general, we define a prism to be a volume 
formed by translating a plane curve along a vector n for a distance d. The curve is defined in a plane 
P whose normal vector is n. Many objects can be defined as collections of prisms. Among them include: 
block letters, machine parts formed by extrusion, simple models of urban architec- ture, surfaces with 
"ridges"- small vertical perturba- tions of a plane which embellish the texture of a surface with sharp 
edges. All these objects can always be modelled as collections of polygons. We consider the case where 
the plane curves defining these surfaces are complex, e.g. having thousands of vertices. Ray tracing 
such prisms defined as collections of polygons would be extremely expen- sive. The technique we describe 
here illustrates a general technique which will be used again later. We take advantage of the essential 
symmetries characterizing these surfaces to reduce the intersection problem from three to two dimensions. 
We then solve this two dimen- sional ray tracing problem by using a representation for plane curves known 
strip trees which is popular in computational geometry (Ballard[2 D. A prism is defined as the union 
of three parts. The first part is the set of sides of the prism given by the locus of points tn + -y(s). 
Where ~/(s) is a curve embedded in a plane P known as the baseplane, n is the unit normal of P, and 0 
< t < h where h is the height of the prism. The second and third parts of the prism are known as the 
base and cap. The base B is set of points interior to the curve if(s) in P, the cap is simply the set 
B -4- hn. 95 We now give the algorithm for intersecting a ray with a prism. First find the intersection 
point of the ray with base and cap planes and project these points down onto the base plane. The ray 
itself is then projected onto the base plane. We have now reduced the problem to a two dimensional one 
because of the following fact. (See figure 7.) Proposition. To intersect a ray r(t) ~ o + tv with a prism 
(B, h, ~,(,)), let the ray-base plane strike point be at t -----to. Let the base plane projection of 
the ray-cap plane strike point be ~ = tl. Then r strikes the prism iff 1) the base plane projection r'(t) 
of r(~) intersects the curve ~/(a) at a point ~ = t I and 0 < I-t01 v.n < h where n is the unit normal 
of the baseplane B, or 2) rr(to) or rr(fz) is in the interior of ~/(s). This proposition then suggests 
the following algorithm: Find the ray strike points t ~-~ to and t ~ tz with the base plane B and with 
the cap plane B -F hr~, respectively. Project the ray r down into the base plane to give the two dimensional 
ray r I. Find all the intersections of the ray r I with As). (We discuss how this will be done later.) 
Sort the intersections by distance t from the ray origin o. Scan the sorted list and perform the actions 
con- ditioned by the following cases: 1) Cap plane strike point: if inside "lr(s) then stop else proceed. 
2) Base plane strike point: if inside "y(s) then stop else proceed. 3) A "y(s) intersection: If the intersection 
point t ~  is such that 0 < It p -- tolv n(B) < h then stop else proceed. How is the intersection of 
the two dimensional ray r t with an arbitrary plane curve ,)'(s) to be done? We use the method of strip 
trees. Strip trees are presented in (Ballard[2]) as generalizations of structures computed in a curve 
digitization algorithm invented by Duda and Hart[7]. A strip tree is a hierarchical structure which represents 
the curve at varying resolutions. The strip tree associated with a curve 3(s) is a tree t with nodes 
n(e, c), where c is a portion of ~/(a) and e is an extent which completely encloses e. An extent e is 
shown in figure 8, it is a triple e = (b, wl, w2) consisting a baseline b, two widths Wl, w2. The baseline 
is a line segment of arbitrary orientation. Geometrically, the extent is a rectangle whose edges are 
determined by the baseline and widths. The extent rectangle is chosen in such a way as to enclose the 
minimum area containing the curve segment c. Thus each edge of e touches at least one point of e. See 
figure 9. The subtrees of node n subdivide c on a point which touches the edge. We now give an algorithm 
for generat- ing a strip tree associated with any plane curve ~/(s). Choose as a baseline the line segment 
connecting the first and last point of the curve. Now scan the curve for the maximum and minimum signed 
distance away from the baseline. These set the widths of the root triangle. Divide the curve at one of 
these points and compute the subtrees recur sively. It is evident that this is an n log n computation. 
A linear time algorithm is also available (Ballard[2]). It is now a simple matter to efficiently intersect 
a two dimensional ray with a plane curve defined by a strip tree. First intersect the ray with the root 
extent. If there is an intersection then intersect with each of the subtrees, if not, there is no intersection 
with the ray. Continue recursively. Given the above algorithms we are now able to calcu- late the intersection 
point of the ray with the prism. Calculating the normal of the surface at the intersec- tion point is 
simple. If the ray strikes either the base or cap planes then the normal is the plane normal. Otherwise, 
it strikes the sides. The three space nor-mal is then a simple linear transformation of the two space 
normal which is given by the strip tree baseline equation. This linear transformation is determined by 
the base plane normal. 4 Surfaces of Revolution Polygon or patch methods may serve as approximations 
to the surfaces of revolution described in this section. However, this technique can model complex surfaces 
which would require dozens of patches or hundreds of polygons. The tracing time of such patches or polygon 
collections would be high. The method described here takes advantange of the high degree of symmetry 
avail- able in the model. As ray tracing algorithms go, it is relatively fast. A surface of revolution 
is defined via a 3-tuple (b, a, p(s)), where b is a point called the bane point, a is the azi8 uector, 
and p(s) is the radius ]unction. See figure 10. Figure 11 shows the geometrical interpretations of the 
following definitions. We define the cut plane as the plane containing the current ray which is parallel 
to the axis of revolution. Let the ray be given as an origin 96 and direction vector: r(O = o + ~v. 
 Then the cut plane normal c is given by c~,Xo. The plane contains the ray origin o. Define d to be the 
perpendicular distance of the base point b to the cut plane. The algorithm first reduces the problem 
of intersecting a ray r with a surface of revolution to the problem of intersecting a two dimensional 
ray d with two plane curves ~, ~2 formed by intersecting the cut plane with the surface of revolution. 
Define a coordinate system in the cut plane as follows. The origin of the plane is the projection b t 
of the base point. The y axis vector is the projection of the axis of revolution a, the z axis unit vector 
is given by a X c. The algorithm now attempts to intersect the two dimensional ray with the two curves 
./1, if2 representing the surface meeting the cut plane: But how are these two plane curves to be determined? 
Figure 12 shows how the curves depend on the radius function p(8) and the distance d, namely: = i fly 
= P~" ff the quantitiy under the root is negative then the curve disappears altogether. Now, the radius 
curve p(s) may contain thousands of points: it would be extremely expensive to calculate the plane curves 
~i for every ray using the above rela- tion. Even if we do, the question of which specific al- gorithm 
to intersect the curves with the ray remains unanswered. Of course, we intend to use strip trees. But 
if we do so directly then we would be forced to recalculate the bounding extents at each intersection 
computation. This is because the plane curves change radically for each different ray. Such a scheme 
would be impractically slow. Strip trees only yield advantages if their extents can be precalculated 
and reused for each ray trace. The solution to this problem is to trace in a different space. Specifically, 
we trace not in (z, y)-space but in (z ~, y)-space. In this space the equation defining the two plane 
curves appears as: '7= ~--- P= --d 2 The original radius curve p is defined as the square of the actual 
radius curve. The plane curve ,y is simply translated by the square of d the distance of the cut plane 
from the base point. Figures 13a,b show examples of this transformation. As the distance of the cut plane 
from the axis grows, more of the curve lies below the axis. Note that we are now tracing curved rays 
that bounce off the origin. Thus the parts of curves below the axis are inaccessible to the ray. To trace 
curved rays we use a slightly different ray equation. Instead of tracing rays defined as r --~ o + tv 
we now trace rays defined as r= = (o= + 2  rzt -----ou + tvu We are now charged with intersecting a 
curved ray defined as above with an arbitrary plane curve. Fortunately, this plane curve is translated 
by d so that now it is a simple matter to use strip trees. We recursively intersect the curved ray with 
an extent box. Each extent intersection is easily acomplished by straightforward solution of a polynomial 
with at most quadratic degree in t. Solving for t gives the distance from the ray origin. Substituting 
into the original vec- tor form for the ray gives the exact intersection point. The above algorithm then 
determines intersection point of a ray with a surface of revolution. To com- plete the ray tracing process 
we need to compute the surface normal. This is done in two steps. The first step is to translate the 
slope of the plane curve in bent (x 2, l/)-space back into a plane normal in fiat (z, V)-space. This 
we do by using the following equa- tions. Let i= be the x-coordinate of the intersection (in fiat two 
space), and /=, l~ the components of the nor-mal vector in bent two space given by the baseline line 
equation in the strip tree defintion, then the normal vector (n=, n~) expressed in fiat two space coordinates 
is: 2/=i= + ly n t/ 97 The second step is to translate the plane normal into space normal using the 
geometry shown in figure 14. The space normal is given by: n = n=i=c X a + n=(-d)c + [=Inca. We have 
now calculated the intersection point and the normal to the surface at that intersection point for a 
surface of revolution. This is all that is required for rendering the surface. 5 Results The above algorithms 
were coded in FORTRAN on a DECSYSTEM-2060. The actual fractal algorithm per- formed differs from the 
presented one in that no selec- tion of closest extents occurs. The facets are expanded and pruned in 
a strict depth first manner with fixed ordering for the four subnodes. Evidently, the pro-grammed algorithm 
is slightly simpler at the cost of increased computation time. Figure 15 shows a sample output of the 
al-gorithm. A scene composed of a reflecting sphere placed above a fractal =valley" evolved from a single 
initial triangle with vertices (-1, -~s, .5), (1, -3 ~, .5), (0, ~, 0). Note that in the reflection of 
the sphere the back side of the mountain can be seen to shadow itself. This image, computed at a resolution 
of 25{}  256 pixels, consumed 190 minutes of CPU time. Figure 16 shows an example of the prism algorithm. 
In this scene, an =E" shaped curve is translated along the z-axis. Two reflecting spheres are placed 
in proximity with the prism. An example of the surface of revolution algorithm is shown in figure 17. 
This scene shows a wine glass resting on a single triangle. The wine glass is defined as a base point 
b on the surface of the triangle, an axis of revolution coincident with the z-axis, and a radius curve 
of only 9 points. This points out an interesting feature of the algorithm. Because the radius curve is 
defined in bent two space, straight lines translate into parabolas. Thus the curvature of the glass is 
due entirely to the curvature of the coordinate system. This image consumed 20'minutes of CPU time. 6 
Further Work There are several obvious extensions of the above al- gorithms which allow for other primitives. 
 A modification of the fractal rendering algorithm will work for arbitrary determiniatic height fields 
defined on triangular grids. Many objects in computer graphics may be so defined: human faces, digitized 
ter- rain, planar polygons with detailed surface features, injection molded objects, etc. In fact, this 
algorithm is applicable to scan-line rendering methods as well as ray tracing. A 8ubdiviMon tree is constructed 
by recursively sub- dividing the height field. Each node of the tree stores an extent enclosing a section 
of the field. Once such a tree has been constructed, we use the fractal rendering algorithm. The algorithm 
operates in exactly the same manner as above except that the tree is fully instan- tiated before rendering. 
Generating the subdivision tree is similar to the strip tree algorithm. Recursively execute the following 
step: Scan the surface defined by the domain triangle, searching for the point of maximum and mini- mum 
distance 91, 92 to the triangle containing the vertices. Store the extent cheesecake with distances Yl,92. 
Subdivide the domain triangle into four subtriangles. Note that the complexity of constructing this tree 
is nlog 4 n where n is the number of points in the height field. A linear time algorithm which computes 
somewhat larger cheesecake extents is also possible by constructing the tree in a bottom up fashion. 
Both these algorithms construct bivariate analogs of strip trees and essentially mimic the strip tree 
algorithms (Ballard[2]). A further modification of the height field algorithm follows from the observation 
that objects other than polygons can serve as leaf nodes in the tree. Such a tree, with extents given 
by procedural definitions, is easily seen to be an elaboration of the well known method of Rubin and 
Whitted[15]. It may well be that extents other than cheesecakes would be more effective for the fractal 
rendering algo- rithm. For example, it is easy to argue that ellipsoids would be tighter extents than 
cheesecakes. The condi- tional variance of a fractal segment is maximum in the center of the fractal 
and diminishes as it approaches the vertex of a facet. Ellipsoids also exhibit this be- havior but cheesecakes 
do not. To construct an ellipsoid extent, one would choose two of the three ellipsoid principal axes 
to be in the plane of the triangle. The principal lengths would be set to cover the vertices. The third 
principal axis should be vertical with principal length sufficient to cover y in the center of the ellipsoid. 
 98 Itshould be easy to calculate intersection points be- tween a ray and an extent. And, certainly, 
intersecting a ray with an ellipsoid is almost as easy as intersecting one with a sphere. Let the matrix 
A be formed using the principal axes as rows. Intersecting s ray r with an ellipsoid is equivalent to 
intersecting a ray A-lr with a sphere. The prism and surface ~f revolution algorithms may be extended 
in a number of ways. One extension is to allow curves other than simple line segments to reside in the 
leaves of the strip tree. There are two requirements that such new leaf curves should satisfy. First, 
they should be completely enclosed by s suitable rectangular extent. Second, it should be relatively 
easy to intersect a ray with these primitive curves. Useful primitives are: circular arcs, parabolas, 
and algebraic curves. Both algorithms may also be generalized by linearly transforming the incoming ray. 
In this way surfaces of revolution may have elliptical instead of circular cross sections. In addition, 
this technique can model skewed surfaces of revolution and skewed prisms. Note also that the methods 
presented here may be applied in a mutually recursive manner to efficiently render objects of potentially 
high complexity. We can best explain what we mean by an example. Suppose the task is to model an office 
building. One way to do this would be to model it as a prism where each face of the prism is the base 
plane of a collection of smaller prisms each of whose faces is, say, a determinis- tic height field. 
The rendering of this building would recursively invoke the algorithms presented above. Of course, the 
normal computation of the surface is subject to the now standard techniques of Phong smoothing, and Blinn 
perturbation mapping. The prism and surface of revolution algorithms also allow a cheaper version of 
these techniques to be applied to the two dimensional normsls as well. Thus, it is pos- sible to generate, 
say, smoothly reflecting and refract- ing prisms even though the boundary curves may be coarsly polygonal. 
ACKNOWLEDGMENTS. I would like to thank Howard Derby for many discussions and much help during the course 
of this investigation. I also thank the SIGGRAPH and TOGS reviewers for suggesting many useful changes. 
7 References 1. APPEL, A. Some Techniques for Shading Machine Ren- derings of Solids. SJCC (1968) 37-45. 
 2. BALLARD, D.I-L Strip trees: a hierarchical repre- sentation for curves. Comm. ACM, 24 (May 1981) 
310-321. 3. BLINN, J.F. Simulation of wrinkled surfaces. Computer Graphite 12 (August 1978) 286-292. 
 4. BRAID, I.C. Designing with Volumes, Ph.D. Dissertation. Univ. Cambridge, England (1973). 5. CARPENTER, 
L.C. Computer rendering of fractal curves and surfaces. SIGGRAPHSO Conference Proceedings Supplement 
(August 1980).  6. CHOW Y.S., TEICHER, H. Probability Theory: Independence, Interch~ngeability, Martingales. 
Springer Verlag, Heidelberg (1978). 7. DUDA, R.O. AND HART,P.E. Pattern Classification and Scene Analysis. 
Wiley-Interscience, New York (1973). 8. FOURNIER, A., FUSSELL, D. Stochastic modelling in computer graphics. 
SIGGRAPHSO Conference Proceeding# Supplement (August 1980).  9. FOURNIER, A., FUSSELL, D. AND CARPENTER, 
L. Computer rendering of stochastic models. Comm. ACM, 25 (June 1982) 371-384. 10. GOLDSTEIN, E. AND 
NAGLE, It. 3D visual simulation Simulation 16 (Jan 1971) 25-31. 11. I~NDELBROT, B. ~tale: Form, Chance, 
and Dimension. W.H. Freeman, San Francisco(1977). 12. MANDELBROT, B. Fractional brownian motions, frac- 
tional noises and applications. SIAM Review 10,4 (October 1968) 422-437. 18. ItEQUIOHA,A.A.G. Representations 
for rigid solids: theory, methods, and systems. ACM Computin# Surveys 12 (December 1980) 437-464. 14. 
ItOTH, S.D. Ray casting for modeling solids. Computer Graphic# and Image Processing 18 (1982) 10g-144. 
 15. ItUBIN, S. AND WHITTED, T. A three-dimensional representation for fast rendering of complex scenes. 
Computer Graphics 14 (1980) 110-116. 16. WHITTED, T. An improved illumination model for shaded display. 
Comm. ACM, 23 (June 1980) 343- 349. 17. WHITTED, T. private communication (1983).  99 Computer Graphics 
Volume 17, Number 3 July 1983 Vs Figure i. Frsetal subdh, bdon. The midpoint of e~ch edge ~" the trLsmg]e 
is ~spbLced in z by s rsndom v~rL~ble. ~Y Figure ~L Estlmatlng wssted eomputatlons. If xn in- coming 
ray strikeJ a fscet xt a smxl] enough ~ngle O, it will mi~ the sldeL FACET . . . " . , : " '*  . . 
 L [~mre 5. Estlmating wasted eomputst|ons. This figure shows a top view of figure 4. We use it to c~culate 
the &#38;re~ of the shxded region. Figure $. A cheesecake extent. This extent is formed by the volume 
Bwept out by trans|~ted a facet in z by lq. 100 A revised version of this paper will appear in the July 
1983 issue of acm Transactions on Graphics. l~.gure 6. Esthnatin E weusted computations. Under typi- 
cal viewing conditions, the angle ol e ray-facet intersections is hardly uniformly distributed. Figure 
10. A surTaee of revolution. The surface is defined by a base point b, an axis vector a, and a rsdins 
function pie). Figure 7. A prism. The ray is shown striking the side of n prism of height h with bue 
plane normal g. Figure 11. Computing the intersect|on. The cut pinne cont,.ins the ray and is par&#38;llel 
to the axis of revolution. It cuts the surface in two curves. Figore 8. A strip tree extent. Figure 12. 
Intersection curves. The cut plane intersection curves vary drarastlcsIly in shape with the cut plane 
to base Figure 0. A strip tree. Rectanguinr e~tents completely point distance. enclose the bold curve. 
 101   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801138</article_id>
		<sort_key>103</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Technical implications of proposed graphics standards (Panel Session)]]></title>
		<page_from>103</page_from>
		<doi_number>10.1145/800059.801138</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801138</url>
		<abstract>
			<par><![CDATA[<p>This panel session is intended to present recent technical developments in the efforts to standardize computer graphics.</p> <p>Several alternative approaches to 3-D standards will be presented and contrasted. The Virtual Device Metafile will be presented, as well as a proposed binding of the GKS standard to Fortran.</p> <p>Presentations will be made by the following individuals:</p> <p>Dr. Peter Bono is the chairman of the ANSI X3H3 technical committee developing standards for Computer Graphics Programming Languages, and will present the current status of the effort to adopt 2-D GKS as an International Standard and as an American National Standard.</p> <p>Tom Wright will be available to respond to questions about the Programmer's Minimal Interface to Graphics (PMIG) proposal, and its current status as a new output level of the draft proposed American National Standard (dpANS) GKS.</p> <p>Mark Skall will present a proposed binding of GKS to the Fortran Language. Mr. Skall will also discuss progress in the field of formal specification of graphics standards and developments in the establishment of conformance/certification procedures for implementations of GKS.</p> <p>Theodore Reed will present the technical content of the Virtual Device Metafile (VDM) draft proposed American National Standard. Discussed will be the relationship of the VDM to GKS and to the yet to be proposed Virtual Device Interface. Specific functionality of the VDM will be discussed as will specific bindings of that functionality as a character set extension and as a binary format.</p> <p>David Shuey will present an overview of the Programmer's Hierarchical Interface to Graphics (PHIGS) proposal. The PHIGS proposal is intended to support hierarchical structuring of graphics data, in contrast to the Core System and GKS proposals. This type of structure addresses highly interactive graphics applications which need to modify the presentation and the relationships within graphics data.</p> <p>Richard Ehlers will present the attribute model of the PHIGS proposal and explore the relationship of attribute model to structured graphics data bases. Also, Mr. Ehlers will discuss the viewing and transformation implications of structured graphics data bases using examples.</p> <p>Gunter Enderle is a member of the West German delegation to the International Standards Organization (ISO) Working Group on Computer Graphics. Herr Enderle will be discussing ISO proposals for the extension of the GKS standard from 2-D to 3-D functionality. Several such proposals have been made, including one by DIN (the official standards making body of the German Federal Republic) and a Norwegian proposal called IDIGS.</p> <p>Elaine Sonderegger was a member of the ACM SIGGRAPH Graphics Standards Planning Committee, and is the ACM SIGGRAPH representative to ANSI X3H3. Ms. Sonderegger will contrast the 3-D functionality of the Core System, IDIGS, DIN proposed 3-D extensions, and PHIGS.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Standards</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010919.10010177</concept_id>
				<concept_desc>CCS->Computing methodologies->Distributed computing methodologies->Distributed programming languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011017</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Domain specific languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Standardization</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P329816</person_id>
				<author_profile_id><![CDATA[81100550687]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Straayer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tektronix, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P223671</person_id>
				<author_profile_id><![CDATA[81100626262]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bono]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Athena Systems]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P333255</person_id>
				<author_profile_id><![CDATA[81332497032]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ehlers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Evans & Sutherland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP48023126</person_id>
				<author_profile_id><![CDATA[81100109845]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Gunter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Enderle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Karlsruhe Nuclear Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P334016</person_id>
				<author_profile_id><![CDATA[81332522908]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Theodore]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reed]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Los Alamos National Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39078987</person_id>
				<author_profile_id><![CDATA[81100603093]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shuey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[McDonnell Douglas Automation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39079178</person_id>
				<author_profile_id><![CDATA[81332528147]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Skall]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Bureau of Standards]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P330107</person_id>
				<author_profile_id><![CDATA[81332528748]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Elain]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sonderegger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SIGGRAPH]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31102008</person_id>
				<author_profile_id><![CDATA[81450594467]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wright]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ISSCO]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PANEL TECHNICAL IMPLICATIONS OF PROPOSED GRAPHICS STANDARDS CHAIR: David Straayer, Tektronix, Inc. 
PANELISTS: Peter Bono, Athena Systems Richard Ehlers, Evans &#38; Sutherland Gunter Enderle, Karlsruhe 
Nuclear Research Theodore Reed, Los Alamos National Laboratory David Shuey, McDonnell Douglas Automation 
Mark Skall, National Bureau of Standards Elaine Sonderegger, SIGGRAPH Tom Wright, ISSCO CHAIRMAN'S INTRODUCTION: 
 This panel session is intended to present recent technical developments in the efforts to standardize 
computer graphics. Several alternative approaches to 3-D standards will be presented and con- trasted. 
The Virtual Device Metafile will be presented, as well as a proposed bind- ing of the GKS standard 
to Fortran. Presentations will be made by the follow- ing individuals: Dr. Peter Bono is the chairman 
of the ANSI X3H3 technical committee developing stan- dards for Computer Graphics Programming Languages, 
and will present the current status of the effort to adopt 2-D GKS as an International Standard and as 
an Ameri- can National Standard. Tom Wright will be available to respond to questions about the Programmer's 
Minimal Interface to Graphics (PMIG) proposal, and its current status as a new output level of the draft 
proposed American National Standard (dpANS) GKS. Mark Skall will present a proposed binding of GKS to 
the Fortran Language. Mr. Skall will also discuss progress in the field of formal specification of graphics 
standards and developments in the establishment of conformance/certification procedures for implementations 
of GKS. Theodore Reed will present the technical content of the Virtual Device Metafile (VDM) draft 
proposed American National Standard. Discussed will be the relation- ship of the VDM to GKS and to the 
yet to be proposed Virtual Device Interface. Specific functionality of the VDM will be discussed as 
will specific bindings of that functionality as a character set extension and as a binary format. 
David Shuey will present an overview of the Programmer's Hierarchical Interface to Graphics (PHIGS) proposal. 
The PHIGS pro- posal is intended to support hierarch~al structuring of graphics data, in contrast to 
the Core System and GKS proposals. This type of structure addresses highly interactive graphics applications 
which need to modify the presentation and the relationships within graphics data. Richard Ehlers will 
present the attribute model of the PHIGS proposal and explore the relationship of attribute model to 
structured graphics data bases. Also, Mr. Ehlers will discuss the viewing and transformation implications 
of structured graphics data bases using examples. Gunter Enderle is a member of the West German delegation 
to the International Standards Organization (ISO) Working Group on Computer Graphics. Herr Enderle will 
be discussing ISO proposals for the exten- sion of the GKS standard from 2-D to 3-D functionality. Several 
such proposals have been made, including one by DIN (the official standards making body of the Ger- man 
Federal Republic) and a Norwegian pro- posal called IDIGS. Elaine Sonderegger was a member of the ACM 
 SIGGRAPH Graphics Standards Planning Com- mittee, and is the ACM SIGGRAPH represen- tative to ANSI 
X3H3. Ms. Sonderegger will contrast the 3-D functionality of the Core System, IDIGS, DIN proposed 3-D 
exten- sions, and PHIGS. 103
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801139</article_id>
		<sort_key>105</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Computer graphic modeling of american sign language]]></title>
		<page_from>105</page_from>
		<page_to>114</page_to>
		<doi_number>10.1145/800059.801139</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801139</url>
		<abstract>
			<par><![CDATA[<p>The essential grammatical information of American Sign Language (ASL) is conveyed through changes in the movement and spatial contouring of the hands and arms. An interactive computer graphic system is described for the analysis and modeling of sign language movement. This system consists of four components. The first component reconstructs actual movements in three dimensions and allows the user to interactively segment and transform the data for later analysis. The second component allows a user to interactively create synthetic signs by specifying angle functions in a jointed model. The third component provides a novel technique for manipulating movement quality independently of spatial path. The fourth component allows the building of complex stimuli and real-time stimulus sequencing for psycholinguistic experiments. The emphasis is on interactive techniques and data structures that allow analysis and modeling of the complex hand and arm movements of American Sign Language.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[American sign language]]></kw>
			<kw><![CDATA[Movement]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Modeling packages</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011066.10011070</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Development frameworks and environments->Application specific development environments</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P331350</person_id>
				<author_profile_id><![CDATA[81332513404]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jeffrey]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Loomis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seacoast Software, The Salk Institute for Biological Studies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31090492</person_id>
				<author_profile_id><![CDATA[81100333647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Howard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Poizner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Salk Institute for Biological Studies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP17013495</person_id>
				<author_profile_id><![CDATA[81100366948]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ursula]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bellugi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Salk Institute for Biological Studies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P328978</person_id>
				<author_profile_id><![CDATA[81100236324]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Alynn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Blakemore]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Salk Institute for Biological Studies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP309460900</person_id>
				<author_profile_id><![CDATA[81543448056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hollerbach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Badler, N. I., Ed. Modeling the human body for animation. (Special issue devoted to the topic). IEEE Computer Graphics and Applications, 2, 9, (November 1982).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Badler, N. I. and Morris, M. A. Modelling flexible, articulated objects. Proceedings of the ONLINE Conference, (1982).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807491</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Badler, N. I., O'Rourke, J. and Kaufman, B. Special problems in human movement simulation. Computer Graphics (Proceedings of SIGGRAPH 1980), 14, 3, (July, 1980), 189-197]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356760</ref_obj_id>
				<ref_obj_pid>356757</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Badler, N. I. and Smoliar, S. Digital representation of human movements. ACM Computing Surveys, 11, (March 1979), 19-38.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Baecker, R. M. Picture-driven animation. Proceedings of the Spring Joint Computer Conference, (May 1969), 273-288.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806806</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Baecker, R. M., Miller, D. and Reeves, W. Towards a laboratory instrument for motion analysis. Computer Graphics. 15, 3 (August 1981), 191-197.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Bell System, UNIX time-sharing system (entire issue devoted to UNIX). The Bell System Technical Journal 57, 6, Part 2 (July-August 1978).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Bellugi, U. The structuring of language: Clues from the similarities between signed and spoken language. In U. Bellugi and M. Studdert-Kennedy (Eds.), Signed and Spoken Language: Biological Constraints on Linguistic Form. Dahlem Konferenzen. Weinheim/Deerfield Beach, Fla.: Verlag Chemie, (1980), 115-140.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Bellugi, U. and Studdert-Kennedy, M. (Eds.). Signed and Spoken Language:Biological Constraints on Linguistic Form. Dahlem Konferenzen. Weinheim/Deerfield Beach, Fla.: Verlag Chemie, (1980).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Donkoh, S. Computer analysis helps train athletes. CIPS Review., (July-August 1980), 14-15.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Futrelle, R. P. GALATEA: Interactive graphics for the analysis of moving images. Information Processing 74. Proceedings of the 1974 IFIP Conference, North-Holland Publishing Company, 712-716.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Futrelle, R. P. and Potel, M. J. The system design for GALATEA, an interactive real-time computer graphics system for movie and video analysis. |Computers and Graphics,1 Pergamon Press, (1975), 115-121.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>58928</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Hildebrand, F. B. Introduction to Numerical Analysis. (1956).]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Johansson, G. Visual perception of biological motion and a model for its analysis, perception and Psychophysics, 14, (1973), 201-211.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Johansson, G. Visual motion perception. Scientific American, 232, 6, (1975), 76-89.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Klima, E. S. and Bellugi, U. The Signs of Language. Cambridge, Mass.: Harvard University Press, 1979.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806812</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Platt, S. M. and Badler, N. I. Animating facial expression. Computer Graphics, 15, 3, (August 1981), 245-252.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Poizner, H. Visual and 'phonetic' coding of movement: Evidence from American Sign Language. Science, 212, (1981), 691-693.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Poizner, H. Perception of movement in American Sign Language: Effects of linguistic structure and linguistic experience.# Perception and Psychophysics, 33, (1983), 215-231.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Poizner, H., Bellugi, U. And Lutes-Driscoll, V. Perception of American Sign Language in dynamic point-light displays. Journal of Experimental Psychology: Human Perception and Performance, 7, (1981), 430-440.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Poizner, H., Newkirk, D. and Bellugi, U. Processes controlling human movement: Neuromotor constraints on American Sign Language.# Journal of Motor Behavior, 15, (1983), 2-18.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Potel, M. J. and Mackay, S. A. Graphics input tools for interactive motion analysis. |Computer Graphics and Image Processing, 14, (1980), 1-23.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Shantz, M. and Poizner, H. A computer program to synthesize American Sign Language. Behavior Research Methods and Instrumentation, 14, 5, (1982), 467-474.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Winter, D. A., Greenlaw, R. K. and Hobson, D. A. Television-computer analysis of kinematics of human gait. Computers and Biomedical Research, 5, (1972), 498-504.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Woltring, M. J. New possibilities for human motion studies by real-time light spot position measurement. Biotelemetry, 1, (1974), 132-146.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Zeltzer, D. Motor control techniques for figure animation. IEEE Computer Graphics and Applications, 2, 9, (November 1981), 53-60.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 COMPUTER GRAPHIC MODELING OF AMERICAN SIGN LANGUAGE Jeffrey Loomis Seacoast Software The Salk Institute 
for Biological Studies Howard Poizner Ursula Bellugi Alynn Blakemore The Salk Institute for Biological 
Studies John Hollerbach Massachusetts Institute of Technology Abstract The essential grammatical information 
 of American Sign Language (ASL) is conveyed through changes in the movement and spatial contouring 
of the hands and arms. An interactive computer graphic system is described for the analysis and modeling 
of sign language movement. This system consists of four components. The first component reconstructs 
actual movements in three dimensions and allows the user to interactively segment and transform the 
 data for later analysis. The second component allows a user to interactively create synthetic signs 
by specifying angle functions in a jointed model. The third component provides a novel technique for 
 manipulating movement quality independently of spatial path. The fourth component allows the building 
of complex stimuli and real-time stimulus sequencing for psycholinguistie experiments. The emphasis 
 is on interactive techniques and data structures that allow analysis and modeling of the complex hand 
and arm movements of American Sign Language. CR Categories and Subject Descriptors: 1.3.0 [Computer 
Graphics]: General; 1.3.4 [Computer Graphics]: Graphics Utilities - Application packages; 1.3.6 [Computer 
Graphics]: Methodology and Techniques - Interaction techniques; 1.3.7 [Computer Graphics]: Three-dimensional 
Graphics and Realism -Animation; J.4 [Computer Applications]: Social and Behavioral Sciences -Psychology; 
J.5 [Computer Applications]: Arts and Humanities- Linguistics General Terms: Computer graphics, modeling, 
simulation, interpolation Additional Keywords and Phrases: American Sign Language, movement Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by permission of the Association for Computing 
Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. &#38;#169; 
ACM 0-89791-109-1/83/007/0105 $00.75 I. INTRODUCTION The general objective of our research is to study 
biological foundations of human language. In their surface forms, the languages of the world differ 
widely--in phonetic detail, in concepts symbolized, in word construction, in typology, and in sentence 
structure. Focusing on the surface, one might conclude that languages can differ in innumerable ways. 
Beneath the surface, however, there are striking similarities in terms of formal properties. A central 
question in the attempt to understand the human capacity for language has been to what extent these underlying 
similarities found across spoken languages are a product of their vocal-auditory transmission. Our laboratory 
has undertaken the first indepth study of languages that have developed in alternative articulatory-perceptual 
chanuels--languages produced by highly articulated movement of the hanas rather than the vocal apparatus, 
and perceived by the eyes rather than the ears. We have been specifying the ways in which the formal 
properties of languages are shaped by their modality of expression, sifting properties peculiar to a 
particular language mode from more general properties common to all languages.  A major finding of our 
research is that American Sign Language (ASL) has developed elaborate morphological devices in a form 
which is unique to a visual-gestural system. Signs undergo a wide variety of grammatical processes--regular 
changes in form resulting in systematic changes in meaning--each of which operates by imposing different 
dynamic changes in movement and spatial contouring. The existence of such a fully expressive system arising 
outside the mainstream of spoken languages affords a new vantage point for investigating biological constraints 
on linguistic form. Our research has focused on the special nature of the grammatical processes that 
have arisen in ASL; strikingly, the essential grammatical information is conveyed by the structured use 
of three-dimensional space and movement (Klima and Bellugi, 1979; Bellugi and Studdert-Kennedy, 1980; 
Bellugi, 1980). That the inflectional level of structure in ASL is instantiated entirely in dynamic 
movement patterns has been shown by studies in which deaf signers are asked to identify inflectional 
processes from their extracted movement patterns alone. In order to isolate information about movement 
from information about form (e.g., handshape) in the language, we have utilized a technique developed 
by Johansson (1975) for studying perception of motion. Small incandescent lights are placed at selected 
points on the body of a signer corresponding to the major joints of the hands and arms (shoulders, elbows, 
wrists, index fingertips). Signing is recorded in a darkened room so that on the videotape, only the 
pattern of moving points of light appears against a black background. Even with such greatly reduced 
information, deaf signers are highly accurate at recognizing and identifying inflections presented in 
these dynamic point-light displays, demonstrating that these patterns of dynamic contours of movement 
form a distinct isolable (but co-occurring) layer of structure (Poizner, 1981, 1983; Poizner, Bellugi, 
and Lutes-Driscoll, 1981; Poizner, Newkirk, and Bellugi, 1983). In this paper we outline a method for 
computer-graphically visualizing, manipulating and analyzing movements of the hands and arms that convey 
both grammatical and lexical information in ASL. We have begun to utilize a method for measuring ASL 
movement that allows three-dimensional measurement, with a digital readout of coordinates fed directly 
to a computer. Using two optoelectronic cameras, we track the positions of light-emitting diodes suitably 
placed on joints of the hand and arm. Three-dimensional coordinates are then calculated from the readouts 
of the two cameras. The ability to visualize and interactively manipulate these reconstructed movements 
in three dimensions is critical for analysis. Movement segmentation is an important linguistic aspect, 
with separate analyses applied to separate movement segments. The interactive control and three-dimenslonal 
visualization of a reconstructed movement makes segmentation and movement analysis a feasible and powerful 
tool. We couple this physical analysis of the sign signal with the computer synthesis of signs. If. 
BACKGROUND The system described in this paper is related both to motion analysis and human movement 
simulation. Recorded movements have been used to study human gait (Winter, Greenlaw, and Hobston, 1972) 
and to assist professional athletes in improving their performance (Donkoh, 1980). Computer graphic techniques 
are used to assist in the extraction of movement data from film or videotape. The earliest and most notable 
system of this type is GALATEA, designed and implemented by Futrelle, Fotel, and others at the University 
of Chicago (Futrelle, 1974; Fotel and Mackay, 1980). Baecker, Miller and Reeves (1981) describe a prototype 
laboratory instrument for the interactive motion analysis of videotape records, a system inspired by 
the success of GALATEA. The primary thrust of our system for motion analysis is the visualization and 
analysis of data acquired automatically by optoelectronic means (Woltring, 1974), rather than assisting 
in the acquisition of the data from film or videotape. We use computer graphics to display the shape 
of three-dimensional time histories, to replay them, to segment and transform them for later analysis. 
The curves that we display are analogous to the p-curves described by Baecker (1969) in that the dynamics 
of the movement are captured in the spacing of the dots along the curve. Our second emphasis is on movement 
simulation, which is a rapidly growing field as the recent special issue of IEEE Computer Graphics and 
Applications (Badler, 1982) attests. Badler and Smoliar (1979) provide a survey and taxonomy of techniques 
for digital representation of human movement, and provide an approach for animating a realistic human 
body in terms of a network of special-purpose processors. In addition, there has been some work in the 
area of modeling sign language. Flatt and Badler (1981) describe an intergrated system for representing 
and simulating human facial movements, the face being one channel of ASL communication. Badler and Morris 
(1982) present a method for representing non-rigid articulated objects and apply it to modeling the shape 
of the hand used in ASL signs. Finally, Shantz and Foizner (1982) present a method for generating ASL 
signs based on a stick figure model. The goal of much of the previous work in human movement simulation 
has been realism, either in the rendering of th'e human figure or in the underlying control mechanisms. 
Zeltzer (1981), for example, views the control of an animated figure as a problem of robotics. Our emphasis 
is on a simple stick figure model, manipuated directly. We emphasize simplicity for two reasons. First, 
we were interested in having a working model fairly quickly, and a simple abstraction is easier to implement. 
Second, movement is critical to sign language. Movement itself conveys the essential grammatical information 
of the language and this information is well conveyed when the display is reduced to only a few moving 
points of light (Poizner, Bellugi, and Lutes-Driscoll, 1981). A stick figure model, while conveying the 
configuration of the hand, allows focus on movement, and in particular, on the subtle and complex movements 
crucial to American Sign Language. The model described in this paper is a direct descendant of a model 
implemented in BASIC on a Hewlett-Fackard 9830-A microcomputer (Shantz and Poizner, 1982). The difficulty 
with the earlier system was the non-interactive and two-dimensional  nature of the display. Functions 
defining how Joint angles changed over time were input via a data tablet, but the resulting motion could 
not be viewed immediately, and then only in two dimensions. We found that the task of specifying positions 
and movements can be difficult even with a three-dimensional interactive display. We describe some techniques 
for improving procedures for analyzing and synthesizing three-dimensional movement. The capacity to synthesize 
ASL signs could well allow major advances in the experimental investigation of the perception of sign 
 language. III. COMPUTER ENVIRONMENT The software described in this paper runs on a PDP-11/34 running 
Bell Laboratory's Unix operating system (Bell, 1978). The graphics hardware consists of an Evans and 
Sutherland Picture System If, along with three banks of sixteen function switches, a bank of eight dials 
and a data tablet. The availability of the function switches dictated the nature of the operater interface. 
Most programs are controlled via function switches rather than using the tablet and software generated 
menus. I The flowchart in Figure I shows the relationship between the various data files and programs 
in the system. The double boxes represent areas which are outside the scope of this paper but are shown 
here for completeness. The large boxes represent interactive graphics programs. The small rectangular 
boxes represent filters, programs which map data from one format into another format. The circles represent 
data files, of which there are several different types. The abbreviations for the various data files 
are expanded in Table I. The system divides naturally into four subsystems, which we refer to as SELD, 
ASLSYN, DYNO, and STIM. SELD displays reconstructed sign language movements acquired using a Selspot 
movement monitoring system. This component allows the investigator to interactively segment and transform 
the data for later analysis. ASLSYN allows the user to interactively specify synthetic sign language 
movements by manipulating the angle functions of a jointed model. In addition to the action file editor 
there are programs for creating target files, which aid the synthesis procedure, and for compiling action 
files. Each frame in a compiled action is a list of points to be displayed, rather than the underlying 
joint angles. DYNO provides a technique for manipulating the temporal characteristics of sign language 
movements. STIM allows the building of complex stimuli and real-time stimulus sequencing. Stimuli can 
consist of a number of actions and objectS, displayed for a specified number of frames. Stimuli are stored 
in a high performance swapping region of the disk, allowing real-time playback. The various subsystems 
interact in several ways. Selspot data can be converted to compiled action files and can COMPONENT RELATIONSHIPS 
 Seld~  Aslsyn~ .... I filtars I ~ Figure 1. Relations among components of the analysis, synthesis 
and stimulus construction programs. Table 1. Data File Definitions 4pt Raw Selspot data for all four 
points (shoulder, elbow, wrist and finger). sld Selspot data for two points, plus velocity and acceleration 
graphs. seg Segment information; begin and end points, minimum, average and maximum velocities and accelerations, 
etc. act Action file. Consists of joint angle functions and static joint information. cac Compiled action 
file. Each frame consists of a list of vectors to be displayed. mat Transformation matrix files. dyn 
Dynamic functions, used to modify the temporal characteristics of sign language movements. ob Object 
definitions. cmd Command files for stimulus generation and sequencing. stim Stimulus files, composites 
of multiple action files and object files. then be displayed as stimuli. Compiled actions can be converted 
to Selspot data format and analyzed using SELD. Selspot data can be converted to target point arrays, 
which are used to aid in the synthesis of sign language movement. IV. COMPUTER-GRAPHIC THREE-DIMENSIONAL 
RECONSTRUCTION OF ASL--SELD Actual sign language movement  trajectories are acquired using the Selspot 
movement monitoring system. The components of this system are a control unit, two special cameras, and 
infrared LEDs attached to a small connector box. The LEDs are taped to limb segments, and the connector 
box is strapped to the body. The control unit pulses up to 30 LEDs at a rate of 315 Hz., and synchronizes 
the camera output with the pulsing. We record signing with four LEDs attached to the dominant arm of 
the signer, at the index fingertip, wrist, elbow, and shoulder. The Selspot cameras have special detector 
plates and electronics which locate the x/y position of an LED spot on the plate. The Selspot control 
unit transmits data to a PDP 11/34 computer through a DMA interface, which automatically reads data into 
successive computer memory locations. At the same time, the computer memory buffer is emptied onto a 
disk to allow large amounts of data to be recorded. A special interface between the control unit and 
the DMA interface allows the computer to limit the data transmitted by transmitting only data for a specified 
number of LEDs and to skip entirely one Selspot cycle to obtain a form of frequency control. The three-dimensional 
coordinates of  an LED are calculated from two cameras by triangulation. First, the data from each camera 
are calibrated for lens and electronic distortions through a table lookup; the tables are specific to 
lens settings, and are formed by exhaustive placement of target LEDs at known locations. Second, the 
data are filtered and missing points are interpolated if desired (missing points occur when an LED goes 
out of view). Third, the three-dimensional coordinates are obtained with the knowledge of relative camera 
 position and orientation. The major advantage of the Selspot system over other movement monitoring 
techniques is the convenience of obtaining the camera coordinates of the LEDs. The movements are also 
unrestrained, with the proviso that the LEDs remain in view; this proviso could eventually be relaxed 
by the addition of more cameras. The accuracy of the system as a whole is fairly good: between 0.5-1.0 
cm within a I cm3 box at a camera distance of 3 meters. The resolution is 0.3 cm at this d~stance, corresponding 
to 10-bit resolution at the cameras. SELD displays the movement trajectories in three dimensions. Trajectories 
for two points can be displayed simultaneously. The three-dimensional structure of the trajectories 
can be visualized using the depth-cueing capability of the Evans and Sutherland Picture System II, by 
viewing the trajectories while they are rotating and by viewing the trajectories in a stereo display 
mode. The stereo display mode consists of a split screen presentation and a stereo hood. For orientation 
a set of labeled axes can be displayed along with the trajectories. A rectangular solid can be drawn 
around the trajectory to delimit the maximum deviation of the movement along each of the three Cartesian 
dimensions. Finally, a three-dimensional calibration index, 50mm on a side, can by displayed to provide 
scale information. The dynamics of the trajectory are captured in the spacing of the dots along the 
trajectory. It is also possible to "replay" the trajectory. In this mode a bar is displayed between the 
points on the two trajectories at a given time. When the time at which the bar is displayed is incremented 
steadily, it appears as if the bar is moving along the original trajectory. The rate at which the bar 
moves is controlled by varying the amount that time is incremented or by varying the number of frames 
that each time is displayed. The velocity and acceleration along the path is determined by calculating 
the distance along path (by summing individual interpoint distances) as a function of time, differentiating 
this function to get velocity along path, and differentiating velocity along path to get acceleration 
along path. A five point Lagrangian interpolation polynominal is used to perform the numerical differentiation 
(Hildebrand, 1956). A graph of velocity and acceleration along path can be displayed for either trajectory 
on the right hang side of the screen. A sweeping axis is displayed at the time at which the bar in the 
trajectory display is being drawn. By manipulating a dial the investigator can control one of two time 
pointers. In this mode, a bar is drawn on the trajectory display and the velocity/acceleration graph 
at the time selected by the active pointer. The nonactive pointer is shown by a dashed axis on the velocity/acceleration 
graph. The investigator selects which time pointer is active using a function switch. Once the investigator 
has identified an interval of interest he can make that interval a window by selecting a function switch. 
This alters ("blows up") the time scale on the velocity/acceleration graph and limits the movement of 
the time pointers. Function switches allow the operator to display the data in the current interval, 
in the window or to display all of the data for either trajectory. At any time the operator can create 
a file for the current interval containing a definition of the segment (begin and end points) and a number 
of  calculated parameters (minimum, average and maximum velocity and acceleration, distance along path, 
etc.). Another feature of the SELD program is the ability to transform the data using the "write back 
to memory" feature of the Picture System. The coordinates in which the data are acquired are often not 
the coordinates of interest to the investigator. The investigator can orient the data so that it is 
aligned along major planes of interest, and then transform the data to the new coordinate system. We 
will illustrate aspects of this interactive display system in the computer graphics reconstruction of 
some American Sign Language grammatical inflections. In Figure 2, two grammatically inflected signs are 
shown together with their movement paths reconstructed in three dimensions. The left panels illustrate 
line drawings of the basic sign LOOK 2 and its form under two inflections for temporal aspect. The basic 
sign LOOK is made with a single outward movement (not shown). One inflected form of the sign is LOOK[ 
Cntinuative], which changes the meaning to 'look for a long time.' The form of the sign under the Continuative 
inflection (shown in the upper left panel) is tense rapid outward movement with an elliptical slow return 
to the starting point. The reconstructed movement of the f~ngertip, along with associated A B velocity 
and acceleration curves is shown in the middle panel. The time pointers on the velocity and acceleration 
graphs were positioned to delineate the first of two movement cycles. Characteristics of this first cycle 
are shown in the upper right hand panel of the figure. In that panel, the time pointer was positioned 
at the point of maximum velocity. The correspondence of that point with the spatial path is indicated 
by the bar in the trajectory display. A second inflectional form is illustrated in the lower left hand 
panel of the figure: LOOK[ Durational]. Under this inflectional form, the meaning changes to 'look continously.' 
The form of the sign under the Durational inflection is a smooth circular even movement, which is repeated 
several times. The middle panel illustrates the movement of the Durational inflection. The right hand 
panel displays the first repetition cycle, corresponding to the time window established by the pointers 
in the middle panel. This procedure allows the segmentation of sign movements into relevant linguistic 
components. In summary, the SELD program provides an extremely powerful tool for visualizing and analyzing 
the large volume of data generated by the Selspot movement monitoring system. C  LOoK(Continuative] 
 LOoK[Durational ] Figure 2. Three-dimensional reconstructions of two ASL inflections. A) line drawings; 
B) the entire reconstructed movement with time pointers 'windowing' one movement cycle; C) characteristics 
of one movement cycle. V. COMPUTER SYNTHESIS OF AMERICAN SIGN LANGUAGE ASLSYN. D~NO - In order to model 
certain linguistic processes, it is necessary to computer-synthesize ASL forms. A signer simply cannot 
produce forms that are exactly equal in trajectory shape, in duration, or in any other physical variable; 
neither can he produce a series of forms that are separated physically by exactly the same increment. 
However, such precise control over the form of stimuli is of crucial importance for many psyeholinguistie 
experiments. In synthesizing signs, we manipulate a representation of the skeletal linkages of the arms 
utilizing the interactive capacities of the Picture System. We can readily access our data on the physical 
characteristics of actually-oecuring sign movement, and perform identical three-dimensional analyses 
(e.g., calculations of velocities, accelerations, etc.) on the sign movements that we generate. Thus, 
we work back and forth between motions that we synthesize and characteristics that we know are critical 
to the form. ASLSYN is an interactive action file editor. An action file consists of a number of angle 
functions, although in the general case any degree of freedom could be represented in these functions. 
An action file has a fixed duration; independent utilities exist for resealing and altering the "time 
scale." A user enters an action by specifying all of the angles at a number of key frames; the rest of 
each function is interpolated linearly. Any angle function can also be modified by drawing in a new function 
on a data tablet. For purposes of selecting angle functions, we divided the angles into a number of categories, 
each containing from two to four angles. The angle group is selected using function switches and all 
of the angles in an angle group can be manipulated using dials. Only one angle can be selected for tablet 
input with this angle selected by toggling a function switch. The relationships among the various angle 
functions are specified in a skeleton definition. A skeleton definition is a table of skeleton operation 
codes and operands. By externalizing the structure of the model, we created a tool which is capable of 
modeling a number of different movements, of which upper arm motion is just a particular instance. This 
feature has been used to handle various types of arms; in one case we completely define the arm and hand, 
in another we model just the arm and index finger. In some cases, particular angles are not dynamic, 
they remain the same throughout the movement. In these cases it i8 possible to eliminate the angle function, 
using instead a fixed value. This generality is implemented by including a map of what angles are represented 
as functions. When an action file is input, the skeleton definition is also input. The definition is 
scanned for operands which specify dynamic angles. If the dynamic angle is actually a static angle, the 
skeleton definition is changed from a  dynamic operation to a static operation and the static value 
is inserted into the skeleton definition. Figure 3 shows the structure of action files. ACTION FILE 
STRUCTURE number of frames minimum values maximum values static values number of functions function map 
 functions Figure 3. The structure of action files. Skeleton definitions were designed so that a single 
action file could be played on either arm. This was done by having two types of rotation operations, 
mirrored and unmirrored. In a mirrored rotation the sign of the angle changes depending on the side that 
the action is being played on. ASLSYN is also capable of doing a different action on each arm.  While 
the three-dimensional interactive technique of specifying action files was an improvement over the two-dimensional 
system, the task was still a difficult one. It can be awkward to manipulate a three-dimensional stick 
figure using dials to control various joint angles, since it is often difficult to specify the various 
positions in a movement without some guidelines. Because of these problems, we developed two techniques 
to simplify the task. The first of these techniques is target points. Target points are an array of 
fingertip positions, one for each frame in the action. Target points can be  specified in one of three 
ways: the array can be drawn using a data tablet, elliptical arrays can be specified by entering an aspect 
ratio (many signs have elliptical paths), or actual sign data can be used from Selspot digitization. 
In the first two methods, the array must be planar. We have not found this to be too limiting a constraint. 
 After specifying the target point array, the user positions the array relative to the simulated signer. 
The array can be translated in x, y and z and rotated into correct orientation. We found that a rotate-by-90 
degrees switch simplified the process of orienting the target array. It was also important to show the 
direction and starting point of the target array. This was done by drawing the first quarter and the 
third sixth of the points as solid lines (see Figure 4). The starting point in Figure 4 is at eleven 
o'clock and the direction of motion is clockwise. o .    " Figure 4. Target points with orientation 
lines. Once the target array has been positioned correctly, the task of entering the angle functions 
is greatly simplified. To correctly position the figure at any point in time, the fingertip has to be 
moved to the target point. Without this constraint it is difficult to determine where the arm should 
be at any given time. The user has only to make sure that the arm is positioned "correctly," since there 
is more than one possible arm position when the fingertip is at the target point. In the future we hope 
that the tedious task of actually moving the fingertip to the target point will be done by the computer. 
The user would correctly position the arm while the computer would "hold" the fingertip to the target 
point (see also Badler, O'Rourke and Kaufman, 1980 for a working system with target capability). The 
second technique for simplifying skeleton positioning was user-defined dials. We found that in order 
to position the fingertip, it was necessary to select the elbow group, tweak the elbow angle, select 
the shoulder group, tweak the shoulder angles, and so on, back and forth between the two angle groups. 
Since we had additional dials, we added a feat ure whereby the user could map any angle to one of the 
additional dials using a command. Being able to simultaneously tweak one of the elbow angles and one 
of the shoulder angles simplified the task of positioning the arm. Figure 5 shows stroboscopic photographs 
of synthesized movements of the hand along the target path together with a plot of how one angle function 
changes over time. Comniled Actions. Because of the computation involved, we could only achieve a rate 
of 10 frames per second with a single fully articulated arm and hand. This rate was too slow for work 
in the DYNO program so an alternate representation was required. Using the "write back to memory" feature 
of the Picture System, it is possible to get the three-dimensional coordinates of the various Joints 
and fingertips. Because the number of points and the number of frames is small, it is posible to store 
an entire motion in memory. Since there is essentially no computation involved, it is possible to achieve 
a rate equal to the update rate of the monitor, 30 or 40 frames a second. Thus, an action is specified 
using the angle function representation and then "compiled" for the high speed displays required for 
manipulating dynamics. Dynamic Functions (DYNO). A given linguistic movement can have different meanings 
depending on the dynamics of the motion, as indicated above. We use the concept of a dynamic function 
to manipulate this variable. An action is defined in the frame domain and displayed in the time domain. 
Dynamic functions map time domain into frame domain. That is, for a given time, a dynamic function specifies 
the frame to be displayed. A typical dynamic function is illustrated in Figure 6. By definition the function 
starts at (0,0) and ends at (f,t), where f is the number of frames in the action and i is the length 
of time the action is to be played. The dashed diagonal line represents the "null" dynamic function, 
where the actual playback rate is the rate at which the action is defined. The solid line is the dynamic 
functlon. Where the slope of the function is shallow the action is slow, where the function is steep 
the action is rapid. During a hold the dynamic function is perfectly flat. The dynamic function is used 
to determine what frame to display at each point in time. At time &#38;, frame ~ is shown. At time ~, 
frame ~ is shown. During a hold the same frame is shown over a number of time increments, and during 
rapid motion, frames are skipped. We are interested in how signers perceive distinctions between ASL 
 Figure 5. Stroboscopic photographs of synthesized signs showing the hand moving along target points. 
 inflections as the movement dynamics gradually change, in equal physical increments from that specifying 
one inflection to that specifying another. In order to investigate this perceptual phenomenon, we developed 
a program for interpolating dynamic functions. Given two dynamic functions, we are able to generate intermediate 
functions. This process was not just averaging the values of two frames at any given point in time, 
but rather involved the following technique. We vary the frame coordinate from ~ to ~. At each step we 
find the corresponding times in the two functions and average them, generating a point on the interpolated 
function, by using a weighted average we can vary the interpolated function smoothly from one function 
to another. For functions with a hold at either end there is no unique time corresponding to the initial 
or final frame. For initial holds we average the time at which the hold terminates; for final holds, 
we average the time at which the hold begins. The generated points are connected to form the interpolated 
function. defined time ( cac frame number)  /// II a actual time (msec) Figure 6. A typical dynamic 
function.  The dynamic function is used in two different ways. In the first, the dynamic function is 
used to select a frame from a static array of frames. If the dynamic function specifies frame 5.6, the 
program displays frame 5. This is adequate for previewing the effects of different dynamic functions 
on a given action. In the second, we use dynamic functions to modify actions, interpolating when the 
dynamic function specifies non-integral frame numbers. In this case a new action file is generated. We 
refer to this as "precise interpolation", and the filter which performs this function is called PINT. 
PINT can also be used with null dynamic functions to rescale action files, changing the number of frames 
which define an action. Figure 7 illustrates a continuum of dynamic functions. The upper panel of Figure 
7 presents seven dynamic functions equally spaced along a continuum. The left-most function produces 
a movement modification which conveys one particular grammatical inflection in ASL (the Intensive, meaning 
'extremely'). The right-most function conveys a different inflection (the Resultative, meaning 'become 
fully'). The five intermediate functions produce movement modifications intermediate in form between 
the two. The lower panel illustrates the nature of the movement modifications produced by each of the 
seven functions. The upper (continuous) row of dots represents the movement without any dynamic function 
applied. The action starts at the left of each row and moves to the right, with each dot representing 
one frame. The second row of dots (int) reflects the nature of the action after the left-most dynamic 
function was applied. The first and final frames are held, only a few of the intermediate frames are 
displayed. The rest of the rows show the selection of frames dictated by the remaining dynamic functions. 
The movements produced by each of the dynamic functions will be used in psychollnguistic experiments 
that investigate how signers discriminate movement changes in ASL. Figure 7. A continuum of dynamic 
functions and their effect on a single action. Stimulus Generation and DisplaY. The ultimate products 
of the simulation system are videotapes of objects and actions to be used as stimuli in perceptual experiments. 
 Each stimulus is a composite of several different actions and objects. The same object or action can 
be instantiated in several different locations. Stimuli are built up using commands, either interactively 
or using scripts (batched commands). The program which builds stimuli is called BSTIM, and the program 
which displays stimuli is called DSTIM. There are also various utilities which store the stimuli in 
an area on the disk for high speed access, and for building the necessary directories for mapping stimulus 
name to disk address. These procedures are essential for the real-time generation of stimuli for psycholinguistic 
experiments. VI. CONCLUSION American Sign Language displays all the complex linguistic structure and 
organization found in spoken language, yet the mechanisms by which the essential grammatical information 
is conveyed are unique to the modality in which the language developed. This grammatical information 
is conveyed by changes in the movement and spatial contouring of the hands and arms. We have presented 
procedures for the three-dlmenslonal reconstruction and manipulation of actual sign movements and techniques 
for synthesizing American Sign Language signs and grammatical processes. These procedures incorporate 
interactive techniques for the visualization and manipulation of signs. This system greatly enhances 
our ability to investigate the perception of sign language, much as computer synthesized speech has aided 
the investigation of speech perception. However, the study of sign language has the added advantage that 
the movements of the articulators are directly observable. The study of sign language thus provides an 
especially promising vehicle for investigating the relationship between the articulatory and perceptual 
properties that underlie linguistic structure. REFERENCES Badler, N. I., Ed. Modeling the human body 
for animation. (Special issue devoted to the topic). IEEE ComPuter GraDhics and AoDlications, ~, ~, (November 
 1982). Badler, N. I. and Morris, M. A. Modelling flexible, articulated objects. Proceedln~s of the ONLINE 
Conference, (1982). Badler, N. I., O'Rourke, J. and Kaufman, B. Special problems in human movement simulation. 
ComPuter GraPhics (Proceedings of SIGGRAPH 1980), 14, 3, (July, 1980), 189-197. Badler, N. I. and Smollar, 
S. Digital representation of human movements. ACM Comoutin~ Surveys, 11, (March 1979), 19-38. Baecker, 
R. M. Picture-drlven animation. Proceedings of the SDrin~ Joint Comouter Conference, (May 1969), 273-288. 
 Baecker, R. M., Miller, D. and Reeves, W. Towards a laboratory instrument for motion analysis. Comouter 
Graphics. 15, 3 (August 1981), 191-197. Bell System, UNIX time-sharing system (entire issue devoted 
to UNIX). The Bell System Technical Journal, 57, 6, Part 2 (July-August 1978). Bellugi, U. The structuring 
of language: Clues from the similarities between signed and spoken language. In U. Bellugi and M. Studdert-Kennedy 
(Eds.), Si~ned and Svoken Language: Biological Constraints on Linguistic Form. Dahlem Konferenzen. Weinhelm/Deerfield 
Beach, Fla.: Verlag Chemie, (1980), 115-140.  Bellugi, U. and Studdert-Kennedy, M. (Eds.). Si~ned and 
Spoken Language: BioiQg~al Constraints on Linguistic Fgrm. Dahlem Konferenzen. Weinheim/Deerfield Beach, 
Fla.: Verlag Chemie, (1980). Donkoh, S. Computer analysis helps train atbeletes. CIPS Review., (July-August 
1980), 14-15. Futrelle, R. P. GALATEA: Interactive graphics for the analysis of moving images. Information 
Processin 74. Proceedings of the 1974 IFIP Conference, North-Holland Publishing  Company, 712-716. 
Futrelle, R. P. and Potel, M. J. The system design for GALATEA, an interactive real-time computer graphics 
system for movie and video analysis. ~omp~ers and Granhics, i, Pergamon Press, (1975), 115-121. Hildebrand, 
F. B. ~nSroduction to Numerical AnalYsis. (1956). Johansson, G. Visual perception of biological motion 
and a model for its analysis, perception and Psvchoohvsics, /_~, (1973), 201-211. Johansson, G. Visual 
motion perception. Scientific American, 2~2, 6, (1975), 76-89. Klima, E. S. and Bellugi, U. The Signs 
of Language. Cambridge, Mass.: Harvard University Press, 1979. Platt, S. M. and Badler, N. I. Animating 
facial expression. ComPuter GraPhics, 15_, 3, (August 1981), 245-252. Poizner, H. Visual and 'phonetic' 
coding of movement: Evidence from American Sign Language. Science, 212, (1981), 691-693. Poizner, H. 
Perception of movement in American Sign Language: Effects of linguistic structure and linguistic experience. 
Perception and ~svchophvsics, 33_, (1983), 215-231. Poizner, H., Bellugi, U. and Lutes-Driseoll, V. 
Perception of American Sign Language in dynamic point-light displays. Journal of Experimental Psychology: 
Human ~erQeption and Performance, ~, (1981), 430-440. Poizner, H., Newkirk, D. and Bellugi, U. Processes 
controlling human movement: Neuromotor constraints on American Sign Language. Journal of Motor Behavior, 
15_, (1983), 2-18. Potel, M. J. and Mackay, S. A. Graphics input tools for interactive motion analysis. 
Computer Graphics and Image Processing, /~_, (1980), 1-23. Shantz, M. and Poizner, H. A computer program 
to synthesize American Sign Language. BehaviQr Research Methods and Instrumentation, /_~, 5, (1982), 
467-474. Winter, D. A., Greenlaw, R. K. and Hobson, D. A. Television-computer analysis of kinematics 
of human gait. Computers and Biomedical Research, i, (1972), 498-504.  Woltring, M. J. New possibilities 
for human motion studies by real-time light spot position measurement. Biotele~e$~y, ~, (1974), 132-146. 
 Zeltzer, D. Motor control techniques for figure animation. IEEE Computer Graphics and Applications, 
~, 9, (November 1981), 53-60. ACKNOWLEDGEMENTS This work was supported in part by National Science 
Foundation Grant #BNS81-11479 and National Institutes of Health Grant #HD13249 to The Salk Institute 
for Biological Studies, and National Institute of Health Research Grant AM 26710, awarded by the National 
Institute of Arthritis, Metabolism, and Digestive Diseases to the Massachusetts Institute of Technology. 
 We are indebted to Dr. Robert Livingston for use of the Evans and Sutherland Picture System II. FOOTNOTES 
 I. The graphics library was written by Tom Ferrin at the University of California, San Francisco. 2. 
Words in capital letters (e.g., LOOK) represent English glosses for ASL signs. A bracketed symbol following 
a sign gloss (e.g., LOOK [Cntinuative]) indicates that the sign is made with some change in form associated 
with a change in meaning from its unmodulated form, and thus indicates grammatical changes on signs. 
  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801140</article_id>
		<sort_key>115</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[INCENSE]]></title>
		<subtitle><![CDATA[A system for displaying data structures]]></subtitle>
		<page_from>115</page_from>
		<page_to>125</page_to>
		<doi_number>10.1145/800059.801140</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801140</url>
		<abstract>
			<par><![CDATA[<p>Many modern computer languages allow the programmer to define and use a variety of data types. Few programming systems, however, allow the programmer similar flexibility when displaying the data structures for debugging, monitoring and documenting programs. <italic>Incense</italic> is a working prototype system that allows the programmer to interactively investigate data structures in actual programs. The desired displays can be specified by the programmer or a default can be used. The default displays provided by Incense present the standard form for literals of the basic types, the actual names for scalar types, stacked boxes for records and arrays, and curved lines with arrowheads for pointers. In addition to displaying data structures, Incense also allows the user to select, move, erase and redimension the resulting displays. These interactions are provided in a uniform, natural manner using a pointing device (<italic>mouse</italic>) and keyboard.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>E.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.2.5</cat_node>
				<descriptor>Debugging aids</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011074.10011099.10011102.10011103</concept_id>
				<concept_desc>CCS->Software and its engineering->Software creation and management->Software verification and validation->Software defect analysis->Software testing and debugging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003152.10003161</concept_id>
				<concept_desc>CCS->Information systems->Information storage systems->Record storage systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10010031</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Data structures design and analysis</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011024.10011028</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language features->Data types and structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010070.10010111.10011710</concept_id>
				<concept_desc>CCS->Theory of computation->Theory and algorithms for application domains->Database theory->Data structures and algorithms for data management</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10002952.10002971</concept_id>
				<concept_desc>CCS->Information systems->Data management systems->Data structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P31915</person_id>
				<author_profile_id><![CDATA[81100013136]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brad]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Myers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xerox Palo Alto Research Center, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ahlberg, J. H., Nilson, E. N., and Walsh, J. L. The Theory of Splines and their Applications. New York: Academic Press (1967).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>811152</ref_obj_id>
				<ref_obj_pid>953064</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Baecker, Ron. Two Systems which Produce Animated Representations of the Execution of Computer Programs. ACM SIGCSE Bulletin. Vol. 7, No. 1 (Feb. 1975). pp. 158-167.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Baecker, Ron. Sorting Out Sorting. 16mm color, sound, film. 25 minutes. Dynamic Graphics Project, Computer Systems Research Group, University of Toronto, Toronto, Ontario (1981). Presented at ACM SIGGRAPH Conference, Dallas, Texas (Aug., 1981).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Balzer, R. M. EXDAMS &#8212; EXtendable Debugging and Monitoring System. Proceedings AFIPS Spring Joint Computer Conference. 34 (1969). pp 567-580.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Christensen, Carlos. An Example of the Manipulation of Directed Graphs in the AMBIT/G Programming Language. Proceedings of the ACM Symposium on Interactive Systems for Experimental Applied Mathematics, Washington, D.C. (August, 1967).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>361591</ref_obj_id>
				<ref_obj_pid>355604</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Dijkstra, Edsger W. The Humble Programmer. Communications of the ACM. Vol. 15, No. 10 (Oct 1972). pp 859-866.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Dionne, M. S. and Mackworth, A. K. ANTICS: A System for Animating LISP Programs. Computer Graphics and Image Processing. Vol. 7 (1978). pp. 105-119.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[English, W. K., Engelbart, D. C. and Berman, M. L. Display Selection Techniques for Text Manipulation. IEEE Transactions on Human Factors in Electronics. Vol. HFE-8, No. 1 (March 1967).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Goldberg, A. and Robson, D. A Metaphor for User Interface Design. Proceedings of the 12th Hawaii International Conference on System Sciences 1979. Vol. 1 (1979). pp. 148-157.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Hain, G. and Hain, K. A general purpose automatic flowcharter. Proc. Fourth Annual Meeting of UAIDE, New York (Oct. 1965). pp. IV-1 to IV-12.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Henderson, D. Austin. A Description and Definition of Simple AMBIT/G&#8212;a Graphical Programming Language. Wakefield, MA: Massachusetts Computer Associates CA-6904-2811 (April 28, 1969). 32 pages.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Herot, Christopher P., Brown, Gretchen P., Carling, Richard T., Friedell, Mark, Kramlich, David, and Baecker, Ronald M. An Integrated Environment for Program Visualization. Proceedings of the IFIP WG 8.1 Working Conference on Automated Tools for Information System Design and Development. New Orleans, LA (January 26-28, 1982). H. J. Schneider and A. I. Wasserman (eds). North Holland, Amsterdam, 1982.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Hopgood, F, R. A. Computer Animation Used as a Tool in Teaching Computer Science. Proceedings of the 1974 IFIP Congress, Applications Volume. (1974) pp 889-892.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>574137</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Jensen, K. and Wirth, N. PASCAL User Manual and Report. Englewood Cliffs, N.J.:Prentice-Hall (1975).]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Knowlton, K. C. L6: Bell Telephone Laboratories Low Level Linked List Language. Two black and white films, sound. Bell Telephone Laboratories, Murray Hill, New Jersey (1966).]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>367620</ref_obj_id>
				<ref_obj_pid>367593</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Knuth, Donald E. Computer Drawn Flowcharts. Communications of the ACM. Vol. 6 No. 9 (Sept, 1963). pp. 555-563.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Laaser, William. Private conversation with the author (Nov. 9, 1979). System was built using DLISP which is described in Warren Teitelman. Display Oriented Programmer's Assistant. Palo Alto: Xerox PARC CSL-77-3 (March 8, 1977).]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>359789</ref_obj_id>
				<ref_obj_pid>359763</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Liskov, Barbara, Snyder, Alan, Atkinson, Russell, and Schaffert, Craig. Abstraction Mechanisms in CLU Communications of the ACM. Vol. 20, No. 8 (Aug, 1977). pp. 564-576.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Mitchell, James, et al. Mesa Language Manual, Version 5.0. Palo Alto: Xerox PARC CSL-79-3 (1979).]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Myers, Brad A. Displaying Data Structures for Interactive Debugging. Palo Alto: Xerox PARC CSL-80-7 (June, 1980). 97 pages.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Rosen, Brian. PERQ: A Commercially Available Personal Scientific Computer. IEEE CompCom Digest (Spring, 1980).]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988122</ref_obj_id>
				<ref_obj_pid>988113</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Shoch, John F. An Overview of the Programming Language Smalltalk-72. ACM Sigplan Notices. Vol. 14, No. 9 (Sept 1979). pp. 64-73.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Sweet, Richard. Appendix B: Implementation Description. Empirical Estimates of Program Entropy. Palo Alto: Xerox PARC CSL-78-3 (1978). pp. 85-96.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Thacker, C. P., McCreight, E. M., Lampson, B. W., Sproull, R, F., and Boggs, D.R. Palo Alto: A Personal Computer. Palo Alto: Xerox PARC CSL-79-11 (August 7, 1979). 50 pages. Paper also appears in Siewiorek, Bell and Newell, Computer Structures: Readings and Examples, second edition.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>540004</ref_obj_id>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[van Tassel, Dennie. Program Style Design, Efficiency, Debugging and Testing. Englewood Cliffs, NJ: Prentice-Hall, Inc. (1974). 256 pages.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Yarwood, Edward. Toward Program Illustration. University of Toronto Computer Systems Research Group Technical Report CSRG-84 (M. Sc. Thesis) (October 1977).]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 INCENSE: A SYSTEM FOR DISPLAYING DATA STRUCTURES Brad A. Myers* Xerox Palo Alto Research Center, California 
ABSTRACT Many modern computer languages allow the programmer to define and use a variety of data types. 
Few programming systems, however, allow the programmer similar flexibility when displaying the data structures 
for debugging, monitoring and documenting programs. Incense is a working prototype system that allows 
the programmer to interactively investigate data structures in actual programs. The desired displays 
c~/n be specified by the programmer or a default can be used. The default displays provided by Incense 
present the standard form for literals of the basic types, the actual names for scalar types, stacked 
boxes for records and arrays, and curved lines with arrowheads for pointers. In addition to displaying 
data structures, Incense also allows the user to select, move, erase and redimension the resulting displays. 
These interactions are provided in a uniform, natural manner using a pointing device (mouse) and keyboard. 
CR Categories and Subject Descriptors: D.2.2 [Soft- ware Engineering]: Tools and Techniques -Structured 
Programming; User Interfaces; D.2.5 [Software Engineering]: Testing and Debugging -Debugging Aids; Monitors; 
D.3.3 [Programming Languages]: Language Constructs -Abstract Data Types; E.1 [Data Structures]; I.Y4 
[Computer Graphics]: Graphics Utilities; 1.3.6 [Computer Graphics]: Methodology and Techniques Interaction 
Tedmiques. General Terms: Design, Human Factors, Languages. *Author's current address: Three Rivers Computer 
Corporation, 720 Gross Street Pittsburgh, Pennsylvania, 15224 Permission to copy without fee all or part 
of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. &#38;#169; ACM 0-89791-109-1/83/007/0115 
$00.75 I. Introduction Many modern computer languages allow the programmer to define and use different 
data types. Few programming systems, however, allow the programmer similar flexibility when displaying 
these data structures in debugging, monitoring and documenting programs. Incense, a system written in 
and for the Pascal-like language Mesa [19], allows the programmer to design and use pictorial representations 
for the display of data structures. 1 Strongly typed languages, such as Pascal [14] and Mesa, allow the 
programmer to define new types using the basic types supplied by the language. These new types can then 
be used to declare variables. Other languages, such as CLU [18] and Smalltalk [22], have the definition 
of types and their operations as the central programming paradigm. A data display system like Incense 
would be a great asset to programmers using a language of either type. Modern Computer Science theory 
promotes the value of information hiding, where only the relevant information at a particular level should 
(or can) be accessed at that level. Most modern languages promote this method of programming. When debugging, 
however, the programmer is typically restricted to viewing the data at a very basic level. Just as the 
user of a real number does not want to have to examine the bit patterns used to represent the number, 
the user of a Ring Buffer or Hash Table generally does not want to see the records, arrays, and pointers 
used to implement them. Incense therefore allows the programmer to define the display for any type and 
have that display used whenever data of that type is shown. Since a programmer may not want to define 
displays for all (or possibly any) of the types used in his program, an extensive set of default displays 
is also provided. llncense was designed and implemented at t_he Xerox Palo Alto Research Center by the 
author. A full description can be found in [2Ol. While the importance and utility of user-defined displays 
is clearly evident for any computer environment, the ability to create genuine pictures enhances Incense's 
effectiveness considerably. The importance of pictures can be understood by looking at a typical programmer's 
bulletin board or note pad. When debugging a program or explaining it to another person, the programmer 
is likely to draw a plethora of pictures representing typical cases or those under consideration. These 
pictures allow the situation to be more easily understood. It is dear, for example, that Figure 1 is 
easier to understand than Figure 2, and that the programmer is more likely to visualize and draw the 
former. A guiding principle of Incense is to automatically create displays that would be similar to those 
the programmer might have drawn on paper. []  [] [] Figure 1. Pictorial representation of a POINTER 
TO INTEGER 2. @1275:14 @1276:25 p: 1276t @1277:37 @1278:85 Figure 2. Typical display for a POINTER TO 
INTEGER in a character oriented system. In some cases an even more pictorial representation than Figure 
1 may be useful. For example, bar graphs, icons, tables and analogical pictures 3 are much more evocative 
than the numbers on which they are based. Frequently, these pictures will provide the right amount of 
detail and can be understood much more quickly. For example, a percent-done thermotneter (Figure 3), 
as a representation for an iteration variable, allows instant recognition of how much of the loop has 
been completed. With the progress in computer hardware, creating these pictures for debugging has become 
increasingly economical. Many personal computers, such as the Xerox Alto [24] and the Three Rivers PERQ 
[21], come with powerful graphic displays that can be easily used by all programs to make dynamic pictures. 
2This figure and all others in this paper except 2, 3, 4 and 18 were created by Incense and taken directly 
from the screen. 3pictures understood by analogywith the physical world. Figure 3. "Percent-done thermometer" 
as an analogical representation for an iteration which is 80% complete. The desired situation would 
be for the computer to create or verify programs automatically and thereby eliminate the need for debugging 
and for the displays of data structures. Dijkstra claims that good programmers could avoid all debugging 
if they would only use struc- tured programming techniques [6]. Debugging, however, is still very much 
a problem for programmers. In fact, van Tassel claims that "a bug-free program is an abstract theoretical 
concept" [25, p. 117]. The programmer currently spends a great portion of his debugging time trying to 
understand the state of the machine. If the computer automatically produced pictures of the data structures, 
the programmer would be freed of one of the more laborious parts of the debugging process. Thus, better 
tools for dcbugging are needed. Since the necessary support hardware is now generally available, the 
time is ripe for a development of systems that support pictorial, user-defined displays of data structures 
such as provided by Incense. II. Previous work. The utility of graphical representations for program 
concepts has long been known. Flow charts were an early attempt at representing computer information 
graphically. In addition, it was felt that they should be produced automatically by a computer since 
the hand made flowcharts were expensive and slow to produce, inaccurate when drawn, and hard to maintain 
[16][10]. A number of systems have been built which can create pictures for a program after the execution 
has finished. Usually, code must be added to the source program to define the output and then the program 
must be run through some sort of pre-or post-processor (examples are [4][26][3]). Some excellent algorithm 
demonstration movies have been produced using this technique [3][13], frequently at a fairly high cost. 
Other systems have produced high quality dynamic real-time displays for a limited set of types. For example, 
there have been a number of tree display programs (e.g. [23]) and LISP list displays [7][17], but these 
are typically restricted to a single, simple type of value at each node. Knowlton [15] describes a system 
for L-6 that displays the language's arbitrary field definitions. III. The Environment for Incense. Incense 
was written on the Alto personal computer at Xerox PARC. The Alto has a 875 by 603 pixel raster scan 
display and a "mouse" pointing device with three buttons. A cursor on the screen follows the mouse's 
movements. Incense was written for the Mesa language. The compiler for Mesa produces extensive symbol 
tables for each program that provide full type information for all constants and variables. IV. The 
Incense system. Unlike the systems described above, Incense can automatically generate pictures for any 
data type during the execution of an actual Mesa program. This is done in real time without modifications 
to the source program. The user need only specify at debug time the string name of a variable to get 
a full pictorial display. If desired, new, pictorial displays can easily be created and associated with 
data structures. These user-defined disph/ys can eliminate unnecessary detail or more graphically depict 
the abstraction that the data structure is implementing. The pictures are then displayed in real time 
while the program is running. Monitoring ("animation") or static visualization of program execution at 
a high conceptual level is therefore possible. All input from the user is supplied in a similar high 
level by using the "mouse" pointing device to draw rectangles or select displays. A unique feature of 
Incense is the ability to provide multiple displays, called Formats, for a single data structure. The 
user might provide both iconic and detailed representations so that detail can be eliminated when not 
needed but still be available to implementers. For each Format, the user can supply a number of Subformats, 
one of which is chosen automatically by Incense based on the amount of screen space given for the particular 
display.  V. Design Considerations for Incense. In designing a system to implement the ideas mentioned 
above, two major problems had to be solved. First, an appropriate structure had to be designed that would 
be sufficiently powerful to allow very complex displays to be created and used. It was important, however, 
that this design make it easy to define and use simple pictures. Second, a design for the display of 
pointers had to be formulated. Pointers are a special problem since the space used to display the referent 
has to be specified. For records and arrays, the subpieces all fit neatly inside the box for the aggregate, 
but for pointers, the referent is displayed outside of the pointer's box. to discover the types of all 
variables in programs using the Mesa compiler's symbol tables. This makes it possible for Incense to 
automatically display any structure given only the name of the variable that holds it. In order to have 
an element of data displayed in Incense, an Artist must be associated with it. An Artist is a collection 
of procedures and data that handles the display, erasure, and modification of the data. An Artist is 
created for each aggregate structure (such as a record) and for each data item, even if the data item 
is a field of an aggregate structure. The association between the data and the Artists to display them 
is maintained auto-matically by Incense. "['he memory address and type of the data, as well as other 
information, are stored as part of the Artist's internal data. Five classes of procedures are also stored 
in each Artist. There are procedures for displaying the data, drawing arrows, erasing pictures and arrows, 
selecting Artists, and editing the associated data structures. Each of these will be discussed below 
in a separate section.  VI. Display. Although pictorial representations are easier to understand, they 
are not always appropriate. For example, the user of a Ring Buffer may want only to see the display of 
Figure 4, but the implementer may need to see the underlying basic types. Incense supports multiple views 
of the same data through the use of Formats. Each Artist has one or more Formats, each of which displays 
the data in a different manner. For example, an Artist for a Time record (Figure 5) might contain Formats 
for displaying itself as a record (a) or a clock (b). Formats are intended to be used for radically different 
ways of viewing a data structure. The implementer of the Artist defines the Formats, but the user specifies 
interactively which Format to use in a particular display. If an implementer wants to protect the internal 
representation of a data structure from the client, he can supply an Artist that only has a Format to 
display an iconic representation. The caller of an Artist Format procedure specifies the area in which 
the display must fit_ Thus the user 4 always specifies the size and position of the display, and Artists 
must be prepared to fit into any size rectangle. Other data display systems have used very different 
strategies. For example, in AMB1T/G, the displayed objects always take a constant amount of space [5], 
and in Smalltalk, the objects themselves decide how much space to take [9]. The advantage of the Incense 
approach is that the user's choice is never pre-empted, and aggregate structures, such as records and 
arrays, can accurately specify the position and size of subpa~. As part of the solution to the fi~t problem, 
Artists 4In the case of nested Artists used by aggregates such as records are used, as discussed below. 
Layouts were invented to and arrays, this is done by a procedure, as explained below. solve the problems 
with pointers (discussed in section 7). A run-time type system, described in [20], allows Incense To 
increase the flexibility of the displays and allow all control to be procedural, each Format contains 
one or more Subformats. Once the user specifies a Format, the system then chooses one of that Format's 
Subformats. This decision is based on various contextual information such as the size of the area in 
which the display is to fit. For example, the standard Artist Format for a Mesa record has two Subformats 
(Figure 6). The first displays the data at full size (a). If insufficient room for the display is provided, 
however, the second Subformat would be invoked. It scales the record display so that it will retain the 
same proportions as the original, yet still fit in the box (b). The client is not allowed to specify 
which Subformat should be used. Instead, the creator of the Artist associates a test with each Subformat. 
This test determines whether the Subformat is applicable in the current context. Since more than one 
of these tests may Figure 4. succeed, the designer also specifies an ordering of the Possible display 
for a ring buffer. Subformats. If none of the Subformat tests succeed, the (This figure was not created 
by Incense). data is simply not displayed. This may happen, for example, if the area in which the data 
is to be displayed is very tiny. The basic types of Mesa, including STRINGS, hours: 16 ~in: 25 [se,=,:,nJ 
s: 30 ] INTEGERS, CARDINALS (positive INTEGERS), BOOLEANS, CHARACTERS, REALS, PROCEDURES, UNSPECIFIEDS, 
WORDS, and Enumerated Types (lists of names, e.g. {Mon, Tues, Wed, Thurs, FRO), all have default displays 
which use (a) (b) text strings of the same form as the literals used by the programmer. These are fully 
interpreted as can be seen Figure 5. in Figure 7. Mesa uses a special representation for Two Formats 
for the record Artist for: Time: RECORD [hours, min, seconds: INTEGER]; holding the time of day: (a) 
as a normal record and (b) as an analog clock. ,_~.: 3 b: ',:~ =a: - 2 int~rnalRec: zb: ;c: This is TRUE 
a ~st r~ -~l---: d: kl (a) (b) Figure 6. Two Subformats for a record: full size (a), and scaled proportionally 
and centered vertically inside a bounding rectangle (b). 11,466916e+i.~l ~ ~ [] UNSPEC,F,EO ,NTEGER REAL 
CARDINAL BOOLEAN CHARACTER WORD STRING ENUMERATED PROCEDURE Figure 7. Default boxed display for the 
basic types. 118 subranges of the above types. Incense hides this transformation so the programmer always 
sees the values he would have typed into the program. There are two Formats for Artists of each of the 
basic types. The first draws a box around the value (as in Figure 7). This is useful when the value is 
standing alone or when there is a pointer to it (see Figure 1). The other Format does not draw a box 
around the value. This is used in aggregate structures such as records and arrays when boxes are drawn 
around the entire field. For each Format of each of the basic types, there are two Subformats. One displays 
the value as a text string. This string is allowed to be clipped slightly (Figure 8 (a) and (b)). If 
there is not enough room for a meaningful portion of the value to be displayed, the second Subformat 
will be used which displays a grey area which has a size proportional to the string that would have been 
displayed (Figure 8 (c) and (d)). (a) (b) (c) (d) Figure 8. Demonstration that clipped strings do supply 
information: (a) and (b) are values of BOOLEANS. Grey areas have different lengths proportional to the 
size of the string: (c) is for FALSE and (d) is for TRUE. The Subformats of some Artists, such as those 
for records, arrays, and pointers, can cause the display of subordinate Artists. For example, the automatically 
supplied Artist Subformat for a record will iterate through the fields calling the appropriate Format 
in the Artist for each. The record Subformat divides the rectangle allocated for the display of the record 
among the fields giving an appropriate portion to each. Thus, the subordinate Artists are called with 
the same types of arguments as the top level Artist, and the record Subformat need not know, for example, 
whether the subordinate is an integer, pointer, or even another record (see Figure 9). c, rmatSet: NN 
]::,roc5; t, yl-',eID: eu_-tdr: la: 3 pe~ent; ca: -2 ,displayed: TRUE cb: This is a t#st displayUsed: 
F:~:.- I [nternalRec: cc; TRUE Iselected; FALSE aa~v, AbsPos: aayAbsPos: J: kl (a) (b) Figure 9. Two 
ways records can contain other records. Full size (a) and reduced (b). The diagonal lines in (b) represent 
the pointer value Nil. Each record field actually consists of two Artists: one for the field's name 
and one for the field's value. The field's name and the value's type are discovered automatically using 
the information from the Mesa symbol tables [20]. The field name is displayed using an Artist to make 
the interface more consistent. This has the additional advantage that standard STRING Artists can be 
used for the names. The field names therefore become grey if the area is too small and can be selected 
and redisplayed in the same manner as all other values (see section 9). It has been suggested that the 
field names should have special Formats that center the name vertically in the area provided, or display 
it at the top of the field's value. This would be a trivial modification and could, in fact, be done 
by the user. Arrays are handled in exactly the same manner as records, except, of course, that there 
are no field names. Special Artists could be created to provide the indices of the values and/or to allow 
scrolling so that large arrays could be easily handled. Both record and array Artists store, as part 
of their internal data, the set of rectangles used to specify the fields' positions. These rectangles 
are defined at Artist creation time based on the types of the values, and the length of the field name 
strings for records. Making arrays display vertically or horizontally is therefore accomplished simply 
by defining the rectangles with the appropriate offsets (Figure 10). Thus two-dimensional arrays are 
easily displayed (Figure 11). The rectangles and other static information stored in the Artists are called 
Form Data since they describe the form of the picture. Other information stored in an Artist, called 
Display Data, is generated when the Artist's associated data structure is displayed. The Display Data 
includes such things as the current screen position of the the picture and whether or not it is selected. 
.If . -,'.. .q z, .~ '1" " 'r I-6 'I-.s I-4 14 I (b) Ca) Figure 10. Arrays oriented vertically (a) and 
horizontally (b). Figure 11. Two dimensional array: ARRAY [1 _3] OF ARRAY [1 _4] OF INTEGER;  VII. Displaying 
Arrows. One of the most complex aspects of the Incense design was choosing the best method for displaying 
pointers. It was clear that they should be shown as arrows, but the difficulty was deciding where to 
put the data to which the pointer points (the referent). The simplest technique would have been to require 
the user to specify where each referent should go. For deep structures, however, this would be very tedious. 
The simplest automatic placement scheme was used by AMBIT/G where the referent simply appeared at a pre- 
determined, fixed position relative to the parent irrespective of what might have been there previously 
[11]. This method is not consistent with Incense's philosophy of hierarchical rectangles and user control. 
The most complextechnique would be to treat the screen space like a heap memory and simply allocate and 
free rectangles of display area. This has the potential for maximal usage of the space. Unfortunately, 
this two-dimensional space allocation problem is very complicated and prone to poor performance. The 
specialized space allocation techniques used in tree and list drawing programs are also unworkable since 
they assume that all nodes will be the same size. This is certainly not necessarily the case in Incense. 
The algorithm finally chosen for Incense is very general and fast, but it does not allocate the screen 
space as well as the general allocator described above. Layouts were invented to hold the pointer and 
its referents. When the user specifies a rectangle for the display of a Layout, that rectangle is subdivided 
into rectangles to be used by the arrow sources and all referents. Each component rectangle is managed 
by a Layout Field. For example, a Layout for a record containing two pointers would have three fields: 
one for the record and one for each referent (see Figure 12). Layouts and Layout Fields are implemented 
as special Artists that have no associated data; they exist solely to locate and manage the various components. 
A user of Incense need never know of the existence of Layouts and Layout Fields since they are created 
and managed automatically by the system. Of course, since Layouts are a special type of Artist, the user 
is free to create his own type of Layouts if desired. less: j j (fl) ~reol, er: /I data: 4 less: ~res.ter: 
(f0) da~: 10.,~ less: ..I j (f2) ~reater: / I Currently, Layouts use a very simple scheme for assuring 
that all the subcomponents fit into the rectangle specified for the Layout. As with records and arrays, 
the rectangles for the various fields (the Form Data) are fully specified at Artist creation time. When 
the Layout is displayed, the subcomponents are simply instructed to fit into the appropriate Layout Field 
rectangle. In a deep recursive structure such as Figure 13, the displays get progressively smaller as 
the nesting level increases. This theoretically would allow an arbitrarily deep structure to be displayed 
in a finite space, but, in fact, display ceases after the pictures become smaller than a threshold size. 
When a Artist Subformat for a pointer is called, it first checks to see if the pointer's value is NIL. 
If so, a diagonal line is drawn through the box for the pointer. This is consistent with the LISP community's 
usual pictorial representation of NIL. Otherwise, the Subformat checks every Artist on the screen to 
see if any are associated nvith the referent. If so, the pointer is referring to data that has already 
been displayed, and an arrow is simply drawn to this occurrence (Figures 14 and 15). This mechanism also 
handles the case where a pointer refers to values inside aggregates (Figure 16). If the referent is not 
currently displayed, the pointer Artist must arrange for the correct Layout Field's Artist to display 
the referent so an arrow can be drawn to it. This process can best be explained by using an example. 
Suppose a record contains a POINTER TO CARDINAL and an integer (Figure 17). When the user requests a 
display for a record of that type, the system first notices that it contains pointers. A Layout is therefore 
created with two Layout Fields (Figure 18 gives the Artist hierarchy). The Layout is then ordered to 
display itself in the rectangle specified by the user. The Layout tells the first Layout Field to display. 
This Field, in turn, calls the appropriate Format in the record Artist. Now the record begins to display 
itself in the manner described earlier. When the pointer Artist is finally called, it will discover that 
its referent (the CARDINAL) has not yet been displayed. The Layout responsible for this pointer must 
therefore be found since only it knows where the referent should be placed. The pointer Artist will therefore 
send a message up the Artist hierarchy to find the Layout that will handle its referent. The message 
will go first to the record Artist, then to Layout Field 1, and finally to the Layout. The Layout, upon 
receiving this message, causes Layout Field 2 to be displayed which, in turn, causes an Artist for the 
CARDINAL to be created and displayed. If this had been a more complex structure, such as the recursive 
structure of Figure 13, more Layouts and Artists would have been created and displayed at this point. 
An Artist is not created for a data structure if the pointer to it is NIL. This is obviously important 
for recursive structures since otherwise an infinite number of Artists would be required. Figure 12. 
Layout with 3 fields: one for record: (f0), and one for each referent: (fl) and (f2). ~----21data: 3 
Figure 13. Deep recursive tree display demonstrating how elements get smaller. Overall structure, however, 
is easily understood. Figure 14. ARRAY [1 ..4] OF POINTER with two POINTERS referring to the same value. 
/z~0ata: z A less: ~ I data: 3 less: ~reater: kreater: Figure 15. This erroneous tree structure demonstrates 
that a pointer to previously displayed object does not generate a new copy. The second arrow is drawn 
to the first occurrence.   I (a)l 1 ~-~wei~rrt~ I75 Mye,, Figure 16. Pointer to value inside a record 
(a) does not get confused with a pointer to the record itself(b). pl:int: -2t~5~ Figure 17. Incense display 
for RECORD [int: INTEGER, pl: POINTER TO CARDINAL]. Layout Layout Field i ) Layout field 2 in~ Figure 
18. Artist hierarchy that would be created for: rec: RECORD [pl : POINTER TO CARDINAL, int: INTEGER]; 
(This figure was not created by Incense). IlozTNome: Myers las*.Name: Myers [initial: 'B iniu.sl: 'B 
 Figure 19. Demonstration of the advantage of curved lines used in Incense (a) over straight lines (b). 
The control points used to specify the spline are shown as black squares in (a). (a) Figure 20. Selections 
(shown by black areas) moving up the Artist hierarchy: from record field (a) to record (b) to Layout 
for record (c) to Layout for everything (d). Upon completion of the display of Layout Field 2, the Layout 
returns control to the original pointer Artist with the Layout Artist as the return value. The pointer 
Artist now sends a different message to the Layout requesting the address of the actual Artist associated 
with the CARDINAL. This message is passed down the Artist hierarchy from the Layout to Layout Field 2 
and finally to the CARt31NAL's Artist. If there had been more Layouts, Layout Fields, and records under 
Layout Field 2, as there would be for recursive structures, this message would have had to pass through 
more levels. In any case, the POINTER now knows which Artist is associated with its referent and that 
the referent has been displayed. Finally, the POINTER Artist can find out where the referent is on the 
screen, and an arrow is drawn from the pointer's box to that of the referent. The actual arrow itself 
is a curved line called a spline [1]. At the end of the spline, an arrowhead is drawn. The size of the 
arrowhead is adjusted to insure that it is never bigger than the destination box. The spline is defined 
by seven control points placed along the arrow's path. Three are placed in a straight horizontal line 
near the source and three near the destination (Figure 19). Three are needed at each end to insure that 
the curves approach the end points from the correct direction. Another control point is then placed between 
the ends to make the spline smoother. Splines were chosen rather than straight lines since they are more 
attractive and are less easily confused with other lines in the picture (see Figure 19). The default 
destination points for arrows to Artists of the basic types are at the center of their left side, For 
records and arrays, however, the destination points are calculated as the center of the left side of 
the first field. Since the user is flee to re-arrange the rectangles for the fields (see section 11)~ 
the arrow end points are stored as part of the Form Data. Thus if the fields are rearranged, the user 
can simply supply a new set of destination points for the arrow. When the record or array is displayed, 
the actual screen points for that instance are calculated. This is done before any of the fields are 
displayed in case a field contains a POINTER back to the record or array itself. VIII. Erasure. In addition 
to display, Artists support a number of other operations. One of these is erasure. An erased Artist 5 
can be redisplayed with a new size, location, and Format. There is an erase .procedure for each Subformat 
of an Artist since only the Subformat knows exactly how the data structure was displayed. The Artist's 
erase procedure therefore calls the internal erase procedure for the Format and Subformat with which 
the Artist was displayed. This internal erase procedure will first call the erase procedures in any subordinate 
Artists. Then, the screen area for the Artist will be painted white, 6 and then any arrows are erased 
by redrawing them with white. Arrows must be erased explicitly since they will lie outside of the rectangle 
tbr the POINTER.  IX. Selection. Incense was designed to be used interactively. Therefore, the user 
needs to be able to refer to various displays on the screen to allow them to be erased, redisplayed or 
modified. Many systems require that the user name the display either by tracing from some root (e.g. 
First*.greatert.lessert.greater*.greater*.greatert.value) or by using some arbitrary labels. Fortunately, 
Incense runs on a computer that provides a much more natural method for referring to displays. The mouse 
is a pointing device that moves a cursor around on the screen [8]. All the rectangles for the display 
are specified using the mouse. In addition, pressing one of the mouse buttons tells Incense to select 
the Artist to which the cursor points. The selected Artist is shown visually by video reversal (see Figure 
20). There can only be one Artist selected at a time; however, due to the hierarchical nature of Artists, 
there are typically many Artists at any point on the screen. For example, with an integer in a record 
in a 5"Artists" here and in later sections will be used to refer both to the object that controls data 
display and the picture on the screen that the Artist creates. 6The pictures are drawn in black on a 
white background as shown in this paper. Computer Graphics Volume 17, Number 3 July 1983 ,.'ei Kht: 
175 I tlqo~r,e: M~,ers 1 [initiel: 'B " | 1 I (a) (b) (c) Figure 21. Normal display for record (a), 
form defined by user (b), and resulting display (c). Note that field order has been switched. Layout, 
pointing to the integer could refer to the integer, the record, the Layout Field, or the Layout. Incense 
solves this problem by having the smallest (in screen area) Artist selected. In order to select a larger, 
enclosing Artist, it is simply necessary to re-select the Artist which is already selected. Thus, for 
example, a pointer value in a record field might be selected (Figure 20a). If some other field was then 
selected, the selection would simply move there. If the original pointer was selected again, however, 
the entire record that contained that pointer would be selected (Figure 20b). Next, the Layout containing 
the record would be selected (c), and finally, the Layout enclosing everything would be selected (d). 
If this outermost Layout was selected again, everything would be deselected. Since Incense frequently 
displays values using grey areas, selection is very important. The user can display a large structure, 
select the part of it that he thinks is important, and then have that portion redisplayed using a larger 
area. Since it is so easy to select and redisplay Artists, the decision to use a simple, fast space allocation 
algorithm is shown to be justified.  X. Editing. Once an Artist is selected, its value could be edited. 
7 Unfortunately, editing is not yet supported by Incense. In a future version, the user might select 
the Artist associated with a BOOLEAN variable, type in FALSE, and have the value of the variable change. 
Type checking of the new values would be done along with the appropriate conversions. For pointers, it 
would be appropriate to allow the user to simply point to the display for the data that will be the new 
referent. The system would then discover the correct value for the pointer cell. Type checking would 
be done in this case also. Different styles for specifying new values other than type-in may also be 
appropriate for other specialized displays. It is the job of the edit routines in the Artist to handle 
these trans-formations, the modification of the actual memory, and the subsequent redisplay of the Artist. 
7Editing is used here solely to mean modifying the value of some data and not for modifying its display. 
 XI. User-defined Artists. The discussion of the previous sections has concentrated on the default displays 
provided by Incense. These are necessary for making the system usable and for demonstrating its feasibility 
and power. A major goal of Incense, however, is to make it easy for the user to specify his own Artist 
styles (called Prototypes). This can currently be done at two levels. As was mentioned in section 6, 
aggregate Artists store the rectangles used for the pieces as part of the Form Data. It is very easy 
for the user to specify these rectangles using the mouse (see Figure 21). If desired, the user is asked 
to point with the mouse to the corners of each rectangle needed for the display. The rectangles define 
the relative size and position of the various fields. This technique can be used to eliminate some fields 
(by giving them rectangles of zero size) or to favor the more important fields. It would be useful to 
have a Forms Editor program to help in aligning and positioning the rectangles. If a more radical change 
in display is desired (such as the clock display in Figure 5), the user simply writes a small program 
in Mesa defining the picture. Incense provides a rich variety of display primitives and parameterized 
procedures to aid in this process. Libraries of Artist Prototypes can be kept so that new Prototypes 
could be created through small modifications to existing Artists. It is felt that this is a better strategy 
than requiring the Alxist designer to learn an entirely new language to define the desired pictures. 
Once a Prototype has been created, the user can then associate it with a specific variable, a type, or 
a basic type. Thus, tbr: Time:TYPE = RECORD [hrs, min, sec: INTEGER]; curTime: Time 4- [3, 30, 14]; a 
Prototype can be associated with the variable curTime, the type Time, or the basic type RECORD. This 
gives the user complete control over all displays in Incense.  XII. Current Implementation and Plans 
for the Future. The Incense system described here currently runs only on Xerox Alto mini-computers that 
have 128K of memory and special micro-code to handle floating point operations. Incense has successfully 
demonstrated the feasibility of user-defined and analogical displays for data structures. Default displays 
currently exist for all the basic types of Mesa, two-dimensional displays are used for records and arrays, 
and arrows are used for POINTERS. All of these displays are automatically created using the type of the 
variable, so that no matter how complex a variable is, the user need only type "Display X" and specify 
a rectangle using the mouse. The display of a basic type takes about 350 milleseconds, for a POINTER 
TO INTEGER, 1.9 seconds, and for the complex structure of Figure 13, 33.4 seconds. The mouse is used 
for specifying all rectangles and for selection. The selected Artist can be erased and redisplayed, but 
editing is not currently supported by Incense. Reasonable defaults are computed automatically for all 
field rectangles in aggregate structures, and the user is free to define these rectangles using the mouse 
if desired. The user can also define displays by writing programs in Mesa. Finally, it is easy to convert 
any display on the screen into a form that can be printed as was done for this paper. There are some 
places in Incense where a reimplementation would allow a substantial decrease in execution time and memory 
usage. Also, certain parts of the system, such as the edit command, have yet to be implemented. The major 
aspect of Incense that remains unfulfilled, however, is the creation of special purpose Artists for various 
types. Designs have already been prepared for the displays of trees, list structures, stacks, processes, 
hash tables, ring buffers (Figure 4), large arrays and some other common data types. Iteration variables 
(Figure 3) and other special purpose data items could also have special displays. These would greatly 
enhance the attractiveness of Incense to potential users. Finally, Incense needs to be integrated with 
a full debugger that can take advantage of Incense's power and flexibility. The debugger would handle 
the specification of what data should be displayed, possibly by having the user point at the variable 
in a window holding the source text of the program. The debugger could call Incense frequently to incrementally 
monitor variables and thereby "animate" the program's execution. If this is not practical, Incense could 
simply be called to create static displays at breakpoints. The parameters to Incense would be the variable's 
string name, a symbol table context in which that variable can be found (found based on the run-time 
stack), and a rectangle for display. The rectangle might be supplied by the user or the debugger might 
have a standard window for data display. An integrated environment using Incense-style displays has been 
proposed in [12]. An Incense style display for data structures will be useful to programmers no matter 
how a debugger is organized. The pictures and the associated data structures can be dynamically rearranged 
and modified. The displays from Incense-like systems help the programmer monitor and understand the execution 
of complex programs. Since the user can specify the desired display, the pictures created can be used 
to provide documentation for the data structures themselves. Finally, debugging will probably be more 
enjoyable when using pictures rather than long strings of characters. This, combined with the higher 
conceptual level provided by the pictures, may make the debugging task easier and thereby increase programmer 
productivity.  Acknowledgments. I would like to thank Xerox PARC CSL and the MIT Cooperative program 
for giving me the opportunity to work on this project. Special thanks go to Dan Swinehart and Butler 
Lampson of PARC for their design and implementation suggestions. John Warnock of PARC supplied the underlying 
graphic system that made this work possible and also helped with the design for Layouts. Ed Satterthwaite 
of Xerox SDD provided a great deal of support while I was creating the run-time type system needed by 
Incense. Finally, I would like to thank Dan Swinehart, Warren Teitelman, Ed Satterthwaite, David Reed, 
the referees, and many others for helpful and thorough comments on this paper. REFERENCES 1. Ahlberg, 
J. H., Nilson, E. N., and Walsh, J. L. The Theory of Splines and their Appli-calions. New York: Academic 
Press (1967). 2. Baecker, Ron. Two Systems which Produce Animated Representations of the Execution of 
Computer Programs. ACM SIGCSE Bul-letin. Vol. 7, No. 1 (Feb. 1975). pp. 158-167. 3. Baecker, Ron. Sorting 
Out Sorting. 16ram color, sound, film. 25 minutes. Dynamic Graphics Project, Computer Systems Re- search 
Group, University of Toronto, Toronto, Ontario (1981). Presented at ACM SIGGRAPH Conference, Dallas, 
Texas (Aug., 1981). 4. Balzer, R. M. EXDAMS --EXtendable Debugging and Monitoring System. Proceedings 
AFIPS Spring Joint Computer Conference. 34 (1969). pp 567-580. 5. Christensen, Carlos. An Example of 
the Manipulation of Directed Graphs in the AMBIT/G Programming Language.  Proceedings of the ACM Symposium 
on Interactive Systems for Experimental Applied Mathematics, Washington, D.C. (August, 1967). 6. Dijkstra, 
Edsger W. The Humble Programmer. Communications of the ACM. Vol. 15, No. 10 (Oct 1972). pp 859-866. 
7. Dionne, M. S. and Mackworth, A. K. ANTICS: A System for Animating LISP Programs. Computer Graphics 
and Image Processing. Vol. 7 (1978). pp. 105-119. 8. English, W. K., Engelbart, D. C. and Berman, M. 
L. Display Selection Techniques for Text Manipulation. IEEE Transactions on Human Factors in Electronics. 
Vol. HFE-8, No. 1 (March 1967). 9. Goldberg, A. and Robson, D. A Metaphor for User Interface Desi,~n. 
Proceedings of the 12th Hawaii International Conference on System Sciences 1979. Vol. 1 (1979). pp. 148-157. 
 10. Hain, G. and Hain, K. A general purpose automatic flowcharter. Proc. Fourth Annual Meeting of UAIDE, 
New York (Oct. 1965). pp. IV-i to 1V-12. 11. Henderson, D. Austin. A Description and Definition of Shnple 
AMBIT/G--a Graphical Programming Language.. Wakefield, MA: Massachusetts Computer Associates CA-6904-2811 
(April 28, 1969). 32 pages. 12. Herot, Christopher P., Brown, Gretchen P., Carling, Richard T., Friedell, 
Mark, Kramlich, David, and Baecker, Ronald M. An Integrated Environment for Program Visualization. Proceedings 
of the IFIP WG 8.1 Working Conference on Automated Tools for Infor-mation System Design and Development. 
New Orleans, LA (January 26-28, 1982). H. J. Schneider and A. I. Wasserman (eds). North Holland, Amsterdam, 
1982.  13. Hopgood, F, R. A. Computer Animation Used as a Tool in Teaching Computer Science. Proceedings 
of the 1974 IFIP Congress, Applications Volume. (1974) pp 889-892. 14. Jensen, K. and Wirth, N. PASCAL 
User Manual and Report. Englewood Cliffs, N.J.:Prentice-Hall (1975).  lS. Knowlton, K. C. L6: Bell Telephone 
Laboratories Low Level Linked List Language. Two black and white films, sound. Bell Telephone Laboratories, 
Murray Hill, New Jersey (1966). 16. Knuth, Donald E. Computer Drawn Flowcharts. Commu-nications of the 
ACM. Vol. 6 No. 9 (Sept, 1963). pp. 555-563. 17. Laaser, William. Private conversation with the author 
(Nov. 9, 1979). System was built using DLISP which is described in Warren Teitelman. Display Oriented 
Programmer's Assistant. Palo Alto: Xerox PARC CSL-77-3 (March 8, 1977). 18. Liskov, Barbara, Snyder, 
Alan, Atkinson, Russell, and Schaffert, Craig. Abstraction Mechanisms in CLU Communications of the ACM. 
Vol. 20, No. 8 (Aug, 1977). pp. 564-576. 19. Mitchell, James, et al. Mesa Language Manual, Version 5.0. 
Palo Alto: Xerox PARC CSL-79-3 (1979). 20. Myers, Brad A. Displaying Data Structures for Interactive 
Debugging. Palo Alto: Xerox PARC CSL-80-7 (June, 1980). 97 pages. 21. Rosen, Brian. PERQ: A Commercially 
Available Personal Scientific Computer. IEEE CompCom Digest (Spring, 1980). 22. Shoch, John F. An Overview 
of the Programming Language Smalltalk-72. ACM Sigplan Notices. Vol. 14, No. 9 (Sept1979). pp. 64-73. 
 23. Sweet, Richard. Appendix B: Implementation De-scription.Empirical l:'stimates of Program Entropy. 
Palo Alto: Xerox PARC CSL-78-3 (1978). pp. 85-96. 24. Thacker, C. P., McCreight, E. M., Lampson, B. 
W., SprouU, R, F., and Boggs, D.R. Alto: A Personal Computer. Palo Alto~ Xerox PARC CSL-79-11 (August 
7, 1979). 50 pages. Paper als0 appears in Siewiorek, Bell and NeweU, Computer Structures: Readings and 
Examples, second edition. 25. van Tassel, Dennie. Program Style Design, Efficiency,Debugging and Testing. 
Englewood Cliffs, N J: Prentice=Hall, Inc. (1974). 256 pages. 26. Yarwood, FAward. Toward Program Illustration. 
University of Toronto Computer Systems Research Group Technical Report CSRG-84 (M. Sc. Thesis) (October 
1977).   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801141</article_id>
		<sort_key>127</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Graphical style towards high quality illustrations]]></title>
		<page_from>127</page_from>
		<page_to>135</page_to>
		<doi_number>10.1145/800059.801141</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801141</url>
		<abstract>
			<par><![CDATA[<p>If there is to be widespread acceptance of computer generated images in areas traditionally served by graphic artists, these images must meet a high standard of quality. Document preparation systems are an application area that is gaining maturity in providing high-quality computer typeset documents. These systems exhibit a trend towards specifying the formatting information for a document separately from the body of the text. The goal is to have the document format designed by someone with expert knowledge of typography. Writers can then apply a format to their own work simply by indicating the semantic content of their text, such as the headings, paragraphs, or footnotes. The result is that a writer can produce properly typeset documents without learning the esthetics of typography. This paper extends this idea to encompass the illustrations in the text. We have developed a prototype system that uses a set of graphical style rules to define the design guidelines for the illustrations. The rules, called a <italic>graphical style sheet,</italic> can be used to control a uniform &#8220;look&#8221; over a set of illustrations, or to change the appearance of a particular illustration to reflect different publishing styles or different media. The prototype coordinates with an existing document preparation system and the combined systems were used to produce this paper. We conclude that this is a viable method for controlling image style for at least one class of illustrations. This approach contributes to image quality by providing a method for capturing knowledge of graphic arts standards, and for ensuring a consistent appearance of related illustrations within technical documentation.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Graphic arts]]></kw>
			<kw><![CDATA[Graphic design]]></kw>
			<kw><![CDATA[Graphical style sheet]]></kw>
			<kw><![CDATA[Illustration]]></kw>
			<kw><![CDATA[Integrated text and graphics]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.7.2</cat_node>
				<descriptor>Format and notation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.7.2</cat_node>
				<descriptor>Languages and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.2.9</cat_node>
				<descriptor>Software quality assurance (SQA)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010510.10010512</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document preparation->Markup languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010510.10011689</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document preparation->Document scripting languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003490.10003507.10003510</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Management of computing and information systems->System management->Quality assurance</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003260.10003309.10010512</concept_id>
				<concept_desc>CCS->Information systems->World Wide Web->Web data description languages->Markup languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010510.10010514</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document preparation->Format and notation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Documentation</gt>
			<gt>Management</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39023641</person_id>
				<author_profile_id><![CDATA[81407594230]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Beach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo and Xerox PARC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31096082</person_id>
				<author_profile_id><![CDATA[81100388123]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Maureen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stone]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo and Xerox PARC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[anon., Letraset Catalog, Letraset USA. (1980).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[anon., Graphic Kernel System (GKS) Functional Description, ISO TC97/SC5/WG2 N117 (1982).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807510</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Baudelaire, Patrick and Stone, Maureen, Techniques for Interactive Raster Graphics, Computer Graphics14, 3, (1980).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bell, Edward, Personal communication regarding the production of illustrations within Scientific American (1982).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Chamberlin, D., King, J., Slutz, D., Todd, S. and Wade, B., JANUS: An interactive System for Document Composition. IBM Systems Journal21, 3 (1982).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Dyck, V.A., Lawson, J.D., Smith, J.A. and Beach, R.J., Computing &#8212; An Introduction to Structured Problem Solving Using PASCAL, Reston (1982).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Kernighan, Brian W., PIC - A Language for Typesetting Graphics, Software Practice &amp; Experience12, 1 (Jan. 1982).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1096882</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Knuth, Donald E., TEX and METAFONT, New Directions in Typesetting, Digital Press (1979).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Lampson, Butler W., Pier, Kenneth A., McDaniel, Gene A., Ornstein, Severo M. and Clark, Douglas W., The Dorado: A High-Performance Personal Computer, Three Papers, CSL-81-1, Xerox PARC (January 1981).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801270</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Lipke, Daniel, Evans, Steven R., Newlin, John K., Weissman, Robert L., Star Graphics: An Object-Oriented Implementation, Computer Graphics16, 3 (1982).]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Lesk, Mike E., Typesetting Documents on UNIX and GCOS: Using the -ms Macros with Troff and Nroff, Unix Programmer's Manual2A, 7th ed., Bell Laboratories (1979).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356889</ref_obj_id>
				<ref_obj_pid>356887</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Meyerowitz, Norman and van Dam, Andries, Interactive Editing Systems, Parts I and II, ACM Computing Surveys,14, 3 (1982).]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Mitchell, James G., Maybury, William and Sweet, Richard, Mesa Language Manual CSL-79-3, Xerox PARC (April 1979).]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Paxton, Bill, The State of Cedar, Xerox PARC videotape, V-163 (1982).]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>567449</ref_obj_id>
				<ref_obj_pid>567446</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Reid, Brian K., A High-Level Approach to Computer Document Formatting, ACM Symposium on Principles of Programming Languages (Jan. 1980).]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Ruegg, Ruedi and Frohlich, Godi, Basic Typography, ABC Verlag Zurich (1978).]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Smith, David Canfield, Irby, Charles, Kimball, Ralph and Verplank, Bill, Designing the Star User Interface, Byte (Apr. 1982).]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Van Wyck, Christopher J., IDEAL User's Manual, Computing Science Technical Report No. 103, Bell Laboratories (1982).]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Waltz, David, Artificial Intelligence, Scientific American247, 4 (1982).]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Warnock, John, Personal communication regarding the mapping of prototype rectangular grid designs on to curve paths (1982).]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801297</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Warnock, John and Wyatt, Douglas K., A Device Independent Graphics Imaging Model for Use with Raster Devices, Computer Graphics16, 3 (1982).]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Graphical Style Towards High Quality Illustrations Richard Beach and Maureen Stone University of Waterloo 
and Xerox PARC  Abstract If there is to be widespread acceptance of computer generated images in areas 
traditionally served by graphic artists, these images must meet a high standard of quality. Document 
preparation systems are an application area that is gaining maturity in providing high-quality computer 
typeset documents. These systems exhibit a trend towards specifying the formatting information for a 
document separately from the body of the text. The goal is to have the document format designed by someone 
with expert knowledge of typography. Writers can then apply a format to their own work simply by indicating 
the semantic content of their text, such as the headings, paragraphs, or footnotes. The result is that 
a writer can produce properly typeset documents without learning the esthetics of typography. This paper 
extends this idea to encompass the illustrations in the text. We have developed a prototype system that 
uses a set of graphical style rules to define the design guidelines for the illustrations. The rules, 
called a graphical style sheet, can be used to control a uniform "look" over a set of illustrations, 
or to change the appearance of a particular illustration to reflect different publishing styles or different 
media. The prototype coordinates with an existing document preparation system and the combined systems 
were used to produce this paper. We conclude that this is a viable method for controlling image style 
for at least one class of illustrations. This approach contributes to image quality by providing a method 
for capturing knowledge of graphic arts standards, and for ensuring a consistent appearance of related 
illustrations within technical documentation. Categories and Subject Descriptors: 1.3.3 ]Computer Graphics]: 
Picture/Image Generation-Display algorithms; 1.3.4 ]Computer Graphics]: Graphics Utilities-Picture description 
languages: 1.3.6 ]Computer Graphics]: Methodology and Techniques-Device independence; 1.7.2 ]Text Processing]: 
Document Preparation-Format and notation; languages; photocomposition; J.5 ]Computer Applications] Arts 
and Humanities-A~s~ fine and performing; Additional key words and phrases: Graphic arts, graphic design, 
graphical style sheet, illustration, integrated text and graphics  Introduction If there is to be widespread 
acceptance of computer generated images in areas traditionally served by graphic artists, these images 
must meet a high standard of quality. Increasingly, we see examples such as chart-making systems or spectacular 
special effects where the quality is defined by the traditional graphic arts standards for print, video 
or film media. This paper describes the development of tools to improve the quality of technical illustrations. 
The inclusion of graphic images into computer-typeset documents is an area of current research and development, 
for example, PIC [7], IDEAL [18], JANUS [5], Etude [12], and the Xerox Star [10,17]. Typical illustrations 
which we wish to include are line art and shaded images. Frequently the composition systems which create 
them are text formatters extended to handle the higher quality output and flexibility available with 
typesetters and laser printers. Computer graphics has evolved along two fronts towards quality images: 
the introduction of new output devices and the development of new rendering algorithms. New devices have 
higher resolution and more color capability making it possible to render images more precisely. New algorithms 
that generate smooth curves and more realistic shaded surfaces provide a way to produce high-quality 
images. Now, artists and designers can expect to find opportunities for creative expression within such 
systems. The style of a document is a phrase that conveys several meanings, all related to quality. The 
word style in a thesaurus refers to the ideas of fashion, method, beauty, class, and expression. To a 
graphic designer, the phrase house style refers to the customary way that a particular publishing house 
handles typesetting or illustrations. Permission to copy without fee all or part of this material is 
granted provided that the copies are not made or distributed for direct commercial advantage, the ACM 
copyright notice and the title of the publication and its dat appear, and notice is given that copying 
is by permission of the Association for Computing Machinery, To copy otherwise, or to republish, requires 
a fee and/or specific permission. &#38;#169; ACM 0-89791-109-1/83/007/0127 $00.75 Traditionally to a 
graphic designer, a style sheet communicates to the compositor how to render a document or image [16]. 
A style sheet, such as the one in Figure 1, provides typographic parameters for typesetting text and 
guidelines for achieving certain visual effects. The purpose of a style sheet is to ensure consistency 
and design discipline within a project, and to provide a rapid and effective means for specifying that 
discipline. Computer Graphics Volume 17, Number 3 July 1983 Content Title Heading Text Tables Numbering 
Figure 1. TRADITIONAL STYLE SHEET for specifying typographic parameters. The rows indicate the parts 
of the document to be treated specially. The columns indicate the typographic parameters which the compositor 
uses when typesetting this job. Entries in the matrix are either checkmarks or numeric values. Computer 
typesetting systems have provided style mechanisms for text composition through formatting macros or 
style sheet databases. The '-ms' macro package supplied with TROFF is an example of formatting macros 
that implement document style [11]. SCRIBE uses document types to establish the formatting details for 
a variety of document styles [15]. The Xerox Star uses property sheets to select parameters to control 
the appearance of selected items of text and graphics in documents [10]. In each of these systems, some 
mechanism is provided to manipulate the content of a document separately from the appearance of the document. 
Thus consistency and design discipline can be achieved. This is accomplished without forcing the author 
to become a graphic designer while the graphic designer can supply specialized knowledge to create the 
document style. Extending these formatting style techniques to include graphic images is a natural evolution. 
In the following sections we describe our concept of graphical style for illustrations and also the artwork-rendering 
prototype that we integrated with an existing text composition system. The illustrations we consider 
will be line drawings, although a provision for continuous tone images will also be described. Examples 
of Graphical Style To motivate our concept of graphical style, we present two examples taken from traditional 
graphic arts productions. The first example presents observations on some stylistic aspects of Scientific 
American illustrations. The second example describes how a consistent style was achieved in producing 
a book having many line drawings. Scientific American Illustration Style Scientific American has established 
a reputation for the clarity and effectiveness of its illustrations. In Figure 2 taken from the recent 
article, 'Artificial Intelligence,' in the October 1982 issue of Scientific American [19], we can observe 
several aspects of the Scientific American style. Lines are generally thin, although with different weights 
to convey detail, arrowheads are always the same open design, lettering is always 8-point Helvetica capitals, 
shades of grey and colors are used only when needed and then only to convey essential meaning, and the 
design is clean and carefully crafted. While the author supplies the concept sketch and the illustrator 
renders the image with great skill, it is the art department that ensures that the traditional style 
of Scien.tific American illustrations is maintained [4]. Traditional Illustrated Book Production A recent 
textbook for introductory computer science co-authored and typeset by the first author of this paper 
[6] required a large number of illustrations. Many of the illustrations were listings of computer programs 
and their output. With a suitable typeface and the text files containing the original programs and computer 
output, these figures were easily composed directly into the main body of the book. In contrast, there 
were over 150 line drawings that were hand-drawn by a draftsman. The production of these illustrations 
presented a considerably different problem. Illustration guidelines were written to establish the desired 
style. The authors and the book designer described how various details were to be handled by the draftsman 
for each type of illustration: mathematical graphs, Pascal syntax diagrams, data structure diagrams, 
and simple line drawings. Y f(~) a (a+b)/2 x Trapezoidal Rule for n=l and n=2. Figure 4. SKETCH OF THE 
ILLUSTRATION for Figure 3 represents the basic geomety of the picture. All the rendering information 
for the figure has been reduced to drawing thin lines and using a typewriter-like typeface. The three 
examples of the Trapezoidal Rule have all been produced by using the same TiogaArtwork file but with 
appropriate differences in the style rules. In a manner analogous to the style mechanisms of text formatters, 
there must be an additional means of including semantic notions in an illustration. Just as not all three-space 
indentations are paragraph indents, not all thin lines are axes on a graph. Therefore, graphical style 
must include mechanisms for capturing the intent of the author/illustrator. One way to do this is to 
provide a level of indirection which names the semantic parts of the illustration. The rendering attributes 
and guidelines associated with these names are defined separately in a graphical style sheet. If this 
level of indirection is available, then it is possible to render the same illustration in quite different 
ways by changing only the graphical style definitions. For example, Figure 3 shows two graphical styles 
with thin, clean lines for a typeset book and with wider, bolder lines and colors for a color transparency. 
Rendering Attributes A graphical style sheet must express how an illustration is to be rendered. Basic 
drawing attributes supported in most graphics packages are obvious candidates for specifying how an artwork 
rendering program should produce an illustration. Examples of such attributes appear in the Griffin illustrator 
[3], the GKS standard workstation attribute model [2] and in the Xerox Star basic graphics feature [10]. 
These examples suggest that at least line weight, line patterns, color specification, and caption typography 
parameters be included in any graphical style sheet. Graphic designers frequently rely on mechanical 
aids and transfer sheets to obtain consistent or special effects, suggesting other sources of rendering 
attributes. A standard reference for transfer sheet designs is the Letraset Catalog [1]. Additional rendering 
algorithms can be created to produce some of those effects. For instance, a line in an illustration sketch 
might be rendered by specifying an arrow design in the graphical style sheet to be drawn along that line. 
Similarly, borders or texture patterns might be rendered from details provided in graphical style sheets. 
  The Prototype System Overview To experiment with these ideas of graphical style we developed a prototype 
suitable for a class of technical illustrations The system, named TiogaArtwork, was designed to coordinate 
with the Tioga document preparation system and an interactive illustration program called Griffin [3]. 
The procedure for generating a figure is to make a draft using Griffin and then convert the figure to 
a special kind of Tioga document. Once the figure is in document form it is possible to adjust both the 
style and the content using a combination of Tioga and TiogaArtwork. All of the figures in this paper, 
except the published example in Figure 2, have been produced using this technique. The TiogaArtwork system 
was developed in the Cedar programming environment [14], which is a research project at Xerox PARC. Cedar 
is both a language, derived from Mesa [13], and a computing environment. The hardware for this environment 
is a Dorado processor [9] with a 1024 by 808 pixel bi-level display and, optionally, a 640 by 480 pixel 
color display. A range of medium-to high-resolution printers is available. The highest-quality printers 
produce digitally half-toned images at phototypesetter-compatible resolutions either in black and white 
or as color separations. The design of the prototype is based on features of the Tioga document preparation 
system, so it is necessary to briefly describe Tioga before going into the details of TiogaArtwork. The 
current implementation of Tioga consists of an interactive text editor and a batch-oriented typesetter. 
Documents in this system consist of two files: a file of text nodes and a file of formatting style rules. 
The text nodes in Tioga have a hierarchial structure similar to that of the NLS editor [12]. A document 
is a tree-structured hierarchy of nodes containing text, for example, in section, subsection, paragraph 
order. A normal tree-traversal results in the familiar form of the document. Each node in the document 
can possess certain node properties. The principal use of these properties is to associate the formatting 
style rules with the nodes. Figure 5 represents the Tioga document structure of some surrounding nodes 
of the document for this paper. Document headl "StyleRule I The Prototype System ] head2 StyleRule I 
Overview paragraph StyleRule I To experiment with these ideas... paragraph StyleRule I The system was 
developed . . . head2 StyleRule Geometric Description [ paragraph StyleRule A Tioga document is a tree 
. . . item StyleRule x y .translate --to translate head1 StyleRule I Conclusions l . , = Figure 5. DOCUMENT 
STRUCTURE provides a hierarchy for organizing a text document. This illustration represents the Tioga 
document structure for the nearby sections and subsections of this paper. The boxes represent text nodes 
and the labels above each box represent the node properties. The StyleRule property indicates the formatting 
attributes for first-level headings, second-level headings, paragraphs, and items within a list. The 
style concept in the Tioga formatter is similar to that in SCRIBE [15]. In Tioga, formatting styles are 
defined by a collection of rules contained in a separate file. The style rules specify formatting parameters 
such as type family, style, and size, indentation, interline leading, text justification mode, and composition 
layout parameters. The formatting rules are written in an interpreted language, so it is possible to 
compute parameters during the formatting process. We have designed a way to include nodes containing 
graphics in a Tioga document. These nodes contain a textual representation of geometric attributes such 
as line coordinates, curve control points, positions, and transformations. The style rules associated 
with these nodes specify graphical parameters and rendering attributes. Tioga's node properties are implemented 
as named property lists, so we defined a new property, which we called ArtworkClass, for the illustration 
nodes. The style machinery for Tioga is extensible, so the current set of tools allows us to build on 
the existing mechanisms for manipulating styles and for adding graphical style attributes. It also means 
that our system can use Tioga facilities for text formatting. A document in our system, therefore, is 
a tree of nodes arranged in a hierarchical structure. Some of the branches of the tree represent paragraphs 
of text and some represent illustrations. An illustration is a subtree in the document tree-structure 
with its root node possessing an ArtworkClass property. Nodes within the subtree structure may be either 
graphics or text. Nodes representing subpictures will have the A rtworkC] ass property, while text captions 
within an illustration will have only the standard Tioga properties. This recursive relationship is important; 
it means that our system can use all the text formatting features of Tioga for text inside of illustrations. 
Figure 6 represents the Tioga document node structure for portions of the Trapezoidal Rule illustrations 
in Figures 3 and 4. TiogaArtwork is the part of the system that interprets the graphical nodes and style 
rules. The pictures can be previewed on a display screen using the Cedar graphics package [21] or converted 
for printing. The combination of using the Tioga document structure and representing the graphics as 
text allows TiogaArtwork to interact easily with the Tioga editor and the Tioga typesetter. This is advantageous 
in a number of ways. One advantage is the ease of creating and editing figures. The Tioga editor does 
not react to the ArtworkC] ass property, so the text and style properties for a graphics node can be 
edited in the normal manner. This means that for the prototype experiment we did not have to write a 
special editor to manipulate the graphics, although we did write a conversion routine which translated 
from Griffin format to Tioga node structure and automatically generated style rule properties from the 
Griffin style attributes. Another advantage of using the Tioga document structure for illustrations is 
that it permits us to typeset any text in the illustration. This is accomplished by setting up a call-back 
mechanism between the typesetter program and TiogaArtwork. As the document tree is traversed, the typesetter 
formats text nodes in the normal fashion. Whenever a node with the ArtworkClass property is encountered, 
that entire subtree is passed to TiogaArtwork. If TiogaArtwork subsequently encounters a text node while 
rendering the illustration, it passes the text branch back to the typesetter. This recursion is guaranteed 
to terminate at the end of the tree-path traversal. Trapezoidal Rule Figure ArtworkClass = ArtworkNode 
I position figure [ ArtworkClass = ArtworkNode [ position axis ] t_~ ArtworkPath, axis StyleRule draw 
y -axis ArtworkClass = ArtworkNode [ position axis I I ArtworkPath,axis StyleRule I draw x-axis ArtworkClass 
= ArtworkNode I position curve I L~ ArtworkPath, curve StyleRule draw curve ArtworkClass = ArtworkNode 
[ position area [ L ArtworkPath, areal StyleRule draw area . . . ArtworkClass = AnworkNode I position 
label [ aption StyleRule ~ ArtworkClass = ArtworkNode I position label I edCaption StyleRule ArtworkClass 
= ArtworkNode I position label I I leftCaptionStyleRule . . . Figure 6. ILLUSTRATIONS in TiogaArtwork 
also use the Tioga document structure. The boxes represent the artwork nodes which contain the geometrical 
representation, and the labels above the boxes represent the node properties. Note that StyleRule properties 
exist on both text and artwork nodes. This structure can be extended to encompass pictures composed of 
many subpictures and text captions in a natural hierarchy. The call-back mechanism is also used to pass 
dimensional information between the two programs. The typesetter provides the dimensional parameters 
for the formatted text with which TiogaArtwork can layout the caption within the illustration according 
to the style rule. TiogaArtwork generates the dimensions of the formatted graphics so the typesetter 
can layout the figures with the paragraphs on the page. This document structure and system design gives 
us a great deal of flexibility for experimenting with the best way to represent illustrations in a document. 
The current structure puts only geometric information in the document body and puts all rendering parameters 
.in the style rules. We found that this representation gives a good semantic description of the picture. 
The next sections will describe our current implementation in more detail. Geometric description A Tioga 
document is a tree of nodes. Some of the branches of the tree represent paragraphs of text, and some 
represent illustrations. For text, the levels of the tree represent the section-subsection-paragraph 
hierarchy of the document. For illustrations, the hierarchy represents a relative set of transformations 
for subpictures. Each subpicture in the illustration is drawn relative to the coordinate system established 
by its ancestors. An ArtworkClass node, therefore, may contain a set of coordinate transformations which 
establish the coordinate system for this node and all of its descendant subpictures: x y . transl ate 
--to translate the origin to <x,y> sx sy . seal e --to scale by x,y scaling factors r . rotate --to rotate 
by r degrees A geometrical shape can be composed of straight lines and curves, and a sequence of these 
lines and curves is called a path [21]. A path can represent a line, an area or a clipping region. The 
path definition is provided by commands to draw lines and curves: x y . moveto --establish the current 
path position <cx,cy> as <x,y> x y . 1 ineto --draw a line from the current position to <x,y>, and reset 
<cx,cy> to <x,y> xl yl x2 y2 x3 y3 .curveto --extend the path with a curve which has the four Bezier 
control points <cx,cy>, <xl,yl>, <x2,y2>, and <x3,y3>, and reset <cx,cy> to <x3,y3> For the kind of pictures 
we are working with, the paths and transformations are all that is specified in the nodes of an illustration 
document. The rest of the rendering information will be supplied by the style rules. Figure 7 is an extract 
from the geometric description used for the three instances of the trapezoid rule diagrams in Figures 
3 and 4: For reasons which become apparent during the discussion of rendering algorithms, it is necessary 
to distinguish between transformatibns and path definitions in the contents of an ArtworkClass node. 
It is also convenient to create artwork nodes which specify the file name ofa TiogaArtwork illustration. 
Continuous-tone images stored as files are another category of illustration that can be accommodated 
by the ArtworkClass property. The values of the ArtworkCl ass property are the following: ArtworkNode 
--node contains the textual representation of an illustration which is not a 15ath, such as transformations 
A r two r k P at h --node contains the geometric definition of a path ArtworkImage --node contains the 
name of a continuous-tone image file stored as an array of intensity samples ArtworkFi 1 eName --node 
contains the name of another TiogaArtwork file Basic style parameters Style rules are described in an 
interpreted language with each format rule expressed as a procedure definition. A typical rule describes 
a list of parameter-value pairs which will be stored in a global association list. The general format 
for this in the Tioga editor is: (name-of-style-rule) "commentary describing the style" { value parameterName 
value parameterName value parameterName } StyleRule The name-of-styl e- rule refers to some semantic 
aspect of the illustration such as axis, curve, or nonterminal. Values are either numbers or keywords 
and may be expressions. Values which are distances may be expressed in most % TiogaArtwork figure for 
Trapezoid Rule % Cluster 1 0 0 .translate i I .scale 0 .rotate % y-axis 11 31 .translate 1 t .scale 
0 .rotate 1 1 .moveto 1 185 .lineto % x-axis 3 39 .translate 1 1 .scale 0 .rotate 1 1 .moveto 249 1 
.lineto % curve 27 87 .translate 1 1 .scale 0 .rotate 1 1 .moveto 8 17 15 33 25 49 .curvete 43 78 71 
106 105 113 .curveto 131 118 161 110 185 97 .curveto 194 92 201 87 209 81 .curveto % area from a to (a+b)/2 
51 39 .translate 1 1 .scale 0 .rotate 1 1 .moveto 1 97 .lineto 81 161 .lineto 81 1 .lineto 1 1.1ineto 
% area from (a+b)/2 to % y-axis label 8 216 .translate 1 1 .scale 0 .rotate Y % x-axis label Z47 36 
.trans]ate 1 1 .scale 0 .rotate x Figu~ 7. GEOMETRIC REPRESENTATION of an illustratien in a textual ~rm 
consists of comments, t~ns~rmatioes, and path definitions. The T~pezeidal Rule illustration was fi~t 
drawn with the GHllln illust~r [~. It was then automatically couveded into a TiogaAdwo~ document gene~ting 
the node stmctuR and node prepe~ies from the Griffin illust~tion file. The indentation iudica~s the nOde 
stmctuR. The comments, which be~u with percent signs, were added manually by editing the document ~xt. 
convenient units. For instance, 2-point line weight can be expressed as 2 p t, colors can be expressed 
by keyword color names such as red, darkBrown, or l ightB] ue, and relative colors can be evaluated as 
some percentage (such as 75 percent) of the brightness or saturation of a named color. The following 
basic set of drawing attributes were defined as graphical style parameters: 1 i n eWe i g h t the line 
thickness pathType the path area/outline type: filled, outlined, fil led+outl ined penType the pen shape: 
round, square, rectangular, elliptical, italic penHeight the pen height as a proportion of l i n eWe 
i g h t penWidth the pen width as a proportion of 1 i n ewe i g h t penAngle the rotation of the pen, 
in degrees from horizontal areaColor the color of filled areas: hue, saturation, brightness outlineColor 
the color of outlines: hue, saturation, brightness The following set of style attributes are supplied 
by the existing formatter hut are interpreted by the artwork-rendering software for illustration captions 
and labels: family the type family, such as Helvetica or TimesRoman size the type size face the type 
style: regular, italic, bold, and bold+italic captionFormat the text justification mode: flushLeft, fl 
ushRight, centered, or justified captionAlign the vertical text justification mode: flushTop, centered, 
basel ine, or fl ushBottom lineLength the length of caption lines leftIndent the left indent for captions 
rightIndent the right indent for captions leading the spacing between lines textRotation the rotation 
of the text line, in degrees from horizontal textColor the color of caption text: hue, saturation, brightness 
To demonstrate both the style language and the graphical style attributes, Figure 8 shows the two style 
definitions necessary for Figure 3. Rendering the illustration TiogaArtwork translates our representation 
of an illustration into a set of calls on the Cedar graphics package [21]. The graphics package implements 
a full set of transformation and clipping operations. Shapes are described as a set of analytical outlines 
that are filled either with a flat color or an image. The geometry in our documents consists of paths 
and transformations. The transformations translate one-for-one into calls on the graphics package. The 
path definitions are compatible with the outline description required by the graphics package. Thus, 
if a node contains a path with the p a t h Ty p e = f i l 1 e d, then the path represents a closed area 
to be colored with a reaCt I o r and can be easily rendered. If the pathType is outlined, then it represents 
the center-line of a line or pen stroke. A thick line is drawn along this path as defined by the l ineWeight 
and pen parameters. Predefined pen shapes are round, square, rectangular, elliptical, and italic. Other 
style parameters control the aspect ratio and rotation of the pen shapes. The graphics package does not 
currently support a pen semantic, so TiogaArtwork reduces this description to an outline. This definition 
of a thick line is similar to that of a stroke in Metafont [8]. Extended style semantics This basic graphical 
style machinery can be extended to express more complicated style semantics. The following examples of 
shadows, arrows, and borders are based on common graphic arts practice.  Shadow Styles Two-dimensional 
shadow effects can be created to emphasize an object. Two simple examples of shadow effects are drop 
% TrapezoidBook. Style % TrapezoidSlide. Style BeginStyle BeginStyle (BasicGraphics) AttachStyle (BasicGraphics) 
AttachStyle (BasicText) AttachStyle (BasicText) AttachStyle (axis) "x,y axes" { (axis) "x,y axes" { black 
outlineColor white outlineColor outlined pathType outlined pathType 1 pt lineWeight 2 pt lineWeight } 
StyleRule ) StyleRule (areal) "dark areas" { (areal) "dark areas" { grey areaColor orange areaColor 
filled pathType filled pathType } StyleRule } StyleRule (area2) "light areas" { (area2) "light areas" 
{ lightGrey areaColor lightYellow areaColor filled pathrype filled pathrype } StyleRule } StyleRule 
(curve) "function line" { (curve) "function line" { black outlineColor white outlineColor outlined pathType 
outlined pathrype 2 pt lieeWeight 4 pt lineWeight } StyleRule } StyleRule (lertCaption) "..." { (leftCaption) 
",.." { "rimesRoman" family "Helvetica" family 8 bp size 12 bp size italic face bold face flushLeft captionFormat 
flushLeft captionFormat flushTop captionAlign flushTop captionAlign 0 lertIndent 0 leftIndent } StyleRule 
white textColor } StyleRule (centeredCaption) "..." { (centeredCaption) "..." { (rightCaption) "..." 
{ (rightCaption) "..." { EndStyle EndStyle Figure 8. GRAPHICAL STYLE SHEETS for the two Trapezoidal Rule 
illustrations in Figure 3 demonstrate the style language and the graphical style attributes. The style 
on the left produces a typeset book quality illustration and the style on the right produces a colored 
35 mm. slide form. Note that the styles differ in the line weights, color selections, and typography 
parameters. shadows and offset shadows, shown in Figure 9. A drop shadow appears to give an object depth 
by extending a slanted shadow line away from the object. An offset shadow gives emphasis by showing an 
underlying copy of the object a short distance away. The style parameters for shadow effects are the 
following: shadowType the shadow effect: drop or offset shadowPathType the offset shadow path type: filled, 
outlined, filled+outlined s h a d owA n g I e the angle of the shadow from the object, in degrees shadowOffsetAmount 
the distance that the offset shadow is placed at shadowAngle shadowDi rection the direction of the shadow 
from the object: upLeft, upRight, downLeft, downright shadowWeight the weight of the drop shadow or outline 
of the offset shadow shadowAreaColor the color of the drop shadow or offset shadow area shadowOutl i 
neCol o r the color of the offset shadow outline   it is possible to define generic style rules, that 
can be referenced in specific style rules to specify the common set of attributes. Tools are needed to 
provide an effective interface to this style machinery. Illustrations are often produced by executing 
other graphics programs. Frequently these programs have little facility for providing graphical style. 
Many times the illustrations produced must be redrawn or the programs fine-tuned to generate publication-quality 
results. Some mechanism for capturing the images produced and for supplying graphical style semantics 
would be most helpful in incorporating such illustrations in documents. Another interesting topic is 
style guidelines which apply to the layout and design of illustrations rather than to simple rendering 
parameters. It can be convenient, for example, to express box dimensions as a function of the size of 
the text in the box. The size of the text depends on its font style, so there needs to be some way of 
specifying this relationship between the style and the geometry. In another example, the actual endpoint 
of an arrow changes when it is pointing to something which is rendered with a thick line than a thin 
one. Style rules might also include document layout parameters. For example, the style of an illustration 
could control the layout so that the figure might have a horizontal orientation when the document is 
typeset with wide columns, a vertical one when narrow columns are used, or a fixed aspect ratio to suit 
videotape or slides. A dynamic reconfiguration capability would be helpful in making the illustrations 
look better in the chosen layout.  Acknowledgements The sensitivity to graphic arts quality and design 
issues in this project is due in part to George Roth, a graphic designer who collaborated on a variety 
of typesetting projects, including the Computing text book [6]. With enthusiasm and insight, he often 
asked "Why can't you program it to do this?" while offering suggestions as to the appearance of the final 
result. Both the design and implementation of Tioga document preparation system are critical to our work. 
Bill Paxton, Michael Plass and Scott McGregor are responsible for Tioga, and we would like to publicly 
thank them for their contributions to the graphical style project. We also thank John Warnock and Doug 
Wyatt for the Cedar graphics package and other graphical tools used in our prototype. One of the principal 
features of Cedar is that it is an integrated environment. In other words, it is very easy to use pieces 
of the existing system in a new application. Our work uses this feature extensively, so we were able 
to build our prototype system in rather short time. Therefore, we will simply thank all of the Cedar 
implementors for their contributions. References [1] anon., Letraset Catalog, Letraset USA. (1980). 
[2] anon., Graphic Kernel System (GKS) Functional Description, ISO TC97/SC5/WG2 Nl17 (1982). [3] Baudelaire, 
Patrick and Stone, Maureen, Techniques for Interactive Raster Graphics, Computer Graphics 14, 3, (1980). 
[4] Bell, Edward, Personal communication regarding the production of illustrations within Scientific 
American (1982). [5] Chamberlin, D., King, J., Slutz, D., Todd, S. and Wade, B., JANUS: An interactive 
System for Document Composition. IBM Systems Journal 21, 3 (1982). [6] Dyck, V.A., Lawson, J.D., Smith, 
J.A. and Beach, R.J., Computing --An Introduction to Structured Problem Solving Using PASCAL, Reston 
(1982). [7] Kernighan, Brian W., PIC - A Language for Typesetting Graphics, Software Practice &#38; 
Experience 12, 1 (Jan. 1982). [8] Knuth, Donald E., TEX and METAFONT, New Directions in Typesetting, 
Digital Press (1979). [9] Lampson, Butler W., Pier, Kenneth A., McDaniel, Gene A., Ornstein, Severo M. 
and Clark, Douglas W., The Dorado: A High-Performance Personal Computer, Three Papers, CSL-81-1, Xerox 
PARC (January 1981). [10] Lipke, Daniel, Evans, Steven R., Newlin, John K., Weissman, Robert L., Star 
Graphics: An Object-Oriented Implementation, Computer Graphics 16, 3 (1982). [11] Lesk, Mike E., Typesetting 
Documents on UNIX and GCOS: Using the -ms Macros with Troff and Nroff, Unix Programmer's Manual 2A, 7th 
ed., Bell Laboratories (1979). [12] Meyerowitz, Norman and van Dam, Andries, Interactive Editing Systems, 
Parts I and II, ACM Computing Surveys, 14, 3 (1982). [13] Mitchell, James G., Maybury, William and Sweet, 
Richard, Mesa Language Manual CSL-79-3, Xerox PARC (April 1979). [14] Paxton, Bill, The State of Cedar, 
Xerox PARC videotape, V-163 (1982). [15] Reid, Brian K., A High-Level Approach to Computer Document Formatting, 
ACM Symposium on Principles of Programming Languages (Jan. 1980). [16] Ruegg, Ruedi and Frohlich, Godi, 
Basic Typography, ABC Verlag Zurich (1978). [17] Smith, David Canfield, lrby, Charles, Kimball, Ralph 
and Verplank, Bill, Designing the Star User Interface, Byte (Apr. 1982). [18] Van Wyck, Christopher J., 
IDEAL User's Manual, Computing Science Technical Report No. 103, Bell Laboratories (1982). [19] Waltz, 
David, Artificial Intelligence, Scientific American 247, 4 (1982). [20] Warnock, John, Personal communication 
regarding the mapping of prototype rectangular grid designs on to curve paths (1982). [21] Warnock, John 
and Wyatt, Douglas K., A Device Independent Graphics Imaging Model for Use with Raster Devices, Computer 
Graphics 16, 3 (1982). 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801142</article_id>
		<sort_key>137</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[The simulation of natural phenomena (Panel Session)]]></title>
		<page_from>137</page_from>
		<page_to>139</page_to>
		<doi_number>10.1145/800059.801142</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801142</url>
		<abstract>
			<par><![CDATA[<p>This panel will discuss the issues and the problems associated with the simulation of natural phenomena. This is a difficult area of research since it generally involves complex data bases and in many instances time variant phenomena. The computational loads can become enormous as one considers the physics or the mathematical modeling of structures.</p> <p>Most items in nature, trees, clouds, fire and comets being some examples, have not been displayed realistically in computer graphics. This lack stems from a few different problems, all of which are significant. The first is the fact that realistic portrayals require large amounts of storage and consequently large compute time. Nature is able to create diverse detail at the most minute levels within an object of grandoise scale. The second problem is that of diversity of design within a given framework. For example, if a scene requires two dozen poplar trees, how does the designer construct trees that look different but are undeniably poplars? Humans typically become tired after the first few iterations of such a design process, with a resulting degradation in the subsequent models. Clearly, this problem applies to all of the phenomena mentioned above. Finally, there is a lack of models. First, second and third order representations are commonly used in computer graphics to model various kinds of surfaces and their boolean combinations. However, their applications to objects, which do not lend themselves well to being described as surfaces has not been addressed sufficiently.</p> <p>Previous attempts at realism have dealt with the appearances of the surfaces being modeled, in terms of their illumination or relief. More recently, fractal methods have introduced a new degree of realism into terrain modeling systems. However, it appears that natural phenomena will require more research into the fundamental way things occur in nature, and in terms of computer graphics, their representation will build on previous work, but will still require new modeling techniques.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Fractals</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P329435</person_id>
				<author_profile_id><![CDATA[81100394889]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Charles]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Csuri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ohio State University]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P131723</person_id>
				<author_profile_id><![CDATA[81100294395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Blinn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Jet Propulsion Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39072448</person_id>
				<author_profile_id><![CDATA[81332501356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Julian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gomez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39044822</person_id>
				<author_profile_id><![CDATA[81100480335]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Nelson]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Max]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lawrence Livermore National Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P334457</person_id>
				<author_profile_id><![CDATA[81100228626]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reeves]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lucasfilm Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>360353</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Blinn, James F. and Martin E. Newell, "Texture and Reflection in Computer Generated Images", Comm. ACM, Vol 19 (10), pp. 542-547 (October, 1976)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>908845</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Blinn, James F. "Computer Display of Curved Surfaces", PhD Thesis, University of Utah, (December 1978)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>507101</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Blinn, James F. "Simulation of Wrinkled Surfaces", Computer Graphics, Vol. 12 (3), pp. 286-292 SIGGRAPH-ACM (August 1978)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>801255</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Blinn, James F. "Light Reflection Functions for Simulation of Clouds and Dusty Surfaces", Computer Graphics, Vol 16 (3) SIGGRAPH-ACM, July 1982]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Brooks, J., Murarka R.S., Onuoha, D., "An Extension of the Combinatorial Geometry Technique for Modeling Vegetation and Terrain Features", NTIS Report AD-782-883, (August 1974)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Carpenter, Loren C. "Computer Rendering of Fractal Curves and Surfaces", Comm. ACM, to appear]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807436</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Dungan W., A Terrain and Cloud Computer Image Generation Model, Proceedings SIG- GRAPH'79, (August 1979), 143-150]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Futrelle, R., "Galatea Project for Analysis of Moving Images", Department of Biophysics and Theoretical Biology, The University of Chicago, Technical Reports, 1973-74]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Mandelbrot, Benoit B., Fractals - Form, Chance and Dimension, W. H., Freeman, San Francisco 1977]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807485</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Marshall, Robert, Wilson, Rodger, and Carlson, Wayne, "Procedure Models for Generating Three-dimensional Terrain", Computer Graphics, Vol. 14, (3), pp. 154-162, Proc. SIGGRAPH 80 (July 1980)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Newell, M.E., The Utilization of Procedure Models in Digital Image Synthesis, Computer Science, University of Utah, UTEC- CSC-76-218 (1975)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>801255</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Blinn, J., "Light Reflection Functions for Simulation of Clouds and Dusty Surfaces", Computer Graphics, Vol 16, No. 3 (1982) pp. 21-29.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>358553</ref_obj_id>
				<ref_obj_pid>358523</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Fournier, A., D Fussel, L. Carpenter, "Computer Rendering of Stocastic Models", Comm. ACM 25, No. 6 (June 1982)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Mandelbrot, B.B., "The Fractal Geometry of Nature", W.H. Freeman, 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>801167</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Reeves, W.T., "Particle Systems - a Technique for Modeling a class of Fuzzy Objects", Proceedings of SIGGRAPH'83, July 1983.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PANEL THE SIMULATION OF NATURAL PHENOMENA CHAIR: Charles A. Csuri, Ohio State University PANELISTS: 
James Blinn, Jet Propulsion Laboratory Julian Gomez, Ohio State University Nelson Max, Lawrence Livermore 
National Laboratory William Reeves, Lucasfilm Ltd. CHAIRMAN'S INTRODUCTION: This panel will discuss 
the issues and the problems associated with the simulation of natural phenomena. This is a difficult 
area of research since it generally involves complex data bases and in many instances time variant 
phenomena. The computational loads can become enormous as one considers the physics or the mathemat- 
 ical modeling of structures. Most items in nature, trees, clouds, fire and comets being some examples, 
have not been displayed realistically in computer graphics. This lack stems from a few dif- ferent problems, 
all of wh~h are signifi- cant. The first is the fact that realistic portrayals require large amounts 
of storage and consequently large compute time. Nature is able to create diverse detail at the most minute 
levels within an object of grandoise scale. The second problem is that of diversity of design within 
a given framework. For example, if a scene requires two dozen poplar trees, how does the designer construct 
trees that look different but are undeniably poplars? Humans typically become tired after the first 
few iterations of such a design pro- cess, with a resulting degradation in the subsequent models. Clearly, 
this problem applies to all of the phenomena mentioned above. Finally, there is a lack of models. First, 
second and third order representations are commonly used in com- puter graphics to model various kinds 
of surfaces and their boolean combinations. However, their applications to objects, which do not lend 
themselves well to being described as surfaces has not been addressed sufficiently. Previous attempts 
at realism have dealt with the appearances of the surfaces being modeled, in terms of their illumination 
or relief. More recently, fractal methods have introduced a new degree of realism into terrain modeling 
systems. However, it appears that natural phenomena will require more research into the fundamental way 
things occur in nature, and in terms of computer graphics, their representation will build on previous 
work, but will still require new modeling techniques. Dr. Nelson Max of the Lawrence Livermore Laboratory 
has dealt with computer graph- ics simulations of mathematical concepts, molecular structure, water and 
clouds. He will present his recent work on the simu- lation of clouds. Dr. William Reeves of Lucasfilm 
did his dissertation research in the area of motion control at the University of Toronto. He has done 
a simulation of fire for the science fiction film Star Trek II and he will discuss the techniques used 
to generate the data and the animation. Dr. James Blinn of the Jet Propulsion Laboratory is well known 
for his basic research in computer graphics. He has produced numerous animated sequences of scientific 
phenomena. His presentation will focus upon smoke, clouds and soft structures. Julian Gomez is a PhD 
candidate in com- puter graphics at the Ohio State Univer- sity. He worked at the Jet Propulsion Laboratory 
for three years in computer graphics. His dissertation research is in the area of the simulation of 
natural phenomena and he will discuss smoke and fire. Blinn, James F. and Martin E. Newell, "Texture 
and Reflection in Computer Gen- erated Images", Comm. ACM, Vol 19 (10), pp. 542-547 (October, 1976) 
Blinn, James F. "Computer Display of Curved Surfaces", PhD Thesis, University of Utah, (December 1978) 
 Blinn, James F. "Simulation of Wrinkled Surfaces", Computer Graphics, Vol. 12 (3), pp. 286-292 SIGGRAPH-ACM 
(August 1978) Blinn, James F. "Light Reflection Func- tions for Simulation of Clouds and Dusty Surfaces", 
Computer Graphics, Vol 16 (3) SIGGRAPH-ACM, July 1982 Brooks, J., Murarka R.S., Onuoha, D., "An Extension 
of the Combinatorial Geometry 137 Computer Graphics Volume 17, Number 3 July 1983 Technique for Modeling 
Vegetation and Ter- rain Features", NTIS Report AD-782-883, (August 1974) Carpenter, Loren C. "Computer 
Rendering of Fractal Curves and Surfaces", Comm. ACM, to appear Dungan W., A Terrain and Cloud Computer 
 Image Generation Model, Proceedings SIG- GRAPH'79, (August 1979), 143-150 Futrelle, R., "Galatea Project 
for Analysis of Moving Images", Department of Biophysics and Theoretical Biology, The University of 
Chicago, Technical Reports, 1973-74 Mandelbrot, Benoit B., Fractals -Form, Chance and Dimension, W. 
H., Freeman, San Francisco 1977 Marshall, Robert, Wilson, Rodger, and Carlson, Wayne, "Procedure Models 
for Gen- erating Three-dimensional Terrain", Com- puter Graphics, Vol. 14, (3), pp. 154-162, Proc. SIGGRAPH 
80 (July 1980) Newell, M.E., The Utilization of Procedure Models in Digital Image Synthesis, puter 
Science, University of Utah, CSC-76-218 (1975) Com-UTEC- PANELISTS' ABSTRACTS:  Julian E. Gomez, The 
Ohio State Univer- sity The basic problem with applying conven- tional modeling methods to the representa- 
tion of natural phenomena is the intracta- bility of the datasets. Objects such as clouds or fire are 
clearly difficult to model with polygons, bicubic surface patches, or quadric surfaces. The first two 
methods require large amounts of data, with a corresponding high computational cost. The third has the 
additional prob- lem of creating surfaces which appear too regular. The problem is compounded by the 
fact that natural phenomena inherently change form with time, thus requiring large numbers of already 
large data bases. Additionally, the objects under discussion are not even surfaces, so the addition 
of detail by, for example, surface normal pertubation or stochastic surface element subdivision is not 
applicable. Neverthe- less, given recent demonstrations by Man- delbrot, Carpenter and Fournier, et. 
al., it is apparent that fractals will have major application to the representation of natural phenomena. 
 These phenomena can be simulated by mas- sive systems of differential equations describing the motion 
of particles within the system. A situation like this is for the most part out of the range of a laboratory 
without access to large or "super" computing capability. One way around this is to "fake" the object, 
i.e., to find a method which reproduces the appearance of the object without actually simulating it. 
 It is difficult to separate the appearance of the object from the systems describing it, because of 
the illumination models required. Keeping in mind a particulate nature, smoke reflects light, fire emits 
and transmits light, and a cloud reflects, transmits and scatters light. High com- plexity illumination 
models, sometimes called ray tracing algorithms, are neces- sary to accurately portray the action of 
light in one of these situations. Finally, a modeling technique should be hierarchical by screen complexity, 
i.e., generate enough detail as necessary to occupy the proper portion of screen space plus a little 
for antialiasing. Nelson Max, Lawrence Livermore National Laboratory If a cloud volume is defined by 
the region between two height fields,.above and below a mean cloud plane, the heights may be computed 
by formulas involving tri- gonometric functions, polynomials, square roots and basic arithmetic operations. 
Since the cloud shape is specified by a small number of coefficients, rather than by random access to 
a large data base, these computations can readily be "vectorized" to run on modern supercomput- ers with 
parallel or pipeline architec- tures. Colour raster animation of these shapes will be shown, including 
semi- transparent 2-D clouds and opaque 3-D c louds. Semi-transparent 3-D clouds will also be illustrated 
rendered by a new algorithm for efficiently integrating the "single- scattering" model of Jim Blinn [i] 
along a ray from the eye through the cloud. When the sun is directly overhead and the cloud has constant 
density, the intensity con- tributed by a segment where a ray inter- sects the cloud is approximated 
by a linear combination of the moments of the region where the cloud intersects a verti- cal plane 
containing the ray. The inde- finite integrals for the moments up to a certain degree are precomputed, 
and reused for the various rays in the vertical plane.  138 An animated sequence simulating a laser 
 fusion target implosion will show smoke produced when the ablator blows off after absorbing the incoming 
radiation. The smoke was made from thousands of small moving dots, as in Bill Reeves' method. Optical 
printing was used to blur them and combine them with the rest of the target image. This eliminates the 
need for motion blur and anti-aliasing and requires fewer dots. Blinn, J., "Light Reflection Functions 
for Simulation of Clouds and Dusty Surfaces", Computer Graphics, Vol 16, No. 3 (1982) pp. 21-29. William 
T. Reeves, Lucasfilm Ltd Image synthesis of scenes containing natural phenomena is, as we all know, 
a difficult problem. Real objects do not have smooth or rectangular surfaces com- posed of bronze, chrome 
or plastic --they are rough, pockmarked dirty, cracked, fluid, amorphous and very irregular. In natural 
images, the complexity and level of detail seems infinite. And yet, with proper anti-aliasing and the 
high resolu- tion framebuffers and film recorders being manufactured today, we are capable of displaying 
very realistic and detailed images. The display-space bandwidth is there -all we have to do is generate 
and correctly filter the signal to be displayed. What follows are some direc- tions in which I think 
graphics research needs to proceed in order to synthesize natural scenes. Computer models of natural 
phenomena are too complex and detailed to be designed efficiently by an interactive user with access 
to only a simple set of building blocks such as polygons or patches. Instead, we must continue to research 
and develop procedurally based modeling domains, and, in particular, those based on stochastic processes 
(e.g., fractals [Mandelbrot 82] [Fournier 82] and particle systems [Reeves 83]. With such tools, the 
interactive user only specifies global constraints and parameters and the pro- cedural and stochastic 
elements are responsible for generating the actual images. We need to develop systems in which to test, 
debug and experiment with procedural models. Traditional modeling primitives, such as polygons, quadrics 
and patches are often inappropriate for modeling natural phenomena. We need new modeling primi- tives 
that are simple to render so that we can afford to compute millions of them to build up image complexity. 
 Texture mapping techniques are effective as a means of adding detail to an image. Photographing a real 
scene, scanning it in, and using it as a texture in a syn- thetic image might be called "cheating" by 
some people, but it works and is quite cost effective. We should continue to develop texture mapping 
and look into using procedural texture maps or writing systems and languages in which one can create 
new textures. The knowledge needed to synthesize a par- ticular natural phenomenon is domain dependent 
-what you need to know to draw trees is quite different from what you need to draw clouds. This information 
is often available in scientific literature. The problems are finding it and then deci- phering what 
it says. Working with other researchers from the field in question is often a good idea. Finally, making 
a natural scene look "right" is only two thirds of the problem. Making it move right is the other one 
third. Motion is often overlooked in image synthesis, especially in the motion of objects within the 
scene. Animated scenes containing natural objects will not look real until we are able to synthesize 
their motions. Procedurally and stochast- ically defined motions are interesting areas of research. 
Fournier, A., D Fussel, L. Carpenter, "Computer Rendering of Stocastic Models", Comm. ACM 25, No. 6 
(June 1982) Mandelbrot, B.B., "The Fractal Geometry of Nature", W.H. Freeman, 1982. Reeves, W.T., "Particle 
Systems -a Tech- nique for Modeling a class of Fuzzy Objects", Proceedings of SIGGRAPH'83, July 1983. 
 139 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801143</article_id>
		<sort_key>141</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[A parallel scan conversion algorithm with anti-aliasing for a general-purpose ultracomputer]]></title>
		<page_from>141</page_from>
		<page_to>150</page_to>
		<doi_number>10.1145/800059.801143</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801143</url>
		<abstract>
			<par><![CDATA[<p>Popular approaches to speeding up scan conversion often employ parallel processing. Recently, several special-purpose parallel architectures have been suggested. We propose an alternative to these systems: the general-purpose ultracomputer, a parallel processor with many autonomous processing elements and a shared memory. The &#8220;serial semantics/parallel execution&#8221; feature of this architecture is exploited in the formulation of a scan conversion algorithm. Hidden surfaces are removed using a single scanline, z-buffer algorithm. Since exact anti-aliasing is inherently slow, a novel parallel anti-aliasing algorithm is presented in which subpixel coverage by edges is approximated using a look-up table. The ultimate intensity of a pixel is the weighted sum of the intensity contribution of the closest edge, that of the &#8220;losing&#8221; edges, and that of the background. The algorithm is fast and accurate, it is attractive even in a serial environment, and it avoids several artifacts that commonly occur in animated sequences.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Parallel processing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Hidden line/surface removal</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>B.3.2</cat_node>
				<descriptor>Shared memory</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010600.10010607.10010608</concept_id>
				<concept_desc>CCS->Hardware->Integrated circuits->Semiconductor memory->Dynamic memory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP43136075</person_id>
				<author_profile_id><![CDATA[81100188679]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eugene]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fiume]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Systems Research Group, Department of Computer Science, University of Toronto, Toronto, Ontario, M5S 1A4]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP42051322</person_id>
				<author_profile_id><![CDATA[81100345881]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alain]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fournier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Systems Research Group, Department of Computer Science, University of Toronto, Toronto, Ontario, M5S 1A4]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15027912</person_id>
				<author_profile_id><![CDATA[81100272981]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Larry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rudolph]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, Carnegie-Mellon University, Pittsburgh, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>358815</ref_obj_id>
				<ref_obj_pid>358808</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Blinn, J.F., L.C. Carpenter, J.M. Lane, and T. Whitted, "Scan line methods for displaying parametrically defined surfaces", Comm. ACM 23, 1 (Jan. 1980), 23-34.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807360</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Catmull, E., "A Hidden-Surface Algorithm with Anti-Aliasing", Computer Graphics (ACM), 12, 3, (Aug. 78), 6-11.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801272</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Clark, J.H., "The geometry engine: a VLSI geometry system for graphics", Computer Graphics (ACM) 16, 3 (July 1982), 127-134.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>359869</ref_obj_id>
				<ref_obj_pid>359863</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Crow, F.C., "The Aliasing Problem in Computer-Generated Shaded Images", Comm. ACM 20, 11 (Nov. 1977), 799-805.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1300012</ref_obj_id>
				<ref_obj_pid>1299941</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Crow, F.C., "A Comparison of Antialiasing Techniques", IEEE Computer Graphics and Applications, 1, 1 (Jan. 81), 40-49.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Fuchs, H., J. Poulton, A. Paeth, and A. Bell, "Developing PIXEL-PLANES, a smart memory-based raster graphics system", 1982 Conference on Advanced Research in VLSI, MIT, January 1982, 137-146.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Fournier, A. and D. Fussell, "On the Power of the Frame Buffer", unpublished manuscript, 1983.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807454</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Fuchs, H. and J. Barros, "Efficient Generation of Smooth Line Drawings on Video Displays", Computer Graphics, 13, 2, (Aug. 79), 260-269.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Fuchs, H., and J. Poulton, "PIXEL-PLANES: a VLSI-oriented design for 3-D raster graphics", CMCCS Conference Proceedings, (June 1981), 343-348.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Fussell, D., and B.D. Rathi, "A VLSI-oriented architecture for real-time raster display of shaded polygons", Graphics Interface '82, May 1982, 373-380.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>810237</ref_obj_id>
				<ref_obj_pid>800179</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Fuchs, H., "Distributing a visible surface algorithm over multiple processors", Proceedings of ACM 1977, Seattle (Oct. 1977), 449-451.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Gottlieb, A., R. Grishman, C.P. Kruskal, K.P. McAuliffe, L. Rudolph, and M. Snir, "The NYU Ultracomputer&#8212;designing an MIMD shared memory parallel computer", IEEE Transactions on Computers, C-32, 2 (Feb. 1983), 175-189.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357206</ref_obj_id>
				<ref_obj_pid>69624</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Gottlieb, A., B.D. Lubachevsky, and L. Rudolph, "Basic techniques for the efficient coordination of very large numbers of cooperating sequential processors", Transactions on Programming Languages Systems (ACM) 5, 2 (Apr. 1983), 164-189.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806791</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Gupta, S., R.F. Sproull, and I.E. Sutherland, "A VSLI architecture for updating raster scan displays", Computer Graphics (ACM) 15, 3 (Aug. 1981), 71-78.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Lawrie, D.H., "Access and alignment of data in an array processor", IEEE Transactions on Computers, C-24, 12 (Dec. 1975), 1145-1155.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806787</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Lee, D.T., "Shading of regions on vector display devices", Computer Graphics (ACM) 15, 3 (Aug. 1981), 37-44.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Newman, W.M., and R.F. Sproull, Principles of Interactive Computer Graphics, Second Edition, McGraw-Hill, New York, 1979.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807467</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Parke, F.I., "Simulation and expected performance of multiple processor z-buffer systems", Computer Graphics (ACM) 14, 3 (July 1980), 48-56.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357116</ref_obj_id>
				<ref_obj_pid>357114</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Schwartz, J.T., "Ultracomputers", Transactions on Programming Languages and Systems (ACM) 2, 4 (Oct. 1980), 484-522.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356626</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I.E., R.F. Sproull, and R.A. Schumacker, "A characterization of ten hidden-surface algorithms", Computing Surveys (ACM) 6, 1 (March 1974), 1-55.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806789</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Weinberg, R., "Parallel processing image synthesis and anti-aliasing", Computer Graphics (ACM) 15, 3 (Aug. 1981), 53-62.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806815</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Whitted, T., and D.M. Weimer, "A software test-bed for the development of 3-D raster graphics systems", Computer Graphics (ACM) 15, 3 (Aug. 1981), 271-277.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801274</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Whelan, D.S., "A rectangular area filling display system architecture", Computer Graphics (ACM) 16, 3 (July 1982), 147-153.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Whitted, T., "An improved illumination model for shaded display", Comm. ACM 23, 6 (June 1980), 343-349.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Whitted, T., "Hardware enhanced 3-D raster display systems", CMCCS Conference Proceedings, (June 1981), 349-356.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Parallel Scan Conversion Algorithm with Anti-Aliasing for a General-Purpose Ultracomputer Eugene Fiume 
Alain Fournier Computer Systems Research Group Department of Computer Science University of Toronto 
Toronto, Ontario, M5S 1A4 Larry Rudolph Department of Computer Science Carnegie-Mellon University Pittsburgh, 
PA 15213 ABSTRACT Popular approaches to speeding up scan conversion often employ parallel processing. 
Recently, several special-purpose parallel architectures have been suggested. We propose an alternative 
to these sys- tems: the general-purpose ultracomputer, a parallel processor with many autonomous processing 
ele-ments and a shared memory. The "serial semantics/parallel execution" feature of this archi- tecture 
is exploited in the formulation of a scan conversion algorithm. Hidden surfaces are removed using a single 
scanline, z-buffer algorithm. Since exact anti-aliasing is inherently slow, a novel parallel anti-aliasing 
algorithm is presented in which subpixel coverage by edges is approximated using a look-up table. The 
ultimate intensity of a pixel is the weighted sum of the intensity contribu- tion of the closest edge, 
that of the "losing" edges, and that of the background. The algorithm is fast and accurate, it is attractive 
even in a serial environment, and it avoids several artifacts that commonly occur in animated sequences. 
CR Categories and Subject Descriptors: B.32 [Memory Structurcs]: Design Styles Shared Memory; D.3.3 [Programming 
Languages]: Language Constructs -Concurrent programming structures; F.2.2 [Analysis of Algorithms and 
Problem Complex-ity]: Nonnumeri, cal Algorithms and Problems Geometrical problems and computc~tions; 
1.3.1 [Computer Graphics]: Hardware Architecture -Ras-ter display devices; 1.3.3 [Computer Graphics]: 
Picture/Image Generation -Display algorithms; 1.3.7 [Computer Graphics]: Three-Dimensional Graphics and 
Realism -Y~sible line~surface algo- rithm. Permission to copy without fee all or part of this material 
is granted provided that the copies are not made or distributed for direct commercial advantage, the 
ACM copyright notice and the title of the publication and its date appear, and notice is given that copying 
is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. 1. Introduction The performance of a raster graphics system is strongly 
influenced by the inefficiency of scan conversion. Several recent papers have proposed speeding up scan 
conversion by employing special- purpose hardware. These systems exploit parallel processing in various 
ways, some of which are: (I) "Intelligent" VLSI-based memory. This includes systems such as PIXEL-PLANES, 
by Fuchs et al. [FuPoSl, FPPB82], the smart memory architec- ture by Gupta etal. [GuSSSI], and the Rec- 
tangular Area Filling Display System Architec- ture by Whelan [Whe182].  (2) Hardware enhancements or 
graphics engines. Clark's geometry engine, although not a scan conversion, system, illustrates the latter 
[ClarSZ], and Whitted's enhanced frame buffer is an example of the former [Whir81]. The pro- posed systems 
of Fussell and Rathi [FuRa82], and Weinberg [Wein81], are graphics engines.  (3) Special-purpose, multiple-processor 
systems. These systems incorporate special-purpose hardware to broadcast image descriptions to the processors. 
Image memory is often parti- tioned to enhance parallelism. Examples include Fueh's central broadcast 
controller [Fuch77], Parke's splitter tree, and Parke's splitter tree/broadcast controller hybrid  
[ParkS0]. Obviously, any parallel-processing scheme should demonstrably hasten scan conversion. The 
above proposals are no exception. Several issues remain open, however. First, few proposals address the 
aliasing problem. Indeed, anti-aliasing is difficult to perform on the systems of Fuchs et al., Fussell 
and Rathi, Whelan, Fuchs, and Parke. Second, display systems exploiting parallelism should always exhibit 
subserial behaviour. Third, it is not clear that a special-purpose system is the best approach if similar 
computational power is required for other tasks. It is likely that the feasibility of large-scale display 
processors with special-put'pose hardware will coincide with that of general-purpose parallei processnrs. 
The u!tracomputer, described below, is one such processor. We wish to demon-strate that the ultracomputer 
can be a very effec- tive "graphics engine" in its own right. This is illus- trated by presenting a parallel 
scan conversion &#38;#169; ACM 0-89791-109-1/83/007/0141 $00.75 algorithm including anti-aliasing. The 
worst case behaviour of the algorithm is subserial. Not all problems necessarily have faster parallel 
implementations. However, problems such as scan conversion, which naturally decompose into a large set 
of independent subproblems, are good candi-dates for parallel processing. The objective of a general-purpose 
parallel processor design is to maximise the degree of subproblem independence over a wide class of tasks. 
Otherwise, the major advantage of such a processor over special-purpose systems is lost. In our ultracomputer 
model, sub- problem independence is facilitated by a Small repertoire of powerful concurrent operations 
on shared memory. To each processing element (PE) of the ultracomputer, a concurrent operation appears 
to execute indivisibly. In fact, an intelligent, multi- stage network cleverly connects the PEs to shared 
memory, and combines all operations simultane-ously directed at a variable into one operation. Pro-grams 
thus appear to have a serial semantics, but parallel execution. Moreover, parallel programs are simply 
expressed, unlike the often more compli-cated techniques required to optimise computa-tions on vector 
or pipeline processors. Because of this "serial semantics/parallel exeetion" property, the algorithms 
below can be implemented on any processor capable of simulating the concurrent operations, although the 
resulting programs may run more slowly. Section 2 outlines the basic ultracomputer archi-tecture. A scan 
conversion algorithm that utilises this parallel processing model is presented in Sec- tion 3. A novel 
anti-aliasing algorithm is given as an integral part of scan conversion. Lastly, the gen-erality of the 
ultracomputer is illustrated by noting other problems to which it can be applied. This is discussed in 
Section 4, as are topics for future research. 2. Ultracomputer Architecture An ultracompz~er is a parallel 
processor composed of many processing elements (PEs), which have multiple-cycle access to shared memory. 
Ultra-computers are a good model of parallel computa- tion. Schwartz has made an extensive survey of 
this field, summarising various upper and lower bounds for parallel sorting algorithms, set operations, 
matrix multiplication, etc. [Schw80]. Ultracomput-ers are more than just a theoretical model, how-ever. 
Indeed, our ultracomputer model is based on the work done at New York University, at which a large-scale 
implementation is planned [GGKM81]. The model we propose is a very slight extension of the NYU model, 
incorporating additional concurrent instructions. An NYU Ultracomputer is composed of N = 2 D auto- nomous 
PEs and connected to N shared memory modules. Local memory for each PE is provided by means of a partitioned 
memory cache. PEs access shared memory via a D=log2N-stage connection network composed of an NxD array 
of "intelligent" a-input, a-output switches 1. Switch interconnection i. The entire architecture can 
be easily genera]ised to N =/~ D PEs and a D --Iogk N-stage network using k-input, k- output switches. 
 is based on Lawries's omega-network [Lawr75], illustrated in Figure i. I~tale 1 I IStaee ~ I I~taee 
3 I Figure 1, Ro~N~g through a~ omegg-~.et~mrk.l'o~" 8 PEs. Con- neetions between PEa, switches, and 
MMs are by means of a slt~ffts-ezcItaw, ge: an object numbered d id  " " c/n in binary is connected 
to the object numbered d z'- 01D d *in the next stage of the network. A message transmitted from PE 
~D  "P * to MM rn~ -  rn * uses output port rn~ when leaving the i m switch. Similarly for travelling 
from MM to PE. The route from PE 5 (101e) to MM 2 (010e) is indicated. The novelty of the NYU design 
rests in the intelli-gent switches, which implement concurrent access to variables in shared memory. 
The network easily realises concurrent fetch or store operations. Other more powerful concurrent operations 
can be implemented. Presently, one such instruction is supported: the replace-add, which creates the 
illu- sion of indivisibly adding a value to a shared vari-able, and returning the sum to the requesting 
PE. Specifically, the format of the operation is RepAdd(V,e) 2, where V denotes a shared (integer) variable 
and e is an integer expression. Let V have value v. Suppose P~:~ issues the command S/ =RepAdd(V,e~ ), 
and PEj issues the command Sj =RepAdd(V,ej ) simultaneously. Then, assuming V is not simultaneously updated 
by another PE, either S/ =v+~i S] =v+e i + el, or S~ =v+e/ +e~ Sj ----v+ej, and in either case, the 
new value of V is v+ei +ej . Note that RepAdd(V,0) is a fetch instruction. When operations on the same 
cell in shared memory meet at a switch, they are synthesised into a single instruction. This is sent 
to the next stage in the network in one cycle. Instruction combining can occur at any stage in the network. 
Hence of all the operations simultaneously directed at a single vari- able, V, only one cumulative operation 
actually "reaches" V. Thus memory traffic is reduced and network bandwidth is increased. Indeed, the 
pro-cessor has the following surprising property: it is 2. 'the semantics of this operation has recently 
been modified in the NYU design, and has since been renamed FetehAdd. Since RepAdd can be easily constructed 
from FetchAdd, we will continue to use RepAdd in our ultracomputer model. particularly efficient when 
many operations are concurrently issued on a small set of variables. Simultaneous update of the same 
variable by all N PEs is resolved in O(log N) time, compared to 0(N) time for typical parallel processors 
using semaphore-like mutual exclusion. This is a useful property which is often exploited. For example, 
RepAdd makes an effective synehronisation primi-tive [GoLR83]. Moreover, data structures allowing parallel 
access are conveniently implemented using RepAdd. A polygon display list can be nicely imple- mented 
as a parallel queue. Suppose the index NextPolygon is used as a subscript into a polygon list. Then every 
PE executing RepAdd(NextPolygon,1) is guaranteed to get a unique value for NextPolygon. The standard 
NYU ultracomputer model supports the three concurrent instructions described above: fetch, store, and 
RepAdd. To realise these opera-tions, a switch only needs a small amount of memory, and an adder. Implementation 
details, together with a network performance analysis, are found in [GGKM81]. Although these instructions 
have proved useful for constructing good parallel solutions to scientific and operating system prob-lems, 
we believe a concurrent, flexible comparison instruction is needed. We propose a new concurrent instruction, 
repLace-minimum, or RepMin, which is easily realised by adding a eomparator to each switch. Its semantics 
is defined as follows. Let V denote a cell of shared memory having value v, and let e be an expression 
such that both v and e are pairs (intensity,depth) of values s. Then RepMin(V,e) causes all of V to be 
replaced by e iff e.depth < v.depth. The value returned by RepMin will be dis- cussed presently. The 
utility of RepMin in scan conversion is obvious. Consider the following paral- lel version of the z-buffer 
algorithm found in [NeSp79]. Here, the entire z-buffer is assumed to be addressable as an nxm array of 
shared memory. Each PE executes the following.  while polygons remain do begin get P from polygon list 
(use RepAdd) Ypixels (x,y) E P do begin i :-- Polygonlntensity(P,x,y) z := PolygonDepth(P,x,y) RepM'm((x,y), 
(i,z)) end end Let us now discuss the value returned by a RepMin operation. We only consider the case 
where n PEs (0-<n~N) simultaneously issue a RepMin for cell V. Informally, of all the RepMin's simultaneously 
directed at variable V, the value returned to a PE is one which has "lost" in at least one comparison. 
Moreover, any value sent by a particular PE is returned exactly once. Perhaps surprisingly, this is achievable 
in the switches, and can be shown by induction on n. The NYU ultracomputer also presently lacks con-current 
logical bit operations. The scan conversion 3. To make the replace-minimum instruction qu~te genera], 
the extent of. the intensity and depth subwords could be con- trolled by a modifiable bit-mask stored 
in each switch. Clearly, the names of the subwords, "intensity" and "depth", are illustrative. In practice, 
the subwords cotdd be known by arbitrary names. algorithm below makes use of another concurrent instruction, 
the RepAnd. This operation has the same format as the RepAdd, but performs a logical and of the arguments 
instead of an addition. Note that in principle, only a few Nand gates in each switch would be required 
to realise all 16 boolean operations as concurrent instructions. In general, an instruction supported 
by the connec-tion network must be associative. Thus concurrent floating point operations cannot be properly 
real-ised 4. There exist inherently non-associative opera- tions, such as the group (or Fourier) commutator. 
Defined as [a ,b ] = aba-lb -5, this operation is not associative for non-commutative groups; thus a 
"RepCom" instruction for matrices under multipli-cation is inherently unrealisable. The serialisation 
principle is a necessary property of the connection network: The network ensures that the effect of simultaneous 
operations by the PEs is equivalent to some serialisation of the opera- tions. 3. A Fast Parallel Scan 
Conversion Algorithm 3.1. Preliminaries Our definition of scan conversion is the traditional one (e.g. 
[NeSp79]). Given a scene represented by P simple polygons, determine the set of pixels and their intensities 
that best approximates the scene. The solution, based on the conventional single-scanline z-buffer algorithm, 
performs hidden-surface removal and anti-aliasing. Serial scanline algorithms typically require a YX-sort 
of polygon spans intersecting with a given scanline [SuSS74]. However, RepMin allows us to drop the X 
sort. The shared memory storing ultimate scanline intensi-ties is assumed to be available to a video 
controller, by dual-ported memory, for instance.  3.2. The Algorithm First we briefly outline the 
major steps performed by each PE. As in traditional scanline algorithms, a -scanline bucket is employed 
to determine polygon segments that enter the scene at scanline y. (I) Remove backfacing polygons.  (2) 
Convert remaining polygons into sets of $1#a~-area~, i.e. trapezoidal or triangular regions. Insert each 
span-area into the Y-bucket corresponding to its largest y-value. (3) Scan convert span-areas: for y 
:= ymin to ymax do  (a) The span-areas from bucket y are inserted into the ac~vs span list (ASL). (b) 
Process active spans for scanline y. Each PE takes a span from the ASL. If the span is large, only a 
fraction of it is taken at a time, thus permitting parallel pro- cessing of the span. For each pixel 
in its portion of a span, the PE computes intensity and depth values, and performs a table look-up to 
approximate the portion of the pixel covered by the span. The left and right end- points of the span 
are then updated. If the span-area is exhausted, it is removed from the ASL. (c) Anti-aliasing. For 
each non-empty pixel, an approxi- mate anti-aliasing procedure is performed by  4. Inmost computers, 
((I0~--i0a)+1) ~ (i0a+(--I0~+I)), for large a. determining the intensity contribution of the closest 
span, and adding in the average contribution of the "losers". The coverage information computed in step 
 (b) is used in these calculations.  3.2.1. Data Structures For clarity, we only use static storage 
in shared memory. ASsume there are P input polygons found in the array InputList. In what follows, let 
V~ be the number of vertices in input polygon P~, and let V be the largest such V~. Assume the PEs are 
pro-grammed in a high-level language such as Pascal or Euclid which allows programmer-defined data types. 
Note that arrays in shared memory are pos-sible, since their starting addresses can be stored in the 
local memory for each PE. The names assigned to variables in shared memory begin with an upper case letter. 
Local variables begin with a lower case letter. Polygon display list I [nputList: array I..P of Polygon 
Ngon: array I..P of Polygon -each polygon Pc contains an array 1..l~ of (x,y,z). I Y bucket. Yp gives 
next available position for seanline y ] : matrix ymin..ymax 1..PV of SpanArea Yp: array ymin..ymax 
of 0..P Active Span List. S reflects the number of spans. ASL: array 1..PV of SpanArea S: 1..PV :-- 0 
I Some indices PolyIn, PolyOut, CurrentSpan: Integer I Locks for synchronisation. Assume they are initialised 
to 0 Loekl, Lock2: 0..P := 0 SpanArea: type record of yt ~top y I dy { height of span-area xl ~current 
LHS xr ~current RHS I xm ~multiplicity-see below; initially xm--xl-M~ dxl ~ &#38;z ofLHS I AN dxr 
[ Az of RHS Ay dyl [ ~ofLHS| Az dyr ~ A.~ of RHS Az DepthInfo Intensitylnfo end  3.2.2. Synchronisation, 
Initialisation, and Backfae-ing Polygon Removal Since the code in this section is familiar, it is a good 
place to illustrate some principles of synchronisa- tion and initialisation. Assume each PE has access 
to a unique identifier in the manifest constant PEid, which takes on a value between 1 and N. The follow- 
ing code initialises PolyIn and PolyOut, performs synchronisation, and removes backfacif~g polygons as 
in [NeSp79, Appendix III]. We assume the polygons in the input list have undergone perspec-tive transformation. 
The reader may wish to verify that two locks are necessary to have fully reusable locks for synchrordsation. 
i, j, p: integer InputList, Ngon, Lock I, Lock2, PolyIn, Poly0ut: shared The first PE in initialises 
Polyln, Poly0ut if RepAdd(Lockl,1) = 1 then PolyIn := Poly0ut :-- Lock2 := 0 while Lock1 < N do ~nothing~ 
I The last PE out resets Loekl for future Use if Lock2 = N-1 then Loekl := 0 RepAdd(Lock2,1) while Lock2 
< N de ~nothing{ p := I~phdd(Poly0ut,1) while p ~ P do begin for polygon InputList[p], calculate c 
E e := ~ (V[i].x-V[j].x)(V[i].y+V[j].y) C ~l where ]=i+l if i<V~ ; otherwise j=l if c .c 0 then ~the 
polygon feces us, add it to Ngon] Ngon[RepAdd(Pelyln,1)] := InputList[p] p := RepAdd(Poly0ut, 1) end 
In the average case, each PE processes about P/N polygons. This algorithm assumes that N<P, since otherwise 
those PEs with PEid>P do no work. The amount of memory traffic this algorithm would cause is suboptimal, 
since polygon definitions are moved around, rather than their pointers. 3.2.3. Decomposition of Polygons 
into Span-areas As presented in this paper, the scan conversion algorithm presumes the input polygon 
list has been decomposed into span-areas: trapezoidal or tri-angular regions. This idea is not new (see 
[LeeBl, WeinS1, WhWeSl]). Unlike polygons, span-areas have a bounded, concise specification in terms 
of left and right edges. Thus span-areas are useful in scanline-oriented algorithms. However, desirable 
properties of trapezoids such as planarity are not necessarily preserved after geometric transforma-tions. 
Consequently, the input polygon list is preprocessed for each frame. This additional com-putation can 
be circumvented if polygons are tri-angulated once and for all, since triangles remain (trivially) planar 
after geometric transformations (see [FuRa82, Whir81]). The scan conversion algo-rithm easily adapts 
to triangles, but since span-areas are so simple to work with, the algorithm is presented using span-areas. 
Both triangles and span-areas can lead to fragmentation of very small (pixel-sized) polygons, making 
anti-aliasing critical. A maximum of V-1 span-areas are generated for a polygon of V vertices. An O(V 
logV) serial algorithm to decompose a simple polygon into span-areas was recently published [LeeS1]. 
A straightforward, polygon-per-PE parallelisation of this algorithm would yield an o(PV-~-log V) average-case 
running time. As each span-area is generated, it is inserted into the Y-bucket corresponding to the largest 
y value of the span-area. This can be determined on-the-fly with no change in the order statistic.  
 3.2.4. Scan Conversion Each PE performs the following scan conversion loop. for y:=ymin to ymax do 
begin UpdateASL(y) InitialiseScanLine ScanConvert(y) <synchronise> end for UpdateASL places the 
contents of bucket Y[y] into the active span list. All PEs synchronise at the com-pletion of scan conversion 
for each scanline. This is not necessary. If sufficient memory is available, the algorithm easily generalises 
to k-scanlines, k 1. We now consider the scan conversion process in more detail. procedure InitialiseScanLine 
InitialiseXBucket CurrentSpan := 1 <synchronise>  end InitialiseScanLine procedure SeanConvert(y: ymin..ymax) 
span: SpanArea spansLeft: Boolean X: shared Get Span(span,spansLeft) while spansLeft do begin Vx E span 
calculate pixeILnfe: intensity, depth, and coverage mask UpdatePixel(x, pixelInfe) Get Span(span,spansLeft) 
end while AntiAliasScanline(y) end ScanConvert The X bucket contains all required scanline informa- 
tion. It will be discussed shortly, as will the rou- tines UpdatePixel and AntiAliasScanline. GetSpan 
does the obvious: it returns an unpro- cessed span to the scan converter. However, the routine is complicated 
by the fact that we wish to get a subserial worst case behaviour. In particular, large spans should receive 
parallel treatment, for otherwise all PEs could wait for one PE to complete a long span. One approach 
is for PEs to recursively subdivide large spans so that each PE processes a smaller portion of the span. 
However elegant this solution appears, this approach is likely to increase memory traffic substantially. 
The approach we have taken avoids this problem. Assume there is a constant M which denotes the maximum 
number of pixels in a span that a PE is allowed to process at a time. This value may be empirically or 
theoretically determined, and represents a good balance between the overhead in GetSpan and the increased 
effi-ciency in parallel processing of large spans. Multi-ple copies of a span may be returned; the index 
xm is used to indicate the leftmost point of the unpro- cessed portion of the span s . The following, 
is one possible implementation of GetSpan. It is some-what tricky since it must cope with the unlikely 
event that two PEs simultaneously try to get an exhausted span. procedure GetSpan(var span: SpanArea; 
var spansLeft: Boolean) gotSpan: Boolean S, ASL, CurrentSpan: shared M: Constant newLHS: Integer spansLeft 
:= CurrentSpan .c S gotSpan := false while ~gotSpan and spansLeft do begin span := ASL[CurrentSpan] 
 with ASL[CurrentSpan] do be~ } calculate new LHS of span, and see if LHS>RHS newLHS := RepAdd(xm,M) 
 gotSpan := newLHS ~ xr if ~gotSpan then if span is exhausted, the first PE advances CurrentSpan and 
processes the remaining span segment ifnewLHS-xr < M the,, RepAdd(CurrentSpan,1) spansLeft := CurrentSpan 
~ S end with/while if gotSpan then span.xm := newLHS spansLeft := gotSpan end GetSpan  3.2.5. Anti- 
aliasir~ The aliasing problem is immediately apparent to anyone who has seen synthesised raster images. 
Various aliasing artifacts are possible in both still and moving images. An abundant literature describes 
the problem and some of its solutions. See [Crow77, Crow81] for a start. It is thus of prime importance 
to examine whether anti-aliasing can be incorporated into our algorithm. Since we currently compute the 
picture scanline by scanline without backtracking over scanlines, we cannot use any scheme where the 
value at one pixel depends on the value of some of its neighbours, unless we arbi- trarily privilege 
the x direction 8. The best solution under the circumstances is what we can call the Exact Area Sampling 
solution, where the intensity for the pixel is I = A~I~/l~. A, and/~ are the areas and intensities of 
the visible surfaces within the pixel, and A is the total area of the pixel. If colour is used, this 
formula is used for the three primaries. As pointed out in [Catm78], and imple- mented there and in [FuBar79], 
this requires a hid- den surface algorithm at the pixel level. We can establish a more formal lower 
bound, by showing that any algorithm that computes the EAS can be used to determine the order of a list 
of ~% non-negative integers. The reduction is as follows. Given alist NI,N2,...,N~ of numbers, construct 
a scene with ~% rectangles of depth Ni , with the left, top and bottom edges coincident with the pixel 
left, top and bottom, and the right edge of rectangle ~ at N~. Without loss of generality, assume that 
the pixel right edge is at max(N i ). Let the intensity ~ of each rectangle be D i-I where D is a constant 
greater than max (Ni) - rain (N~). The answer to the gAS problem is then: 6. The idea is not totally 
without merit, since as seen on broad-cast television it produces decent images. Note, moreover, 5. See 
the definition of the SpanArea data type above, that a k-sean/ine version (k > I) of the algorithm would 
per- rnit a mltltJp]e-pixe] anti-aliasing scheme. H x~Di-I(N~-Np ) where H is the height of the pixel, 
and N~ is the predecessor of N in the sorted order. The predecessor of min(Ni) is 0. This transformation 
can be done in 0(n) time. It is clear that the answer, when expressed as a base D number, contains Ni 
-Np in the i th digit (from the least significant), and that therefore in 0(n) time one can find, for 
every number, its predecessor in the sorted order. Computing the answer to the EAS problem allows sorting 
with a 0(n) time transfor-mation, and therefore takes at least 0(nlogn) time. While this does not prove 
that it is necessary to solve the hidden surface problem to solve the EAS problem, this shows that nothing 
easier than sort-ing will do it. For other results about the EAS, see [FoFu83]. In view of this result, 
we will aim for an approxi-mate solution. Our approach will be to limit the amount of computation and 
to utilise parallelism as much as possible. We subdivide the pixel into ~z n subpixels. It is convenient 
to have ~ a power of 2, for example n =23=8. For each line which intersects a pixel, the two intersection 
points along the boundaries of the pixel are used as an index into a lookup table, whose entries give 
the subpixels covered by the halfplane defined by this line. We will call this entry the mask for this 
halfplane. In our example, the mask would be a 64 bit number. Each intersection with the boundaries of 
the pixel is computed with k bits of fraction (where there are 2 k intervals, since the fraction n/n= 
1 in the current pixel is 0 on the next pixel). In our example, then, k=3. Thus each inter-section can 
be fully described as a k +2 bit number, 2 bits to identify the boundary crossed, and k bits to give 
the crossing position along the boundary (see Figure 2). The total entry for a line is then a 2(/ +2) 
bit number, which in our example is a i0 bit number. This gives a 1K64 bit table, which is small enough 
to allow a copy for each PE. Alternatively, several PEs could directly share such a read-only table. 
  t lz / Code= Iii0100011 O0 Ol ~'~ i r t 51 3  c/ II II Mask= FSFOE00080000000 X b i0 Figure 2. 
PL~el-/,~,ne ~,~tersecL~o~ e~cod'i,~g. The order of the intersections is relevant, since the line should 
be oriented. We adopt a convention that the inside is to the right when going from the first intersection 
to the second. The size of the table can be reduced by making it into a triangular array, and using an 
extra bit to indicate the direction, which will tell whether to complement the mask or not. The table 
is of course precomputed, and each bit is on if the subpixel corresponding to it is more than half-covered 
by the halfplane described by the index. Of the four edges of a normal span-area, two are horizontal, 
and are relevant only at the start and at the end of its scanning. For these, a small special lookup 
table can be used, with the y fraction used as the index. For the other two edges, updating the intersection 
information from pixel to pixel is fairly simple, and requires only additions and subtrac-tions. The 
coverage mask has interesting boolean proper-ties. Indeed, the mask for the intersection of a span-area 
with a pixel is the and of the masks of the span-areas edges which cross the pixel. Thus we get an accurate 
representation of the subpixels covered by a given span-area. It is also easily seen that the mask for 
the background (indicating the subpixels where the background is seen) is the com-plement of the or of 
all the span-area masks for this pixel. It is unfortunately impossible to go much farther without making 
some approximations. The problem is that we do not want to compute the Z values at the subpixel resolution, 
since it would be tantamount to going to a higher resolution. Each span-area at a given pixel is then 
associated with only one Z value, namely its Z at the centre of the pixel. Given that, we cannot guarantee 
that the depth comparison allows the visible areas to be exactly determined, unless the planes of support 
of the span-areas do not intersect within the pixel (see Figure 3). We will give two approximation algo-rithms, 
and discuss where they succeed, and where they fail. Let weight (mask) be the fraction of the pixel covered 
by a mask (this can be easily com-puted by counting the number of one bits in the mask). The span-area 
with the smallest Z value is called the zvirLner; the others are called losers. There are two ways to 
compute the final pixel inten- sity. One way necessitates the use of an X-bucket to hold pixel information 
for each span-area intersect-ing with the current scanline; a pass over the con-tent of this bucket would 
be performed at the end of the scanline, since the final intensity cannot be computed until the winner 
is known. The other approximation can be computed on-the-fly, and is almost as accurate as the first. 
The two methods calculate intensities fz and 12, respectively, as fol-lows. It = ff~.'n~e~'Comp + LoserCo~rtp 
1 + B~ckWro~nddb~rtp [m= ~vtvte~.Corctp + Los~rCor~p 8 + Igaclc~rou~tdCo~p W~vtne~Comp = 1. x~veight 
(.~ask~ ) B~.ckgrouTtdCov%p = It x~uegh~ (AAtlMa~k~ ) ZoserCornp * = Cot ,x~l~=,, x'wegh, f (m.ask~ 
Arn, o.s/c.=, ) Lose?'Corrt~ = = CoT" =xtueglt~ (rrtcslc= )X ~.~ x~ue~gltf (rrtllskl ) =ll l oe~,llh, 
t (ra.ask,~ Arn, asl~ ) , CO~"~ = ~ue~glt (wta.slcb Ar/taSP.~.). The subscripts ~o ,t, and b, stand 
for "winner", "loser", and "background", respectively. The correction factors are ratios of the actual 
coverage by the losers compared to the sum o3 their indivi- dual coverage as computed by each algorithm. 
Therefore the correction factors give a measure of the amount of overlap of the losers, hence of the 
possible error.  3.2.5.1. First approximate anti-aliasing algorithm This solution requires an X bucket. 
For each pixel, several additional pieces of irdormation are kept: the current winner, background data, 
the losers' intensity, and their sum of coverage-mask weights. The following data structures are used. 
 x bucket. Xp contains list of number of span-areas per pixel I X:matrix xmin..xmax I..PV of PixeIInfo 
Xp: array xmin..xmax of 0..PV := 0 Additional pixel information I Pixels: array xmin..'xmax of Winner, 
Back: PixeIInfo Loserlnt, Sum0fWeights: Integer  PixelInfo:type record of Depth Int ~intensity Mask 
~ coverage mask J end Pixel[nfo The ScanConvert routine above executes the follow- ing version of UpdatePixel 
and AntiAliasSeanline. Recall that each PE executes ScanConvert. procedure UpdatePixel(x: xmin..xmax, 
pix: PixelInfo) Add pixel from this span into bucket X[x,RepAdd(Xp[x], I)] := pix pix may be a "winner" 
 RepMin(Pixels [x] .Winner,pix) I Determine how much background is covered by pix  RepAnd(Pixels [x].Back.Mask, 
pix.Mask) end UpdatePixel procedure AntLAliasScanline(y: ymin..ymax) x:Integer winner,pix: PixelInlo 
 Initialise Cx to xmin while Cx ~ xmax do be~ } Many PEa work on each pixel (i.e. X bucket) x := 
RepAdd(Xp[Cx],-1) + i ~ get pixel info for span winner := Pixels[Cx].Winner while x > 0 do begin pix 
:= X[Cx,x] if pix ~ winner then begin pix is a loser, calculate its contribution newMask := pix.Mask 
A winner.Mask newlnt := Weight(newMask) x pix.lnt RepAdd(Pixels [ Cx].Loserlnt, newInt) RepAdd(Pixels 
[Cx]. SumOfWeights, Weight(newMask)) end if x := RepAdd(Xp[Cx],-I) + 1   end while if x = 0 then 
begin PE with x=O adds background and losers' contrib I for Pixels[Cx], compute: c := Weight(Back.Mask/\Winner.Mask)/SumOfWeights 
RepAdd(Winner.Int, cxLoserlnt + Back.lnt) R~pAdd(Cx, i) end then else <synchronise> }all other PEa 
waitl  end while end AntiAliasScanline 3.2.5.2. Second approximate anti-aliasing algo- rithIn No X 
bucket is required in this solution. We only keep four pieces of information for each pixel, Winner, 
Back, Sum0fWeights, and Losers. Winner, Back, and Sum0fWeights are as in the first solution; Losers is 
used to keep track of the losers' coverage and intensity contributions on-the-fly. procedure UpdatePixel(x: 
xmin..xmax, pix: PixelInfo) loser: PixelInfo loser := RepMin(Pixels[x].Winner, pix) intContrib := loser.lnt 
x Weight(loser.Mask) RepAdd(Pixels [x].Loser s.lnt, intContrib) RepAnd(Pixels [x].Back.Mas k, loser.Mask) 
 RepAdd(Pixels [x].S um 0fWeights, Weight(pix.Mask)) end UpdatePixel procedure AntiAliasScanline(y: 
ymin..ymax) each PE handles a pixel, so if N > X, some PEs are idle I x := PEid + xmin -I while x 
< xmax de begin pix := Pixels[x] compute background and losers' intensity contrib I backlnt := pix.Back.Int 
 Weight(pix.Back.Mask)  c := Weight(pix.Back.Mask/Xpix.Winner.Mask)/pix.Sum0fWeights loserInt := pix.Losers.Int 
x Weight(pix.Winner.mask)  c RepAdd(Pixels [x].Winner.lnt, back[nt+loserLnt) x:=x+N end while  end 
AntiAliasScanline 3.2.,5.3. Analysis of the approximations These approximations, and indeed all approxima- 
tions of this kind, should be characterised in three ways: when they are right (here right is to be under- 
stood exact within the subpixel resolution), when they are wrong and how wrong they can be, and when 
they are cortsiste~ztty wrong. The last is important, since aliasing is particularly noticeable  Computer 
Graphics in motion, by crawling, scintillation and other annoying artifacts. If an algorithm computes 
a wrong shade, but is consistent as the polygons move, then these artifacts will be avoided. Both solutions 
will be right when there is only one span-area within the pixel, whether it covers the whole pixel or 
not. As long as a span-area covers at least one subpixel (1/64 of a pixel in our example), it will contribute 
to the total intensity of the pixel. Both solutions are also right when none of the span-areas overlap. 
This is especially important, since we might have cut a polygon into numerous small span-areas. Fortunately 
we will not have to pay a heavy price in aliasing problems. In fact, the problems, if any, will be at 
the silhouette edges of the objects, and not against the background, but against each other. The first 
solution has the addi-tional advantage of being right when the winner overlaps the losers, but the losers 
do not overlap each other. The second algorithm will be right in case of overlap by the winner if the 
loser coverage ratio is sensibly the same under the winner as in the rest of the pixel. Figures 3 and 
4 give examples of wrong cases, and the errors made by each algorithm. Figure 3 shows the worst case 
for both algorithms, where the amount of overlap of the losers and the area they cover is maximal. Figure 
4 shows a case where the first algorithm is right and the second is wrong. ~xture 3. The worst case for 
both algo'M,f, hrr~s. Span 1, the winner, covers a sliver of the pixel. Losing span 2 obscures another 
loser, 3. Z I<Z ~<Z 3 Correct Answer -~ Iz Iz + Is Computed Answer = -- 2 Error = 18 -Is 2 Figure 
4. A bad case for algor'~thrn 2 o~y. Winning span 1, covering half the pixel, obscures losing span 3. 
Losing span 2 covers half the pixel as well. Z,<Zz<Zs Correct Answer = Ii + le Z i  Algorithm 1 - I1 
+ I8 2 It+ Iz+Is  Algorithm2 = 2 2 Error 2 = Iz - [a 4 A gross estimate of the extent of the errors 
for 103 polygons, covering an average of l0 s pixels each, and  Volume 17, Number 3 July 1983 with 
10 2 boundary pixels each, on a screen with 10 e pixels, shows that less than 5E of the pixels would 
have an error, and that for these the average error would be less than 10~ of the shade of the pixel. 
As the polygons move with respect to each other, we avoid the numerous problems of point sampling. Since 
the wrong cases are computed from averages, the errors made will not exhibit large discontinui-ties, 
but will be consistent from frame to frame. In the example of Figure 3, as polygon 3 moves out of the 
pixel, its contribution to the pixel intensity will go smoothly from ~-13 (which is wrong), to 0 (which 
is right). 3.2.6. Discussion An implementation of this algorithm has been made, demonstrating that the 
approach works, and i11us-trating a realisation in a pseudo-concurrent language. The implementation is 
written in Con-current Euclid, a language developed at the Univer-sity of Toronto which supports processes 
and moni- tors. Concurrent operations such as RepAdd are simulated using monitors. The only relevant 
aspect in which our implementation differs from one on an ultracomputer is speed. The lack of "true" 
con-currency, and the 0(N) performance of concurrent operations (compared to O(IogN) on an ultracom-puter), 
make our implementation somewhat slower than would be expected on an ultracomputer. The algorithm above 
has several appealing proper-ties. It is independent of N, the number of PEs in the ultracomputer. Indeed, 
the speed of the algo-rithm is inversely proportional to N, up to a lower >XY bound constant when N --~--. 
A good serial algo- rithm is obtained when N= i. We emphasise the fact that the anti-aliasing techniques 
presented here easily transfer to serial environments, as illus-trated here. Another property of the 
algorithm is that although it scan converts polygons, the general approach adapts to other scene representations 
(e.g. scanline methods for parametric surfaces as in [BCLW80]). Several improvements could be made to 
the algo-rithm. An issue deserving of attention is space complexity and memory traffic. By using dynami-cally 
allocated shared memory and pointers, the amount of storage required would be drastically reduced; memory 
traffic would decrease, since pointers would travel through shared memory. However, indirect shared memory 
references require two passes through the connection network. A solution is to make greater use of the 
cache memory local to each PE. A copy of the static pointers may be placed in the local memory for each 
PE, thus saving the O(log N) connection net-work cycle time. 4. Other Ultracomputer Applications and 
Future Research As the plethora of published parallel algorithms shows [Schw801, the ultracomputer is 
truly a power- ful, general-purpose tool. Fast parallel algorithms exist for matrix multiplication, 
sorting, linear pro- gramming, fluid dynamics, etc. We hope to have demonstrated that the ultracomputer 
has great 148 potential in the computer graphics field. Other applications would also significantly 
benefit from ultracomputer implementation. For instance, a parallel queue could be exploited to parallelise 
ray-tracing algorithms [WhirS0]. Since the processing of one ray (or alternatively, one pixel) is an 
indepen-dent task, we believe significant speed-up in ray-tracing can be achieved on an ultracomputer. 
Simi-larly, we believe many problems in image process-ing, signal processing, and artificial intelligence 
are likely to benefit. 5. Acknowledgements We wish to thank John Amanatides and Peter Schoeler for their 
suggestions, which have improved the clarity of this paper. The first two authors gratefully acknowledge 
the financial sup- port of the Natural Sciences and Engineering Research Council of Canada. References 
BCLW80 Blinn, J.F., L.C. Carpenter, J.M. Lane, and T. Whitted, "Scan line methods for displaying parametrically 
defined surfaces", Comm. ACM 23, 1 (Jan. 1980), 23- 34. Catm78 Catmull, E., "A Hidden-Surface Algorithm 
with Anti- Aliasing", Computer Graphics (ACM), 12, 3, (Aug. 78), 6-11. Clar82 Clark, J.H., "The geometry 
engine: a VLSI geometry system for graphics", Computer Graphics (ACM) 16, 3 (July 1982), 127-134. Crow'/? 
Crow, F.C., "The Aliasing Problem in Computer- Generated Shaded Images", Comm. ACM 20, 11 (Nov. 1977), 
799-805. CrowS1 Crow, F.C., "A Comparison of Antialiasing Tech- niques", IEEE Computer Grc*pltics a~.d 
Applieat~o~zs, 1, 1 (Jan. 81), 40-49. FPPB82 Fuchs, H., J. Poulton, A. Paeth, and A. Bell, "Develop- 
ing PIXEL-PLANES, a smart memory-based raster graphics system", 1982 Conference oR Advanced ReseaTch 
in VLSI, MIT, January 1982, 137-146. FoFuB3 Fournier, A. and D. Fussell, "On the Power of the Frame 
Buffer", unpublished manuscript, 1983. FuBar79 Fuchs, H. and J. Barros, "Efficient Generation of Smooth 
Line Drawings on Video Displays", Computer Graphics, 13, 2, (Aug. 79), 260-269. FuPo81 Fuchs, H., and 
J. Poulton, "PtXEL-PLANES: a VLSI-oriented design for 3-D raster graphics", CMCCS Conference Proceedings, 
(June 1981), 343-348. Full82 Fussell, D., and B.D. Ratl~i, "A VLSI-oriented architec- ture for real-time 
raster display of shaded polygons", Graphics/nferfa~e "82,May 1982, 373-380. Fueh77 Fuchs, H., "Distributing 
a visible surface algorithm over multiple processors", Proceedings of ACM 1977, Seattle (Oct. 1977), 
449-481. C~KM8$ Gottlieb, A., R. Grishman, C.P. Kruskal, K.P. ~IcAu- liffe, L. Rudolph, and M. Snir, 
"The NYU Ultracomputer--designing an MIMD shared memory parallel computer", IEEE Transactions on Comput-ers, 
C-32, 2 (Feb. 1983), 175-189. GoM~3 GuSS81 La~-75 Lee81 NeSp?9  ParkS0 Sehw80 SuSS74 Wein81 WhWe51 Whe152 
WhitSO Whit81 Gottlieb, A., B.D.Lubachevsky, and L. Rudolph, "Basic techniques for the efficient coordination 
of very large numbers of cooperating sequential proces-sors", l~ransact~i,o~s oR Progrcsrnrni, ng Languages 
a~l ~ystem.s (ACM) 5, 2 (Apr. 1983), 164-189. Gupta, S., R.F. Sproull, and I.E. Sutherland, "A VSLI architecture 
for updating raster scan displays", ComputeT Graphics (ACM) 15, 3 (Aug. 1981), 71-78. Lawrie, D.H., "Access 
and alignment of data in an array processor", IEEE Transactions oR Computers, (:-.24,12 (Dec. 1975), 
1145-1155. Lee, D.T., "Shading of regions on vector display dev- ices", ComputeT GrapAics (ACM) 15, 
3 (Aug. 1981), 87-44. Newman, W.M., and R.F. Sproull, Principles of 2nteTac~ve ComputeT Graphics, Second 
Edition, McGraw-Hill, New York, 1979. Parke, F.I., "Simulation and expected performance of multiple processor 
z-buffer systems", Computer Ccraph~cs (ACM) 14, 3 (July 19B0),48-56. Schwartz, J.T., "Ultracomputers", 
Tran~ac~$on.v oR Programming I_~nguages and ~lstern~ (ACM) 2, 4 (Oct. 1980), 484-522. Sutherland, I.E., 
R.F. Sproull, and R.A. Schumacker, "A characterization of ten hidden-surface algo-rithms", Compu~ng Surveys 
(ACM) 6, 1 (March 1974), 1-55. Weinberg, R., "Parallel processing image synthesis and anti-aliasing', 
Computer Graphics (ACM) 15, 3 (Aug. 1981), 53-62. Whitted, T., and D.M. Weimer, "A software test-bed 
for the development of 3-D raster graphics systems", Computer Graphics (ACM) 15, 3 (Aug. 1981), 271-277. 
Whelan, D.S., "A rectangular area filling display sys- tem architecture", Computer Graphics (ACM) 16, 
3 (July 1982), 147-153. Whirred, T., "An improved illumination model for shaded display", Comm. ACId 
23, 6 (June 1980), 343- 349. Whitted, T., "Hardware enhanced 3-D raster display systems", CMCCS Conference 
ProceedS, rigs, (June 1981), 349-356.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801144</article_id>
		<sort_key>151</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[Anti-aliased line drawing using brush extrusion]]></title>
		<page_from>151</page_from>
		<page_to>156</page_to>
		<doi_number>10.1145/800059.801144</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801144</url>
		<abstract>
			<par><![CDATA[<p>This algorithm draws lines on a gray-scale raster display by dragging a &#8220;brush&#8221; along the path of the line. The style of the line is determined by the properties of the brush. An anti-aliasing calculation is performed once for the brush itself and thereafter only a trivial additional operation is needed for each pixel through which the brush is dragged to yield an anti-aliased line.</p> <p>There are few constraints on the size, shape, and attributes of the brush. Lines can be curved as well as straight, It is possible to produce lines with a three dimensional appearance.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Anti-aliasing]]></kw>
			<kw><![CDATA[Filtering]]></kw>
			<kw><![CDATA[Line drawing]]></kw>
			<kw><![CDATA[Painting]]></kw>
			<kw><![CDATA[Raster display]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Paint systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Line and curve generation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P285628</person_id>
				<author_profile_id><![CDATA[81100586999]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Turner]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Whitted]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Laboratories, Holmdel, New Jersey]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ackland, Bryan D., and Neil H.E. Weste, "The Edge Flag Algorithm - A Fill Method for Raster Scan Displays," IEEE Transactions on Computers,C-30, 1, January 1981, pp. 41-48.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807454</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Barros, Jose, and Henry Fuchs, "Generating Smooth 2-D Monocolor Line Drawings on Video Displays," Computer Graphics,13, 2, August 1979, pp. 260-269.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bishop, Gary, "Gary's Ikonas Assembler," Department of Computer Science, University of North Carolina at Chapel Hill, 1982.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907242</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Catmull, Edwin E., "A Subdivision Algorithm for Computer Display of Curved Surfaces," PhD thesis, Dept. of Computer Science, University of Utah, December 1974.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807360</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Catmull, Edwin E., "A Hidden-Surface Algorithm with Anti-Aliasing," Computer Graphics,12, 3, August 1978, pp. 6-11.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907952</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Crow, Franklin C., "The Aliasing Problem in Computer Synthesised Shaded Images," PhD thesis, Dept. of Computer Science, University of Utah, March 1976.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807359</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Crow, Franklin C., "The Use of Grayscale for Improved Raster Display of Vectors and Characters," Computer Graphics,12, 3, August 1978, pp. 1-5.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Crow, Franklin C., "A Comparison of Antialiasing Techniques," IEEE Computer Graphics and Applications,1, 1, January 1981, pp. 40-48.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807507</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Feibush, Eliot A., Marc Levoy and Robert L. Cook, "Synthetic Texturing Using Digital Filters," Computer Graphics,14, 3, August 1980, pp. 294-301.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806783</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Gupta, Satish, and Robert F. Sproull, "Filtering Edges for Gray-Scale Displays," Computer Graphics,15, 3, August 1981, pp. 1-5.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806784</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Kajiya, James T., and M. Ullner, "Filtering High Quality Text for Display on Raster Scan Devices," Computer Graphics,15, 3, August 1981, pp. 7-15.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360684</ref_obj_id>
				<ref_obj_pid>360680</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Kernighan, Brian W., and Lorinda L. Cherry, "A System for Typesetting Mathematics," Communications of the ACM,18, 3, March 1975, pp. 151-157.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Kernighan, Brian W., "PIC - A Graphics Language for Typesetting," Software Practice and Experience, January 1982.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Newman, William M., and Robert F. Sproull, Principles of Interactive Computer Graphics, (McGraw-Hill, New York, ed. 2, 1979), p. 263.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Ossana, Joseph F., "NROFF/TROFF User's Manual," Bell Laboratories Computing Science Technical Report #54, 1976.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807456</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Smith, Alvy Ray, "Tint Fill," Computer Graphics,13, 2, August 1979, pp. 276-283.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Smith, Airy Ray, "Paint," in Tutorial: Computer Graphics, J.C. Beatty and K.S. Booth, eds., (IEEE Press, 1982), pp. 501-515.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357309</ref_obj_id>
				<ref_obj_pid>357306</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Turkowski, Kenneth, "Anti-Aliasing through the Use of Coordinate Transformations," ACM Transactions on Graphics,1, 3, July 1982, pp. 215-234.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806813</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Wallace, Bruce A., "Merging and Transformation of Raster Images for Cartoon Animation," Computer Graphics,115, 3, August 1981, pp. 253-262.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807508</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Warnock, John, "The Display of Characters Using Gray Level Sample Arrays," Computer Graphics,14, 3, July 1980, pp. 302-307.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Anti-Aliased Line Drawing Using Brush Extrusion Turner Whitted Bell Laboratories Holmdel, New Jersey 
07733  Abstract This algorithm draws lines on a gray-scale raster display by dragging a "brush" along 
the path of the line. The style of the line is determined by the properties of the brush. An anti-aliasing 
calculation is performed once for the brush itself and thereafter only a trivial additional operation 
is needed for each pixel through which the brush is dragged to yield an anti-aliased line. There are 
few constraints on the size, shape, and attributes of the brush. Lines can be curved as well as straight, 
It is possible to produce lines with a three dimen- sional appearance. CR Categories and Subject Descriptors: 
B.3.2 [Memory Structures]: Design Styles-cache memories; 1.3.3 [Computer Graphics]: Picture/Image Generation-display 
algorithms General Terms: Algorithms Key Words: Line Drawing, Anti-aliasing, Painting, Filtering, Raster 
Displays Prior Work This paper describes a method of displaying anti- aliased lines on a raster display 
by painting with an anti- aliased brush. It combines a painting technique developed by Smith [17] with 
an adaptation of an anti-aliasing tech- nique described by Crow [7]. While there are several commercial 
anti-aliasing paint systems, the one described here is particularly flexible, yet simple to implement. 
Line drawing on raster displays usually emulates plotters, i.e. the lines are narrow and have uniform 
shade. Unless the line drawing procedures take account of the sampling that is inherent in raster displays, 
lines will have the jagged appearance of a staircase caused by aliases in the sampled signal. According 
to the sampling theorem, graphical objects can be rendered faithfully on a sampled Permission to copy 
without fee all or part of this material is granted provided that the copies are not made or distributed 
for direct commercial advantage, the ACM copyright notice and the title of the publication and its date 
appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy otherwise, or to republish, requires a fee and/or specific permission. &#38;#169; ACM 0-89791-109-1/83/007/0151 
$00.75 raster display only if a low pass filtering step is included in the display process prior to sampling. 
Existing anti-aliased line drawing algorithms fall into two classes: those which emulate plotters [7,10], 
and those more general polygon rendering algorithms which treat lines as long, thin polygons [5,2,9]. 
The latter class, which we shall call "skinny polygon" algorithms in this report, are more difficult 
to implement than the former class. Raster displays are capable of a much richer variety of line styles 
than plotters. Paint programs illustrate one of the simplest methods of exploiting this versatility by 
allowing an artist to drag patterned brushes along the path of brush strokes to create patterned lines. 
Smith describes a technique called "z-paint" that allows users to create lines that appear to be glossy 
tubes by drawing with a brush that looks like a glossy sphere [17]. The algorithm described here is an 
extension of this technique. There is no need to limit painting to artist drawn strokes. Brush extrusion 
paths can just as easily be speci- fied by a vector display list. If alphanumeric characters are defined 
by strokes, then the technique can be applied to text as well. Although anti-aliased text can be more 
rapidly displayed by preparing an entire set of anti-aliased fonts [20,11], the ability to draw stroke 
drawn characters at arbitrary scales is a useful feature.  Painting and Brush Extrusion The fundamental 
operation of paint programs is copy- ing pixel data from a source image to a destination image, dst .-src 
binop dst , usually called "bit-bit" or "raster-op" [14]. For images with one bit per pixel, the binary 
operation binop can be any one of 16 boolean functions of two variables. Paint software copies a small 
source pixel array, the brush, into a larger destination pixel array, usually a frame buffer memory, 
at locations which correspond to the position of a stylus on a tablet. For gray-scale displays, the logical 
binary operator is replaced by an arithmetic operator. The most useful one is linear interpolation: dst 
~ a*src + (1-a)*dst or with one less multiplication dst *--- a* (src-dst) + dst. (1) 151   [13] Kernighan, 
Brian W., "PIC -A Graphics Language for Typesetting," Software Practice and Experience, January 1982. 
[14] Newman, William M., and Robert F. Sproull, Princi-ples of Interactive Computer Graphics, (McGraw-Hill, 
New York, ed. 2, 1979), p. 263. [15 ] Ossana, Joseph F., "NROFF/TROFF User's Manual," Bell Laboratories 
Computing Science Technical Report #54, 1976. [16] Smith, Alvy Ray, "Tint Fill," Computer Graphics, 13, 
2, August 1979, pp. 276-283. [17]Smith, Airy Ray, "Paint," in Tutorial: Computer Graphics, J.C. Beatty 
and K.S. Booth, eds., (IEEE Press, 1982), pp.501-515. [18] Turkowski, Kenneth, "Anti-Aliasing through 
the Use of Coordinate Transformations," ACM Transactions on Graphics, 1, 3, July 1982, pp. 215-234. [19] 
Wallace, Bruce A., "Merging and Transformation of Raster Images for Cartoon Animation," Computer Graphics, 
115, 3, August 1981, pp. 253-262. [20] Warnock, John, "The Display of Characters Using Gray Level Sample 
Arrays," Computer Graphics, 14, 3, July 1980, pp. 302-307. Appendix Real pictures almost never contain 
isolated lines which do not cross each other, but many techniques for anti-aliasing seem to ignore this 
case. Notable exceptions are [5], [2], and [9]. A good reason for ignoring the case of crossing lines 
is that it tends to complicate the anti-aliasing algorithm. An analysis is presented here that should 
shed some light on how good or had various approx- imations are for these cases. Assume that the background 
is a signal of zero fre-quency, i.e. a constant intensity everywhere. If we Simplify the analysis by 
expressing the continuous line i(x,y) as the difference between its value and the background, we can 
conveniently ignore the background. For a pixel cen- tered at screen coordinates (x,y), the intensity 
is given by the convolution integral oo oo I(x,y) ~ y f i(u,v) h(x-u,y-v) dudv UE--oo yE~oo where h (,) 
is the filter kernel. Let I 1 and 12 be the inten- sity of sample (x,y) for crossing lines il(x,y) and 
i2(x,y) respectively when each is filtered in isolation. Further-more, let .41 be the area covered by 
il, .42 the area covered by i 2, and A12 the area of overlap of i 1 and i 2. Then I(x,y) = yyil(u,v) 
h (x-u,y-v) dudv AI + ffi2(u,v) h (x-u,y-v) audv A2 - ffi2(u,v) h (x-u,y-v) dudv hi2 II + 12 -- f overlap 
(i2,.412 ) " (A1) There are four common ways of handling the case of crossing lines: 1) Do the entire 
convolution exactly. This implies that A 2 be clipped against A 1 as in [5], [9], and [21. As stated 
earlier, the clipping is expensive. In the limit, as the sampling frequency nears infinity, supersam- 
pling [8] yields the same result. 2) Let I(x,y) be the greater of 12 or I r Eq. A1 shows that the error 
of this approximation is either ll--foverlap (i2,A 12) or 12--foverlap (i2,A 12 ) . This approach is 
clearly inappropriate for the line styles described in this paper. 3) Let I(x,y) be the sum of 12 and 
11 [7]. The error is equal to foverlap(i2,A12) unless arithmetic overflow occurs. This approach is intended 
for line drawing algorithms that emulate plotters. 4) Paint the lines in depth order, i.e. paint i 2 
first. Line i2's contribution to l(x,y) can be expressed as a linear combination of an intensity derived 
from i 2 and the background ( which is zero from a previous sim- plifying assumption ). Then after painting 
i 2 l(x,y) = ai 2 ~ 12 where (l-a) is the background blending factor for the line i 2 at (x,y). Next, 
after drawing the line i 1, I(x,y) = fli 1 + (1--fl)ai 2 =/3i I + cti 2 -- ot~i 2 =I 1 +12-otfli 2. where 
(1-/~) is the background blending factor for line i r Note that the form of this expression is the same 
as the exactly filtered intensity. The error of the approximation is the difference of the foverlap(i2,Al2) 
and ct/$i 2 functions. This method is the merging operation described in the body of this paper as well 
as in [19]. Because method 4 avoids the computational expense of the method 1, it pays a penalty of somewhat 
lower image qual- ity. The most visible artifact is the leakage of intensity from a line that should 
be completely obscured by a line draw over it. Although this algorithm is a adaptation of supersampling, 
it does not provide quite the same quality. 156 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801145</article_id>
		<sort_key>157</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[Edge Inference with Applications to Antialiasing]]></title>
		<page_from>157</page_from>
		<page_to>162</page_to>
		<doi_number>10.1145/800059.801145</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801145</url>
		<abstract>
			<par><![CDATA[<p>An edge, when point-sampled for display by a raster device and not aligned with a display axis, appears as a stair-case.This common aliasing artifact often occurs in computer images generated by two- and three-dimensional algorithms. The precise edge information often is no longer available but, from the set of vertical and horizontal segments which form the staircase, an approximation to the original edge with a precision beyond that of the raster may be inferred. This constitutes a smoothing of the staircase edge.</p> <p>Among other applications, the inferred edges may be used to reshade the pixels they intersect, thereby antialiasing the inferred edges. The antialiased inferred edges prove a more attractive approximation to the real edges than their aliased counterparts.</p> <p>Presented here are algorithms for the detection and smoothing of edges and the filtering of an image in accordance with the inferred edges.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Antialiasing]]></kw>
			<kw><![CDATA[Edge Inference]]></kw>
			<kw><![CDATA[Filtering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.6</cat_node>
				<descriptor>Edge and feature detection</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Sampling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Smoothing</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010246</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Interest point and salient region detections</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P150698</person_id>
				<author_profile_id><![CDATA[81100193431]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jules]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bloomenthal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Laboratory, New York Institute of Technology, Old Westbury, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Agrawala, A. and Kulkarni, A., "A Sequential Approach to the Extraction of Shape Features", Computer Graphics and Image Processing, v.6, December 1977, pp. 538-557.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Baumgart, B.G., "Geometric Modeling for Computer Vision", Report STAN-CS-74-463, Stanford Artificial Intelligence Laboratory, Stanford University, 1974.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Butler, J.W., Butler M.K., and Stroud A., "Automatic Analysis of Chromosomes", Data Acquisition and Processing in Biology and Medicine, v.3, pp. 261-275, Pergamon, Oxford, 1963.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907242</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Catmull, E.E., "A Subdivision Algorithm for Computer Display of Curved Surfaces", Ph.D. Thesis, University of Utah, Computer Science Department, 1974.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807360</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Catmull, E.E., "A Hidden-Surface Algorithm with Antialiasing", Computer Graphics, v.12, August, 1978.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>359869</ref_obj_id>
				<ref_obj_pid>359863</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Crow, F.C., "The Aliasing Problem in Computer Generated Shaded Images", Communications of the ACM, v.20, November, 1977.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Crow, F.C., "A Comparison of Antialiasing Techniques", IEEE Computer Graphics and Applications, v.1, January, 1981.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Duda, R.O. and Hart, P.E., Pattern Classification and Scene Analysis, John Wiley and Sons, New York, 1973.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807383</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Dungan, W., Stenger, A., and Sutty, G., "Texture Tile Considerations for Raster Graphics", Computer Graphics, v.12, August, 1978.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807507</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Feibush, E.A., Levoy, M., and Cook, R.L., "Synthetic Texturing Using Digital Filters", Computer Graphics, v.14, July, 1980.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Freeman, H., "On the Encoding of Arbitrary Geometric Configurations", IRE Trans. EC- 10, pp. 260-268, June, 1961. 1p Freeman, H., "Boundary Encoding and Processing", Picture Processing and Psychopictorics, B. Lipkin and A. Rosenfeld, editors, Academic Press, Inc., NY, 1970.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>359027</ref_obj_id>
				<ref_obj_pid>359024</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Pitteway, M. and Watkinson, D., "Bresenham's Algorithm with Grey Scale," Communications of the ACM, v.23, November 1980, pp. 625-626.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Roth, S.D., "Ray Casting for Modeling Solids", Computer Graphics and Image Processing, v.18, February, 1982, p. 109-144.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578095</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Rosenfeld, A. and Kak, A.C., Digital Picture Processing, Academic Press, New York, 1969.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>908714</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Stern, G., "A System for Computer-Aided Keyframe Animation" Ph.D. Thesis, University of Utah, Computer Science Department, 1978.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357309</ref_obj_id>
				<ref_obj_pid>357306</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Turkowski, K., "Antialiasing Through the Use of Coordinate Transformations", Computer Graphics, v.16, July, 1982.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Whitted, T., "An Improved Illumination Model for Shaded Display", Communications of the ACM, v.23, June, 1980.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Williams, L.J., "Pyramidal Parametrics", Siggraph notes, Advanced Image Synthesis Seminar, August, 1981; see also Computer Graphics, v.17, July, 1983.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Williams, L.J., "Brute Force in Image Space", Ph.D. Thesis (unpublished), University of Utah, Computer Science Department.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Edge Inference with Applications to Antialiasing Jules Bloomenthal Computer Graphics Laboratory New 
York Institute of Technology Old Westbury, New York Abstract An edge, when point-sampled for display 
by a raster device and not aligned with a display axis, appears as a stair- case. This common aliasing 
artifact often occurs in computer images generated by two-and three-dimensional algorithms. The precise 
edge information often is no longer available but, from the s~t of vertical and horizontal segments which 
form the staircase, an approximation to the original edge with a precision beyond that of the raster 
may be inferred. This constitutes a smoothing of the staircase edge. Among other applications, the inferred 
edges may be used to reshade the pixels they intersect, thereby antialias- ing the inferred edges. The 
antialiased inferred edges prove a more attractive approximation to the real edges than their aliased 
counterparts. Presented here are algorithms for the detection and smoothing of edges and the filtering 
of an image in the inferred edges. accordance with General Terms: Algorithms. Keyw0rds and Phrases: Inference, 
Filtering. Ant ialiasing, Edge CR Categories: 1.3.7 [Computer Graphics]: Three-Dimensional Graphics 
and Realism-- visible line/surface algorithms; 1.4.1 I~ge Processing]: Digitization-- sampling; 1.4.6 
[Image Processing]: Segmentation--edge and feature detection. Permission to copy without fee all or 
part of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. &#38;#169; ACM 0-89791-109-1/83/007/0157 
$00.75 i. Introduction In 1961 Herbert Freeman published "Computer Processinq of Line Drawings" in which 
he introduced the chain coding scheme for representing and processing edges [Freeman 1961; Freeman 1970]. 
Sub- sequent research with line representation has been applied widely to the problems of artificial 
intelligence [Duda 1973; Rosen- feld 1969] but the ability to construct edges from an image has rarely 
been applied to computer graphics. One such application was presented in a 1978 dissertation by Garland 
Stern; as part of an animation system, outlines of animated figures were extracted from an image and 
then smoothed. In 1974, Bruce Baumgart, in a seminal study of geometric modeling, extracted and smoothed 
line drawings from real world images of an object as an initial step in creating a computer model of 
the object [Baumgart 1974]. Eight years later, Scott Roth, using some of Freeman's techniques, extracted 
chain-encoded line structures from a rastered description of three- dimensional objects and, after processing 
the chain-codes, produced quite convincing line drawings of the objects [Roth 1982]. The extraction 
of edges may also be used to modify the original image. In 1977, Lance Williams demonstrated a tech- 
nique ("dekink") for estimating edges of a two-dimensional region and then filtering the horizontal and 
vertical segments of the edges in two separate iterations [Wil- liams]. It is not difficult to imagine 
other applications such as deriving closed con- tours from computer images in order to interpolate images 
or convert image data into a contour form (such as topographical maps). Techniques for extracting edges 
from images are presented by Rosenfeld and Kak in their text Digital Picture Processing [Rosenfeld 19~]? 
~pp~ for smooth- ing the edqes are suggested as well. 2__t. Edge Inference and Antialiasing Aliasing 
of objects (manifested by staircase edges, disappearing objects, or disconnected lines) has been minimized 
by a number of techniques including super- sampling [Crow 1981], adaptive super- sampling [Whitted 1980; 
Roth 1982], and analytical integration for certain kinds of objects [Catmull 1978]. There are also techniques 
which address aliasing in tex- ture [Blinn 1976; Dungan 1978; Feibush 1980; Williams 1981], and highlighting 
[Whitted 1980; Roth 1982]. All of these techniques filter before sampling the image, but, with the excep- 
tion of the computationally expensive super-sampling technique, none of the techniques works well with 
z-buffer based programs which render multiple types of objects (such as polygons, quadrics, and patches). 
Neither do they work well with "paint" programs or other programs which generate images in which edge 
information is not explicitly available. Thus, the most intriguing use of edge inference may be to minimize 
the aliasing artifacts near object edges in the point- sampled computer generated images typi- cally 
produced by z-buffer algorithms or paint programs. Filtering the image along inferred edges enjoys the 
advantages of post-processing flexibility and the con- centration of processing along edges, where aliasing 
is usually most apparent The process of edge smoothing will be described below in two sections. The 
first describes a method for finding and tracking those rastered edges created by the point-sampling 
process. The second section discusses the smoothing of the rastered edges. Filtering an image along the 
smoothed edges is presented in a third section. ~. Edge Tracking The tracking process is performed 
upon a frame of discrete, single-valued regions. It is usually a simple matter to create such a frame 
during the computer generation of an image. Alternatively, rastered images may be processed into a collection 
of regions of interest by vari- ous segmentation methods [Rosenfeld 1969]. A scan-line method similar 
to one developed by J. W. Butler [Butler 1963] was used to produce starting points for all the regions 
in a frame. A scan-line edge-detector could also be used [Agrawala 1977]. 3.1. Tracking a Region The 
"region" containing pixel (x,y) may be defined as the set of pixels con- nected to pixel (x,y) through 
a four- connected path of pixels, all the same value as pixel (x,y). A "four-connected path" is a connection 
or a series of con- nections from a pixel to any of its per- pendicular neiqhbors. In this discussion, 
"starting point" means a pixel located at a lower left corner of a region (see Figure I). Given any 
point within a region, the determina- tion of a starting point requires only a small amount of processinq. 
Once the starting point is known, a reqion's boun- dary may be tracked, forming a closed cir- cuit when 
the starting point is again reached Rather than the chain coding representation of Freeman, the boundary 
is represented here as a sequence of corner locations in frame buffer coordinates (Figure i). This has 
the advantage of readily accessible positional data for the boundary. + + + + + + + + + ~ + represents 
a pixel center represents a corner o is a potential starting point Figure I: A 4-Connected Region 
 The tracking process is presented by the flowchart in Figure 3. The tracking proceeds in a clockwise 
direction about the region; the ability to determine the concavity or convexity of a corner is required 
(see Figure 2). The algorithm ensures that any two corner points share either their x or y coordinate 
and that no three corners are collinear. A detail of the flowchart is given in Figure 4. a convex corner 
a concave corner Figure 2: Two Types of Corners ~iven starting point I ~ yes done? ~ ~exit ~ O UP 
till corner.~ convex? J es l no no <gO LEFT till RIGHT till convex? corn~o convex? corne~ no ~g ~ 
rn~e~ yes o DOWN till co convex? Figure 3: Flowchart for Edge Tracking  no,anI gel  I save upper 
left 1 pixel iorner I no corner Is convex yes I save lower right" pixel corner corner is concave 
Figure 4: Detail for Figure 3 4. Edge Smoothing  Edges of surfaces in real space, when transformed 
to screen space, become aliased if the edge pixel values are not properly computed. This aliasing is 
mani- fested by "jaggies", sudden pixel-sized shifts in an otherwise straight line. Edge smoothing must 
disambiguate intended changes in direction from changes produced by jaggies. A sudden pixel-sized shift, 
or "jag" is determined from four sequential corner points when the direction from point 1 to 2 is the 
same as from 3 to 4 and the distance from point 2 to 3 is one pixel unit. From a repeated jag pattern, 
Figure 5, left, a straight edge may be inferred with reasonable certainty. In Figure 5, right, the jag 
pattern changes midway and a change of slope is inferred. unit pixel length I I I I J f Figure 5: 
Jags The edge inference algorithm cycles through each corner of a region. A first-in, first-out buffer 
of nine points is maintained which, for a given corner point, provides the positions of the four preceding 
and four following points, as well as the length and direction of the segments connecting the points. 
A series of tests is performed on this information to determine if and where a new endpoint should be 
created for the smoothed edge. It is important to note that the inferred edge endpoints are given in 
floating point coordinates resolved beyond the raster. Further, the inferred edges are constrained to 
be within one pixel of the original, rastered edges. 5. Image Filtering In this section "pixel" is 
used to mean the unit square surrounding the pixel center. Given the inferred boundary of a region, those 
pixels in the image which are intersected by the inferred edges are reshaded. In the present implementation, 
the percentage of the pixel covered by the region, as defined by the inferred edges, is first computed; 
it serves as a weight- ing coefficient for averaging the sur- rounding pixels. This is equivalent to 
a simple box filter; more complicated filters may be implemented as look-up tables with the table index 
a function of the distance from the inferred edge to the pixel center [Feibush 1980; Turkowski 1982]. 
 Once the weighting coefficient is estimated, the pixel is reshaded as a linear combination of surrounding 
pixels averaged according to whether or not they belong to the region.  5.1. Computing the Estimate 
 Pitteway and Watkinson developed a method similar to Bresenham's line drawing algorithm which computes 
pixel area covered by an edge [Pitteway 1980].  Referring to Figure 6, one observes that the area covered 
in Pixel B is equal to that covered in Pixel A plus the shaded area. Thus, given an initial area, the 
areas of the remaining intersected pixels may each be computed with a single addi- tion. At Pixel D, 
however, it is neces- sary to subtract the area of the black triangle from the new total. The area of 
the black triangle is then the initial area for Pixel E. This incremental tech- nique is complicated 
by the need to accom- modate endpoints not on a pixel boundary, or edges which re-enter a pixel. increases 
the accuracy of the edge posi- tion but usually limits the program to images generated using polygonal 
models. (Most hidden line eliminators work with polygonal surfaces only.) Conversely, antialiasing is 
not the only application for the inference of edges. 7. Acknowledgements Lance Williams offered consistent 
support for this research, as well as many insightful concepts. Paul Heckbert pro- vided critical reading 
of this paper as well as the model descriptions for Figure 8. E  f J C D Figure 6: Computing Pixel 
Area 6. Conclusion  Proper antialiasing of computer gen- erated pictures requires that the image be 
filtered before it is sampled for display on a raster device [Crow 1977]. Unfor- tunately, a high precision 
representation of the image is not always possible or practical with some hidden surface algo- rithms. 
Certain programs and input dev- ices also produce aliased images. Inferring, and then smoothing, an 
edge from an aliased image can produce convincing results, as shown in Figure 7. Figure 8 demonstrates 
one result of the post-process technique as well as a com- parison with an image which exhibits no aliasing 
artifacts. Figure 9 presents magnified views of Figure 8. The post- process technique exhibits errors 
at some of the vertices as well as on the silhouette of the sphere. Further, the technique does not address 
other forms of aliasing such as small objects disappear- ing and thin, continuous lines breaking up. 
 It is worth noting that all the algo- rithms presented here operate in linear time with respect to the 
length of an edge. Figure 8, a 512 by 486 resolution image, was processed in 17 seconds with a VAX/780. 
 Some aspects of the technique are not limited to inferred edges. Filtering may occur along the true 
edges calculated by a hidden line eliminator program. This Figure 7: Test pattern before (top) and after 
(bottom) processing  8. References  Agrawala, A. and Kulkarni, A., "A Sequen- tial Approach to the 
Extraction of Shape Features", Computer Graphics and Image Processing, v.6, December 1977, pp. 538- 
557.  Baumgart, B.G., "Geometric Modeling for ComDuter Vision", Report STAN-CS-74-463, Stanford Artl---~ial 
Intelligence Labora- tory, Stanford University, 1974. Butler, J.W., Butler M.K., and Stroud A., "Automatic 
Analysis o_~f Chromosomes", Data Acquisition and Processing in Biology and Medicine, v.3, pp. 261-275, 
Pergamon, Oxford, 1963. Catmull, E.E., "A Subdivision Algorithm for Computer ~ of Curved Surfaces", 
Ph.D. Thesis, University o-f U--~h~ Co-~-p~er Science Department, 1974. Catmull, E.E., "A Hidden-Surface 
Al~orithm with Antialiasing", Computer Graphics, v.12, August, 1978. Crow, F.C., "The Aliasing Problem 
in Com- puter Generated Shaded Images", Communica- tions of the ACM, v.20, November, 1977. Crow, F.C., 
"A Comparison o_~f Antialiasin 9 Techniques", IEEE Computer Graphics and Applications, v.l, January, 
1981. Duda, R.O. and Hart, P.E., Pattern Clas- sification and Scene Analysis, John Wiley and Sons, New 
York, 1973. Dungan, W., Stenger, A., and Sutty, G., "Texture Tile Considerations for Raster Graph--~' 
, Computer Graphics, v.12, August, 1978. Feibush, E.A., Levoy, M., and Cook, R.L., "Synthetic Texturing 
Using Digital Filters", Computer Graphics, v.14, July, 1980. Freeman, H., "On the Encoding of Arbitrary 
Geometric Configurations", IRE Trans. EC- i0, pp. 260 -268, June, 1961. ip Free- man, H., "Boundary Encoding 
and Process- ing", Picture Processing and Psychopictor- ics, B. Lipkin and A. Rosenfeld, editors, Academic 
Press, Inc., NY, 1970.  Pitteway, M. and Watkinson, D., "Bresenham's Algorithm with Grey Scale," Communications 
of the ACM, v.23, November 1980, pp. 625-626. Roth, S.D., "Ray Casting for Modeling Solids", Computer 
Graphics and Image Pro- cessing, v.18, February, 1982, p. 109-144. Rosenfeld, A. and Kak, A.C., Digital 
Pic- ture Processing, Academic Press, New York, 1969. Stern, G., "A System for Computer-Aided Keyframe 
Animation"~ Ph.D. Thes{s, Univer- sity of Utah, Computer Science Department, 1978. Turkowski, K., "Antialiasing 
Through the Use of Coordinate Transformations", Com- puter Graphics, v.16, July, 1982. Whitted, T., 
"A__nn Improved Illumination Model for Shaded Display", Communications of the ACM, v.23, June, 1980. 
 Williams, L.J., "Pyramidal Parametrics", Siggraph notes, Advanced Image Synthesis Seminar, August, 1981; 
see also Computer Graphics, v.17, July, 1983. Williams, L.J., "Brute Force in Image Space", Ph.D. Thesis 
(unpublished), University of Utah, Computer Science Department.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801146</article_id>
		<sort_key>163</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Solid modeling(Panel Session)]]></title>
		<page_from>163</page_from>
		<page_to>165</page_to>
		<doi_number>10.1145/800059.801146</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801146</url>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Boundary representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Splines</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003648.10003649.10003657.10003659</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic representations->Nonparametric representations->Spline models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010400</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Point-based models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14022247</person_id>
				<author_profile_id><![CDATA[81100027175]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ronald]]></first_name>
				<middle_name><![CDATA[N.]]></middle_name>
				<last_name><![CDATA[Goldman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Control Data Corporation]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P61757</person_id>
				<author_profile_id><![CDATA[81100084160]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gossard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP309155100</person_id>
				<author_profile_id><![CDATA[81452609795]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Riesenfeld]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P330789</person_id>
				<author_profile_id><![CDATA[81332533732]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Herbert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Voelcker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Rochester]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40036975</person_id>
				<author_profile_id><![CDATA[81544895656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Tony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Woo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Michigan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Light, R.A., "A Dimension Based 2-D Shape Editing Feature for Computer-Aided Drafting Systems", Bachelor's thesis, M.I.T. Cambridge, MA, June 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Gossard, D.C., Light, R.A. and Lin, V., "A Computer Aided Design System for Design by Variation", Proceedings of the Fourth International Conference on Production Engineering, Tokyo, Japan, August, 1980.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Gossard, D.C., Light, R.A., Meyfarth, P.F., and Lin, V.C., "The Use of Symbolic Dimensioning in Computer-Aided Design", Annals of the CIRP, Vol. 29/2/1980.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Lin, V.C., "Three Dimensional Variational Geometry in Computer- Aided Design", Master's thesis, M.I.T., Cambridge, MA, May 1981]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Gossard, D.C., Light, R.A., "Modification of Geometric Models through Variational Geometry", Computer Aided Design, Vol 14, No. 4, July 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PANEL SOLID MODELING CHAIR: Ronald N. Goldman, Control Data Corporation PANELISTS: David Gossard, 
MIT Richard Riesenfeld, University of Utah Herbert Voelcker, University of Rochester Tony Woo, University 
of Michigan CHAIRMAN'S INTRODUCTION: Many different techniques are now avail- able for generating, 
storing, analyzing, and manipulating computer models of solid objects. These techniques include: con- 
structive solid geometry and ray firing, boundary files and local operations, B- spline surfaces and 
subdivision tech- niques, and variational geometry and dimensioning schemes. In this panel dis- cussion 
four leading academic experts representing four distinct and often divergent points of view, will discuss 
the pros and cons, the uses and abuses, the suitability and inappropriateness of these various techniques. 
To set the stage for the discussion, each panelist will give a formal ten to fifteen minute presentation 
on an aspect of solid modeling about which he feels most strongly. The panel chair- man will then lead 
an informal discussion concentrating on specific areas of disagreement among the panelists. Dave Gossard 
received his Bachelor's and Masters degrees in Mechanical Engineering from Purdue University in 1968 
and 1970 respectively. He obtained his PhD from the Massachusetts Institute of Technology in 1975. He 
is currently an Associate Professor of Mechanical Engineering at M.I.T. where he is the Director of 
the Computer-Aided Design Laboratory. Richard Riesenfeld received a Ph.D from the School of Computer 
and Information Science at Syracuse University, Syracuse, New York. He is professor and chairman of computer 
science and head of the Computer-Aided Geometric Design Group in Computer Science at the University of 
Utah, where he joined the faculty in 1972. Riesenfeld has puDlished and consulted in the area of computer 
graphics and CAD/CAM/CAE and has gained an acquaintance both with the academic issues and with industrial 
applications and motivations in the field. The B-spline method of curve and surface design, which he 
proposed in his doctoral thesis, is used by many industries in many countries. His major research focus 
for the last year has been the design and implementation of a CAD system Alpha_l which advances the 
state- of-the-art in both high quality computer graphics and freeform surface models. This system 
is designed to exploit the capabilities of the Oslo Algorithm, on ~hich he collaborated. Herbert Voelcker 
is Professor of Engineer- ing at the University of Rochester and Director of the Production Automation 
Pro- ject. He holds B.S. and M.S. degrees in mechanical and electrical engineering from MIT and a PhD 
degree from the Imperial College of Science and Technology. Mr. Voelcker is a Fellow of IEEE and a member 
of ASME, ACM, SME, and NCS. He served in the U.S. Army for seven years, has worked for or consulted for 
various industrial and governmental organizations, and has won various research prizes, teaching prizes, 
and fellowships. Tony Woo received his PhD in Electrical Engineering at the University of Illinois in 
1975. After his graduation, he taught there in the Departments of Electrical Engineering and Mechanical 
and Industrial Engineering. Currently, he is Assistant Professor in Industrial and Operations Engineering 
and in the Program of Com- puter, Information and Control Engineer- ing. Dr. Woo's research interests 
are in computer graphics, geometric modeling, and computational geometry with applications to manufacturing 
and design. PANELISTS' ABSTRACTS: D. C. Gossard, MIT Boundary representations of solid objects are 
particularly well suited for interac- tive applications. This presentation will discuss the use of interactive 
methods for modifying boundary representations. Dimensions such as appear on a mechanical drawing are 
considered to impose con- straints on the permissible locations of 163 geometry to which they refer. 
These con- straints may be described by a set of non- linear algebraic equations. These con- straint 
equations constitute a description of the entire family of parts which share a generic shape. This approach 
provides a powerful method for modification of models and a foundation for the automatic scaling of 
assemblies of components. Light, R.A., "A Dimension Based 2-D Shape Editing Feature for Computer-Aided 
Draft- ing Systems", Bachelor's thesis, M.I.T. Cambridge, M~, June 1979. Gossard, D.C., Light, R.A. 
and Lin, V., "A Computer Aided Design System for Design by Variation", Proceedings of the Fourth International 
Conference on Production Engineering, Tokyo, Japan, August, 1980. Gossard, D.C., Light, R.A., Meyfarth, 
 P.F., and Lin, V.C., "The Use of Symbolic Dimensioning in Computer-Aided Design", Annals of the CIRP, 
Vol. 29/2/1980. Lin, V.C., "Three Dimensional Variational Geometry in Computer- Aided Design", Master's 
thesis, M.I.T., Cambridge, MA, May 1981 Gossard, D.C., Light, R.A., "Modification of Geometric Models 
through Variational Geometry", Computer Aided Design, Vol 14, No. 4, July 1982. Richard Riesenfeld 
 University of Utah The next generation of geometric modelling systems will probably provide a combina- 
tion of sculptured surface capabilities and constructive solid geometry tools. Toward this goal, the 
Alpha_l System is being developed at the University of Utah. In its evolution we have found the boun- 
dary representations are very useful in achieving this greater generality, for sculptured surfaces do 
not typically exhi- bit a simple intrinsic alternative manner of representation. On the other hand, the 
added complexity of using a spline surface representation may be offset to some extent by making the 
general intersection problem more tractable. The experience of developing Alpha_l has also brought 
us to investigate more relaxed object validity constraints than some which are currently in use. Overall, 
 the advance to spline-based solid model- ling will not be simply an incremental one but rather a larger 
one in which many existing concepts will have to be general- ized. Herbert B. Voelcker University 
of Rochester Solid modeling's raison d'etre is informa- tional completeness, i.e., the notion that one 
can calculate automatically (at least in principle) any well defined geometrical property of a solid 
from a 'solid model'. Two sets of issues arise: how can one represent solids in an informationally complete 
manner, and then how can one use such representations to do useful work? Quite alot was learned about 
representa- tions in the 1970's, and today we have a new generation of CAD/CAM systems entering the market 
that is based on the best of the late 70's boundary- representation, constructive solid geometry (CSG), 
and approximation technologies. As yet these systems can't do very much in a free standing sense, because 
only a few appli- cations (graphics, mass properties, static interference checking) are well understood 
in mathematical and algorithmic terms. Clearly a lot of work is needed to enable solid modellers to support 
a much broader range of applications. The character of this work is easy to specify: mathematiza- tion 
of each potential application, fol- lowed by algorithm development. Results from the first stage --mathemati- 
cal problem formulation --should apply to all solid modeling schemes, while second- stage results will 
be highly scheme- dependent. These latter should enable us to rank the effectiveness of the currently 
competing approaches to solid modeling and design yet another generation of systems that may embody 
elements form all of the current schemes. A useful 81 entry bibliography that is only slightly obsolete 
may be found in "Solid Modeling: A Historical Summary and Contemporary Assessment", by A.A.G. Requicha 
and H. B. Voelcker, IEEE Computer Graphics &#38; Applications, pp. 9-24, March 1982. The four other papers 
in that issue of CG &#38; A provide additional material and reference lists. Tony C. Woo, University 
of Michigan Set operations on boundary models of solids involve at least of the order of N 2 time to 
compute, where N is the number of surfaces of the solids. Because of the 0 (N 2-) nature, we can think 
of set opera- tors as "global" operations. In this talk we examine a class of "local" operations that 
mimic set operations. The issue is how much do they cost? 184 We present a topological data structure 
that requires O(N) storage and gives con- stant expected time for retrieval. This performance is made 
possible by a surpris- ing theorem: In an arbitrary 3D object with V Vertices, E edges, F faces and 
G holes, the average number of neighbours for any face (or vertex) is a constant C. We next define locality 
L = log cN as the number of subsequent topological neigh- bours an operation affects. If C is a small 
constant then a local operation has a global effect but gives a constant time performance. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801147</article_id>
		<sort_key>167</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[A device-independent network graphics system]]></title>
		<page_from>167</page_from>
		<page_to>173</page_to>
		<doi_number>10.1145/800059.801147</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801147</url>
		<abstract>
			<par><![CDATA[<p>The design and implementation of a basic graphics system for a heterogeneous network environment is described. The design has been influenced by the SIGGRAPH Core System, GKS, and proposals being considered by the ANSI Technical Committee on Computer Graphics Programming Languages. It permits hierarchical object definition, direct and indirect attribute specification, screen window management and complex styles of interaction. Important parts of the implementation include a device-independent database for graphical objects, a workstation driver which produces device code, and a device kernel which manages the display list. Problems relating to device independence and network partitioning are discussed.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Attributes]]></kw>
			<kw><![CDATA[Core system]]></kw>
			<kw><![CDATA[Graphical kernel system]]></kw>
			<kw><![CDATA[Graphics input]]></kw>
			<kw><![CDATA[Symbol system]]></kw>
			<kw><![CDATA[Workstation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.2</cat_node>
				<descriptor>Distributed/network graphics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Graphics packages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Device independence**</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Standardization</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P329836</person_id>
				<author_profile_id><![CDATA[81100323682]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Deborah]]></first_name>
				<middle_name><![CDATA[U.]]></middle_name>
				<last_name><![CDATA[Cahn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science and Mathematics Department, Lawrence Berkeley Laboratory, University of California, Berkeley, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P328932</person_id>
				<author_profile_id><![CDATA[81100579868]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Albert]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Yen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science and Mathematics Department, Lawrence Berkeley Laboratory, University of California, Berkeley, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ANSI X3H31. Functional specification of the Programmer's Hierarchical Interactive Graphics System. Working Document ANSI X3H31/82-45 (1982).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807432</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Anson, E. The semantics of graphical input. Comput. Gr. (Proc. Siggraph '79) 13, 2 (Aug. 1979), 113-120.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801269</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Anson, E. The device model of interaction. Comput. Gr. (Proc. Siggraph '82) 16, 3 (July 1982), 107-114.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563869</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Benson, W.H. and Kitous, B. Interactive analysis and display of tabular data. Comput. Gr. (Proc. Siggraph '77) 11, 2 (1977), 48-53.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988586</ref_obj_id>
				<ref_obj_pid>988584</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Buxton, W. Lexical and pragmatic considerations of input structures. Comput. Gr. 17, 1 (Jan. 1983), 31-37.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Cahn, D. U., Johnston, W.E., and Yen, A.C. Network Graphics System Design Document, Version 1.0. Technical Report LBID-644, Lawrence Berkeley Laboratory (Nov. 1982).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806798</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Foley, J.D. and Wenner, P.A. The George Washington University Core System implementation. Comput. Gr. (Proc. Siggraph '81) 15, 3 (Aug. 1981), 123-132.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>6684</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Foley, J.D. and van Dam, A. Fundamentals of Interactive Computer Graphics. Addison-Wesley (1982).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988498</ref_obj_id>
				<ref_obj_pid>988497</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[GSPC (SIGGRAPH-ACM). Status Report of the Graphics Standards Planning Committee. Comput. Gr. 13, 3 (Aug. 1979).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[ISO. Graphical Kernel System (GKS): Functional Description. ISO TC97/SC5/WG2 N117 (Jan. 14, 1982).]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Johnston, N.E., Cahn, D.U., and Johnston, W.E. Grafpac. LBL Report UCID-8094, Lawrence Berkeley Laboratory, Berkeley (Jan. 1979).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Joy, W. et. al. 4.2 BSD System Manual. Draft Report, Department of Electrical Engineering and Computer Science, University of California, Berkeley (Sept. 1982).]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[O'Dell, M.D. The CSS programmer's network semantic model. Internal Report, Lawrence Berkeley Laboratory (Nov. 1982).]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Postel, J. DoD Standard Internet Protocol. RFC 760, IEN 128, USC Information Sciences Institute (Jan. 1980).]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Postel, J. DoD Standard Transmission Control Protocol. RFC 761, IEN 128, USC Information Sciences Institute (Jan. 1980).]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988587</ref_obj_id>
				<ref_obj_pid>988584</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Rosenthal, D.S.H. Managing graphical resources. Comput. Gr. 17, 1 (Jan. 1983), 38-45.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807367</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[van den Bos, J. Definition and use of higher-level graphics input tools. Comput. Gr. (Proc. Siggraph '78) 12, 3 (Aug. 1978), 38-42.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>804734</ref_obj_id>
				<ref_obj_pid>957197</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Wallace, V.L. The semantics of graphics input devices. Comput. Gr. 10, 1 (Spring 1976).]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Yen, A., Holmes, H., and Wood, P. Moving interactive thematic mapping from mainframe to mini: some design possibilities and development experience. Proc. International Symposium on Computer-Assisted Cartography (AUTO-CARTO IV), Vol. II (1979), 379-389.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Yen, A.C. DI/DD Interface Proposal. Internal Report, Lawrence Berkeley Laboratory (Nov. 1980).]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Device-Independent Network Graphics System Deborah U. Cahn Albert C. Yen Computer Science and Mathematics 
Department lawrence Berkeley Laboratory University of California Berkeley, California 94720 Abstract 
The design and implementation of a basic graphics system for a heterogeneous network environment is described. 
The design has been influenced by the SIG- GRAPH Core System, GKS, and proposals being con-sidered by 
the ANSI Technical Committee on Computer Graphics Programming Languages. It permits hierarchi- cal object 
definition, direct and indirect attribute specifi- cation, screen window management and complex styles 
of interaction. Important parts of the implementation include a device-independent database for graphical 
objects, a workstation driver which produces device code, and a device kernel which manages the display 
list. Prob-lems relating to device independence and network parti- tioning are discussed. CR Categories 
and Subject Descriptors: 1.8.2 [Computer Graphics]: Graphics Systems--distributed/net~oork graphics; 
].3.4 [Computer Graphics]: Graphics Utilities-- graphics pczckages; 1.3.6 [Computer Graphics]: Methodol- 
ogy and Techniques--device inc~pendence, interact/on techniques General Terms: Design, Standardization 
Additional Key Words and Phrases: Core System, Graphi- cal Kernel System, symbol system, workstation, 
attri-butes, graphics input 1. Introduction This paper describes the development of a new graphics system 
called the Network Graphics System (NGS) [6] which is intended to support interactive, distri- buted 
applications. In its design we have drawn from our experience with other systems; for example, we partici- 
pated in the George Washington University implementa- tion [7, 20] of the SIGGRAPH Core System [9]. We 
have also been heavily involved with the work on Core-like sys- tems of ANSI X3H3, the Technical Committee 
on Com-puter Graphics Programming Languages [1] and GKS, the ISO standard [i0]. The computing environment 
which supports com-puter science research at Lawrence Berkeley Laboratory is characterized by diversity. 
It includes several dif-ferent host computers, using different operating systems and utility software, 
connected in a local area network. Many types of graphics hardware are in use, including both calligraphic 
and raster devices, color and mono-chrome, storage tube and display-list, interactive and hardcopy, with 
or without local intelligence. Two aspects of this environment have been particu- larly critical to the 
NGS project. First is the existence of the local area network and the possibility of distributed computing. 
A network graphics system should allow dev- ices attached exclusively to a single host to be used by 
applications which run exclusively on other network nodes (due to database location, host configuration, 
software requirements, etc.). A number of operating systems are found on the net- work, in particular 
UNIX, VMS and RSX-11M. Many software packages have different versions to run on dif- ferent operating 
systems and host machines, but this graphics system potentially must deal with multiple operating systems 
in the context of a single distributed application. For example, a mapping application, using a large 
census database which resides on disks available only to a VAX-11/780 running VMS, may need to use a 
Megatek 7255 which is connected to a VAX-11/750 run-ning UNIX. A network graphics system should use inter-process 
communications in such a way that it not only hides operating system differences, but also supports a 
flexible configuration of resources, including graphics devices, host processors and file servers, according 
to applic ation requirements. The second critical feature of our computing environment is that the graphics 
devices to be supported have a wide range of capabilities. Current developments in standards for user 
interfaces to computer graphics, such as the Core System, GKS, and the ANSI XSH3 work in progress, assume 
that general-purpose, medium to high- level graphics functions can be made available in a device-independent 
way. However, existing implementa- t.ions of these standards have set some pragmatic limits, by allowing 
only a single workstation, or only homogene- ous workstations, and by not dealing with a network environment. 
One aspect of this project is a deliberate attempt to test new ideas in device-independent graphics standards 
in a particularly diverse and challenging environment. Applications anticipated for NGS include: display 
and analysis of scientific data, presentation and "business" graphics and picture editors [19, 4]. We 
explicitly did not consider applications such as CAD/CAM modeling and oth- ers in which application-specific 
hierarchical modeling or database systems are included in the non-graphics appli- cation. ~. The Design 
The Core System and our previous Grafpac package [ii] were the two starting points for the design of 
NGS. But we added major new functionality to meet the needs of evolving applications and chaxtging computing 
environ- ments. The following sections describe some significant aspects of this design.  2.1. Object 
Definition and Display Graph/ca/ objects, in NGS, are ca/led structures. The body of a structure is 
a sequence of output primi- tives ~ines, markers, polygons, test, and rester sequences), attribute selection 
.functions, and references to other structures. A reference to another structure can hove one of ttvo 
forms, one macro-like, COPY, which i~2er- polates a copy of the named structure into the ccdging structure, 
and the other subroutine-like, EXECUTE, which is a cagl by reference. Both COPY and EXECUTE can refer 
to structures which have not yet been defined (a null struc- t~zre is referenced). A structure can be 
extended or deleted at any time. Since structures can contain other structures, NGS supports hierarchical 
picture definition. Hierarchical picture definition is needed to help users compose non- trivial pictures, 
especially for applications such as pic-ture editors, chartmaking, and scientific data display which 
do not already include database systems for hierarchical models. Furthermore, dynamic picture modification 
is needed since the picture composition pro- cess is seen as a dynamic interaction between a user and 
a set of pictures displayed at workstations. These requirements meant that a simple method of picture 
definition, the one-dimensional segmentation model of Core-like systems, was inadequate. Hierarchy can 
be built on top of a Core-like system, but dynamics cannot. NGS conceptually includes a database system, 
that is, it provides procedures to create, retrieve, modify and destroy structures. This kind of simple 
modeling facility is frequently called a symbol system [6]. It does not pro- vide for inclusion of application 
data in the graphical database. The two structure invocation facilities, coPY and EXE- CUTE, support 
different application needs. Copied struc- tures lose their separate identity and are not affected by 
changes to the original; sometimes this is the desired effect. On the other hand, all instances of a 
structure called by EXECUTE imrnediateiy reflect chart~;es to the ori- ginal. tn addition to its body, 
a structure also has a header. The header initializes the attribute stale when the sbructure is ~rwversed; 
it may held a specific aftra-bute value or specify that ~rL at~'i, bute value is "inhev- ited" from its 
caller. The current att~bute sta2e is saved when a structure is called and restored when it exits. The 
structure header (and only the header) may be d~rmzmi- cally edited at any time. The separate structure 
header allows structure con- tent to be modified dynamically in a limited way. For the applications NGS 
is to support, we felt that a more ela- borate modeling system which permitted generalized dynamic editing 
of structure elements was not necessary. Attributes in NGS include the modeling and viewing transformations, 
so that the geometry of an image, as well as its appearance, can be affected by the hierarchi- cal relationships 
and dynamic interaction. The process of picture definition is independent of picture displczy. A picture 
is displayed by explicitly post- ing one or more sbructur~s to a workstation. This con- ceptually sends 
a copy of the named structures and all their hierarchical descendants to the workstation for display. 
As with the other structure invocations, a StTUC- tare can be posted before it is definzd. Orthogonality 
of object definition and picture display facilitates a very spontaneous interaction style in multi- workstation 
applications, since the creation of objects and the use of them to compose and display pictures on devices 
is completely independent. The application has completely flexible control over what image or combina- 
tion of images is displayed on each display surface. Since structures can be referenced before definition, 
top-down or other structured approaches to hierarchical object definition are permitted; the picture 
can be displayed in real-time as created. In NGS, storage of structures in a device-independent form 
is implicit and is performed above the workstation level. This differs from GKS, which stores device-independent 
segment definitions on a special kind of workstation. Both approaches contrast with that of the Core 
System, in which no device-independent t~ormation is retained and the association between segments and 
devices is determined by the state of the system at seg- ment creation time. t~ctures can also include 
non~etained or unstruc-tured data. These are output primitives, attribute selec-tions, and structure 
invocalions which exist outside of any structure. Unstructured data is not kept in any device-independent 
display file, and may disappear from a workstation when a picture regeneration is performed. Unstructured 
data allows an application to bypass the complexity and overhead associated with storage in a device-independent 
database. 168 2.2. Attributes NGS has a two tier attribute model, allou~nll an application both to 
select (or set) attribute values and, when ~p,rotm~zte , to define the ~orkstation-dependent moanings 
of those values. Each workstation maintains its mun set of attribute registers, one register for each 
of the many attributes. The value in any register can be set globally, by an explicit function which 
applies to all workstations, or locally, using a stack discipline, during traversal of a st~cture. Most 
attributes can be specified either directly, by giving a particular value, or indirectly by giving an 
index into a table of values. Each workstation has its men table for each indexed attribute. The table 
entries, ~hich can be workstation-dependent, are initialized by NGS and may be redefined by the application 
at any time. Indirect specification of attributes allows the defini- tion of an object to be device-independent, 
but permits its interpretation to vary on different devices so that the final visual effect can be the 
same (or different, when that is desired!). For example, the interpretation of color specifications could 
be workstation-dependent to com-pensate for differing hardware renditions of identical RGB coordinates. 
Polygons on one workstation could be filled with solid colors, while being filled with hatch patterns 
on another. Core-like systems only permit device- independent picture specification, not device- independent 
rendering. In addition, any interpretation, traversal, or binding of a structure to a device-specific 
image is conceptually postponed until the latest possible moment, when the device is actually displaying 
the pic- ture. Thus an image can be modified by dynamic redefin- ition of the workstation attribute tables. 
 GKS and the work of ANSI XSH3 also use indirect specification through the use of index tables, but NGS 
combines this with direct specification. Direct specifica- tion presents a simpler user interface, especially 
when index tables can vary greatly in size between devices. The mechanism of bundling can be used to 
compose additional attributes jerom a set of independent basis attributes. Each workstation has a bundle 
table whose entries are selected by the attribute bundle.J.ndex. Each bundle specifLes a selection of 
basis attributes and the values they are to assume, overriding their individual settings, when that bundle 
is selected. This bundling mechanism is similar to that included in GKS and proposed by ANSI X3H3. It 
could be used, for example, to compose an attribute called density, specify-ir~g how visually dense a 
primitive appears. This attribute would be "bundled" on each workstation as a workstation-dependent combination 
of the attributes linewidth and color. In NGS, the color attributes them-selves are bundled, permitting 
a variety of color models at the user interface. 2.~ Workstations The NGS application interfaces to physical 
devices through the abstract virtual v#orkstation. Each worksta- has at most one logical display surface 
for outtmt and may have one or more logical input devices. A workstation may have one or more "hardcopy" 
devices slaved to it, such as a paper copier or a camera. Some workstatiems are perw~znently de, ned; 
each of these is associated w~th a particular device, and is the default mechanism by which an application 
would access that device. In addition, vJorkstalions can be created dynami- caily. Several of these can 
be assigned to a single terrfti- hal and each allocated a portion of its display surface or other resources. 
The NGS workstation model is an enhanced version of the GKS workstation. "Virtual" workstations allow 
an output surface to be partitioned into different functional areas or screen windows, each controlled 
by an indepen- dent application or subprocess. Such a cEpability helps to support high-quality interactive 
systems [16]. 2.4. Interaction Graphicat intyut is performed by interaction tasks. l~z interaction task 
has an externally visible state con-s/sting of/is value and its trigger. The value is a vari- able which 
can be read by other interaction tasks, and the t~ger is a signal which can be delivered to other tasks 
on request. Interaction tasks m~J exist at several levels. At the "terminal" or physical device level 
are the tm~mitive interaction tasks which are abstractions of physical input devices. A terminal-level 
task may serve several workstations, one at a t~me or simultaneously. For exam- ple, a terminal equipped 
~ith a keyboard has an interac- tion task of type "keyboard". This task J~res a trigger ~henever a key 
is depressed, and its value is that of the key. Above the terminal level, interaction tasks are application-dependent; 
these could be at either the "workstation" or "application" levels. Input devices are much more diverse 
than output devices. Earlier graphics systems have attempted to impose some order on this diversity by 
dividing input devices into logical classes, such as "pick", "valuator", "locator", etc. In these systems, 
detailed control of phy- sical devices and simulation of absent ones is provided by the graphics system's 
input device driver, and is beyond the control of the application [ 18]. Although this model allows an 
application to be tran- sported easily from one device to another, this device independence has a high 
price. Since the way in which a user perceives an interactive system depends primarily on its pragmatics 
or ergonomics [5], an application's user interface is frequently dependent on one particular input device 
or set of devices. Transporting an application sub- stitutes a new set of devices. Because the application's 
designer has no control over the pragmatics of these new devices, the quality of its user interface may 
change dramatically. Therefore, NGS provides a model in which an applica- tion can conceivably access 
physical input devices at a low level. Furthermore, simple abstract input devices can be combined into 
more complex ones in a hierarchi- cal manner. This allows interaction styles to be individu- ally tailored 
for each terminal, while isolating the device dependencies. The NGS design does not define any specific 
logical devices or interaction styles. Rather, it provides a framework on which may be built many dif-ferent 
models of interaction, including the logical input classes of the Core or GKS. Interaction tasks communicate 
through connec-tions, a~td a task may be connected to several client tasks simultaneously. Once a connection 
is established, it must be enabled in order for data to pass across it. Ena-bling a contraction to a 
task starts its measure process, -a/t/cA is respons/b/e/or maintaining the task's value and trigger. 
NGS manages an application-level in2eraction task as a coroutine of the parent program. To define each 
such task, the application supples seven f'uncticns, ~oh/ch are invoked, through NGS, to establish or 
break a connection, to enable or disable the task, to read ttm task's value, or to perform other task-specif/c 
actions. One function serves as the task's meaxwre process, Con-trol is transferred to a measure process, 
by NGS, when the main application calls the AWAIT INTERACTION function. Until the main program's request 
is satisfied, NGS switches control between active measure processes in response to AWAIT INTERACTION 
or FIRE TRIGGER calls. Though it runs synchronously with the main application, a-n appl/cation-level 
measure process has an independent flow of control. The concept of interaction tasks is similar to ideas 
proposed by van den Bos [17] and Anson [2, 3]. While these authors each proposed special languages for 
defin- ing input devices, we wanted NGS to work within the lim- its of existing languages and programming 
systems. Thus, rather than being definitional in nature, our model is procedural, involving explicit 
communications between independent tasks. Because an interaction task is a coroutine of the parent application, 
it may reference glo- bal program variables as well as call on any NGS function. Using coroutines allows 
the establishment of multiple threads of control within a single host operating system process. In general, 
device-independent graphics systems find it very difficult to avoid placing limits on the use of dev- 
ices so that the hardware appears more homogeneous to the device-independent software. Workstation level 
interaction tasks are one way of bypassing these limita- tions. They can be used to perform complex functions, 
including echoing, local image transformations, and modification of workstation attribute definitions. 
NGS also includes another mechanism, escape functions, which allow access, when needed, to the device- 
dependent capabilities of workstations. Reasons for their use include improved performance, more precise 
control over the appearance of the image, and use of the works- ration for special purposes.  3. The 
Implementation Internally, NGS is divided into several major com-ponents (Figure 1). As well as partitioning 
the work of implementatior~ the divisions represent cleavage points at which the code could be divided 
in a distributed sys-tem. These components include: user interface func-tions, device-independent structure 
database, worksta-tion driver, and device kernel. 3.1. Device-Independent Structure Database Structure 
data is kept in the Device-Independent Structure Database (DISDB). The interface to the DISDB is procedural, 
and is used by both the workstation-independent and workstation layers. The workstation-independent layer 
creates, modifies, and deletes informa- tion in the database, while the workstation-dependent layers 
read structure information in order to update their displays. The DISDB provides a means of hiding the 
detailed characteristics of the workstations from the workstation-independent layers. This makes it easier 
to implement new workstation types. Since the upper layers cannot anticipate the requirements of each 
works- tation, transmission of data from the DISDB to a workstation is controlled from below by the workstation 
driver itself. APPLICATION   APPLICATION INTERFACE LAYER  WORKSTATION- ] INDEPENDENT  LAYER DEVI 
CE- INDEPENDENT STRUCTURE DATABASE  I wORKSTATION ] DRIVER DEVICE KRNEL J  TERMINAL Figure 1. Flow 
of Control in NGS A primitive locking mechanism is a part of the DISDB. The entire database is locked 
during a major res- tructuring (such as caused by the DELETE ALL STRUCTURES command), but ordinarily 
workstation drivers are allowed to read a structure body (for example if posted or other- wise needed 
at that workstation) concurrently as the workstation-independent layer is extending it. To sim- plify 
synchronization of writing and reading, the workstation-independent layer notifies the appropriate workstations 
when it sends a packet of data to the data- base. A flexible approach to dynamic storage allocation has 
been taken, since NGS will be running in various environments. The manager of the DISDB accepts data 
descr/~vtors rather than data itself. Storage is usually allo- cated or deallocated dynamically by the 
process which is controlling the generation of the data to fill the storage. This technique is less cumbersome 
than requiring the application to use fixed length blocks only, making multi- ple requests until all 
the information has been passed. 3.2. Workstation Driver The workstation driver is responsible for converting 
device-independent structure definitions into device-specific graphics instructions. A major source of 
com- plexity at this level is the implementation of attributes. At the user level, each attribute corresponds 
to a rag~ster in the workstation; the value in this register can be set in a structure header, structure 
body, or directly by the applications program. The attribute registers are saved when a structure is 
invoked and restored upon exit. This view is a generalization of the way real graphics dev- ices work. 
However, different devices have widely differ- ing sets of hardware registers while the graphics system 
is required to present to the user a uniform set of attri- butes. If there is no hardware register corresponding 
to a specific abstract attribute, then it must be simulated. Usually this means that the value of the 
attribute must be bound within the body of a structure when it is translated into device code. For example, 
NGS defines separate color attributes for different contexts, "vector color", "text color", etc.; however, 
real devices com-monly have only one register to control color. Therefore, it is necessary to incorporate 
code, which sets this regis- ter, into the body of a translated structure whenever a primitive in one 
context is followed by one belonging to another. Other attributes, such as linewidth or text precision, 
may cause the output code to contain different combinations of primitives depending on their values. 
Such artificial bindings must take place when a structure is translated, even though they are not contained 
in its device-independent structure definition. The requirements for hierarchical picture definition 
and device-independent attribute specification compli- cate the design of the driver. A structure can 
be invoked by several callers and inherit a different set of attribute values from each. Therefore, for 
each structure, the driver maintains one or mere incarnations, each of which binds a different set of 
attribute values in its body. An incarnation need not occupy real physical storage. For devices that 
are refreshed from the host machine's memory (non-display-list or "unbuffered" devices) each incarnation 
can be generated "on the fly". On the other hand, with display list devices, each incarnation must be 
computed and stored separately. The driver processes a structure only when it is immediately going to 
be displayed, since at this point the attribute environment is fully known. This attribute environment 
is compared with those of the existing incar- nations of the structure to find a match. If a new incar- 
nation is required, it can be generated in one of two ways depending on the properties of the mismatched 
attri- butes. An attribute may be controlled by a specific device register, but that register is shared 
by other attributes, e.g. vector color or text color if both are used in the same structure. In this 
case, the attribute is set by a specific device instruction. If there is an existing incar- nation such 
that all mismatched attributes fall in this category, the workstation driver makes a new incarnation 
by replacing the required instructions in the existing one. An attribute may be expressed by different 
combina- tions of primitives depending on its value, for example viewing, modeling, and depending on 
the device, polygon fill style. ~uch attributes must be bound during code generation. To change these 
attributes, the driver retrieves a device-independent copy of the structure and retranslates it. The 
workstation driver operates in the context of a virtual workstation. However, it also performs the trans- 
lation from device-independent virtual workstation func- tions to specific device instructions. This 
translation, whose parameters are obtained from the device kernel when the virtual workstation is opened, 
includes convert- ing from virtual screen to physical screen coordinates, and changing references to 
virtual index tables into references to physical ones, 3.3. Device Kernel The device kernel manages a 
single physical worksta- tion, or terminal. This includes creating virtual worksta- tions and allocating 
device resources among them; such resources include display screen space and hardware index tables. Since 
it controls regeneration of the displayed pic- ture, the kernel is responsible for managing display list 
memory for the workstation drivers. In this, it deals only with device-code translations of structures, 
which the drivers transmit. The driver stores these structures, controls their visibility, and also implements 
the struc-ture invocations, COPY and EXECUTE. Computer Graphics Volume 17, Number 3 July 1983 3.4. Network 
Interface NGS accesses the local area network with the set of user-level functions described in [12] 
and [13]. These are built on top of the ARPA TCP/IP [i4, 15] communications protocols. The interface 
functions support network-wide address space, plus name services which can translate a logical resource 
name to a specific network address. A server process may be associated with a network address; then a 
client process can, by opening a connection to the designated address, rendezvous with the server. When 
two processes rendezvous, a virtual c/reu/t is established between them. This is a reliable bi-directional 
link which can handle an indefinite amount of data. A single server process can converse with many clients 
at once and can control the order in which clients' messages are received and processed. Each graphics 
terminal is known on the network by a unique logical resource name. While a terminal can also be addressed 
by its specific network address, use of the resource name insulates user programs from changes in device 
configuration. Thus, to use the terminal as a graphics workstation, a user program opens a connection 
using the terminal's resource name, which activates a server process. This server always includes the 
device kernel, but since the network partitioning may depend on device type, it may also include higher 
levels of NGS. When a user program wants to create a virtual workstation, the server process obtains 
a new network address and returns it to the client while associating itself as the server. The new workstation 
can then be used by any process which has obtained its address. In addition, the virtual workstation 
can be associated with a resource name, so that its specific network address need not be known. We have 
assumed that programs using vir- tual workstations will cooperate; thus, access control and other protection 
mechanisms are outside the scope of NGS. 4. Conclusions NGS is a high-level device-independent graphics 
sys- tem which supports hierarchical object definition and dynamic picture changes. It also allows an 
application to sacrifice device-independence for close centre[ of a specific device if so required. To 
implement these func- tions on a wide range of graphics devices requires a com- plex implementation, 
which is made easier by clearly separating the device-independent and device-dependent parts. Another 
division separates the functions of device code generation from display list management. The net- work 
interface allows a single device to host several vir- tual workstations, each controlled by a separate 
process. At this time, NGS is in the implementation stage, with drivers initially planned for the Tektronix 
4014 and Megatek 7255 graphics devices. We plan to instrument this implementation for performance analysis. 
With information on the amount of data passed to and from each module, we will be able to refine the 
interfaces between NGS subsystems in order to reduce the amount of network overhead. 5. Acknowledgements 
The authors wish to acknowledge the contributions of the membership of ANSI X3H81 and the participants 
in the ACM-SIGGRAPH Workshop on Graphical Input Interac- t.ion Techniques. We would also like to thank 
Bill Johnston for many valuable discussions and general encourage-ment. This work was supported by the 
Applied Mathemati- cal Sciences Program of the Office of Energy Research, U.S. Department of Energy under 
Contract DE-AC03-76SF00098 and by the U.S. Defense Advanced Research Projects Agency under Order 4306. 
 References 1. ANSI X3H31. Functional specification of the Programmer's Hierarchical Interactive Graphics 
System. Working Document ANSI X3H31 / 82-45 (1982). 2. Anson, E. The semantics of graphical input. Corn-~rut. 
Gr. (Prec. Siggraph '79) 13, 2 (Aug. 1979), 113-120. 3. Arisen, E. The device model of interaction. 
Cornput. Gr. (Proc. Siggraph '82) 18, 3 (July 1982), 107-114. 4. Benson, W.H. and Kitous, B. Interactive 
analysis and display of tabular data. Comput. Or. (Proc. Siggraph '77) 11, 2 (1977), 48-53. 5. Buxton, 
W. Lexical and pragmatic considerations of input structures. Comlrut. Gr. 17, I (Jan. 1983), 31-37. 
6. Cab_n, D.U., Johnston, W.E., and Yen, A.C. Network Graphics System Design Document, Version 1.0. Techni-cal 
Report LBID-644, Lawrence Berkeley Laboratory (Nov. 1982). 7. Foley, J]D. and Wenner, P.A. The George 
Washington University Core System implementation. Corn/mr. Gr. (Prec. Siggraph '81) 15, 3 (Aug. 1981), 
123-132.  B. Foley, J.D. and van Dam, A. Fund~me~als of Interactive Computer Graphics. Addison-Wesley 
(1982). 9. GSPC (SIGGRAPH-ACM). Status Report of the Graph- ics Standards Planning Committee. Cornput. 
Gr. 13, 3 (Ang. 1979). 10. IS0. Graphical Kernel System (GKS): Functional Description. ISO TC97/SCS/WG2 
Nl17 (Jan. 14, 1902). 11. Johnston, N.E., Cahn, D.U., and Johnston, W.E. Graf- pac. LBL Report UCID-8094, 
Lawrence Berkeley Labora- tory, Berkeley (Jan. 1979). 12. Joy, W. et. al. 4.2 BSD System Manual. Draft 
Report, Department of Electrical Engineering and Computer Sci- ence, University of California, Berkeley 
(Sept. 1982). 13. O'Dell: M.D. The CSS programmer's network seman- tic model. Internal Report, Lawrence 
Berkeley Labora- tory (Nov. 1982).  14. Postel, J. DoD Standard Internet Protocol. RFC 760, IEN 128, 
USC Information Sciences Institute (Jan. 1980). 15. Postel, J. DoD Standard Transmission Control Proto- 
col. RFC 761, IEN 138, USC Information Sciences Insti- tute (Jan. 1980).  18. Rosenthal, D.S.H. Managing 
graphical resources. Cbrnput. Gr. 17, i (Jan. 1983), 38-45. 17. van den Bos, J. Definition and use of 
higher-level graphics input tools, Comp~i Gr. (Proc. Siggraph '78) la 3 (Aug. 1978), 38-42, 18. Wallace, 
V.L. The semantics of graphics input dev-ices. Comput. Gr. 10, 1 (Spring 1976). 19. Yen, A., Holmes, 
H., and Wood, P. Moving interactive thematic mapping from mainframe to mini: some design possibilities 
and development experience. Proc. IntBrn~- l'ional Symposium en Computer-Assisted Cartography  (AUTO-CARTO 
IV), Vol. II (1979), 379-389. 20. Yen, A.C. DI/DD Interface Proposal. Internal Report, Lawrence Berkeley 
Laboratory (Nov. 1980).  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801148</article_id>
		<sort_key>175</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[A core graphics environment for teletext simulations]]></title>
		<page_from>175</page_from>
		<page_to>181</page_to>
		<doi_number>10.1145/800059.801148</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801148</url>
		<abstract>
			<par><![CDATA[<p>The development of our graphics environment over the past year has been directed towards demonstrating the concept of broadcast teletext service, where pages of graphics and text are received in the home over existing television channels. We have developed a graphics editor, Radix, which uses an underlying Core graphics package to create teletext-compatible images. Radix is also used to display these images at arbitrary resolutions by scaling the graphics primitives, selecting the corresponding sizes of fonts and bitmaps, and then precisely adjusting the output video signal. The combination of powerful display hardware and a robust set of software tools has allowed us to rapidly prototype potential teletext features, and to provide these accurate simulations of teletext pages at any resolution. While this general graphics environment does impose some inefficiencies in running graphics applications, they are far outweighed by the ability to quickly respond to changing requirements.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[fonts]]></kw>
			<kw><![CDATA[graphics editors]]></kw>
			<kw><![CDATA[picture files]]></kw>
			<kw><![CDATA[rapid prototyping]]></kw>
			<kw><![CDATA[resolution independence]]></kw>
			<kw><![CDATA[software tools]]></kw>
			<kw><![CDATA[teletext]]></kw>
			<kw><![CDATA[television compatibility]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Graphics editors</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Graphics packages</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14034086</person_id>
				<author_profile_id><![CDATA[81544526756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Dixon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RCA Laboratories, Princeton, NJ]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AT&amp;T Corp. Frame Creation Terminal. Product literature, 1982.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Dixon, D. F., Keith, M., Marshall, S. A., and Ripley, G.D. Graphics Programmer's Manual. RCA Laboratories, April 1983.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Draft Proposed American National Standard. Videotex/Teletext Presentation Level Protocol Syntax (North American PLPS). October 1982.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[EIA Draft Proposal. A General Service Reference Model for Teletex October 1982.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Feiner, S., Salesin, D., and Banchoff, T. Dial: A Diagrammatic Animation Language. IEEE Computer Graphics and Applications 2, 7 (September 1982), 43-54.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357296</ref_obj_id>
				<ref_obj_pid>357290</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Feiner, S., Nagy, S., and van Dam, A. An experimental system for creating and presenting interactive graphical documents. ACM Transactions on Graphics Systems 1, 1 (January 1982), 59-77.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>6684</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Foley, J. D., and van Dam, A. Fundamentals of Interactive Computer Graphics. Addison-Wesley, 1982.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988498</ref_obj_id>
				<ref_obj_pid>988497</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[GSPC. Status report of the Graphics Standards Planning Committee.  Computer Graphics 13, 3 (August 1979).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Ikonas Graphics Systems. Ikonas RDS-3000 User's Guide. April 1982.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Ikonas Graphics Systems. IDL Reference Manual. May 1982.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578706</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Kernighan, B. W., and Plauger, P. J.Software Tools. Addison-Wesley, 1976.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Ossanna, J. F., Nroff/Troff User's Manual Computer Science Technical Report 54. Bell Laboratories, April 1977.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Pato, J. N., and Nanian, D. B. Simple Graphics Package Users Guide Version SGP2v1. Brown University, September 1982.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Truxal, C. Americanizing videotex. IEEE Spectrum 19, 11 (November 1982), 52-56.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Core Graphics Environment for Tcletext Simulations Douglas F. Dixon RCA Laboratories Princeton, NJ 
08540 ABSTRACT The development of our graphics environment over the past year has been directed towards 
demonstrat-ing the concept of broadcast teletext service, where pages of graphics and text are received 
in the home over existing television channels. We have developed a graphics editor, Radix, which uses 
an underlying Core graphics package to create teletext-compatible images. Radix is also used to display 
these images at arbitrary resolutions by sealing the graphics primi-tives, selecting the corresponding 
sizes of fonts and bitmaps, and then precisely adjusting the output video signal. The combination of 
powerful display hardware and a robust set of software tools has allowed us to rapidly prototype potential 
teletext features, and to provide these accurate simulations of teletext pages at any resolution. While 
this general graphics environ-ment does impose some inefficiencies in running graphics applications, 
they are far outweighed by the ability to quickly respond to changing requirements. CR Categories and 
Subject Descriptors: D.2.2 [Software Engineering]: .Tools and Techniques; 1.3 [Computer Graphics]: 1.3.1 
[CG] Hardware Architecture -raster display dev/ces; 1.3.4 [CG] Graphics Utilities -graphics poe&#38;ages, 
picture description languages, software support; 1.3.6 [CG] Methodology and Tech-niques -device independence; 
1.7.2 [Text Processing] Document Preparation; General Terms: Design, Experimentation, Stan-dardization 
Additional Key Words and Phrases: teletext, rapid prototyping, graphics editors, picture files, fonts, 
software tools, television compatibility, resolution independence 1. Introduction Our involvement with 
teletext vividly demonstrates the need for a simulation system in developing and refining a new product 
concept. In our case, a VAX Permission to copy without fee all or part of this material is granted provided 
that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice 
and the title of the publication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or 
specific permission. &#38;#169; ACM 0-89791-109-1/83/007/0175 $00.75 host computer, Ikonas graphics 
display hardware, and supporting software tools are all being used to simu-late the features that soon 
could be built into home television sets. With this extremely flexible graphics environment, we have 
been able to quickly experiment with new features in response to technical questions, and to package 
demonstrations that have been used to promote the teletext concept to company manage-ment, standards 
organizations, other groups interested in the teletext market, and the press. We have been able to exploit 
the speed and flexibil- ity of our hardware environment by implementing new graphics commands in software 
on the VAX, and relying on the Ikonas microcode to draw them quickly. With this approach, we have been 
able to rapidly prototype new features such as proportional fonts and complex fill styles within a matter 
of days, instead of making the commitment to develop and maintain new micro- code routines for each function. 
Our graphics software environment begins w~th our Radix graphics editor, which allows the creation, edit- 
ing, and display of pictures using teletext primitives, and also provides complete control over the charac-teristics 
of the resulting television image. Radix com- municates with the Ikonas and other output devices through 
a Core graphics package from Brown Univer- sity which not only provides the expected segmenta-tion, input, 
and graphics translation functions, but also was explicitly designed to be re-configurable for exper- 
imentation. This paper describes the technical issues and deci- sions that we have addressed in developing 
a graphics environment directed toward the simulation of pro- posed teletext features. It begins with 
a brief overview of the history and development of videotex and teletext. The following sections then 
provide a general overview of the capabilities of our hardware and software environment by discussing 
our approach in terms of specific problems raised by the teletext requirements. These issues include 
the display of arbitrary-resolution pages on a full television screen, supporting both Core and the teletext 
standard in our Radix editor, the selection and sealing of fonts and bit- maps for a desired resolution, 
and the rapid prototyp- ing of new features in our system. 2. The Development of Videotex and Teletext 
The development of videotex and teletext systems which transmit text and graphics to be viewed on home 
television sets has been a world-wide effort, with much  work done in Europe, Canada, and Japan since 
the early 1970s, followed by more recent developments in the United States [14]. The idea of providing 
a video magazine to consu_mer~ stressing timely features such as news updates, weather reports, and consumer 
features is quite attractive to both broadcasters and advertizers. The user of such a system can browse 
at will, simply by calling up the index page, and then directly requesting a page containing an interesting 
news item, or stepping through consecutive pages of sports scores. 2.1. Videotex and Teletext Videotex 
service provides for two-way communica- tions, so that a user request is forwarded over a tele- phone 
line or cable system to the service provider, who then retrieves the information and transmits it back 
to the user. In contrast, one-way teletext service piggy- backs on the existing television channels by 
transmit- ting digital information in the vertical blanking interval (the time between successive updates 
of the screen which is currently used in the United States to provide captioning for the deaf). Only 
a limited number of pages are therefore available, transmitted in a round- robin cycle. After requesting 
a page, the user must wait until it is next broadcast, so that it can be received, decoded, and displayed 
by the teletext decoder. The method of transmitting pages, however, is independent of the standard used 
to describe and encode the image. The first systems developed by the English and French format a page 
simply as a fixed grid of rows and columns of characters, where only patterns of these alphamosaic characters 
are available for creating an image. While this approach requires only a simple decoder, the resulting 
images have a coarse and blocky appearance due to the low effective graphics resolution, as well as the 
additional limitation that only two colors can be used in each character cell. In Figure 1, the mosaic 
United States weather map clearly shows distinct horizontal and vertical edges at character boundaries, 
and also demonstrates the difficulties of creating small graphics symbols with these mosaic characters. 
 2.2. North American PIPS The Canadians then developed a new display proto- col in conjunction with a 
more complex decoder which can execute instructions to draw graphics primitives into a bitmap. Recently, 
both mosaic characters and graphics primitives have been incorporated into the Presentation Level Protocol 
Syntax (PLPS), which is now being standardized in North America [S]. The advantages of PLPS over mosaics 
include the improved quality of the graphics and bitmaps, as shown by the PLPS weather map in Figure 
2, and also the promise of continued compatibility over a wide range of resolu- tions as graphics technology 
becomes less expensive. RCA Laboratories has been involved with teletext in several areas. We have developed 
prototype decoders to experiment with the transmission and decoding of the PLPS. We also have been involved 
in refining the general PLPS standard and developing minimum requirenlents for teletext use [4]. However, 
a fixed product prototype and paper studies are not flexible enough to quickly answer technical questions, 
or to provide continued support for demonstrating a product concept under changing assumptions. The flexible 
design of our hardware and software graphics environment was the answer for meeting this continu- ing 
need. 3. Resolution Control Using The Ikonas Hardware Our graphics hardware environment, as shown in 
Figure 3, is well suited to our requirements for rapid and accurate prototyping of teletext features. 
The DEC VAX 11/780 and the Ikonas RDS-3000 are both powerful machines, so that we are usually not limited 
by speed or space constraints. Although we do have to be sensi- tive to the needs of the other timesharing 
users on our host VAX, it does provide the large resources such as virtual memory and disk space necessary 
for process- ing graphics data, especially fonts and bitmaps. The Ikonas display processor [9] has a 
general bus-oriented design, with software control over the configuration of each module, including the 
ability to define the precise characteristics of the output video signal. We therefore are able to draw 
an image into the bitmap at any resolution, and then adjust the image so that it fills the entire screen. 
We can then choose to view the image on an RGB monitor, or, since the signal is NTSC-compatible, view 
it on a baseband television monitor or antenna-attached television set. We display pages at arbitrary 
resolutions by first instructing the Core package to scale the graphics into a viewport of the proper 
x and y pixel size in the bot- tom left corner of the Ikonas display memory. Then, we not only zoom the 
display to approximately the correct size, but we also adjust the Ikonas window and viewport registers 
so that only the desired pixels are displayed on the screen. Finally, we position the image on a particular 
monitor with the appropriate safe time (the active image time within each horizontal line) by adjusting 
the pixel clock to compress the image and add more black space on both sides. As a result, we can perform 
fair and accurate comparisons of the same page at different resolutions, with each image filling the 
entire area of the display screen. For exam- ple, the image in Figure 4 was created by displaying the 
weather map page at six different resolutions, cap- turing them in bitmap files, and then composing them 
into a single 512 x 512 bitmap. Notice the variations in aspect ratio and font sizes that are necessary 
to pro- duce final images of the same size once the Ikonas is adjusted for these resolutions. The result 
of this effort has been the ability to not only examine the effect of different resolutions on sample 
pages, but also to study the effect of resolution changes on proposed features of the teletext standard. 
For example, we have been able to make recommenda- tions for minimum resolutions to be considered for 
an initial product, acceptable line and fill styles, and the size and length of text lines under various 
safe time constraints. 4. Core and PLPS Support in the Radix Editor A graphics development environment 
requires not only a general graphics package, but also a format for defining pictures and a means for 
editing and display- ing them. The user's view of our graphics system is focused on our Radix graphics 
editor, which provides all the high-level functions required for both editing and displaying pictures, 
using the underlying Core graphics model [8]. Figure 5 is a general block diagram of the major software 
components of our graphics environment, showing the data flow from using Radix to edit a picture file, 
through the Core package, and then to the display on the Ikonas system. Because of our software tool 
approach, most of our application programs do not call Core directly, but instead direct their output 
through Radix in order to use its higher-level capabilities. In fact, our most-used demonstrations are 
simply command files which prompt the terminal and then generate commands for Radix  4.1. The SGP Core 
Graphics Package Our graphics software is based on the SGP (Simple Graphics Package) Core graphics system 
from Brown University [13], which is described in Foley and van Dam's book [7]. Actually, SGP is simply 
the subroutine-call interface used by the applications pro- grammer, while the major portion of the work 
is done by the COCS (Configurable Graphics System) back-end. COGS is the pipeline of Core graphics functions 
which clips primitives to the current window, maps them to the viewport on the output device, stores 
them in user-named segments, and finally displays them using device-specific drivers SCP / COGS, like 
most of our software, is written in the C language and runs under the Euniee implementa- tion of Unix?, 
running with VMS on our VAX. While SGP is currently a relatively simple 2D Core package, it was designed 
from the start to be easily configurable and modifiable, and has never prevented us from imple-menting 
new primitives and definitions demanded by the PLPS. 4.2. The Radix Graphics Editor The Radix graphics 
editor was developed at RCA Laboratories to create, modify, and display pictures composed of teletext 
graphics primitives and attri-butes. It uses a traditional menu-ormnted user inter-face, along the lines 
of the picture layout system described in [6], with editing operations specified by using a stylus and 
data tablet to indicate data points and select entries from a tree-structured menu. How-ever, Radix was 
designed from the start explicitly for designing teletext pages, so it provides the user with a complete 
set of PLPS-like instructions for page crea-tion, and then translates them into calls to the SCP Core 
package. Figure 6 shows Radix being used to edit the weather map page, with the menu of PLPS primi- tives 
visible, and Figure 7 is a verification page, show-ing the range of PLPS primitives and attributes that 
Radix supports. We have found little problem in mapping between PLPS concepts and Core instructions. 
Although Radix is designed as a vehicle for simulations, and not as a production te]etext system like 
the AT&#38;T Frame Crea-tion Terminal [ 1], we also provide a separate translator [rom Radix format to 
PLPS binary instructions, which are formatted and optimized for broadcast transmis-sion. Radix also includes 
a display mode that turns off the menu and reformats the picture so that it exactly fills the entire 
screen. This allows a page to be edited with the menu at a standard resolution, and then pre- viewed 
with the menu off at arbitrary resolutions and with precise control over the characteristics of the $ 
Unix is a trademark of Bell Laboratories. television image. All the images in this paper were displayed 
using Radix, either in editing mode with the menu on, or in display mode with the menu off. 4.3. Picture 
Files in Text Format We have designed Radix to use an ASCII text format for picture files Each primitive 
is entered as a text string, containing the command name as listed in the menus followed by its numeric 
or text parameters. For example, the picture file text for the verification page is shown in Figure 8. 
While this decision does impose some overhead in parsing and formatting picture files, it does have major 
benefits. The picture files are immediately accessible: they can be directly edited with a screen editor, 
and Lhere is no need for a conver- sion program to print them out for debugging pur-poses. There are 
also many cases where a desirable but uncommon transformation can be easily applied to a picture file 
with a text editor or trivial program which parses and formats text. This kind of structural change to 
the components of an image is almost unLhinkable with hand-editing of its visual representation. In concert 
with the Unix software tools philosophy [11], this text format is also quite convenient for build- ing 
individual tools to manipulate the picture files For example, we have implemented the converter [rom 
Radix text format to PLPS binary, a text-processor [ront-end modelled on Nroff [12], an optimizer to 
remove redundant or unused definition statements, and tools to perform various graphics transformations 
on a picture, including perspective rotation in 3D [2]. Under Unix, we can also interactively pipeline 
the output from one program to the text input for Radix. For example, one demonstration runs a program 
to poll an infrared remote receiver and pipes its output to Radix When a key on the wireless remote is 
pressed, the program outputs page display commands which then are passed directly to Radix and immedi-ately 
executed. In addition, we have converted the Dial animation language from Brown [5] Co generate Radix 
commands from animation scripts. Again, although this imposes additional overhead when previewing the 
animation, it also allows us to use the full capabilities of Radix to specify and display the animated 
sequences, and also to convert the output directly into PLPS instructions. Since our Ikonas driver is 
capable of capturing the generated Ikonas binary instructions in a file, we can package and run official 
demonstrations with no visible delay, even on the timeshared VAX. 5. Variable-Resolution Font and Bitmap 
Support In order to display an image at an arbiLrary resolu- Lion, it is not enough just to scale the 
graphics primi- tives, but the text strings and bitmaps must be adjusted to the correct size as well. 
Text on teletext pages is typically laid out in a grid of twenty rows of forty characters each. When 
displaying pages at different resolutions, Radix is therefore capable of selecting the appropriate font 
size and correctly posi- tioning the text in relation to the graphics primitives. Since the Ikonas hardware 
and microcode directly sup- port character generation with user-defined fonts stored in the Ikonas memory, 
we have been able to build a catalog of fonts in the necessary sizes for the commonly-demonstrated teletext 
resolutions.   5.1. Scaling of Text and Bitmaps 6. The Rapid Prototyping Approach Radix provides two 
levels of support for positioning text in relation to graphics primitives. First, it imple- ments a text 
"gravity" mode which forces each charac- ter of each text string to the correct grid location for the 
current text format by generating individual draw instructions for each character. This is different 
from the more usual graphics gravity mode, which forces the points of each primitive (including the starting 
point of a text string) to the corners of a user-specified overlay grid. Second, since text gravity still 
does not resolve the problem of scaling the text to the current resolution, Radix allows the user to 
either select a specific font by name, or to choose a generic font name and rely on Radix to select the 
closest available size of that font from our font data base to match the current text for- mat and resolution. 
This produces a much more accu-rate result than attempting to algorithmically zoom and scale characters 
on the fly. We also support bitmap drawings defined as large special characters in the teletext Dynamically 
Redefinable Character Set (DRCS). These are imple-mented as single-character Ikonas fonts and also are 
directly supported by Radix, which again selects the closest available DRCS font for the current resolution 
from the DRCS data base. Figures 9 and 10 show a sample sports page displayed at both a reasonable resolution 
for initial teletext service (256 x 210), and double that size to near NTSC television resolution (512 
x 480). Radix not only has scaled the graphics primi-tives into the proper bitmap for each resolution, 
but also has chosen the closest available font size for both the lines of text and the two DRCS baseball 
team logos. 5.2. Font Files and Tools Again following the software tools approach, we also use an ASCII 
text format for our font files, and have implemented several font tools to convert from existing sources 
of fonts to this text format, and then to the Ikonas binary format. This text representation, as shown 
in Figure 11, allows us to use the standard text editor to create and edit fonts, and to perform major 
changes like shifting all the characters in a font. We have also developed tools to algorithmically scale 
fonts so that new sizes can be created with a minimum of hand-editing. As a result, we have quickly developed 
a catalog of over a hundred fonts and DRCS characters. Our philosophy of using software tools permitted 
 us to add support for proportional fonts with only two days of work. First, the font conversion tools 
were changed to add width information for each character, and generate a variable-width binary file for 
the Ikonas driver containing offset and width information for each individual character. "The VAX driver 
then was modified to use the existing Ikonas microcode for fixed-size characters to position each individual 
character of a text line into its correct place in the display. As a result, proportional fonts are now 
available directly from application programs simply by specifying the appropriate font name. For example, 
Figure 12 shows the sports page with the fixed-width font converted to variable-width, and Figure 13 
shows the higher- resolution page with a different font designed explicitly .for proportional spacing. 
Well-defined graphics applications such as aircraft cockpit simulations typically require implementing 
special functions in microcode in order to achieve real-time performance. Howev.er, in our case, the 
speed and generality of our graphics environment gives us the freedom to experiment by implementing experi-mental 
features in the driver running on the VAX, without suffering a major degradation in the speed at which 
our pages are displayed. The Ikonas Display Language (IDL) microcoded interpreter [10], gives us much 
more flexibility in implementing new features than would be provided by the instruction set of a more 
traditional graphics pro-cessor. For example, we have implemented an arbitrary-length delay by generating 
an IDL loop-repeat construct that contains a wait-for-next-frame instruction. Using this approach, we 
have studied possible polygon fill patterns by implementing fill algorithms in the VAX driver. This not 
only allows rapid changes to the built-in fill styles, but also to the interaction of fill with the other 
polygon attributes. We therefore gen-erate and download a long list of horizontal vector instructions 
for each filled polygon. In some cases, the Ikonas microcode still can process and draw these vec-tor 
lists at a speed indistinguishable from the solid polygon fill microcode. Eventually, we will be able 
to define a general-purpose polygon fill microcode routine to satisfy all our needs, including arbitrary 
repeating fill patterns for filled, outlined, and highlighted polygons with different foreground, background, 
and highlight colors. The usefulness of this host-based approach was again reinforced when we were asked 
to simulate the effects of viewing pages when they are generated at different drawing rates. First, we 
slowed down the Iko- has microcode by generating wait instructions between each graphics instruction. 
Then, for even more accu-racy in simulating drawing speed, we inserted addi-tional waits before the generation 
of each individual character and before each line of a complex polygon fill. This was possible because 
the driver already con-tained explicit code for these functions in order to sup- port proportional fonts 
and fill patterns. The other alternative would have been, to modify the actual Iko-has microcode routines 
to parameterize them with a slowdown feature. While this approach does involve a performance penalty 
from generating extra instructions (which also fill up the output buffer sooner) and the longer drawing 
time (especially for complex fills), each of these new features was designed, implemented, and refined 
within a matter of days. As we become more certain of their importance, we can then consider migrating 
them into microcode. 7. Conclusion Our continuing success in satisfying new demands to simulate possible 
teletext features is due to the power and flexibility of our hardware and software environment, and also 
to our decision to develop a robust set of integrated and modifiable graphics tools. The Radix editor 
uses a less efficient but much more accessible text format for its picture files, which allows us to 
routinely manipulate Radix commands from the    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801149</article_id>
		<sort_key>183</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>24</seq_no>
		<title><![CDATA[Minimal GKS]]></title>
		<page_from>183</page_from>
		<page_to>189</page_to>
		<doi_number>10.1145/800059.801149</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801149</url>
		<abstract>
			<par><![CDATA[<p>Minimal GKS is a subset of the Draft International Standard Graphical Kernel System. Minimal GKS has been implemented at Sandia in the programming language C. This implementation confirms that Minimal GKS does indeed have the anticipated advantages of easy implementation and small size. Experience in using this implementation has also demonstrated that Minimal GKS is easy to learn and use, yet is powerful enough for non-trivial, real-world applications.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[ANSI X3H3]]></kw>
			<kw><![CDATA[Graphical kernel system]]></kw>
			<kw><![CDATA[Virtual device interface]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Graphics packages</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Virtual device interfaces</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Standards</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011017</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Domain specific languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010919.10010177</concept_id>
				<concept_desc>CCS->Computing methodologies->Distributed computing methodologies->Distributed programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Standardization</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P239100</person_id>
				<author_profile_id><![CDATA[81332527816]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Randall]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Simons]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sandia National Laboratories, Albuquerque, New Mexico]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ANSI. "Proposal for an ANSI X3 Standards Project for the Programmer's Minimal Interface to Graphics", ANSI Doc. X3H3/81-87 (December 1981).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ANSI. "X3H35 Issues", ANSI Doc. X3H3/83-11 (February 1983).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Erickson, K. M. and Simons, R. W. "Functional Specification of the Sandia Virtual Device Interface (SVDI)", SAND81-1900, Sandia National Laboratories, Albuquerque, NM (February 1982).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358659</ref_obj_id>
				<ref_obj_pid>358656</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Isner, J. F. "A Fortran Programming Methodology Based on Data Abstraction", Comm. Assoc. Comput. Mach. Vol. 25(10), pp. 686-697 (October 1982).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[ISO. "Graphical Kernel System (GKS) - Functional Description", Draft International Standard ISO/DIS 7942 (November 1982).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Rosenthal, D. S. H., and ten Hagen, P. J. W. "GKS in C", Proceedings of EUROGRAPHICS '82 pp. 359-369 (September 1982).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 17, Number 3 July 1983 Minimal GKS Randall W. Simons Sandia National Laboratories 
Albuquerque, New Mexico 87185 Abstract: Minimal GKS is a subset of the Draft International Standard 
Graphical Kernel System. Minimal GKS has been implemented at Sandia in the programming language C. This 
implementation confirms that Minimal GKS does indeed have the anticipated advantages of easy implementation 
and small size. Experience in using this implementation has also demon- strated that Minimal GKS is easy 
to learn and use, yet is powerful enough for non-trivial, real-world applications. CR Categories and 
Subject Descriptors: 1.3.4 [Computer Graphics]: Graphics Utilities - Graphics packages; 1.3.6 [Com- puter 
Graphics]: Methodology and Techniques - Device Inde- pendence General Terms: Design, Standardization 
Additional Keywords and Phrases: Graphical Kernel Sys- tem, Virtual Device Interface, ANSI X3H3 1. What 
Is Minimal GKS? Early versions of GKS were developed in West Germany by DIN, the German National Standards 
body. The International Standards Organization working group on Computer Graphics (ISO/TC97/SC5/WG2) 
to which DIN, ANSI, and other nation- al standards bodies send representatives, modified GKS to its present 
version [5], and made it a Draft International Standard. The Computer Graphics committee of the American 
National Standards Institute (ANSI X3H3) is considering making GKS an American National Standard. These 
two versions of GKS will hereafter be referred to as ISO GKS and ANSI GKS, respectively. Among other 
changes, ANSI X3H3 is considering adding another output level to GKS before adopting it as an American 
National Standard. ISO GKS includes a level structure which defines three input levels and three output 
levels, such that one can choose any input level and any output level and combine the functions in each 
to define a valid level of GKS. The output level which ANSI X3H3 may add is a subset of what is currently 
the smallest level in ISO GKS. This new lowest output level, when combined with input level "a" (which 
contains no functions), is what I will call Minimal GKS. The determination of which functions wouldbe 
included in Minimal GKS is the result of the work of an ANSI X3H3 subcommittee called X3H35: Programmer's 
Minimal Interface to Graphics, or PMIG, plus inputs from the entire ANSI X3H3 committee. The goals for 
the set of functions developed by the PMIG subcommittee included easy imple- mentation, learning, and 
use, implementation by small and efficient code, and sufficient power to support a wide range of 2-D 
passive graphics applications [1]. The functions selected are listed in appendix 1. Issues, alternatives, 
and arguments discussed by the PMIG subcommittee when selecting these functions and reviewed by the full 
ANSI X3H3 committee are recorded in [2]. 2. The C Language Binding GKS is defined independent of any 
particular programming language, so it is necessary to bind the abstract functions and data types of 
GKS to actual functions and data types in the language to be used for implementation. This was done for 
the programming language C by Rosenthal and ten Hagen [6]. C presents some problems for the language 
binder, primari- ly because there is no standard for C that defines precisely what is in and what is 
out of the language. Sandia's C compiler apparently differs from the one used by Rosenthal and ten Hagen 
in the following ways. It does not support the enumerat- ed data type, which Rosenthal and ten Hagen 
used to bind some of the GKS abstract data types. It requires function names to be unique in the first 
six characters, but two of the names chosen by Rosenthal and ten Hagen (close__gks and close_ws) were 
not. Both of these problems were easy to fix. One can use integer data type instead of enumeration, and 
define constants with integer values so the GKS programmer can use the same nice names he would have 
had with an enumerated data type. This fix is even portable between the two systems, if you don't mind 
ignoring warnings about mismatched data types. A solution to the other problem is to change the names 
(clos_gks, clos_ws), but this unfortunately makes the programs not portable be- tween the two implementations. 
There is another problem with C in that it relies heavily on "standard" libraries and UNIX* system utilities 
for such func- tions as I/O, character string handling, math functions, and dynamic memory management. 
It is not always clear which of these capabilities one should expect to find in every C imple- mentation, 
since they are not part of the compiler, but part of the compiler's operating environment. For example, 
the operat- ing environment for Sandia's C compiler does not include dynamic memory management, yet this 
was a feature used by Rosenthal and ten Hagen. This can affect the C binding because one of the parameters 
to open gks is the size of a memory block given to GKS for use by its memory manager. This parameter, 
and more importantly, the memory it represents, are meaning- less in a context without dynamic memory 
management. It was *UNIX is a Trademark of Bell Laboratories. decided for portability's sake to leave 
the parameter in and just ignore it. The absence of dynamic memory management is no problem for Minimal 
GKS, as we shall see later. A final problem with the C binding did not surface until using Minimal GKS 
with a large application program. By defining data types for every parameter to every function in Minimal 
GKS, the C binding requires a calling program to use large amounts of symbol table space to store these 
defined data types. If the calling application program also requires large amounts of symbol table space 
for its own purposes, this can lead to symbol table overflow at compilation time. One solu- tion is to 
not define all these data types in the calling program, but simply use the basic data types of C. In 
this case the lint pro- cessor will complain about mismatched data types, but the program will execute 
properly. This problem is of course entire- ly dependent on the symbol table space available to the compil- 
er and the application program's use of that space, so in some environments the problem may never arise. 
3. The Sandia Implementation Sandia National Laboratories needed a graphics package to support real-time 
display of data. Since a Sperry Univac V77 minicomputer had already been designated as the host, memo- 
ry space and speed of execution were serious considerations. It was anticipated that the graphics terminals 
used for display would be replaced in the near future, to take advantage of emerging technology, so device 
independence was required. A careful analysis of the data display requirements showed that Minimal GKS 
had all the functions required for this applica- tion. virtual device interface (see below) to each active 
workstation are no longer needed. Arrays or linked lists of open and active workstations become single 
variables. Device dependent code doesn't have to be made reentrant, and no mechanism is needed to switch 
this code from one physical device to another except at the time a workstation is opened. Despite all 
these advantages of a single workstation imple- mentation, the requirements of our application were the 
over- riding factor. In this application, the same picture must often be presented simultaneously to 
several operators at different workstations. Therefore, the Sandia Minimal GKS implemen- tation includes 
multiple simultaneous workstation capabilities. This is much more efficient than sequentially opening 
different workstations and redrawing the picture multiple times. GKS requires a minimum of two normalization 
transfor- mations, one of which is the unity transformation and can't be changed. By supporting only 
this minimum, only a single settable transformation needs to be saved. Explaining to the programmer becomes 
easier, too. He has the simple choice of using NDC coordinates (transformation 0), or setting up his 
own arbitrary world coordinates (transformation 1). The Sandia Minimal GKS implementation allows the 
num- ber of normalization transformations to be selected at compila- tion time. Currently, we have no 
need for more than transfor- mations 0 and 1 mentioned above, but can easily add more later if that requirement 
should arise. 6. The Virtual Device Interface (VDI) 6.1 Functional Level of the VDI A Digital Equipment 
Corporation PDP 11/44 running UNIX was used for developing this Minimal GKS implementa- tion. Once development 
on UNIX was complete, the software was ported to the target Univac V77. 4. Size of Minimal GKS vs. Other 
Levels The ISO GKS document is 285 pages long. We hoped that Minimal GKS would allow us to ignore the 
majority of those pages, and we were not disappointed. The entire sections on segment functions, input 
functions, metafile functions, and utility functions are not needed by Minimal GKS. Full GKS, i.e., the 
highest input and output levels com- bined, includes 110 functions plus 75 inquiry functions. Even the 
lowest level of ISO GKS requires 52 functions plus 38 inquiry functions. Minimal GKS uses only 31 of 
these functions plus 17 inquiry functions (listed in appendix 1). The data structures required by Minimal 
GKS are also significantly smaller than those defined for full GKS in the ISO GKS document. The segment 
state lists and input queue disap- pear completely. What Minimal GKS requires for a GKS state list, workstation 
state list, and workstation description table are given in appendix 2. The length of these lists is about 
23%, 15%, and 14% respectively of the length of the corresponding lists in the ISO GKS document. The 
list of errors that can be generated by Minimal GKS (appendix 3) is about 35% as long as that of full 
GKS. 5. Other Decisions Affecting Size The number of simultaneously open workstations and the number 
of normalization transformations are constants that have to be chosen for each GKS implementation, regardless 
of level. Choosing the minimum in either case can result in a smaller, simpler implementation. Supporting 
only one open workstation at a time can affect an implementation in many ways. The loops that check wheth- 
er a specified workstation is open or active degenerate to a single if test. Likewise, the loops that 
send commands across the Placing the virtual device interface is one of the most important decisions 
for any device independent graphics pack- age. The VDI is the interface between device independent code 
and device dependent code. If the interface is placed too high, i.e., includes much powerful functionality, 
then device drivers for simple devices will be large and difficult to write (for example, see Figure 
l). On the other hand, if the interface is placed too low, device drivers will not be able to take advantage 
of all the capabilities of more intelligent devices (for example, see Figure 2). This tradeoff must be 
resolved in the context of how the graphics package will be used and for what devices [3]. The following 
guidelines were considered desirable in San- dia's environment. First, all device dependencies must be 
below the VDI, i.e., code above the VDI should operate identi- cally for all devices. This rules out 
GKS device independent code inquiring device capabilities and determining what to do from the answers 
it gets. Second, everything below the VDI should be capable of migrating naturally into a (hypothetical) 
Device Driver Device] Figure 1. High VDI, Simple Device GKS Polyline Emulate LT Emulate LW vDi Device 
Driver Device   I Po;_,,----I I Ll.t,.. I I uo.-,,t. I Figure 2. Low VDI, Intelligent Device intelligent 
device. This means, for example, that you don't want to pass a workstation identifier across the VDI, 
because if there is a physical device immediately below that VDI, it certainly doesn't need to be told 
which workstation it is. Making the workstation identifier an external variable is preferable, so it 
will be available for code shared by different devices, but can be ignored by any physical device that 
takes VDI commands directly. The structure of GKS suggests a natural place for the VDI, namely the workstation 
interface. Adopting this strategy, each device driver is responsible for maintaining its own worksta- 
tion state table and performing all workstation dependent functions. The device independent portion of 
GKS above the VDI which is shared by all devices, performs all workstation independent functions, and 
passes on all workstation depen- dent functions through the VDI to the device drivers. These functions 
and their parameters are listed in appendix 4, using the C language type definitions from [6]. This approach 
has several advantages. It is not necessary to introduce a new concept into GKS to describe the VDI, 
since the VDI can simply be thought of as the interface to the workstations. By making each workstation 
responsible for its own workstation state table, we can treat each workstation as a data abstraction 
which hides the internal details of its data structures, and is accessible only through a well-defined 
set of operations -- the VDI functions. The use of such data abstrac- tions is a valuable technique for 
managing complex software while improving its reliability and maintainability [4]. This decision also 
impacts the method used for selecting the devices to which you output. Each workstation description table 
includes a list of pointers to VDI functions to be called to VDI access the device corresponding to that 
workstation. Thus invoking a function in a particular device is done by referencing the workstation description 
table. This isolates the knowledge of device dependent function names to the workstation descrip- tion 
table. Unlike some other schemes where the device inde- pendent code needs to know the appropriate function 
names for each of the devices it could output to, this method requires no changes to the device independent 
code to add a new device driver. 6.2 Subroutine Interface vs. Data Interface There are two popular methods 
of implementing a VDI, as a subroutine interface or a data interface. In a subroutine interface, a separate 
subroutine is provided for each VDI function. These functions are invoked by calling the subroutine and 
passing the data required by the function as the subroutine parameters (Figure 3). A data interface, 
on the other hand, has only one subroutine as the entry point for VDI functions. Functions are invoked 
by encoding the function and its re- quired data in a standard format and passing the whole package to 
the single subroutine which then decodes the package and performs the specified function (Figure 4). 
The encoding and decoding overhead of the data interface are clearly worthwhile in certain cases. If 
the functions and data have to be encoded anyway to be stored in a segment, stored in a metafile, or 
sent to an intelligent device, then the same encoding might be used by the VDI. But Minimal GKS does 
not support segments or metafiles, and it's unlikely that all devices supported by an implementation 
would have sufficient intelligence to emulate the workstation interface. A subroutine interface implementation 
of the VDI seems preferable for use by Minimal GKS. The ability to support multiple simultaneous workstations 
is sometimes cited as a benefit of a data interface. But a subroutine interface can also provide this 
capability, and do so elegantly if function pointers are available as in C. By using the function pointers 
in the workstation description table as dis- cussed above, there is no problem routing control to the 
proper subroutines for each workstation. 7. Implementation Differences Between Minimal GKS and Full GKS 
There are a number of implementation issues that may best be resolved differently for Minimal GKS and 
full GKS. 7.1 Dynamic Storage Allocation In full GKS there are certain requirements for storage that 
can theoretically grow without bounds. Every time you open a new workstation, a new workstation state 
list is needed. Also, every new segment defned requires a segment state list and storage for the segment. 
If the environment in which GKS is running supports dynamic storage allocation, it works well to Encode 
Encode  I __J I I IEncodel Decode Device Driver Device Driver Figure 3. Subroutine Interface Figure 
4. Data Interface allocate and free these required storage areas as they are needed. But if no dynamic 
storage allocation is supported, all one can do is reserve a big block of memory at load time and write 
your own allocation utilities. This doesn't work so well because it ties up a lot of memory which in 
some cases won't be needed, and in other cases won't be enough. These problems disappear when implementing 
Minimal GKS supporting only one workstation at a time. Only one workstation state list is needed, and 
since Minimal GKS doesn't support segments, no storage is required for them either. 7.2 Attribute Setting 
 ISO GKS states that geometric primitive attributes are expressed and stored (in the GKS state list) 
in world coordi- nates, and are subject to the same transformations as the primitives to which they are 
bound. A straightforward way of implementing this is to transform an attribute value each time it is 
actually used, i.e., when an output primitive affected by that attribute is invoked. However, many devices 
are capable of storing attribute values in device coordinates, so one might prefer to transform an attribute 
only once when it is first set, and let the device remember what its value is each time an output primitive 
uses it. In implementing this second solution, one must remember to transform attributes again whenever 
the normalization transformation or a workstation transformation changes, or a new workstation is activated. 
A variant of this second approach is to set a flag in each attribute, transforma- tion, and activate 
workstation function, and check this flag in each output primitive function to determine if attributes 
need to be transformed again. A number of interrelated factors affect this decision, includ- ing single 
vs. multiple simultaneous workstations, single vs. multiple settable normalization transformations, and 
device capabilities. But the most important consideration is applica- tion-dependent and requires that 
one know the relative fre- quencies of invocation of attribute, transformation, activate workstation, 
and output primitive functions. For the first solu- tion, it is the output primitive functions that require 
the costly transformation, while in the second solution it is the attribute, transformation, and activate 
workstation functions that incur the cost. Because Minimal GKS supports fewer attributes than full GKS, 
one might expect fewer attribute function invocations, so the second solution is relatively more attractive 
for Minimal GKS than full GKS.  7.3 Emulation of Unsupported Functions In any device independent graphics 
package with a reason- ably high-level VDI, it will be necessary to emulate certain functions for devices 
that don't support those functions direct- ly. When emulation is required, at least two approaches are 
possible. The emulation can be done below the VDI by each device driver (see Figure 1). Alternatively, 
the device indepen- dent code can inquire whether the VDI supports a given function and if not, perform 
the emulation itself, above the VDI. In the first case, emulation code will be loaded for every workstation 
that needs it (unless it can be implemented as shared reentrant code), but not for workstations that 
don't need it. With the second solution, the emulation will be loaded only once, but it is loaded even 
when it is not needed. When you consider that one possible emulation might be software charac- ter generation, 
you realize the size of this code may be signifi- cant. In a Minimal GKS implementation where only one 
work- station is allowed at a time, there is no need for sharing emulation code. Since one of the expected 
advantages of Mini- mal GKS is small load size, we should choose emulation below the VDI so that devices 
that don't need the emulation aren't forced to load it anyway.  7.4 Clipping and Transformations GKS 
defines two clipping boundaries: the viewport of the normalization transformation, and the workstation 
window. Instead of actually dipping twice, it will usually be more efficient to compute the intersection 
of the areas enclosed by these two boundaries and clip only once. This has to be done below the VDI, 
since the second clipping boundary is worksta- tion dependent. This behavior is practically required 
by ISO GKS, since clipping must be delayed until after primitives are extracted from segments and are 
to be displayed. One might think efficiency could be gained in a similar way by combining the normalization 
transformation and the work- station transformation. But full GKS supports segments and segments require 
storage of primitives in NDCs. A full GKS implementation needs to keep the two transformations separate 
so it can still get at the NDC values for storage in segments. Not so with Minimal GKS, since there are 
no segments. This optimization is worth considering for Minimal GKS if speed is important, but it does 
effectively redefine the placement of the VDI. Coordinates passed to the VDI have been raised from the 
NDC level to the world coordinate level. 8. Performance Results The Sandia implementation of Minimal 
GKS, including a Tektronix 4014" VDI driver, took less than three man-months of effort to design, code, 
test, and debug on the UNIX system. This effort borrowed from several available resources [3, 5, 6]. 
All of these are available to the public, and any would-be implementor starting the same task tomorrow 
could probably use them and this paper to generate the same product in the same amount of time. Conversion 
from the UNIX system to the Univac V77 took about one man-month of debugging and learning how to rewrite 
code so it was portable between the two systems. The finished product was very satisfactory in that exactly 
the same source code is used for both the UNIX and V77 versions. Based on personal past experience with 
other VDI implementations, this portability seems to be a major advantage of C over FOR-TRAN. The size 
of the implementation as measured in lines of code (excluding comments) and bytes of object code (stripped 
of symbol table) are given below for each class of GKS function: Table 1 - Code Size lines bytes GKS 
typedefs 62 control 200 1730 output 106 878 attributes 166 1166 transforms 150 1290 inquiry 21 219 errors 
29 238 VDI typedefs 243 control 115 1534 output 156 2004 attributes 77 786 transforms 60 688 inquiry 
52 458 clipping 118 1158 TOTAL 1555 12148 *Tektronix is a registered trademark of Tektronix, Inc. This 
implementation of Minimal GKS is small enough to fit in the limited memory available on the V77 minicomputer 
while leaving adequate room for the application programs which use Minimal GKS. We have not yet checked 
the speed of this implementation which must eventually run in a multipro- grammed environment and display 
data as it is received in real time. However, the small size and simplicity of the code encourage us 
to believe it will be fast. Measuring the usefulness of the functionality of any pack- age, including 
Minimal GKS, is not easy to do. The fact that Minimal GKS proved completely adequate for the data display 
requirements of this project proves that the functionality, small as it is, provides enough power for 
some real-world applica- tions. We have built a simple plot package on top of Minimal GKS which provides 
single-call plot commands, linear and logarithmic axis commands, control over tick marks, tick mark labels, 
and axis labels, and plots data with or without markers and connecting lines. No omissions or errors 
in the functiona- lity of Minimal GKS were uncovered while building this plot package. So far, four people 
without any previous experience with GKS have learned to use Minimal GKS, each coding and debugging their 
first display in about four to eight hours, depending on display complexity. The biggest problem encoun- 
tered in learning Minimal GKS was understanding when differ- ent coordinate systems applied. The fact 
that character height was specified in world coordinates and thus changed when the normalization transformation 
changed seemed counterintui- tive in this plotting application. The use of Minimal GKS has not influenced 
the nature of the displays produced, since the display formats were all de- fined long before Minimal 
GKS was chosen for this project. Since the new displays using Minimal GKS are replacing displays coded 
directly in Tektronix 4014 control codes, the biggest immediate effects are ease of programming and device 
independence. Expected long term effects include the easy conversion to more diverse, powerful, and modern 
graphics devices. While Minimal GKS contains all the functions cur- rently needed, one of its advantages 
is the possibility of future expansion to higher levels to take advantage of more powerful devices. Acknowledgments 
The concept of a minimal graphics package and its func- tional design came out of the ANSI X3H35 subcommittee 
on the Programmer's Minimal Interface to Graphics. I would especially like to acknowledge the contributions 
of my fellow members of longest standing in that group, Tom Wright, David Straayer, Elaine Sonderegger, 
and Fred Canfield. I would also like to thank Tom Ciarkson, Ted Reed, Hal Short, and Peter Watterberg 
for insightful discussions of Virtual Device Inter- faces and implementation issues. Finally, my thanks 
to David Rosenthal and Paul ten Hagen for sharing with me their work on the C binding and a C implementation 
of full GKS. References 1. ANSI. "Proposal for an ANSI X3 Standards Project for the Programmer's Minimal 
Interface to Graphics", ANSI Doc. X3H3/81-87 (December 1981). 2. ANSI. "X3H35 Issues", ANSI Doc. X3H3/83-11 
(February 1983). 3. Erickson, K. M. and Simons, R. W. "Functional Specifica- tion of the Sandia Virtual 
Device Interface (SVDI)", SAND81-1900, Sandia National Laboratories, Albuquer- que, NM (February 1982). 
 4. Isner, J. F. "A Fortran Programming Methodology Based on Data Abstraction", Comm. Assoc. Comput. 
Mach. Vol. 25(10), pp.686-697 (October 1982).  5. ISO. "Graphical Kernel System (GKS) -Functional De- 
scription", Draft International Standard ISO/DIS 7942 (November 1982). 6. Rosenthal, D. S. H., and ten 
Hagen, P. J. W. "GKS in C', Proceedings of EUROGRAPHICS "82 pp.359-369 (Septem- ber 1982).  Appendix 
1 - Minimal GKS Functions ACTIVATE WORKSTATION CLEAR WORKSTATION CLOSE GKS CLOSE WORKSTATION DEACTIVATE 
WORKSTATION ERROR HANDLING ESCAPE FILL AREA INQUIRE CLIPPING INDICATOR INQUIRE COLOUR FACILITIES INQUIRE 
COLOUR REPRESENTATION INQUIRE CURRENT INDIVIDUAL ATTRIBUTE VALUES INQUIRE CURRENT NORMALIZATION TRANSFORMATION 
NUMBER INQUIRE CURRENT PRIMITIVE ATTRIBUTES VALUES INQUIRE FILL AREA FACILITIES INQUIRE LEVEL OF GKS 
INQUIRE LIST OF COLOUR INDICES INQUIRE MAXIMUM DISPLAY SURFACE SIZE INQUIRE NORMALIZATION TRANSFORMATION 
INQUIRE POLYLINE FACILITIES INQUIRE POLYMARKER FACILITIES INQUIRE TEXT EXTENT INQUIRE TEXT FACILITIES 
INQUIRE WORKSTATION CONNECTION AND TYPE INQUIRE WORKSTATION TRANSFORMATION OPEN GKS OPEN WORKSTATION 
POLYLINE POLYMARKER SELECT NORMALIZATION TRANSFORMATION SET CHARACTER HEIGHT SET CHARACTER UP VECTOR 
SET CLIPPING INDICATOR SET COLOUR REPRESENTATION SET FILL AREA COLOUR INDEX SET FILL AREA INTERIOR STYLE 
SET LINETYPE SET MARKER TYPE SET POLYLINE COLOUR INDEX SET POLYMARKER COLOUR INDEX SET TEXT ALIGNMENT 
SET TEXT COLOUR INDEX SET VIEWPORT SET WINDOW SET WORKSTATION VIEWPORT SET WORKSTATION WINDOW TEXT UPDATE 
WORKSTATION Appendix 2 --Minimal GKS State Lists and Description Tables GKS State List set of open workstations 
set of active workstations current normalization transformation number list of transformation numbers 
ordered by viewport input priority (0 highest initially) list of windows (in above order) list of viewports 
(in above order) clipping indicator current linetype current polyline colour index current marker type 
current polymarker colour index current text colour index current character height current character 
up vector current text alignment (horizontal and vertical) current fill area interior style current fill 
area colour index  Workstation State List workstation identifier connection identifier workstation 
type number of colour table entries table of colour representations containing: colour index colour (red/green/blue 
intensities) implicit regeneration mode workstation transformation update state display surface empty 
requested workstation window current workstation window requested workstation viewport current workstation 
viewport new frame action necessary at update Workstation Description Table workstation type device 
coordinate units maximum display surface (visible area of the display surface) in length units in device 
units raster or vector display (VECTOR = vector display, RASTER = raster device, OTHER = other device, 
eg vector+raster) number of available linetypes list of available linetypes number of available character 
heights (if the workstation supports continuous character heights, the number will be zero) list of available 
character heights (if the workstation supports continuous character heights, the minimum and maximum 
will be listed) number of eolours (if workstation supports a continuous range of colours, the number 
will be zero) colour available maximum number of colour indices number of predefined colour indices (representations) 
table of predefined colour representations, for every entry colour (red/green/blue intensities) default 
value for: implicit regeneration mode colour representation workstation transformation Appendix 3 -Minimal 
GKS Error Messages States 1 GKS not in proper state: GKS must be in the state GKCL 2 GKS not in proper 
state: GKS must be in the state GKOP 3 GKS not in proper state: GKS must be in the state WSAC 5 GKS not 
in proper state: GKS must be either in the state WSAC or in the state SGOP 6 GKS not in proper state: 
GKS must be either in the state WSOP or in the state WSAC 7 GKS not in proper state: GKS must be in one 
of the states WSOP, WSAC, or SGOP 8 GKS not in proper state: GKS must be in one of the states GKOP, WSOP, 
WSAC or SGOP Workstations 20 Specified workstation identifier is invalid 21 Specified connection identifier 
is invalid 22 Specified workstation type is invalid 23 Specified workstation type does not exist 24 Specified 
workstation is open 25 Specified workstation is not open 26 Specified workstation cannot be opened 29 
Specified workstation is active 30 Specified workstation is not active Transformations 50 Transformation 
number is invalid 51 Rectangle definition is invalid 52 Viewport is not within the Normalized Device 
Coordinate unit square 53 Workstation window is not within the Normalized Device Coordinate unit square 
54 Workstation viewport is not within the display space Output Attributes 62 Linetype is less than or 
equal to zero 63 Specified linetype is not supported on this workstation 66 Marker type is less than 
or equal to zero 67 Specified marker type is not supported on this workstation 73 Character height is 
less than or equal to zero 74 Length of character up vector is zero 77 Specified fill area interior style 
is not supported on this workstation 85 Colour index is less than zero 86 Colour index is invalid 87 
A representation for the specified colour index has not been defined on this workstation 88 Colour is 
outside range [0, 1] Output Primitives 100 Number of points is invalid 101 Invalid code in string Escape 
180 Specified function is not supported 181 Contents of escape data record are invalid  Implementation 
Dependent 300 Specified function is not supported in this level of GKS 301 Storage overflow has occurred 
in GKS 309 Arithmetic error has occurred Appendix 4 --VDI Functions Wss * /* Returns pointer to workstation 
state list.*/ dtlope(dev, wsd) /* OPEN WORKSTATION */ File *dev; /* File for workstation (must be open).*/ 
Wsd *wsd; /* Workstation description table.*/ dtlclo0 /* CLOSE WORKSTATION */ dtlcle(cf) /* CLEAR WORKSTATION 
*/ Clearf cf; /* Control flag.*/ dtlupd(rf) Regenf rf; dtlesc(ef, ed) Es func ef; Es data *ed; dtlpln(n, 
p) Size n; Nc *p; dtlpmk(n, p) Size n; Nc *p; dtltxt(p, s) Nc *p; String s; dtlfar(n,p) Size n; Nc 
*p; dtlltp(lt) Ltype lt; dtllco(ci) Cindex ci; dtlmtp(mt) Mtype mt; dtlmco(ci) Cindex ci; dtltco(ci) 
Cindex ci; /* UPDATE WORKSTATION */ /* Regeneration flag.*/ /* ESCAPE */ /* Function.*/ /* Data.*/ 
 /* POLYLINE */ /* Number of points.*/ /* Coordinates of points.*/ /* POLYMARKER */ /* Number of points.*/ 
/* Coordinates of points.*/ /* TEXT */ /* Text position.*/ /* Character string.*/ /* FILL AREA */ /* 
Number of points.*/ /* Coordinates of points.*/ /* SET POLYLINE LINE TYPE */ /* Linetype.*/ /* SET 
POLYLINE COLOUR INDEX */ /* Colour index.*/ /* SET POLYMARKER MARKER TYPE */ /* Marker type.*/ /* SET 
POLYMARKER COLOUR INDEX */ /* Colour index.*/ /* SET TEXT COLOUR INDEX */ /* Colour index.*/ dtlcht(ht) 
Charhtn ht; dtlcal(ha, va) Horal ha; Veral va; dtlfis(is) Istyle is; dtlfco(ci) Cindex ci; dtlcol(ci, 
col) Cindex ci; Colour *col; dtlclp(cl) Nrect *cl; dtlcli(in) Bool in; dtlwin(nr) Nrect *nr; dtlvie(dr) 
Drect *dr; dtlite(p, s, cat, tr) Nc *p; String s; Nc *cat; Nrect *tr; /* SET CHARACTER HEIGHT */ /* Character 
height.*/ /* SET CHARACTER ALIGNMENT */ r. Horizontal alignment.*/ r. Vertical alignment.*/ l. SET 
FILL AREA INTERIOR STYLE */ l. Interior style.*/  l* SET FILL AREA COLOUR INDEX */ /* Colour index.*/ 
 f* SET COLOUR REPRESENTATION */ l* Colour index.*/ l* Colour.*/ /* SET CLIPPING RECTANGLE (from normalization 
xform).*/ /* Clipping rectangle.*/ /* SET CLIPPING INDICATOR */ /* Clipping indicator.*/ /* SET WORKSTATION 
WINDOW */ /* Workstation window limits.*/ /* SET WORKSTATION VIEWPORT */ /* Workstation viewport limits.*/ 
 /* INQUIRE TEXT EXTENT */ /* Text position.*/ /* Character string.*/ /* Concatenation position.*/ /* 
Text rectangle.*/  189 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801150</article_id>
		<sort_key>191</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>25</seq_no>
		<title><![CDATA[Japanese computer graphics (Panel Session)]]></title>
		<subtitle><![CDATA[Challenges and opportunities]]></subtitle>
		<page_from>191</page_from>
		<page_to>192</page_to>
		<doi_number>10.1145/800059.801150</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801150</url>
		<categories>
			<primary_category>
				<cat_node>I.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31081364</person_id>
				<author_profile_id><![CDATA[81100241671]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Laurin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Herr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pacific Interface]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332263</person_id>
				<author_profile_id><![CDATA[81100236512]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Madoka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Katou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dentsu Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35043552</person_id>
				<author_profile_id><![CDATA[81540664356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Akira]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Amano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Freelance writer]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39050442</person_id>
				<author_profile_id><![CDATA[81100605028]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yoichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawaguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nippon Electronics College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P331870</person_id>
				<author_profile_id><![CDATA[81543513956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kouichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Omura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PANEL JAPANESE COMPUTER GRAPHICS: CHALLENGES AND OPPORTUNITIES PART I: COMPUTER GRAPHICS IN THE WORLD 
OF JAPANESE COMMERCIAL PRODUCTION FOR ART, ENTERTAINMENT AND ADVERTISING CHAIR: Laurin Herr, Pacific 
Interface PANELISTS: Akira Amano, freelance writer Madoka Katou, Dentsu Inc. Yoichiro Kawaguchi, Nippon 
Electronics College Kouichi Omura, Osaka University CHAIRMAN'S INTRODUCTION In a country as media-saturated 
as Japan, where the preservation of traditional social and aesthetic forms has been bal- anced over 
the past 120 years by a relent- less quest for new ideas, fashions and techniques, computer graphics 
has found favor because of its novelty, expressive power and futuristic look. It fits nicely with the 
high-tech products and corporate images of many of Japan's largest advertisers. It is consistent with 
the national movement to enthusiastically embrace the "information age". And, it seems to offer significant 
productivity gains in several areas of commercial pro- duction. Finally, computer graphics has been the 
subject of extraordinary scrutiny simply because it is widely perceived in Japan to be the image-making 
system of the future. The past eighteen months have witnessed the beginning of a computer graphics boom 
within the world of Japanese commercial production and media. There have been many reports in books, 
maga- zines, newspapers and on television. Com- puter graphics conferences and events have been organized. 
The large advertising agencies have begun to show wide- ranging interest in the field. And several new 
 computer graphics production companies have established facilities in Tokyo. This panel will introduce 
developments in the Japanese computer graphics world that could have impact on the international commercial 
production industry. The panelists were selected to represent a wide range of viewpoints: agency pro- 
 ducer, industry observer, artist/educator and engineer. The audience will be encouraged to ask questions. 
 Laurin Herr graduated from Cornell Univer- sity in 1972 with a B.A. degree in Govern- ment. He is currently 
the Director of Pacific Interface, a private New York- based firm founded in 1980 which special- izes 
in computer graphics and Japanese/American interaction. His first trip to Japan was in 1973 and he has 
returned many times, both to continue his studies of the language and culture and to work as an interpreter/coordinator 
on a variety of international projects. Mr. Herr became involved in computer graphics in 1979 when he 
was international staff liaison for the production of the "Mind of the Universe Electro-Opera". He has 
authored a comprehensive report on "Sig- graph'81 and trends in the Computer Graph- ics Industry" for 
a Japanese client, co- authored a six-part series of articles about computer graphics published in the 
Japanese magazine ASCII and produced the American segments of several lengthy Japanese television reports 
about the field. Most recently, he has served as the North American Coordinator for the Micrograph'82 
Conference and as the Pro- gram Coordinator for the Intergraphics'83 Conference, both held in Tokyo. 
He currently resides in Tokyo where he serves as Siggraph's volunteer representative in Japan. Madoka 
Katou will speak about common prob- lems that often arise in Japanese/ Ameri- can computer graphics 
productions, the possible uses of computer graphics at the 1985 Science and Technology Exposition planned 
for Tsukuba, and trends in Japanese commercial production. He will also show a short selection of Japanese 
commercial spots utilizing computer graph- ics and computer graphics look-alike tech- niques. Mr. Katou 
graduated from the Art Department of Nippon University in 1967 and entered the Dentsu advertising agency 
as a designer the same year. He became a 191 commercial film ~lanner/coordinator in 1970 and has worked 
on commercial produc- tion projects for many of the largest advertisers in Japan, including Hitachi, 
Matsushita Electric, Toyota, Coca Cola, Fujitsu and House Foods. His current interests are in the production 
of visuals for pavillions at the Tsukuba Expo'85 and other large-scale events. Akira Amano will report 
on computer graph- ics production facilities in Japan, emphasizing current developments and future trends 
in the areas of hardware utilization, visual effect/software tools creation, and human resources. Mr. 
Amano graduated from the Art Department of Nip- pon University in 1966 with a major in Broadcasting. 
After working in television and publishing, he became a freelance writer in 1980 and currently commutes 
between Tokyo and New York. He frequently contributes articles about communications and high-technology 
to Japanese magazines and has served as the Editor-in-Chief of Business Video Magazine. In 1982, he wrote 
and edited The Leadin@ Edge of Com- puter Graphics: CG'82 a large format four-color book. Yoichiro Kawaguchi 
will explore the role of the artist in computer graphics and the education of computer graphics artists 
in Japan. He will also show new works util- izing his "Growth Algorithm" first intro- duced at Siggraph'82. 
Mr. Kawaguchi gra- duated from the Kyushu College of Indus- trial Design in 1976 with a major in Visual 
Design. He received his MFA from the Tokyo Education College (now Tsukuba University) in 1978 and is 
currently an Assistant Professor of Computer Graphics in the Department of Art of the privately-run Nippon 
Electronics College in Tokyo. He is a member of the Japan Graphics Designers Association (JAGDA) and 
has shown his computer graphics art work widely, both in Japan and overseas. Kouichi Omura will report 
on the hardware architecture, software concepts and per- formance of the LINKS-l, a parallel pipe- lined 
multi-microcomputer system for image creation that he developed with a team of researchers at Osaka University. 
He will also discuss his theories of visual pro- duction, outline his involvement with parallel processing 
system for graphics applications and show sample output from the LINKS-I system. Dr. Omura graduated 
from Osaka University in 1960 with a major in Electrical Engineering. He received his doctorate in 1968 
for research in the field of Very High Speed Logic Circuit Elements and became an Associate Professor 
of Electrical Engineering in the Depart- ment of Engineering at Osaka University in 1973. His current 
interests include multi-computer applications and robotics. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801151</article_id>
		<sort_key>193</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>26</seq_no>
		<title><![CDATA[Local control of bias and tension in beta-splines]]></title>
		<page_from>193</page_from>
		<page_to>218</page_to>
		<doi_number>10.1145/800059.801151</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801151</url>
		<abstract>
			<par><![CDATA[<p>The Beta-spline introduced recently by Barsky is a generalization of the uniform cubic B-spline: parametric discontinuities are introduced in such a way as to preserve continuity of the unit tangent and curvature vectors at joints (<italic>geometric continuity</italic>) while providing bias and tension parameters, independent of the position of control vertices, by which the shape of a curve or surface can be manipulated. Using a restricted form of quintic Hermite interpolation, it is possible to allow distinct bias and tension parameters at each joint without destroying geometric continuity. This provides a new means of obtaining local control of bias and tension in piecewise polynomial curves and surfaces.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Beta-splines]]></kw>
			<kw><![CDATA[computer-aided design]]></kw>
			<kw><![CDATA[geometric continuity]]></kw>
			<kw><![CDATA[polynomial splines]]></kw>
			<kw><![CDATA[tension]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>G.1.1</cat_node>
				<descriptor>Spline and piecewise polynomial interpolation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003649.10003657.10003659</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic representations->Nonparametric representations->Spline models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P32440</person_id>
				<author_profile_id><![CDATA[81100387067]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Barsky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P142871</person_id>
				<author_profile_id><![CDATA[81100324790]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Beatty]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>910231</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BARSKY, B.A. The Beta-spline: A local representation based on shape parameters and fundamental geometric measures. Ph.D. dissertation, Dept. of Computer Science, Univ. of Utah, Salt Lake City, Utah, Dec., 1981.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BARSKY, B.A. Exponential and polynomial methods for applying tension to an interpolating spline curve. Computer Vision Graphic Image Processing 1983, to appear.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>894419</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BARSKY, B.A. A study of the parametric uniform B-spline curve and surface representations. Tech. Rep. CSD 83/118, Computer Science Div., Univ. of Calif., Berkeley, Calif., May 1983.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BARSKY, B.A. The Beta-spline: A curve and surface representation for computer graphics and computer aided geometric design. To be published.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BARSKY, B.A. Algorithms for the evaluation and perturbation of Beta-splines. To be published.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[BARSKY, B.A., BARTELS, R.H., AND BEATTY, J.C. An introduction to the use of splines in computer graphics. CS-83-9, Dept. of Computer Science, Univ. of Waterloo, Waterloo, Ontario, Canada, 1983.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>894461</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[BARSKY, B.A., AND BEATTY, J.C. Varying the betas in Beta-splines. CS-82-49, Dept. of Computer Science, Univ. of Waterloo, Waterloo, Ontario, Canada, 1982. Also Tech. Rep. CSD 82/112, Computer Science Division, Univ. of Calif., Berkeley, Calif., Dec. 1982.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[B&#201;ZIER, P.E. Emploi des Machines &#224; Commande Num&#233;rique. Masson et Cie., Paris, 1970. English ed., Numerical Control&#8212;Mathematics and Applications, A. R. Forrest and A. F. Pankhurst, Trans.,Wiley, New York, 1972.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[B&#201;ZIER, P.E. Essai de d&#233;finition num&#233;rique des courbes et des surfaces exp&#233;rimentales. Ph.D. dissertation, Univ. Pierre et Marie Curie, Paris, Feb. 1977.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[BOGEN, R., GOLDEN, J., GENESERETH, M., AND DOOHOVSKOY, A. MACSYMA Reference Manual, version 9., Massachussetts Institute of Technology, Cambridge, Mass., 1977.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[DE BOOR, C. A Practical Guide to Splines, vol. 27, Applied Mathematical Sciences. Springer-Verlag, New York, 1978.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360971</ref_obj_id>
				<ref_obj_pid>360924</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[CLINE, A.K. Scalar- and planar-valued curve fitting using splines under tension. Commun. ACM 17, 4 (Apr. 1974), 218-220.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>889976</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[COONS, S.A. Surfaces for computer-aided design. Design Div., Mechanical Engineering Dept., Massachusetts Institute of Technology, Cambridge, Mass., 1964.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>889976</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[COONS, S.A. Surfaces for computer-aided design of space forms. MAC-TR-41, Project MAC, Massachusetts Institute of Technology, Cambridge, Mass., June 1967.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[FATEMAN, R.J. Addendum to the MACSYMA Reference Manual for the VAX. Univ. of Calif., Berkeley, Calif., 1982.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578513</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[FAUX, I.D., AND PRATT, M.J. Computational Geometry for Design and Manufacture. Wiley, New York, 1979.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[GORDON, W.J., AND RIESENFELD, R.F. B-spline curves and surfaces. In Computer Aided Geometric Design, R. E. Barnhill and R. F. Riesenfeld, Eds. Academic Press, New York, 1974, pp. 95-126.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>908305</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[LANE, J.M. Shape operators for computer aided geometric design. Ph.D. dissertation, Univ. of Utah, Salt Lake City, Utah, June 1977.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[NIELSON, G.M. Some piecewise polynomial alternatives to splines under tension. In Computer Aided Geometric Design, R. E. Barnhill and R. F. Riesenfeld, Eds. Academic Press, New York, 1974, pp. 209-235.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[NIELSON, G.M. Computation of Nu-splines. Dept. of Mathematics, Arizona State Univ., Tempe, Ariz., June 1974.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[PILCHER, D.T. Smooth approximation of parametric curves and surfaces. Ph.D. dissertation, Univ. of Utah, Salt Lake City, Utah, Aug. 1973.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>906872</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[RIESENFELD, R.F. Applications of B-spline approximation to geometric problems of computer-aided design. Ph.D. dissertation, Dept. of Systems and Information Science, Syracuse Univ., New York, May 1973.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[SCHWEIKERT, D.G. An interpolation curve using a spline in tension. J. Math. Phys. 45 (1966), 312-317.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Local Control of Bias and Tension in Beta-splines BRIAN A. BARSKY University of California, Berkeley 
and JOHN C. BEATTY University of Waterloo, Canada The Beta-spline introduced recently by Barsky is a 
generalization of the uniform cubic B-spline: parametric discontinuities are introduced in such a way 
as to preserve continuity of the unit tangent and curvature vectors at joints (geometric continuity) 
while providing bias and tension parameters, independent of the position of control vertices, by which 
the shape of a curve or surface can be manipulated. Using a restricted form of quintic Hermite interpolation, 
it is possible to allow distinct bias and tension parameters at each joint without destroying geometric 
continuity. This provides a new means of obtaining local control of bias and tension in piecewise polynomial 
curves and surfaces. Categories and Subject Descriptors: 1.3.5 [Computer Graphics]: Computational Geometry 
and Object Modeling--Curve and surface representation General Terms: Algorithms Additional Key Words 
and Phrases: Beta-splines, computer-aided design, geometric continuity, polynomial splines, tension 1. 
INTRODUCTION Early work by Coons [13, 14] and B~zier [8, 9] introduced the use of nonlinear parametric 
polynomial representations for the segments and patches which are stitched together to form piecewise 
curves and surfaces, establishing their viabil- ity. More recently, Riesenfeld [17, 22] has advocated 
the use of B-splines to represent such polynomials on the grounds of greater flexibility and efficiency. 
Parametric B-splines have many advantages. Among them is the ability to control the degree of continuity 
at the joints between adjacent curve segments, and at the borders between surface patches, independent 
of the order of the This work was supported by funds from the U.S. National Science Foundation under 
grant ECS-82- 04381, the Defense Advanced Research Projects Agency of the United States under contract 
N00039- 82-C-0235, the Natural Sciences and Engineering Research Council of Canada under grants A3022 
and G0651, and the National Research Council of Canada under contract 05SU.31155-1-3103. Authors' addresses: 
B. A. Barsky, Berkeley Computer Graphics Laboratory, Computer Science Division, University of Calif., 
Berkeley, CA 94720; J. C. Beatty, Computer Graphics Laboratory, Department of Computer Science, University 
of Waterloo, Waterloo, Ontario, Canada N2L 3G1. Permission to copy without fee all or part of this material 
is granted provided that the copies are not made or distributed for direct commercial advantage, the 
ACM copyright notice and the title of the publication and its date appear, and notice is given that copying 
is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. &#38;#169; ACM 0-89791-109-1/83/007/0193 $00.75 Reprinted From acm 
Transactions On Graphics--April 1983--Vol. 2, No. 2 B. A. Barsky and J. C. Beatty segments or the number 
of control vertices. However, in [1] it is shown that the notion of parametric first- or second-degree 
continuity at joints does not always correspond to intuition or to a physically desired effect. For piecewise 
cubic curves and bicubic surfaces these parametric continuity constraints can be replaced by the more 
meaningful requirements of continuous unit tangent and curvature vectors. Doing so introduces certain 
constrained discontinuities in the first and second parametric derivatives. These are expressed in terms 
of bias and tension parameters, called fl 1 and f12 in [1], which give rise to Beta-spline curves and 
surfaces. The application of tension to the cubic interpolatory spline was first analytically modeled 
in [23]. An alternative development was given in [12] and generalized in [21]. A detailed derivation 
of the generalized form based on a variational principle is given in [2]. The result of this approach 
is a spline curve which is no longer piecewise polynomial, but piecewise exponential; that is, each curve 
segment is expressed in terms of exponential functions. From a computational standpoint, however, polynomial 
functions are much more desirable than exponentials. For this reason Nielson developed the Nu- spline, 
a polynomial alternative to the exponential spline under tension [19, 20]. It is derived in detail in 
[2] from the cubic Hermite basis functions, thereby emphasizing its relation to the conventional cubic 
interpolatory spline. Unfortunately, neither of the above-mentioned interpolating spline curve rep- resentations 
provides local control, that is, the capability of modifying one portion of the curve without altering 
the remainder. Local control is inherent in the B-spline formulation [3, 22], suggesting that it is a 
natural representation from which to develop a means of applying tension locally. Lane experimented with 
this idea by adding knots to a nonuniform B-spline curve in the region of desired tension [18]. The Beta-spline 
is a new curve and surface representation having an inherent capability for modeling tension and containing 
the uniform cubic B-spline as a special case. This representation generalizes previous work on the mathematical 
modeling of tension insofar as it is based on the shape parameters fl 1 and fl 2. The basic theory of 
Beta-splines is developed in [1] and [4]. This theory is expressed in terms of shape parameters which 
can be either uniform or varying throughout the curve or surface being defined. For the purposes of computer- 
aided geometric design, the ability to alter shape parameters is particularly useful because it provides 
an additional means of manipulating a curve or surface. In this paper we extend previous work on varying 
shape parameters. We begin in Section 2 with a review of pertinent terminology, notation, and definitions. 
(A more leisurely presentation of this material may be found in [6].) In Section 3 we review the fundamental 
definition of a Beta-spline from [1] and [4]. This leads naturally to a discussion of how to vary the 
fl parameters in a Beta-spline curve, and an analysis of the basic properties of the curves and surfaces 
which result. 2. PRELIMINARIES The curves in which we are interested are piecewise cubic polynomials 
Q(u) = (X(u),Y(u)) of the parameter u E [u0,um]. The values u0 < ul < ... < um of u Reprinted From acm 
Transactions On Graphics--April 1983--Vol, 2, No. 2 V3 V q- .... ..:...:"':'""";'"'-.-:...L. Jr- v4 
\. + ........ ../  "?'.:..-....:-":" + + V~ V2 V 5 Fig. 1. An example of a curve defined by a sequence 
of control vertices, represented here by "+" signs, near which the curve passes. The lightly dotted line 
connecting the control vertices forms the controlpolygon, and indicates the order in which the control 
vertices are to be approximated. The solid and heavily dotted curves represent distinct curve segments. 
Each is a pure parametric cubic. corresponding to the joints between successive polynomial segments are 
called knots. The segment generated as u runs from ui-1 to ul is the ith segment Qi(u). We will always 
assume that the knots are a consecutive sequence of integers, often with ui = i; this is called a uniform 
knot sequence. Usually the piecewise polynomials X(u) and Y(u) are required to satisfy some continuity 
constraints at the joints between successive segments; if the 0th through dth derivatives X(u), X~(u) 
..... X~d~(U)are everywhere continuous (in particular, at the joints), then X and Y are said to be C 
d continuous. Also, it is usually simpler when formulating equations for the ith segment to write X(u-ui-~) 
rather than X(u), and we generally do so; the reparametrization is easily accomplished by substitution. 
There are a variety of ways to actually define a specific curve. We are concerned with techniques in 
which the user provides a number of control vertices, near which the curve is to pass (see Figure 1). 
Moving the control vertices then alters the curve. Such curves can be constructed in the following way. 
Let us denote the control vertices by Vi = (xi, yi). Generically, one writes X(u) = ~ xiBi(u) and Y(u) 
= ~ yiBi (u) (1) i i so that Y(u), for example, is a weighted sum of the basis functions Bi(u), as in 
Figure 2. In vector notation this becomes Q(u) = Y~ViBi(u) = ~ (xiBi(u), yiBi(u)). (2) i One chooses 
to work with some class of basis functions because of particular properties it possesses or imparts to 
the curves it can be used to define. For example, when manipulating a curve it is often desirable that 
the position of each control vertex affect only a limited portion of the curve. Such local control makes 
it possible to change one part of a curve without altering other portions of the curve whose design is 
already satisfactory. To obtain local control, Reprinted From acm Transactions On Graphics--April 1983--Vol. 
2, No. 2 . " ........ " 1 I I 1 I 'M'/,- 2 U/,-1 ~'~ ~'4+1 "U'~+2 Fig. 3. The uniform cubic B-spline 
basis function Bi(u) centered at u~. It is zero for u _ u~-a and for u _> ui+2.  ..'"'o-.,. ...*"~ 
.~"o%.... '% .:" >:." X  ",.. .........../. ......../ .i'.::4><", ............', ........... 
 %-I % Fig. 4. The four uniform cubic B-spline basis functions which are nonzero on the interval [u~-l, 
u~). All the B-spline basis functions are identical in shape; Bi(u), however, is translated so as to 
be centered over ui. 1 Because each Bi(u) is nonzero on only four successive intervals (see Figure 4), 
if ui-i ~ u < ui then we may write (2) as r=+l Qi(u) = ~] Vi+rBi+r(U) r=--2 = Vi-2Bi-2(u) + V.Bi-l(u) 
+ ViBi(u) + Vi+lBi+,(u). (7) If we replace each basis function Bj(u) by the particular basis segment 
that pertains to the interval [ui-1, ui), then (7) can be written as r=+l Qi(u) = ~, Vi+rbr(u) r=-2 = 
Vi-:b-2(u) + V.b-l(u) + Vibo(u) + Vi+lbl(u). (8) " Notice that the segments of our basis function are 
numbered from right to left in Figure 3 because that is the order in which they appear when summed to 
form a curve; the leftmost control vertex weights the rightmost basis segment, and so on. Equation (8) 
also reflects the convenience of parametrizing each basis segment from u = 0 at its left end; since the 
basis functions are all translates of one another, this convention allows us to use the same formulas 
in defining and computing each curve segment. Authors like de Boor Ill] who are concerned with B-splines 
of more than one order usually find it necessary (although it would be less natural for our purposes) 
to index each basis function from the left end of its support, so that in our case Bi(u) would name the 
uniform cubic basis function centered at ui+2 rather than the basis function centered at ui. Reprinted 
From acm Transactions On Graphics--April 1983--Vol. 2, No. 2 II B.A. Barsky and J. C. Beatty 3. UNIFORMLY 
SHAPED BETA-SPLINES The details of what follows may be found in [1]. The unit tangent vector of a curve 
Q(u) is Q(i)(u) (9)  ~(u) -i Q(,)(u) I and the curvature vector is ~(~)(u) K(u) = ,~ (u)lC, T(u) = 
,c (u) --(10) I t*"(u)l where ~ (u) is the curvature of Q at u and 14(u) is a unit vector pointing 
from Q(u) toward the center of the osculating circle at Q(u). 2 'r(u) and K(u) capture the physically 
meaningful notions of the direction of motion and curvature at a point on the curve. With some effort 
it is possible to show that Q*')(u)  Q(21(u)  QIi)(u) K(u) = (11) ]Q(l)(u)14 (Equations (9) and (11) 
are (essentially) two of the Frenet-Serret formulas, written in terms of an arbitrary parametrization 
[16].) Q(u), ~(u), and K(u) are easily seen to be continuous away from the joints of a piecewise polynomial; 
from eqs. (9) and (11) it is possible to show that in order for Q(u), t(u), and K(u) to be continuous 
also at the joints between consecutive curve segments of Q(u), which we ca]] G 2 or second-degree geometric 
continuity, it is sufficient that Qi-l(ui) -~ Qi(ui) (12) f11Q~'21(ui) = Q!l)(ui) (13) fll2Q~2_)l(ui) 
+/~2Ql~l(Ui) = Ql2)(ui) (14) at every knot ui and for any fll and f12 [1]. These equations are, by definition, 
less restrictive than simple continuity of position and parametric derivatives, which is the special 
case in which fll = 1 and f12 = 0. Equation (12) enforces positional continuity. Equation (13) requires 
that the first parametric derivative vectors from the left and right at a joint be collinear, but allows 
their magnitudes to differ. There is an instantaneous change in velocity at the joint, but not a change 
in direction. The intuition behind eq. (14) is somewhat more subtle. A sufficient condition for curvature 
continuity is that Q}2) (ui) ----f112Q!~l (ui), the factor of fll 2 arising from the assumption that 
eq. (13) holds (see [1], [4] and [6] for details.) However, Q I 2~ (ui) may have an additional component 
directed along the tangent Q}121 (ui), since acceleration along the tangent does not "deflect" a point 
traveling along the curve, and so does not affect the curvature there. Let us consider a basis function 
composed of four cubic polynomial basis segments, like the uniform cubic B-splines, but ask that they 
satisfy the geometric 2 The osculating circle at Q(u) is the circle whose first and second derivative 
vectors agree with those of Q at u. The curvature ~ (u) is then the reciprocal of the radius of this 
osculating circle. Reprinted From acm Transactions On Graphics--April 1983--VoL 2, No. 2 constraints 
(12)-(14) instead of the parametric constraints (3)-(5). If we para- metrize each basis segment individually 
on [0, 1) as we did in (6), the equations which result are O=bl(O) 0 --bt '~(0) bl(1) ---- bo(O) 131b~')(1) 
= b~a)(O) bo(1) = b-l(O) 131 b~')(1) = bI(0) b-~(1) = b-2(O) 131 b~(1) --b~(O) b-2(1) = 0 131 b(-'~(1) 
= 0 (15)0 = b ~2) (0) 1312 b~2)(1) + 132 b(~')(1) -- b(o2)(O) 1312 b~2)(1) + f12 b~l)(1) = bff~(O) 1312 
bff~(1) + 132 bff~(1) --bff2)(O) fll 2 b(_Z2) (1) +/32 b~2) (1) --O. In order that the identically shaped 
basis functions we obtain have the "convex hull property" discussed in Section 4.4, it is convenient 
to normalize their height by requiring that r=+l Bi+r(Ui) ---bl(0) + b0(0) + b_,(0) + b-2(0) = 1, r=--2 
yielding a total of 16 equations in 16 unknowns. For any particular values of 131 and 132 these equations 
can be solved numerically to obtain explicit formulas for the polynomials comprising the basis segments. 
This is not very practical, however, since we do not want to solve a new system every time we wish to 
alter one of the 13 parameters. Instead we can solve this system once and for all using a symbolic manipulation 
system such as Vaxima [10, 15], to obtain the following symbolic representation of the basis segments 
for all values of 131 and 132. 1 [2u3 ] bl(U) -- bo(u) = 1 [2 + (6(131)u + (3 132 + 6 1312)u 2 -(2 132 
+ 2 1312 + 2 131 + 2)u 3] 1 b-,(u) = ~ [(132 + 4 1312 + 4 131) + (6 1313 - 6 131)u (16) -(3 132 + 6 1313 
+ 6 1312)u 2 + (2 132 + 2 1313 + 2 1312 + 2 131)u 3] 1 b-2(u) = ~ [(2 1313) -(6 1313)u + (6 1313)U 2 
-- (2 1313)u 3] Reprinted From acm Transactions On Graphics--April 1983--Vol. 2, No. 2 II B.A. Barsky 
and J. C. Beatty V 1 V 2 V 5 V 1 V 2 V 5 ........ ,....+ + + tS-', .......... ...,.. i81 = 1 ~\ ~2 = 
0 ,81 = 2 -~ f12 = 0 + V o + ......... V 3 + V 4 4-V 0 i  ~'~ V 3 -  + V 4 V 1 V 2 + ........... ;.... 
 .+ V 5 + + .................;.... + V 5 + =4 =0 ~2 ---- 0 + + .-....... + Vo v3 V 4 V 0 V 3 V4 i Fig. 
5. This sequence of curves illustrates the effect of increasing fll on a uniformly shaped Beta- spline. 
where 6 = f12 + 2 fll ~ + 4 fll 2 + 4 fll + 2 # 0. Notice that if we substitute fll = 1 and f12 = 0 into 
the Beta-spline constraint equations (15) we obtain equations which define the uniform cubic B-splines, 
and that substituting these values into the Beta-spline basis segments (16) we obtain the B-spline basis 
segments (6). For other values of fll and f12 the Beta-spline basis segments fail to be C 2 continuous 
at knots, although they do satisfy eqs. (15) and are therefore G 2 continuous. Equations (16) can, of 
course, be evaluated more rapidly if they are factored. Since for any particular values of fll and f12 
they are cubic polynomials in u, forward differencing can also be used where appropriate. The efficient 
evaluation of these equations is discussed in [1] and [5]. We refer to a Beta-spline curve whose segments 
are defined by eqs. (7) and (16) as a uniformly-shaped Beta-spline, to distinguish it from the more general 
class of Beta-spline curves that are defined subsequently. Increasing fll increases the "velocity" with 
which we traverse a curve (say from left to right) immediately to the right of a joint, with respect 
to the "velocity" just left of the joint, tl~us serving to bias the curve: values in excess of one cause 
the unit tangent vector at the joint (which is, of course, continuous) to have greater influence to the 
right than to the left, in that the curve will "continue in the direction of the tangent" longer in the 
rightmost segment (see Figure 5). Values of fll ranging from one down to zero have the reciprocal effect, 
causing the curve to lie close to the tangent longer to the left of a joint than to the right. Because 
Beta-spline curves are weighted sums of basis functions, it is instruc- Reprinted From acm Transactions 
On Graphics--April 1983--Vol. 2, No. 2 Local Control of Bias and Tension in Beta-splines 1.0 1.0 --~ 
r T--] p~ = 1 #2 = o Pl = 2 P2 = 0 1.0 1.0 i ...,....-""" __ k,... .... ~81 = 4 ~2 ----0 ~I = 8 ~2 
= 0 Fig. 6. These are the basis functions corresponding to the curves of Figure 5. tive to examine the 
basis functions which define the curves of Figure 5. These are shown in Figure 6. Each is computed for 
a distinct value of ill, which determines the relative magnitude of the slopes to the left and right 
of each joint. Notice that since the same basis function is used for each of X(u) and Y(u), any continuous 
basis function whose first derivative is continuous except for a jump of some arbitrary value (ill) at 
the knots suffices to define a curve with unit tangent continuity. The f12 parameter serves to control 
tension in the curve: altering the value of f12 moves the joint between Qi-1 (u) and Qi (u) along a vector 
that passes through the ith control vertex, and this happens simultaneously for all the joints in a uniformly-shaped 
curve. For example, increasingly positive values move each joint toward its corresponding control vertex 
and flatten the curve against the control polygon (see Figure 7}. The corresponding basis functions are 
shown in Figure 8. Notice that as f12 increases, the peak of the basis function approaches one and the 
"tails" of the basis function, lying in the leftmost and rightmost intervals of its support, approach 
zero. This behavior is predictable from eqs. (16). In some circumstances fll also functions as an "asymmetric 
tension parameter." If for any f12 we allow fll to become arbitrarily large, then the ith curve segment 
converges to a straight line between Vi-2 and Vi-1. If f12 is zero, then as fll goes to zero the ith 
curve segment converges to a straight line between Vi and Vi+l. A more detailed discussion of this behavior 
may be found in [6] and [7]. fll and f12 may be altered, independent of the control vertices, to change 
the shape of the curve. In the curves we have been discussing a single value of fll is used for the entire 
curve, and the same is true for f12. We would prefer, if possible, to specify distinct values of fll 
and f12 at each joint. Before discussing how this can be done, we indicate briefly how uniformly-shaped 
Beta-spline surfaces can be constructed from uniformly-shaped curves. Reprinted From acm Transactions 
On Graphics--April 1983--Vol. 2, No. 2 B. A. Barsky and J. C. Beatty V I V 2 V 1 V 2  + ~+ \. \: ~i 
= I -~2 = o #1 ----1 t" ~2 --5 ...,.  + + ......... + + + V o V 3 V 4 Vo V 3 V 4 V~ V 2 V 5 +~_. ..... 
:_~+ + f12 = 100 fll = 1 i ~2 = 25 + .... + +  V o V 3 V 4 V o V 3 V Fig. 7. This sequence of curves 
illustrates the effect of increasing f12 on a uniformly shaped Beta- spline. 1.0 1.0 .//"~ J" " i ~7" 
......... 7 ............ I ............ -- B1 = 1 B2 -~ 0 1.0 1.0 ~1 = 1 ~'2 = 100 fll = 1 ~2 = 25 Fig. 
8. These are the basis functions corresponding to the curves of Figure 7. 3.1. Surfaces We want to form 
our surface as a weighted sum of basis functions, as in (2), but now X, Y, and Z must be functions of 
two independent variables: Q(u, v) -- ~ Vi,jBi,j(u, v) Z,] = ~ (xijBi,j(u, v), yi,jBi,j(u, v), zijBi,j(u, 
v) ). (17) t,J Reprinted From acm Transactions On Graphics--April 1983--Vol. 2, No. 2 For weights we 
again use the x-, y-, and z-coordinates of what is now a two-dimensional array of control vertices called 
the control mesh or control graph. To obtain locality, we would like the new basis functions B~,j (u, 
v) to be nonzero only for a small range of u and v. An easy way to arrange this is to let B~.j(u, v) 
= Bi (u)Bj (v), where Bi (u) and Bj (v) are the univariate basis functions (16) which we developed for 
the Beta-spline curves. Since each is nonzero only over four successive intervals, if u~-~ _< u < ui 
and vj_z ___ v < vj, we can rewrite (17) as r=+l s=+l Q(u, v)= ~ ~ Vi+r.j+sBi+r(u)Bj+s(V). (18) r=-2 
s=-2 If we rewrite this in terms of basis segments instead of basis functions and adopt the convention 
that the portion of Q(u, v) defined by this set of values for u and v is denoted by Qi.j(u, v), then 
we can write r=+l s=+l Qij(u,v) = ~ ~. Vi+r,j+sbr(u)b.(v), (19) r=--2 s=-2 so that Qi.j(u, v), the i, 
jthpatch, is completely determined by 16 control vertices. The separability of Bi,j (u, v) into Bi (u) 
and Bj (v) can be useful. For example, we can expand (19) as Qi.~(u, v) = [Vi-2j+lb-2(u) + Vi-lj+lb-l(u) 
+ Vi.j+Ibo(u) + Vi+l.j+lbl(u)]bl(V) + [Vi-2j b-2(u) + Vi-l,j b-l(U) +Vij bo(u) + Vi+l.y bl(u)]bo(v) + 
[Vi-2j-lb-2(u) + Vi-lj-lb-l(u) + Vi,j-lbo(u) + Vi+Li-lbl(u)]b_l(v) + [Vi-2j-2b-2(u) + Vi-l.i-2b-l(u) 
+ Vi,j-2bo(u) + Vi+l.j-2bl(u)]b-2(v). (20)  From this it is clear that if we fix u at some arbitrary 
value between 0 and I then we can write (20) as Qi,j,.(v) = W-2b-2(v) + W-lb-l(v) + W0b0(v) + Wlbl(v) 
where Wl = Vi-2,y+lb-2(u} + Vi-l,j+lb-l(U) -4-Vi,j+~bo(u) + Vi+~,j+lb~(u) Wo = Vi-2,j b-2(u) "l- Vi-l,j 
b_~(u) + Vi.j bo(u) + Vi+Lj bl(U) W-1 ----Vi-2,j-lb-2(u) + Vi-l,j-lb-l(U) + Vi,j-lbo(u) + Vi+l,j-lbl(u) 
W-2 ----Vi-2,j-2b-2(u) + Vi-l,j-2b-l(U) + Vi,j-2bo(u) -4- Vi+l,j-zbl(U). Thus Qi,j,. (v) is simply the 
uniformly-shaped Beta-spline curve segment defined by the "control vertices" W-2, W-l, Wo, and Wl. It 
is not hard to see that Qi,j+l,=(v), in the next patch "up," is given by Qijl,.(v) -- W-lb-2(v) + Wob-~(v) 
+ W~bo(v) + W2b~(v) where W2 ----Vi-2,j+2b-2(u) + Vi-l,j+2b-l(U) + Vi,j+2bo(u) + Vi+l,j+2bl(U). This 
is simply the second segment in a uniformly-shaped Beta-spline curve defined by the "control vertices" 
W-2, W-~, Wo, W~, and W2. It follows imme- Reprinted From acm Transactions On Graphics--April 1983--Vol. 
2, No. 2 B. A. Barsky and J. C. Beatty diately that this curve is G 2 continuous. Since a completely 
analogous argument can be made with respect to u by factoring the br(u) out of (19) instead of the bs 
(v), the uniformly shaped Beta-spline surface we have defined is G 2 continuous along lines of constant 
u and v. 4. INTERPOLATING DISTINCT fl's Let a li and a 2i be the values of fl 1 and fl 2 to be associated 
with the joint between Qi-l(u) and Qi(u). We would like to use the basis segments given by eqs. (16}, 
making fl 1 and f12 functions of u in such a way as to interpolate between the a l's and a2's at each 
end of a segment while preserving G 2 continuity of the curve. Let us consider the following derivative 
with respect to u of a representative term of (16). 3 C[}~ (u) ]PUq (21) 6(u) where c is a constant. 
Its first parametric derivative is cq[fl(u)]Pq q-' cp[fl(u)]p-'fl('~(u)u q c[fl(u)]P6('~(u)u q 6(u) + 
6(u) 6(u) 2 (22) where 6(u) = fl2(u) + 2[ill(u)] 3 + 4Jill(u)] 2 + 4ill(u) + 2 ~(1)(u ) _- ~2(1)(u) + 
6[fll(u)]2fll(l~(u) + 8fll(u)fll('~(u) + 4fll(')(u) (23) Examination of (22) and (23) reveals that the 
second and third terms of (22) involve products with flll')(u) or fl2(1)(u), while the first term of 
(22) would constitute the complete parametric derivative if fl i and f12 were not functions of u. If 
we were to compute fl l(u) and fl2(u) by interpolating between the ali's and a2z'S in such a way as to 
cause fll(1)(u) and fl2(1)(u) to be zero at each joint, then eqs. (13) would hold and G' continuity would 
be preserved. Similarly, the second parametric derivative of (21) is c(n-1)n[fl(u)]mu n-2 ~(u) --C[fl(u)]m6(2~(U)U 
n + 2C[fl(U)]m6('~(U)2U" 6(u) 2 6(u) 3 2cm[B(u)]m-'fl(1)(u)6(ll(u)u " 2cn[fl(u)]m6(')(u)n "-' 8(u) 2 
t~(u) 2 cm[~(a)]m-'~(2)(u)u n c(m-1)m[~(u)]m-2~(1)(U)2Un + ~(u) + ~(u) 2cmn[B (u)]m-'fl('~(u)u=-I + (24) 
6(u) 3 We use fl(u) rather than ill(u) or fl2(u) when the argument applies to both, but no confusion 
can occur because products of fl i and f12 do not arise. Similarly, a~ is used to represent both a li 
and a 2i. ReprintedFrom acm Transactions On Graphics--April 1983--Vol. 2, No. 2 Computer Graphics Volume 
17, Number 3 July 1983 where 8~2~(u) = f12~2~(u) + 6[fll(u)]2fll~2~(u) + 8fll(u)fll~2~(u) + 4fll(2~(u) 
+ 12fll(u)[flll~(u)] 2 + 8f11~ll(u) 2. Again, only the first term of (24) lacks a product with at least 
one of fll*Z)(u), fl2~l~(u), fll*2~(u), or f12*2~(u), and the first term would constitute the complete 
second parametric derivative if fl I and f12 were not functions of u. Thus arranging for all four derivatives 
to have the value zero at joints should be sufficient to preserve G 2 continuity of the curve. This is 
easily accomplished in the following manner. Suppose that we use a polynomial H(o/i-~, o/i; u) to interpolate 
between o/i-1 and o/i. We have six constraints, since we would like H(o/i-1, o/i; O) = o/i-1 H(o/i-1, 
o/i; 1) = o/i Ht~(o/i-1, ~; O) = 0 H(1)(o/i-1, o/i; 1) =.0 H(2)(o/i-1, ai; 0) = 0 H(2)(O/i-], o/i; 1) 
= 0. This suggests the use of a fifth-degree polynomial. If H(ai-l, o/i; u) = a + bu + cu 2 + du 3 + 
eu 4 + fu ~, then the above equations take the form H(o/i-1, o/i; 0) ~ o/i--1 ~ a H(o/i-1, o/i; 1) =o/i=a 
+ b+ c + d + e + f H(1)(O/i-1, Oli; 0) =0= b HIl~(o/i_l, ai; 1) = 0 = b + 2c + 3d + 4e + 5f H(2)(O/i-1, 
o/i; 0) = 0 = 2c H(2)(o/i-1, ai; 1) =0=2c+6d+ 12e+20f. It is straightforward to solve these for the polynomial 
t~i(u) = H(o/i-1, o/i; u) = ai-1 -I-10(o/i -- o/i-1)U 3 -- 15(o/i--o/i-1)U a + 6(o/i -- o/i-1)U 5 m ai--1 
+ (o/i --ai-1)[10u 3 -15u 4 + 6u~], (25) which is, in fact, a special case of quintic Hermite interpolation. 
(Figure 9 illustrates the use of this polynomial to interpolate between o/li-1 = 0.5 and o/Ii = 1.6.) 
By the argument given above, the use of (25) to interpolate fll and f12 in (16) preserves G 2 continuity 
of the curve. It is, of course, possible that the derivative terms appearing in (22) and (24) might sum 
in such a way as to yield G 2 continuous curves even though the derivatives were nonzero; we have not 
ruled this out for all other interpolation Reprinted From acm Transactions On Graphics--April 1983--Vo1.2, 
No. 2 Computer Graphics Volume 17, Number 3 July 1983 m B. A. Barsky and J. C. Beatty H ( O ) H(1)=1.6 
 I I %-1 % H(a,b,u) = a + (b-a)E10u3-15u4+6u5-I Fig. 9. An application of the formula for interpolating 
a values between joints. schemes. However, using Vaxima [10, 15], it is not hard to produce examples 
which demonstrate that neither linear interpolation nor cubic Hermite interpo- lation work. Moreover, 
geometric continuity is not necessarily preserved by using general quintic Hermite interpolation, even 
if the same two nonzero values are used for the first and second derivatives of ill(u) at the joints 
(and similarly for fl2(u)). Thus C 2 continuity of ill(u) and fl2(u) is not sufficient to ensure G 2 
continuity. (See [7] for an example.) We refer to the curves whose segments are defined by eqs. (7) and 
(16), where ill(u) and fl2(u) are interpolated by eq. (25), as continuously-shaped Beta- splines.  
4.1 Locality As with the uniformly-shaped Beta-splines, each basis function is nonzero only over four 
successive intervals. Since each control vertex is used to weight a particular basis function, moving 
a control vertex will alter only the four corre- sponding curve segments. These are, of course, consecutive. 
The effect of altering an a value is more localized still. The a value at a particular joint determines 
how the corresponding fl parameter is interpolated over the segments which meet at that joint, so that 
only two curve segments are changed. 4.2 Bias Figure 10 illustrates a few of the effects which can be 
obtained by altering a l's. The extreme locality of the Beta-spline curves with respect to changes in 
the shape parameters can result in "kinks" if there are large differences in the a values at consecutive 
joints. A modest reduction in the size of the jumps (see Figure 11) can be used to ameliorate such effects, 
if that is desirable. 4.3 Tension Since this scheme interpolates the all and a2i, the discussion of 
tension in [1] and [4] is equally applicable here: (C -cV~) Qi(O) -Vi -- Q.(1) -Vi - (c + a2i) Reprinted 
From acm Transactions On Graphics--April 1983--V01.2, No. 2 V 1 V 2 V I V 2 V 5 + ~;...: .. + + . ~ a 
1 =2" ..... ~2 = 0 "a1=1 "". f12 = 0 .\ . ",, a1--1 a1=1 . ~a1=1 a1=1 + + ......... + + 4- ......... 
4-V o V3 V4 V 0 V 3 V 4 V1 V2 :5 V I V 2 V 5 + ...... ....+ + /~a.=";~'"":" + + ~',,.,~.~ : J (~,j= "~....... 
al "~ P2 = 0 ",=~ ~ ,e 2 = 0 ai=3 :~ + + ......... + + + ......... + V o V 3 V 4 V 0 V 3 V 4 Fig. 
10. For these curves f12 is held constant at zero while fl 1 interpolates the a 1 values shown. V I V 
2 V 5 V 1 V 2 V 5 ,:, ................... . .......   o=ii --//~4'" = ....... ;":"~...%"~ ~2 = 0 
#2 = 0 a4=2 a1=3~=1 at1=1 V 0 V 3 V 4 Vo v3 v4 Fig. I1. Eliminating the kink in Figure 10. is the vector 
from the ith control vertex to the joint between Qi-](u) and Qi(u), where C = 2al~V/-1 + 4ali(ali + l)Vi 
4- 2Vi+l c = 2al/a + 4al/2 + 4ali + 2. Altering a2i merely changes the length of this vector: values 
approaching -c "push" the joint arbitrarily far away from Vi, while large positive or negative values 
draw the joint arbitrarily close to Vi (see Figure 12). Hence a2i serves as a tension parameter, just 
as for uniformly-shaped Beta-spline curves. Again, wildly disparate values of f12 for adjacent control 
vertices can be used to produce kinks. These can be removed (see Figure 13), if that is desirable, by 
smaller adjustments in neighboring f12 values. For comparison with earlier illustrations, Figure 14 gives 
some examples of continuously-shaped basis functions. Reprinted From acm Transactions On Graphics--April 
1983--Vol. 2, No. 2 BB B.A. Barsky and J. C. Beatty V 1 V 2 V 5 V 1 V 2 V 5 a2=0 0(2=0 %=0 2= \. 81 = 
1 ", '. = 81 = I :'% V o V 3 V 4 V 0 V 3 V 4 V 1 V 2 V 5 V 1 V 2 V 5 .y a2= 100' i a2=0 := i i 81 = 
1 81=1 ,. = .'.~~ .0. o V o Y 3 V 4 Vo V 3 V 4 Fig. 12. The value of f12 at the joint nearest to V2 
is increased from 0 to 100 in three steps, pulling the joint toward V2. In the limit this joint converges 
to V2. Fig. 13. Altering the value of f12 at a joint V 1 V 2 V 5 affects only the two curve segments 
that meet f a2=I00 ":" there. Making one such a2 very large in com- ia2=6 i parison with its neighbors, 
as in Figure 12, ! causes these two segments to be abruptly pulled close to the control polygon. The 
value of f12 at adjacent joints can be adjusted to smooth out #1 = 1 i .\ a2=9 a2=O J the curve. V 0 
V 3 V 4 1.0 1.0 '" y aI=1,1,2,4,1 #2----0 #i=I a2=0,5,25,1 00,0 Fig. 14. Here we illustrate 
the effect of interpolating fl values on the basis functions. On the left al changes value from joint 
to joint, while on the right a2 changes value. 4.4 Convex Hull Intuitively, we obtain the convex hull 
of a set of vertices by "shrink wrapping" a rubber band in 2D or a balloon (in 3D) around them. A line 
segment joining any two points within such a convex hull is itself entirely within the convex hull. More 
Reprinted From acm Transactions On Graphics--April 1983--Vol. 2, No. 2 Computer Graphics Volume 17, Number 
3 July 1983 generally, it is not hard to show (see [6] for a simple induction argument) that if we "weight" 
any set of m + 1 vertices Vo, ..., Vm by nonnegative coefficients Co..... ca which sum to one, then the 
point P = c0Vo + c~V~ + Cm-lVm-1 + cmVm lies within the convex hull of the Vi. Like the uniformly-shaped 
Beta-splines, continuously-shaped Beta-spline curves posses a convex hull property in that the ith segment 
lies within the convex hull of control vertices Vi-2, V,, V,, and V,+I, so long as tl and i2 are nonnegative. 
The argument, as we shall see, is straightforward. Recall that each basis function is nonzero over four 
intervals, so that Qi(u) = Vi-2b-2(u) -4- Vi-lb-l(u) 4- Vibo(u) 4- Vi+lbl(U). (26) Now for any given 
value of u, tl(u) and i2(u) yield some particular value of il and i2. By simply summing eqs. (16) we 
see that for every such ill, i2, and u bdu) + bo(u) + b-du) + b-2(u) ~-1. Next we must verify that these 
basis segments are nonnegative for all u in the interval [0, 1). If we rewrite eqs. (16) in the form 
1 (2U3) bl(u) = bo(u) =-~ [2f112ue(3 -u) + 2ilu(3 -u 2) + i2u2(3 -2u) + 2(1 -u3)] b-~(u) --  [2f113u((1 
-u)(2 -u) + 1) + 2flle(u 3 - 3u 2 4- 2) + 2i1(u 3 - 3u + 2) + i2(2u 3- 3u 2 + 1)] b-2(u) ~-~[2ila(1 
-u) a]  where tl -- f12 + 2ill 3 + 4ill 2 + 4ill + 2 for fll _> 0, f12 _> 0, and u E [0, 1), it is easy 
to see by inspection that bl(u), bo(u), and b-2(u) are nonnegative. For b-~(u), elementary consideration 
of the zeros of the derivatives 3u(u -2), 3(u -1)(u -1), and 6u(u -1) of u 3 -3u 2 + 2, u 3 - 3u + 2, 
and 2u 3 - 3u 2 + i yields the same conclusion. Since fll and f12 are actually interpolated by (25), 
it is necessary to show that fli(u) = ai-1 + (ai -ai-D[lOu a -- 15u 4 + 6u ~] --> O, if ai-1 --> O, ai 
--> O, and u E [0, 1]. Consider t~I)(U) ----30(ai --ai-1)U2(1 --U) 2. Clearly the slope changes sign 
only at u = 0 and u = 1. Since ~-i 4- O/i ti(0.5) --->_ 0 if ai-1, ai >- O, 2 fli(u) must be nonnegative 
on [0, 1) so long as the ai are nonnegative. Reprinted From acm Transactions On Graphics--April 1983--Vol. 
2, No. 2 B. A. Barsky and J. C. Beatty Hence so long as the ali -> 0 and a2i >- 0, each Qi (u) lies within 
the convex hull of Vi-2, Vi-1, Vi, and Vi+l. 4.5 End Conditions A properly defined curve segment is the 
sum of four weighted basis functions, as in eq. (26). Thus rn + 1 control vertices Vo,  , Vm can be 
used to define m -2 segments, which we index as Q2 (u) ..... O~n-1 (U). The Beta-spline curve does not, 
in general, begin at a control vertex, 4 or even at a point along the line segment from V0 to Vl. The 
most that we can say is that the initial point lies within the convex hull of V0, Vl, and V2. To obtain 
better control of the endpoints, one therefore often treats them specially. Let Q(u)be a continuously-shaped 
Beta-spline with fll = all and f12 = a2i at the joint between the ith and (i + 1)st segments. Let R(u) 
be a uniformly-shaped Beta-spline curve defined by the same control vertices, but with fll = all and 
f12 = a2i throughout. By the definition of Q(u) we must have Q(u) = R(u), Q(l)(u) = R(1)(u), and Q(2)(u) 
= R (2)(u) at the joint in question. Hence the analysis of end conditions in [1] applies immediately 
to the continuously-shaped Beta-splines. For convenience we summarize these results. --A Double First 
Vertex. We define an additional segment at the beginning of the curve by Ql(u) = V0[b-2(u) + b-l(U)] 
+ V, bo(u) + V2bl(u). Ql(U) begins at a point lying along the line segment from Vo to Vl, at which point 
it is tangent to that line and has zero curvature. (Figures 1 and 2 illustrate the use of double initial 
and final vertices.) --A Triple First Vertex. We define two additional segments at the beginning of the 
curve by Qo(u) = V0[b-2J(u) + b-~(u) + bo(u)] + Vlb~(u) Q,(u) = Vo[b-2(u) + b-l(U)] + Vlbo(u) + Vebl(u). 
The curve then begins at Qo (0) = Vo, and the first segment of the curve is a short straight line. The 
behavior of the second segment Q~ (u), which has a double first vertex, is described above. The analysis 
of double and triple vertices is equally applicable on the interior of a curve. Triple interior vertices 
are particularly interesting since they result in a cusp, as in Figure 15. This cusp is not a violation 
of G 2 continuity because, or at least in the sense that, the first parametric derivative vector has 
the value (0, 0) at the joint that coincides with the interpolated control vertex where the cusp occurs, 
so that the unit tangent vector is not defined. Multiple vertices give a tension-like effect, and it 
is instructive to compare the effect of repeating a vertex in Figure 15 with the effect of altering f12 
at the corresponding joint in Figure 16. An alternative way of controlling the beginning of a curve is 
to automatically 4 The terminal point of the curve is analyzed in an exactly analogous manner, and we 
therefore omit explicit treatment of it. Reprinted From acm Transactions On Graphics--April 1983--Vol. 
2, No. 2 a single vertex, a2=0 a double vertex, G{2=0 a tr[ple vertex, a2=0 4" ..~t.. "%,,,\ '"''..% 
f 4" 4"  4- + 4" 4-4-4" 4" Fig. 15. al is one and a2 is zero at all joints; these are in fact simply 
uniform cubic B-splines, although a cusp results at a triple vertex for any values of al and a2 unless 
the control vertices immediately preceding and following the vertex are both collinear with it. The double 
control vertex is not interpolated, while the triple vertex is. o single vertex, a2=5 a single vertex, 
a2=25: a single vertex, O{2=--34" + f .............,.\ 4" : 4" 4" 4" + f ..........-,......... + 4" 
4" 4" 4" 4-+ Fig. 16. Here a l is one at all joints and a2 is zero except as indicated. define a phantom 
vertex V-~ and a corresponding initial segment Qi(u) --V-lb-2(u) + Vob-l(U) + Vlbo(u) + V2bl(u) in such 
a way as to satisfy some requirement. We may ask that --Q~ (0) interpolate some furnished point (usually 
with nonzero curvature), --Q1 (0) interpolate Vo(at which point the curvature is then zero), --Q ~) (0) 
have a specified value (usually resulting in nonzero curvature), _ Q~2~ (0) have a specified value (usually 
resulting in nonzero curvature), _ Q ~2) (0) be zero, resulting in zero curvature at QI (0). All of these 
techniques involve extending the curve by one or two segments at either end. This implies the existence 
of additional joints and associated a values. Hence the sequence of control vertices is extended in order 
to specify behavior at the ends of the curve, and additional al and a2 values must be specified as well. 
These may take any value without affecting the behavior described above. In practice it is probably easiest 
simply to replicate a values as well as vertices. Again, the arguments establishing these results appear 
in [1] and the details are therefore omitted. The curves we have discussed so far are open curves, which 
is to say that the two endpoints do not, in general, coincide. A G2-continuous closed curve whose endpoints 
do meet and which is G2-continuous there as well is obtained if the Reprinted From acre Transactions 
On Graphics--April 1983--Vol. 2, No. 2 Computer Graphics Volume 17, Number 3 July 1983 BB B.A. Barsky 
and J. C. Beatty first three control vertices are identical to the last three and the same values of 
fll and ,82 are used at the joint between the beginning and ending of the curve. 4.6 Evaluation If a 
single point on Q(u) is to be determined, eq. (25) can be evaluated in six multiplications and four additon/subtractions 
per coordinate if it is factored into the form H(ai-1, ai; u) = oti-1 .-b (oti -ai-1)[10 + (6u -15)u]u 
3. Since both ill(u) and fl2(u) must be computed, both H(ali-1, all; u) and H(a2i-1, a2i; u) must be 
evaluated. However, since [10 + (6u -15)u]u 3 need only be evaluated once, the total cost of interpolation 
is seven multiplications and six addition/subtractions. If the uniformly-shaped Beta-spline basis functions 
are evaluated using the algorithms described in [ 1] and [5], then the cost of evaluating a continuously-shaped 
Beta-spline curve at one point is about 20 percent more than the cost of evaluating a uniformly-shaped 
Beta-spline. More often we wish to evaluate a sequence of points along each segment in order to render 
a curve. If we compute these points by repeatedly evaluating the basis function as described above, then 
a continuously-shaped 2D Beta-spline segment can be evaluated at r values of u with about twice as much 
work as is required to evaluate a sequence of points on a uniformly-shaped segment. The difference results 
from the need to reevaluate the coefficients of the polynomials forming the basis segments, owing to 
the fact that fll and f12 are no longer constant, as well as from the cost of actually performing the 
interpolation. If instead we first sum the terms in eqs. (8) so as to compute the coefficients of X(u) 
and Y(u), and then use Horner's rule (nested multiplication), then the evaluation of a 2D continuously-shaped 
Beta-spline segment at r points requires approximately eight times as much work as does evaluation of 
the analogous uniformly-shaped segment. A third alternative is to use forward differencing techniques. 
For large values of r the evaluation of a 2D uniformly-shaped curve in this way is almost a factor of 
17 faster than the evaluation of a continuously-shaped curve using Homer's rule, although it is subject 
to cumulative roundoff error. While, in principle, forward differencing is applicable to the continuously-shaped 
Beta-splines as well, in fact it is impractical since each coordinate is the quotient of an 18th- and 
a 15th-degree polynomial. Where cost is a crucial factor, it may be desirable to fix fll at one and manipulate 
f12 alone. Doing so significantly reduces the expense of evaluating eqs. (16) after interpolating f12; 
each coordinate is then the quotient of an 8th- and a 5th-degree polynomial. There are other possibilities. 
The basis functions of a uniformly-shaped Beta- spline are translates of one another and need only be 
evaluated for the first segment drawn if they are saved and reused. In the case of continuously-shaped 
Beta-splines, each joint is associated with distinct values of fll and f12, so that in general each basis 
function has a different shape and must be individually evaluated. Altering an existing curve can be 
done much more efficiently. If a control vertex is moved, then only four segments of the curve must be 
recomputed, since Reprinted From acm Transactions On Graphics--April 1983--Vol. 2, No. 2 the basis function 
which the vertex weights is nonzero on only four successive intervals. Since the vertex is usually moved 
several times in succession, it is advantageous to save the basis segments as they are first evaluated 
to avoid recomputing them. Moreover, the portions of the computation for each segment which are actually 
dependent on the vertex which is being moved may be segregated from those portions of the computation 
which are not, and which therefore need not be recomputed. Altering an a parameter necessitates recomputing 
only two intervals, although all the basis segments in each must be reevaluated. 4.7 Surfaces Continuously-shaped 
Beta-spline curves can be elegantly generalized to define surfaces which preserve G 2 continuity at the 
boundaries between adjacent patches. The generalization we present allows the user to specify a bias 
and tension parameter at each corner of a patch; of course, patches which share a corner make use of 
the same a values at that corner. The technique is to generalize the univariate interpolation formula 
(25) to a bivariate formula in such a way that --the a values at the four corners of a patch are interpolated, 
--two patches that share an edge will have the same fl values along that edge, --the first and second 
partial derivatives of ill(u, v) and fl2(u, v) across a patch boundary will be zero. This last property 
will allow us to ignore (at boundaries) all but one of the terms which arise in computing the partial 
derivatives of a Beta-spline surface in which ill(u, v) and fl2(u, v) are allowed to vary, so that the 
properties of a uniformly- shaped Beta-spline surface will be inherited by our continuously-shaped surface. 
Thus our first consideration is to develop a bivariate interpolation formula. It is at least plausible 
that we would like lines of constant u or of constant v on a continuously-shaped surface to be continuously-shaped 
curves. Along such curves we would then expect fll and f12 to vary as they do along continuously-shaped 
Beta-spline curves. For convenience let us write eq. (25) in two pieces as s= 10u 3-15u 4+6u ~ H(cti-], 
ai; u) = (1 -s)ai-1 + sai, and along the top and bottom boundaries of the patch interpolate the a values 
Oli--~ Oli,j } I Oli-- 1 ,j-- 1 Oli,j-- 1 with our special quintic Hermite formula to obtain O~top = 
H(oli-l,j, o~i,j; u) = (1 --s)o~i-],j 4" SOZi,j O~bot = H(o~i-Lj-], oLi,j-1; U) ~ (1 --S)oLi-I,j-1 -t- 
SOil,j-1. This yields values of a at parametric distance u from the left edge along the top and bottom 
of the patch. To interpolate in the v direction across the interior of Reprinted From acm Transactions 
On Graphics--April 1983--Vol. 2, No. 2 BI B.A. Barsky and J. C. Beatty the patch it is natural to again 
use the formula H(abot, atop; V) = (1 -t)abot + tO, top with t= 10v 3- 15v 4+6v ~. Substituting, we obtain 
the desired bivariate interpolation formula 1~i,j(U, U) = (1 -s)(1 -t)oti-l,j-1 + s(1 -t)ai,j-1 + (1 
-s)t oli-l,j -~- st oli,j (27) with S = IOu 3 --15U 4 + 6u 5 t = IOv 3 --15V 4 + 6v 5. (We emphasize 
that s and t are used here for notational convenience.) fli,j(u, v) has some rather attractive properties: 
--It interpolates ai- ~,j- ~, aij- 1, ai- ~,j, and ai,j. --Along any of the four borders of a patch it 
reduces to the univariate interpo- lating formula (25). -- The first and second partial derivatives of 
fli,j(u, v) with respect to v for v = 0 and v = 1 (i.e., across a "horizontal" patch boundary) are zero, 
as are the first and second partial derivatives with respect to u for u = 0 and u = 1. Now let us define 
a continuously-shaped Beta-spline surface patch Qg,j by eq. (19) except that we let fll and f12 be functions 
of u and v, using eq. (27) to interpolate between a values associated with the corners of each patch. 
To simplify the notation, we shall actually discuss Q2,2 and Q2,3, which are defined by the control vertex 
mesh Vo,4 Vl.4 V2.4 V3.4 Vo,3 Vl.3 V2,3 V3,3 Vo,2 Vl,2 V2,z V3,z Vo,1 Vl, l V2,1 V3,1 Vo, o Vl,o V2,0 
V3,0 (The generalization for arbitrary patches is straightforward.) Since the br (U) and b~(v) are now 
functions of ill(u, v) and fl2(u, v), we write eq. (20) for Q2,3 as Q2,3(u, v) --[ Vo.4b_2(fll, fl2; 
u) + V~.4b-.(fll, fl2; u) + V2.4bo(fll, fl2; u) + V3.4b~ (fll, fl2; u) ] b~ (fll, fl2; v) + [Vo,3b-2(,81, 
,82; u) + V~,3b ,(81, ,82; u) + V2,a bo(,81, f12; u) + V3,~/:h (ill, ,82; u)] bo(fll, ,82; v) + [Vo,2b-2(fll, 
,82; u) + V~,2b-~(fll, f12; u) + V2,2bo(fll, ,82; u) + V3,2 b~ (,81, ,82; u)] b-~ (,81, ,82; v) + [ 
Vo.,b_~(,al, ,82; u) + V,.,b-,(fll, B2; u) + V=.,bo(B1, B2; u) + V~., b~(pl, B2; u)]b-=(/l, ,e2; v). 
 (28) Reprinted From acm Transactions On Graphics--April 1983--V01.2, No. 2 Q2,2 is similarly defined 
by Q2,=(u, v) = [Vo.ab_2(fll, f12;u) + V,.ab-l(fll, f12;u) + V2,3bo(fll, f12; u) + Va.3 b, (ill, f12; 
v)] b, (ill, f12; v) + [ Vo.2b-z(fll, f12; u) + Vl.2 b-, (ill, f12; u) + V2.2 bo(fll, f12; u) + V3.2 
b~ (ill, f12; u)] bo (ill, f12; v) + [Vo.,b-2(fll, f12; u) + V,., b_, (ill, f12; u) + V2.I bo(fll, f12; 
u) + Va,, b, (ill, f12; u)] b-, (ill, f12; v) + [Vo.ob-2(fll, f12;u) + V,.ob-,(fll, f12;u) + V2,obo(fll, 
f12; u) + V3.0b, (ill, f12; u)]b-2(fll, f12; v)  (29) We shall discuss the behavior of these patches 
at their common "horizontal" boundary, which is Q2.3(u, 0) and Q2.2(u, 1). (The argument for common "vertical" 
boundaries is analogous, and is therefore omitted.) First, of course, we must verify that the curves 
Q2,3(u, 0) and Q2,2(u, 1) are actually identical. For any fixed u we may rewrite (28) and (29) as Qbot(V) 
= W-2b-2(v) + W-,b-,(v) + Wobo(v) +W,b,(v) (30) and Qtop(V) --w_, b_2(v) + Wob-,(v) + W, b0(v) + W2b,(v) 
(31) where W2 Vo,4b-2(u) + V,,4b-,(u) + V2,4bo(u) + V3,4b,(u) Wl ----Vo,3b-2(u) + V,,3b-l(u) + Ve,3bo(u) 
+ V3,3b,(u) Wo = Vo,2b-2(u) + V,,2b-,(u) + V2,2bo(u) + V3,2b,(u) (32) W-1 ~-Vo, lb-2(u) -4-V1,]b-,(u) 
+ V2,,bo(u) + V3,1bl(U) W-2 Vo.ob-2(u) + V,.ob-,(u) + V2,obo(u) -4- V3.ob,(u). Along the common border 
f12,3(u, 0) and f12,2(u, 1) both reduce to H(a,,2 a2,2; u). Hence the fll and f12 which appear in (28) 
and (29) are identical, so that (30) and (31) are simply two successive segments on a uniformly shaped 
Beta-spline curve. Hence Qbot(1) ----Qtop(0). Hence, Q2,e(U, 1) = Q2,3(u, 0), as desired. Tangent and 
curvature continuity between patches follow similarly if we apply the argument used earlier. Recall that 
the partial derivatives of ill(u, v) and fl2(u, v) with respect to v for v = 0 and v = 1 are zero. If 
we fully expand eq. (28) or (29), a typical term has the form c[fll(u, v)]m[fl2(U, V)]"UPV q [fl2(U, 
V)] + 2Jill(u, V)] 3 + 4[ill(u, V)] 2 + 4[ill(u, V)] + 2" If we then compute the first partial derivative 
of this term with respect to v, we find, after repeated application of the product, quotient, and chain 
rules, that the only resulting term which does not contain a product with at least one of 8 ill(u, v) 
and ~v fl2(u, v), Ov Reprinted From acre Transactions On Graphics--April 1983--V01.2, No. 2 B.A. Barsky 
and J. C. Beatty f12 = 25 Fig. 17. On the left is a Beta-spline surface in which fl 1 = 1 and f12 = 0, 
a uniform bicubic B-spline surface. On the right the f12 value at the joint corresponding to the indicated 
control vertex has been increased to 25. The 12 boundary vertices in the control graph have been "doubled" 
so as to define a total of 9 patches; otherwise the 16 control vertices shown would define only a single 
patch lying close to the 4 central control vertices. both of which are zero, is cq[fll(u, v)]~[fl2(u, 
v)]" u p v q-1 [flZ(u, v)] + 2[fll(u, v)] 3 + 4[fll(u, v)] z + 4[fll(u, v)] + 2" This is exactly the 
derivative which would have been obtained if fll and f12 had not been functions of v. Therefore the first 
partial derivative of (28) with respect to v, for any u and v -- 0, is exactly (1) Qtop(0) ----W-lbgz)(0) 
+ W0b(-'~(0) + Wab(01)(0) + W2b~)(0), and the first partial derivative of (29) with respect to v, for 
any u and v = 1, is exactly Q(b~t(1) = W-2bPz)(1) + W_~b~(1) + Wob(0~)(1) + Wl b~ ~) (1). These are simply 
the derivatives of two successive segments of a uniformly-shaped Beta-spline curve for particular values 
of fll and f12, and we already know that such a curve has tangent continuity at its joints. Hence our 
surface has unit tangent vector continuity along its "horizontal" boundaries. The same argument works, 
mutatis mutandis, for the "vertical" boundaries as well, and generalizes to arbitrary patch boundaries, 
so that our surface is everywhere G ~ continuous. An analogous argument suffices to establish curvature 
vector continuity. G 2 continuity can also be directly verified using Vaxima by evaluating the Beta- 
spline constraint equations if (27) is used to compute the values of fll and f12. Two continuously-shaped 
Beta-spline surfaces are shown in Figure 17. The 12 boundary vertices in the control graphs shown have 
been "doubled" so that the control array defining ttfese surfaces is actually Vo,3 Vo,3 V,,3 V2,3 V3.3 
V3,a Vo,3 Vo,3 V,,3 V2,3 V3,3 V3,3 Vo,2 Vo,2 V1,2 V2,2 V3,2 V3,2 V0A V0,1 Vl,1 V2,1 V3,1 V3,1 V0,0 V0,0 
Vl,0 V2,0 V3,0 V3,0 Vo,o Vo,o Vl,0 V2,0 V3,0 V~,o Reprinted From acm Transactions On Graphics--April 
1983--Vol. 2, No. 2 Local Control of Bias and Tension in Beta-splines Thus each surface actually consists 
of 9 patches, each of which is rendered in Figure 17 by drawing 7 equally spaced lines of constant u 
and 7 equally spaced lines of constant v. Boundary conditions for Beta-spline surfaces are discussed 
further in [1], [4], and [6]. 5. CONCLUSIONS Though the continuously-shaped Beta-splines are more expensive 
to compute than their uniformly-shaped brethren, they provide the first means of locally controlling 
the bias and tension in a cubic polynomial spline. This is an important feature in computer-aided design 
applications. Although the continuously varying Beta-splines are, in principle, rather high-degree polynomials, 
they are naturally factored into tractable pieces. Indeed, they are interesting precisely because they 
provide a useful and convenient way of controlling higher than cubic piecewise polynomial curves. REFERENCES 
1. BARSKY, B.A. The Beta-spline: A local representation based on shape parameters and funda- mental geometric 
measures. Ph.D. dissertation, Dept. of Computer Science, Univ. of Utah, Salt Lake City, Utah, Dec., 1981. 
 2. BARSKY, B.A. Exponential and polynomial methods for applying tension to an interpolating spline curve. 
Computer Vision Graphic Image Processing 1983, to appear. 3. BARSKY, B.A. A study of the parametric 
uniform B-spline curve and surface representations. Tech. Rep. CSD 83/118, Computer Science Div., Univ. 
of Calif., Berkeley, Calif., May 1983. 4. BARSKY, B.A. The Beta-spline: A curve and surface representation 
for computer graphics and computer aided geometric design. To be published. 5. BARSKY, B.A. Algorithms 
for the evaluation and perturbation of Beta-splines. To be published. 6. BARSKY, B.A., BARTELS, R.H., 
AND BEATTY, J.C. An introduction to the use of splines in computer graphics. CS-83-9, Dept. of Computer 
Science, Univ. of Waterloo, Waterloo, Ontario, Canada, 1983. 7. BARSKY, B.A., AND BEATT, J.C. Varying 
the betas in Beta-splines. CS-82-49, Dept. of Computer Science, Univ. of Waterloo, Waterloo, Ontario, 
Canada, 1982. Also Tech. Rep. CSD 82/112, Computer Science Division, Univ. of Calif., Berkeley, Calif., 
Dec. 1982. 8. BEZIER, P.E. Emploi des Machines ~ Commande Numdrique. Masson et Cie., Paris, 1970. English 
ed., Numerical Control--Mathematics and Applications, A. R. Forrest and A. F. Pankhurst, Trans., Wiley, 
New York, 1972. 9. B~ZIER, P.E. Essai de d~finition num6rique des courbes et des surfaces exp6rimentales. 
Ph.D. dissertation, Univ. Pierre et Marie Curie, Paris, Feb. 1977.  1O. BOGEN, R., GOLDEN, J., GENESERETH, 
M., AND DOOHOVSKOY, A. MACSYMA Reference Man- ual, version 9., Massachussetts Institute of Technology, 
Cambridge, Mass., 1977. 11. DE BOOR, C. A Practical Guide to Splines, vol. 27, Applied Mathematical Sciences. 
Springer- Verlag, New York, 1978. 12. CLINE, A.K. Scalar- and planar-valued curve fitting using splines 
under tension. Commun. ACM 17, 4 (Apr. 1974), 218-220. 13. COONS, S.A. Surfaces for computer-aided design. 
Design Div., Mechanical Engineering Dept., Massachusetts Institute of Technology, Cambridge, Mass., 1964. 
 14. COONS, S.A. Surfaces for computer-aided design of space forms. MAC-TR-41, Project MAC, Massachusetts 
Institute of Technology, Cambridge, Mass., June 1967. 15. FATEMAN, R.J. Addendum to the MACSYMA Reference 
Manual for the VAX. Univ. of Calif., Berkeley, Calif., 1982. 16. FAUX, I.D., AND PRATT, M.J. Computational 
Geometry for Design and Manufacture. Wiley, New York, 1979. 17. GORDON, W.J., AND RIESENFELD, R.F. B-spline 
curves and surfaces. In Computer Aided Geometric Design, R. E. Barnhill and R. F. Riesenfeld, Eds. Academic 
Press, New York, 1974, pp. 95-126.  Reprinted From acm Transactions On Graphics--April 1983--Vol. 2, 
No. 2 B. A. Barsky and J. C. Beatty 18. LANE, J.M. Shape operators for computer aided geometric design. 
Ph.D. dissertation, Univ. of Utah, Salt Lake City, Utah, June 1977. 19. NIELSON, G.M. Some piecewise 
polynomial alternatives to splines under tension. In Computer Aided Geometric Design, R. E. Barnhill 
and R. F. Riesenfeld, Eds. Academic Press, New York, 1974, pp. 209-235. 20. NIELSON, G.M. Computation 
of Nu-splines. Dept. of Mathematics, Arizona State Univ., Tempe, Ariz., June 1974. 21. PILCHER, D.T. 
Smooth approximation of parametric curves and surfaces. Ph.D. dissertation, Univ. of Uta.h, Salt Lake 
City, Utah, Aug. 1973. 22. RIESENFELD, R.F. Applications of B-spline approximation to geometric problems 
of computer- aided design. Ph.D. dissertation, Dept. of Systems and Information Science, Syracuse Univ., 
New York, May 1973. 23. SCHWEIKF.RT, D.G. An interpolation curve using a spline in tension. J. Math. 
Phys. 45 (1966), 312-317.  Received April 1983; accepted May 1983 Reprinted From acm Transactions On 
Graphics--April 1983--V01.2, No. 2  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801152</article_id>
		<sort_key>219</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>27</seq_no>
		<title><![CDATA[Topologically reliable display of algebraic curves]]></title>
		<page_from>219</page_from>
		<page_to>227</page_to>
		<doi_number>10.1145/800059.801152</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801152</url>
		<abstract>
			<par><![CDATA[<p>An algebraic curve is a set of points in the plane satisfying an equation <italic>F(x,y)</italic> &equil; 0, where <italic>F(x,y)</italic> is a polynomial in x and y with rational number coefficients. The topological structure of an algebraic curve can be complicated. It may, for example, have multiple components, isolated points, or intricate self-crossings. In the field of Computer Algebra (Symbolic Mathematical Computation), algorithms for exact computations on polynomials with rational number coefficients have been developed. In particular, the cylindrical algebraic decomposition (cad) algorithm of Computer Algebra determines the topological structure of an algebraic curve, given <italic>F(x,y)</italic> as input. We describe methods for algebraic curve display which, by making use of the cad algorithm, correctly portray the topological structure of the curve. The running times of our algorithms consist almost entirely of the time required for the cad algorithm, which varies from seconds to hours depending on the particular <italic>F(x,y)</italic>.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Algebraic curves]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.1.2</cat_node>
				<descriptor>Algebraic algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Line and curve generation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010148.10010149.10010150</concept_id>
				<concept_desc>CCS->Computing methodologies->Symbolic and algebraic manipulation->Symbolic and algebraic algorithms->Algebraic algorithms</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P65588</person_id>
				<author_profile_id><![CDATA[81100235064]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dennis]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Arnon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Purdue University, West Lafayette, Indiana, USA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1089271</ref_obj_id>
				<ref_obj_pid>1089270</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[DS Arnon, "Automatic analysis of real algebraic curves," SIGSAM Bulletin (of the Association for Computing Machinery)15, pp. 3-9 (1981).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>910085</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[DS Arnon, "Algorithms for the geometry of semialgebraic sets," Technical Report #436, Computer Science Dept., University of Wisconsin, Madison,Wisconsin(1981). (Ph.D. thesis)]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[DS Arnon, GE Collins, and S McCallum, "Cylindrical algebraic decomposition II: an adjacency algorithm for the plane," Technical Report CSD TR-428, Computer Science Dept., Purdue University(December, 1982).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>700247</ref_obj_id>
				<ref_obj_pid>646656</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DS Arnon and S McCallum, "Cylindrical algebraic decomposition by quantifier elimination," pp. 215-222 in Proceedings of the European Computer Algebra Conference (EUROCAM '82), ed. J Calmet,Lecture Notes in Computer Science, 144, Springer-Verlag, Berlin(1982).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[DS Arnon, GE Collins, and S McCallum, "Cylindrical algebraic decomposition I: the basic algorithm," Technical Report CSD TR-427, Computer Science Dept., Purdue University(December, 1982).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[GE Collins, "Computer algebra of polynomials and rational functions," American Mathematical Monthly80, pp. 725-755 (1973).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>697342</ref_obj_id>
				<ref_obj_pid>646589</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[GE Collins, "Quantifier elimination for real closed fields by cylindrical algebraic decomposition," pp. 134-163 in Proceedings of the Second GI Conference on Automata and Formal Languages, Lecture notes in Computer Science, 33, Springer-Verlag, Berlin(1975).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[GE Collins, "SAC-2 and ALDES now available," SIGSAM Bulletin (of the Association for Computing Machinery)14, p. 19 (1980).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801287</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[JT Kajiya, "Ray tracing parametric patches," Computer Graphics16, pp. 245-254 (1982).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[R Pavelle, M Rothstein, and J Fitch, "Computer algebra," Scientific American245, pp. 136-152 (1981).]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>538576</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[T Pavlidis, Algorithms for graphics and image processing, Computer Science Press, Rockville, Maryland(1982).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[DR Stoutemyer and DYY Yun, "Symbolic mathematical computation," in Encyclopedia of Computer Science and Technology, ed. J Belzer, AG Holzman, A Kent,Marcel Dekker(1980). (Supplement)]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 TOPOLOGICALLY RELIABLE DISPLAY OF ALGEBRAIC CURVES Dennis S. Arnon Computer Science Department Purdue 
University West Lafayette, Indiana, USA 47907 ABSTRACT An algebraic curve is a set of points in the 
plane satisfying an equation F(z,y) = 0, where F(x,y) is a polynomial in x and y with rational number 
coefficients. The topological structure of an algebraic curve can be complicated. It may, for example, 
have multiple components, isolated points, or intricate self-crossings. In the field of Computer Algebra 
(Symbolic Mathematical Computation), algorithms for exact computations on polynomials with rational number 
coefficients have been developed. In particular, the cylindrical algebraic decomposition (cad) algorithm 
of Computer Algebra determines the topological structure of an algebraic curve, given F(x,y) as input. 
We describe methods for algebraic curve display which, by making use of the cad algorithm, correctly 
portray the topological structure of the curve. The running times of our algo-rithms consist almost entirely 
of the time required for the cad algorithm, which varies from seconds to hours depend- ing on the particular 
F(x,y). CR Categories and Subject Descriptors: 1.33 [Com-puter Graphics]: Picture/Image Generation -Display 
algo- rithms; 1.1.2 [Algebraic Manipulation]: Algorithms -Alge-braic algorithms; 135 [Computer Graphics]: 
Computational Geometry and Object Modeling; 1.1.4 [Algebraic Manipula- tion]: Applications General Terms: 
Algorithms Additional Key Words and Phrases: Algebraic Curves 1. Introduction. An algebraic curve is 
a set of points in the plane satisfying an equation F(x,y)= 0, where F(x#) is a polynomial in x and y 
with rational number coefficients. The following are some examples: Example 1. YCx,Y)=y3- 3xy +x 3. 
 9 Figure 1 Permission to copy without fee all or part of this material is granted provided that the 
copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the 
title of the publication and its date appear, and notice is given that copying is by permission of the 
Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific 
permission. &#38;#169; ACM 0-89791-109-1/83/007/0219 $00.75 Example 2. F(x,y) =y2 +x 3 .i. x2. Figure 
2. Example 3. F(x~y) = 4), 4 + 17x2y 2 . 20y 2 + 4x 4 . 20x2 + 17.  These examples indicate some of 
the diversity of structure found among algebraic curves. Example 1 crosses itself (at the origin). Example 
2 has two connected components, one a curve and the other an isolated point (the origin). Exam-ple 3 
has four components, each a closed curve. By the term topological structure of a curve, we have in mind 
the properties of the curve which remain unchanged if the Cartesian plane in which the curve lies is, 
for example, rotated, bent, stretched, or compressed. Such transformations of the plane we will call 
continuous defor-mations of it. We may describe the topological structure of a curve in terms of finitely 
many vertices and edges, thought of as lying in the plane. Each edge either is incident with two vertices 
(which need not be distinct; the edge is then a loop) and hence is finite in extent, or is incident with 
exactly one vertex, and its other end is thought of as going to infinity. Edges may only meet at a vertex. 
The union of the vertices and edges must be a sub- set of the plane which, under some continuous deformation 
of the plane, can be transformed to the curve. The follow- ing is a collection of one vertex and three 
edges giving the topological structure of Example 1: Figure 3 Figure 4 A description of the above sort 
is a kind of graph; we call it a structure graph for the curve, or S-graph for short. An S-graph for 
a curve is not unique; given one we can get another by subdividing any edge into a vertex and two edges. 
Also, the specific Cartesian coordinates of the ver-tices of an S-graph, and of the particular points 
on its edges, are not important. It is enough to say how many vertices and edges there are, and which 
vertices each edge is incident with. Thus the following is as good an S-graph for Example 1 as the previous 
figure: Y Figure $ The diverse structures of Examples 1,2, and 3 are cap- tured by their S-graphs. Any 
S-graph for Example 2 has a vertex with no incident edges. Any S-graph for Example 3 must have four components. 
A standard definition of curve display is: "given a mathematical expression, identify the pixels that 
must be marked so that an image of the curve described by the expression is displayed" ([Pav82a], p. 
215). A picture of a curve produced by an actual display device is necessarily only an approximation 
to the true curve. In looking at a picture of a curve, one consciously or unconsciously dis- cerns some 
particular topological structure for the curve. Putting this another way, given a picture of a curve, 
one could get an S-graph from it, if asked to do so. For some algebraic curves, most any picture on most 
any display dev- ice is likely to be sufficient for the discernment of a correct S-graph for the curve. 
Lines, parabolas, and ellipses are examples of such curves. For more complex algebraic curves this may 
not be true. Even if we have somehow arranged for a display algorithm to deal with the specification 
of the curve in the implicit form F(x~v)= O, we may wonder if all components (especially isolated points) 
are shown, and whether such potential behavior as: Figure 6 is correctly distinguished from: Figure 7 
 In the present paper we report on three display algo- rithms which take a polynomial F(x,y) as input, 
and which are guaranteed to produce a topologically correct picture of the curve F(x,y) = 0. By a topologically 
correct picture we mean a picture in which one may readily discern a correct S-graph for the curve. In 
fact, the output of the first algo- rithm is just an S-graph. Our other two algorithms extend the first 
to produce increasingly accurate plots of the curve (in Cartesian coordinates), at corresponding increases 
in computing time. To reliably determine the topological structure of a curve we use the cylindrical 
algebraic decomposition (cad) algorithm of Computer Algebra, as described in Sections 2 and 3 below. 
Section 4 describes S-graph display. Section 5 describes our general strategy for accurately plotting 
the shape of a curve. Section 6 presents our second display algorithm, which we call numerical plotting. 
This algorithm does a relatively fast plot using a standard numerical ODE solver (e.g. Runge-Kutta) applied 
to the differential equa- tion: Fx r, where Fx and Fy denote the partial derivatives of F(x,y). The use 
of the ODE routine is controlled by the (previ-ously determined) S-graph for the curve, insuring that 
the final picture will correctly exhibit the S-graph. Our third display algorithm, which we call algebraic 
plotting, is slower, but is free from the effects of numerical instability. It uses Computer Algebra 
as much as possible, only con-verting to floating point numbers to pass the plotted points to a display 
device. Section 7 treats this algorithm, which has been previously abstracted in [Arn81a]. In Section 
8 we discuss the performance of our algorithms, and mention the possibility of developing similar algorithms 
for surface display. 2. Computer Algebra. Computer Algebra is only one of the names currently in use 
for the field concerned with exact, symbolic, computations on mathematical expressions. Poly- nomials 
in several variables with rational number coefficients are a fundamental class of expressions con-sidered. 
Other names used for the field are Symbolic Mathematical Computation, Algebraic Manipulation, and Formula 
Manipulation. Typical problems considered in the field are exact arithmetic of arbitrarily large integers, 
rational numbers, and symbolic matrices, polynomial fac-torization, symbolic integration, and simplification 
of mathematical expressions. Historically, there has been equal concern to develop new algorithms, and 
to imple-ment existing algorithms in large, publicly available systems. Surveys of the field are [Pavgla] 
and [Sto80a]. Corresponding to the issues of roundoff and trunca-tion error in numerical analysis, one 
has in Computer Alge- bra a persistent issue usually referred to as ~intermediate expression swelF. It 
is common for an algebraic computa- tion whose input and output are brief to cause enormous intermediate 
expressions to be generated. In the worst cases the time and space required for subsidiary computations 
are so great as to make the desired computation impossible. Our curve display algorithms do not escape 
this problem. One can easily find curves which have simple topological structure, but which will cause 
intermediate expression swell sufficient to exhaust the available computing resources. A family of such 
curves is x 2~ + yZ~ + 1 = 0, for sufficiently large positive integers n. These curves are all empty, 
but for increasing n the cad algorithm (see Section 3) will carry out increasingly large intermediate 
computa- tions. 3. Cylindrical algebraic decompositions. To determine the topological structure of an 
algebraic curve we utilize a major algorithm of Computer Algebra called the cylindrical algebraic decomposition 
algorithm [Co175a]. Given a bivari- ate polynomial F(x,y) with rational number coefficients, the cad 
algorithm finds a decomposition of the plane into points, arcs, and "patches of white space" such that 
certain of the points and arcs constitute an S-graph for the curve defined by F (x ~v ) = 0. For example, 
if F (x ~y ) = y3 - 3xy + x 3 as in Example 1, the cad algorithm would produce the fol- lowing decomposition 
of the plane: Figure 8 Clearly from this cad we may extract an S-graph for the curve consisting of three 
vertices and five edges. The cad algorithm was first implemented in 1981, in the SAC-2 Computer Algebra 
system [Col80a, ArnSlb]. It requires as subalgorithms a number of standard Computer Algebra algorithms 
which fall under the general heading "algebra of polynomials'. The basic capability needed is exact arithmetic 
on arbitrary length integers, which is implemented with list processing. Building on this are algorithms 
for exact arithmetic on rational numbers of arbi- trary size, followed by algorithms for exact arithmetic 
on multivariate polynomials with rational number coefficients, for polynomial greatest common divisors, 
and for polyno- mial factorization. These various algorithms are described in detail in [Co173a]. Because 
of the many elements and specialized theory of the cad algorithm, we do not attempt to further describe 
it here. We refer the reader to [Arn82a,Arn82b, Arn82c], and [Co175a] for further information. It is 
interesting to note that resultant calculation, apparently used for the first time in Computer Graphics 
in [Kaj82a], is an important part of the cad algorithm. 4. Structure graph display. Clearly. a description 
of an S-graph can be simple indeed. One need only specify how many vertices and edges there are, and 
specify incidences in such terms as "edge #2 is incident with vertices #5 and #9". It is convenient, 
however, for the cad algorithm to produce a slightly different description of the S-graph which has the 
advantage of giving one an idea of the actual shape of the curve. As suggested by Figure 8, the subsets 
of the plane which make up a cad (these subsets are called the cells of the cad) can be arranged into 
vertical strips. The cells which make up a particular strip are "stacked on top of each other'. Thus 
to each cell we can assign an index consisting of two positive-integer components. We number the vertical 
strips from left to right, and define the first component of a cell's index to be the number of the vertical 
strip in which it lies. Within each vertical strip, we number the cells from bottom to top, and define 
the second component of a cell's index to be its number with respect to this ordering. Thus the indices 
of the cells in the cad of Figure 8 are (1,1), (1,2), (1,3), (2,1), (2,2), (2,3), (3,1), (3,2), (3,3), 
(3,4), (3,5), (3,6), (3,7), (4,1), (4,2), (4,3), (4,4), (4,5), (5,1), (5,2), and (5,3). Our algorithm 
describes an S-graph as a collection of the following three kinds of statements: "edge (3,2) is incident 
with vertices (2,2) and (4,6)', "edge (1,2) is incident with vertex (2,2) only', and "vertex (4,4) is 
an iso- lated point'. Thus from the cad of Figure 8, we get the fol- lowing description of the S-graph 
for the curve y3. 3xy +x3 =0: edge (1,2) is incident with vertex (2,2) only edge (3,2) is incident with 
vertices (2,2) and (4,2) edge (3,4) is incident with vertices (2,2) and (4,4) edge (3,6) is incident 
with vertices (2,2) and (4,4) edge (5,2) is incident with vertex (4,2) only We can gain some knowledge 
of the shape of the curve by just diagramming this information:  Figure 9 $. Accurate shape portrayal: 
general strategy. The output of our second and third display algorithms will be a collection of sequences 
of points in the plane. Each point in such a sequence will consist of a floating-point x-coordinate and 
y-coordinate; e.g. "-3.256 12.621" is such a point. The sequences of points will be fed to a display 
device which, for each sequence, will plot the points in the sequence and connect consecutive points 
with line segments. In our case, the sequences of points are passed to the "graph" command of the UNIXTM 
operating system running on a VAXTM 11/780 computer, driving a Versatec TM printer/plotter. The output 
of the cad algorithm includes information (the so-called sample points for cells) which enables us to 
construct arbitrarily accurate floating-point approximations to the x- and y-coordinates of each point 
that is to be a vertex of the S-graph (see [Arn82a] and [Co175a] ). The cad output also enables us to 
construct arbitrarily accurate floating-point approximations to as many points as desired on each edge 
of the S-graph (using the so-called defining formulas for cells). The actual construction of such approx- 
imations is made possible by algebraic algorithms for approximating the real roots of a polynomial to 
any accu-racy desired accuracy (see e.g [Arn81b]. We use the abcve properties of cad output in the following 
scheme for mak- ing up sequences of points. For every edge of the S-graph, we make a preliminary sequence 
consisting of closely spaced, consecutive floating- point points, each of which lies close to an actual 
point on the edge. The user controls the horizontal spacing of the points. For edges incident with two 
vertices, the first point of the preliminary sequence lies close to one vertex, and the last point lies 
close to the other. For edges incident with only one vertex, the first point of the preliminary sequence 
lies close to that vertex, and the remaining points ~go out towards infinity" along the edge for some 
distance specified by the user. With the algebraic plotting algorithm of Section 7 the user can specify 
a limit to the distance between each constructed point and a true edge point, hence with algebraic plotting 
the shape of the curve can be portrayed as accurately as desired. With the numerical plotting algorithm 
of Section 6 one cannot specify such a limit, but judicious use of the numerical differential equa- tion 
solver can keep the approximation error small. Regardless of how large the approximation error becomes, 
the following precaution taken for both plotting methods suffices to insure that the final picture will 
show correct topological structure. We check each constructed point of each sequence to insure that the 
line segments joining the points of any one sequence will be disjoint from the line segments joining 
the points of every other sequence. Having constructed a preliminary sequence for each edge of the S-graph, 
we use the ability to construct approx- imations to vertices, a~d the known incidences of the S- graph, 
to make final sequences. First, we construct floating-point approximations to each vertex. Next, when- 
ever an edge is incident with a particular vertex, we make (the approximation to) that vertex the new 
initial or final point, whichever is appropriate, of the sequence for that edge. Finally, we make each 
vertex which is not incident with any edge into a sequence consisting of one point, and set a flag to 
have that point tagged with some easily visible character (we have arbitrarily selected asterisk) by 
the display device. In this way the viewer of the final picture will be unmistakably informed of isolated 
points. 6. Accurate shape portrayal with numerical edge plotting. Given a curve F(x,y)=0, by the Implicit 
Function Theorem we have the ordinary differential equation F, F, It is a property of cad's that if c 
is a cell of a cad which is an edge of an S-graph, then Fy(x,y) is nonzero at every point (x,y) of c 
(see [Arn82a] or [Co175a] ). Thus we can use a numerical ordinary differential equation solver (e.g. 
Runge-Kutta) to plot points of c "from left to right" and obtain one of the preliminary sequences defined 
in Section 5. We do this as follows. As mentioned in Section 5, we use the output of the cad algorithm 
to compute an (approx- imation to) a particular point of c "near the lefthand end" of c. The user specifies 
a stepsize, and we then run the ODE solver up to a point "near the righthand end" of c. Numerical effects 
can significantly distort the shape of the curve, as the following display of y3.3xy + x 3 = 0 shows: 
Figure 10 We remark that the SAC-2 Computer Algebra system is implemented in Fortran, so it is not difficult 
to mesh the algebraic and numerical algorithms. 7. Accurate shape portrayal with algebraic edge plotting. 
Alge-braic edge plotting has the same general plan as the numer- ical plotting described in the last 
section, namely an edge c is plotted from left to right, using a stepsize specified by the user. But 
to do the plotting we use the defining for- mula for the cell constructed by the cad algorithm. This 
formula is a boolean combination of polynomial equations and inequalities which precisely defines the 
points in the plane belonging to the edge. For example, the defining for- mula for cell (3,2) in the 
cad of Figure 8 is: ~t'(x,y) =(x > O) &#38; (x3-4 < O) &#38; (x-2 < O) &#38; O' < O) &#38; (y2-x > 0) 
&#38; (y3-3~ +x3=0). AS in this example, the formula for a cell which is an edge in an S-graph always 
has the form 0(x~v) &#38; F(x~y)= O. Since the initial x-value is a rational number, and the step* size 
is a rational number, for every x-value a at which we wish to compute a point (a,b) of c, a is a rational 
number. Hence we can evaluate 0(a~v)=4~(y) and F(a,y) exactly (using Computer Algebra). We construct 
the exact real roots of F(a~y) as algebraic numbers (see [Arn82a] or [Co175a]), and evaluate 4,(y) at 
each. There will be exactly one root which satisfies 4,, and thus we have found a point (a ,b) on c, 
which we may approximate to any desired preci- sion. Clearly a smaller step size and more accurate approx- 
imation in the above process will produce a more accurate rendering of the shape of c, but require more 
time. The algorithm used in this section was used to pro- duce Figures 1,2, and 3 (the actual plots shown 
are copies made by a draftsman). 8. Conclusions. The times for Examples 1,2, and 3 serve to illustrate 
the performance of our display algorithms. The total CPU time for Example 1 was about one and a half 
minutes (all times are on a VAXTM 11/780 computer run-ning the UNIXTM operating system driving a VersatecTM 
printer/plotter). Example 2 took about four minutes, and Example 3 about eight minutes. Example 3 is 
a curve of degree four; it is not difficult to find degree four curves that will cause the cad algorithm 
to run for several hours. As mentioned in Section 2, curves of sufficiently high degree will cause the 
cad algorithm to exhaust available resources. Although it has been established that the com- puting time 
of the cad algorithm is polynomial in the degree of F (x,y) [Co175a], its expected performance is still 
poorly understood. The cad algorithm is applicable to polynomials in any number of variables. The cad-based 
approach we have used for curve display can be extended to surface display. Acknowledgement We thank 
Professor J. Rice for alerting us to the equation y, = - F~ / Fy. References Arn81a. DS Arnon, "Automatic 
analysis of real algebraic curves," SIGSAM Bulletin (of the Association for Com-puting Machinery) 15, 
pp. 3-9 (1981). Arn81b. DS Arnon, "Algorithms for the geometry of semi-algebraic sets," Technical Report 
#436, Computer Science Dept., University of Wisconsin, Madison, Wisconsin(1981). (Ph.D. thesis) Arn82b. 
DS Arnon, GE Collins, and S McCallum, "Cylindrical algebraic decomposition II: an adjacency algorithm 
for the plane," Technical Report CSD TR-428, Computer Science Dept., Purdue University(December, 1982). 
Arn82c. DS Arnon and S McCallum, "Cylindrical algebraic decomposition by quantifier elimination," pp. 
215-222 in Proceedings of the European Computer Algebra Conference (EUROCAM '82), ed. J Calmet ,Lecture 
Notes in Computer Science, 144, Springer-Verlag, Berlin(1982). Arn82a. DS Arnon, GE Collins, and S McCallum, 
"Cylindrical algebraic decomposition I: the basic algorithm," Technical Report CSD TR-427, Computer Science 
Dept., Purdue University(December, 1982). Co173a. GE Collins, "Computer algebra of polynomials and rational 
functions," AmeFican Mathematical Monthly 80, pp. 725-755 (1973). Co175a. GE Collins, "Quantifier elimination 
for real closed fields by cylindrical algebraic decomposition," pp. 134- 163 in Proceedings of the Second 
GI Conference on Auto- mata and Formal Languages, Lecture notes in Com-puter Science, 33, Springer-Verlag, 
Berlin(1975). Col80a. GE Collins, "SAC-2 and ALDES now available," SIGSAM Bulletin (of the Association 
for Computing Machinery) 14, p. 19 (1980). Kaj82a. JT Kajiya, "Ray tracing parametric patches," Computer 
Graphics 16, pp. 245-254 (1982). Pav81a. R Pavelle, M Rothstein, and J Fitch, "Computer alge- bra," Scientific 
American 245, pp. 136-152 (1981). Pav82a. T Pavlidis, Algorithms for graphics and image process- ing, 
Computer Science Press, Rockville, Mary-land(1982). Sto80a. DR Stoutemyer and DYY Yun, "Symbolic mathemati- 
cal computation," in Encyclopedia of Computer Science and Technology, ed. J Belzer, AG Holzman, A Kent 
,Marcel Dekker(1980). (Supplement) 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801153</article_id>
		<sort_key>229</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>28</seq_no>
		<title><![CDATA[Curve-fitting with piecewise parametric cubics]]></title>
		<page_from>229</page_from>
		<page_to>239</page_to>
		<doi_number>10.1145/800059.801153</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801153</url>
		<abstract>
			<par><![CDATA[<p>Parametric piecewise-cubic functions are used throughout the computer graphics industry to represent curved shapes. For many applications, it would be useful to be able to reliably derive this representation from a closely spaced set of points that approximate the desired curve, such as the input from a digitizing tablet or a scanner. This paper presents a solution to the problem of automatically generating efficient piecewise parametric cubic polynomial approximations to shapes from sampled data. We have developed an algorithm that takes a set of sample points, plus optional endpoint and tangent vector specifications, and iteratively derives a single parametric cubic polynomial that lies close to the data points as defined by an error metric based on least-squares. Combining this algorithm with dynamic programming techniques to determine the knot placement gives good results over a range of shapes and applications.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor>Least squares approximation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor>Spline and piecewise polynomial approximation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.8</cat_node>
				<descriptor>Dynamic programming</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10003809.10011254.10011258</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Algorithm design techniques->Dynamic programming</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003736.10003737</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Functional analysis->Approximation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003720</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on polynomials</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003636</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Approximation algorithms analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003736.10003737</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Functional analysis->Approximation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP40036714</person_id>
				<author_profile_id><![CDATA[81332521410]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Plass]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMAGINE SCIENCES LABORATORY, Xerox Palo Alto Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31095950</person_id>
				<author_profile_id><![CDATA[81100388123]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Maureen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stone]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMAGINE SCIENCES LABORATORY, Xerox Palo Alto Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>578775</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Aho, A.V., Hopcroft, J.E. and Ullman, J.D., The Design and Analysis of Computer Algorithms, Addison-Wesley, Reading, Massachusetts, 1974.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578776</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Barnhill, R.E. and Riesenfeld, R.F., eds., Computer Aided Geometric Design, Academic Press, New York, 1974.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807510</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Baudelaire, P. and Stone, M., "Techniques for Interactive Raster Graphics." Computer Graphics 14, 3 (1980), 302-307.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Baudelaire, P.,The Draw User's Manual, internal report. Xerox Palo Alto Research Center, Palo Alto, CA, 1978.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Baudelaire, P., The Fred User's Manual, internal report. Xerox Palo Alto Research Center, Palo Alto, CA, 1976.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>862270</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Bellman, R., Dynamic Programming, University Press, Princeton, N.J., 1957.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>355877</ref_obj_id>
				<ref_obj_pid>355873</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Chung, W.L., "Automatic Curve Fitting Using and Adaptive Local Algorithm." ACM Transactions on Mathematical Software 6, 1 (1980), 45-57.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578374</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Conte, S.D., and de Boor, C., Elementary Numerical Analysis: An Algorithmic Approach, second edition, McGraw-Hill, New York, 1972.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Cox, M. G., "Curve Fitting with Piecewise Polynomials." J. Inst. Maths Applications 8, (1971), 36-52.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[de Boor, C., A Practical Guide to Splines, Springer-Verlag, 1978.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Dierckx, P., "Algorithms for Smoothing Data with Periodic and Parametric Splines." Computer Graphics and Image Processing 20, (1982), 171-184.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578513</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Faux, I.D., and Pratt, M.J., Computational Geometry for Design and Manufacture, Ellis Horwood, 1979.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Flegal, R., private communication. Xerox Palo Alto Research Center, Palo Alto, CA, 1978.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>355737</ref_obj_id>
				<ref_obj_pid>355732</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Ichida, K and Yoshimoto, F., "Curve Fitting by a One-Pass Method with a Piecewise Cubic Polynomial." ACM Transactions on Mathematical Software 3, 2 (1977), 164-174.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1096882</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Knuth, D.E., TEX and Metafont: New Directions in Typesetting, Digital Press and American Mathematical Society, 1979.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801270</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Lipkie, D.E., Evans, S.R., Newlin, J.K., and Weissman, R.L., "Star Graphics: An Object-Oriented Implementation." Computer Graphics 16, 3 (1982), 115-124.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Manning, J. R., "Continuity Conditions for Spline Curves." Computer Journal 17, (1974), 181-186.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Nielson, G. M. "Some Piecewise Polynomial Alternatives to Splines in Tension" in Computer Aided Geometric Design, Barnhill, R.E. and Riesenfeld, R.F., eds., Academic Press, New York, 1974.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>538576</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Pavlidis, T., Algorithms for Graphics and Image Processing, Computer Science Press, 1982.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>909883</ref_obj_id>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Reeves, W. T., "Quantitative Representation of Complex Dynamic Shapes for Motion Analysis." PhD Thesis, Department of Computer Science, University of Toronto, 1980.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Rice, J. R., The Approximation Of Functions, vols. 1 and 2, Addison-Wesley, Reading, Mass., 1964.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>355777</ref_obj_id>
				<ref_obj_pid>355769</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Rice, J. R., Algorithm 525. ADAPT, adaptive smooth curve fitting, ACM Transactions on Mathematical Software 4, 1 (1978) 82-94.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>63448</ref_obj_id>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Rogers, D. F., and Adams, J.A., Mathematical Elements for Computer Graphics, McGraw-Hill, New York, 1976.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Schultz, M., Spline Analysis, Prentice-Hall, Englewood Cliffs, N.J., 1973.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Schumaker, Larry L., Spline Functions: Basic Theory, John Wiley &amp; Sons, New York, 1981.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Stone, M., The Griffin User's Manual, internal report. Xerox Palo Alto Research Center, Palo Alto, CA, 1979.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801297</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Warnock, J. and Wyatt, D., "A Device Independent Graphics Imaging Model for use with Raster Devices." Computer Graphics 16, 3 (1982), 313-320.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>359848</ref_obj_id>
				<ref_obj_pid>359842</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Wu, S., Abel, J.F., and Greenburg, D.P., "An Interactive Computer Graphics Approach to Surface Representation." CACM 20, 10 (1977), 703-712.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Yamaguchi, F., "A New Curve Fitting Method Using a CRT Computer Display." Computer Graphics and Image Processing, 7 (1978), 425-437.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Curve-Fitting with Piecewise Parametric Cubics Michael Plass and Maureen Stone IMAGING SCIENCES LABORATORY 
Xerox Palo Alto Research Center Abstract: Parametric piecewise-cubic functions are used throughout the 
computer graphics industry to represent curved shapes. For many applications, it would be useful to be 
able to reliably derive this representation from. a closely spaced set of points that approximate the 
desired curve, such as the input from a digitizing tablet or a scanner. This paper presents a solution 
to the problem of automatically generating efficient piecewise parametric cubic polynomial approximations 
to shapes from sampled data. We have developed an algorithm that takes a set of sample points, plus optional 
endpoint and tangent vector specifications, and iteratively derives a single parametric cubic polynomial 
that lies close to the data points as defined by an error metric based on least-squares. Combining this 
algorithm with dynamic programming techniques to determine the knot placement gives good results over 
a range of shapes and applications. CR Categories and Subject Descriptors: G.1.2 [Numer- ical Analysis]: 
Approximation-Least squares approxima-tion; Spline and piecewise polynomial approximation; G. 1.5 [Numerical 
Analysis]: Roots of Nonlinear Equations~tera- tive methods; 1.2.8 [Artificial Intelligence]: Problem 
Solving, Control Methods and Search-Dynamic programming; 1.3.3 [Computer Graphics]: Picture/Image Generation-Digitiz-ing 
and scanning; 1.3.5 [Computer Graphics]: Computa-tional Geometry and Object Modeling-Curve, surface, 
sol- id, and object representations General Terms: Algorithms Permission to copy without fee all or 
part of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. &#38;#169; ACM 0-89791-109-1/83/007/0229 
$00.75 1. Introduction. We are interested in the problems that arise when try- ing to use two-dimensional 
curved shapes in an interac-tive design environment. In particular, we are interested in finding the 
best ways for a designer to define such shapes. Our immediate focus is the production of publication-qual- 
ity documents. The documents are designed using an in-teractive system with a raster display, then camera-ready 
copy is digitally produced at very high resolutions. Our design methodology is to represent all shapes 
analytically using piecewise parametric cubic polynomials [27]. There are many published methods for 
specifying such curves [2,12,15,23]. These methods all have in common the ap-proach that the shape designer 
provides a small set of con- trolling points and parameters which act as handles to shape the curve. 
At Xerox PARC such algorithms, and similar ones for conics, have been used in design systems for making 
illustrations [4,3,16,26] and digital typefaces [5]. We conclude that it would be a powerful addition 
to the set of tools for specifying shapes to be able to reliably derive an efficient piecewise parametric 
cubic polynomial repre-sentation from a sequence of closely-spaced points that ap- proximate the desired 
curve. The points could be produced in a variety of ways such as sketching with a digitizing pen, scanning 
and finding edges in an existing drawing, or writing a program to compute a set of data points. In this 
paper we will present a method deriving such an analytical representation from digitized data. For the 
data in our curve-fitting work we have experi- mented with hand-tuned digital curves, optically-scanned 
images and simple hand-drawn sketches. We have found that the source of the digitized curve is relatively 
unim- portant. The differences principally affect how tightly you want to follow the data; is it noisy, 
does it have pronounced rastering effects, are corners well defined? A scanned image will not have as 
sharp corners as a hand-tuned bitmap, for example. Also, the tolerance is a function of the image resolution. 
We feel our results are generally applicable to the problem of generating piecewise approximations to 
shapes from an ordered set of discrete data points. The heart of our method is an algorithm that takes 
a set of sample points and derives a parametric cubic curve that lies close to the data points; this 
algorithm is presented in section 4. A more elaborate version of the algorithm, presented in section 
5, allows the endpoints and/or the tangent directions at the endpoints of the piece to be specified. 
A piecewise approximation to a shape consists of a number of single cubic pieces connected end-to-end. 
The location and smoothness of the joints or knots between the pieces is critical to getting a good representation 
of the shape. Sometimes we want a smooth connection between pieces, and sometimes we want to produce 
a sharp "corner." If we somehow knew in advance the position of the knots, and the tangent direction 
at each non-corner knot, we could simply apply the single-piece fitting algorithm to the points in each 
span, yielding a piecewise approximation. Finding the best knot positions and tangents, however, is no 
easy task. Our most promising results were obtained by setting tangents using fairly local information, 
and then using dynamic programming to search the space of potential knot positions. The single-piece 
fitting routine is used in several places in the overall process. In the dynamic programming phase it 
is used as a way of measuring how well a span of sample points may be fit with a single cubic piece, 
and, after the knots have been found, it is used to find the piecewise approximation. The algorithm may 
also be used in the initial phase to compute tangent directions, by fitting a range of samples around 
the point of interest and finding the tangent of the resulting curve. This method works well for estimating 
tangent directions in noisy data. In this paper we want to emphasize our paramet-ric cubic curve-fitting 
algorithm and the use of dynamic programming to find the knots. While it is important to accurately find 
corners and tangents, we feel the choice of algorithms to do so is highly application dependent and we 
are not in a position to give a complete analysis here. The next section of this paper is a general discussion 
of the problem of fitting shapes with piecewise functions. In section 3 we describe how we use dynamic 
programming to find the knot positions. Sections 4 and 5 will present in detail our algorithm for coml~uting 
a single paramet- ric cubic given a set of data points. We conclude with a summary and examples of our 
results. 2. Pieeewise function approximation to shapes. What do we mean by the best piecewise approximation 
to a shape such as Figure l(a)? The curve must lie near the data points, of course, and the representation 
will be most efficient if we use as few pieces as possible. This presents an obvious trade-off between 
a tight fit and an efficient representation. Also, the desired tightness of the fit will vary with the 
amount of noise in the data. Usually the curve should be smooth, and the pieces of the curve should fit 
smoothly together. In some cases, however, we find that the shape will look better if we introduce sharp 
bends at locations that are perceived as "corners". There are other issues of perception as well. For 
example, if the samples lie on a straight line, most users will object if the approximating curve is 
slightly wavy even if it lies quite close to the sample points. In this paper we will parameterize the 
problem as follows: The closeness of the fit is specified by a user-defined tolerance; it is the user's 
job to consider the resolution and accuracy of the input when specifying the tolerance. The fit is smooth 
if the direction of the tangent vector is continuous. Restricting only the direction of this vector to 
maintain smoothness gives us the maximum flexibility for two-dimensional curves. We use one of a number 
of algorithms to find the corners before finding any of the other knot locations. These methods all have 
the property that the user sets some parameters to tune the algorithm for a particular set of data. If 
a point is identified as a corner, we put a knot at that location and simply do not force tangent continuity 
there. While a parametric cubic is flexible enough to have a sharp cusp in it, the behavior of the curve 
near the cusp is quite constrained so we generally do not try to model corners with cusps. Figure 1 is 
an example of the process we use to fit a shape. The original data, which in this case is derived from 
a high resolution bitmap, is shown in figure l(a). We need to limit the choice of knot locations to a 
finite set so we can use a finite search technique like dynamic programming. This set is defined to be 
the set of sample points. For efficiency, we want to reduce the number of possible knot locations even 
further. In this example we restrict the potential knot locations to the vertices of a polygon fit to 
the sample points. Any vertex with an angle sharper than 135  is taken to be a corner. For this kind 
of data we usually filter the samples between corners to remove the more pronounced rastering effects. 
After filtering, the tangent vectors are set on each potential knot that is not a corner. Figure l(b) 
shows the unfiltered samples, the potential knots, and the normal vectors to the tangents at each potential 
knot. If the potential knot is a corner, it has no tangent set and is marked with a cross. To calculate 
the tangents in this example we used the curve-fitting algorithm to fit a small number of points around 
each potential knot and determined the tangent to the resulting curve at the point nearest the potential 
knot. The computed curved shape is shown in figure l(c). Figure l(d) shows the result in more detail. 
The small dots are the samples, and the large dots are the knots that were actually chosen. Further examples 
of shapes fit with our techniques are included in the conclusions.  3. Choosing the knots. Before discussing 
methods for choosing knots, let us look first at the problem of predicting whether a set of samples can 
be closely fit by a single piece of the curve. A parametric cubic polynomial is quite flexible, but has 
some obvious limits. For example, it can contain at most one loop or, if it has no loop, at most two 
inflection points. It is tempting, therefore, to try to derive a set of algorithms that measure space-curve 
characteristics such as slope and curvature and use this information to set the knot positions. However, 
as you add constraints to guarantee continuity, the degrees of freedom available to fit the cubic to 
the samples are reduced. Furthermore, the fit is very vulnerable  ================================================ 
 !" i i i~ X........................................ X ............ X X '>< "i: i. .---v-- ... )<.. 
 / ........ '/ .........I ........ 1 .......... I ......... ::t .......... xx  (a): Original data, 
approximately 150 by 160 bits. (b): Samples, potential knots and normal vectors. (c): Result of curve 
fitting algorithm, 14 cubic pieces. (d): Samples, curve and knots. Figure 1. An example showing the 
steps involved in and the corresponding normal vectors. Potential knots fitting a parametric cubic to 
a sampled shape. Figure (a) marked with a cross are corners. Figure (c) is the final is the original 
data taken from a high resolution bitmap. shape. Figure (d) shows the curve, knots and samples in Figure 
(b) shows the sample points, the potential knots more detail. to a poorly chosen continuity constraint 
such as a bad tangent vector. The only reliable way we have found to tell if a set of samples can be 
fit by a single cubic with constraints is to try it. Given that we are going to pay the cost of trying 
out different cubic pieces, one simple method for defining the knots is to "grow" the pieces out until 
the fit for that piece exceeds some threshold. In other words, starting with the previous knot, keep 
adding data points until the piece is as long as possible. Each new piece must be constrained to maintain 
continuity. This works, in the sense that the resulting curve will fit everywhere within the specified 
tol- erance, but is quite vulnerable to local phenomena such as noisy data or a badly defined tangent. 
We found this method works only moderately well in terms of the total number of knots in the final solution 
especially when trying to maintain tangent continuity. Subdivision is another obvious choice which has 
the same problem. These two techniques are also described by Reeves [20]. If we want to maintain continuity, 
the globally-best set of knots cannot be found by a method that only optimizes locally. For example, 
if the first and last sample points are constrained to lie on the curve, the error in fitting a para- 
metric cubic piece is not a monotone function of the number of samples; it may be possible to get a better 
fit by using a longer run of samples with better endpoint locations. This is especially true if the data 
is noisy. In theory, to find the best solution we could try all possible arrangements of knots and then 
take the best fit. This solution is exponential in the number of data points, and is clearly out of the 
question for any useful cases. It is possible, however, to reduce the complexity to O(n 3) by using dynamic 
programming. Dynamic programming [1,6] is a method for efficiently searching a solution space where the 
problem can be recur- sively broken up into a set of subproblems. Since the same subproblems contribute 
to the solution of several larger sub- problems, the total number of computations can be reduced by storing 
the results of each subproblem. We can apply this method to finding the knots as follows: Let eij be 
the error obtained when a single piece is fit to samples i through j. The exact definition of eij does 
not matter insofar as the dynamic programming algorithm is concerned, and we have obtained good results 
with several different ones. One reasonable choice for eij is simply the sum of the squares of the distances 
from the sample points to the curve, plus a constant T > 0. Increasing the value of v will encourage 
fewer knots, at the expense of not fitting the samples as closely. The total error for a given arrangement 
of knots is just the sum of the errors for the individual pieces. Let Eij be the least total error over 
all possible arrangements of knots for samples i thru j. This can be computed as Eij -~ min(Eek -{- ekj), 
i < k < j, where we have already computed and tabulated the values for Eik for i < k < j --1. We can 
compute all the values for ekj by working backwards and fitting single pieces from samples j to k for 
all values of k in the range. Even using dynamic programming it is very expensive to test all possible 
subranges for the best fit. We can do several things to prune the search space and improve the performance 
at the cost of no longer guaranteeing the optimum solution. For example, it is usually true that for 
each piece the shape of the curve distant from the area of interest will contribute little to the local 
best solution. Therefore, we limit how far back along the curve we search when computing Ekj above. As 
in the example of Figure 1, we generally use a subset of the sample points as potential knot positions 
to further reduce the cost. The main contribution of dynamic programming to this algorithm is a way of 
searching in a more global manner for the best solution set of knots. However, one of the principal advantages 
of piecewise functions is that they are controlled mostly by local information. Said another way, for 
a particular subsection of the curve, the best locations for the knots is not going to be affected much 
by the shape of the curve far away. So it may be possible to construct a more efficient algorithm than 
ours that looks only in a small region of the curve for each knot location. What our experience indicates 
is that it is important to do some searching to give immunity from local aberrations. The next two sections 
will describe our algorithm for fitting a parametric cubic polynomial to a set of data points. 4. Least-squares 
curve fitting with a parametric cubic polynomial. For computer graphics applications, the common spec- 
ification of a parametric cubic polynomial is F(X(t), Y(t)), where X and Y are cubic polynomials and 
t lies in the range 0 to 1. Such a curve can be thought of as a three- dimensional curve in the space 
X,Y, t. The curve is con- strained such that the projections on the X-t and Y-t planes are cubic polynomials. 
What is usually displayed is the projection of this curve on the X-Y plane. Our problem is: given a set 
of points on the X-Y plane, find the 3-space curve whose X-Y projection is such that the sum of the squares 
of the distances between the points and the projected curve is minimized. Note that there is not a unique 
solution to this problem. A simple example of this is that if X is a scalar multiple of Y, the resulting 
projected curve is a straight line, and many different (X, Y) pairs will yield the same line. Let us 
look at this problem a bit differently. We are given a set of data points. If we knew the parametric 
curve that gave the best fit, we could find the points on the projected curve that were nearest each 
of the data points. If we assigned the corresponding value of t to the data point, we are in effect positioning 
the data points near the curve in X,Y, t space. The interesting thing about this representation is that 
if we consider the projection of the points and curve on, for example, the X-t plane, we have the cubic 
polynomial that minimizes the sum of the squares of the distances to the points in X. This means if we 
only knew the correct t values for our data points, we could solve for our curve by performing simple 
least-squares fitting in X and Y independently. Our algorithm is an iterative technique that converges 
to this set of t values. Our algorithm is simple to state: Assign an initial approximation to t for 
each data point. Solve X(t) and Y(t) independently using conventional least-squares techniques. Measure 
the result, and if necessary, adjust the t values for the data points and repeat. This is a fixed-point 
iteration that, if it converges, will converge to a local minimum for our non-linear system. For the 
class of curves we are using it usually does converge and gives a good solution. We use an initial approximation 
based on the Euclidean distance between the data points in the X-Y plane. That is, let k--1 $ s~ = ~ 
~/(~,+1 -~)~ + (y~+l -y~)~, (1) i=1 be the total length of the polygonal segment connecting points 1 
through k. We set the initial value of tk be sk/sn. To solve for X(t) and Y(t), simple least-squares 
curve- fitting [8] will be fine for this problem as stated. We will defer any further discussion of this 
part of the algorithm until section 5, where we will extend this technique so that it is possible to 
specify endpoint and tangent conditions for the final curve. Given X and Y, we adjust the ti values by 
finding for each data point the point on the curve closest to it. We then assign this new t value to 
the data point. In X, Y, t space, we are moving the data point closer to the current curve, changing 
only the value for t. We stop iterating when the values for t no longer change significantly, or when 
some other condition is satisfied; for example, if the maximum distance from the points to the curve 
falls below some threshold. We also set an upper bound on the number of iterations. The square of the 
distance between a given point (x, y) and any point (X(t), Y(t)) that lies on the curve is (X(t) -- x) 
2 --[- (Y(t) --y)2 (2) To find where this is minimum we differentiate and equate to zero, yielding 2(X(t) 
--x)X'(t) J- 2(Y(t) --y)Y'(t) = 0 (3) The left side of this equation is a fifth-degree polynomial in 
t. Since we have an approximation to the desired root, the previous value, we can use Newton-Raphson 
iteration to find the root. The formula for Newton-Raphson iteration for solving f(t) = 0 is t ~-t f(t) 
(4) f,(t) so each iteration should decrease t by the amount (X(t) --x)X'(t) + (Y(t) --y)Y'(t) x,(t)~ 
+ g,(t) 2 + (x(t) -~)x,,(t) + (g(t) -y)y,,(t) (5) Because Newton-Raphson iteration converges quickly, 
only a few steps are needed to find a close approximation to the root. In fact, the algorithm performs 
well when only one iteration step is used to make the adjustment. After all the ti have been adjusted, 
it is important that they still lie in the range 0 to 1. Therefore, we linearly scale the values before 
each re-calculation of X and Y. Figure 2 shows an example of the algorithm in opera- tion. The small 
dots mark the sample points, and lines have been drawn from each data point (xi, Yi) to the correspond- 
ing point (X(ti), Y(ti)) on the curve. When the algorithm has converged, it can be seen that the point 
(X(ti),Y(ti)) is the closest point on the curve to (xi, Yi)- 5. Handling continuity constraints. The 
algorithm as stated in the previous section is not suitable for fitting smooth, continuous, piecewise 
functions because we cannot easily constrain the endpoints and tan- gents for each cubic piece. To understand 
how to extend this method, we must look at least-squares curve fitting in some more detail. Initial fit. 
1 iteration. 10 iterations. 100 iterations. Figure 2. This figure shows several iterations of the A portion 
of the curve outside the region 0 ~ t ~ 1 is curve-fitting algorithm. The sample points are shown as 
included give a better feel for the overall shape of the small dots, which are connected by straight 
lines to the cubic. points on the curve with the corresponding value of t. Fitting a polynomial of one 
variable using least squares curve fitting means the following: given a set of data points {(xi, yi), 
1 < i < n}, find the polynomial p(x) that minimizes the sum // ~(p(~) -y,)~ (6) i=l  This problem has 
a unique solution that can be derived as follows. Let the polynomial p(x) be expressed as a linear combination 
of polynomial basis functions d E ajCj(x) (7) j=0 The problem at hand is to determine A ----(ao, al,.., 
ad) such that the function n d )2 S=i~l(j~oaiCj(xi)--yi_ _ (8) is minimized. To do this we take the 
partial derivatives of S with respect to each value of A and set them to zero to obtain a set of d Q- 
1 linear equations. The partial derivatives of S are given by o ) Oak i=l -- and by setting these to 
zero and rearranging, we obtain the linear system E aj Cj(xi)k(xi) = yiCk(xi), 0 < k < d. j=0 i=1 i=1 
(10) This linear system may be easily solved, for example by using Gaussian elimination. Furthermore, 
for a small linear system like this, it is seldom necessary to worry about numerical instability problems. 
The basis functions Cj = x j, where j = (0, 1, 2, 3> are typically used for cubic polynomials. However, 
the values of A multiplying these basis functions have no intuitive meaning with respect to the shape 
of the curve. Since any set of linearly independent polynomials would work as basis functions, consider 
what happens if we use instead the the Hermite polynomials <s0, Sl, s2, s3) shown in Figure 3. These 
polynomials have the property that if you combine scalar multiples of them to produce a curve, the values 
of the position and tangent at each endpoint of the curve are each controlled by exactly one of the basis 
functions. The values of A are just scalar multiples for the basis functions, so this means that by setting 
an element of A to some fixed value, we can predefine the endpoint or tangent of the resulting curve. 
Stated more formally: We may treat just a subset of {a0, al,..., ad} as variables, and the rest as constants. 
To simplify the notation, rename the basis functions as needed so that {ao, al,...,a,~--l} are the free 
variables, so(t) = 2t 3 -- 3t 2 + 1 ~(t) = t 3 -t ~ sl(t) = t 3 -- 2t 2 Jr- t s3(t) = --2t a -I-3t 2 
Figure 3. The Hermite basis functions. and {am, am+l,...,act} are the constants. Then equation 10 becomes, 
for 0 _~ k < m, j=0 i=l i=i x j=m (11) which specifies an m X m linear system. This means that by using 
the Hermite basis in our least-squares fitting, we can preset the endpoints of the resulting cubic. The 
tangents in the above discussion, however, are the slopes of X or Y with respect to t. If we try to fix 
the slope of our curve by fixing those two values, the resulting curve is overconstrained for our purposes. 
The only value we need to fix is the slope of Y with respect to X; that is, the slope of the curve as 
projected on the X-Y plane. The length of this vector can be left free and used to improve the fit. We 
would like a representation where it is possible to have the length of this tangent be a variable in 
A. As long as we are fitting the x and y components of the parametric curve independently, we cannot 
design our basis functions such that a single value of A controls the length of the tangent vector in 
the X-Y plane. The solution is to fit both x and y simultaneously by specifying the curve as a linear 
combination of basis functions that are vector-valued functions of the variable t. In other words, F(ti) 
----(x, y). Formally, given a sequence of data points {zi E R211 < i _~ n}, a corresponding sequence 
of parameters {ti E ~] 1 < i < n}, a set of linearly inde- pendent basis functions {Oj : -~ R 2 I 0 
~ j < d}, and real numbers {am, am+l,..., ad}, we will find real numbers {a0, al,..., am--l} that minimize 
the function n d 2 S=i~=l(j~oaJC~j(ti)-:--zi)__ (12) The square denotes the dot product of the argument 
with itself. For each point zi we are computing the square of the magnitude of the vector between the 
point and the value of the curve at t i. In other words, once F(ti) is the closest point on the curve 
to zi, we are minimizing the projected distance between the data point and the curve, just as we wanted. 
As before, however, we don't know which values of t are the best to use in our minimization. If we think 
of each data point as a point in space at (zx, zy, t), the inner term in equation 12 is just the square 
of the distance between the curve and the sample point computed in the plane t ---~ ti. Using this distance 
in our algorithm is equivalent to solving for X and Y independently. At each ti, we are now computing 
(Fz(ti) --z=) 2 -f- (Fy(ti) --zy) 2. Before, we were computing for X the value (X(ti) --zz) 2 and for 
Y similarly. Since n ~ n --z,) = F_,(x(t,) --+ F,(Y(t,) - i~l i~l i~l if X(ti) ~ Fx(ti) and Y(ti) ----Fy(ti) 
the results will be identical. The derivation goes through as before, and the result- ing linear system 
is %(t,).k(td = z,- jcj(t ) .k(td j~0 i~l --j~rn (13) for 0 < k < rn. Once again we will use basis 
functions that are com-binations of the Hermite polynomials. Each basis function will be of the form 
(t) = (q(t), r(t)) where q and r are scalar multiples of Hermite polynomials. There are up to 8 basis 
functions, one for each degree of freedom in the curve. For example, if we choose the functions bj(t) 
= (sj(t), 0); Oj+3(t) = (0, sj(t)), j = (0, 1, 2, 3) we get the same results as solving X and Y independently 
using the Hermite basis functions. To pick a basis function such that direction of the tangent in the 
X-Y plane is constrained to a value, for example, (~, v) at t = 0, we use a function of the form (I)(t) 
: (~81(t),.81(t)) where sl(t) is the Hermite polynomial that has a slope of 1 at t ---- 0. If we pick 
the rest of our basis functions such there is no other eontribution to the slope at that point, the slope 
of the final curve is guaranteed to lie along the specified vector. Let us give a complete example. Suppose 
we have the data points as shown in Figure 4(a), along with the desired position of the endpoints (x0, 
Y0), (Xl, y~) and the desired tangent direction (~, v) at t ----1. The basis function that will control 
the tangent is ~0(t) ---~(~s2(t), vs,(t)) and its associated free variable is ao. To leave the other 
tangent free we need two basis functions: ~l(t) ~-(81(t), 0), ~2(t) --~ (0, and two free variables: To 
fix the endpoint positions we use 3(t) ---- (XoSo(t), yoso(t)), (I)4(t) -----(Xls3(t), YlS3(t)) and 
preset the values of a3 and a4 to be 1. We now have 5 basis functions, 3 values of A that will vary and 
2 fixed values of A that specify the solution for this example. There are 8 degrees of freedom in a parametric 
cubic piece, but since our goal is to restrict the shape of the curve that can result, we should not 
be surprised that we end up with fewer than 8 basis functions. In this example, we have a 3 X 3 linear 
system to solve at each iteration of the general algorithm. The values of the ti are still adjusted at 
each step, in accordance with equation 5. However, it is no longer necessary to rescale the ti to lie 
between 0 and 1, because those extra degrees of freedom have already been specified by fixing the positions 
of the endpoints. When the algorithm is run, the result is the curve shown in Figure 4(b). To show that 
the tangent vector at (xl, yl) is really parallel to (~, v), we can calculate this vector at t = 1 as 
d aj%(1) = (14) j~0 The value of aj~.(1) is (0,0) for j ~ 0 because either the slope of the Hermite 
polynomial is 0 at t = 1, or the corresponding aj ~ O. (x0, y0) (~) 9 (~, ~3) (2~1, Yl) Figure 4. (a) 
Data points with both end positions and the tangent vector at t ~ 1 specified. The tangent direction 
is (~, v). (b) The result of applying the fitting algorithm to (a). Clearly this method of specifying 
parametric curves is not restricted to basis functions that are combinations of Hermite cubic polynomials--any 
set of linearly inde-pendent twice-differentiable functions will serve as well. If quadratic basis functions 
are substituted for the cubic ones, least-squares fitting of parabolas may be obtained. Circles or ellipses 
may be fit by using an appropriate set of trigonometric basis functions. It is interesting to note how 
this representation for piecewise cubic polynomials relates to more common spec-ifications, specifically 
B6zier curves and B-splines. In these representations, the variables are vector values (often called 
control points) which are multiplied by real-valued func- tions of t to fix the shape of the curve. Such 
a specifica-tion can be converted to a scheme of vector-valued basis function and real-valued coefficients 
as follows: let the con- trol points be specified as aj aljblj ~-a2jb23, each = control point having 
its own coordinate system with basis {blj, b2j}. Furthermore, let the original basis functions be {j(t)}. 
Then the parametric curve is E ajCj(t) = E(aljblj + a2jb2j)j(t) J J (15) = E (alj(blj~j(t)) -~- a2j(b2jCj(t))) 
J so the appropriate set of vector-valued basis functions is {bijCj(t) I i --~ 1, 2}. This means we 
can use this algorithm to solve for cubic polynomials with B-spline or Bernstein basis functions, and 
we can preset linear constraints on the control points for the resulting curve. Since each control point 
has been given its own coordinate system, by appropriate choice of the coordinate systems and the free 
coefficients, each control point can be constrained to be on a particular point or line, or to be free 
to move anywhere in the plane. 6. Relation to previous work. There is a wealth of literature on piecewise 
polynomial functions or splines, and much of it is available in textbook form [10,21,24,25]. Spline functions 
are often used to approximate more complicated functions because they are computationally simpler. Much 
of the spline literature involves methods for smoothly interpolating a small set of data points. This 
is similar to the problem of designing with a small number of interpolating points. The applications 
that are relevant to our problem are those which try to fit a large set of data points with a small number 
of pieces. The first reference to this problem we can find is by Rice [21]. The paper by Cox [9] gives 
a nice description of the problem of curve fitting with splines using the least-squares error metric. 
He also mentions using dynamic programming as the method for finding knots, but does not extend it to 
spline functions with constraints. Some more recent publications that discuss curve fitting with splines 
are listed in the references [7,11,14,19,22]. There are two significant issues that separate the prob- 
lem of fitting shapes from the problem of function ap-proximation. First, shapes are usually represented 
as para-metric functions to make the representation independent of the choice of axis. The usual technique 
in computer graphics applications is to use two functions, X and Y, each of which is a piecewise polynomial 
function of the parameter t. By assigning some value of t to each of the data points, it is possible 
to extend methods developed for spline functions to parametric spline functions by treating X(t) and 
Y(t)independently [11,20]. However, the shape of the curve can be dramatically influenced by the choice 
of parameterization (figure 5). The second issue is that the continuity constraints at the knots for 
shapes are quite relaxed by function fitting standards. A polynomial spline function of order k generally 
has k -- 1 continuous deriva- tives at the knots. This is not necessary, or in most cases, even desirable 
for two-dimensional shapes. If we want the most efficient representation, clearly we must not constrain 
the curve unnecessarily. A more complete discussion of continuity conditions is available in the literature 
[17,18]. Flegal developed a method for fitting curves to hand- drawn sketches at Xerox PARC in 1974 [13]. 
In his thesis, Reeves [20] presents and compares several techniques, in-eluding Flegal's algorithm, for 
fitting piecewise paramet- ric cubic polynomial curves to hand-drawn sketches. The focus is on algorithms 
that are computationally efficient enough to give real-time feedback in a sketching system, so in general 
the techniques tend to sacrifice output quality for speed. For many of our purposes, especially for deriv- 
ing analytical representations for digital typefaces, we are willing to trade computational speed for 
better results. There are two published algorithms which take a set of samples and produce the B-spline 
control points for the approximating curve for use in an interactive environment [28,29]. These algorithms 
create a representation that has the knots evenly spaced with respect to the parameter, which is not 
the most efficient representation for our pur-poses. Also, these techniques include a significant amount 
of user intervention to achieve the final shape. We want to minimize the amount of user interaction necessary. 
7. Conclusions. We have presented a method for fitting shapes defined by a discrete set of data points 
with a parametric piecewise cubic polynomial curve. Our goal is to give an efficient rep- resentation 
that "looks good" for graphics arts applications. The two new techniques we have introduced are dynamic 
programming for determining the knot positions, and an iterative method for fitting a parametric cubic 
with optional endpoint and tangent vector constraints to a set of data points. Most of our work to date 
has been with letter-form shapes. One of the goals of our laboratory is to have a uniform, resolution-independent 
font representation for our digital printers. Many of the fonts we would like to use are currently defined 
only as a bitmaps. Therefore, we have been motivated to apply our system to convert fonts t=~ t~t____O 
t=l Figure 5. The curves are each defined by the same four data points. The only difference is the values 
of the parameter associated with the inner points. By comparing the two curves it is easy to see that 
the shape of the curve can be dramatically influenced by the choice of parameterization. from bitmaps 
to curve outlines. Figure 1 and figure 6 are 8. Future work. examples of data derived from such bitmaps. 
There are several ways in which we would like to Figure 7 shows an example of a more general graphics 
improve our method, a few of which are worth mentioning arts application. We have also used our methods 
on very here. simple hand-drawn sketches. The data points in figures 1 We do not completely understand 
the convergence and 2 are hand-drawn, for example. properties of our algorithm, especially why it sometimes 
AB CDEF GHIJK LMNOPQRSTU VWXYZ abcdefghij klmno  pqrstuvwxyz Figure 6. This font was converted from 
a bitmap (160 justed, no further user intervention was required to pixels per em) which was originally 
produced using produce the cubic representation. Metafont[15]. Once the tolerance parameters were ad- 
Figure 7. The data for this sample comes scanning a approximately 72 by 72. (b) The sample points and 
3/4 inch high original with a 95 line/inch scanner. Some the result of the curve-fitting process. (c) 
Final curve detail was lost in this process, especially with respect in more detail, showing the curve, 
the knots and the to corners. The image was fit using a loose tolerance samples. to achieve further smoothing. 
(a) The original image, does not converge at all. While our system can easily reject these cases as part 
of the fitting process (because they are simply instances of a very bad fit), we would like to avoid 
them. The type of iteration method we use is suitable for convergence acceleration techniques. However, 
we observe that these methods decrease the stability of the algorithm even further. Further understanding 
of these properties, especially with respect to the initial approximation for the parameter values, should 
lead to a more efficient and stable algorithm. We have chosen a method for fitting piecewise func- tions 
that separates somewhat the problems of finding the knots and fitting the curve for each piece. This 
is not the only way to solve this problem. It is possible to set up a global system that does a least-squares 
fit for an entire spline [9,11,22]. Then, on each iteration we can adjust the values for t or adjust 
the knot locations or both. It is easiest to think of solving X and Y separately in this case, although 
it is also possible to extend the idea of vec- tor basis functions to spline curves. The problem with 
this approach is that it requires the solution of a large hnear system, proportional to the number of 
pieces, during each iteration. Our experience has been that this quickly be- comes too expensive to be 
practical. Also, it is not clear what is the best way to adjust the number and location of the knots 
each time. However, a good algorithm of this sort might converge to a globally better solution. Both 
the iteration technique and the dynamic pro-gramming application presented in this paper can be com- 
putationally expensive, and some applications do not need the full power of these algorithms to get acceptable 
results. We have developed a system for experimenting with meth- ods for fitting shapes. In the future, 
we would like to get a better understanding of how to parameterize the applica- tion-dependent aspects 
of this problem. 9. Acknowledgements. We would like to thank John Warnock, Robin Forrest and Lyle Ramshaw 
for their contributions to this project and for their many helpful comments on this paper. 10. References. 
1. Aho, A.V., Hopcroft, J.E. and Ullman, J.D., The Design and Analysis of Computer Algorithms, Addison-Wesley, 
Reading, Massachusetts, 1974. 2. Barnhill, R.E. and Riesenfeld, R.F., eds., Computer Aided Geometric 
Design, Academic Press, New York, 1974. 3. Baudelaire, P. and Stone, M., "Techniques for Interactive 
Raster Graphics." Computer Graphics 14, 3 (1980), 302-307. 4. Baudelaire, P., The Draw User's Manual, 
internal report. Xerox Palo Alto Research Center, Palo Alto, CA, 1978. 5. Baudelaire, P., The Fred User's 
Manual, internal report. Xerox Palo Alto Research Center, Palo Alto, CA, 1976. 6. Bellman, R., Dynamic 
Programming, University Press, Princeton, N.J., 1957.  7. Chung, W.L., "Automatic Curve Fitting Using 
and Adaptive Local Algorithm." ACM Transactions on Mathematical Software 6, 1 (1980), 45-57. 8. Conte, 
S.D., and de Boor, C., Elementary Numerical Analysis: An Algorithmic Approach, second edition, McGraw-Hill, 
New York, 1972. 9. Cox, M.G., "Curve Fitting with Piecewise Polynomials." J. Inst. Maths Applications 
8, (1971), 36-52. 10. de Boor, C., A Practical Guide to Splines, Springer-Verlag, 1978. 11. Dierckx, 
P., "Algorithms for Smoothing Data with Periodic and Parametric Splines." Computer Graphics and Image 
Processing 20, (1982), 171-184. 12. Faux, I.D., and Pratt, M.J., Computational Geometry for Design and 
Manufacture, Ellis Horwood, 1979. 13. Flegal, R., private communication. Xerox Palo Alto Research Center, 
Palo Alto, CA, 1978. 14. Ichida, K and Yoshimoto, F., "Curve Fitting by a One-Pass Method with a Piecewise 
Cubic Polynomial."  ACM Transactions on Mathematical Software 3, 2 (1977), 164-174. 15. Knuth, D.E., 
TEX and Metafont: New Directions in Typesetting, Digital Press and American Mathematical Society, 1979. 
16. Lipkie, D.E., Evans, S.R., Newlin, J.K., and Weissman, R.L., "Star Graphics: An Object-Oriented Implementation." 
Computer Graphics 16, 3 (1982), 115-124. 17. Manning, J.R., "Continuity Conditions for Spline Curves." 
Computer Journal 17, (1974), 181-186. 18. Nielson, G.M. "Some Piecewise Polynomial  Alternatives to 
Splines in Tension" in Computer Aided Geometric Design, Barnhill, R.E. and Riesenfeld, R.F., eds., Academic 
Press, New York, 1974. 19. Pavlidis, T., Algorithms for Graphics and Image Processing, Computer Science 
Press, 1982. 20. Reeves, W.T., "Quantitative Representation of Complex Dynamic Shapes for Motion Analysis." 
PhD Thesis, Department of Computer Science, University of Toronto, 1980. 21. Rice, J.R., The Approximation 
Of Functions, vols. 1 and 2, Addison-Wesley, Reading, Mass., 1964. 22. Rice, J.R., Algorithm 525. ADAPT, 
adaptive smooth curve fitting, ACM Transactions on Mathematical Software 4, 1 (1978) 82-94. 23. Rogers, 
D.F., and Adams, J.A., Mathematical Elements for Computer Graphics, McGraw-Hill, New York, 1976. 24. 
Schultz, M., Spline Analysis, Prentice-Hall, Englewood Cliffs, N.J., 1973.  25. Schumaker, Larry L., 
Spline Functions: Basic Theory, John Wiley ~ Sons, New York, 1981.  26. Stone, M., The Griffin User's 
Manual, internal report. Xerox Palo Alto Research Center, Palo Alto, CA, 1979. 27. Warnock, J. and Wyatt, 
D., "A Device Independent Graphics Imaging Model for use with Raster Devices." Computer Graphics 16, 
3 (1982), 313-320. 28. Wu, S., Abel, J.F., and Greenburg, D.P., "An Interactive Computer Graphics Approach 
to Surface Representation." CACM 20, 10 (1977), 703-712. 29. Yamaguchi, F., "A New Curve Fitting Method 
Using a CRT Computer Display." Computer Graphics and Image Processing, 7 (1978), 425-437.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801154</article_id>
		<sort_key>241</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>29</seq_no>
		<title><![CDATA[Japanese computer graphics]]></title>
		<subtitle><![CDATA[Challenges and opportunities]]></subtitle>
		<page_from>241</page_from>
		<page_to>242</page_to>
		<doi_number>10.1145/800059.801154</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801154</url>
		<abstract>
			<par><![CDATA[<p>PANEL COORDINATOR'S INTRODUCTORY REMARKS</p> <p>Japanese interest in computer graphics in the past has been led by users whose needs have been satisfied primarily by U. S. sources. However, as the Japanese market has grown, domestic players have emerged, especially commercial production companies and venture-type hardware manufacturers. In the world of commercial production, this trend has been spurred by the cross-cultural difficulties and expenses of trans-Pacific projects, as well as the new accessibility of computer graphics production technology. In the hardware field, this tendency has been fed by rapid Japanese advances in semiconductor memories and color monitors.</p> <p>Growing Japanese strength in computer graphics is important to the international community for several reasons. First, it confirms the viability and sales potential of the domestic Japanese market. Second, it increases competition within Japan. Third, it will probably increase competition outside of Japan as well. Fourth, this competition should stimulate development of superior products at a lower cost, yielding considerable benefit to users. Fifth, there will be many new opportunities for internationally- minded distributors, systems builders, software suppliers and R &amp; D groups. Finally, if approached correctly, Japan can be a source of fresh aesthetic and technical input.</p> <p>During the past few years, many Japanese have made extensive efforts to learn everything they can about computer graphics. Numerous foreign experts have been invited to Japan to lecture at conferences. The Japanese themselves have criss-crossed America visiting major computer graphics installations. At Siggraph'82 238 Japanese registered for tutorials and technical sessions and many more attended the hardware exposition. Japanese press coverage was aggressive. This panel is an attempt to balance the information flow by offering Siggraph attendees an opportunity to educate themselves about Japanese computer graphics.</p> <p>CHAIRMAN'S INTRODUCTION:</p> <p>While the Japanese electronics industry in general has been dominated by a few large firms, the computer graphics industry in Japan has been led by a number of small, venture-type companies offering original technology and close customer support. They are self- sustaining and not part of so-called &#8220;Japan Inc&#8221;. Currently, there are quite a number of these firms with a relatively high level of hardware and software expertise. This knowledge base has been developing for some time. For example, in association with some of the earlier companies, the Tokyo Raster Computer Graphics (TRCG) Project was begun in 1968 to develop a raster system capable of displying 4096 colors concurrently. The partial results the TRCG research were reported at the first Siggraph Conference held in Boulder, Colorado in 1974. This panel will examine progress since that time. The panelists were selected to fairly represent the people behind the development of Japanese computer graphics companies who significantly influence industrial strategies. Each panelist will briefly describe his involvement in computer graphics, views of the world market, and areas of current interest in terms of future product development. General discussion of differences and commonalities among the panelists will follow the presentations. Questions from the audience will be encouraged.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.1</cat_node>
				<descriptor>Markets</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003458</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing industry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP40036150</person_id>
				<author_profile_id><![CDATA[81332510163]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Toshiyasu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kunii]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31079577</person_id>
				<author_profile_id><![CDATA[81100241671]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Laurin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Herr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pacific Interface]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39078144</person_id>
				<author_profile_id><![CDATA[81100485651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tsuneo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ikedo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seillac Company Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P331783</person_id>
				<author_profile_id><![CDATA[81332506353]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kazukiyo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yamaha Research Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39078256</person_id>
				<author_profile_id><![CDATA[81100382275]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kansei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Graphica Computer Corp.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P331785</person_id>
				<author_profile_id><![CDATA[81100603274]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Kazuo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naito]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Daini Seikosha]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P333769</person_id>
				<author_profile_id><![CDATA[81100104486]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Shohei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Saimi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Radio Corp.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PANEL JAPANESE COMPUTER GRAPHICS: CHALLENGES AND OPPORTUNITIES PART II: INDUSTRIAL STRATEGIES OF JAPANESE 
COMPUTER MANUFACTURERS CHAIR: Toshiyasu Kunii, University of Tokyo PANEL COORDINATOR: Laurin Herr, Pacific 
Interface PANELISTS: Tsuneo Ikedo, Seillac Company Ltd. Kazukiyo Ishimura, Yamaha Research Institute 
Kansei Iwata, Graphica Computer Corp. Kazuo Naito, Daini Seikosha Shohei Saimi, Japan Radio Corp. PANEL 
COORDINATOR'S INTRODUCTORY REMARKS Japanese interest in computer graphics in the past has been led by 
users whose needs have been satisfied primarily by U. S. sources. However, as the Japanese market has 
grown, domestic players have emerged, especially commercial production companies and venture-type hardware 
manufacturers. In the world of commercial production, this trend has been spurred by the cross- cultural 
difficulties and expenses of trans-Pacific projects, as well as the new accessibility of computer graphics 
produc- tion technology. In the hardware field, this tendency has been fed by rapid Japanese advances 
in semiconductor memories and color monitors. Growing Japanese strength in computer graphics is important 
to the international community for several reasons. First, it confirms the viability and sales potential 
of the domestic Japanese market. Second, it increases competition within Japan. Third, it will probably 
increase competi- tion outside of Japan as well. Fourth, this competition should stimulate develop- ment 
of superior products at a lower cost, yielding considerable benefit to users. Fifth, there will be many 
new opportuni- ties for internationally- minded distribu- tors, systems builders, software suppliers 
and R &#38; D groups. Finally, if approached correctly, Japan can be a source of fresh aesthetic and 
technical input. During the past few years, many Japanese have made extensive efforts to learn everything 
they can about computer graph- ics. Numerous foreign experts have been invited to Japan to lecture at 
confer- ences. The Japanese themselves have criss-crossed America visiting major com- puter graphics 
installations. At Sig- graph'82 238 Japanese registered for tutorials and technical sessions and many 
more attended the hardware exposition. Japanese press coverage was aggressive. This panel is an attempt 
to balance the information flow by offering Siggraph attendees an opportunity to edu- cate themselves 
about Japanese computer graphics. CHAIRMAN'S INTRODUCTION: While the Japanese electronics industry 
in general has been dominated by a few large firms, the computer graphics industry in Japan has been 
led by a number of small, venture-type companies offering original technology and close customer support. 
They are self- sustaining and not part of so-called "Japan Inc". Currently, there are quite a number 
of these firms with a relatively high level of hardware and software expertise. This knowledge base has 
been developing for some time. For example, in association with some of the earlier companies, the Tokyo 
Raster Com- puter Graphics (TRCG) Project was begun in 1968 to develop a raster system capable of displying 
4096 colors concurrently. The partial results the TRCG research were reported at the first Sigg=aph Conference 
held in Boulder, Colorado in 1974. This panel will examine progress since that time. The panelists were 
selected to fairly represent the people behind the development of Japanese computer graphics companies 
who significantly influence 241 industrial strategies. Each panelist will briefly describe his involvement 
in com- puter graphics, views of the world market, and areas of current interest in terms of future product 
development. General dis- cussion of differences and commonalities among the panelists will follow the 
presentations. Questions from the audi- ence will be encouraged. Toshiyasu L. Kunii received the B.Sc. 
and M.Sc. and B.Sc. degrees in chemistry from the University of Tokyo in 1962, 1964 and 1967, respectively. 
He is currently Pro- fessor of Information and Computer Science in the Department of Information Science 
of the Faculty of Science at the Univer- sity of Tokyo. His research interests include computer graphics, 
database sys- tems and software engineering. He has authored and edited more than 70 refereed academic/technical 
papers in computer sci- ence and application areas. He was the principal researcher in the Tokyo Raster 
Graphics Project. He is currently President of the Japan Computer Graphics Association, an editor of 
IEEE Computer Graphics and Applications, and the Chair- man of the IBM, Japan Computer Science Symposium 
Organizing Committee. He is on the IFIP Database Working Group and the IFIP Systems Modeling Working 
Group. From 1976 to 1981, Dr. Kunii organized and chaired the Technical Committee on Software Engineering 
of the Information Processing Society of Japan. Most recently, he served on the Organizing Com- mittee 
of the Micrograph'82 Conference and was Program Chairman of the Intergraph- ics'83 conference, both held 
in Tokyo. Tsuneo Ikedo graduated from Tokyo Metro- politan University in 1971 with a major in Electrical 
Engineering. He is the Founder and President of Seillac Co. Ltd., a manufacturer of high performance 
computer graphics display systems. He is dedicated to the pursuit of the state of the art in computer 
graphics technology, and is par- ticularly interested at this time in spe- cialized processors such as 
homogeneous and spline processors (either micro-coded or hard-coded). Kazukiyo Ishimura graduated from 
Maseda University in 1960 with a major in Elec- tronics. He is General Manager of Research and Development 
for Nippon Gakki Co. Ltd. (Yamaha). His current area of interest is the development of interactive color 
graphics terminals for home enter- tainment centers which would also ~nclude digital electric organs 
and other digital audio components. Kansei Iwata graduated from Tohoku Univer- sity in 1964 with a 
major in Electrical Engineering. He founded Graphica Computer Co. Ltd. in the early 1970's and now serves 
as the President of this pioneering firm. His interest has been in the development of fast, multi-colored 
raster graphics systems with very deep bit-maps and/or flexible pipeline architecture for easy integration 
into customized computer graphics or image processing systems. Kazuo Naito graduated from Yamanashi 
University in 1945 with a major in Indus- trial Engineering. He has been a key fig- ure in the development 
of SEIKO's computer graphics technology and now serves as Gen- eral Manager of the Industrial Electronics 
Division and Senior Managing Director of Dainiseikosha Co. Ltd. His current area of interest is in the 
creation of a broad family of mass- produceable, highly reli- able raster graphics products character- 
ized by a relatively modest number of con- currently displayable colors. Shohei Saimi graduated from 
the University of Osaka in 1954 with a major in Physics. He is currently the manager of the Systems 
 Engineering Department of the Japan Radio Corporation (JRC). At JRC he has emphasized the development 
of high and multi-color graphics with reliable, well- tested hardware/software components. 242 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801155</article_id>
		<sort_key>243</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>30</seq_no>
		<title><![CDATA[Solid model input through orthographic views]]></title>
		<page_from>243</page_from>
		<page_to>252</page_to>
		<doi_number>10.1145/800059.801155</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801155</url>
		<abstract>
			<par><![CDATA[<p>This paper describes the results of basic studies on procedures for creating solid models of component geometry from two-dimensional orthographic projections. An interactive graphic program was developed to allow the input of three orthographic views of a component geometry by digitizing from a drawing. The views may contain straight lines and circular arcs, solid or dashed. No restrictions are placed on the order or direction of lines and arcs in any view. Using an extension of the Wesley-Markowski procedure, the program constructs a three-dimensional solid model of the object. When the projections are ambiguous, multiple solid models are produced. The solid model may contain planar, cylindrical, conical, spherical and toroidal surfaces. Topological information of the solid model is stored in a winged edge structure. Geometric information is stored as vertex coordinates and surface equations.</p> <p>The procedure for 2D-3D conversion provides a powerful new method for manual input of solid models, a common interface to all turnkey graphics systems, and, properly integrated with existing technology for scanning of drawings, a powerful new method for acquisition of CAD/CAM data bases from existing drawings.</p> <p>The procedure is described, examples of typical input and output are shown, and possible extensions are discussed.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31095040</person_id>
				<author_profile_id><![CDATA[81100436510]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakurai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Research Engineer, Nissan Motor Company]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P61756</person_id>
				<author_profile_id><![CDATA[81100084160]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Gossard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Associate Professor, Computer Aided Design Laboratory, Department of Mechanical Engineering, Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>988036</ref_obj_id>
				<ref_obj_pid>988026</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[I.E. Sutherland, "Three-Dimensional Data Input by Tablet", Proc. IEEE, Special Issue on Computer Graphics, Vol. 62, No. 4, 1974.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[R.W. Thornton, "Interactive Modelling in Three Dimensions through Two-Dimensional Windows", Third International Conference and Exhibition on Computer in Engineering and Building Design, 1978.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[M. Idesawa, "A System to Generate a Solid Figure from a Three View", Bull. JSME 16, pp. 216-225, February 1973.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[M. Idesawa, et al, "Automatic Input of Line Drawing and Generation of Solid Figure from Three-View Data", Proceedings of the International Joint Computer Symposium, pp. 304-311, 1975.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563295</ref_obj_id>
				<ref_obj_pid>965143</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[G. Lafue, "Recognition of Three Dimensional Objects from Orthographic Views", Computer Graphics, Vol. 10, No. 2, 1976.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[K. Preiss, "Constructing the 3-D Representation of a Plane-Faced Object from a Digitized Engineering Drawings", Fifth International Conference and Exhibition on Computer in Engineering and Building Design, 1980.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[G. Markowsky and M.A. Wesley, "Fleshing Out Wire Frames", IBM J. Res. Develop. 24, pp. 582-587, September 1980.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[G. Markowsky and M.A. Wesley, "Fleshing Out Projections", IBM J. Res. Develop. 25, pp. 934-954, November 1981.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[H. Sakurai, "Solid Model Input Through Orthographic Views", M.S. Thesis, Massachusetts Institute of Technology, Cambridge, Massachusetts, 1982.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SOLID MODEL INPUT THROUGH ORTHOGRAPHIC VIEWS Hiroshi Sakurai Research Engineer Nissan Motor Company 
 David C. Gossard Associate Professor Computer Aided Design Laboratory Department of Mechanical Engineering 
Massachusetts Institute of Technology Abstract: This paper describes the results of basic studies on 
procedures for creating solid models of component geometry from two-dimensional orthographic projections. 
An interactive graphic program was developed to allow the input of three orthographic views of a component 
geometry by digitizing from a drawing. The views may contain straight lines and circular arcs, solid 
or dashed. No restrictions are placed on the order or direc- tion of lines and arcs in any view. Using 
an extension of the Wesley-Markowski procedure, the program constructs a three-dimensional solid model 
of the object. When the projections are ambiguous, multiple solid models are produced. The solid model 
may contain planar, cylindrical, conical, spherical and toroidal surfaces. Topological information of 
the solid model is stored in a winged edge structure. Geometric information is stored as vertex coordinates 
and surface equations. The procedure for 2D-3D conversion provides a powerful new method for manual 
input of solid models, a common interface to all turnkey graphics systems, and, properly integrated with 
existing technology for scanning of drawings, a powerful new method for acquisition of CAD/CAM data bases 
frora existing drawings. The procedure is described, examples of typical input and output are shown, 
and possible extensions are discussed. 1. INTRODUCTION Many research efforts are being conducted to 
realize complete geometric models of mechanical components for use in design and manufacturing. One method 
used to create a complete model of the shape of a mechanical part has been called constructive solid 
geometry. In this method, set operators are applied to a collection of primitive solids (e.g. cube, cylinder, 
sphere, cone, ...) to construct a solid model. Though this approach has Permission to copy without fee 
all or part of this material is granted provided that the copies are not made or distributed for direct 
commercial advantage, the ACM copyright notice and the title of the publication and its date appear, 
and notice is given that copying is by permission of the Association for Computing Machinery. To copy 
otherwise, or to republish, requires a fee and/or specific permission. its advantages, it cannot convert 
white-frame models constructed by wire-frame CAD/CAM systems into solid models. Because solid modeling 
systems will coexist with wire-frame systems for some years to come, an interface between the two is 
desireable. Also, systems based on constructive solid geometry cannot utilize a variety of graphic input 
techniques used in wire-frame CAD/CAM systems. This research effort was aimed at interfacing wire-frame 
and solid modeling CAD/CAM systems as well as at providing designers with more natural ways of constructing 
solid models. A program was developed in which a set of orthographic views, a very natural way for designers 
to express geometry which can be created easily with wire-frame systems, is processed into.a solid model. 
Though currently only lines and arcs are accepted in orthographic views, many mechanical parts can be 
expressed by these two geometric entities. Hidden lines and hidden arcs must be identified as in conventional 
engineering drawings. No restrictions are placed on the order and direction of lines and arcs in each 
view. Planar, cylindrical, conical, spherical and toroidal surfaces are allowed in the solid model generated. 
 A graphics program was also developed to allow the input of orthographic views by digitizing and to 
allow visual verification of a generated solid model. Much work has been done to generate polyhedral 
objects from their projections. Sutherland's work in three dimensional input [;] was focused on hardware 
and software for digitizing. He introduced a tablet with multiple pens so that a 3D vertex could be generated 
by digitizing vertices in two views. He also discussed how to treat digitized data from perspective views. 
Thornton's work [2~ concerned interactive tech- niques for three dimensional input from 2D views. Neither 
Sutherland nor Thornton investigated algorithms for construction of solid models from projections. The 
first effort to construct solid models from projections was initiated by Idesawa [3], [4]. His approach 
was as follows. 1. Generate 3D vertices from 2D vertices.  2. Generate 3D edges from 3D vertices. 
 &#38;#169; ACM 0-89791-109-1/83/007/0243 $00.75 3. Generate 3D faces from 3D edges 2.2 Orthographic 
Views  4. Assemble faces into an object.  Among many conventions in engineering drawing, With this 
approach, false elements can be generated. the following are important in this work. Idesawa identified 
some criteria to remove these false elements. For example, a 3D vertex belonging to only one face is 
false. Lafue ~] added a heuristic procedure for removing false elements and finding true elements. This 
approach has two drawbacks. First, the method cannot remove all the possible false elements and can remove 
some true elements in multiple solution cases. Second, they require input data to conform to a prescribed 
format. For example, Lafue required that 2D lines be input in such a way that a sequence of lines bounds 
a face. Preiss [6J freed input from a prescribed for- mat. He generated visible surfaces from polygons 
in orthographic views. He claims that he generated invisible faces using a scene analysis technique, 
but does not show the method. His approach also does not work for multiple solution cases. G. Markowsky 
and M.A. Wesley [72, ~8] treated the construction problem in a mathematically formal way so that even 
pathological cases, (not allowed in former work) can be accepted. An example pathological case is an 
edge shared by more than two faces. Their most important achievement is their method of assembling faces. 
Faces are con- nected into closures which divide infinite space into subspaces. The combinations of subspaces 
which give the input projections are solutions. In this way, their approach can remove all false elements 
and ensure that all the solutions are obtained. All the above work concerns planar polyhedral geometries 
whose projections contain only lines. The work described here extends the approach of Wesley and Markowsky 
to create geometries which include planar, cylindrical, spherical, conical, and toroidal surfaces. 2. 
OBJECT AND ORTHOGRAPHIC VIEWS In this section, orthographic views as the input to the solid model generation 
procedure and the object as its output are discussed briefly. 2.1 Object  An object is a finite segment 
of three- dimensional space and is expressed with the follow- ing elements. Face: A face is a finite 
segment of a surface which is a part of the boundary separating an object from the rest of the three 
dimensional space. Edge: An edge is a finite segment of a three dimensional line or curve which is a 
part of the boundary between a face and the remainder of the surface. An edge is shared by at least two 
faces. Vertex: A vertex is one of the two points which separate an edge from the remainder of the line 
or curve. A vertex is shared by at least three edges. |. Three Orthographic Views: Basically, engineers 
express objects (mechanical parts) by a number of projected views of which three ortho- graphic or principal 
views are the most important: top view, front view and side view. This work deals with the case of three 
orthographic views. 2. Identification of Hidden Edges: In engineering drawings, hidden edges are identified 
by dashed lines.  3. Silhouette Lines and Arcs: Another basic convention is to draw silhouette lines 
and arcs of curved faces in orthographic views. An artificial edge corresponding to a silhouette line 
or arc is called a silhouette edge hereafter. It lies on only one surface and appears in only one of 
the three views.  4. Tangency edge: In objects with planar and curved faces, two faces can be tangent 
to each other along a shared edge. This edge is called a tangency edge hereafter. A tangency edge does 
not appear in any views.   2.3 Geometric Elements Lines and arcs are accepted in the input orthographic 
views. On the output object, the following types of surfaces and edges are allowed. I. Surfaces: Planar, 
cylindrical, conical, toroidal and spherical surfaces. The axes of cylindrical, conical, and toroidal 
surfaces must be parallel to one of the axes of the body coordinate system. 2. Edses: Three dimensional 
lines and cir- cular arcs. The axes of circular arcs must be parallel to one of the axes of the body 
coordinate system.  Though these restrictions seem to be severe, most mechanical parts can be expressed 
by this set of surfaces anG edges.  3. PROCEDURE  The object generation procedure has six steps. In 
each step, all the possible elements are generated. In some steps, generated elements are tested with 
various criteria, which do not elimi- nate all false elements but significantly reduce the effort in 
later steps. The final test is performed by comparing the input orthographic views with the three orthographic 
views regenerated from the generated object. If they match each other, the object is one of the objects 
or the only object expressed by the input views.  3.1 Generate P-Vertices A p-vertex is a 2D point 
in a view and is a projection of a vertex. Because of the existence of silhouette edges and tangency 
edges, all the  p-vertices cannot be obtained by just calculating intersections in input views. In addition 
to inter- sections (p-vertices with the attribute of standard), 2D points where lines are tangent to 
arcs (p-vertices with the attribute of tangency) and 2D points where lines parallel to coordinate axes 
are tangent to arcs (p-vertices with the attribute of silhouette) are calculated. Then using the coordinate 
values of thes points, lines in other views are cut to obtain candidate p-vertices on silhouette edges 
and tangency edges (candidate p-vertices with the attribute of tan- gency-generated and silhouette-generated). 
A p-vertex can have more than one attribute.  3.2 Generate Candidate Vertices In this step, candidate 
vertices (c-vertices) are generated from candidate p-vertices. Then vertices whose projections were not 
recovered in the preceding step are generated. 1. Generate candidate vertices from candidate p-vertices: 
A 3D point is generated fromapair of candidate p-vertices which belong to different views and have the 
same coordinate value for the shared coordinate. Then a projection of the 3D point onto the third view 
is searched. If it is found in the third view, the 3D point is a c-vertex. Although there are five types 
of candi- date p-vertices, only four combinations of them are allowed. - A c-vertex with standard attribute 
is generated from three candidate p-vertices, all with standard attribute.  - A c-vertex with tangency 
attribute is generated from three candidate p-vertices, one with tangency attribute and the other two 
with tangency-generated attribute.  - A c-vertex with silhouette attribute is generated from three candidate 
p-vertices, one with silhouette attribute, one with standard attribute and one with silhouette- generated 
attribute.  - A c-vertex with double-silhouette attribute is generated from two candidate p-vertices, 
both with silhouette attribute.  All the vertices, except ones on both of silhouette edges and tangency 
edges, are recovered. 2. Generate c-vertices directly from the three views: A toroidal surface is assumed 
when the following facts are found in the three views. - Two arcs in different views have the same radius. 
 - The center points of the two arcs have the same coordinate value for the shared coordinate.  - The 
distances from the center points to the center point of an arc in the third view are equal.  A spherical 
surface is assumed when the follow- ing fact is found in the three views. - A 3D point generated from 
arc centers in two views has its correspomding arc center in the third view. Once a torus is assumed 
from arcs, then c- vertices on that surface can be generated. Those c-vertices are given torus attribute. 
A c-vertice can have more than one attibute. Figure 3 shows c-vertices generated.  3.3 Generate Candidate 
Edges Candidate edges (c-edges) are generated from c-vertices generated in the previous step. C- edges 
have three types, standard, silhouette, and tangency. Standard c-edge: Two c-vertices are checked if 
their projections lie on the same line or arc in each view. If they do, the two vertices are connected 
by a 3D line or arc to form a standard c-edge. Silhouette c-edge: Two c-vertices with sil- houette attribute 
are checked if their projections lie on the same line or arc in a view. If they do, the two c-vertices 
are connected by a 3D line or arc to form a silhouette c-edge. Tangency c-edge: A line-tangency c-edge 
is generated from two c-vertices with tangency attri- butes whose candidate p-vertices lie on the same 
arc or the coaxial arcs at the same angle. Arc- tangency c-edges are generated as follows. Two arc c-edges, 
each of which has at least one c-vertex with tangency attribute, torus attribute, or sphere attribute, 
are tested if they lie on the same torus or sphere. If they do, an arc-tangency c-edge is generated by 
connecting the two vertices of each arc c-edge. In practice, more detailed c-edge types are required 
to avoid as many false edges as possible. Once c-edges are generated, all the c-vertices are tested 
whether they lie on more than or equal to three edges, otherwise, the c-vertex is deleted and c-edges 
are generated again from a new set of c-vertices. Figure 4 shows c-edges generated.  3.4 Generate Candidate 
Faces This step is divided into four parts. First, surfaces are generated from c-edges. A planar surface 
is generated from an arc c-edge or two line c-edges sharing a c-vertex. A cylindrical or a conical surface 
is generated from two arc c-edges connected by at least one line c-edge. A toroidal or a spherical surface 
has already been generated in the c-vertex generation step. For each surface, all the c-edges lying on 
the surface are collected. (Figure 5 (a)). Second, a set of c-edge loops which divides the surface into 
the smallest subsurfaces is searched. C-edge loops, which bound their left side of a surfacep are found 
by traversing c-edges with turn-to-the-leftmost rule. The procedure used by Markowsky and Wesley 7 to 
find out loops on a plane can be applied to curved surfaces with small modification of the criterion 
differentiating be- tween in-loop and out-loop. [9] In-loop bounds its  Computer Graphics Volume 17, 
Number 3 July 1983 inside and out-loop bounds its outside (Figure 5(b)). 4. IMPLEMENTATION Third, a 
hierarchical tree of c-edge loops is generated for each surface and candidate faces (c-faces) are derived 
from the tree. The tree shows the relationships between c-edge loops, i.e., which loop includes which 
loop. From the tree, c-faces are found as follows. The out-loop which is not contained in any in-loop 
is discarded. In-loops at the root of the tree bound c-faces. An in-loop with its subordinate out-loops 
also bounds a face. (Figure 5(c)). Fourth, the following conditions are tested for all the c-faces generated. 
Any standard c-edge and tangency-edge should lie on at least two different surfaces, otherwise, the c-edge 
~s deleted. A c-face should be tangent to another c-face at its tangency c-edge, otherwise, the c-face 
is deleted. A c-edge related to a dashed entity in a view should have at least one c-face which hides 
the c-edge when it is projected onto that view, otherwise, the c-edge is deleted. If any of the c-edges 
are deleted, the process has to be carried out again from the beginning of this step. (Figure 5(d)). 
  3.5 Generate Candidate Objects C-faces are connected to form c-face loops which divide the three 
dimensional space into subspaces in the analogical way as c-edges are connected to c-edge loops. [9] 
Two c-faces shar- ing an edge are connected to each other. When more than two c-faces share a c-edge, 
a c-face which gives the smallest face connection angle is chosen. Each c-face loop forms a candidate 
object (c-object). One of the c-object generated is an infinite subspace and is discarded.  3.6 Generate 
Objects C-objects can be categorized into certain c-objects, and uncertain c-objects. If a 2D edge, 
a straight line or a circular arc in any projec- ted view, is related to only one c-object, then that 
c-object is a necessary part of the object and is called a certain c-object. If a c-object has no 2D 
edge which is related uniquely to the c-object, that object is an uncertain c-object. All the certain 
c-objects are assembled and tested. Then each of the possible combinations of uncertain c-object is added 
to the assembly of the certain c-objects and tested. The test is made as follows. The assembly is checked 
if it can give all the 2D edges in the input views, if so, the orthographic view is generated for each 
view and compared with the output views otherwise the assembly is discarded. If the projection of the 
assembly matches the input view for every view, the assembly is one of the objects or the only object 
expressed by the input views. Based on the algorithm described above, a solid model generation program 
was developed on VAX 11/750. The program is comprised of six parts which correspond to the six steps 
of the algorithm. It generates solid models expressed in a winged edge data structure from the data of 
three ortho- graphic views. Though the focus of this research is the gen- eration of solid models, a 
graphic program was also developed, using MEGATEK 7200 connected to the VAX II/750, to allow the interactive 
input of ex- isting drawings via a digitizer and to allow the visual verification and the selection of 
the gen- erated solid models. As the solid model generation program was incorporated into this graphic 
program, all the work from the input of drawings to the selection of the results can be done interactively. 
 Solid models composed of planar, cylindrical, conical, toroidal and spherical bounding surfaces have 
been generated from the data describing their three orthographic views containing both lines and circular 
arcs. Figures 9, 10 and 11 show some examples. Twenty to thirty seconds of VAX 11/750 CPU time were 
required to generate each object. 5. CONCLUSION The method described here is more practical and useful 
than existing methods for the following reasons. I. Circular arcs in input views: Most draw- ings of 
mechanical parts contain circular arcs. The method described here is the only one which allows circular 
arcs in input views. 2. Minimum input requirement: This method imposes less requirement on input than 
existing methods. For example, there is no requirement on the order of input or the labeling of the out- 
put. The system based on the algorithm discussed here can serve as an interface between wire-frame CAD 
system and solid model CAD system. The system can be also used directly for the purpose of the input 
of solid models. To make the system more practical, a number ofproblems have to be solved. In engineering 
 drawings, there can be more or less than three projections for an object, partial projections rather 
than full projections may appear in some views, auxiliary views and/or sectional views may be given. 
It is believed that the algorithm described here can be extended to take care of the problems above, 
and the extension is under study. ACKNOWLEDGEMENT The authors gratefully acknowledge the finan- cial 
support of the Control Data Corporation and the Nissan Motor Company.  REFERENCES D] r@ [3] [4] [5] 
 [71 [8] [9] I.E. Sutherland, "Three-Dimensional Data Input by Tablet", Proc. IEEE, Special Issue 
on Com- puter Graphics, Vol. 62, No. 4, 1974. R.W. Thornton, "Interactive Modelling in Three Dimensions 
through Two-Dimensional Windows", Third International Conference and Exhibition on Computer in Engineering 
and Building Design, 1978.  M. Idesawa, "A System to Generate a Solid Figure from a Three View", Bull. 
JSME 16, pp. 216-225, February 1973.  M. Idesawa, et al, "Automatic Input of Line Drawing and Generation 
of Solid Figure from Three-View Data", Proceedings of the Interna- tional Joint Computer Symposium, pp. 
304-31|,  1975. G. Lafue, "Recognition of Three Dimensional Objects from Orthographic Views", Computer 
Graphics, Vol. I0, No. 2, 1976. K. Preiss, "Constructing the 3-D Representation of a Plane-Faced Object 
from a Digitized Engineering Drawings", Fifth International Conference and Exhibition on Computer in 
Engineering and Building Design, 1980. G. Markowsky and M.A. Wesley, "Fleshing Out Wire Frames", IBM 
J. Res. Develop. 24, pp. 582-587, September 1980. G. Markowsky and M.A. Wesley) "Fleshing Out Projections", 
IBM J. Res. Develop. 25, pp. 934-954, November 1981. H. Sakurai, "Solid Model Input Through Orthographic 
Views", M.S Thesis, Massa- chusetts Institute of Technology, Cambridge, Massachusetts, 1982. ES ELS, 
EAT FIGURE ; Candidate FIGURE 2 ~KS Standard edge EAS Silhouette edge Tangency edge SILHOUETTE AND 
TANGENCY EDGES I , 1 I I I I 4 " t I I I t I I I p-vertices with the attribute of Standard x Tangency 
o Silhouette A Tangency-generated  Silhouette-generated EXAMPLES OF CANDIDATE P-VERTICES 'i~%~ ..... 
i~ ~ .,'" ~__~"..", ........ !i "-.. .... ~. ..'" "" .~" : ..... '-" ", ~e, ",'~-t.-"""..4 ,/ "'":x'" 
..-":: ...... :.; :: /-......... --..><................ /-/ ~.:~ " ........... ...-.....- "--..  ~" 
z C-vertices with the attribute of Standard o Silhouette Double-silhouette Silhouette and tangency 
A Double-silhouette and tangency x ~orus-silhouette and tangency FIGURE 3 C-VERTICES /. I/ //,  
,, i ES Standard e-edge t , ELS Line-silhouette c-edge ' EAS Arc-silhouette c-edge A ! EAT Arc-tangency 
c-edge FIGURE 4 C-EDGES  Computer Graphics (a)  (b)  L I L2 L3 L4 L5 LI L.2" L3 (c) I L4 J5 (d) 
 FIGURE 5 C-EDGE GENERATION: (a) c-edges on a plane; (b) c-edge loops; (c) hier- archical tree of c-edge 
loops; (d) c-faces generated FIGURE 6 C-FACES Volume 17, Number 3 July 1983 c-fac~~ c-face l face connection 
angle When c-face ] encloses the hatched side, c-face 2 is connected, otherwise c-face 3 is connected. 
 FIGURE 7 C-FACE CONNECTION jf"~ .\ FIGURE 8 C-FACE CONNECTION FIGURE 9 EXAMPLE E J I i l I I 
I I I FIGURE ]0 EXAMPLE 2 i I I I I I I I I I I I I I J F I I I I I I I I J II, I,I .I I i I I I FIGURE 
I) EXAMPLE 3  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801156</article_id>
		<sort_key>253</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>31</seq_no>
		<title><![CDATA[Spatial input/display correspondence in a stereoscopic computer graphic work station]]></title>
		<page_from>253</page_from>
		<page_to>261</page_to>
		<doi_number>10.1145/800059.801156</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801156</url>
		<abstract>
			<par><![CDATA[<p>An interactive stereoscopic computer graphic workspace is described. A conventional frame store is used for three-dimensional display, with left/right eye views interlaced in video and viewed through PLZT shutter glasses. The video monitor is seen reflected from a half silvered mirror which projects the graphics into a workspace, into which one can reach and manipulate the image directly with a &#8220;magic wand&#8221;. The wand uses a magnetic six degree-of-freedom digitizer. In an alternative configuration, a graphics tablet was placed within the workspace for input intensive tasks.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Stereoscopic display]]></kw>
			<kw><![CDATA[half silvered mirror]]></kw>
			<kw><![CDATA[three-dimensional digitization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Stereo</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P47905</person_id>
				<author_profile_id><![CDATA[81100444749]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schmandt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Architecture Machine Group, Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Burton, R.P. and Sutherland, I.E. Twinkle box: a three-dimensional computer input device. NCC 1974, pp. 513-520.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Callahan, M.A. A 3-D display head set for personalized computer, M.S. Thesis, Dept. of Arch., MIT 1983.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360329</ref_obj_id>
				<ref_obj_pid>360303</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Clark, J.H. Designing surfaces in 3-D. Comm. ACM 19,8 (Aug. 1976) pp. 454-460.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Fisher, S.S. Viewpoint dependent imaging: an interactive stereoscopic display, M.S. Thesis, Dept. of Arch., MIT 1981]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Fuchs, H., Pizer, S.M., Tsai, L.C., Bloomberg, S.H., and Heinz, E. R., Adding a true 3-D display to a raster graphics system. IEEE Computer Graphics and Applications, Sept. 1982. pp. 73-78.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>22140</ref_obj_id>
				<ref_obj_pid>22112</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Ginsberg, C.M. and Maxwell, D. Graphical marionette. Proceedings, SIGGRAPH/SIGART interdisciplinary workshop, ACM. April, 1983 pp. 172-179.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Knowlton, K.C., Computer displays optically superimposed on input devices. Bell Syst. Tech. J. 56:3 (1977).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Lippman, A. And seeing through your hand. Proceedings of the SID, 22:2 (1981) pp. 103-107.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Littlefield, R.J., Stereo and motion in the display of 3-D scattergrams. Proc., Engineering Society of Detroit Computer Graphics Conference, 1982.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Okoshi, T. Three Dimensional Imaging Techniques. Academic Press, N.Y. 1976.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Nisselson, J. A model kit: a system for constructing three-dimensional interactive graphic models. M.S. thesis, Dept. of Arch., MIT 1983.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Polhemus Navigational Sciences, Inc. P.O. Box A, Essex Junction, VT.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Rabb, F.H., Blood, E.B., Steiner, T.O., and Jones, H.R. Magnetic position and orientation tracking system. IEEE Trans. on Aerospace and Electronic Systems Sept. 1979. pp 709-718.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Roberts, L.G. The Lincoln wand. MIT Lincoln Laboratories Report, Lexington, MA. 1966.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807423</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Roese, J.A. and McCleary, L.E., Stereoscopic computer graphics for simulation and modeling, Proc. SIGGRAPH 13:2 (1979). pp. 41-47.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Schmandt, C.M., Interactive three-dimensional computer space, Proc., SPIE conference on processing and display of three-dimensional data, Aug. 1982 (in publication).]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>906408</ref_obj_id>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Vickers, D.L. Sorcerer's apprentice: headmounted display and wand. Ph.D. thesis University of Utah, Dept. Elec. Eng:, 1974]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Woltring, H.J. and Marsolais, E.B. Optoelectric (Selspot) gait measurement in two- and three- dimensional space, a preliminary report. Bulletin of Prosthetics Research 17:2 (fall, 1980). pp. 46-52]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SPATIAL INPUT/DISPLAY CORRESPONDENCE IN A STEREOSCOPIC COMPUTER GRAPHIC WORK STATION Christopher Schmandt 
 Architecture Machine Group Massachusetts Institute of Technology ABSTRACT An interactive stereoscopic 
computer graphic workspace is described. A conven- tional frame store is used for three- dimensional 
display, with left/right eye views interlaced in video and viewed through PLZT shutter glasses. The video 
 nonitor is seen reflected from a half silvered mirror which projects the graphics into a workspace, 
into which one can reach and manipulate the image directly with a "magic wand". The wand uses a magnetic 
six degree-of-freedom digitizer. In an alternative configuration, a graphics tablet was placed within 
the workspace for input intensive tasks. CR catagories: 1.3.2 [Computer Graphics]: Graphics Systems; 
1.3.6 [Computer Graphics]: Methodology and Techniques; 1.4.8 [Image Processing]: Scene Analysis- Stereo. 
 Key Words: Stereoscopic display, half silvered mirror, three-dimensional digi- tization. INTRODUCTION 
 The last decade has seen a resurgence of interest in three-dimensional imaging, such as holography, 
commercial 3-D movies, experimental anaglyphic television broad- casts, and lenticular 35mm cameras. 
Some of this enthusiasm has overflowed into computer graphic applications, most note- ably computer aided 
design, medical imag- ing, and seismic exploration, but wide- spread appreciation of the utility of stereoscopic 
displays has yet to happen. This may be in part due to the lack of convincing interactive capabilities 
of such displays. The fields with the most interest in three-dimensional display are those with a need 
to analyze complex spatially struc- tured real world data, gathered by or part of a process requiring 
expensive data acquisition hardware (hence the willing- ness to experiment with relatively expen- sive 
displays). Although a variety of techniques exist for stereoscopic display, there has been little work 
done in inter- acting with 3-D data by 3-D modes: using motion parallax to look around objects [Fisher, 
Littlefield], peeling away inter- fering data layers, or constructing addi- tional data types in image 
space itself. Permission to copy without fee all or part of this material is granted provided that the 
copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the 
title of the publication and its date appear, and notice is given that copying is by permission of the 
Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific 
permission. &#38;#169; ACM 0-89791-109-1/83/007/0253 $00.75 Figure i. A cutaway view of the work station, 
showing the view through the half silvered mirror of the video monitor and user's hand.  graphics with 
reality. The various head mounted displays attempted to place graphics in known positions in the environ- 
ment with mirrors mounted immediately in front of the viewer's eyes [Vickers, Callahan]. Larger mirrors 
had been used to overlay graphics onto keyboards and tablets [Knowlton, Lippman]. Our task was to extend 
this concept into three-space. A 19 inch color CRT is mounted at a 45 degree angle above and in front 
of the operator's head, and viewed through the half-silvered mirror parallel to the floor (figure 6). 
Thus the reflected graphics appear to lie in the space in front of the operator and below the mirror. 
This space below the mirror is painted black and illuminated, so he can see his hand in the midst of 
the image. In some configur- ations this lighting was dimmed under computer control. The following 
sections will describe several early applications for which this workspace was used, with the wand as 
input. Despite the optimism of our early reports [Schmandt], various limitations interfered with fully 
satisfactory interaction. After discussing these flaws, and our attempts to remedy them, we will present 
 a second generation of applications which matched the capability of this configura- tion more closely. 
More recent work has explored alternative input schemes, using a graphics tablet. IMAGE CREATION 
3-D Paint Early work was aimed at developing spa- tial correspondence algorithms in the context of 
a 3-D paint program. Menu buttons at the front edge of the work station allowed color selection; when 
the user pressed the button in the wand, colored paint would appear in space, at the location of the 
white spot on the end of the wand (figure 7), dribbling out as, the wand was moved. If the positions 
of the graphic and wand were well correlated, no cursor would be necessary, as the white spot on the 
wand is itself a real world "cursor". Several calculations determine the posi- tion of the paint. 
First, the digitizer's location, in its own coordinate system, must be rotated, translated, and scaled 
 into the screen coordinate system (actually, that of the projected image of the screen). This transformation 
matrix is a function of the user's point of view, so it was derived from a calibration step at the 
beginning of a session. Next, the line of sight from the eye to the dig- itizer is extended to intersect 
the pro- jected screen. This gives an (x,y) coordinate in screen space (that of the frame buffer, 
except that x is mirrored). The z coordinate is used to determine the left/right disparity with which 
to draw the graphic at that point. In practice, this correspondence works excellently most of the 
time and the paint actually appears to flow from the end of the wand, provided it is not moved quickly 
(see below). One can draw lines, spirals, knots, etc.; nearer objects obscure more distant ones, but 
in turn draw behind still closer lines. Depth perception adjuncts Early in this stage we discovered 
that only an experienced viewer could reliably and comfortably see the proper depth in the PLZT video 
image. Purely binocular convergence is missing many strong depth cues such as shadows, textures, perspec- 
 tive, and motion parallax. Of course, hidden surface obscuration provides an obvious depth perception 
aid, but further cues ease perception greatly. In all demonstrations, hidden surface removal is done 
in real time during image generation using false color and arithmetic frame buffer write operations. 
Associated with each of the 64 discrete depth levels are several slots in the color table, one for each 
of the eight displayable colors. Distance is encoded in the high order bits of color, increasing farther 
away from the viewer; while the low order bits determine which particular color is displayed. Graphics 
are added to the screen by writ- ing the appropriate value (a function of color and distance) into 
the frame buffer under a minimum function; the lesser (i.e., closer) of the new value and current value 
gets written into the display. When additional depth cues are provided, depth perception is significantly 
facili- tated. The brightness and size of lines decrease as they are drawn farther away, as exaggerated 
perspective and lighting effects. We noted that although stereo- scopic depth perception was improved 
with these adjuncts, they did not work well enough in isolation to render the conver- gence cues redundant; 
rather convergence, obscuration, luminance and size all worked together to give a strong three-dimensional 
 sense. VLSI design station These results were encouraging enough to move toward a more practical 
demonstration, using the wand to input vertices of a 2 dimensional polygon database. Such data- bases 
were later used to generate anima- tion on a write-once optical videodisc, for viewpoint dependent 
image work [Fisher]. For a practical application, we chose to work with the graphic component of a 
VLSI integrated circuit design sta- tion. Integrated circuits consist of approxim- ately rectilinear 
components ordered spatially in a laminar manner, with much information contained in how such layers 
overlap spatially. This content lends itself well to the 2- D environment that is suited to real-time 
frame buffer work. As we were concerned with only the graphic component, VLSI designs of varying complex- 
ity were selected from textbooks, and entered by several students using the work- space and wand. INTERACTION 
LIMITATIONS After initial enthusiasm over the ability to accurately track the wand graphically in three 
space wore off, we were con- fronted with several aspects of the work- space which detracted from performance 
in the chosen applications. Magnetic interference The most disturbing aspect of this hard- ware configuration 
is magnetic interfer- ence with the Polhemus digitizer caused by the color CRT. Magnetic fields from 
the monitor disturb the digitizer in two ways: the normally Cartesian digitizer coordin- ate space 
is badly warped, and noise is introduced. As the magnetic space warping is tempor- ally constant, 
it was sufficient to der- ive a single mapping from the warped to a Cartesian three space. This was 
done by hand mapping magnetic contour lines in the workspace, and approximating them by circular sections. 
Such an approach works reasonably well except at extremes of the active graphical volume. The unwarped 
 coordinates are then transformed, in soft- ware, by a transformation matrix that is generated before 
each session by having the user touch a succession of displayed dots. Digitizer noise is induced by 
strong mag- netic field components at the 15.75 kHz horizontal video frequency. Despite the addition 
of a hardware notch filter to attenuate this frequency in the preamp- lifier, noise remains excessive 
with the close proximity of monitor and radiator in the workspace. We placed an ordinary aluminum window 
screen across the face of the monitor (it can be seen in some of the photographs), which reduces noise 
to a level such that software filtering could generate useful data. This additional filtering has the 
unfor- tunate effect of introducing a lag, or lack of responsiveness, into the system, with the obvious 
loss of interactivity. We partially compensated for this with an adaptive filter, which uses a larger 
 sample window when the sensor is nearer the CRT, and hence more noise prone, but remain disappointed 
with response time. Depth judgement As already mentioned, exaggerated drop off in brightness and premature 
foreshortening were early additions to aid depth percep- tion, an indication that purely binocular depth 
cues may be insufficient. Although this enhances perception of the relative distance of pieces of the 
image, it remains very demanding to properly deter- mine the distance of one's hand relative to the 
image. Accurate distance judgement is crucial in an input task in which dis- crete objects may be logically 
required to lie on coincident planes, or when relative depth between planes conveys needed infor- 
mation. A partial explanation of this difficulty is that although image components may obscure each 
other, they are always vis- able through the user's hand. We attempted, not particularly successfully, 
to alleviate this problem by allowing computer control of the workspace lighting. When the wand was moved 
farther away, the lights were dimmed to allow nearer, and hence brighter, components of the image to 
obscure the hand; unfortunately, the light dimmer response time was poor and the overall effect novel 
but unconvincing. Drawing difficulties Most users of the VLSI design station com- plained of the difficulty 
experienced in trying to draw planar objects floating in space because of the lack of arm support 
and limited hand stability. It was nearly impossible to repeatably draw rectilinear objects with proper 
right angled corners, and required great care to insure that all vertices of a polygon lay on the same 
z plane, aggravated by the depth perception difficulty. Several improvements were tried. An invisible 
grid of variable resolution is used to force input points to land on lower resolution (x,y) coordinates, 
making input of orthogonal line segments easier. Sub- sequent vertices were automatically forced to 
lie in the plane of the initial vertex, a usefual aid only if the first vertex is indeed at the proper 
depth. Although these additions are in fact helpful, most users find complex input tasks difficult and 
tedious. NEW DIRECTIONS From the work with VLSI design graphics, several new directions emerged due 
to recognition of the above constraints. User response had indicated that although the spatial correspondence 
between graphics and the wand was a simple and intuitive mapping, the arrangment was unsuitable for 
tasks highly oriented toward the input of data. One follow on approach explored the useful- ness of 
the same work station configuration for tasks more oriented toward interaction with or manipulation 
of previously input or generated data. The other path focused on overcoming input limitations using a 
graphics tablet instead of the 3-D digit- izer. Both approaches retained the ster- escopic video display 
viewed through the half-silvered mirror. Data manipulation Efficient graphical input requires a stable 
and responsive digitizing technology; these requirements were mutually exclusive in the workspace, with 
the wand so close to the monitor. Accuracy could be traded off for speed by modifying the software filter, 
suggesting use of the wand as a target selection, rather than positioning, device. In the case of complex 
3-D data, one may well need to manipulate or highlight the data in order to understand it; interactive 
graphics should assist this. One of the obvious complexities with a three- dimensional database is the 
variety of ways objects can obscure each other. Tools are needed to take scenes apart, rearrange them, 
or accent local volumes for more detailed analysis or separation. A fairly ambitious implementation 
of these concepts was attempted in a "Model Kit" project [Nisselson], which allows the rearrangement 
of 3-D scenes composed of pieces of small, digitized pictures of architectural landmarks, medical images, 
or microscopy slides as primitives (figure 9). Using the wand in the same work sta- tion, the user 
can move or remove objects, call up new primitives from the photo- graphic database, add text, and 
save periodic "snapshots" of work in progress. Many prior limitations of the wand as an input device 
are avoided by this approach. As it is used much more often to "acquire" one target from many in the 
scene, highly accurate positioning is less crucial, and response is quickened by tolerating more noise 
with less filtering. Tablet input An alternative input technology was needed for such tasks as building 
the polygonal components of VLSI design. We mounted a conventional graphics tablet, painted black to 
avoid image crosstalk, inside the workstation at a 45 degree angle to the floor, such that the image 
of the CRT screen coincided with the plane of the tablet (figure i0). We desired to retain as much spatial 
correspondence as possible, with an intuitive 3-D interaction. The tablet is used to input data only 
on the plane defined by its surface. Access to the complete visual volume is provided by moving the 
entire image towards or away from the viewer, under control of a soft "slider" selection graphic, until 
the appropriate image slice corresonds to the tablet plane. The whole image moves in and out such that 
one is always working on a level currently displayed with zero dis- parity (figure ii). Moving the 
entire image in the Z direction is done by shifting the horizontal posi- tion of the odd scan line 
graphics with respect to the even lines. This is done by two methods. The odd scan lines can be read 
out of the frame buffer, then written back with the appropriate new x offset. A coherence table indicating 
the left and rightmost extent of each line is maintained to minimize the pixel data transfer necessary. 
 A faster method utilizes two complete frame buffers, one for the left eye view and the other for the 
right eye view. The two images are mixed into one signal in the video domain by the same electronics 
 that detect interlace to control the glasses; the vertical interval detector selects alternate fields 
from two video sources. With this configuration, the image can be moved very quickly simply by a horizontal 
pan by one of the frame buffers. In addition to greatly increasing the speed with which the image 
can be shifted in and out, this technique can allow faster image generation and manipulation. Instead 
of breaking the graphics into an even and odd line portion, preventing use of conventional internal 
frame buffer primitives (such as "line", "rectangle", etc.), each frame buffer maintains a separate 
complete image. Left and right eye views can be generated in parallel using the processors in the frame 
stores. Tablet input has been used in a variety of styles. The original VLSI design task lends itself 
quite well to this method, allowing easy input of rectilinear graphic components while retaining needed 
 stereoscopic viewing. Other work will use the tablet to input vertices of 3-D solids, built and edited 
as wire frame models. Once the final design is satis- factory, slower shaded surface algorithms generate 
solid model views of these objects for animations. The higher speed, high resolution, and noise immune 
tablet data is much more suited to these graph- ically detailed tasks than the wand. CONCLUSIONS A 
stereoscopic video display lends itself well to interactive computer graphics, literally opening up 
a new dimension of graphical control. Elegant input tech- niques require careful thought to provide 
 the tools of real use, however. All work described in this paper exploits the spatial mapping of the 
graphic display into or over the work area, an arrangement which was found to provide intuitive access 
to the third dimension. This was without a doubt the most exciting element of the project. Our work 
certainly sug- gests that the input configuration and support software will need to be developed around 
requirements of each individual application, and indeed a task area may require several configurations, 
some for input and others for image manipulation. ACKNOWLEDGEMENTS As with many projects at the Architecture 
Machine Group, many people participated in various aspects of this work. Topping the list are Jim Zamiska, 
who wrote the most difficult early software, and Scott Fisher for the optics and convergence algorithm. 
Without their work this project would have never been more than an idea. Eric Hulteen built the workstation. 
Additional proqram- ming was done by David Mellinger, Jane Nisselson, Charles Simmons, and Carol Yao. 
Photos are by Scott Fisher and Barry Arons; Carol Boemer drew the first illustration. Kathy Coppola assisted 
with production. Orlen Rice of Honeywell graciously loaned the Laboratory our first pair of PLZT glasses, 
where it all began. This project was funded by Atari, Inc. and the Defense Advanced Research Projects 
Agency, Cybernetics Technology Division, under contract MDA 903-81-C-0097. REFERENCES Burton, R.P. 
and Sutherland, I.E. Twinkle box: a three-dimensional computer input device. NCC 1974, pp. 513-520. 
Callahan, M.A. A 3-D display head set for personalized computer, M.S. Thesis, Dept. of Arch., MIT 1983. 
 Clark, J.H. Designing surfaces in 3-D. Comm. ACM 19,8 (Aug. 1976) pp. 454-460. Fisher, S.S. Viewpoint 
dependent imaging: an interactive stereoscopic display, M.S. Thesis, Dept. of Arch., MIT 1981 Fuchs, 
H., Pizer, S.M., Tsai, L.C., Bloomberg, S.H., and Heinz, E. R., Adding a true 3-D display to a raster 
graphics system. IEEE Computer Graphics and Appli- cations, Sept. 1982. pp. 73-78. Ginsberg, C.M. and 
Maxwell, D. Graphical marionette. Proceedings, SIGGRAPH/SIGART interdisciplinary workshop, ACM. April, 
 1983 pp. 172-179. Knowlton, K.C., Computer displays optical- ly superimposed on input devices. Bell 
Syst. Tech. J. 56:3 (1977). Lippman, A. And seeing through your hand. Proceedings of the SID, 22:2 (1981) 
pp. 103-107. Littlefield, R.J., Stereo and motion in the display of 3-D scattergrams. Proc., Engineering 
Society of Detroit Computer Graphics Conference, 1982. Okoshi~ T. Three Dimensional Imaging Techniques. 
Academic Press, N.Y. 1976. Nisselson, J. A model kit: a system for constructing three-dimensional interactive 
graphic models. M.S. thesis, Dept. of Arch., MIT 1983. Polhemus Navigational Sciences, Inc. P.O. Box 
A, Essex Junction, VT. Rabb, F.H., Blood, E.B., Steiner, T.O., and Jones, H.R. Magnetic position and 
orientation tracking system. IEEE Trans. on Aerospace and Electronic Systems Sept. 1979. pp 709-718. 
 Roberts, L.G. The Lincoln wand. MIT Lincoln Laboratories Report, Lexington, MA. 1966. Roese, J.A. 
and McCleary, L.E., Stereo- scopic computer graphics for simulation and modeling, Proc. SIGGRAPH 13:2 
(1979). pp. 41-47. Schmandt, C.M., Interactive three- dimensional computer space, Proc., SPIE conference 
on processing and display of three-dimensional data, Aug. 1982 (in publication). Vickers, D.L. Sorcerer's 
apprentice: headmounted display and wand. Ph.D. thesis University of Utah, Dept. Elec. Eng:, 1974 Woltring, 
H.J. and Marsolais, E.B. O~t0- electric (Selspot) gait measurement in two-and three- dimensional space, 
a preliminary report. Bulletin of Prosthe- tics Research 17:2 (fall, 1980). pp. 46-52  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801157</article_id>
		<sort_key>263</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>32</seq_no>
		<title><![CDATA[<italic>Three dimensional computer graphics for craniofacial surgical planning and evaluation</italic>]]></title>
		<page_from>263</page_from>
		<page_to>273</page_to>
		<doi_number>10.1145/800059.801157</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801157</url>
		<abstract>
			<par><![CDATA[<p>The understanding of complex craniofacial deformities has been aided by high resolution computed tomography. Nonetheless, the planar format limits spatial comprehension. Reconstruction of fully three-dimensional bony and soft tissue surfaces from high resolution CT scans has been accomplished by a level slicing edge detector coupled to a hidden surface processor without perspective depth transformation. This method has clarified aberrant anatomy, facilitated surgical planning and improved quantitative postoperative evaluation in more than 200 clinical cases. Advanced computer aided design techniques, originally developed for the manufacture of military aircraft, have been applied to the planning and evaluation of craniofacial procedures as well. This allows the application of interactive digital graphic technology to surgical patient management.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Scanning</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Health</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor>Computer-aided design (CAD)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010472.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Architecture (buildings)->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010449</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Health informatics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010446</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Consumer health</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P199144</person_id>
				<author_profile_id><![CDATA[81452612125]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Vannier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mallinckrodt Institute of Radiology, Washington University School of Medicine, 510 South Kingshinghway Boulevard, St. Louis, Missouri]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P137848</person_id>
				<author_profile_id><![CDATA[81100322834]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jeffrey]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Marsh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cleft Palate and Craniofacial Deformaties Institute, St. Louis Children's Hospital, Washington University School of Medicine, St. Louis, Missouri]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P331265</person_id>
				<author_profile_id><![CDATA[81332534511]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[O.]]></middle_name>
				<last_name><![CDATA[Warren]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[McDonnell Douglas Aircraft Company, P.O. Box 516, St. Louis, Missouri]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kawamoto H. The kaleidoscope world of rare craniofacial clefts: Tessier classification. Clinics in Plastic Surgery 1976; 3:529-572.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Artzy E, Frieder G, and Herman GT. The theory, design, implementation and evaluation of a three-dimensional surface detection algorithm. Comput Graph &amp; Image Proc 1981; 15:1-24.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Herman GT Image reconstruction from projections. New York: Academic Press, 1980:260-176.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Nagao M, Matsuyama T. Edge preservation smoothing. Proc International Joint Conference Pattern Recognition, Kyoto, Japan, 1978;IV:518-520.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Rabiner LR, Sambur MR, Schmidt CE. Applications of a nonlinear smoothing algorithm to speech processing. IEEE Trans. Acoust. Speech and Signal Proc. 1975; 23:552-557.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>321609</ref_obj_id>
				<ref_obj_pid>321607</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Akima H. A new method of interpolation and smooth surface fitting based on local procedures. JACM 1970; 17:589-602.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Fu KS, Mui JK. A survey on image segmentation. Pattern Recog 1981; 13:65-77.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>539567</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Huang TS, ed. Two-dimensional digital signal processing II. Transforms and median filters. Springer Verlag, Berlin, 1981, 3-7 and 161-218.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Vannier M, Marsh JL, Warren JO and Barbier J. Craniofacial disorders. Diagnostic Imaging, (5)3, 36-43.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Marsh JL and Vannier MW. The third dimension in craniofacial surgery. Plast and Reconstr Surg 71:June, 1983.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Vannier MW, Gado MH and Marsh JL. Three dimensional display of intracranial soft tissue structures, American Journal of Neuroradiology, May, 1983.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Merz B. Computer guides facial reconstruction. Journal AMA, 18 March 83, (249) 11, 1409-15.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Clayton D. Craniofacial CT. Computers in Medicine, (12), Feb. 1983, 3-4.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Vannier MW, Marsh JL, Warren JO and Barbier J. Three dimensional CAD for craniofacial surgery. Electronic Imaging, in press.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Doelling AE. CAD/CAM Computer Aided Technology, Engineering Communications Report, McDonnell Douglas Aircraft Co., 26pp.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 THREE DIMENSIONAL COMPUTER GRAPHICS FOR CRANIOFACIAL SURGICAL PLANNING AND EVALUATION Michael W. Vannier, 
M..D. I Jeffrey L. Marsh, M.D. 2 James O. Warren, B.S.A.E. 3 I Mallinckrodt Institute of Radiology, Washington 
University School of Medicine, 510 South Kingshighway Boulevard, St. Louis, Missouri 63110. 2Cleft Palate 
and Craniofacial Deformities Institute, St. Louis Children's Hospital, Washington University School of 
Medicine, St. Louis, Missouri 63110. 3McDonnell Douglas Aircraft Company, P.O. Box 516, St. Louis, Missouri 
63166. Abstract The understanding of complex craniofacial deformities has been aided by high resolution 
computed tomography. Nonetheless, the planar format limits spatial comprehension. Reconstruction of fully 
three-dimensional bony and soft tissue surfaces from high resolution CT scans has been accomplished by 
a level slicing edge detector coupled to a hidden surface processor without perspective depth transformation. 
This method has clarified aberrant anatomy, facilitated surgical planning and improved quantitative postoperative 
evaluation in more than 200 clinical cases. Advanced computer aided design techniques, originally developed 
for the manufacture of military aircraft, have been applied to the planning and evaluation of craniofacial 
procedures as well. This allows the application of interactive digital graphic technology to surgical 
patient management. CR Categories and Subject Descriptors: 1.3.0. (Computer Graphics): General; 3.6 (Computer 
Aided Engineering) -computer aided design (CAD); 3.3 (Life and Medical Sciences) -Biology; Health. General 
Terms: Computer aided design, computer graphics, craniofacial surgery, CT scan. INTRODUCTION Craniofacial 
surgery has developed over the past 15 years as a discipline capable of habilitating individuals with 
major deformities of the head (1). This discipline is based on the premise that surgical correction of 
aberrant skeletal anatomy must precede attempts at soft tissue reconstruction for normalization of craniofacial 
features and functions. Thus, the planning and evaluation of such operations is dependent on radiologica] 
imaging for defining the underlying bony structures and their relationship to overlying soft tissue contours. 
Traditionally this planning has been accomplished using acetate tracings of cephalometric Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by permission of the Association for Computing 
Machinery, To copy otherwise, or to republish, requires a fee and/or specific permission. &#38;#169; 
ACM 0.89791-109-1/83/007/0263 $00.75 skull radiographs in the AP and lateral projections (Fig. 1). Recently, 
the availability of high resolution CT scanners has altered our approach to the study of complex craniofacial 
anomalies. We now routinely obtain, pre- and postoperatively, a set of equi-spaced, nonoverlapping, high 
resolution CT scans of the face with narrow collimation. This set of scans is augmented by reformatting 
sagittal, coronal and oblique planar reconstructions to demonstrate aberrant anatomy and facilitate surgical 
planning and evaluation. Unfortunately, the complex structural anomalies the surgeon encounters are not 
completely explained by planar reconstructions. These malformations are 3-dimensional and no purely 2-dimensional 
reconstruction will suffice. To date, the surgeon has had to imagine the 3-dimensional projection, based 
on a sequence of 2-dimensional images (CT axial scans and reformatted planar views) supplied by the radiologist. 
Some preliminary experiments in 3-dimensional reconstructions from CT scans have been performed by others 
(2,3). In general, these methods are quite complex9 time consuming and require computer equipment that 
is not available in most x-ray departments. These methods may involve three-dimensional edge detection 
(2) or encoding and display from a 3-dimensional geometrical model derived from the original scans (3). 
These steps imply a computational burden that is orders of magnitude greater than desired and storage 
requirements that exceed reasonable limits for 3-dimensional reconstructions with acceptable resolution. 
We have developed a new computer method which reconstructs three dimensional bone and soft tissue surfaces 
given a series of high resolution CT scans of the face. This method, implemented as a series of computer 
programs, is efficient in both computation time and storage requirements and can be added with modest 
effort to virtually any CT scanner. We have applied these programs to sets of scans from more than 200 
patients with congenital and acquired craniofacial abnormalities. The surface reconstructions of bone 
and soft tissue contours has been helpful for understanding and interpretation of the abnormalities under 
study.  MATERIALS AND METHODS CT Scans High resolution CT scans of facial structures were obtained using 
an unmodified commercially available instrument (Siemens Somatom 2). Each study consisted of between 
30 and 64 non-overlapping sections obtained at 2ram intervals with 2ram collimation (Fig.2). A Computer 
Graphics Volume 17, Number 3 July 1983 256X256 reconstruction matrix was used. To better delineate the 
facial structures, only the frontal half of the subject's skull was included in the original transverse 
transaxial CT images. Young subjects were sedated during scanning to minimize motion and improve the 
registration of the scan sequence. The reconstructed scans were recorded on double- density floppy diskettes 
and transferred to an independent CT scan viewing console (Siemens Evaluscope RC, Fig. 8). The image 
data was copied to a large capacity (28 Mbyte) cartridge disk (Digital Equipment Corp., Maynard, Mass., 
Model RK07) integrated in the viewing console. Routine planar reconstructions in the sagittal, coronal 
and oblique orientations were obtained. Computer Programs A set of computer programs was specifically 
developed for 3-dimensional surface reconstruction from sequences of high resolution CT scans. These 
programs were written in the Fortran IV language to operate on a minicomputer (Digital Equipment Corp., 
Maynard, Mass., Model PDP-I1/30, Figure 8) incorporated in the CT viewing console. These programs will 
operate without modification on the CT scanner (Siemens Somatom 2) as well. The programs accept the sequence 
of transaxial high resolution CT scans as input and produce a set of 3- dimensional surface reconstruction 
images as output. Algorithm The surface reconstruction computer program reads each of the original high 
resolution CT scans (Fig.3A) in sequence from the cartridge disk and loads the image into the 256X256 
display memory of the evaluation console (or CT scanner). Since the scans are obtained with the patient 
supine, in passing from the top of each image to the bottom, we encounter air, skin, facial bones and 
finally intracranial contents. Each column of the image is read in sequence from left to right. The soft 
tissue contour (Fig. 3B) is extracted by comparing each value of CT density in the column from top to 
bottom of the image with a preset threshold. (This threshold represents the CT attenuation that distinguishes 
air from soft tissue (nominally -100 CT units for our scanner). The column index where an air to soft 
tissue transition occurs is used to form the soft tissue contour for each CT scan slice. This index value 
is scaled and output to the cartridge disk as a line in the reconstructed soft tissue 3-dimensional surface 
image. In a similar manner, each column of the CT scan slice is examined from top to bottom and compared 
with a bone threshold (+170 CT units nominally). The set of scaled indices of soft tissue to bone density 
transitions detected in this manner is the bone contour (Fig. 3C). The bone contour is written to a separate 
file on the cartridge disk. During the contour extraction process, the detected frontal contours are 
written on the evaluation console display as a quality control measure (Fig. 3D). In order to complete 
the surface reconstructions, each succeeding CT scan is loaded into display memory, bone and soft tissue 
contours extracted, and results (Fig.teA-D) written to the cartridge disk. In order to facilitate the 
display and handling of surface reconstruction files, these 3-dimensional images were written on the 
cartridge disk in the same format as the original CT scans. These 3-dimensional reconstructions can be 
viewed and manipulated in the same manner as any CT scan. The relationship between pairs of soft tissue 
and bone reconstructed surfaces is maintained and comparisons of soft tissue thicknesses may be directly 
computed as their difference, for example. The gray scale values in the surface reconstructions represent 
actual distances, and within the geometrical resolution of the CT scanner, can be used for rectilinear 
measurements. Smoothing and Interpolation In order to obtain an accurate surface image, the extracted 
contours are replicated several times in the output file so the vertical sample interval is equal to 
the horizontal interval. For example, when the pixei size in the raw CT scans is 0.Smm square, four lines 
must be replicated for each contour extracted to produce correct vertical spacing given 2ram non-overlapping 
slice thickness. Using a 256X256 matrix, this means that up to 64 sequential CT slices may be included 
in each surface reconstruction. The replication of contours produces undesirable effects with a coarse 
"digital" appearance that is particularly pronounced at obliquely oriented edges, such as the lateral 
orbital margins (Fig. irA,B). Smoothing programs have been developed to reduce this effect based on local 
averaging or median filtering. The median filter (t~,5,8) is especially useful since significant edges 
are preserved while artifacts are reduced (Fig. t~C,D). There is no information on the behavior of the 
extracted surfaces between the contours, and we may also expect the presence of some motion and partial 
volume effects in the original set of CT scans. The appearance of the surface reconstructions may be 
improved by intergolating between the extracted contours using a bilinear interpolator or smooth surface 
fitting based on local procedures (6). Both of these methods have been implemented. Efficiency The computational 
efficiency of the entire contour extraction and surface reconstruction process is excellent since minimal 
computer operations are needed. The speed of the reconstruction process is mostly determined by the input/output 
operations to and from the cartridge disk. Typically, the reconstruction process for a sequence of 40 
scans requires approximately l0 minutes. This process results in a complete set of soft tissue and bone 
contours. Contour extraction at other than soft tissue-air or soft tissue-bone interfaces requires an 
image segmentation (7) scheme that is more sophisticated than the simple thresholding or level slicing 
we have employed. This sophistication would be reflected in increased computational and storage requirements. 
 Display Considerations The surface reconstruction algorithm produces very accurate and detailed information 
regarding craniofacial morphology. However, these surfaces have considerably greater dynamic range and 
resolution than can be displayed on a typical monochrome (black and white) CT scanner or evaluation console 
display using a simple linear gray scale window. We have employed histogram modification and contrast 
enhancement techniques to improve the information transfer from the surface display to the observer. 
Color display may allow even greater detail and contrast and true color renditions including flesh tones, 
hair and eye color may become available. Results The programs developed for high resolution CT scan 
surface reconstruction are available for routine use at the Mallinckrodt Institute of Radiology (9-10). 
These programs were developed to study craniofacial deformities and several repres entative cases are 
described. Case No.l G3 is an 18 year old white male with polyostotic fibrous dysplasia involving the 
skull, especially the facial bones on the right side. Surface 3-dimensional facial reconstructions were 
helpful (Fig. 0) in the planning and evaluation of an extensive debulking procedure. The marked facial 
asymmetry due to bony overgrowth on the affected right side is evident. CCase No. 2 LD is an 11 year 
II month old white female with Crouzon's disease. A tracheostomy, in infancy, was the only surgical intervention 
made prior to our initial evaluation. Photographs in the frontal, chin up and bird's eye projections 
were made (Fig. 5 A-C). These photographs demonstrate the stigmata of craniofacial dysostosis including 
maxillary retrusion, exorbitism and moderate oxycephaly. Computer reconstruction of frontal bony and 
soft tissue surfaces was performed using a set of 40 high resolution CT scans. The same set of soft tissue 
contours used to reconstruct the 3-dimensional soft tissue surface (Fig. 5F) was used to generate worm's 
eye (Fig. 5B) and bird's eye contour plots (Fig. 5D). Case No. 3 3H is a 12 year old black male with 
Tessier type 4- l0 craniofacial clefts (5). The left globe is deviated laterally and inferiorly in comparison 
with the normal right side. Surgical cranioplasty was performed on two occasions in early childhood with 
placement of a methylmethacrylate plate and tantalum mesh over the left frontal encephalocele defect. 
The upper and lower tid colobomata were repaired as well, but the asymmetric hypertelorism was not. In 
the preoperative surface reconstructions, the soft tissue protruberance (Fig. 6C) over the large frontal 
cleft (Fig. 6B) is seen. The postoperative reconstructions (Fig. 6D,E) show mesial and superior movement 
of the bony orbit to restore facial symmetry. Interactive Computer Aided Design The design of advanced 
military aircraft is dependent on the availability of computer aided technology to permit examination 
of alternative solutions. The three dimensional analysis of an aircraft fuselage surface is analogous 
to the bony and soft tissue surface of the face. Using a sophisticated computer aided design system (15) 
(CADD for Computer Aided Design and Drafting system by McDonnell Douglas Automation Company, St. Louis, 
Mo.) operating on an advanced three dimensional display console (Evans and Sutherland Picture System 
2, Salt Lake City, Utah), we have developed methodology for the planning and evaluation of craniofacial 
surgical procedures. After extracting frontal soft tissue and bone contours using the level slicing method 
described above, the corresponding point data (Fig. 7A) was curve-fitted using cubic splines (Fig. 7B). 
For patient number 3, the three dimensional surface geometry of frontal bony contours was encoded in 
fewer than 2500 coordinates using 00 CT scans as input (Fig. 7D). Soft tissue contours were separately 
entered in the system using the same technique. Using the interactive capablities of this system, these 
contours were rotated to show the plan or top view (Fig. 7D). The inferolateral displacement of the left 
orbit is seen on the near frontal view. The left frontal bone cleft is clearly demonstrated. In both 
the frontal (Fig. 7C) and plan (Fig. 7D) views, the right handed three dimensional coordinate system 
is shown in the left corner. By convention, we consider the X-Y plane to be parallel to the transaxial 
CT plane of section. This is nearly parallel to the base of the skull in CT examinations conducted without 
gantry tilt. We have assigned the Z-axis to the longitudinal body axis. The major advantages of the CADD 
system approach to craniofacial surgical planning and evaluation include interactive display, quantitation 
of linear, surface and volumetric measurements, and ability to move image components independently. The 
soft tissue and bony contours may be overlaid to study their relationship with the face by displaying 
them simultaneously (Fig. 7E). A most effective means of planning an orbital translocation procedure 
is demonstrated in this example of asymmetric hypertelorism. The right orbit and its contents are normal 
while the left orbit is displaced inferiorly, anteriorly and laterally. By measuring these displacements 
in three dimensions, the surgeon can execute the orbital translocation with improved precision. The computer 
aided measurements were obtained with CADD in the planning of orbital translocation by choosing a plane 
of symmetry in the mid-sagittal region. An arbitrary point of reference was selected above the nasion 
in the midfrontal region. The normal right orbit was converted from a solid to dashed line type for ease 
of display. The delineation of the normal orbit was then reflected about the mid-sagittal plane of symmetry 
(Fig. 7F). Measurements of the X,Y and Z displacements were made with references to this arbitrary fixed 
point. The dimensions are given in inches on the images in the frontal (Fig. 7F) and plan views (Fig. 
7G). (Advanced American-made military aircraft are presently built to English measures, although metric 
conversion is underway). The most useful measurements are the X,Y and Z differences between the normal 
transposed and abnormal dislocated orbit. These measurements serve to define the required translocation 
procedure to realign the left orbit to normalcy. The required superior, medial and posterior displacements 
(Fig. 7F-G) are shown to be 0.231 inches (0.59cm), 0.008 inches (1.04cm) and 0.266 inches (0.68cm) respectively. 
Discussion Craniofacial surgical procedures have traditionally been planned and evaluated radiographically 
using cephalometric AP and lateral skull x-rays. With the availability of high resolution CT scanners, 
reformatted (sagittal, coronal and oblique) images obtained from contiguous sections through the facial 
structures have found application. Both methods are subject to the limitations imposed by 2-dimensional 
imaging applied to the analysis of 3-dimensional aberrant anatomy. CT scans and reformatted planar images 
contain significant information regarding anatomic abnormalities, The form of CT data presentation, however, 
may create difficulties in interpretation for the radiologist and problems when the scan results must 
be communicated Computer Graphics Volume 17, Number 3 July 1983 to the surgeon. Consider the difficulty 
in mentally reconstructing a patient's facial appearance from a sequence of CT scans (Fig. 2). If we 
cannot visualize the exact nature of abnormal facial appearance by inspecting a set of CT scans) do we 
truly understand the nature of the bony defects that created the problem? The radiologist or surgeon 
can better plan and evaluate the complex procedures needed to correct B-dimensional bony anomalies with 
a means to fully characterize facial anatomy. This tool, realized by computer processing of a sequence 
of CT scans, is B-dimensional surface reconstruction of bony and soft tissue structures. We have developed 
such a tool and recognize several additional practical benefits. There is no geometric error due to magnification 
effects (inherent in cehpalometric skull x-rays) in these surface images. The true character of bilaterally 
asymmetric malformations may be appreciated (Fig.6). The surface images allow the surgeon to precisely 
visualize the abnormality to be corrected at operation. Life size surface images may be produced to permit 
accurate dimensional measurement required for surgical planning. Finally) using interactive digital graphic 
systems) the surface images can be altered to simulate surgery. These images are helpful for instructing 
physicians and even patients themselves in pathologic anatomy and craniofacial surgical procedures. Conclusion 
A new method for reconstruction of a B-dimensional surface from a sequence of high resolution CT scans 
has been developed. This algorithm) realized as a set of computer programs that can operate on a CT scanner 
or evaluation console, is both efficient and easy' to implement. No operator intervention is required. 
The CT data has also been used to generate surface images on an industrial computer aided design system. 
This allows the generation of industrial blueprint diagrams for application to craniofacial surgery (Fig. 
7F- G). These methods have been utilized to assist management of over 200 patients with congenital or 
acquired craniofacial deformities by extracting important anatomic details from a sequence of high resolution 
CT scans. These surface reconstructions have aided the surgeon's comprehension of the primary deformity) 
facilitated surgical planning and clarified postoperative results. Acknowledgements The generous assistance 
of McDonnell Douglas Automation and Aircraft Company (St. Louis) Missouri) and especially Mr. A. Doelling, 
Mr. F. Phillips) and Mr. 3. Sullivan is appreciated. Technical consultaton and the use of facilities 
at the Evans and Sutherland Computer Corporation (Salt Lake City) Utah) with the assistance of Mr. T. 
Naanes, Mr. 3. Callen and Mr. S. Artick is acknowledged. REFERENCES 1. Kawamoto H. The kaleidoscope 
world of rare craniofacial clefts: Tessier classification. Clinics in Plastic Surgery 1976; 3:529-572. 
 2. Artzy E) Frieder G) and Herman GT. The theory) design) implementation and evaluation of a three-dimensional 
surface detection algorithm. Comput Graph &#38; Image Proc 1981; 15:1-2t~. 3. Herman GT Image reconstruction 
from projections. New York: Academic Press, 1980:260-  176. 4. Nagao M) Matsuyama T. Edge preservation 
smoothing. Proc International Joint Conference Pattern Recognition) Kyoto) 3apan) [978;IV:518- 320. 5. 
Rabiner LR) Sambur MR) Schmidt CE. Applications of a nonlinear smoothing algorithm to speech processing. 
IEEE Trans. Acoust. Speech and Signal Proc. 1975; 23:552=557. 6. Akima H. A new method of interpolation 
and smooth surface fitting based on local procedures. 3ACM 1970; 17:589-602. 7. Fu KS, Mui 3K. A survey 
on image segmentation. Pattern Recog 1981; 13:65-77. 8. Huang TS, ed. Two-dimensional digital signal 
processing II. Transforms and median filters. Springer Verlag, Berlin) 1981, 3-7 and 161-218. 9. Vannier 
M, Marsh 3L) Warren 30 and Barbier 3. Craniofacial disorders. Diagnostic Imaging) (5)3) 36-43. 10. Marsh 
3L and Vannier MW. The third dimension in craniofacial surgery. Plast and Reconstr Surg 71:June, 1983. 
 11. Vannier MW) Gado MH and Marsh ]L. Three dimensional display of intracranial soft tissue structures) 
American 3ournal of Neuroradiology) May) 1983. 12. Merz B. Computer guides facial reconstruction. 3ournal 
AMA, 18 March 83) (249) 11) 1t~09-15. 13. Clayton D. Craniofacial CT. Computers in Medicine, (12)) Feb. 
1983) 3-4. 14. Vannier MW) Marsh 3L) Warren 30 and Barbier 3. Three dimensional CAD for craniofacial 
surgery. Electronic Imaging) in press. 15. Doelling AE. CAD/CAM Computer Aided Technology, Engineering 
Communications Report) McDonnell Douglas Aircraft Co.) 26pp.    SOFT TISSUE   BONY R ISSUE Figure 
7A. 4. Single CT slice, points ,4. 4- corresponding to soft tissue k and bone contours obtained bythreshold 
edge detector. 4. 4. Slice was taken'at superior ,4. ,4. margin of bony orbits. 4..i. 4.'.+ 4.4. 4. 
 ~4. 4. '* "+~4. ,4~ "~4. 4.-I. + +4.+,,I-,. I.  SOFT TISSUE ,*  .+ -~  ,,~4. .~ + 4. + +.+ -I-4. 
   BONY TISSUE 4. f Figure 7B. Same as Fig. 7A, except the contours have been curve fit using cubic 
splines. o, contour, ob,a,ned ,rom ~ ~ ~ a sequence of 40 high resolution CT ,cans. Figure 7D. ~IIP-/~ 
k~_~ TPm~hPlanm~Vi~~abnnYhcnt~ur~bbtra~e~eft~~ ~ ~'l[)~'k ~ ~~,~"~~~' and normal right orbits. 
~,/#/ "~ " ~ ~ ~k~ I Figure 7. Case No. 3, CADD Analysis. 7 ~ ' , ~,~ 272  F,XEO REFERENCE PO'NT RIGHT 
EYE TRANSPOSED ,'',.52~ ..... /--~ i LEFT EYE ORIG POSI O2", '!~-~K) R l GTflT, =,=" i I Figure 7F. 
Frontal view with measurements of orbital G23 ' ! E -O,'<.; LEFt: T= displacement in the X-Z plane. 
  ii12;18L ~---~-------..=."-~-.,'i t " " "~- --~--/ ,'" / ~-,~-~~----~ ~ ,~--/ RIGHT@ ~'~ LEFT PLANE 
O~ SYFIrlCTRY z'~ RICHT EYE CONE G24 (2.OK) /~ RIGHT EYE CONE TRANSPOSED ( TE~, / \ xOR,C.~. E,~ CONE 
Figure 7G, Plan view with measurements of orbital &#38;\ldl/ \ ~ ' ~l, displacement in the X-Y plane. 
"~ ~ , 266 .783 8  MIR IMAGE PROCESSING SYSTEM I AMM I 'I Figure 8. Hardware configuration block 9tR 
diagram for the digital image processing system used to generate surface images. '~,'""' k ' '\ITZ] 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801158</article_id>
		<sort_key>275</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>33</seq_no>
		<title><![CDATA[Computer graphics and visual designers (Panel Session)]]></title>
		<page_from>275</page_from>
		<page_to>277</page_to>
		<doi_number>10.1145/800059.801158</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801158</url>
		<categories>
			<primary_category>
				<cat_node>I.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.6.1</cat_node>
				<descriptor>Strategic information systems planning</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003457.10003490.10003491.10003492</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Management of computing and information systems->Project and people management->Project management techniques</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39049611</person_id>
				<author_profile_id><![CDATA[81100583778]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aaron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marcus]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Aaron Marcus and Associates]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P329842</person_id>
				<author_profile_id><![CDATA[81100363599]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Del]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Coates]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[design consultant]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39076123</person_id>
				<author_profile_id><![CDATA[81100360776]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitchell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CAD Design Group]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Marcus, Aaron, "Color: A Tool for Communication", in Greenberg, Donald, Aaron Marcus et al, The Computer Image, Addison-Wesley, Reading, 1982]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Marcus, Aaron and Ronald Baecker, "On the Graphic Design of Program Text", Proc. Graphics Interface '82, 1982 pp. 303-311]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Marcus, Aaron, "Designing the Face of an Interface", Proc. Second National Computer Graphics Assoc. Conference 1981, pp. 207- 219, and Tech. Memo. No. LBL-12733, Lawrence Berkeley Laboratory, Berkeley, 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Marcus, Aaron, "Graphic Design and Computer Design: Know Business is Show Business, Centerline, Center for Design, San Francisco, July 1981, pp. 6-7, and Industrial Design, January-February 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Marcus, Aaron, "Computer-Assisted Chart Making from the Graphic Designer's Perspective", Lawrence Berkeley Laboratory, Tech. Report LBL-10239 April 1980.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Marcus, Aaron, "A Prototypical Computerized Page Design System" Visible Language, Vol. 5, No. 3, summer 1971, pp. 196-220.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Marcus, Aaron, The Designer, the Computer and Two-Way Communication, Print Magazine, Vol. 25 No.4, July/August 1971, pp.34-39.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Coates, Del. "The Computer-Aided Designer", in each issue of Industrial Design since November/December 1981]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Coates, Del. "Industrial Design and the CAD/CAM Revolution" in The 1981 IDSA Papers, published by the Industrial Designers Society of America, 1981, pp. 17-20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Coates, Del. "Understanding aesthetics: from old shoes to a teacup" Industrial Design, September/October 1979, pp 32-35.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>6684</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Foley, J.D., &amp; A. van Dam, Fundamentals of Interactive Computer Graphics, Addison- Wesley, Reading, MA, 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Newman, William M. &amp; Sproull, Robert F. "Principles of Interactive Computer Graphics", 2nd Ed., McGraw-Hill, New York, 1979]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Whitney, John. Digital Harmony, Byte Books/McGraw-Hill, Peterborough, N.H. 1980.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>540117</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Mitchell, William J, Computer Aided Architectural Design, Van Nostrand Reinhold, New York 1977]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>540292</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Eastman, Charles, Ed., Spatial Synthesis in Computer Aided Design, Applied Science London, 1975]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[John F. Gero, Ed., Computer Applications in Architecture, Applied Science, London 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PANEL COMPUTER GRAPHICS AND VISUAL DESIGNERS CHAIR: Aaron Marcus, Aaron Marcus and Associates PANELISTS: 
Del Coates, design consultant William Mitchell, CAD Design Group CHAIRMAN'S INTRODUCTION Within the 
last three years, the computer graphics field has become increasingly attentive to design issues in the 
 research, development, and production of terminals, hard copy devices, worksta- tions, and networks. 
Characteristic of these changes have been the enhancement of symbol processing and visual database 
management functions, increased visual resolution and speed of display, and the elaboration of "friendly" 
iconic inter- faces. Many of these changes have resulted from interactions with graphic designers and 
industrial designers who have influ- enced strategic planning and practical marketing decisions among 
business graph- ics, CAD/CAM, games, video production sys- tems, etc. Recently the design professions 
 themselves have come to the attention of the computer graphics industry as a poten- tial market for 
graphics editing work sta- tions. A number of new start-up companies are currently at work on designer-oriented 
work stations, and some larger manufactur- ers have focused efforts in this direc- tion. Changes in 
computer graphics technology have evoked changes in attitude among the visual design professionals such 
as graphic designers, industrial designers, and architectural designers. The profes- sional design community 
is rapidly becom- ing aware of what computer graphics can and can not accomplish. These profession- als 
and their managers are investigating the retooling of their work environments to accommodate the potential 
of electronic telecommunication and sophisticated mass communication/production systems. Because of these 
changes it is appropriate that Siggraph offer design professionals an opportunity to clarify their needs 
and desires. Designers have continuously grum- bled about the lack of specific functions suited to their 
natural working methods and the poor quality of the graphic display media in terms of typography, color, 
line weight, solid modeling, symbol libraries, etc. In this panel session we shall explore the nature 
of the designer's cognltlve approach, task structure, and the capabil- ities for computer graphics to 
enhance the working environment. Representatives from graphic design, industrial design, and architectural 
design will present relevant examples of computer graphics output, dis- cuss the shortcomings of current 
systems, and suggest what computer graphics should be developing for the design professional in the next 
five years of computer graph- ics. The lessons to be learned can be gen- eralized to all users of computer 
graphics systems in the sense that computer graph- ics permits and encourages every user to become a 
visual designer. Aaron Marcus is the head of Aaron Marcus and Associates, a graphic design research 
and development firm specializing in com- puter graphics. They develop specifica- tions for unique or 
generic displays of text documents, forms, charts, maps, and diagrams in a wide range of applications, 
from educational games to financial graph- ics. They also design the screens and paper documentation 
of the human-computer interface for interactive systems in the CAD/CAM, automated office, and personal 
 computer fields. They are currently engaged in a two-year research effort to determine the ideal graphic 
design format for computer programs in a project spon- sored by the US Department of Defense. Although 
educated as a graphic designer at Yale University, Mr. Marcus learned com- puter graphics programming 
on his own and was a consultant in computer graphics to Bell Telephone Labs in the early 1970's while 
he taught at Princeton University. He was recently a staff scientist in the Computer Science and Mathematics 
Depart- ment of Lawrence Berkeley Laboratory. He is a member of the American Institute of Graphic Arts, 
is on the advisory board of the Design Arts Program of the National Endowment for the Arts, and is the 
co- author of The Computer Image , published by Addison-Wesley. 275 Del Coates has BS and MA degrees 
in indus- trial design from the University of Michi- gan, where he also studied computer graph- ics. 
He was an industrial designer with General Electric, Herman Miller Research Corp. and Ford Motor Company. 
For 16 years he was professor and chairman, Department of Industrial Design, at the Center for Creative 
Studies, College of Art and Design in Detroit. As a consultant he has done product planning, design, 
and human factors for automotive and product manufacturers, and advised industrial design offices in 
matter of computer-aided design (CAD). He writes a regular column on CAD for Industrial Design magazine 
and has conducted CAD workshops for the Indus- trial Designers Society of America. He does research on 
quantification of aesthetic design variables. William J. Mitchell is Professor of Archi- tecture and 
head of the Architecture/Urban Design Program at the Graduate School of Architecture and Urban Planning, 
UCLA. He has also taught at Berkeley, Carnegie Mel- lon, Sydney, Yale and Cambridge. His research 
focuses on the logic of design processes, and computer methods in archi- tecture. He is the author of 
numerous technical papers in these areas, and of the text Computer-aided Architectural Design. ~ is 
~so principal of the Computer-Aided Design Group, a Santa Mon- ica, California software and development 
 firm. PANELISTS' ABSTRACTS: Aaron Marcus, Aaron Marcus and Associ- ates Graphic designers use typography, 
symbol- ism, color, spatial organization, and tem- poral sequencing to communicate facts, concepts, and 
emotions through texts, forms, charts, maps, diagrams and illus- trations in a wide and every changing 
array of mass communication media. They develop ideas through sketching, critique and refine their graphics 
visually, present visual prototypes to clients, and oversee the final production of visual communication. 
Computer graphics systems must emulate and enhance the traditional means for accomplishing these tasks. 
Currently computer graphics systems spe- cialize in elaborate image development for slide, film and video 
production, a lim- ited set of chart-making operations, or else sophisticated typesetting for printed 
documents. Few systems enjoy extensive applications development, database manage- ment or networking 
capabilities. Among other features that general purpose graphic design workstations must account for 
are multi-media orientation that com- bines illustration, typesetting, and data- base management capabilities. 
Non-verbal visually oriented interfaces requiring a minimum of keyboard activity also seem desirable. 
Finally, elaborate online help and symbol libraries together with network access to visual mail would 
seem to be useful. In combining and focusing special- ized features of current and state-of- the-art 
computer graphics systems, effec- tive work stations for graphic designers can emerge. Marcus, Aaron, 
"Color: A Tool for Communi- cation", in Greenberg, Donald, Aaron Marcus et al, The Computer Image, Addison-Wesley, 
Reading, 1982 Marcus, Aaron and Ronald Baecker, "On the Graphic Design of Program Text", Proc. Graphics 
Interface '82, 1982 pp. 303-311 Marcus, Aaron, "Designing the Face of an Interface", Proc. Second National 
Computer Graphics Assoc. Conference 1981, pp. 207- 219, and Tech. Memo. No. LBL-12733, Lawrence Berkeley 
Laboratory, Berkeley, 1981. Marcus, Aaron, "Graphic Design and Com- puter Design: Know Business is 
Show Busi- ness, Centerline, Center for Design, San Francisco, July 1981, pp. 6-7, and Indus- trial 
Design, January-February 1982. Marcus, Aaron, "Computer-Assisted Chart Making from the Graphic Designer's 
Per- spective", Lawrence Berkeley Laboratory, Tech. Report LBL-10239 April 1980. Marcus, Aaron, "A 
Prototypical Computer- ized Page Design System" Visible Language, Vol. 5, No. 3, summer 1971, pp. 196-220. 
 Marcus, Aaron, The Designer, the Computer and Two-Way Communication, Print Magazine, Voi. 25 No.4, July/August 
1971, pp.34-39. Del Coates, Del Coates Associates Like mechanical engineers, industrial designers 
define and develop product con- cepts. They are concerned with mechanical feasibility, performance, 
efficiency, reliability, manufacturability, and cost. Like human factors engineers, they are concerned 
with aesthetic determinants of consumer preference. Using the skills of visual artists, they visualize 
imaginary concepts and communicate them to others as realistically as possible. The ideal computer 
system for industrial designers would incorporate the most advanced capa- bilities of both 3-dimensional 
CAD and animation systems: solid-modeling for complete physical and functional descrip- tions; and 
ability to depict all material and surface qualities with photographic fidelity. It would stress easy 
interac- tive capabilities like quick response to user input, comprehensive communication and "transparent" 
analog input and output devices. It would have a high-resolution 276 Computer Graphics Volume 17, Number 
3 July 1983 color display capable of depicting both diffuse and specular reflection. Beyond that, progress 
will parallel development of artificial intelligence and "expert systems" that will lead eventually to 
automatic analysis and adjustment of aesthetic variables to optimize the visual appeal of products. 
Coates, Del. "The Computer-Aided Designer", in each issue of Industrial Design since November/December 
1981 Coates, Del. "Industrial Design and the CAD/CAM Revolution" in The 1981 IDSA Papers, published 
by the Industrial Designers Society of America, 1981, pp. 17-20. Coates, Del. "Understanding aesthetics: 
 from old shoes to a teacup" Industrial Design, September/October 1979, pp 32-35. Foley, J.D., &#38; 
A. van Dam, Fundamentals of Interactive Computer Graphics, Addison- Wesley, Reading, MA, 1982. Newman, 
William M. &#38; Sproull, Robert F. "Principles of Interactive Computer Graph- ics", 2nd Ed., McGraw-Hill, 
New York, 1979 Whitney, John. Digital Harmony, Byte Books/McGraw-Hill, Peterborough, N.H. 1980. William 
J. Mitchell, Computer-Aided Design Group The design of a building is normally a top down process, taking 
place over an extended period: weeks, months, or even years. It begins with the rapid explora- tion of 
schematic design concepts, through the medium of quick, rough sketches, at a high level of abstraction, 
and proceeds by a process of recursive elaboration and refinement through successively more pre- cise 
and detailed representations of the proposed building to terminate with the production of very extensive 
and detailed working drawings and specifications. Dur- ing this process, frequent use is made of catalogue 
and reference databases of vari- ous kinds: previous projects, standard details, building product catalogues, 
mas- ter specifications, etc. In addition to the architect, various specialized consul- tants (structural 
engineers, mechanical engineers, interior specialists, etc.), work on the design as it evolves, and must 
have access to current and accurate descriptions of it. An effective computer-aided architectural design 
system must integrate the flow of graphic data, both vertically through stages of a pro3ect, and horizontally 
across different design disciplines. This requires sophisticated database capabili- ties and great flexibility 
in the graphics interface. Exploration of schematic design alternatives is very different from the drafting 
of final working drawings, and a mechanical engineer's view of a building is very different from that 
of an archi- tect. The design process must be sup- ported by extensive and conveniently accessible libraries 
of architectural sym- bols, details, etc. This has implications for the design of menus and commands. 
In order to achieve useful productivity, high level design operations, incorporating knowledge of architectural 
techniques and conventions must be available; low level general drafting operations will not suf- fice. 
Display technology and interfaces must be capable of dealing with drawings that are both very large and 
very dense in detail. Integration of non-graphic func- tions with the graphics system must be supported 
to the maximum feasible extent. All this must be accomplished at rela- tively low cost, since most architectural 
firms are small and do not have a high and steady workflow. Mitchell, William J, Computer Aided Archi- 
 tectural Design, Van Nostrand Reinhold, New York 1977 Eastman, Charles, Ed., Spatial Synthesis in 
Computer Aided Design, Applied Science London, 1975 John F. Gero, Ed., Computer Applications in Architecture, 
Applied Science, London 1977.  277 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801159</article_id>
		<sort_key>279</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>34</seq_no>
		<title><![CDATA[Localized set operations for solid modeling]]></title>
		<page_from>279</page_from>
		<page_to>288</page_to>
		<doi_number>10.1145/800059.801159</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801159</url>
		<abstract>
			<par><![CDATA[<p>Set operation algorithms form an important component of solid modeling systems. Their efficiency can be enhanced by localizing the search for geometric intersections to the region of overlap using a spatial directory. We present an algorithm that employs a three-dimensional extendible cell (EXCELL) directory to the set operation problem, and demonstrate by practical experiments the efficiency and the local nature of the algorithm.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Geometric directories]]></kw>
			<kw><![CDATA[Set operations]]></kw>
			<kw><![CDATA[Spatial search]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.2.7</cat_node>
				<descriptor>Data dictionary/directory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10002952.10003212.10003224</concept_id>
				<concept_desc>CCS->Information systems->Data management systems->Database administration->Data dictionaries</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P192797</person_id>
				<author_profile_id><![CDATA[81100402407]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Martti]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[M&#228;ntyl&#228;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Helsinki University of Technology, Laboratory of Information Processing Science, 02150, Espoo 15, Finland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P191410</person_id>
				<author_profile_id><![CDATA[81100452648]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Markku]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tamminen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Helsinki University of Technology, Laboratory of Information Processing Science, 02150, Espoo 15, Finland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baumgart, B.G., Geometric modeling for computer vision. Report No. AIM-249, STAN-CS-74-463, Stanford AI Lab., Stanford University, Oct. 1974]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356797</ref_obj_id>
				<ref_obj_pid>356789</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bentley, J.L. and Friedman, J.H., A survey of algorithms and data structures for range searching. ACM Comp. Surv. 11(1979)4, p. 397-409.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bentley, J.L. and Wood D. An optimal worst case algorithm for reporting intersections of rectangles. IEEE Trans. Comp. C-29(1980)7, p. 571-577.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Braid, I.C., Notes on a Geometric Modeller. CAD Group Document No. 101, Computer Laboratory, University of Cambridge, Cambridge, England, June, 1979]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Eastman, C.M. and Weiler, K., Geometric Modeling Using the Euler Operators. Proc. First Ann. Conf. on Computer Graphics in CAD/CAM Systems, MIT, Apr,. 1979, p. 248-254]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>320092</ref_obj_id>
				<ref_obj_pid>320083</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Fagin, R., Nievergelt, J., Pippenger, N. and Strong H.R., Extendible hashing, a fast access method for dynamic files. ACM TODS 4(1979)3, p. 315-344.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807480</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Franklin, W.R., A linear exact hidden surface algorithm. ACM Computer Graphics 14(1980)3, p. 117-123.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Griffiths, J.G., Eliminating hidden edges in line drawings. Comput. Aided Des. 11(1979)2, p. 71-78.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>809229</ref_obj_id>
				<ref_obj_pid>800263</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Kedem, G., The quad-CIF tree data structure for hierarchical on-line algorithms, Proc. ACM Nineteenth Design Automation Conference, 1982, Las Vegas.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Lomet, D.B., Digital B-trees. Proc. 7th Int. Conf. on Very Large Data Bases, Cannes 1981, p. 333-344.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Meagher, D., Geometric modeling using octree encoding. Computer Graphics and Image Processing 19(1982), p. 129-147.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[M&#228;ntyl&#228;, M. and Sulonen, R., GWB - A Solid Modeler With Euler Operators. IEEE Computer Graphics &amp; Applications 2(1982)7, p. 17-31]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358681</ref_obj_id>
				<ref_obj_pid>358656</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Nievergelt, J. and Preparata, F.P., Plane-sweep algorithms for intersecting geometric figures. Comm. ACM 25(1982)10; p. 739-747.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Okino, N. et al., TIPS-1 '77 Version. Hokkaido University, 1977.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Quinlan, K.M. and Woodwark, J.R., A spatially-segmented solids datamase - justification and design, Proc. CAD 82 Conference, Brighton, England, 1982.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356833</ref_obj_id>
				<ref_obj_pid>356827</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Requicha, A.A.G., Representations of rigid solids: theory, methods and systems. ACM Comp. Surv. 12(1980), p. 437-464.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Requicha, A.A.G. and Voelcker, H.B., Constructive Solid Geometry. Tech. Memo. No 25, Production Automation Project, University of Rochester, Rochester, N.Y., 1977.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Requicha, A.A.G. and Voelcker, H.B., Solid modeling: a historical summary and contemporary assessment. IEEE Computer Graphics and Applications 2(1982)6, p. 9-24.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Roth, S.D., Ray Casting for Modeling Solids. Computer Graphics and Image Processing 18(1982)2, p. 109-144]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>908431</ref_obj_id>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Shamos, M.I., Computational Geometry. PhD thesis, Yale University, 1978.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Six, H.W. and Wood, D., Counting and reporting intersections of d-ranges. IEEE trans. Comp. C-31(1982)3, p. 181-187.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Steinberg, H.A., Overlap and separation calculations using the SynthaVision three dimensional solid modeling system. Proc. Conf. On CAD/CAM Technology in Mechanical Engineering, Cambridge, Mass., March 24-26, 1982, p. 13-19.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356626</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I.E., Sproull, R.F. and Schumacher, R.A., A characterization of ten hidden-surface algorithms. ACM Comp. Surv. 6(1974)1, p. 1-55.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Tamminen, M., A note on extendible hashing with overflow. Info. Proc. Lett. 15(1982)5, p. 227-232.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>809228</ref_obj_id>
				<ref_obj_pid>800263</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Tamminen, M. and Sulonen, R., The EXCELL method for efficient geometric access to data. Proc. ACM Nineteenth Design Automation Conference, 1982, Las Vegas, p. 345-351.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Tikkanen, M., User's manual of Blk. Rep. HTKK-TKO-B36, CAD-Project, Laboratory of Information Processing Science, Helsinki University of Technology, Espoo, 1981]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Tilove, R.B., Set membership classification: a unified approach to geometric intersection problems. IEEE Trans. Computers, C-29(1980)10, p. 874-883.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Tilove, R.B., Exploiting spatial and structural locality in geometric modeling. Tech. Memo. No 38, Production Automation Project, University of Rochester, 1981.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Voelcker, H.B. and Requicha, A.A.G., Geometric Modeling of Mechanical Parts and Processes. IEEE Computer 10(1977)2, p. 48-57]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 LOCALIZED SET OPERATIONS FOR SOLID MODELING Mar tti MVintyl~ Markku Tamminen Helsinki University of 
Technology Laboratory of Information Processing Science 02150, Espoo 15) Finland ABSTRACT Set operation 
algorithms form an important com-ponent of solid modeling systems. Their efficiency can be enhanced by 
localizing the search for geometric intersections to the region of overlap using a spatial directory. 
We present an algorithm that employs a three-dimensional extendible cell (EXCELL) directory to the set 
operation problem, and demon-strate by practical experiments the efficiency and the local nature of the 
algorithm. CR Categories and Subject Descriptors: H.2.2 Database Management : Access methods; 1.3.5 Computer 
Graphics : Geometric algorithms, languages, and systems General Terms: algorithms, data structures, perfor-mance 
Additional Key Words and Phrases: set operations, geometric directories, spatial search 1. Introduction 
A solid modeler is the part of a computer-aided design system responsible for creating, manipulating, 
storing) and transmitting spatially complete data on the geometric form of three-dimensional solid objects 
(Requicha 1980) Requicha and Voelcker 1982). So called set operations form an especially important class 
of manipulations. With them) the user (a hu-man being or an algorithm) may easily add, subtract, or intersect 
solids with each other to generate new models of unlimited complexity. It is customary to divide solid 
modelers into two families with regard to the role of set operations. So-called Constructive Solid Geometry 
modelers (Voelcker and Requicha I977) view set operations as the primary means of definition. They represent 
a solid by a tree of set operations and rigid motions. In contrast) Botmdaury Representation modelers 
(Braid 1979, Eastman and Weiler 1979) define a solid by a collection of faces) edges and vertices and 
their neighborhood relationships. A third) radically dif-ferent approach is the octree solid modeling 
scheme (Meagher 1952), which records the regions of space occupied by a solid at a fixed maximal resolution. 
A set operation algorithm depends completely on the underlying solid representation. CSG modelers calcu-late 
set operations by so-called Boundary Evaluation (Requicha and Voelcker 1977)9 ordinarily based on a "divide-and-conquer" 
algorithm reflecting the struc-ture of the CSG tree. Boundary modelers operate directly on faces) edges) 
and vertices. The basic computational task of a set operation algo- rithm is to find the intersection 
of the boundaries of two solids. Since any algorithm must with certainty detect each intersection between 
faces) and since for objects of sizes n and m there may be O(nm) inter-sections, the worst-case computational 
complexity must be at least O(nm). In practical cases the effect of a set operation is often "small" 
compared to the total amount of data (Tilove 1981). In Figure t-/ nm = 2,5000)000) but only $0 true intersections 
between faces exist. Figure I-IPermission to copy without feeall or part of this material is granted 
Test Case A(I600) of Section 6 provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the The expected, "practical" complexity of a good 
setpublication and its date appear, and notice is given that copying is by operation algorithm should 
be much smaller thanpermission of the Association for Computing Machinery. To copy O(nm). Denote the 
number of pairwise intersections otherwise, or to republish, requires a fee and/or specific permission. 
&#38;#169; ACM 0-89791-109-1/83/007/0279 $00.75 between faces by s. Assuming that we have to gen-erate 
the two solid data structures, the lower limit for the complexity of set operations can be rewritten 
as O(n + m + s) because each intersection gives raise to a new edge that must be formed. While s may 
be O(nm), in practical situations it is typically O(m), m being the size of the "smaller" argument. Thus 
the aim of algorithm design would be to achieve a com-plexity of just O(n + m). Going a step further) 
set operations are often used repetitively on solid models generated by previous operations. An example 
is the "drilling" of a se-quence of small holes into a large model (Figure 5-2). In this dynamic situation, 
a complexity of O(m + s), where m and s denote the size of the small argu- ment and the number of intersections, 
respectively, should be achievable for each operation. If s = O(m), this amounts to mere O(m). In this 
paper, we aim to derive set operation algo-rithms achieving the above limits. This is possible only if 
we can localize the computations to the re-gion where intersections may occur, thus avoiding the explicit 
complete geometric search. Our solution to this is a separate geometric directory (in the data base terminology) 
to the relevant parts of the .under- lying data structures. The directory is organized by a variant of 
the EXCELL principle (Tamminen and Sulonen 1982) making it very adaptable while allowing especially efficient 
geometric search by location. The practical framework of our research is the Geometric Workbench (M~ntyl~i 
and Sutonen I982), an experimental solid modeler constructed at the Hel-sinki University of Technology. 
GWB is based on a pervasive use of Euler operators (Baumgart I97t+, Eastman and Weiler 1979, Braid 1979) 
for all manipu-lations of the underlying data structures. This is also a characteristic of our set operation 
algorithm. However, the EXCELL directory is in no way limited to this approach. To make this exposition 
self contained we first give a "bird's-eye-view" of set operation algorithms in Section 2. Section 3 
describes the geometric directo- ry, and Section 4 applies it to the set operation problem. We verify 
the performance claims by ex-perimental results in Section 5. By series of synthet-ic test cases we show 
that the processing is truly lo-cal in the sense defined above; the computational cost of a series of 
"small" set operations depends only on the amount of "geometric interaction" between the solids. Section 
6 discusses briefly relat-ed approaches. 2. Set Operation Algorithms Set operation algorithms are complicated 
programs, necessitating a clear design rationale. This section provides an overview of the algorithm 
used as the basis of our work. The complexity of the problem is mainly a result of the combinatorial 
richness of situations arising in three dimensions. To present the algorithms in a concise manner, we 
here rule out the special cases where the two solids just "touch" each other along co-planar faces. With 
this restriction, if two solids intersect at all, they do so along one or more closed (non-planar) intersection 
polygons. For instance, the intersection polygon of two inter-secting blocks is depicted in Figure 2-I. 
Any set operation can be performed by creating the intersec-tion polygons and "summing" the relevant 
parts of boundaries along them. I Figure 2-I Intersection Polygon of Two Blocks Generally set operation 
algorithms first locate the in-dividual intersections between faces of the two argu-ment solids. Next, 
intersection polygons are formed by inserting new edges and vertices into both solids. Finally) resulting 
faces are classified according to whether they must be included into the final result. The result is 
then calculated by removing irrelevant parts. An unusual feature of our algorithm is the separation of 
the steps of intersection location and intersection polygon formation. As the algorithm of Steinberg 
(1982) also ours works by comparing edges of one solid against faces of the other. To locate all inter-sections, 
this process is performed twice, once for edges of solid A and faces of solid B, and once vice versa. 
After all intersections of an edge are found, they are analyzed treating separately the various spe-cial 
cases (such as edge-edge intersections) and the underlying data structure is updated by Euler opera-tors. 
In a second step, the individual intersections are joined (using Euler operators) to construct the in-tersection 
polygons. See (M~intyl~i and Sulonen 1982) for a more detailed description of similar operations. After 
constructing the intersection polygon the boun-daries of the solids can immediately be classified with 
respect to each other. The faces of solid A, for instance, are divided into sets AinB, AoutB of faces 
inside or outside the other solid, B. The con-cept of classification and the notation for its results 
stems from Tilove (1980). An overview of the set operation algorithm is given by Figure 2-2. The main 
problems in any complex geometric algo-rithm are the integrity of data structures, numerical robustness, 
and computational efficiency. We ap-proach the problem of integrity by performing all modifications by 
Euler operators. Thus the data structures are always topologically consistent. Prob- lems of numerical 
robustness are lessened by the separation of the detection of intersections from their analysis. Computer 
Graphics Volume 17, Number 3 July 1983 procedure set_op(A, B:solid, op:opcode) begin for each edge E 
of A do begin /* locate intersections */ for each face F of B do if E and F intersect then store_intersection; 
/* analyze intersections and store */ /* them in both solids */ sort intersections along E; analyze_and_process_intersections(); 
end; /* repeat for edges of B and faces of A */ for each edge E of B do  begm for each face F of A do 
if E and F intersect then storeintersection; sort intersections along E; analyze_and process_inter sections0; 
end; /* join intersections in both solids */ connect(); /* divide solids along intersection polygons 
*/ divide(A, AinB, AoutB); divide(B, BinA, BoutA); /* form result */ case op of "union": result = glue(AoutB, 
BoutA); "minus": result = glue(AoutB, BinA); "inter": result = glue(AinB, BinA);  endcase end; Figure 
2-2 The Basic Set Operation Algorithm Computational efficiency remains a problem, however. In a naive 
implementation for tasks of moderate size, over 90 percent of time is typically spent in locating intersecting 
faces and edges. This can be viewed as a "geometric query" against the two data structures: Given an 
edge E, list all faces that (may) intersect it. The straightforward implementation ("compare E against 
all faces") leads to a quadratic asymptotic growth rate. For the rest of this paper we concen-trate on 
a specific approach to enhancing the effi-ciency of geometric search. 3. The EXCELL Geometric Directory 
3.1. Geometric Queries and Directories (I) Box Query--which geometric entities intersect a parallelepiped 
with faces parallel to the coordi-nate axes ("box")? (2) Edge Query: which entities intersect a given 
edge ? (3) Neighbor Query= which entities are close to a given point?  We call these and similar tasks 
geometric queries. A geometric directory is a data structure that enhances the efficiency of geometric 
queries. These terms are chosen in analogy to the corresponding data base management concepts. Ordinary 
access methods (B-trees, hashing, and so forth) are not directly applicable to geometric direc-tories, 
because coordinate values may seldom be util-ized as exact keys in access by location. While ac-cess 
to points is similar to range queries in a general data base (Bentley and Friedman 1979), entities with 
geometric extent pose more difficult problems. The simplest useful geometric directory for set operations 
is a 2-level hierarchy consisting of a box enclosing each solid and each face. This is the reference 
method in our comparisons. More compli-cated hierarchic box directories will be discussed in Section 
6. 3.2. EXCELL: an Extendible Cell Structure Grid directories form a specific family of geometric directories. 
A grid directory consists of a collection of nonoverlapping cells that can be efficiently ac-cessed. 
Each cell points to all geometric entities (usually faces) that potentially intersect it. Regular grids 
form the simplest case of grid direc-tories. If the individual cells may vary in size ac-cording to the 
amount of data we obtain an extendi-ble cell structure. Such a structure is analogous to the spatial 
partitions used in the Warnock hidden sur-face method (Sutherland et al. 1974) and tlae octree and quadtree 
approaches (Meagher 1982). The geometric directory used in our work, EXCELL (Tamminen and Sulonen 1982), 
is an extendible cell structure. It is an application of order preserving extendible hashing (Fagin et 
al. 1979) to multidimen-sional problems. An EXCELL structure consists of two parts: a data part and a 
directory. The cells of the data part are formed by recursively halving the space of interest until some 
criterion of well-formedness (usually buck-et size) is met. The directory is an array providing maximally 
efficient access by location to data cells. Figure 3-1 shows EXCELL applied to point files. Here the 
halving is controlled by'the bucket size b, i.e. the maximal number of points in a data cell. The basic 
task of Algorithm 2-2 typifies the more general problem of providing access by location in geometric 
algorithms. Other examples are listed below. i i I i I I  -~~--3--ie4 ~- I I V- I , ) IP I >( I I 
I'""~"1 4 ) I ..... ~.,.~..I.~... I I t I I L II II I) data part directory x= points o= cursor ...... 
: cell borders Figure 3-1. An example of point-EXCELL concepts. The two-dimensional space of interest 
is divided into four data ceils (rectangles), none of which contains more than three (bucket size) points. 
The directory is an array of elements each corresponding to a rectan-gle of minimal size and indicating 
the data cell con-taining it. The cell of a cursor point p is retrieved by first calculating the directory 
array index. The insertion of the new point (+) would here require the cell halving and directory doubling 
operation as indi-cated by short dashes. 3.3. EXCELL for Three-Dimensional Boxes When applied to structured 
geometric objects, EX-CELL data cells contain a reference to all entities (potentially) intersecting 
it. Now the halving process is controlled by the maximal number of references allowed per ceil. The result 
of applying the above approach in two di-mensions is depicted by Figure 3-2. This structure may be used 
for any kinds of geometric entities by using their enclosing boxes to construct it. I I []I[]I l i 
Figure 3-2 Two-Dimensional Box-Excell, Data Part, b = 3 A naive generalization of the above to three 
dimen-sions suffers from not being able to handle a situa-tion where more than b boxes intersect each 
other. Also the directory becomes prohibitively large when the spatial density of data is very variable. 
As demonstrated in (Tamminen 1982) for point files, these problems can be overcome by including hierarchic 
directories and overflow into the basic method. We call the result hierarchic box-EXCELL with overflow, 
or box-EXCELL for short. As a con-cept it is rather similar to the digital B-tree (Lomet 1981). A two-level 
hierarchy seems sufficient and even optimal in external storage. 12 I /I' t I' 13  I, ,2 )3 I I --! 
 ! LJl dir I I / z; " , ! Iglg i I | , i ! I , I i i |~i I I I Ist level data part 2rid level directory 
directory Figure 3-3 Hierarchic EXCELL with D = In box-EXCELL the directory size may not exceed some 
threshold D. Should this happen, we (instead of the ordinary halving) form a second level directory to 
correspond to this data ceil. The growth of second level directories is controlled by allowing overflow 
in certain cases. The parameter controlling overflow is R, the maximal allowed ratio of the number of 
ele-ments in the directory to the number of its data ceils. Should a data cell division cause this ratio 
to exceed R) we let the cell overflow, i.e. intersect more than b boxes. Figure 3-0 depicts a solid we 
have used in tests (a sphere approximated with q00 faces) together with a hidden surface view of a corresponding 
EXCELL structure with parameters b = 22 and R = 5. Figure 3-0 Three-Dimensional Box-EXCELL for Faces 
of a Solid Figure 3-5 gives an EXCELL algorithm for the edge query defined in section 3,1. We have used 
a similar algorithm for a point-in-volume "ray"-test. Box insertion is effected by first performing a 
box query for all ceils intersecting the new box. It is then inserted into each of these ceils. If a 
ceil is full it must first be divided and then a similar inser-tion applied recursively (Figure 3-6). 
Box deletion is implemented similarly but without recursion. var sb : set o! box; procedure open_equery(var 
E : edge) begin /* open query for edge E */ sb = empty set; while E not null do begin c = get_cell(start_point_of(E)); 
/* cell containing start of E */ for each box b in c do sb = sb + b; /* set addition */ E = portion of 
E outside of c end end; iunction next_queryO : box begin /* get next box potentially satisfying query 
*/ if sb = empty set then next_query = NIL else begin choose a box b of sb; sb = sb -b; next_query = 
b end  end; Figure 3-5 Edge Query Algorithm  procedure insert__gdir(bex: B,BI) begin /* B: box to be 
inserted; BI: for recursion */ /* the original call is: insert_gdir(B,B); */ sc = set of all cells intersected 
by 151; for each cell c of sc do if not full(c) then insert into_cell(c,B); else begin divide_cell(c);- 
inser t_.gdir(B,inter section_of(B,c)) end end; Figure 3-6 Insertion of a Box into EXCELL 4. Set Operations 
With EXCELL Two enhanced versions of the Algorithm 2-2 were implemented using box-EXCELL. The first one 
has directories for both argument solids. Each directory is created by inserting the boxes of all the 
faces in-tersecting the global box of the other solid. Exhaustive search is now replaced by an edge query 
that gives the set of boxes intersecting a given edge. The modularity of the basic algorithm has the 
nice consequence that no other changes need to be made. The relevant new steps are given in Figure #-l. 
In Algorithm 4-1 two directories are generated in each set operation. If a model is repeatedly used as 
an argument it would be more efficient to construct the directory once, updating it as needed. This is 
implemented in our "toolbench" algorithm. In it a distinct initialization step creates a directory for 
a solid, the "workpiece". After this, a sequence of other solids, "tools", can be operated against it. 
The toolbench algorithm typifies the very common situa-tion where the result of a set operation is used 
as input to the next one in a long chain. proce~lure set_op_A(A, B: solid, op: opcode) begin /* initialize 
*/ boxa = global box of A; boxb = global box of B; /* phase I */ create gdirO; for each :[ace F of 
B do if F intersects boxa then insert_gdir(box of F) l /* geometric step */ (or each edge E of A do 
if E intersects boxb then begin /* open edge-query by E in directory of b */ open_equery(E); for each 
box = next_query0 do begin if E and face of box intersect then store intersection end; /* analyze 
*/ sort intersections along E; analyze_and process_intersections(); end; /* phase 2 as above, other 
steps as in Fig. 2-2 */  end; Figure #-1 Set Operation With Two Directories The first pass of edge-face 
comparisons (i.e. edges of tool against faces of workpiece) is similar to that of Figure 4-1. T6 perform 
the second pass, "relevant" edges of the workpiece (i.e. those that may intersect faces of tool) are 
generated by a box query with the global box of the tool. The second pass (i.e. com-parison between edges 
of workpiece and faces of tool) is then performed by comparing each relevant edge against all tool faces. 
The toolbench algorithm (Figure 4-2) must update the geometric directory of the workpiece. The availabili-ty 
of the total fourfold classification of faces makes the update operation straightforward. For instance, 
in a "minus" operation workpiece faces within the tool are removed, tool faces within the workpiece are 
added, and workpiece faces intersected are updated. 5. Analysis and Experiences 5.1. Methods and Test 
Series To analyze empirically the efficiency of the enhanced set operations, Algorithms 4-1 and 4-2 were 
imple-mented in C under UNIX on a VAX 11/750 without floating point accelerator, Their asymptotic (within 
computational resources) behaviour was compared to that of the reference method described in section 
4. As the topological manipulations in all these algo-rithms were the same, we made measurements only 
of the geometric steps. Computer Graphics Volume 17, Number 3 July 1983 /* the global box and directory 
of workpiece are created in a separate initialization step */ procedure set_op_B(Workpiece, Tool: solid) 
op: opcode) Toolbox = global box of Tool l /* phase I */ for each edge E of Tool do if E intersects Workbox 
th(m begin /* perform edge query as in Figure #-I */ ,..  end; /* perform a box query */ Rel_edges 
= empty set; open_bquery(Toolbox); for each box = next_query0 do add edges of this face to Rel_edges; 
/* phase 2 */ for each edge E of Rel_edges do begin for each face F of Tool do if E and F intersect 
store_intersection; end; /* construct intersection polygons */ connect(); /* divide boundaries */ divide(Work, 
Work in Tool) Work out Tool); divide(Tool, Tool in Work, Tool out Work); /~ update geometric directory 
*/ case op of "union": begin for each face F of Work in Tool do remove_~dir(F); for each face F of Tool 
out Work do inser t_gdir(F); for each face F of Work out Tool adjacent to intersection polygons do begin 
remove_gdir(box of F); recalculate box of F; insert_gdir(box of F); end end /~ other cases similar "1 
 endcase; /* form results as in Figure 2-2 */ .).  end; Figure #-2 The Toolbench Algorithm  Extrapolation 
from theoretical work on other variants of EXCELL (Tamminen and Sulonen 1952) makes us expect that for 
homogeneous geometric models of "size" n the expected cost of establishing an EXCELL structure is O(n), 
and that the expected cost of a geometric query is O(R), where P. is the amount of data retrieved. Here 
these statements must be considered only infor= mal objectives. We have no analytical means of studying 
box-EXCELL or characterizing homogeneity precisely. Intuitively) we expect that as the number of parts 
in a model grows these parts and typical geometric queries get relatively "smaller". As there are no 
well established series of test cases or synthetic models such as those utilized in (Suther- land et 
aL 197#) we have been obliged to construct artificial series of tasks. Despite their simplistic na-ture, 
we believe that they allow extrapolations be-cause they correspond to what Tilove (1981) considers typical 
of practical applications. They also allow easier comparisons with other methods than "real" problems. 
All our test objects are planar approximations of solids with curved faces: cylinders, spheres and tori. 
Test series A constructs the union of two spheres) approximated with n faces (Figure 1-1). Test series 
B consists of two non-intersecting tori, each approxi-mated with n faces. In these series Algorithm #-[ 
was used. Figure 5-I Test Case B(256)  Test series C is somewhat more complicated and util- izes the 
"toolbench" Algorithm 4-2. This series "drills" holes into a sphere) modeled with a varying number n 
of faces. The numbers of side faces (10) approximating each hole and the number of holes (20) remain 
constant. As n grows we make the holes pro- portionally smaller so as to maintain the number of faces 
intersected by each hole approximately constant throughout the series. All our test runs were performed 
in central storage. However, box-EXCELL is implemented on top of a buffered paged memory manager Blk 
(Tikkanen 1951) to permit the transparent use of external storage. Ceils were represented by a linked 
list of fixed-length pages (arrays). A list with more than one page is needed only in case of overflow. 
The resource utilization of Blk is included in our results. Direct implementation on top of the operating 
system would make the system somewhat more efficient. Experiments with page size led to the choice of 
128 bytes) sufficient for a maximum of b = 22 page references. As the size of the first directory level 
we have chosen D = 512 = 2**(3*3) and as the over-flow parameter R = 5. Computer Graphics Volume 17, 
Number 3 July 1983 Figure 5-2 Test Case C(100) 5.2. Results The tables and figures of this section each 
highlight a specific aspect of EXCELL on the basis of a single test series. However, extrapolation to 
the other series is possible because they all behaved similarly. Table 2 describes the hypotheses of 
our tests. Asymptotic behaviour of Series EXCELL Reference  A O(n) O(n2~) B O(n) O(n") C EXCELL formation 
O(n) in total O(n*t) geometric queries O(t) Table 2. Expected behaviour of tests. Parameter n is the 
number of faces in an approximation. In series C parameter t is the number of tooling operations per-formed. 
In the actual tests t is held constant as 20. We first demonstrate the function of box tests and a geometric 
directory in Table 3. Test case A(1600) Box tests Box tests Faces passing Face-edge reference EXCELL 
box tests intersections 6,800,000 79,000 8#8 80 Table 3. Pruning power of box tests In test series A 
and B2the total number of box tests is expected to be O(n ) in the reference method and O(n) for EXCELL. 
Our hypotheses for series A and 5 were largely validated. Table # demonstrates this for series B. The 
surprisingly sublinear performance of EXCELL is explained by the tori not intersecting each other. The 
geometric queries are often per-formed against empty ceils. Actually, in task 5(102#) all queries touch 
only empty ceils. Size of task 6# 256 102# EXCELL 102# 3280 #096 reference 3328 25600 3##06# Table ~. 
Number of box tests in test series B Run time differences between EXCELL and the refer-ence method are 
smaller than in the number of box tests. However, for large tasks (as in Table 3), box tests account 
for most of the cost of the geometric step even for EXCELL. Figure 5-3 demonstrates the different behaviour 
of the run time of ~XCELL (O(n)) compared to the reference method (O(n)). In all the figures x-axis is 
the number of faces in an approxi-mation. Y is in CPU seconds (time) or pages of 128 bytes (storage). 
 1500 /v REF. / / 100O /  / i// i 500 0 500 i00O 1500 z000 Figure 5-3 Run Time of Geometric Step 
for Series A  We studied also some variants of the A series. If the two spheres overlap only slightly 
the global box test of Algorithm #-1 does most of the work and EXCELL hardly differs in performance from 
the reference method) as expected. With more overlap our results are accentuated. In the toolbench algorithm, 
the cost of establishing the EXCELL structure for the workpiece must be separated from the cost proportional 
to the number of set operations here fixed as 20. Figure 5-# shows these components (approximately) verifying 
the hy-potheses of Table 2. We have found no good expla- nation for the cycle or jump at the end of the 
query cost curve. Cyclic variation is typical of EXCELL type data structures (Fagin et aL 1979). To some 
extent the phenomenon is also explained by that the lengths of the hole-cylinders do not vary with n. 
As the EXCELL structure grows each cylinder intersects more cells. 0 500 I000 IS00 Z0O0 EXCELL Figure 
5-4 Time Components in Series C Figure 5-5 compares the total run times of the geometric step of EXCELL 
to those of the reference method for test series C. 300 280 I =~00 150 10O 50 0 580 t080 iS00 2000 Figure 
5-5 Run Times for Geometric Step of Series C We have not optimized our data structures with respect 
to size or for use in central memory. How-ever, Figure 5-6 shows that the size of an EXCELL structure 
responds well to the growth of a model. The figure depicts the end state of EXCELL after all the updating 
brought about by the set operations in series C. too l )"~' O ! '0 500 1000 1500 2000 Figure 5-6 Storage 
Usage of EXCELL, Series C In two dimensions certain important statistics of EX-CELL depend only on the 
page size and not on the size of the task (Tamminen and Sulonen 1952). An interesting finding of the 
present work is that similar results hold also in the homogeneous three-dimensional case. The most important 
statistics for storage and processing performance are the average numbers of cells intersected by a box 
and of ceils in- tersected by query edges. Table 5 shows the relative invariance of these measures in 
our tests. Specifical-ly, the table shows that most queries touch only one cell. Test case A(000) A(1600) 
B(1025) cells/box 2.2 2.0 2.3 cells/query l.t~ 1.3 1.2 Table 5o Main relative performance statistics. 
In our experience, the size of an EXCELL structure seldom exceeds the amount of data required to define 
the boxes. It appears that our choice of page size (22 references) is good= for smaller pages both indica- 
tors above grow rapidly) while for larger ones they decrease quite slowly. Both hierarchy and overflow 
are necessary; to store a sphere with $0x$0 faces, we must be able to store the t~0 boxes intersecting 
at the poles. Our largest directory, in task C(1936), reaches a depth of 15 corresponding to a one level 
directory of 32000 ele-ments. The hierarchic version utilizes only 956 ele-ments, however. 6. Related 
Work Every serious solid modeler must use performance enhancing techniques to speed up geometric search. 
Hierarchic structures form an important class of these techniques. For instance, BUILD-2 (Braid 1979) 
uses a hierarchic "bubble environment" formed expli-citly during the generation of the model. A similar 
method is used by Roth (1952). Both structures mir-ror the way solids are constructed. In the above approaches, 
the directory is intricately embedded in the underlying data structure and its generation algorithms. 
In our opinion, it is better to have them logically separate as in data base manage- ment systems thus 
making programs simpler and more adaptable to new geometric search strategies. This has been one our 
aims. Grid directories have been widely used in geometric problems, such as hidden line and surface removal 
(Griffiths 1979, Franklin 1980). TIPS (Okino et al. 1977) gives an early example of their use in solid 
modeling. Hierarchic grid structures have been sug= gested by Tilove (1951). He also has compared so 
called primitive disjointness tables to uniform grids. Quinlan and Woodwark (1982) marry the octree idea 
with the CSG approach while Kedem (1982) presents a two-dimensional "box-directory" based on quad trees. 
It is somewhat more complicated than box-EXCELLo In our opinion, box-EXCELL is more robust against non-uniform 
spatial distribution of data than simpler fixed or variable grid methods. In three dimensions regular 
grids tend to grow especially large. Binary division of space is also more efficient than the 8-ary octree 
approach in the number of data ceils formed. At the same time the directory provides for maximally efficient 
access. The advantages of EX-CELL are most evident if secondary storage must be used. If worst case efficiency 
is essential geometric inter-section problems (Shamos 1978) should be approached in other ways, for instance 
by plane sweep algorithms (Nievergelt and Preparata 1982) that organize all pro-cessing in a spatially 
sorted manner. See also (Six and Wo6d 1982) for an O(nJogn)-space tree structure for reporting intersections 
of three dimensional boxes. However, it is not clear how "small" operations can be processed locally 
using such global methods. 7. Extensions References the EXCELL directory is especially suited for geometric 
data residing in external storage in that it provides very good loa~lity of access and a data base type 
programming environment. Table 3 is indicative of the dramatic difference between EXCELL and the reference 
method if operated in external storage. However, some of this effect is lost if also other parts of the 
data structure are not suitably localized. In a separate project we are developing GWB/DMS) a system 
that includes both geometric directories and tools to control locality of access. 8. Conclusions The 
problem of localizing geometric computations seems most relevant now as solid modelers are striv-ing 
for practical recognition. Although the general problem (geometric search) that we pose has been ex-tensively 
studied under the heading of computational geometry, not many practical experiences have been reported. 
Many algorithms are "primarily theoretical devices (being very difficult to code)" as confessed in (Bentley 
and Wood 1980). In contrast the EXCELL directory presented here is very simple to implement. As shown 
in more detail elsewhere (Tamminen and Sulonen 1982) the efficiency of EXCELL in localized geometric 
search can be characterized as similar to that of external hashing schemes for exact match search. We 
have shown that EXCELL never brings about large extra costs and -for moderate to large tasks -greatly 
enhances the efficiency of the geometric step of a set operation. While our limited resources enabled 
us to demonstrate this only for a rather sim-ple reference method, we believe that similar but more complicated 
alternatives would merely change the break-even point. Our hypotheses on the asymptotic behaviour of 
EX-CELL were roughly validated. The toolbench algo-rithm 4-2 demonstrates that dynamic maintenance of 
the directory is possible without loss of efficiency) with the consequence that good locality may be 
achieved in set operations. We have not been able to construct realistic tasks that would have been dif- 
ficult for EXCELL, even though this is possible in theory. Acknowledgements This work has been supported 
by the Academy of Finland. We thank Matti Mitjonen for his help and the anonymous referees for their 
detailed comments. 1. Baumgart, B.G., Geometric modeling for computer vision. Report No. AIM-249) STAN-CS-74-463, 
Stanford AI Lab., Stanford University, Oct. 1974 2. Bentley, 3.L. and Friedman, 3.H., A survey of al= 
gorithms and data structures for range searching. ACM Comp. Surv. 11(1979)4, p. 397-409. 3. Bentley) 
3.L. and Wood) D.) An optimal worst case algorithm for reporting intersections of rec-tangles. IEEE Trans. 
Comp. C--29(1980)7, p. 4. Braid, I.C., Notes on a Geometric Modeller. CAD Group Document No. I01, Computer 
La= boratory, University of Cambridge, Cambridge, England, 3une, 1979 5. Eastman) C.M. and Weiler, K., 
Geometric Model= ing Using the Euler Operators. Proc. First Ann. Conf. on Computer Graphics in CAD/CAM 
Sys-tems, MIT) Apr. 1979, p. 245-254 6. Fagin, R., Nievergelt, ]., Pippenger, N. and Strong H.R., Extendible 
hashing, a fast access method for dynamic files. ACM TODS 4(1979)3, p. 315-3~4. 7. Franklin) W.R., 
A linear exact hidden surface al-gorithm. ACM Computer Graphics 1t1(1980)3) p. 117-123. 8. Griffiths, 
3.G., Eliminating hidden edges in line drawings. Comput. Aided Des. 11(1979)2, p. 71- 78. 9. Kedem, 
G., The quad-CIF tree data structure for hierarchical on-line algorithms, Proc. ACM Nineteenth Design 
Automation Conference, 1982, Las Vegas. 10. Lomet, D.B., Digital B-trees. Proc. 7th Int. Conf. on Very 
Large Data Bases, Cannes) 1981, p. 333-344. 11. Meagher, D., Geometric modeling using octree encoding. 
Computer Graphics and Image Process= ing 19(1982)9 p. 129-147. 12. M~ntylii, M. and Sulonen, R., GWB 
-A Solid Modeler With Euler Operators. IEEE Computer Graphics &#38; Applications 2(1982)7, p. 17-31 
13. Nievergelt, 3. and Preparata, F.P., Plane-sweep algorithms for intersecting geometric figures. Comm. 
ACM 2Y~1982)I0) p. 739-747. 14. Okino, N. et al., TIPS-I '77 Version. Hokkaido University, 1977. 15. 
Quinlan, K.M. and Woodwark, 3.R., A spatially- segmented solids datamase -justification and design, Proc. 
CAD 82 Conference, Brighton, Eng- land, 1982. 16. Requicha) A.A.G., Representations of rigid solids: 
theory, methods and systems. ACM Comp. Surv. 12(1980), p. 437-464.  571-577.  17. Requicha, A.A.G. 
and Voelcker, H.B., Construc-tive Solid Geometry. Tech. Memo. No 25, Pro-duction Automation Project, 
University of Ro-chester, Rochester, N.Y., 1977. 18. Requicha, A.A.G. and Voelcker, H.B., Solid modeling: 
a historical summary and contemporary assessment. IEEE Computer Graphics and Appli-cations 2(1982)6, 
p. 9-24. 19. Roth, S.D., Ray Casting for Modeling Solids. Computer Graphics and Image Processing 18(1982)2, 
p. 109-144 20. Shamos, M.I., Computational Geometry. PhD thesis, Yale University, 1978. 21. Six, H.W. 
and Wood, D., Counting and reporting intersections of d-ranges. IEEE trans. Comp. C-31(1982)3, p. 181-187. 
 22. Steinberg, H.A., Overlap and separation calcula-tions using the SynthaVision three dimensional solid 
modeling system. Proc. Conf. on CAD]CAM Technology in Mechanical Engineering, Cambridge, Mass., March 
24-26, 1982, p. 13-19. 23. Sutherland, I.E., Sproull, R.F. and Schumacher, R.A., A characterization 
of ten hidden-surface algorithms. ACM Comp. Surv. 6(1974)1, p. 1-55. 24. Tamminen, M., A note on extendible 
hashing with overflow. Info. Proc. Lett. 13(1982)5, p. 227-232. 25. Tamminen, M. and Sulonen, R., The 
EXCELL method for efficient geometric access to data. Proc. ACM Nineteenth Design Automation Conference, 
1982, Las Vegas, p. 345-351. 26. Tikkanen, M., User's manual of Blk. Pep. HTKK-TKO-B36, CAD-Project, 
Laboratory of In-formation Processing Science, Helsinki University of Technology, Espoo, 1981 27. Tilove, 
R.B., Set membership classification: a unified approach to geometric intersection prob- lems. IEEE Trans. 
Computers, C--29(1980)I0, p. 874-883. 28. Tilove, R.B., Exploiting spatial and structural lo- cality 
in geometric modeling. Tech. Memo. No 38, Production Automation Project, University of Rochester, 1981. 
 29. Voelcker, H.B. and Requicha, A.A.G., Geometric Modeling of Mechanical Parts and Processes. IEEE 
Computer 10(1977)2, p. 48-57   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801160</article_id>
		<sort_key>289</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>35</seq_no>
		<title><![CDATA[Design of solids with free-form surfaces]]></title>
		<page_from>289</page_from>
		<page_to>298</page_to>
		<doi_number>10.1145/800059.801160</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801160</url>
		<abstract>
			<par><![CDATA[<p>We propose a unified method of generating a wide range of three dimensional objects from polyhedra to solids with free-form surfaces. Modeling systems for polyhedra and systems for free-form surfaces have been developed independently in the past because of the difference in their underlying theory and practices. However, this is not desirable for a designer. So in this paper, we have shown a method in which a wide range of shapes are generated in one system by using local modifications. Local modifications are procedures used to change the shape of solids locally. The construction and the modification of three dimensional shapes by these procedures are natural and easy for a designer in many cases. The implementation of these procedures in a computer is easy and their execution does not require much time.</p> <p>Our method to construct a solid with free-form surfaces consists of following three phases. 1) A solid which serves as a basis of free-form shape design is generated by local modifications. Edges of this solid are straight lines but its faces are not necessarily flat planes. 2) From this model, a curve model which adequately represents the characteristics of a free-form shape is generated. 3) Surface equations interpolating over the curve model are generated.</p> <p>We have made a geometric modeling system MODIF. Using this system, a complicated solid with free-form surfaces can be designed easily. MODIF can generate color shaded pictures and cutter path data for making a real object model by NC machining tool.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Shape</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010249</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Shape inference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P110669</person_id>
				<author_profile_id><![CDATA[81409593388]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chiyokura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dept. of Precision Machinery Engineering, University of Tokyo, Hongo, Bunkyo-ku, Tokyo, JAPAN]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39027394</person_id>
				<author_profile_id><![CDATA[81100106501]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fumihiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dept. of Precision Machinery Engineering, University of Tokyo, Hongo, Bunkyo-ku, Tokyo, JAPAN]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Barnhill,R.E., Brown,J.H. and Klucewicz,I.M.: A New Twist in Computer Aided Geometric Design, Computer Graphics and Image Processing 8, pp. 78-91, 1978.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>892020</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Baumgart,B.G.: GEOMED-A GEOMETRIC EDITOR, Stanford Artificial Intelligence Laboratory Memo AIM-232, Computer Science Department Report no. cs-414, May 1974.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bezier,P.E.: Numerical Control - Mathematics and Applications, John Wiley and Sons, London, 1972.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Braid,I.C., Hillyard,R.C. and Stroud,I.A.: Stepwise Construction of Polyhedra in Geometric Modelling, Mathematical Methods in Computer Graphics and Design (Ed. by K.W.Brodlie) Academic Press, pp. 123-141, 1980.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>889976</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Coons,S.A.: Surfaces for Computer Aided Design of Space Forms, MIT Project MAC TR-41, June 1967.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Catmull,E. and Clark,J.: Recursively Generated B-spline Surfaces on Arbitrary Topological Meshes, Computer Aided Design, pp. 350-355, vol. 10, no.6, November, 1978.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Doo,D.: A Subdivision Algorithm for Smoothing Down Irregular Shaped Polyhedrons, Proc. Conf. Interactive Technique in CAD,. pp. 157-165, IEEE Computer Society 78CH1289-8C, 1978.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578513</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Faux,I.D. and Pratt,M.J.: Computational Geometry for Design and Manufacture, Chapter 7, ELLIS HORWOOD LIMITED, 1979.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Forrest,A.R.: On Coons and Other Methods for the Representation of Curved Surfaces, Computer Graphics and Image Processing 1, pp. 341-359, 1972.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807401</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Forrest,A.R.: A Unified Approach to Geometric Modelling, SIGGRAPH 78 Proceedings, pp. 264-269, 1978.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Gregory,J.A.: Smooth Interpolation Without Twist Constraints, in Computer Aided Geometric Design (R. E. Barnhill and R. F. Riesenfeld, Eds.), pp. 71-87, Academic Press, New York, 1974.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Hosaka,M. and Kimura,F.: An Interactive Geometrical Design System with Handwriting Input, Information Processing 77, pp. 167-172, North-Holland, Amsterdam, 1977.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Hosaka,M. and Kimura,F.: Synthesis Methods of Curves and Surfaces in Interactive CAD, Proc. Conf. Interactive Technique in CAD, pp. 151-156, IEEE Computer Society 78CH1289-8C, 1978.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356833</ref_obj_id>
				<ref_obj_pid>356827</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Requicha,A.G.: Representation for Rigid Solids: Theory, Method and Systems, Computing Surveys, vol.12, no.4, pp. 437-464, December 1980.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>906872</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Riesenfeld,R.F.: Applications of B-spline Approximation to Geometric Problems of Computer Aided Design, Ph.D. thesis, Syracuse University, May 1973.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 DESIGN OF SOLIDS WITH FREE--FORM SURFACES Hiroaki Chiyokura &#38; Fumihiko Kimura Dept. of Precision 
Machinery Engineering University of Tokyo Hongo, Bunkyo-ku, Tokyo, JAPAN Abstract I. Introduction We 
propose a unified method of generating a wide range of three dimensional objects from polyhedra to solids 
with free-form surfaces. Modeling systems for polyhedra and systems for free-form surfaces have been 
developed independently in the past because of the difference in their underlying theory and practices. 
However, this is not desirable for a designer. So in this paper, we have shown a method in which a wide 
range of shapes are generated in one system by using local modifications. Local modifications are procedures 
used to change the shape of solids locally. The construction and the modification of three dimensional 
shapes by these procedures are natural and easy for a designer in many cases. The implementation of these 
procedures in a computer is easy and their execution does not require much time~ Our method to construct 
a solid with free-form surfaces consists of following three phases, i) A solid which serves as a basis 
of free-form shape design is generated by local modifications. Edges of this solid are straight lines 
but its faces are not necessarily flat planes. 2) From this model, a curve model which adequately represents 
the characteristics of a free-form shape is generated. 3) Surface equations interpolating over the curve 
model are generated. We have made a geometric modeling system MODIF. Using this system, a complicated 
solid with free-form surfaces can be designed easily. MODIF can generate color shaded pictures and cutter 
path data for making a real object model by NC machining tool. CR Categories and Subject Descriptors: 
1.3.5 [Computer Graphics] : Computational Geometry and Object Modeling - Curve, Surface, Solid and Object 
Modeling; J. 5 [Computer Applications] : Computer Aided Engineering - Computer Aided Design (CAD) Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by permission of the Association for Computing 
Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. &#38;#169; 
ACM 0-89791-109-1/83/007/0289 $00.75 In computer aided design and manufacturing, models representing 
the shapes of three dimensional objects play an important role. By using geometric information in the 
models, a computer can do various engineering applications such as drafting and planning for NC machining. 
Therefore, how these objects are modeled in a computer is an important problem. However, practical engineering 
objects have a wide range of shapes from polyhedra to free-form shapes, that is, solids with free-form 
surfaces. So we have devised a unified method for modeling a wide range of shapes with more ease and 
developed a geometric modeling system MODIF. Broadly speaking, there are two fields in computer aided 
geometric design, that is, sculptured surface modeling and solid modeling. The systems in each field 
have been developed independently in the pest because of the difference in their underlying theory and 
practices [1Z]. Sculptured surface modeling is a study for representing curved surfaces which are involved 
in such shapes as the bodies of a car or an airplane. Various mathematical expressions for surface shape 
design have been proposed by Coons [5] , Be zier [3] and Riesenfeld [15]. They have been used in many 
fields of practical applications. There are already many methods for representing sculptured surfaces 
by these patches, and many systems have developed. But there still remain many problems for practical 
shape design. In solid modeling, three dimensional objects are represented as rigid solids. The usefulness 
of a solid model has been discussed by many people and solid modeling systems have been implemented in 
several places [12,14]. In these systems, complicated shapes are usually constructed from some primitive 
elements such as cubes and cylinders by set operations (sometimes called Boolean operations). The set 
operation has many advantages for constructing shapes and detecting interference between objects. But 
primitive solids are normally polyhedra or simple curved surface objects such as cylinders and cones. 
If simple free-form shapes are added as primitive solids, it become difficult to develop the set operations 
for these. In shape design process, it is desirable to represent a wide range of shapes from polyhedra 
to free-form shapes in one system. We therefore propose a unified method, in which a wide range of Computer 
Graphics Volume 17, Number 3 July 1983 shapes can be constructed by local modification procedures. Using 
local modifications, geometrical and topological information representing the shape of a solid can be 
changed locally by producing, deleting and moving local elements of a solid such as faces, edges and 
vertices. Our method to construct a free-form shape model consists of the following process. First, a 
polyhedral model which serves as a basis of free-form shape design is generated by local modifications. 
Next, a curve model for a free-form shape is generated from this polyhedral model. Finally, surface equations 
interpolating over the curve model are generated. The construction of the shapes by using local modifications 
can naturally be understood for a designer in many cases. From an implementer's point of view, local 
modifications and our surface interpolation method are easy and their execution in a computer does not 
require much time. So our method is feasible for interactive shape design. The Euler operation which 
is a basis of local modifications was developed by Batmgart [2]. By this operation, faces, edges and 
vertices are added or deleted according to the Euler equation for solid objects. Later, Braid et. al 
[4] developed Stepwise Construction method as an extended Batnngart's method. While these operations 
have been used in order to construct polyhedra and solids with simple curved surfaces, we devised a rounding 
modification as one of local modifications to round off edges and corners of solids locally, and use 
local modifications to construct complex free-form shapes. The methods for rounding off corners and 
edges of polyhedra have been proposed by Doo et. al [6,7]. In Doo's method, extroudinary points are represented 
by many subdivided patches and globally rounded surfaces are generated directly from polyhedra. By such 
means, it is difficult to round off a solid locally or generate sharp edge curves on which two different 
surfaces meet. Not only smoothness of surfaces but also characteristic curves are important for a designer. 
So we have devised a new rounding method which solves these problems. Since our method is divided into 
a phase for producing polyhedral models and a phase for producing curve models distinctly, rough modifications 
can be done to polyhedral models and precise modifications can be done to curve models. It is therefore 
possible to realize subtle features of free-form shapes which a designer wishes. Section 2 of this paper 
shows the total process of our free-form shape design and section 3 discusses the modifications of polyhedra, 
which are the bases for our new method. Section 4 describes a rounding modification for modeling free-form 
shapes. In section 5, a new method for interpolating curve models by surface patches is shown. 2. Free-Form 
Shape Design Process Our free-form shape design process consists of the following three phases. i) 
A solid which serves as a basis of a free-form shape design is generated by local modifications. Edges 
of this solid are straight lines, but faces need not be flat surfaces. This solid will be referred to 
as a basis solid in this paper. A solid model in MODIF has the topological information, how faces, edges 
and vertices are connected, represented by winged-edge structure [2]. But since this model does not 
store surfaces equations, it is allowable to generate a solid with twisted faces. 2) Three dimensional 
curve models are generated from basis solids by rounding modification, and then precise curve modifications 
are done. For rounding modifications, a designer adds the information for rounding off corners and edges 
to all the edges in the model. If he wishes that an edge should be rounded off, the information "0" is 
added. If he does not wish so, the information "i" is added. According to this information, new curved 
or straight edges are made instead of edges and vertices in the original solid, and a curve model for 
a free-form shape are built from these edges. Fig.l shows an example of rounding modifications. The edges 
of the curve models consist of cubic Bezier curves and straight lines. The curve model has the topological 
information similar to a basis solid. (a) (b) Fig.l. An example of a rounding modification 3) Smooth 
surface patches interpolating over faces of the curve model are generated automatically and the model 
is represented as a closed space in a computer. Because surface patches are defined only by the boundary 
curves with ease, the model in MODIF does not store surface patch expression. Only specified faces are 
interpolated when surface equations are necessary, for example, when cutter path data and color shaded 
pictures are generated. Our method can interpolate over not only four-sided faces but also faces having 
three, five or more edges. In the third phase, a designer does not modify the shape but check the smoothness 
of surface patches defined by the boundary curves. For this purpose, MODIF can draw cross sections and 
generate color shaded pictures and cutter path data for making real object models by NC machining tool. 
When a designer want to change the shape, he should return to basis solid or curve model design phase 
and modify these models. In our method, the important task of a designer is to produce curve models which 
adequately represent the characteristics of free-form shapes. And then, the smooth surface patches are 
automatically generated over the given curve model. So the final curve model must not have curved faces 
whose interpolation is difficult. For example, these are faces whose boundary shapes are extremely concave. 
If the curve model has these faces, a designer must subdivide these faces by adding edges in the second 
phase or return the first phase and modify basis solids in order not to produce these faces by rounding 
modification.  3. Local Modifications for Polyhedra Set operations have been used as primary procedures 
for solid construction in many geometric modeling systems. However, when the shape of a solid is complicated, 
set operations are not necessarily reliable, and require much computational time. So in MODIF, local 
modifications are used to construct and modify solids. They are easy and natural for a designer. Fig.2 
shows a summary of the functions of local modifications procedures. Lifting is an operation to add a 
solid generated by Vertex Edge Face Faceset Lift Make Kill Move Fig.2. Local modification procedures 
for a polyhedron (d) sweeping local elements to the original solid. Making and killing change the topology 
of solids. These topological operations are often used as preparations for lifting and a rounding modification. 
Also they are used to implement the rounding modification. This is described in the next section in detail. 
Moving is an operation to translate local elements. It is difficult to change shapes by moving, if the 
condition that solids must be polyhedra is imposed. But this is a useful operation to construct basis 
solids. Fig.3 shows the process that a basis solid for a telephone is ganerated by local modifications. 
 When a designer uses local modifications in MCDIF, local elements such as faces and edges are usually 
specified by a graphic cursor and movements vectors inputted by keyboad. In local modifications, the 
topological consistency of an object is kept automatically, but its geometric consistency is not always 
kept. So if the designer makes mistakes in the selection of local elements and a distance of a movement 
for lifting and moving, a model of an object intersecting itself is sometimes constructed. By drawing 
the model on a graphic display after this modification, a designer can usually find these errors. However, 
the program for checking the correctness of the model will be required. This still remains as our next 
study. 4. Rounding Modification for Free-Form Shape Design 4.1 The rule of new edges generation This 
section shows how new curved or straight edges of a free-form shape are generated according to edges 
and vertices of a basis solid with rounding information when rounding modifications are done. This rule 
of new edge generation is for a designer to understand how to specify rounding information '0' and 'i' 
of edges. When we produced the rule, we considered that solids with dangling edges must not  be generated 
even if rounding information is given arbitrarily. But this rule was produced in an experiential point 
of view and does not have theoretical bases. So various modifications and extensions of this rule for 
a designer's wish will be possible. First we show how new edges are made according to the edges of basis 
solids. Fig.4 shows that edge E~ belongs to two faces F 1 and F 2 and runs betwee~ vertices V 1 and V 
9. El, E2, E 3 and E 4 are edges touching E0, F 1 and F 2. P6ints Ps, G 1 and G 2 are the centers of 
edge Es and face~ F 1 and F 2. Here, we define that the centers of an edge and a face are the average 
of coordinates of vertices belonging to them. When the information for rounding off edge Es is "0", a 
curve segment is made, as shown oO 1 PO G2 Fig.3. Process of basis solid generation Fig.4. Edge of 
a basis solid Computer Graphics Fig.5(a). The end points of the curve are G. and G~ and tangents at 
G 1 and G 2 are line G.P 0 a~d P_G^~ Then, when the information is "I", two ~ine segments G.P_ and P_G 
2 are produced, as shown in Fig.5(b). B~t~if new e~ges are produced only by this rule, the model will 
have some unnecessary edges for the representation of the shape. So when the information of all edges 
around a face in a basis solid is "i", a new edge is not made on the face, as shown in Fig.5(c). Also, 
when edges El, E2, Eq and E 4 have the information "I", t~e rounding information of edge E 0 are ignored 
and a new edge will not be not made, as shown in Fig.5(d). {a) (b) (c] (d) Fig.5. New edges generated 
on an edge of a basis solid  When new curved edges are produced according to edges belonging to a face 
whose boundary shape is concave, a self-intersected solid is sometimes generated, because new vertices 
are produced on the outside of the faces. In order to avoid this problem, a designer must subdivide the 
faces having edges with rounding information "0" into some convex faces by adding appropriate edges. 
There is no problem about the faces whose edges all have the rounding information "i". Such subdivision 
is not troublesome but it will be necessary to devise a rule for rounding over edges touching concave 
faces. Next, we show the rule of making new edges for a free-form shape according to vertices of a basis 
solid. Fig.6 shows that vertex V 0 is connected to edges E.(i=l..n). Points P.(i=l..n) are the centers 
 1 1  of these edges. Vertex V 0 zs rounded off accordlng to the rounding information of edges E i. 
When there E2 92   ~, ~= E1 Vo E4 Fig. 6. Vertex of a basis solid  Volume 17, Number 3 July 1983 
is no edge or one edge which has the information "i", no edge is made since vertex V_ is rounded off, 
as shown in Fig.7(a) and (b). When~only two edges E k and E m (l<k,m<n) have the information "i", a curve 
is m~de as an--edge, as shown in Fig.7(c). The end points of the curve are P and P , and the tangents 
at P. and P are lines P. V~_ and V~ . When K m Io . ~a,m there are more than two edge~ whzch have the 
information "i", line segments are made from the centers of these edges to vertex V0, as shown in Fig. 
7 (d). 00 0 I  / (b) Em ~.= 1 vo Pk Ek VO   (c) ~ (d) Fig.7. New edges generated on vertices of 
a solid When a curve is generated in the rounding modification, three points Ps, P1 and P2 are always 
given, as shown in Fig.8. Ps~and~P 1 are end points of the curve generated. Line PsPI is a tangent at 
 P. and line PIP2 is a tangent at P^. This condition c~n be satisfied by a quadratic ~ezier curve whose 
control points three points P_, P. and P~. However, I .Z a quadratic curve is not nece~sarzly falr. 
When the length of line P-P1 is the same as the length of line PIP^, i~ is desirable that a curve approximating 
a arc is generated. So in the rounding modification, the method are used which can satisfy this requirement 
by using a cubic Bezier curve. Designers will wish to control the shape of curves when curve models 
are generated. So we show one simple method. The shape of an curved edge in a free-form shape is determined 
by the position of the centers of a face or an edge in an original solid. P1 % Fig.8. Condition of 
a curve generation  292 So, if appropriate weights are given to vertices, the centers of faces and edges 
are moved toward vertices with large weights. By using this property, we can control the shape of the 
curves with ease. Fig.9 shows examples done by this method. The shape shown in Fig.9(b) is made from 
the polyhedron shown in Fig.9(a), whose edges El, E 9 and E~ are given the rounding information '0' ana 
weights of all vertices are i. The shape shown in Fig.9(c) is made by changing the weights of vertices 
V0, VI, V 2 and V 3 to 9, 3, 3 and 3 respectively. l 3 i /. Fig.9. Rounding control by adding weights 
 to vertices 4.2 Implementation of a Rounding Modification This section shows how to implement rounding 
modifications. The rule described in the previous section explains only how to make curves and lines. 
This is not for the implementation. It is difficult to make models having the topological information 
from many curves and i ines. So in the implementation, necessary edges added to the solid and then unnecessary 
edges are deleted according the rule, so that the model can keep the topological information. Fig.10 
shows how this procedure is done in a computer. The solid shown in Fig.10(a) is a cube, whose three edges 
have the information '0' and all other edges have the information 'I' First, new edges from the center 
of faces to the center of edges are made and faces are subdivided as shown in Fig.10(b). Here, if all 
edges of a face are 'i', the face will not be subdivided. According to the rule of generating new edges 
around vertices /" (a) (b)  of the original solid, unnecessary edges are deleted and curved edges 
will be made from two straight edges, as shown in Fig.10(c). Similarly, around the vertices made on the 
center of all original edges, edges are deleted or curved edges are made, as shown in Fig.10(d). These 
procedures can be composed by local modifications previous section. for polyhedra described in the 4.3 
Examples of Rounding Modifications Some examples of the models shown in Fig.ll Fig.12 are produced by 
MODIF. Fig.ll(a) and and Fig.12(a) are basis solid models which are produced by local modifications 
described in section 3. Fig.ll(b) and Fig.12(b) are curve models for free-form shapes which are produced 
from the basis solids by rounding modification. (a) ..~Jj/ (b) Fig. ii. Telephone (a)  ~.. !!  Fig.iZ. 
Implementation process of the ......... rounding modification Fig.12. Space schuttle 293 5. Interpolation 
over Faces The curve models produced by rounding modification usually include non-rectangular faces 
and vertices connected to three, five or more edges. In this section, an interpolation method of such 
irregular curve model is shown. Bicubic Coons patches and Bezier patches have been used to interpolate 
over curve models. However, these are generally patches for interpolating only over regular curve models 
shown in Fig.13 under the condition that the directions of the surface normal are continuous across the 
boundary. Faces of these models always have four curves, and its vertices are defined as cross points 
of two curves. Also, the cross-boundary tangent magnitude ratio k must be constant along the common boundary 
[8]. For example, k], k~and k~ in Fig.13 should have the same value. This restriction is very severe 
for representing free-form shapes by curves. We therefore devised a method of generating surfaces whose 
normal directions are continuous over curve models having irregular regions shown in Fig.14. Fig.14(a) 
shows the regular curve model whose cross-boundary tangent magnitude ratio is not constant. Fig.14(b) 
shows one example that three or more than four patches meet at one corner. In addition, Fig.14(c) shows 
a non-rectangular face. In this paper, we assume that designers should produce a curve model which represents 
the characteristics of a free-form shape adequately. So when one face is interpolated in our method, 
patches are defined only by edges attached to the interpolated face. For example, if a face has four 
sides, a patch is defined only by four edges. Because this method is simple, the implementation is easy 
and its execution does not require much time. This means that the internal model of solids in a computer 
need not have the control points of the patches if a modeling system has our interpolation Fig.13. Regular 
curve model  procedure. So MODIF does not store patch data. Generally speaking, large memory area is 
used in order to store patch data. Therefore, it is possible to represent complicated free-form shapes 
by relatively small area in MODIF. 5.1 Gregory Patch This section describes about a patch used in our 
interpolation method. The bicubically blended Coons patch S(u,w) [9] can interpolate four boundary curves 
and normal derivatives along boundary curves, as shown in Fig. 15. However, these normal derivatives 
can not be specified independently. We must specify these, satisfying the following equation on four 
corners of a patch. ~2S (u,w)/~uOw = ~2S(u,w)/OwOu (i) This is called a "compatibility condition" [i]. 
 When various surfaces are interpolated by this patch, this condition becomes constraint. So Gregory 
[II] has modified an equation of this patch and devised a new patch whose four normal derivatives can 
be specified with no constraint. In this paper, we modify a bicubic Bezier patch [13] similarly which 
has good properties for shape design, and propose an interpolation method using this patch. We call 
this new patch a "Gregory patch". As shown in Fig.16, this patch S(u,w) is defined by twenty control 
points and the equations are as follows. . Fig.15. Normal derivatives of a patch .... ;" / Z --~ 
32  (a) (b) (c) P30 Fig.14. Irregular curve regions Fig. 16. Gregory patch S(u,w) = (I-u+uE)3 (l_w+wF)3 
P00(u,w) (2) 0<u,w<l WPll,u 0 + uPII,0 w Pll (u,w) = U+W WPl2,u I + (l-U)Pl2,0 w PI2 (u,w) = (l-u) 
+ w (I-w)P21,u 0 + uP21,1 w P21(U,W) = u + (l--w) (I-w)P22,u I + (I-u)P22,1 w P22(u,w) = (l-u) + (l-w) 
 The shift operators E and F have the property that EPi = Pi and FP i. = P::.I. These equations (2) 
mea~ that ~ positio~ Ij . . . . . patch of a polnt on thzs Is represented as an addition of sixteen paints 
to which various weights are given. Point P'l moves on straight line segments between two control points 
PII . 0 and Pll ~ according to parameter u and w. SlmiYarly, ~561~Es P]9, Pg] and P~? move on straight 
line segments between ~SrrespoSaing two control points. Twelve other points which define boundary curves 
are fixed points. This patch has the convex hull property like a Bezier patch that an arbitrary point 
on the patch exists in the convex hull defined by all control points. This property is advantageous for 
rough interference check. 5.2 Connection of Patches In order to represent smooth free-form surfaces 
by patches, it is necessary that the surface normal directions are continuous across the boundary curves. 
In this section, we discuss the connection of two Gregory patches in preparation for explaining our interpolation 
method. Fig.17 shows connected patches Sa(U,W) and Sb(U,W) . In Fig.17, a., b:(i=0..3) and c.(i=0..2) 
are vectors between the c6ntrol points and~are defined as follows. a0 = P30 -P20 ' = P31 - P21,1w a 
a al a a a2 = P32 - P22,1w " = P33 - P23 b b , bl b b a a a3 a a b0 = PI0 -P00 = Pll,0w - P01 (3) 
 C 2 / Fig.17. Connection of two patches b b b3 b b b2 = PI2,0w - P02 ' = PI3 - P03 a a Cl a a , 
c2 a a c0 = P31 - P30 ' = P32 - P31 = P33 - P32 pa and pb are control points. . of patches Sa and S 
b respectively. The derzvatlves of parameters u and w along the boundary curves of the patches are defined 
by a., b. and c.. The derivatives are represented l by the fol~owzng ~ezler expresslons. ~Sa(l,w)/~u 
= 3 (l-w+wE)3a 0 ~Sb(0,w)/@u = 3 (l-w+wE)3b 0 (4) ~Sa(l,w)/~w = 3 (l-w+wE)2c0 Under the condition 
of normal direction continuity, three derivatives of a point on the boundary curve always exist in the 
same plane. This is represented as follows. ~S b (0 ,w) ~S a (i ,w) ~S a (i ,w) k(w)--+ h(w)-- (5) eu 
8u ~w k(w) and h(w) are scalar functions. At the patch corner V and V. in Fig.17, three vectors must 
satisfy t~e I . followlng equatlons in order to generate connected patches. b 0 = k0a 0 + h0c 0 (6) 
b 3 = kla 3 + hlC 2 k_, k., h 0 and h I are scalar constants and can be o~tai~ed easily because curved 
edges are already represented by Bezier curves. To satisfy equations (5) and simplify the computation, 
we ass~ne that k(w) and h(w) are linear functions as follows. k(w) = (1-w) k 0 + wk I (7) h(w) = (l-w)h 
0 + wh 1 We also require that the normal derivative of a patch S is quadratic to satisfy equation (5). 
This a zs represented as follows. a 0 - 3a I + 3a 2 - a 3 = 0 (8) The following equations, the relation 
of a., b i and ci, are given from equations (4), (5) and ~7). b I = (kl-k0)a0/3 + k0a I + 2h0ci/3 + 
hlC0/3 (9) b 2 = kla 2 - (kl-k0)a3/3 + h0c2/3 + 2hlCi/3 When the control points of two patches satisfy 
equations (8) and (9), normal directions of two patches become continuous. 5.3 Interpolation of four 
sided faces This section shows an integrated interpolation method of various four sided faces shown 
in Fig.ll and Fig.12(a) (b) . We showed equations (9) for connecting patches in the previous section. 
But by equations (9) alone, the control points of patches can not be determined. The normal derivative 
of patch S (u,w) must be specified under the condition that t~e function is quadratic. However, since 
the relation of the normal derivative and the shape of  bicubic patches are used in these method, the 
 the patch is not obvious, it is difficult for a  connections of patches on the internal curves and 
 designer to specify the derivative. There also the boundary curves are difficult. But our method remains 
another serious problem. If normal using Gregory patches can avoid this difficulty. derivatives are 
determined by equations (9) alone,  The detail of our interpolation method for the normal derivative 
of patch S will be quadratic . a .  non-rectangular faces will be appeared in the other but the derlvatlve 
of patch S h wlll become cublc. report. Fig.20 shows examples of interpolated This means that asymmetric-surfaces 
are generated non-rectangular faces. Fig.20 (a) (c) (e) show edges when symmetric curved edges are 
given. These and cross sections of solids. In addition, Fig.20 problems will be solved by the usage 
of a "basis (b)(d) (f) show control points of patches. Because patch". faces in which control points 
are not drown are planes, patches are not generated. As shown in Fig.18, we consider the connection 
of real patch &#38; and virtual basis patch S' in order to ,D a  determine the control polnts of S 
h. The normal derivative of S' is defined by the~boundary curves of patch &#38;. Vector a" in S' is a 
unit vector which .p ~. a is normal to c 0 and exlsts on a plane deflned by b_ and c~ Similarly, a" 
is defined by b~ and c^ an8 19" z is normal to c 2. ~hen the curve mo~el represents the characteristics 
of the shape adequately, the normal derivatives along the boundary curves changes smoothly. So we ass~ne 
that the derivatives are (a) (b)  linear and determine a~and a~. From equations (9), Fig.19. Process 
of non-rectangular face b I and b 2 are given. This procedure for earning b 1 interpolation and b 
2 is done on four edges of the interpolated face. Then one patch is generated. When the boundary curves 
of faces satisfy equations (6), two normal derivatives of respective basis patches defined by the boundary 
of patch S h and patch S become the same. Accordingly, two- patches ar a connected smoothly and problems 
described above are solved. The shape edge curves where two different surfaces meet does not satisfy 
equation (6). In such case, patches whose normal directions are not continuous are generated. In our 
method, the boundary curves determine if normal directions of patches are continuous or not. Tangent 
plane I C2 c, b2 Sb w c a~  u (c) Tangent pl "~e~0- Fig.18. Connection of a basis patch and a o 
N real patch  -N i:i / 5.4 Interpolation of non-rectangular faces  When non-rectangular faces are 
interpolated, the I ~< normal derivatives across the boundary curves are 1 .'" determined by using the 
basis patch, as shown in I . " Fig.19 (a). Then, internal curves satisfying these -~ derivatives are 
generated, and faces are subdivided~ by some rectangular faces, as shown in Fig.19(b). Finally, Gregory 
patches are generated over these rectangular faces under the condition that all patches satisfy equation 
(i) at the patch corner P where internal curves meet. These procedures are (f) done automatically. The 
methods using subdivisions Fig.20. Non-rectangular faces interpolated have been already proposed. Because 
conventional by Gregory patches this paper is one solution to this problem. If solld modeling systems 
use boundary representations, it is possible to add this method to the systems. And if systems have Euler 
operators (making edges, killing vertices and so on), it will become more easy to add the method. In 
this paper, we have shown a basic principle of modeling free-form shapes. The important matters in modeling 
free-form shapes are not only producing fine and precise models quickly but also flexible representations 
of the shape for various modifications and useful techniques of changing the shapes. Conventional curve 
and surface models can represent the shapes but are not necessarily feasible representations for various 
modifications. When the objects designed are polyhedra, it is not difficult to change the shape. However, 
when the object " has free-form surfaces, designer's wish for the shape becomes complex. Accordingly, 
the flexible representation and useful modifications will be necessary. In our method, the basis solid 
with rounding information is one flexible representation. Using the basis solid, it is possible to control 
curved surfaces and sharp curved edges where two different curved surfaces meet. But our rounding modifications 
and local modifications alone are not sufficient for free-form shape design. Extension of the rounding 
modification's rule and developing new modification's techniques are our future work. Acknowledgments 
 We wish to thank Prof. M.Hosaka and Prof. T.Sata for helpful suggestions. We also wish to thank M.Fujimoto 
and N.Okada for technical help in using the NC machine tool, and A.Hayashi and S.Uno for their assistance 
in implementing the color shading program. In addition, we thank K.Matsushima and Y.Ohno for variable 
comments on ealier version of this paper. References [8] Faux,I.D. and Pratt,M.J. : Computational Geometry 
for Design and Manufacture, Chapter 7, ELLIS HORWOOD LIMITED, 1979. [9] Forrest,A.R. : On Coons and 
Other Methods for the Representation of Curved Surfaces, Computer Graphics and Image Processing i, pp.341-359, 
1972. [10] Forrest,A.R. : A Unified Approach to Geometric Modelling, SIGGRAPH 78 Proceedings, pp.264-269, 
1978. [II] Gregory,J.A. : Smooth Interpolation Without TWist Constraints, in Computer Aided Geometric 
Design (R. E. Barnhill and R. F. Riesenfeld, Eds.), pp.71-87, Academic Press, New York, 1974. [12] Hosaka,M. 
and Kimura,F. : An Interactive Geometrical Design System with Handwriting Input, Information Processing 
77, pp.167-172, North-Holland, Amsterdam, 1977. [13] Hosaka,M. and Kimura,F. : Synthesis Methods of 
Curves and Surfaces in Interactive CAD, Proc. Conf. Interactive Technique in CAD, pp.151-156, IEEE Computer 
Society 78CH1289-8C, 1978. [14] Requicha,A.G. : Representation for Rigid Solids : Theory, Method and 
Systems, Computing Surveys, vol.12, no.4, pp.437-464, December 1980. [15] Riesenfeld,R.F. : Applications 
of B-spline Approximation to Geometric Problems of Computer Aided Design, Ph.D. thesis, Syracuse University, 
May 1973. [i] Barnhill,R.E., Brown,J.H. and Klucewicz,I.M. : A New TWist in Computer Aided Geometric 
Design, Computer Graphics and Image Processing 8, pp.78-91, 1978. [2] BaL~ngart,B.G. : GEOMED-A GEOMETRIC 
EDITOR, Stanford Artificial Intelligence Laboratory Memo AIM-232, Computer Science Department Report 
no. cs-414, May 1974. [3] Bezier,P.E. : Numerical Control - Mathematics and Applications, John Wiley 
and Sons, London, 1972. [4] Braid,I.C., Hillyard,R.C. and Stroud,I.A. : Stepwise Construction of Polyhedra 
in Geometric Modelling, Mathematical Methods in Computer Graphics and Design (Ed. by K.W.Brodlie) Academic 
Press, pp.123-141, 1980. [5] Coons,S.A. : Surfaces for Computer Aided Design of Space Forms, MIT Project 
MAC TR-41, June 1967. [6] Catmull,E. and Clark,J. : Recursively Generated B-spline Surfaces on Arbitrary 
Topological Meshes, Computer Aided Design, pp.350-355, vol.10, no.6, November, 1978. [7] Doo,D. : A 
Subdivision Algorithm for Smoothing Down Irregular Shaped Polyhedrons, Proc. Conf. Interactive Technique 
in CAD, pp.157-165, IEEE Computer Society 78CH1289-8C, 1978.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801161</article_id>
		<sort_key>299</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>36</seq_no>
		<title><![CDATA[Geometric modelling and display primitives towards specialised hardware]]></title>
		<page_from>299</page_from>
		<page_to>310</page_to>
		<doi_number>10.1145/800059.801161</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801161</url>
		<abstract>
			<par><![CDATA[<p>Work over the last ten years developing a simple geometric modelling scheme has led to the design of a high speed display processor capable of generating real time moving displays directly from a three dimensional model. The geometric model consists of a graph-matrix boundary representation linked to a boolean expression volume overlap representation. The architecture of the display processor is particularly suitable for implementation as a pipeline of VLSI components, and current work is exploring this possibility. A divide and conquer, quad tree algorithm applied to the boolean expression model allows the system to make use of scene coherence, and used with the hardware will make it possible to handle scenes of high complexity.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Hidden line/surface removal</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14145157</person_id>
				<author_profile_id><![CDATA[81100410077]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[A.]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Thomas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Mathematical Sciences, Durham University, Durham, ENGLAND]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[APPELL, A. 'Modelling in Three Dimensions', Interactive Graphics in Data Processing IBM Systems J. 7 No. 3,4 (1968).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806007</ref_obj_id>
				<ref_obj_pid>800196</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[APPELL, A. 'The Notion of Quantitative Invisibility, and the Machine Rendering of Solids', Proceedings ACM National Conference (1967).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>891970</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BAUMGART, B.G. 'Winged Edge Polyhedron Representation', Stanford University Computer Science Department, Stanford-320 (1972).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BAUMGART, B.G. Ph.D. Thesis 1974 Stanford University.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BRAID, I.C., HILLYARD, R.C. &amp; STROUD, I.A. 'Stepwise Construction of Polyhedra in Geometric Modelling'. (Ed) Brodlie, K.W. Mathematical Methods in Computer 'Graphics and Design, Academic Press (1980).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[COMBA, P.G. 'A Language for Three Dimensional Geometry', Interactive Graphics in Data Processing, IBM Systems J. 7 No. 3,4 (1968).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[COOK, B.G. 3'A Computer Representation of Plane Regional Boundaries', Australian Computer J. (1967).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[JONES, C. 'A New Approach to the Hidden Line Problem', Computer J. 14, part 3 (August 1971).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356626</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[SUTHERLAND, I. E., SPROULL, R.F. &amp; SHUMACKER, R.A. 'A Characterisation of Ten Hidden Surface Algorithms', Computing Surveys 6 No.1 (March 1974).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[THOMAS, A.L. The Application of Computer Graphics to Planning. British Council/Latin American Planners Seminar, Dept. of Urban Design and Regional Planning, University of Edinburgh, August 1972.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[THOMAS, A.L. Two Models for Spatial Information Processing and Display. NATO Advanced Study Institute on Display and Analysis of Spatial Data, University of Nottingham, 1973.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[THOMAS, A.L. 'Spatial Models in Computer Based Information Systems', Ph.D. Thesis, University of Edinburgh (April 1976).]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[THOMAS, A.L. 'Data Structures for Modelling Polygonal and Polyhedral Objects', First International Advanced Study Symposium on Topological Data Structures for Geographic Information Systems (1977. (Ed.) G. Dutton (Addison Wesley, 1979).]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[THOMAS, A.L. 'Hardware Display Processor, Displays Magazine (IPC Business Press Ltd., October 1979).]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[THOMAS, A. L. 'Micro Display-Processor Components', CAD 80 Conference Proceedings (IPC Science and Technology Press, March 1980).]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[WARNOCK, J.E. 'A Hidden-Surface Algorithm for Computer Generated Pictures', University of Utah, Computer Science Department Report TR4 - 15 (1969).]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>905548</ref_obj_id>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[WATKINS, G.S. 'A Real Time Visible Surface Algorithm', Thesis, Computer Science Department, University of Utah. UTECH-CSc-70-101 (June 1970).]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[WAUGH, T.C. &amp; THOMAS, A.L. Geographic Information Management and Mapping System. System Manual, Leyland Systems, Boston, June 1970.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[WEHRLI, R., SMITH, M.J. &amp; SMITH, E.F. 'The Architect's Computer Graphics AID', Thesis, Computer Science Department, University of Utah. UTECH-Csc-70-102 (June 1970).]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[WOODWARK, J.R. &amp; QUINLAN, K.M. 'The Derivation of Graphics from Volume Models by Recursive Subdivision of the Object Space', Proceedings, Computer Graphics '80 Conference.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 GEOMETRIC MODELLING AND DISPLAY PRIMITIVES TOWARDS SPECIALISED HARDWARE Dr. A.L. Thomas Department of 
Mathematical Sciences Durham University Durham ENGLAND Abstract Work over the last ten years developing 
a simple geometric modelling scheme has led to the design of a high speed display processor capable of 
generating real time moving displays directly from a three dimensional model. The geometric model consists 
of a graph-matrix boundary representation linked to a boolean expression volume overlap representation. 
The architecture of the display processor is particularly suitable for implementation as a pipeline of 
VLSI components, and current work is exploring this possibility. A divide and conquer, quad tree algorithm 
applied to the boolean expression model allows the system to make use of scene coherence9 and used with 
the hardware will make it possible to handle scenes of high complexity. CR Categories and Subject Descriptors 
1.3.1 [Computer Graphics] Hardware Architecture- Raster Display Devices; 1.3.5 [Computer Graphics] Computational 
Geometry and Object Modelling; 1.3.7 [Computer Graphics] Three Dimensional Graphics and Realism -Visible 
Line/Surface Elimination; B.7.1 [Integrated Circuits] Algorithms Implemented in Hardware, VLSI; C.1.2 
[Processor Architectures] Multiple Data Stream Architectures -Pipeline Processors. Introduction The current 
project arose from a longer term, more broadly based study which was started in 1970. The aim was to 
improve information processing techniques which depended on the use of drawings in the design work of 
architects and engineers by applying computer based methods. It became clear almost as soon as the study 
was started that for most engineering applications, products which were traditionally modelled as a set 
of two dimensional drawings would in the new environment be far more usefully modelled as three dimensional 
entities.(1) Since then most of the work has been concentrated on building three dimensional models of 
objects, and finding ways of using and displaying the results. The first objective was to find a way 
of describing three dimensional objects in a convenient formal language, which was versatile enough to 
allow a variety of computer models to be constructed for different applications. These object descriptions 
had to satisfy demands made on them at two levels. Firstly they had to be self consistent, non ambiguous 
and in a form which could act as an interface to higher level systems which might want to automa-tically 
create or manipulate object descriptions. Secondly they had to make man machine interaction simple and 
convenient to manage. The second main objective also arose from this need to improve the man machine 
interface and consisted of finding better ways of generating graphic displays directly from three dimensional 
models. Geometric Models Geometric models are multi-level data structures where the lowest level consists 
of primitive geometric components such as points, lines, planes, spheres, cylinders and even sculptured 
surface patches; and the upper levels consist of structures which compose these basic elements together 
in a way which defines the required shape. For example~ a general form for one type of object model often 
consists of a set of surface patches, linked to a boundary network which ties them together as a closed 
surface. If the surface patches in such a model are restricted to being planes9 then the model will be 
restricted to representing polyhedra. By 1972 two interrelated data stuctures had been defined which 
allowed polyhedra to be constructed in a moderately controlled way.(10,11) These two structures were 
a) graph-matrix volume models and derived from them b) boolean expression volume models. Permission to 
copy without fee all or part of this material is granted provided that the copies are not made or distributed 
for direct commercial advantage, the ACM copyright notice and the title of the publication and its date 
appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy otherwise, or to republish, requires a fee and/or specific permission. &#38;#169; ACM 0-89791-109-1/83/007/0299 
$00.75 Graph-Matrix Models The starting point in developing these models was the set of polygon-boundary 
lists used in contemporary systems to define the surface facets of a polyhedron(I), Following this~ the 
dual naming of edges, as used in the GIMMS(18) cartographic system was introduced to link together adjacent 
facet boundaries and cut down search time in volume editing operations. The edges of facets were usually 
defined by pairs of vertex labels, for example 5/4 in Figure 1; but it was also possible in a network 
to name edges by using the labels of the zones separated by each edge. In the example, 5/4 is C/B and 
the complete dual name for the edge becomes 5/4"C/B.(7) I~ C:_ 1,5,4-,~-,I (ch E#% A ', *) =' I Figure 
1 The consequence of using dual naming was that the polygon boundary lists were tied together in a single 
structure which was more complex to manipulate. This in itself was not a problem so long as the programmer 
could avoid having to work at the level of the linked lists all the time. One way to avoid this low level 
programming was to work through a limited set of well understood higher level procedures such as Euler 
Operators, which were more closely related to the geometric and topological properties of the complete 
model.(4,5) A related way was to convert the lists into a more readily comprehended intermediate data 
structure. This is the approach adopted below, and the structure used is the graph matrix. 123# 1~.3+ 
I li A 2.  I 2-1 A. 3 ! 3 iA ....... c B .-......... P Figure Figure 2 shows the way to code up a graph-matrix. 
If the edges of each facet are treated as a directed graph but instead of the boolean 0 or 1 the name 
of the facet is entered into the matrix, the result for a single facet will be that shown in Figure 2, 
and the result for a tetrahedron that shown in Figure 3. I I.CAB A .9-A DC B '2. ~" 3 5A. D c 4-CDiB" 
D 3 Figure 3 The dual naming of edges results in two forms for these matrices-the vertex facet form 
and the facet vertex form, both shown in Figure 3. The order in which vertex labels are taken round a 
polygon boundary has to be consistent, for example if a clockwise order is used to reference the inside 
of the polygon, then an anti-clockwise order will reference its outside. I~I.AND Fiqure 4 Consequently, 
the inside outside relationship for a polygon can be reversed by reversing the order of coordinates in 
the polygon boundary list. A similar operation applies to the volume matrices. Given a set of vertices 
and a matrix defining a solid, then the transpose matrix will model a void; in much the same way that 
the island is converted into a lake in Figure 4. ASCD AI~CD A 311 2 A 1 213 T 15 ~ ,4-3 B 31. I ~ c2 
I" 4-c I 4-.. 2 p 234-' Figure 5 Under certain conditions it is possible to add and multiply these matrices 
together. Addition is a simple operation once it is remembered that rows and columns are labelled and 
it is only the contents of cells indexed by the same names which can be added together. A geometrical 
interpretation for this addition is defining the union of the volumes which the matrices represent. Matrix 
multiplication based on a scalar operation which is a form of label comparison (A.A = A, A.0 = 0, A.B 
= 0, A+B = A+B) provides a way of recovering the order of facet boundary lists. Circuits, either round 
named facets or named vertices, can be obtained by following the changing column positions of a given 
name in a particular row, through a series of matrix multiplications.(13) However there is a simpler 
way to obtain the same result. Figure 6 These graph-matrices do not in themselves restrict the model 
to plane faceted objects; however if such a restriction is made, some very useful simplifications result: 
the immediate one being that all geometrical operations can be carried out on the volume's set of edges. 
Joining and Editing Building more complex volumes can be approached in two ways. The direct way is the 
one illustrated in Fioure 7. A collection of vertices are linked together as a set of adjacent tetrahedra 
which are then joined together. The outcome of this operation will be a fully triangulated surface network, 
and final editing to any required shape can be achieved by locally transforming vertex coordinates. 
Figure 7 The topological operation of joining two tetrahedra is demonstrated in Figures 8 and 9. The 
first step is to add the two component matrices together to give a single common matrix as shown in Figure 
8.  I /~BCD EF~H c A .:l +3 ' I B 31" 1:2 . . C I 2 ~ ~" 3 "~ D CL3'a - E E f" i.I. 1. Islsl~ F 
~ I I t I.... 171"15161 G, I.I. -15161-18] H I'l" J817161"1 G Figure 8 If the two separate volumes are 
merged at faces E and D so that the vertex pairs 2,5l 3,7 and 4,8 are made to coincide, then these two 
facets will vanish from the final volume. In Figure 9 vertex labels 5,7,8 are replaced by 2,],4 respectively, 
and rows and columns labelled D and E are removed. However it is an important property of these matrices 
that the set of labels in any column has to be the same as the set of labels in the corres-ponding row. 
Deleting the rows and columns labelled D and E will violate this property. It is a simple matter, however, 
to correct the matrix structure by locating the rows and columns which are missing the same label and 
entering it where they cross This bookkeeping operation is shown in Figure 9. A 6 c..DE. F~ H A[~II I*I~+~-FH~I 
ABcF~. I-I,l*ll',;l'lll A J +- "i3 D[~I~,~-I,IIII[IJ~III ii!  I Elill',}',',l] '"''''' --> c , ~ '*" 
 I--r'T I--i F ' 3" " 2 ~0  H 1,1 , ,. I. u . lal6l.l Fioure 9 Obtaining a specified object this way 
requires the number of edges, vertices and facets to be known. An intermediate triangulated volume has 
to be constructed and then edited to conform to the specification. This approach like the use of Euler 
operators, ensures a correct and complete volume model, An example of the editing operation used in this 
process consists of removing an edge to merge two facets into one-" shown in Fiqure 10 for B/F. A 6C 
GH ABCeH A '. ,4-r3 At, i 4-1.3 B F3 J i~46   t 2. 4-. _._.> C I 2 .4-- F -~ :,: (~ Gi2 . 4- 2 .~&#38; 
H 48. 6- H 14 " ~ ~ Figure iO This topological editing, however, can raise geometrical problems, if, 
for example, facets are to be maintained as plane surfaces Overlap and Selection An alternative way of 
generating volume models where non triangular plane facets are required can achieved by overlapping existing 
volumes and be then selecting the new volume from the result. th, Q ~_ CPEFGr Fe ~ F I ~r" ~3 s s 4-. 
85t  c 3+ .s.71 D "8 7 516  E I 5 6-1:z D. 7 F :'9 3 71G'. &#38; ~.' 8 -'~Jz Figure 11 It is 
a simple task to set up an accurate universe box whose facets are parallel to the axes planes of the 
coordinate system being used. Facets from a new convex volume can then be created by taking the plane 
of each of its facets in turn and using it as a cutting plane to cut off a portion of the initial box(19). 
The principal geometrical operation employed in this task is illustrated in Figure 12 and is the familiar 
line clipping function which uses the dot product of the coefficients of the cutting plane equation, 
and the coordinates of the end points of each edge being clipped P Figure 12 The simplest application 
of the cutting plane process produces the convex volume which is the intersection of the inside regions 
of the half spaces defined by the cutting planes A cutting plane (3 is illustrated in Figure 11. The 
first step using O is to test on which side of it each vertex in the universe volume lies. All edges 
with both vertices inside the cutting plane can be retained, and all edges with both vertices outside 
the cutting plane can be discarded Edges crossing the cutting plane must be clipped and the new end points 
entered into the matrix to replace the ones left out. Once this is done, the entries in the matrix can 
be completed by using the bookkeeping operation shown in Figure 13. ABC DE G A BC ~ EEr A I~ !4-i ~, 
Io A t r, 9 to 4- !e,:5 I C 8 II io4-. e /I 5 r---~ -~c D .aN 8r! 5t2 E I 5 I 5!. 12-9    ~,. 
~ E F " " "~J 9 po t l t2. ,-'~ G C. 9 to1 n I/2 Figure i] The simplest way of extending this process 
to create objects with concave surfaces is by overlapping convex volumes, and then creating their volume 
of union. Figu~'e 14 illustrates the union of two tetrahedra giving the labelling in the exploded view 
to the right. Vl +V:z Vt Vz ~- p 82 8 ~1 G 6 ~o tl   7 7 ~2~1" t3 Vl.Vz Fiqure 14 Figure 15 shows 
the volume matrix representing the figure Vl+V2 shown in Figure 1/4. It also shows a problem if these 
matrices are implemented using conventional arrays. The assumption that each edge only generates two 
facet names and two vertex labels is not necessarily true. For example, edge A/C is made up from two 
line segments 2/10 and 9/4, which if they are to be entered into the matrix means that more than one 
label must be entered in each matrix cell. A B c W 'K Y Z A I 4-+1o 2. II 12 9 5 + s I C +9 4- 3 ro 
14- 13 P l 3 2_ W 5 7 x Io 14- 5+11 8 Y II 13 5 8~rl " 7+IZ z 7 6 18"I~ Figure 15 Matrix Partitioninq 
In some cases it is possible to systematically restructure the matrix to remove this naming problem. 
If each pair of facets referencing multiple edge segments is duplicated, creating new facet names, and 
the vertex labels redistributed to maintain the correct matrix structure, then the result is a partitioned 
matrix of the form shown in Figure 16. The duplicated facets in Figure 16 have been given the new names 
AtI, C:.], X:K, Y:L, Z:M. ABc I>WX yZ 1"3" K LM A Vt s c P W !l ........ i Z I V~.V~ k: L M Fiqure 
16 The partitioning of the matrix corresponds to the volume being divided into sub-volumes. An interesting 
aspect of the example given is the way these sub-volumes are related to the original tetrahedra Vl and 
V2. The partition regenerates Vl and V2 and also a new volume which is modelled by the transpose of the 
matrix which would represent the intersection volumes of Vl and V2. If we were to use the two original 
matrices to represent the union of V1 and V2, simply by adding them together, then intuitively it can 
be seen that any zone of overlap would be counted twice, and such a model could not therefore be used 
to calculate the volume of the final figure, or its surface area using conventional methods. This double 
counting is removed in Figure 26 by including a form of negative volume which represents the zone of 
overlap. Implementations of Graph Matrices These graph matrices can be implemented as standard arrays 
for small simple volumes. However, if we consider Euler's relationship between the number of edges, facets 
and vertices of a polyhedron: n-m+r =2 where n: number of facets, m: number of edges, and r: number 
of vertices; then if the rows and columns of a particular matrix are labelled by the facet names then 
the array will be n x n. If we assume that n = r then m = 2n -2, and the approximate number of empty 
cells in-the array will be n 2 -4n + 4. If n = 16 there will be at least 196 empty cells in the 1-6 by 
16 array. Clearly, implementation as an array becomes grossly inefficient for larger volumes. An obvious 
alternative is to implement the matrices as sparse arrays, when the volume in question gets beyond a 
critical size. If these are implemented as linked lists and a few extra links are included in each cell-record, 
to make them easier to use, such as two way pointers between diagonally symmetrical cells, then an implementation 
reminiscent of a winged edge data structure starts to emerge.(:]) A slightly less obvious alternative 
arises from the way the matrices are used. Consider the union operation. In Figure 14 the first stage 
is to process all the edges stored as sets of 4 tuples A/B:t/2 using geometrical tests to see whether 
they need to be modified. The second stage then consists of using the matrix structure to carry out the 
bookkeeping tasks necessary to complete the new volume description. If only the edges which need to be 
modified are collected together and their incident facet labels are used to create a werkspace array, 
then in most cases this will be much smaller than the full volume array, but will still be adequate to 
allow the bookkeeping tasks to be completed. Accessing the new array can be achieved by indirect indexing 
as shown in Figure 17. L K WORKSP#,CI INI)E'#.ES }KILl I w[e,D]=5 Figure 17 This werkspace array 
could be more efficiently implemented as a list using associative memory, if the host computer provided 
such a facility. Contents addressable memory would appear to provide the best hardware primitives an 
which the use of these matrices could be developed. Boolean Expression Models In this project, the use 
of boolean expressions as a geometric modelling structure evolved from the use of graph matrix models.(6) 
They were used initially as a way of specifying the volume required from overlapping simpler volumes. 
The extension of the system to allow the cutting plane to be used in the boolean expressions led to their 
use as an independent model-" initially as a powerful language form for entering volume descriptions 
and later as a way of defining displays. The basic syntax to enter a polyhedron as a boolean expression 
model will be of the form- <statement> =:= NAME, '=',<right name>. <right name> ::= <plane definition>l 
<boolean expression>. <plane definition> ::= '(',NUMBER, NUMBER, NUMBER, NUMBER, ')', <boolean expression> 
::= <phrase>,'+',<boolean expression> I <phrase>. <phrase> ::= <factor>,'.',<phrase>l <factor>. <factor> 
,,: <operand> 1 '!',<operand>. <operand> ::= NAMEl'(',<boolean expression>,')'. tree where alternating 
levels represent boolean product and the boolean summation strings in turn. To achieve this result it 
is necessary to apply De Morgan's Theorem in the conversion, to change subexpressions of the form !(A+B) 
into !A.!B, so the expression !(A+B+!C).D + F creates one of the trees in Figure 18. ~I___4 ~ 1 ' I X 
~ C P i B c (0 Figure 18 Once this boolean expression data structure has been created, it can be used 
in a variety of ways. It can, as already suggested, be used to control the creation of graph-matrices. 
However, to do this it is necessary to define an operation on the boolean expression model, which is 
made up from overlapping subvolumes, that will generate a new model which is a boundary model more closely 
related to the graph matrix structure. Consider the triangles shown in Figure 19; if we create a boundary 
operator j~ then the edges of the two triangles can be givenby the two expressions given below, with 
their expansions. c ...:.  ~(A s c')= #As.c + #S. A c+#c A.B Figure 19 What these expansions do is 
to take each half space in turn, convert it into a surface and then clip it into a facet by selecting 
the part which lies within the appropriate boolean function of the remaining half spaces. The same process 
can be applied to more complex boolean expressions. It is possible to create a graph-matrix representation 
of the boundary of a convex solid or void using the cutting plane process already outlined. The graph 
matrix for any volume can be generated using the boundary operator expansion in the way illustrated in 
Figure 20 for the union of two tetrahedra. ~V~ V~. 4-/~V2. Vi . Applying this boolean expression syntax 
creates a tree data structure for a particular volume. Although the parse tree generated by syntax analysis 
is not the ideal form for this tree, it is a fairly simple task to convert the parse tree into a Knuth 
Fiqure 20 In this case the basic unit for the expansion of the expression is the convex volume made up 
in the form A.B.C.D or A+B+C+D where A,B,C,D represent plane half spaces. The boundaries of these volumes 
are created using the cutting plane process, and then the boundary expansion is used to determine which 
parts of these convex surfaces will be retained in the final volume matrix. The semantic stage of the 
boolean expression analysis, required to create these matrices, must collect together at each level in 
the expression tree all the plane half spaces and group then together to give the convex subvolumes used 
in the cutting plane process in the way shown in Figure 21. A.15. (~C'~.~ + H'). K "+L+M  (L,M ( A,B, 
k: (C,N G)')')') w X Y Z . .'. i .: :'..'. [ x. Cv+z)] .. , , , , ,,  ' ' # X. (v* z)] ] x ] Figure 
21 The convex volumes relabelled W,X,Y,Z in Figure 2t can each be converted into a graph-matrix directly, 
by using cutting planes. The set of edges created in this way can then be clipped using the boundary 
phrases generated by the boundary-operator expansion; and finally these edges can be entered into a new 
volume matrix: missing edge segments being filled in by the appropriate bookkeeping operation. Generating 
Displays for Interactive Use Both these forms of model provided a reasonably tractable way of building 
volume descriptions. However, in both cases when complex volumes were encountered, or when fine editing 
adjustments needed to be made, it became necessary to provide a display so that the process could be 
carried out interactively. In the early work this could only be done easily for the edge based models. 
This was because hidden line or hidden area removal algorithms at the time all seemed to use edge based 
models.(8,16,17) This restriction biased the initial explorations of volume construction methods towards 
the approach summarised in Figure 7, since this approach worked directly with edges and vertices and 
only required one process to create a display. The overlap approach, on the other hand, required two 
consecutive processes to create a display both computationally intensive. Edge Based Hidden Line Removal 
The hidden line removal problem for edge based models is illustrated in its general form in Figure 22, 
where the object is constructed from a collection of curved Batches.(12) Figure 22 Figure 23 In this 
diagram the silhouette edges of the objecJ: do not exist explicitly as edges in the model of the volume. 
They have to be derived from the view point position viewing direction and the geometry of the surface 
patches defining the volume. These new edges can be generated in two stages. First of all the boundary 
line between front facing surfaces and back facing surfaces can be generated as illustrated in Figure 
23 for the object in Figure 22. Figure 24 Secondly, once this contour is defined, it can be converted 
into the boundary of the visible front facing surfaces by simple line crossing overlap test, and the 
lines which lie on this section of the volume's surface can then be drawn in as shown in Figure 24. 
How this general problem is tackled clearly depends on the way that the surface patches are represented. 
If we restrict the patches to plane facets, then we obtain important simplifications.(9) The silhouette 
edges of an object can in this case be obtained as a subset of the facet edges which are used to model 
the volume itself. Processing these edges with other visible edges becomes a problem very similar to 
a polygon overlay analysis.(13) Unfortunately the best worst-case behaviour of this kind of algorithm 
is O(n 2) where n edges are being processed in an arrangement like that shown in Figure 25. Figure 25 
Fortunately, most scenes are built up in a simpler way than this example, and the problem in reality 
is how to produce an algorithm which will take advantage of the coherent structure of a scene to improve 
on this worst case behaviour. There seem to be two main, general approaches to carrying out this task. 
The first is based on a form of binary subdivision, either of the object space or of the display space, 
and results in a divide and conquer algorithm.(16) The second is based on sorting objects into spatial 
order, and this generally results in the object or display space being divided into strips.(17) Both 
these methods were applied successfully to the graph-matrix models, but none of the existing implementations 
seemed applicable to a boolean expression model based on half spaces. Ray Casting Algorithms Once the 
formal language advantages of the boolean expression models were reatised, ways were sought to simplify 
the display generation process associated with them. FeedbaCk was needed to check input, and it did not 
have to be very sophisticated to be useful. A model based on surface definitions as the only explicit 
geometr ic data needed a different approach, and this approach was provided by ray casting. Ray casting 
is one of the main geometric constructions of the perspectivist; however, in this context it can be said 
to emerge from work on display space algorithms. In the display space there is little point in taking 
the analysis of hidden areas beyond the accuracy that can be displayed by the resolution of the screen. 
In simple terms this means that it is adequate to follow a single viewing ray from the eye through each 
pixe] position in the display surface and selecting the first surface facet encountered as the visible 
surface for that pixe] position. This approach avoids the need to explicitly define silhouette edges 
for the general curved patch models, and it also allows illumination effects to be modelled in a direct 
way. The ray casting algorithm designed for use with the boolean expression mode] was a little more complex 
than selecting the first surface encountered, since the surfaces were infinite in extent, unlike the 
edge bound facets of the matrix models. Figure 26 illustrates one way in which the front and back surface 
of a convex object, defined by the intersection of half-spaces can be determined. (a) (b) ;o. ", . . 
 b , . , .. . :/ t~AX :FI~ON'I- I::'L.,a,NF_S M I N : BACK. PLANE8 Figure 26 The front surface of the 
convex object will be defined by taking the front plane which intersects the viewing ray at the greatest 
distance from the eye, as shown in Figure 26a, while the back surface wilt be defined by taking the back 
plane which intersects the viewing ray at the least distance from the eye, as shown in Figure 26b. Clearly 
these results are conditional on the back surface being behind the front surface, as shown in Figure 
27a. In other words, F must be less than B for the object to be defined. If more than one convex object 
is found along a given viewing ray, then the nearest Lo the eye wilt be the visible one, as shown in 
Figure 27b. (a) (b) > F<B : OB..TE'CT"I~I~ MIN: VISIBLE OB3"E~---'T l I I Figure 27 The distance from 
a plane surface to a display point R along a viewing ray can be calculated in the following way. In Figure 
28: MR = CR ME PE MR.PE = CR.(MR + RE) MR = RE.CR (PE-CR) &#38; / P!C..TU ~.E ,.1 PI.AN E \. \E VIEV,/INCr 
M/ .----'~ RAY ~-/f- ~ L- zB Figure 28 Since for a particular pixel position RE will be a constant for 
all planes, the distance to plane A can be expressed as the ratio= L1A where LIA represents the dot product 
of the coefficients of the plane equation of A and the coordinate of the point R, and L2A represents 
the corresponding product for the point E. In fact, the display algorithm given above does not require 
the exact distances to planes, merely the order in which they intersect the viewing ray to be provided. 
Consequently we can convert the relationship= LIA < LIB < L1C (L2A-LIA) (L2B-LIB) (L2C-L1C) to L1A < 
L1B < L1C LZA LZB L2C which is an important simplification because the value of this ratio lies between 
0 and i. If  L1 = A.X1 + B.Y1 + C.Z1 + D = Z [1] L2 A.X2 + B.Y2 + C.Z2 + D is used to define the display 
space shown in Figure 29, then we can express the relationship between the object space and the display 
space by re-arranging Ill to give where the matrix defines the perspective trans-formation. Once in this 
form it is simple to see how this matrix can be concatenated with rotation and translation matrices in 
the standard way, allowing us to work with a new plane in the display space given by: If the eye is assumed 
to be at the origin looking along the Z axis in the object space, then distance Z to this transformed 
plane at X=0 and Y=0 will b'e -D'/C'. If the display screen is placed parallel to the XY plane with raster 
lines parallel to the X axis, then if the spacing between pixels is u in the X direction and v in the 
Y direction, the increments i to the value of Z moving from pixel to pixel along a row will be A'.u/C' 
and along a column B'.v/C'. .Z (o,a~) "" DISPLAY h' - ~ X (~o,o? Figure 29 This approach allows us 
to obtain the distances from any given pixel to all the planes in a scene at a reasonable computational 
cost. The initial depth values can be calculated at some reference point, say the top left hand corner 
of the display screen. The depth values for the remaining pixels can then be generated by stepping from 
one pixel to the next, adding or subtracting the appropriate increment at each step. The only difficulty 
with this scheme occurs when a plane is perpendicular to the picture plane. In practice it is possible 
to limit all such planes to a slope represented byan increment of 2 in Z moving from one pixel to the 
next, since steeper slopes could not be displayed. This means all planes can be represented in the same 
way and the algorithm to process them can be correspondingly simple. The display file for the reference 
pixel position consists of a list of the form A.B.C*.D*+E.F* where the names stand for the initial depth 
values. The list is ordered so that in each product phrase back planes, marked by an asterisk, come after 
front planes. The boolean operators are associated directly with each following plane to control the 
display algorithm. At the beginning of the expression list, the first depth value is entered into a temporary 
storage space. It is then compared with subsequent depth values in the first product phrase. [f the depth 
value of any front plane is greater than it, then they are swapped. [f the depth value of any back plane 
is less than it, then the result is ignored and the process moves on to the next product phrase. When 
the end of the first product phrase is reached the first temporary depth value is copied to a second 
temporary storage location. At the end of subsequent product phrases, their first stage depth values 
will be copied to the second storage space only if they are smaller than the existing stored value. When 
the end of expression is reached the output from this second storage location will provide the visible 
surface for that pixel position. Once a plane has been processed in this way its depth value in the list 
can be incremented to define the surface at the next pixel position, ready for the whole process to be 
repeated. Display Hardware The display algorithm outlined above divides into two stages. The first generates 
the depth values for planes the second compares and selects the depth values along with associated property 
data, under the control of the boolean expression structure which defines the scene: Zx COIV~ PAI~.E 
S E LECr" Figure 30 Although this algorithm is computationally intensive, it is extremely simple, and 
can be implemented directly in low level hardware. More significantly it can be implemented as parallel 
hardware. q Figure 31 Figures 30, 31 and 32 illustrate the components of a processing unit designed 
to implement the algorithm. The incrementing unit shown in Figure 30 receives the depth values for the 
current pixel position and depending on the next pixel position adds to or subtracts from them either 
the appropriate mcrement's or Y increments. The depth values for the current pixel are passed to the 
comparison and selection unit shown in Figure 31. This consists of two similar units in sequence. The 
first extracts product-phrase results; in other words determines whether convex intersection volumes 
exist at the current pixe] position, the second stage selects the nearest convex volume, and in doing 
so generates a display of the union of product-phrase volumes. Figure 32 The property data unit in Figure 
32 acts as a slave unit that merely selects the property data of the planes selected in the main unit 
shown in Figure 31. These units are used in parallel to give the pipeline processor shown in Figure 53. 
Is 'L-'cr j l / _I I . SEL CrA-'l sE, = FlS -cr I . 4, 4, 4, > l=~,op E~-ty BOS Figure 33 This represents 
only one of four possible ways in which the basic processing units can be combined in parallel(24), but 
it is the design with the smallest number of inter processor links and can, in fact, be implemented as 
an integrated circuit, within the current limits of 64 to 80 pins per IC package.(15) By directing all 
output onto a bus, the actual number of processors placed on a single IC is not affected by the pin count, 
so this approach is well set to take advantage of the improvements in the scale of integration promised 
for the next decade. The basic display process can successfully create displays from boolean expressions 
of the form; A.B.C.D + E.F.G.H + .... at video rates, as can be seen from the colour plates, where output 
is given from an editing sequence using a prototype pipeline processor built in 1977. Displays are generated 
directly without using a frame buffer. More complex boolean expression models have to be converted into 
this form to use the algorithm. In early experiments using a software system, complex object descriptions 
were created and then automatic algebraic manipulation was employed, similar to that used in the design 
of combinatorial circuits to give the appropriate form for the display file. The difficulty which emerged 
from this work was the explosive expansion of the expression which defined volumes with concave surfaces. 
Consider the objects shown in Figure 34, containing three spheres. Initially the two examples can be 
modelled by equal-lengthed boolean expressions. If each sphere is the intersection volume of 200 plane 
facets, then for the second example we will have an expression of the form: (A1.AZ.A3...A200).(B1.B2...B200).(C1...C200) 
However, this will multiply out to give" A1.A2...A200.BI.Cl + A1.A2...A200.B1.C2 + .... A 1. A 2... A 
200.B 200. 2b~"0"6 The original string length will be 600 names long, but the expansion will be 8,080,000 
names long! This is clearly unacceptable. VI +V2 +V3 N/I .x,/2 .N/3 Figure 34 There are three ways in 
which this problem can be effectively tackled. 1. Build models in ways which specifically avoid a combinatorial 
explosion. 2. Employ the boundary expansion to obtain single level expressions. 3. Develop a divide 
and conquer strategy to make use of scene coherence.  A simple way of using the first approach is to 
start with an edge based model, and convert each facet polygon into a pyramid using some convenient internal 
point as the apex. This will convert the object into a set of convex volumes which can be used directly 
with the simple display algorithm described above. The second approach achieves a similar result by manipulating 
the boolean expression model. In the example in Figure 35 the boundary expansion starts with the convex 
surfaces A,B,C and D, and then clips them to give the boundary segments B1, B2, B3 and B4. The expanded 
display string becoming: /SA. [(~+B).~ ] + /SC. [A.~] + I$B. [A.~] +/SD.[A.(~'+B) ] Figure 35  This 
string can still be processed by the circuit outlined in Figure 31. The boundary surface, for example 
~A is handled in the same way as a simple convex volume. If it exists at a particular pixel position, 
then its depth value will be held in the first stage store in Figure 31. This depth value can then be 
tested against the clipping expression in the square brackets. The clipping expression can be viewed 
as a set of convex volumes related together in a boolean expression. The depth value will lie inside 
one of these convex volumes if it is less than the back surface depth value and greater than the front 
surface depth value of the convex volume. This is a boolean result: inside or outside, which can be combined 
with other results from the clipping expression for other component convex volumes. If the boundary lies 
inside the clipping volume, its depth value can be passed to the second stage selection process as before, 
otherwise it must be discarded. In the example shown in Figure 35, simple expansion gives a string length 
of 16,080,000 names, but boundary expansion gives a string length of 2,800 names from an original expression 
of 800 names. This is a major improvement and is achieved without making use of scene coherence. ' + 
 Figure 36 centre point of a quad to the whole area of the quad. This can be done by geometrically expanding 
each convex volume in the way illustrated in Figure 38. ~Y ~Y P, ~ P.." L . . X Z I" t DI SPrY SUI~.PAOE 
S~HON Figure 38 If a volume is expanded by the appropriate function of the quad size, then a point test 
at the centre of the quad can be used to determine whether the volume exists anywhere else in the quad. 
The simplest way to implement this volume expansion is to move each plane sideways the appropriate amount. 
This can be done by an operation on the depth values: Z E = Z -IdZx I IdZy I front faces Z E = z + IdZx 
I + IdZy I back faces Where the object is round as shown in Figure 39a, this is an adequate approach; 
where an object has a feathered edge as in Figure 39b, then it may expand too far in a particular direction 
for efficiency, In such a case the solution is to include vertical and horizontal planes boxing the object, 
before expansion, This gives a similar result to convolving the object with the quad as shown in Figure 
40, which is mathematically the ideal form of expansion, Scene Coherence The third approach adopts a 
recursive subdivision of the display screen fo[ the surface based boolean expression model, an approach 
first employed by Warnock(16) for edge based models. In Figure 36, a recursive incrementing procedure 
is illustrated for generating plane surfaces. This permits the visibility of a scene to be evaluated 
at the nodes of a quad-tree in the way shown in Figure 37. Q Figure 37 The first step is to establish 
the visibility of each convex volume in the model. This is done before the expression is expanded because 
it will be possible to carry out sub-expression pruning; for example, if A were not to be visible in 
Figure 35, then B, C and D would be irrelevant and could be ignored.(20) The next step is to extend the 
result obtained for the (a) (b) @ Figure 39  Once an object is found to be invisible, or not to exist 
in a quad, then its description can be removed from the scene description used for generating the display 
for that quad. This provides the basic mechanism for the divide and conquer algorithm. The display space 
is divided into four and the scene display list is reduced into four lists of quarter the original length. 
 , Figure 40 This algorithm can be improved in a variety of ways still under investigation. It is possible 
to geometrically contract volumes which prove to be visible before expansion. If they remain visible, 
then the conclusion is that they cover the quad in question9 and this can be used to remove objects with 
greater depth values from the display list for that quad. The most important overall aspect of this approach 
is its simplicity, and the way it allows display lists to be subdivided to the point where the number 
of planes in a list is less than or equal to the number of processors in a pipeline of the form shown 
in Figure 33. This means real-time display of scenes of high complexity will be possible using processors 
of the layout shown in Figure 41. ~DEL AqqON~ I COMT]aOL 4% 4, " I PlVI DE AND CONQL)h-~-- CONX/~I~3-- 
CIJ~VEI~ SUI~FACI~ TO PL.~N F_.S d. Figure 41 Conclusions The two main objectives for CAD systems appear 
to have been achieved. Current work is exploring the way that the boolean expression and matrix based 
models can be implemented as an extension to an existing high level language, as a specialised data type 
for geometric modelling and display purposes. If this work is successful it will allow higher level geometric 
modelling systems to be based on these new language primitives. The hardware develop- ments should lead 
to small real-time display systems~ and because they are based on overlap testing could also find application 
in future robot systems. This would allow the language extensions to be used in writing control systems 
which employ spatial information. AcknowledRements Discussion with Dr. M. Sabin, Dr. a. Woodwark and 
Dr. P. Willis on divide and conquer algorithms. Dr. M.3. Morant, Mr. D. Dwyer, VLSI Development: High 
Speed Display Processor Research Group, Durham University. Science Research Council grant number B/RG/90495 
to Heriot Watt University Department of Electrical and Electronic Engineering. References 1. APPELL, 
A. 'Modelling in Three Dimensions', Interactive Graphics in Data Processing iBM Systems J. 7 No. 3,4 
(1968). 2. APPELL9 A. 'The Notion of Quantitative Invisibility, and the Machine Rendering of Solids', 
Proceedings ACM National Conference (1967). 3. BAUMGART, B.G. 'Winged Edge Polyhedron Representation', 
Stanford University Computer Science Department, Stanford-320 (1972). 4. BAUMGART, B.G. Ph.D. Thesis 
1974 Stanford University. 5. BRAID, I.C., HILLYARD, R.C. &#38; STROUD, I.A.  'Stepwise Construction 
of Polyhedra in Geometric Modelling'. (Ed) Brodlie, K.W. Mathematical Methods in Computer 'Graphics and 
Design, Academic Press (1980). 6. COMBA, P.G. 'A Language for Three Dimensional Geometry', Interactive 
Graphics in Data Processing, IBM Systems J. 7 No. %4 (1968). 7. COOK, B.G. 'A Computer Representation 
of Plane Regional Boundaries', Australian Computer J.  (1967).  8. JONES, C. 'A New Appi:oach to the 
Hidden Line Problem', Computer J. 14, part } (August 1971). 9. SUTHERLAND, I.E., SPROULL, R.F. &#38; 
SHUMACKER, R.A. 'A Characterisation of Ten Hidden Surface Algorithms', Computing Surveys 6~ No.1 (March 
1974). 10. THOMAS, A.L. The Application of Computer Graphics to Planning. British Council/Latin American 
Planners Seminar, Dept. of Urban Design and Regional Planning, University of Edinburgh, August 1972. 
 11. "THOMAS, A.L. Two Models for Spatial Information Processing and Display. NATO Advanced Study Institute 
on Display and Analysis of Spatial Data, University of Nottingham, 1973. 12 THOMAS, A.L. 'Spatial Models 
in Computer Based Information Systems', Ph.D. Thesis, University of Edinburgh (April 1976).  13. THOMAS, 
A.L. 'Data Structures for Modelling Polygonal and Polyhedral Objects', First International Advanced Study 
Symposium on Topological Data Structures for Geographic Information Systems (1977. (Ed.) G. Dutton (Addison 
Wesley, 1979). 14. THOMAS, A.L. 'Hardware Display Processor, Displays Magazine (IPC Business Press Ltd., 
October  1979).  15. THOMAS, A.L. 'Micro Display-Processor Components', CAD 80 Conference Proceedings 
(IPC Science and Technology Press, March 1980). 16. WARNOCK, 3.E. 'A Hidden-Surface Algorithm for Computer 
Generated Pictures', University of Utah, Computer Science Department Report TR4 -15 (1969). 17. WATKINS, 
G.S. 'A Real Time Visible Surface Algorithm', Thesis, Computer Science Department, University of Utah. 
UTECH-CSc-70-101 (June 1970). 18. WAUGH, T.C. &#38; THOMAS, A.L. Geographic Information Management and 
Mapping System. System Manual, Leyland Systems, Boston, June 1970. 19. WEHRLI, R., SMITH, M.J. &#38; 
SMITH, E.F. 'The Architect's Computer Graphics AID', Thesis, Computer Science Oe.partment, University 
of Utah. UTECH-Csc-70-102 (June 1970). 20. WOO.DWARK, J.R. &#38; QUINLAN, K.M. 'The Derivation of Graphics 
from Volume Models by Recursive Subdivision of the Object Space', Proceedings, Computer Graphics '80 
Conference.    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801162</article_id>
		<sort_key>311</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>37</seq_no>
		<title><![CDATA[Artists interfacing with technology]]></title>
		<subtitle><![CDATA[Basic concepts of digital creation]]></subtitle>
		<page_from>311</page_from>
		<page_to>313</page_to>
		<doi_number>10.1145/800059.801162</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801162</url>
		<categories>
			<primary_category>
				<cat_node>J.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39028804</person_id>
				<author_profile_id><![CDATA[81100128202]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Frank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dietrich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[West Coast University]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P331974</person_id>
				<author_profile_id><![CDATA[81100574836]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Larry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cuba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[independent artist]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31039341</person_id>
				<author_profile_id><![CDATA[81100371395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Darcy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gerbarg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39037364</person_id>
				<author_profile_id><![CDATA[81100315933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lippman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15032716</person_id>
				<author_profile_id><![CDATA[81100455127]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sandin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Illinois - Chicago]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[COMPUTER GRAPHICS AND ART, ed. Grace Hertlein, 5 Vols. 1976-1981]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Cuba, Larry. The Rules of the Game, Computer Graphics World, Vol 2 #8, 1979, pp. 52-55]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Davis, Douglas, Art and the Future, New York: Praeger 1973]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Dietrich, Frank/ Molnar, Zsuzsa. Pictures by Funny Numbers, Creative Computing, Vol 7, #6, 1981 pp.102-107]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>4130</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Franke, Herbert W. Computer Graphics - Computer Art. New York: Phaidon 1973]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Gerbarg, Darcy. Art Gallery. Computer Graphics World, Vol 4, #7, 1981]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Kawaguchi, Yoichiro, Digital Image. Tokyo: ASCII Publishers. 1982]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Kranz, Stewart. Science &amp; Technology in the Arts. New York: Van Nostrand - Reinhold, 1974]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Leavitt, Ruth, ed. Artist and Computer. New York: Harmony 1976]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807465</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Lippman, Andrew. Movie-Maps: An Application of the Optical Videodisc to Computer Graphics. Proceedings: SIGGRAPH 80, Seattle, 1980, pp.32-42]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[________, Backer, D. Future Interactive Graphics: Personal Video. Proceedings NCGA Conference, Baltimore 1981]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>542853</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Malina, Frank, ed, Visual Art, Mathematics &amp; Computers, Oxford: Pergamon Press 1979]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Palyka, Duane. Artistic Reflections on Man-Machine Interfaces. Proceedings: Graph,s Interface, Toronto, 1982, pp.291-293.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Reichardt, Jasia. Ed. Cybernetic Serendipity: The Computer and the Arts. New York: Studio International, 1968]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Sandin, Dan. Videotape by de Lignieres, Christine/Morton, Phil. Video Data Bank, School of the Art Institute of Chicago, 1981]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Small Computers in the Arts: Proceedings: ist and 2nd Symposium, IEEE, Philadelphia 1981 and 1982]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Whitney, John. Digital Harmony. Peterborough: BYTE Books 1980]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Wilson, Stephen. Computer Art: Artificial Intelligence and the Arts. Leonardo Vo. 16 #1, 1983, pp.15-20]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Youngblood, Gene. Expanded Cinema. New York: Dutton 1970]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PANEL ARTISTS INTERFACING WITH TECHNOLOGY: BASIC CONCEPTS OF DIGITAL CREATION CHAIR: Frank Dietrich, 
West Coast University PANELISTS: Larry Cuba, independent artist Darcy Gerbarg, New York University Andrew 
Lippman, MIT Dan Sandin, University of Illinois - Chicago CHAIRMAN'S INTRODUCTION Computer art has 
in its short two decade history undergone significant changes. Initially, only a few scientists started 
 using the expensive machines in their research labs to generate visuals; now computers have become 
an affordable house- hold item, and accessible to children. The advent of graphic peripherals and pro- 
gramming languages specially designed for artists established the computer as a medium well suited 
for the creation of visual artwork. More than just a mechanical device, the computer challenges the 
technological artist to discover unknown visual terri- tory by finding appropriate concepts of digital 
creation. The computer is a gen- eral purpose machine capable of accom- plishing a variety of different 
tasks. The creative concepts chosen by the artist to define a particular aesthetic strategy remain the 
most important artistic ingredient, even though the specific con- figuration of the computer used determines 
 to a certain degree the final visual result. VI SUALI ZATION OF ABSTRACT STRUCTURES: Programming is 
an essential way of con- trolling digital machines. The artist structures the elements of the visual 
out- put in a logical fashion by breaking down the entire process of creation into finite steps. The 
result is a visualization of the abstract logic involved or a simula- tion of a geometrically conceived 
world. The strict determinism of the machine to execute prescribed rules offers the chance to systematically 
explore the complete set of possible combinations. Chance can be evoked by introduction of automatic 
ran- domness or by spontaneous artist-machine interact ion. CONNECTING THE COMPUTER TO REALITY : The 
 possibility of feeding visuals into the machine with an electronic pen or a video camera brought computer 
technology signi- ficantly closer to existing artistic activities. Painters and video artists don't 
have to give up their traditional expressive forms of creativity in order to gain the additional power 
of image pro- cessing methods unique to the computer. The real world becomes connected to the machine 
by sensing the brush stroke of the artist or by collecting the video data gathered by the camera. INTERACTIVE 
ENVIRONMENTS: Finally, the artist can set up a complex human-machine communication environment inviting 
the participation of an audience. The work becomes a process of ongoing collective creativity. The artist 
initially facili- tates the communication environment with the computer as an information processor and 
display device at its core. But the installation only starts generating com- munication and meaning if 
activated by the aud ienc e. Further artistic experiments in digital creation will contribute to a 
higher level of computer literacy and visual communcia- tion within our computerized society. COMPUTER 
GRAPHICS AND ART, ed. Grace Hertlein, 5 Vols. 1976-1981 Cuba, Larry. The Rules of the Game, Com- puter 
Graphics World, Vol 2 #8, 1979, pp. 52-55 Davis, Douglas, Art and the Future, New York: Praeger 1973 
 Dietrich, Frank/ Molnar, Zsuzsa. Pictures by Funny Numbers, Creative Computing, Vol 7, #6, 1981 pp.102-107 
 Franke, Herbert W. Computer Graphics - Computer Art. New York: Phaidon 1973 Gerbarg, Darcy. Art Gallery. 
Computer Graphics World, Vol 4, #7, 1981 Kawaguchi, Yoichiro, Digital Image. Tokyo: ASCII Publishers. 
1982 Kranz, Stewart. Science &#38; Technology in the Arts. New York: Van Nostrand -Rein- hold, 1974 
 311 Leavitt, Ruth, ed. Artist and Computer. New York: Harmony 1976 Lippman, Andrew. Movie-Maps: An 
Applica- tion of the Optical Videodisc to Computer Graphics. Proceedings: SIGGRAPH 80, Seat- tle, 1980, 
pp.32-42 , Backer, D. Future Interactive Graphics: Personal Video. Proceedings NCGA Conference, Baltimore 
1981 Malina, Frank, ed, Visual Art, Mathematics &#38; Computers, Oxford: Pergamon Press 1979 Palyka, 
Duane. Artistic Reflections on Man-Machine Interfaces. Proceedings: Graph,s Interface, Toronto, 1982, 
pp.291-293. Reichardt, Jasia. Ed. Cybernetic Serendi- pity: The Computer and the Arts. New York: Studio 
International, 1968 Sandin, Dan. Videotape by de Lignieres, Christine/Morton, Phil. Video Data Bank, 
 School of the Art Institute of Chicago, 1981 Small Computers in the Arts: Proceedings: ist and 2nd 
Symposium, IEEE, Philadelphia 1981 and 1982 Whitney, John. Digital Harmony. Peter- borough: BYTE Books 
1980 Wilson, Stephen. Computer Art: Artifi- cial Intelligence and the Arts. Leonardo Vo. 16 #i, 1983, 
pp.15-20 Youngblood, Gene. Expanded Cinema. New York: Dutton 1970 PANELISTS' ABSTRACTS: Larry Cuba, 
Independent Artist Paralleling the development of the commer- cial film industry, there is a history 
of individual artists working within the dis- cipline of the fine arts, creating an alternative film 
art guided by the aesthetics of music and painting rather than drama. Even before film technology was 
developed, the founders of abstract art, Kandinsky in particular, invoked musical analogies to describe 
their artis- tic goals. Many saw film as a new tech- nology that would make a new art possible, an art 
that would combine abstract paint- ing and music. Artists involved in the movements of Futu- rism in 
Italy and Cubism in France began making abstract films as early as 1910. Throughout the history of abstract 
film, artists continually used musical analogies in the titles and descriptions of their work: "Chromatic 
Music", Colored Rhythm", "Rhythmus 21", and "Diagonal Symphony" They also encountered the many production 
problems associated with animation. Most returned to painting or turned to live action filmmaking. Some 
realized that film by itself was not the musical/ visual art technology they had hoped it would be, 
and that another was needed to produce the countless drawings required. In 1921, Van Doesburg wrote, 
"...the need for these rigorously precise drawings demand that...these drawings be mechanically pro- 
 duced... When one realizes that 300 draw- ings are needed for a film "score", one can understand that 
abstract filmmaking is an area where mechanical means of drawing can render important services." It 
has been said that these film artists are like aviation's early pioneers who tried to fly with wings 
of feather and wax. The graphics computer may be (at last) the flying machine of musical/ visual art, 
but we have yet to learn how to fly. Darcy Gerbarg, New York University My background is painting. 
I've used many mediums including acrylic, watercolor, gouache, silkscreen, lithography, etching, paper, 
ceramics, etc. The computer is my favorite. It is closest to my sensibili- ties. It allows me to create 
images in an endless variety of ways and constantly challenges me to try new ideas. In a way, making 
the image on the computer is the easy part. I use highly interactive, user friendly computer graphic 
paint systems. Because of this, the transition from pig- ment (paint) to computers is not as great 
 as one might imagine. Instead of mixing a pallet of paint before beginning to paint, I mix light to 
create a colormap. Color- maps and pallets are very similar. Each contains a specific set of colors. 
When working with pigment, I would choose brushes to paint with of varying sizes according to my needs. 
On the computer I create brushes I wish to use; thick ones, thin ones, multicolor ones. There is vir- 
 tually no delay between the act of creat- ing a picture on a computer and seeing it created. The picture 
happens in "real" time. If I choose to change a color or remove a line, I can do it easily on a computer. 
I can work and rework a picture until it is exactly what I want and then have the computer give me 
a full color slide of that image. The image can also be stored in the computer's memory, mani- pulated 
or transformed. The hard part is taking the image off the system and turn- ing it into an art object, 
i.e., physical reality. Until I have finished art work in some physical medium the image exists only 
in the memory of the computer or the CRT (TV screen). Many artists are interested in moving pictures, 
film and video, but I am not. I want my images to hold up over time as all great works of art have 
from the past. The challenge for 312 me is to develop ways of creating images on Computers and then 
take them and fabri- cate finished works that can hang on walls, cover floors or ceilings. To date, I 
have done editions of prints, c ibachromes, ceramic tiles, printed fabric, and murals on canvas using 
my com- puter images. Each physical medium has its strengths and limitations. Part of the difficulty 
is determining what medium will enhance a specific image. Finding new ways to take information from the 
com- puter system so that the information is in a useable form is another area that requires alot of 
research. Each time I create a piece it is the culmination of experimentation with many parameters. Of 
course I remain an artist and keep aesthetic concerns in mind as I work. I am a formalist painter; I 
paint directly with light, colored light. Andrew Lippman, MIT An interesting and perhaps unavoidable 
corollary of technical innovations is that they are often misunderstood and parochi- ally developed until 
compelling applica- tions emerge and become directive forces. As an example, electricity was used first 
to further research in the physical sci- ences and later as a carrier of power and information. Digital 
computing is as revolutionary, and only now is it develop- ing in the context of applications; its history 
is as an arithmetic processor but its future is intertwined with artificial intelligence and signal processing 
as well. In social terms, it is possible that the impact of the arithmetic revolu- tion may be past, 
but the impact of the communicative revolution caused by comput- ing is only beginning. This presentation 
will concentrate on the emergence of personal computing as a creative and cognitive aid, a communica- 
tions medium in its own right. In con- trast to what is commonly associated with personal computing, 
the interface rather than the processing power is the critical element. As a personal aid, the communci- 
ation with the owner is as critical as the electrical communications. Further, the technical means are 
available to vastly improve the interface past the simple goal of "user friendliness". This will be illustrated 
by several examples. Each is an attempt to explore an application from the point of view that the introduction 
of computing to it can result both in the standard improvements associated with "electronification" 
and more fundamental change: an electronic map can display cartography on television screens, but it 
can as well serve as an experimental fami- liarization tool. More important, how- ever, the examples 
are indications of a more general system approach to the merger of computing with traditional communica- 
tions media. They generate directive guidelines for interface and interactive systems design that can, 
with proper test, be more generally useful. Illustrations will include the following: --The Movie Manual: 
A personalized electronic book. Optical storage contains the data and images that describe mainte- nance 
procedures presented as dynamic "pages" that are generated on-the-fly as needed, and are explorable as 
part of the reading process. --The Electronic Newspaper: An analysis interface to rapidly changing data. 
The paper analogy is a newsmaga- zine, personally published, and read asso- ciatively, in combination 
with a growing library from past issues. --Scripting by enactment: Full body involvement in the direction 
of instruc- tions for graphical actors. Dan Sandin, University of Illinois What Digital Software/Hardware 
Designers Can Learn From Analog Sound and Video Instrument Designs There are two common paradigms used 
in co.mputer graphic systems: writing programs in a general purpose computer language and menu-driven 
paint programs. Traditions in analog music and video synthesizer design use different paradigms from 
analog computation and musical instruments. Videotapes of analog audio and video syn- thesizers and 
the Graphic Symbiosis System will be shown to demonstrate these control structures. In addition, these 
paradigms will be applied to a new digital image processing system which is presently under develop- 
 ment. 313
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801163</article_id>
		<sort_key>315</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>38</seq_no>
		<title><![CDATA[An inexpensive scheme for calibration of a colour monitor in terms of CIE standard coordinates]]></title>
		<page_from>315</page_from>
		<page_to>321</page_to>
		<doi_number>10.1145/800059.801163</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801163</url>
		<abstract>
			<par><![CDATA[<p>The Commission Internationale d'Eclairage system of colorimetry is a method of measuring colours that has been standardized, and is widely used by industries involved with colour. Knowing the CIE coordinates of a colour allows it to be reproduced easily and exactly in many different media. For this reason graphics installations which utilize colour extensively ought to have the capability of knowing the CIE coordinates of displayed colours, and of displaying colours of given CIE coordinates. Such a capability requires a function which transforms video monitor gun voltages (RGB colour space) into CIE coordinates (XYZ colour space), and vice versa. The function incorporates certain monitor parameters. The purpose of this paper is to demonstrate the form that such a function takes, and to show how the necessary monitor parameters can be measured using little more than a simple light meter. Because space is limited, and because each user is likely to implement the calibration differently, few technical details are given, but principles and methods are discussed in sufficient depth to allow the full use of the system. In addition, several visual checks which can be used for quick verification of the integrity of the calibration are described.</p> <p>The paper begins with an overview of the CIE system of colorimetry. It continues with a general discussion of transformations from RGB colour space to XYZ colour space, after which a detailed step-by-step procedure for monitor calibration is presented.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[colorimetry]]></kw>
			<kw><![CDATA[colour calibration]]></kw>
			<kw><![CDATA[colour science]]></kw>
			<kw><![CDATA[computer graphics]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Standards</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011017</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Domain specific languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010919.10010177</concept_id>
				<concept_desc>CCS->Computing methodologies->Distributed computing methodologies->Distributed programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Standardization</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P298825</person_id>
				<author_profile_id><![CDATA[81410592379]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[B.]]></middle_name>
				<last_name><![CDATA[Cowan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Optics Section, Division of Physics, National Research Council of Canada, Ottawa, Ontario, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Anstis, S. and Cavanaugh, P., "An Apparent Motion Technique for Judging Equiluminance of Colours in a Television Display", Proceedings of Colour Vision, an International Conference, Cambridge, 1982 (in press).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bartleson, C.J., "Changes in Color Appearance with Variations in Chromatic Adaptation", Color Research and Application, 4 119-138, 1979; "Predicting Corresponding Colors with Changes in Adaptation", Color Research and Application, 4 143-155, 1979.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807417</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Catmull, E., "A Tutorial on Compensation Tables", Computer Graphics, 13 1-7, 1979.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Cowan, W.B., "CIE Calibration of Video Monitors", National Research Council of Canada Technical Report, 1983.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807502</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Meyers, G.W. and Greenberg, D.P., "Perceptual Color Spaces for Computer Graphics", Computer Graphics, 14 254-261, 1980.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807361</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Smith, A.R., "Color Gamut Transform Pairs", Computer Graphics, 12 12-19, 1978.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Tansley, B.R. and Boynton, R.M., "A Line, not a Space, Represents Visual Distinctness of Borders formed by Different Colors", Science, 191 954-957, 1976.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Wyszecki, G. and Stiles, W.S., "Color Science", second edition, New York: Wiley, 1982.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 AN INEXPENSIVE SCHEME FOR CALIBRATION OF A COLOUR MONITOR IN TERMS OF CIE STANDARD COORDINATES William 
B. Cowan Optics Section Division of Physics National Research Council of Canada Ottawa, Ontario, Canada. 
 ABSTRACT The Commission Internationale d'Eclairage system of colorimetry is a method of measuring colours 
that has been standardized, and is widely used by industries involved with colour. Knowing the CIE coordinates 
of a colour allows it to be reproduced easily and exactly in many dif- ferent media. For this reason 
graphics installations which utilize colour extensively ought to have the capability of knowing the CIE 
coordinates of displayed colours, and of displaying colours of given CIE coordinates. Such a capa-bility 
requires a function which transforms video monitor gun voltages (RGB colour space) into CIE coordinates 
(XYZ colour space), and vice versa. The function incor-porates certain monitor parameters. The purpose 
of this paper is to demonstrate the form that such a function takes, and to show how the necessary monitor 
parameters can be measured using little more than a simple light meter. Because space is limited, and 
because each user is likely to implement the calibration differently, few techni- cal details are given, 
but principles and methods are dis-cussed in sufficient depth to allow the full use of the sys- tem. 
In addition, several visual checks which can be used for quick verification of the integrity of the calibration 
are described. The paper begins with an overview of the CIE system of colorimetry. It continues with 
a general discussion of transformations from RGB colour space to XYZ colour space, after which a detailed 
step-by-step procedure for monitor calibration is presented. KEYWORDS: computer graphics, colour science, 
colorimetry, colour calibration. I. Introduction. Colour graphics applications generally use raster CRT 
technology for colour generation. Voltages, which are applied to each of the three guns of a colour video 
moni- tor, produce distinct levels of excitation in the three phos- phors of the monitor, and the consequent 
light, when it reaches the eye, produces a sensation of colour. The vol- tages are controlled by digital 
to analogue converters whose inputs come from application software. Users of the software generally select 
colours by trial and error. That is, a peripheral device alters the inputs to the D/A and the user alters 
the colour (using a set of valuators or a pick) while observing the result on the monitor. When he sees 
the colour he wants he accepts it, and uses it. Now, sup-pose the same colour is to be retrieved on another 
occa-sion. 1. The user may wish to produce exactly the same colour on the same monitor at a later time. 
 2. He may wish to produce the same colour on another monitor, or an another graphics system. 3. He 
may wish to produce the same colour in a dif-  ferent medium, e.g. paint, dye, ink. Each of these proposals 
raises a different set of problems. 1. It is easy to reproduce the voltages that were applied to the 
monitor when the colour was initially generated. But, is the colour the same? This amounts to asking 
how stable the monitor is, and there is no immediate set of units for measuring the stability, and no 
obvious way to tell how much instability is objectionable. 2. It is easy to reproduce the voltages, 
and apply them to the second monitor, but not very likely that the colour is the same. How should the 
voltages be changed to produce the same colour? 3. For colours produced in different media there is 
no  reference to voltages at all. What is wanted in each case is a visual match. In other words, it 
should be the case that a "normal" observer, looking at the two colours under appropriate viewing con- 
ditions, should see them to be the same. This does not necessarily mean that two sources of photons should 
have the same spectral distributions; that objective is impossible for most cross monitor and cross medium 
transfers. Rather, it means that two impressions of colour must be made the same, which involves quantifying 
the human response to light. Problems similar to the ones mentioned above have been addressed for many 
years in industries that utilize colour. They have been solved by devising a method for deciding when 
two coloured samples will be seen to be the same. This method, which has been standardized by the Commission 
Internationale d'Eclairage (CIE), is an inter-face for users of colour. Thus, each particular industry 
need know only how to measure the CIE coordinates of colours it produces, and how to generate colours 
of given CIE coordinates. These abilities are used on a day-to-day basis for the standardization, reproduction, 
and transfer of colours. Clearly, a CIE interface for video display technol- ogy solves the three problems 
mentioned above, and a great many more like them. It also provides a link to colour research and to the 
many industries that use CIE coordinates. The purpose of this paper is to address these issues by describing 
the principles whereby a video monitor may be calibrated in terms of CIE coordinates, so that the transfer 
of colours among monitors and other media may be performed. 2. The CIE System The basic CIE system has 
been set up to allow a determination of whether or not two colour samples will be seen to be the same 
colour. Extensions to it address peri-pheral problems, e.g. colour differences, brightness of lights 
differing in hue, etc, which are not of concern to the subject of this paper. It is based on two concepts: 
metamerism and additive colour mixture. Metamerism expresses the empirical fact that lights of different 
spectral composition can be seen to be the same colour. For example, white can be produced by light which 
has roughly equal numbers of photons at each wavelength, s~ch as sunlight, or by a mixture of rela- tively 
narrow-band red green and blue components, such as occurs in a colour monitor. Lights of differing spectral 
composition that are the same colour are said to be metamerism. We can say that the set of all possible 
spec- tral compositions is divided by colour matching into sub- sets such that all members of any subset 
are metameric to all other members. Then, the problem of transferring colour across systems amounts to 
finding on the target system colours that are metameric to those of the source system. Thus, we cannot 
reproduce the spectral composi- tion of sunlight on a colour monitor, but we can produce its colour by 
appropriate values of the red green and blue components. Metamerism is closeiy linked to additive colour 
mix- ture. Additive mixture occurs when two lights coincide spatially and temporally so as to be seen 
as a single colour. Specifically, the photons from each source add together in stimulating a single area 
of the visual field. Such mixture occurs in theatre lighting and offset printing. Yellow on a colour 
video monitor is an example of additive mixture, since red and greerl phosphors are active in such close 
proximity that they are mixed additively into a yellow colour. Metamerism is preserved under additive 
mixture, which means that when additive mixture is performed the resulting colour is independent of whichever 
of the metam- eric possibilities are used for the component colours. Taking metamerism and additive mixture 
together it is clear that there is a minimal set of primary colours from which any colour can be produced 
by additive mixture. Furthermore, the method of production, for a given minimal set, is unique. Trichromacy, 
which is almost cer-tainly related to the existence of three types of photorecep- tor, is the statement 
that the minimal set has exactly three members for observers with normal colour vision. Thus, for a given 
set of three primaries there is a unique way to specify each colour, which is the same for all spectral 
dis- tributions that match the colour, and is different for all that do not. The CIE is responsible for 
specifying a stan-dard set of primaries, which allows common use of the sys- tem. The ones in most common 
use are the (XYZ) set of primaries, standardized in 1931. Using them, a colour is specified by a triplet 
of positive numbers, known as tris-timulus values, which are defined in terms of the spectral power, 
e(~,). x = fe(~J~xdZ Y fPO.)f#~. = z = fPO.)ixd)~ The functions xx, y~, and z~ are the colour maLching 
func- tions. They are defined in terms of empirical data, and tabulated, among other places, in [8], 
where a much fuller account of these matters is presented. For our present pur- poses, however, all we 
must know is how to determine x, r, and z for colours on a colour video monitor, and how, given x, Y, 
and z, to display the colour they signify. The colour systems in common use in colour graphics are easily 
related to xYZ space. All utilize the same metamerism relations as XYZ. Some are linear, as is the xYz 
space. (Linearity means that when two lights are mixed additively their sum is at the colour space point 
which is the vector sum of points corresponding to the two component lights.) Three examples are RGB, 
Y1Q, and CIELuv. Others are non-linear, such as Munsell, HSV, and CIELab. [6] shows how to convert HSV 
and YIQ to RGB; [8] how to convert CIELuv, Munsell, CIELab, and many other systems to xYz; sections 4 
to 9, below, will show how to interconvert RGB and XYZ.  3. Calibration Generalities A video monitor 
is calibrated if we know the functions X = fx(VR,V a,vB) Y = fr(vR,va,vB) Z = fz(VR,Va,VB) which give 
the tristimulus values x, Y, and z of the colour produced in terms of the voltages vR, vo, and vB applied 
to the three guns of the monitor. (We depart somewhat from common usage, using vR, vc, va for the gun 
voltages; ER, Ea, EB (below) for the excitation of the phosphors. R, G, B are sometimes used to represent 
the voltages; more often they are the excitations. The latter sense is used in defin- ing the RUB colour 
system, as in [6] or [5]. The present notation is used to remove any ambiguity.) These func-tions, and 
their inverses, can be implemented in software to transform voltages to tristimulus values, and vice 
versa. Such an implementation calibrates the monitor. There is a brute force solution to this problem, 
using a spectrophotometer to measure the spectral power of the emitted light. It is straight-forward 
to sample exhaustively the voltage space, calculating x, Y, and z for each sample. The results can be 
used as a calibration, either in parametrized functional form, or as an interpolated table. This approach 
is unsatisfactory for several reasons: 1. Spectrophotometers adequate for this task are quite expensive. 
 2. Because many phosphors have narrow spectral lines, P(~) must be measured at 1-2 nanometer intervals, 
so that the spectral scan is very time-consuming. 3. The main reason for using this method would be 
its potential accuracy (better than 1%), but our experi-ence suggests that colour monitors are insufficiently 
stable to warrant this precision, especially when the time needed to make a complete calibration (many 
hours) is considered. It is not likely, on the basis of monitors we have examined, that the monitor has 
the same calibration function (to the degree of precision of which spectrophotometry is capable) at the 
end of the calibration as it did while the calibration was in progress.  An approach which is probably 
better than the brute force method for most cases breaks determination of the calibration functions into 
several stages. Most of the stages can be done without needing spectrophotometry (i.e. inexpensively), 
and those that do need it often have the necessary spectrophotometric data available from the mon-itor 
manufacturer or some other source. These features (phosphor chromaticities) are among the most stable 
of monitor properties, so that there is likely to be no need for spectrophotometry. Furthermore, the 
calibration stages admit of quick visual checks, which can be programmed into the system that drives 
the monitor. Through them the calibration can be checked each time the monitor is used, giving confidence 
that the calibration is accurate, and minimizing the need for recalibration. 4. Preliminaries Before 
a monitor is calibrated it should be set up exactly as it will be when in use. This includes: 1. Its 
position in the laboratory, since moving or rotating a monitor can change its calibration. 2. Purity 
and convergence adjustment. 3. Adjustment of picture size, brightness, contrast, rela-tive gains of 
the three guns, etc. These quantities can be adjusted in a way that minimizes the measurement needed 
for calibration. It seems preferable, however, to adjust them so that the picture is best for the appli- 
cation, then to calibrate that setting of the monitor.  This paper assumes that the monitor is set up 
to suit the application and then calibrated, since the measurements needed for the general case are only 
slightly more compli-cated than for any particular case. 5. Phosphor Chromaticities When phosphor a 
(a = R, G, or B) of a colour monitor is excited it emits light of relative spectral power p~()~), which 
is independent of the level of excitation. Thus, if Ea(VR,V~,vB) is a variable representing the degree 
of excita- tion of phosphor a, which can, in general, depend on all three voltages, the light output 
from the monitor is P(~,) = ~_~Ea(VR,VG,VB)pa@). a To find out the colour of this light calculate its 
tristimulus values x = Z, eo(vR,vc,vB)f~Wo(~.)dX a = ~.aXaEa(VR,VG,VB) a Y= ~yaE~(vR,v~,vs) a Z: ~_.~ZaEa(VR,VG,VB) 
a  The quantities Xa = f ~zpaOOd)~ y,, = ffizp,~()~)d~. zo = f zzpoOOd~ are known as the chromaticity 
coordinates of phosphor a. They are normalized so that xa + ya + Za = 1 with any normalizing factor 
absorbed into Eo(VR,VO,v.). (The above equations are a generalization of equation (3) of [5].) Thus, 
it is necessary to find the chromaticities of the three phosphors. There are several methods, listed 
here from worst to best. 1. Do not use the standard P22 chromaticities, since many monitors have phosphors 
that do not match the P22 chromaticities. 2. Measure the phosphors with a tristimulus colorimeter. Such 
an instrument uses a filter/photodector combina- tion that mimics the spectral response of the colour 
matching functions: xx, yx, and z~. Although the meter does not precisely reproduce the desired functions, 
the error is not serious for measuring broad band spectral distributions, such as occur in natural objects, 
pig-ments, etc. It is more of a problem when measuring samples with narrow spectral bands, such as monitor 
phosphors. If this method is used, it should agree with some other one, probably 3. 3. Use chromaticities 
supplied by the manufacturer. These are presumably the result of a spectropho-  tometric measurement, 
though not of the exact tube to be calibrated. There is not much variation from tube to tube, but one 
must be confident that conver-gence and purity are good to use this method. Be sure that the manufacturer 
is not specifying "nominal" chromaticities. 4. Have a friend who has done method 5. on a monitor with 
the same tube. 5. Make a spectrophotometric measurement of the phos- phors. This method is best, catching 
narrow spectral lines, misconvergence, and lack of purity. It should be done at 1-2 nanometer intervals, 
and checked against  3. or 4. for best results. As long as the purity and convergence remain well adjusted, 
it is not necessary to redetermine phosphor chromaticities when the preliminary adjustments of section 
4 are redone. 6. Phosphor Excitations In general, the excitation of phosphor a depends on the voltages 
of all three guns. Thus, we have written E,,(vR,va,vB). It is possible, but unduly complicated, to cali- 
brate such a monitor using only a simple light meter. (The details may be found in [4].) The purpose, 
however, of con- vergence and purity adjustments is to make phosphor a excited only by gun a, and we 
can assume that this condi- tion, which is called gun-independence, is fulfilled for all well-adjusted 
monitors. When gun- independence, which is expressed as Ea(VR,VG,VB) =-Ea(va) is true, the relationship 
of phosphor excitation to gun vol- tage is known as gamma correction. We must remember, however, that 
it holds only for a range of gun voltages, albeit a wide one. The range varies from monitor to moni- 
tor, and must be determined empirically. Outside it the calibration is not valid. 7. Gamma Correction 
Gamma correction can be done using any light meter whose output is linear with the number of input photons. 
The meter's spectral sensitivity need not be known. A pho- todiode, or a hand-held luminance meter is 
ideal for this measurement. To make it, split the phosphor excitation function into two factors Ea(va) 
~ Nae.(Va) where e,~(v,), the relative excitation function, gives the vol- tage dependence, and No, the 
phosphor-gun normalizing factor, is determined by how the three guns have been bal- anced. Normalize 
e,(v,,) so that ea(Vamax ) = 1 with v .... at the top of the gun independent range. Now, when gun a only 
is turned on to voltage v~ the meter reads R(va)'= Naea(Va) f F(~,)pa(~.)d~, where F(~,) is its unknown 
spectral sensitivity. From the readings we get the three excitation functions R(va)ea(vo) R (v .... ) 
These functions can be stored for table look-up [3], or maintained in parametrized form. Gun-independence 
should be confirmed by measuring these functions with the other guns having various values within the 
gun-independent range. For such measurements R(v~) = N~ea(v=) f F(X)p~Q,)dX + ]~ Nbeb(vb)fF(X)pb(X)dX 
a~:b and R(va) --R(O) ea(Va) R(v .... ) --e(o) If gun-independence holds the same function will appear 
in each case. These functions should be checked fairly fre-quently. ~ I 10% "  . "? . o 4  .  
J I I 5.7 6.0 6.5 6.7 In(voltage) Figure 1. Natural logarithm of meter reading, plotted as a function 
of the natural logarithm of driving voltage for the red gun. The main curve () has various field rates, 
various blue and green gun voltages, and was measured on various days. The other curves () were produced 
at two settings of the contrast, with brightness set to make the curve a straight line.  Computer Graphics 
Volume 17, Number 3 July 1983 Regarding parametrized forms, let us examine some measurements performed 
on one of our video monitors, a 13", high-resolution (512 x 720), in-line gun unit. Figure 1 shows the 
light-meter output as the voltage delivered to the red gun is varied. In figure 1, the main curve represents 
the results of many runs, made on several days. Brightness and contrast settings of the monitor were 
held constant at randomly chosen values for all these runs. Observe the following: 1. The monitor has 
day-to-day drift the order of a few percent. To that accuracy a calibration can be made and used for 
several days (until some visual test, such as those described below, shows it to be varying). It is hopeless 
to try to calibrate it to better accuracy. 2. The field rate and the green and blue voltages varied 
from run to run, but the curve didn't shift. The guns are indeed independent, and, within the range tested, 
the calibration did not depend on field rate. 3. The points do not lie on a straight line, so that the 
frequently-quoted parametric expression  ea(va) = (va/v .... )'to (see, for example, [5]) does not fit 
them well. In fact, such an expression, depending on the part of the curve from which ~,a is derived, 
makes errors up to 100%, which can, for example, change unsaturated red to unsaturated blue. 4. e,,(va) 
is well fit by the two parameter expression ln(ea(Va)) = Aoln(va/V .... ) + Ba(ln(v,Jv .... ))2 5. The 
slope (~,) of the fit ranges between 2.75 at the highest values of gun voltage and 4.0 at the lowest 
values. This range is well above the often-quoted value of 2.5. The two other sets of points are produced 
by altering the brightness control to a point where the In-In plot is a straight line and taking two 
different settings for contrast. At this setting gamma correction is appropriate, with ~, at 2.33. However, 
when we examine all three guns at this brightness setting (figure 2) we see that the blue gun requires 
a two parameter fit, and the green gun's ~ ranges between 2.33 and 2.44. This suggests that using the 
simple exponent law is not reasonable, even in this case. We think that the best strategy is to find 
brightness and contrast set- tings which suit the application, then to use a light meter to find an appropriate 
parametrized+form and to set the parameters. In most cases the two parameter form (above) will work adequately. 
It is important to check periodically whether the calibration parameters continue to be correct. This 
can be done by eye, without needing a meter. Display two pat- terns in such a way that they are additively 
mixed, either by temporal succession (e.g. the two patterns alternate so fast that they are fused into 
one) or by spatial contiguity (e.g. the two patterns are mixed together spatially -dith-ered -so that 
any discriminable visual area contains ele-ments of both patterns). Then the meter reading at any point 
will be R(Vla,V2a ) = Na[ea(Vla ) + ea(V2a)] X f F(~)pa(X)d~. where v~a (v2a) is the voltage applied 
at that point in pat- tern 1 (2). Different areas of the patterns will sum tQ the same reading only if 
those areas have the same sums ea(vta) + ea(v2,). Thus, we can set up two patterns, each comprising many 
areas, such that the sum of the two pat- terns is the same in every area if the relative excitation function 
is right. Such a pattern is, of course, visually homogeneous, so that by displaying it on the monitor 
we have a quick visual check that this part of the calibration is correct. Only when inhomogeneities 
can be seen need a meter be used to recalculate the parameters. Such a visual check,which requires only 
a minute or two, should be per- formed frequently for each gun, and with more than one gun turned on 
to ensure gun independence. Visual parameter setting can also be done, using pat- terns like those described 
above, and adjusting parameters until the disply is homogeneous. It is not as precise, how-ever, as using 
a meter, and it is very important to use a meter at least once to to establish the right parametric form 
for a particular monitor. I- I 10% k 4 z + 4 4 4 4 4 4 4 I- I i I i 5.0 5.5 6.0 6.5 In(voltage) Figure 
2. Natural logarithm of meter reading, plotted as a function of the natural logarithm of driving voltage 
for all three guns. Brightness is set to make the red gun curve (~) a straight line, but green (.L) and 
blue (-I) gun curves are not. 8. Gun Normalization To complete the calibration we must find the phosphor-gun 
normalizing factors. If a colour sample with known tristimulus values is available, this process is straightforward. 
Adjust the gun voltages until the screen matches the sample. Then X = ~Naxaea(va) a y = ~l"Vayaea(va) 
a Z = ]~Nazaea(Va) a and the three normalizing factors can be found directly. This might be done using 
a standard TV colour compara- tor. Alternatively, if a spectrophotometer is available, measure the tristimulus 
values corresponding to a set of gun voltages and solve the above equations. It is essential to match 
or measure several different values to test the consistency of the calibration. The above procedure, 
if it can be performed, com-pletes the calibration; the remainder of this section is not necessary. Otherwise, 
separate the phosphor-gun normaliz-ing factors into relative normalizing factors, no, and a sin-gle overall 
normalizing factor, N. That is N a = Nn a and arbitrarily set ]~na = 1. Now, if we have a colour sam- 
ple of known chromaticity, (x,y), we can adjust the gun voltages until the screen matches it. Then naXaea(Va) 
a x ~_~naea(Va) a ~.nayaea(va) a Y ~_,naea(Va) a from which the values of n a can be calculated. Alternatively, 
we can use a technique whereby two different colours are made equal on a single tristimulus value. Suppose 
the value to be. Y. Adjust the red gun vol- tage, vR, in one area and the green gun voltage, v~, in another 
until the two areas have equal Y values. For these voltages nRyReR(VR) = nayaec,(v~) which determines 
the ratio nR/na. Repeat this process using the blue gun and the ensuing ratios, together with the condition 
~_.,na = 1, give the relative normalizing fac- tors. There are visual techniques for making two colours 
equal on the Y tristimulus value. One is heterochromatic flicker [8]. If two colours are alternated temporally 
at about 20 Hz (25 msec of one followed by 25 msec of the other) there is a visual impression of flicker. 
As the overall level of one is varied there is a point at which the impres- sion of flicker is a minimum. 
There the two colours have the same Y value. A similar technique is called minimally distinct border 
[7]. When two colours adjoin one another there is a border (edge) produced by the difference between 
them. If the level of one is adjusted until the border is minimally distinct, the two colours have the 
same Y value. Other techniques (e.g. [1]) exist as well. Non-visual techniques also exist. Spectrophotometry 
is a possibility, or a luminance meter may be used. Unfor-tunately, the former is too expensive, and 
the latter too imprecise. They can be combined in a useful way by using the spectrophometer to calibrate 
the luminance meter when spectrophotometry is being done. Remember that any calibration is specific to 
a particular set of phosphors. To sum up, the non-visual techniques for gun normali- zation, matching 
or using a meter, are best. Visual tech-niques are less accurate. However, because they are easy to set 
up, and quick to do, they are ideal as a check to ensure that a monitor has not drifted seriously out 
of cali- bration. 9. Overall Normalization If a chromaticity match or a visual match is used for the 
phosphor-gun normalizing factor, there is an overall normalizing factor, N, which remains undetermined. 
Accu-rate determination of this factor is, in fact, not too impor- tant, since error in its determination 
changes only the overall luminance of all colours in the scene. The eye is not very sensitive to such 
changes, so that errors of 50% or so are quite tolerable. By contrast, errors in the determination of 
relative normalization factors are quite noticeable, since they alter hue and saturation. Because high 
precision is not important here, it is quite adequate to set up a white colour and measure it with an 
inexpensive luminance meter, or to match a not very well known white luminance by varying voltages to 
the screen. Either method determines N to a precision adequate for practical purposes. 10. Discussion 
Sections 4 to 9 suffice to completely calibrate a moni- tor in terms of CIE coordinates. Thus, if the 
voltages pro- ducing a colour are known, its CIE tristimulus values are X = N~_.naXaea(Va) a Y = N~.~nayaea(Va) 
a z = N~nazoeo(vo) a with ln(e,,(v,,)) = Aaln(va/v .... ) + Ba(ln(va/v .... ))2 The inverse transformation, 
which determines the voltages needed to produce given tristimulus values, requires the inverse of matrix 
M. XR XG M = YR YG YB ZR ZG ZB Using the inverse of M, noec = I(M-I) 1 nBeB and the voltages are given 
by va = v~maxexp[ln(ea)/(A, + Baln(e~)/ (A, + B, ln(ea)/(Aa + ...)))] The continued fraction form for 
the solution of a quadratic equation is most convenient computationally. In the preceding we have not 
discussed the conse-quences of calibration error. Clearly, some types of mis- calibration are more objectionable 
than others, but the subject of how observers respond to overall distortions of colour space is very 
complicated. Although a large litera- ture exists (see [2] for references), little is understood. One 
rule of thumb is probably worth remembering. Miscalcu- lating the gamma correction function on a single 
gun means that, although chromaticities are true at the gun voltage where the gun balance is done, they 
vary, with respect to the true chromaticity, as the gun voltage varieb. Such discrepancies are, in most 
cases, more objectionable than an error of similar size in the gun balance, for such an error produces 
a shift of chromaticities which doesn't vary with level. Least objectionable of all, as mentioned above, 
is an error which gets chromaticities right but shifts the overall level. In this connection, let us 
consider the usual method of balancing a broadcast monitor. Briefly, the electronics of the monitor are 
tuned so that white is produced whenever all three guns are driven equally. This procedure produces the 
same gamma correction for each gun, and then uses adjustment by eye to generate a natural range of colours 
around white. |t works well for realistic scenes, which have a wide and predictable range of chromaticities. 
In these cases the viewer's colour intuition works well, and there is an adequate variety of colour for 
his eye to sample. Simi- lar methods, used to balance the colour in printing or pho- tography, produce 
reproductions which, although they may not be close colour matches to the original monitor colours, seem 
to catch the important colour relations. Unfor-tunately, the conditions that make these intuitive pro-cedures 
possible are often not met in computer-generated images. Thus, it is necessary to go through a calibration 
procedure like the one described in this paper. Length constraints have prohibited much discussion of 
the practical details of calibration: actual instrumentation, exact procedures for visual tests, actual 
monitor properties, etc. These matters are discussed in a National Research Council of Canada technical 
report [4] which will be avail- able from the author. 11. Acknowledgements The author wishes to thank 
Peter Tanner for his encouragement, and for helping to find references, G. Wyszecki for general advice 
on calibration, and the anonymous reviewers, whose comments materially improved the paper. He also wishes 
to thank the Computer Graphics Lab at the University of Waterloo, which made production facilities available 
when the NRC secretarial staff was unable to produce this paper. 12. References 1. Anstis, S. and Cavanaugh, 
P., "An Apparent Motion Technique for Judging Equiluminance of Colours in a Television Display", Proceedings 
of Colour Vision, an International Conference, Cambridge, 1982 (in press). 2. Bartleson, C.J., "Changes 
in Color Appearance with Variations in Chromatic Adaptation", Color Research and Application, 4 119-138, 
1979; "Predicting Corresponding Colors with Changes in Adaptation", Color Research and Application, 4 
143-155, 1979. 3. Catmull, E., "A Tutorial on Compensation Tables", Computer Graphics, 13 1-7, 1979. 
 4. Cowan, W.B., "CIE Calibration of Video Monitors", National Research Council of Canada Technical Report, 
1983. 5. Meyers, G.W. and Greenberg, D.P., "Perceptual Color Spaces for Computer Graphics", Computer 
Graphics, 14 254-261, 1980. 6. Smith,, A.R., "Color Gamut Transform Pairs", Com- puter Graphics, 12 
12-19, 1978. 7. Tansley, B.R. and Boynton, R.M., "A Line, not a Space, Represents Visual Distinctness 
of Borders formed by Different Colors", Science, 191 954-957, 1976. 8. Wyszecki, G. and Stiles, W.S., 
"Color Science", second edition, New York: Wiley, 1982.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801164</article_id>
		<sort_key>323</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>39</seq_no>
		<title><![CDATA[Interactive image query system using progressive transmission]]></title>
		<page_from>323</page_from>
		<page_to>330</page_to>
		<doi_number>10.1145/800059.801164</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801164</url>
		<abstract>
			<par><![CDATA[<p>There is a growing need for people to browse through files of images such as satellite or medical photos, to determine which ones warrant further examination. Users located at some distance from the image archive often must use slow transmission links such as telephone lines. If an image is scanned out line-by-line, top-to-bottom, the user must often wait too long to determine whether the image is of any use. Using the Progressive Transmission Method, however, images are encoded so that during transmission the entire display shows a rough version of the image in 'fat pixels'. If the user wishes to see more detail, additional data is sent and used to refine these pixels, until the exact original image is seen. We report here on extensions of this method to the rich color imagery found in remote-sensing applications. A significant advance is the elimination of unworkably large look-up tables by using a single simple algorithm for performing the required encoding and decoding operations. The method is conceptualized in a 'transmission cone' context which assists a user in interacting with the system. The user can roam over large images, zoom to various levels of resolution, and cause specified subregions of interest in the image to fill in to full resolution, at a tremendous saving in time.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.4.3</cat_node>
				<descriptor>Information browsers</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.6</cat_node>
				<descriptor>Pixel classification</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010246</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Interest point and salient region detections</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003260.10003300.10003302</concept_id>
				<concept_desc>CCS->Information systems->World Wide Web->Web interfaces->Browsers</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14159664</person_id>
				<author_profile_id><![CDATA[81100454413]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[F.]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Hill]]></last_name>
				<suffix><![CDATA[Jr.]]></suffix>
				<affiliation><![CDATA[University of Massachusetts, Amherst, Mass.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P333754</person_id>
				<author_profile_id><![CDATA[81100423555]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sheldon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Walker]]></last_name>
				<suffix><![CDATA[Jr.]]></suffix>
				<affiliation><![CDATA[University of Maine, Orono, Maine]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P330315</person_id>
				<author_profile_id><![CDATA[81100132196]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Fuwen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Beijing Normal University, Beijing, China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[K. R. Sloan &amp; S. L. Tanimoto, "Progressive Refinement of Raster Images", IEEE Trans. on Computers, C-28, No. 11, Nov. 1979, pp. 871-874.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ken Knowlton,"Progressive Transmission of Grey-Scale and Binary Pictures by Simple, Efficient, and Loss-less Encoding Schemes" Proc. of IEEE, Vol. 68, No. 7, 1980, pp. 885-896.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Theo Pavlidis, GRAPHICS AND IMAGE PROCESSING, Computer Science Press, Rockville, MD 1982.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[A. Hanson &amp; E. Riseman, "Segmentation of Natural Scenes" in COMPUTER VISION SYSTEMS (A. Hanson &amp; E. Riseman, eds.) Academic Press, New York, 1978.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[S. E. Walker, Jr &amp; F. S. Hill, Jr., "Progressive Transmission of Remotely Sensed Images" 4-th Symposium on Automation in Engineering Data Handling, Naval Postgraduate School, Monterrey, CA, Nov. 3, 1982.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 INTERACTIVE IMAGE QUERY SYSTEM USING PROGRESSIVE TRANSMISSION by F.S. HilI,Jr University of Massachusetts 
Amherst, Mass. Sheldon Walker,Jr. University of Maine Orono, Maine Fuwen Gao Beijing Normal University 
Beijing,China ABSTRACT There is a growing need for people to browse through files of images such as 
satellite or medical photos, to determine which ones warrant further examination. Users located at some 
distance from the image archive often must use slow transmission links such as telephone lines. If an 
image is scanned out line-by-line, top-to- bottom, the user must often wait too long to determine whether 
the image is of any use. Using the Progressive Transmission Method, however, images are encoded so that 
during transmission the entire display shows a rough version of the image in 'fat pixels'. If the user 
wishes to see more detail, additional data is sent and used to refine these pixels, until the exact original 
image is seen. We report here on extensions of this method to the rich color imagery found in remote-sensing 
applications. A significant advance is the elimination of unworkably large look-up tables by using a 
single simple algorithm for performing the required encoding and decoding operations. The method is conceptualized 
in a 'transmission cone' context which assists a user in interacting with the system. The user can roam 
over large images, zoom to various levels of resolution, and cause specified subregions of interest in 
the image to fill in to full resolution, at a tremendous saving in time. I). INTRODUCTION: Our goal 
is to provide a system of users with rapid and flexible access to images. As shown in Fig.l, we envision 
a central 'image repository' consisting of a host computer and a large archive of many images, along 
with transmission links to a network of interactive remotely situated users. The users can browse through 
a file of images, obtain copies of their favorites at various levels Permission to copy without fee 
all or part of this material is granted provided that the copies are not made or distributed for direct 
commercial advantage, the ACM copyright notice and the title of the publication and its date appear, 
and notice is given that copying is by permission of the Association for Computing Machinery. To copy 
otherwise, or to republish, requires a fee and/or specific permission. &#38;#169; ACM 0-89791-109-1/83/007/0323 
$00.75 of resolution, and possibly collaborate with other users in processing and analyzing them. The 
ability to browse is particularly important when using satellite images, since one wants to look over 
the most recently acquired file, yet some images may suffer from excessive cloud cover. A user wants 
to detect this situation quickly and abort further transmission of faulty or uninteresting images, in 
order to browse through others. The remote stations must be reasonably priced and user-friendly, and 
the browse capability provided must be responsive enough so that user's attention does not waiver excessively. 
 Central to the system envisioned is the need for transmitting images over possibly slow communication 
links, yet with time delays which are acceptable to interactive users. ~ USER ~! We focus on remote-sensing 
images here, but one can foresee a much broader set of potential users, in such fields as medicine, law, 
intelligence, and the graphic arts. We emphasize images such as LANDSAT and GOES photos here, anticipating 
a potentially large base of image users in the Northeastern United States, for such uses as weather, 
intelligence, land use planning, and agriculture, who may wish to link into the proposed satellite tracking 
ground-station to be located at the University of Massachusetts. With standard transmission techniques, 
an image is sent to a terminal so that it is scanned out as in a tv raster: line-by-line from top-to- 
bottom. Normally at least half of the image must be received before the user can make a judgement as 
to its use-fulness. If standard telephone lines are used, it can take a pro-hibitively long time to send 
a small fraction of an image in this manner. Computer Graphics Volume 17, Number 3 July 1983 We envision 
images with r rows, c columns, and b bits/pixel, hence r*c*b bits in all. Usually c and r will be the 
same power of 2. For Concreteness we hereafter refer to the 'typical image' as having 512 rows, 512 
columns, and 24 bits/pixel, giving r.c.b = 6.3 megabits. We also take as 'typical' a transmission rate 
of 4800 bits/second, so that the 'typical image' would require 21.8 minutes to be received. The transmission 
time is the major bottleneck here, far outweighing any local processing time we envision. IA). SCENARIO: 
 Our approach to dealing effectively with long image transmission times is illustrated in the following 
user interaction sequence. For each image requested, data is sent so that the entire image emerges progressively, 
from low to higher resolution, until the user can decide whether the image is potentially of interest. 
(Examples below indicate that this point may occur after only 2% of the image data has been receivedl) 
If it is of no further interest, the user signals that a new image is to be sent, and no precious time 
is wasted in sending useless data. If it appears useful, on the other hand, the user has several options. 
Frequently he wishes to closely examine only a portion of the image, and so he specifies a window into 
that portion of the low-resolution version, whereupon it fills into full resolution. This 'selective 
fill-in' occurs rapidly since only the data relevant to pixels in the windowed area need be transmitted. 
In this fashion the user can 'roam' over the image, obtaining full resolution versions of all interesting 
parts. Local processing of course allows any portion to be enlarged to fill the display screen. IB). 
PREVIOUS APPROACHES AND LIMITATIONS. The problem of excessive transmission time has been addressed 
through the use of progressive refinement of images using quadtrees <I>, as well as extensions to binary 
trees using Progressive Transmission (hereafter PT) <2>. Both approaches send 'coarse information first' 
to inform the user early in the reception phase about the general nature of the image. The quadtree 
method usually forms 'fat pixels' by computing numerical averages of four neighboring small pixels. 
This approach can suffer from round-off error unless additional bits are used to represent the average 
values <3>. By contrast, with the PT method there is no loss of precision as the encoding progresses: 
exactly the same number of bits is needed to represent the image at each stage. The method is simple 
and efficient, and is completely reversible, as discussed below. With the PT method, the original r*c*b 
bits of the image are first ENCODED into a new set of r*c*b bits at the archive site, and stored in encoded 
format. Supposing this has been done, we focus first on the image reception phase, which involves DECODING. 
When an image is requested by a remote user, bits are sent so that the received image fills in. During 
reception, the image is successively bisected (alternately in the horizontal and vertical directions) 
by sending b bit quantities which Knowlton called DIFFERENTIATORS (hereafter "diff's"). Figure 2 shows 
the progression. Each diff combines with a displayed COMPOSITE ("comp") value to produce two new comp's 
for the new smaller pixels.  8= b 2b 4b 16b r,.b Fig. 2 Image Seen after Receipt of B bits. The decoding 
process begins upon receipt of b bits for the comp of the single fat pixel, and each splitting requires 
b bits more. Hence after M*b bits have been received the displayed image consists of M 'fat pixels'. 
In order that the low resolution versions faithfully imitate the true image, the b bit color/intensity 
in each fat pixel must approximate the average value of all the small pixels contained within the fat 
pixel. An important advantage in Knowlton's PT method is that all quantities are b bit integers, and 
the usual numerical roundoff errors which would accumulate in finding the averages are avoided. Instead 
of calculating averages, look-up tables (of size 2**b of a side) are used to map a comp and diff pair 
into two new comp's. One may view this process as a decoding transformation D which maps pairs of b bit 
values into similar pairs, denoted: D(eomp, diff) = (compl ,comp2). (I) The possible comp &#38; diff 
values are used as indices, and the resulting pair of decoded comp's is found in the table. (This is 
consistent with Knowlton's approach, although couched in somewhat different terms.) Fig.3a shows such 
a table for the simple case b=2. For example, (2,3), (i.e. the element in row 2 and column 3), maps 
into (3,0), so a fat pixel with eomp value of 2 will be split by a diff of value 3 into 2 new comp's 
with values 3 and 0.  a.) b.) 3 23 33 32 I 31 i 2 13 22 21 J 30 , o '~, o ,,, 1 03 12 11 20 i 1/,ol/=o 
 0 02 Ol O0 10 oo%1% 0 1 2 3 0 1 2 3 Look-up Table Error Distribution Fig.3 An Example Look-Up Table. 
 In concert with the decoding process performed at the user's terminal, the ENCODING process maps original 
pixel pairs into (comp,diff) pairs. Adjacent image pixels are combined in pairs, the two comp values 
producing a single fat pixel comp and a new diff according to an encoding transformation E(compl,comp2) 
= (comp,diff). This encoding is done at the image archive site, after which the image is stored in PT 
format, as one comp and r'c-1 diff's. Knowlton again used look-up tables for this mapping E. An essential 
requirement is that the mappings D and E be inverses of each other: D(E(.,.)) : (.,.), so that the recovered 
pixels are always identical to the originals. In this sense the method is information lossless and reversible, 
while the numerical averaging approach is reversible only if averages are computed and retained exactly. 
 Returning to the decoding stage, the table defining mapping D is arranged so that each comp splits into 
two new comp's whose average is as close as possible to the original comp. Inspection of the table in 
Fig.3a shows an error distribution (error = true average -original comp) as in Fig.3b. For instance, 
along the i=j diagonal the error can be made 0 by using i as the comp. In general,the error values are 
bounded by 2"*(b-2). Due to reversibiity, however, these errors do not accumulate: if the decoding is 
allowed to proceed to its conclusion, there will be NO error in the final recovered pixel values. It 
is useful to view PT in the context of a 'transmission cone' as shown in Fig.4, analogous to the familiar 
'processing cones' and 'quadtree pyramids' <3,4>. Although in fact images are bisected at each stage,allowing 
a gradual increase in resolution, we think of combining two stages together, bisecting the image in 
both directions (thus quadrupling the number of fat pixels), and denote different LEVEL's of resolution. 
Thus an image of level L contains 4**L pixels, and the 'typical image' has a maximum display level 
of 9. The encoding process progresses down the cone until the single fat pixel at level 0 is reached, 
while decoding with the stream of diff's progresses up the cone to the level desired. To obtain a level 
L+I image from one at level L requires the transmission of an additional 3"(4"*L) diff's, and so takes 
four times as long to receive the data for each successive level. For this reason it is particularly 
advantageous for a Origtelnl Image Level log 4 (b)  DecodingLevel 2 Encoding Level 1  Level O (Single 
Plxel) Fig.4 The Transmission Cone. user to be able to decide at a low level whether or not an image 
is interesting. For instance, if Time(L) is the time required to transmit an image at level L then Time(6)/Time(9)= 
1/64, and the user acquires a level 6 version in 1/64 the time needed for the level 9 image. A significant 
limitation in the originSl PT scheme is the requirement for the two look-up tables. In Knowlton's examples 
each pixel was represented by only four bits, hence the tables were only 16 by 16. However, the much 
richer images associated with remotely sensed data may contain 24 bits/pixel, requiring a user to construct 
two unmanageable 8.5 gigabyte tables to use PT. We present below an extension of Knowlton's method to 
allow its application to the very demanding area of full-color images with any number of bits/pixel. 
The look-up tables are replaced with a SINGLE simple procedure that works for BOTH encoding and decoding. 
Its space complexity is independent of the number of bits/pixel. We discuss the tables and algorithms 
in the context of more general transformations. We also present several results with color images. We 
describe an efficient and unifom storage scheme which facilitates the various query modes, and describe 
ways to save even futher time by reducing the color resolution during 'browsing' under the user's control. 
 II. EXTENDING PROGRESSIVE TRANSMISSION TO RICH IMAGERY. IIA). Elimination of the Look-up Tables. We 
focus on the nature of the transformations D and E above which map ordered pairs of b-bit numbers into 
other ordered pairs. Knowlton reported one such mapping for 4-bit values, in which he distributed values 
in a clever ad hoc way to build the desired tables: those having small errors in their error distributions 
(see Fig.3b). Because the table was constructed in this way, it is not clear whether a simple and compact 
algorithm exists that COMPUTES D(comp,diff) rather than looks up its values. We also need ways to extend 
the method to higher values of b. We first develop a simple geometric view of D which allows one to 
build a look-up table for any b. The tables agree exactly with Knowlton's reported cases. Using this 
structure we then construct a simple algoritbm which replaces the table, at a tremendous space saving. 
 IIB). The Ring Neighbor Algorithm: An image using b bits/pixel requires the equivalent of an M by M 
table, where M=2**b. Call by N the set of possible pixel values: N = (0,1 .....M-I). (2) Elements of 
the table are ordered pairs with components taken from set N. The table may be   Computer Graphics 
Volume 17, Number 3 July 1983 viewed as a mapping from the Cartesian product N X N onto itself: the pair 
of indices (i,j) maps into the pair D(i,j). The table must be one-to-one and onto (i.e. must be a permutation) 
for the inverse transformation E to exist: otherwise encoding and decoding would not be inverse operations, 
and PT would not be information lossless. There are many possible such tables, (Knowlton reported a 
particularly good one for b=4), but only a few will yield good error distributions. For instance, the 
IDENTITY mapping D(i,j) = (i,j) is indeed a permutation, but does not produce comp'a with the desired 
average value. In fact, one can see that the use of PT with the identity mapping is a simple sampling 
operation: each fat pixel at level L equals the Northwest pixel in its group of four at level L+I. Figure 
5 shows a table template for the case b=3 which reveals the structure of PT tables, and leads to a general 
algorithm. View the table as consisting of concentric rings. The innermost ring is composed of the central 
four elements, while the r-th ring has 4"(2r-I) elements. We find the 'ring neighbor' of any cell in 
the r-th ring by traversing the ring r elements in a clockwise direction, as shown in Figure 5. To build 
the table we use for each cell the INDICES of its ring neighbor. That is, for the cell at indices (i,j): 
 a). Determine which ring this cell lies within. b). Find the indices of its ring neighbor. c). Interchange 
the indices. d). Write them into cell (i,j) as the required ordered pair. cell Ring 3~ ~----its Ring 
 Neighbor ,i,g~ 1.11 I I I I ITI I Fig.5 Defining Ring Neighbors.  For example, in Fig. 3a, cell (2,3) 
lies in ring 2, and has as ring neighbor the cell at (0,3), so place (3,0) in the cell at (2,3). Note 
that a table built using this method will be identical to Knowlton's, except for a reflection about the 
i=j axis (a transposition), an operation which has no effect on the average error distribution of the 
PT method. (Also note that the table displaying the first element in each cell is a simple quarter- turn 
rotation of the table displaying the second element.) A r~arkable property of the tables which are 
generated by this approach is that they are INVOLUTIONS: Applying the mapping twice recovers the original 
index pair: D(D(i,j))=(i,j). This means that D is its own inverse, and that E and D are the SAME! Hence 
only ONE transformation must be constructed, and can be used for both encoding and decoding, a welcome 
simplification. Involutions are easy to construct: In general a table is an involution whenever the 
elements of N X N are taken in pairs, and each is placed at the other's index position. What is interesting 
here is that Knowlton's table (after a reflection) is automatically an involution. The ring neighbor 
approach demonstrates this involution property: For any cell find its ring neighbor, then reflect about 
the i=j axis, to locate the cell whose position is the image of the original cell. Now start at this 
point, again find the ring neighbor and reflect. You always return to the original cell. In the Appendix 
we list procedure CODE which computes D(i,j) for any (i,j). It is based on the ring neighbor algorithm, 
capitalizing on certain symmetries in order to reduce computation time. It operates on integers and has 
a trivial space complexity independent of b. Each encode or decode operation requires at most 4 integer 
compares and 7 integer adds,(3 compares and 4 adds on the average). IIC) ON THE TRANSMISSION CONE The 
transmission cone's pyramid structure helps the user visualize the various processing steps, and simplifies 
queries, while organizing the data for efficient image management. Although our transmission cone is 
quite similar to processing cones and quadtrees, several significant differences are worth noting: a). 
 Two passes over the image are required to move from one level to the next. Each pass requires only very 
simple algorithms, however, compared with the corresponding quadtree operations. Whereas the resulting 
intermediate stage is not normally available with the quadtree structure, with PT the user can display 
this intermediate stage if desired, and thus attain a smoother increase in resolution for a given volume 
of data. b). The ring neighbor encoding algorithm is used in lieu of numerical averaging. Hence moving 
between levels is an exactly reversible operation. e). If it is important to save memory at the user's 
terminal, only one level of the image need be stored at a time. Once the data for a certain evel image 
has been received, the user can migrate to lower levels by converting some comp's back to diff's, reusing 
the same memory locations. If he wishes to do some local processing, however, such as lowpass filtering 
the image in order to reduce the 'jaggies' , or some early segmentation of the image, the diff's would 
have to be stored in auxiliary memory.   To roam one simply does a sequence of window specifications, 
followed by fill's. Figure 12 shows some examples, where the user has filled in various regions of interest, 
tracing along rivers and mountains, until his curiosity was satisfied. The user's productivity is greatly 
increased using this mode, not only because each region fills in so quickly, but also because he ADAPTIVELY 
chooses the next region to fill in based on viewing earlier regions. The user can also zoom in on a region 
if desired, by pixel replication, (Fig.s 13 &#38; 14). 8x8  Level 9 ~ Le vel g Fig. ll Cone with Fill-in 
Operation.  There would be additional utilities stored in the user terminal to do processing on images 
at whatever level had been received. Besides routines to change between levels, some operations would 
manipulate image data at the same level, performing such things as edge detection or geometric corrections. 
Software and protocols would also be available to allow the user's terminal to receive and store images 
in an unattended mode, say overnight. Some images such as GOES weather photos (r=1800,c=3822,b=8, two 
taken each hour) are so large they cannot be displayed at full resolution on a typical (level 9) terminal. 
If they are encoded into PT format at the archive, however, the user can display one or any portion of 
one up to level 9. Each displayed pixel is then actually an average of many original pixels. In Fig. 
15 a 1024 X 1024 (level 10) LANDSAT image is shown on a level 9 display, providing the user with a more 
global view of the area. IIIA). Image Memory Organization. In order to carry out the various tasks, 
it is important to store image data at both the archive site and the user's terminal in a manner that 
permits easy access and modification, yet is not wasteful of memory. For instance, when the user wishes 
to fill-in only a portion of the image, how are the new requested diff's accessed and folded into the 
existing image memory? Fig. 16 suggests one scheme for storing a completely encoded image at the archive 
site. A single comp c0 will reside in the upper left corner of archive memory, and all other locations 
will contain diff's, ready for transmission. For concreteness view 'typical' archive memory as a 512 
by 512 array of cells. User refresh memory may be similarly configured, or may be reorganized so Archive 
Memory: D2 M +-64 ---~ f i D 1 .--*- ---~--'-- | I ,J L..., . Fit p|xel I / Fig. 16 Storage Scheme 
for Encoded Image. that for low-level images the refresh buffer can use a flood-fill approach to paint 
fat pixels without disturbing stored diff's. To transmit an image, the diff's are sent in the order 
dl, d2,d3,... As dl is decoded it combines with the previously received c0 to form two new comp's, say 
ci and c2. These are stored in the user terminal, and are are displayed by flood- filling the upper and 
lower half of the screen respectively. Similarly, d2 will combine with cl, and split the upper half into 
two square regions, etc. The important feature is that the relevant diff at each splitting stage resides 
in archive memory at the upper left corner of a certain region, and this position corresponds to the 
new subregion in user refresh memory that is being formed in the split. This hierarchical imbedding 
process continues to all levels, as suggested in the figure. One can see by working this process backwards 
that during the encoding phase of comp's to diff's no extra memory is required: as each pair of comp's 
is mapped into a new comp and a diff, the new values are put right back into memory in the spaces originally 
occupied by the two comp's. This both saves extra memory and facilitates addressing. Now suppose the 
user displays a level 6 image, indicates a window to be filled in, and describes the window to the archive. 
The chosen window will contain several fat pixels, each one corresponding in archive memory to 64 cells, 
containing one comp and 63 diff's. These are precisely the diff's needed to fill in that fat pixel. The 
proper diff's for the fat pixels are thus easily identified at the archive and sent to the user. IIIB). 
Color Quantization: In browse mode the user will not usually need to see the fat pixels in their true 
colors. In order to save transmission time, the first set of comp's and diff's can be sent with fewer 
than b bits. For instance, the two most significant bits for each of the three colors could be sent, 
thus giving the fat pixels 64 possible colors. Alternatively, the user could request to see only say, 
the green portion of the image, and perhaps only its first 2 or 3 bits. Later, if the user desires to 
let the image fill in to its full quality, the remaining bits for each pixel could be sent (perhaps 
between interactive sessions in a form of batch mode), and the final image constructed. There are many 
possible ways to precode, transmit, and recombine the color information, and suitable studies will be 
done on how much color information a user needs in order to make reasonable judgements about the content 
of an image based on viewing pixels at reduced resolution in both the spatial and color domains. This 
is currently a topic of ongoing research. IV).CONCLUSION: An image query system is described wherein 
an image archive host sends image data to remote users upon request over possibly slow transmission links. 
Progressive transmission is used to provide the user with a quick-look capability at reduced resolution, 
so that he can abort further transmission if the image is not needed. We have extended the PT scheme 
to the area of rich full color imagery, circumventing the barrier of impossibly large look-up tables 
by fashioning a simple algorithm to perform the required intensity mappings. Some more fundamental properties 
of this mapping are noted as well. We also describe different user query modes in the context of a 'transmission 
cone' metaphor, which is used to allow the user to fill in selected subregions of an image to full resolution, 
and to roam incrementally over images. Acknowledgement : This work was supported in part by the Remote 
Sensing Center at the University of Massachusetts, through the ASTECH Corp. The Visions Group at the 
University graciously donated use of their facilities. Thanks also go to Ken Ward for his help in coding 
various experiments. REFER~CES: (I) K. R. Sloan &#38; S. L. Tanimoto, "Progressive Refinement of Raster 
Images", IEEE Trans. on Computers, C-28, No. 11, Nov. 1979, pp.871- 874.  (2) Ken Knowlton, "Progressive 
Transmission of Grey-Scale and Binary Pictures by Simple, Efficient, and Loss-less Encoding Schemes" 
Proc. of IEEE, Vol. 68, No. 7, 1980, pp.885-  896.  (3) Theo Pavlidis, GRAPHICS AND IMAGE PROCESSING,Computer 
Science Press, Rockville, MD 1982.  (4) A. Hanson &#38; E. R iseman, "Segmentation of Natural Scenes" 
in COMPUTER VISION SYSTEMS  (A. Hanson &#38; E. Riseman, eds.) Academic Press, New York, 1978.  (5) 
S. E. Walker, Jr &#38; F. S. Hill, Jr. , "Progressive Transmission of Remotely Sensed Images" 4-th Symposium 
on Automation in Engineering Data Handling, Naval Postgraduate School, Monterrey, CA, Nov. 3, 1982. 
 APPENDIX : A listing of procedure CODE follows, showing how to map two pixel values PI and P2 into 
a comp and a diff. The same routine may be used for the inverse mapping. M=2**b, where b is the number 
of bits/pixel. Procedure Code ( M , P I , P2 : i n t eg e r ; V a r comp,diff:integer); Var MM: integer; 
 begin MM : = M div 2; if (PI+I)<= MM then if P2<=PI then begin comp:=P2; diff:= PI+MM -P2 end else 
begin comp:=PI+P2-MM; if comp<=P1 then comp:=P1; diff:=P1+MM -P2; if MM < P2 then if P2 < (M- PI) then 
diff := PI else diff := M -P2 -I end else if (P2+I) <= MM then begin comp:: PI+P2+I-MM; diff:= PI; 
if diff <= (M-P2-1) then diff:= M - P2-I end else begin comp:=P2; if comp <= PI then comp:=P1; diff:= 
PI+MM -I -P2 end end (*proc CODE ~);   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801165</article_id>
		<sort_key>331</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>40</seq_no>
		<title><![CDATA[Graphics in overlapping bitmap layers]]></title>
		<page_from>331</page_from>
		<page_to>355</page_to>
		<doi_number>10.1145/800059.801165</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801165</url>
		<abstract>
			<par><![CDATA[<p>One of the common uses of bitmap terminals is storing multiple programming contexts in multiple, possibly overlapping, areas of the screen called windows. Windows traditionally store the visible state of a programming environment, such as an editor or debugger, while the user works with some other program. This model of interaction is attractive for one-process systems, but to make full use of a multiprogramming environment, windows must be asynchronously updated, even when partially or wholly obscured by other windows. For example, a long compilation may run in one window, displaying messages as appropriate, while the user edits a file in another window.</p> <p>This paper describes a set of low-level graphics primitives to manipulate overlapping asynchronous windows, called <italic>layers,</italic> on a bitmap display terminal. Unlike previous window software, these primitives extend the domain of the general bitmap operator <italic>bitblt</italic> to include bitmaps that are partially or wholly obscured.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Raster image]]></kw>
			<kw><![CDATA[bitblt instruction]]></kw>
			<kw><![CDATA[bitmaps]]></kw>
			<kw><![CDATA[windows]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Bitmap and framebuffer operations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP45026692</person_id>
				<author_profile_id><![CDATA[81343502664]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pike]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BRESENHAM, J. E. Algorithm for computer control of a digital plotter. IBM Syst. J. 4, 1 (1965), 25-30.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357308</ref_obj_id>
				<ref_obj_pid>357306</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[2. GUIBAS, L. J., AND STOLFI, J. A language for bitmap manipulation. ACM Trans. Gr. 1, 3 (July 1982), 191-214.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[INGALLS, D. H. H. The Smalltalk graphics kernel. Byte 6, 8 (Aug. 1981), 168-194.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806574</ref_obj_id>
				<ref_obj_pid>800215</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[LANTZ, K. A. AND RASHID, R. F. Virtual terminal management in a multiple process environment. In Proc. 7th Syrup. Operating Systems Principles (Pacific Grove, Calif., Dec. 10-12), ACM, New York, 1979, pp. 86-97.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806607</ref_obj_id>
				<ref_obj_pid>800216</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[MEYROWITZ, N., AND MOSER, M. BRUWIN: An adaptable design strategy for window manager/ virtual terminal systems. In Proc. 8th Syrup. Operating Systems Principles 15, 5 (Pacific Grove, Calif., Dec. 14-16), ACM, New York, 1981, pp. 180-189.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[NEWMAN, W. M., AND SPROULL, R. F. Principles of Interactive Computer Graphics, 2nd ed. McGraw-Hill, New York, 1979]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[TESLER, L. The Smalltalk environment. Byte 6, 8 (Aug. 1981), 90-147.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[WEINREB, D., AND MOON, D. Introduction to using the window system. Symbolics, Inc., 1981.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Graphics in Overlapping Bitmap Layers ROB PIKE Bell Laboratories One of the common uses of bitmap terminals 
is storing multiple programming contexts in multiple, possibly overlapping, areas of the screen called 
windows. Windows traditionally store the visible state of a programming environment, such as an editor 
or debugger, while the user works with some other program. This model of interaction is attractive for 
one-process systems, but to make full use of a multiprogramming environment, windows must be asynchronously 
updated, even when partially or wholly obscured by other windows. For example, a long compilation may 
run in one window, displaying messages as appropriate, while the user edits a Efie in another window. 
This paper describes a set of low-level graphics primitives to manipulate overlapping asynchronous windows, 
called layers, on a bitmap display terminal. Unlike previous window software, these primitives extend 
the domain of the general bitmap operator bitblt to include bitmaps that are partially or wholly obscured. 
CR Categories and Subject Descriptors: 1.3.3 [Computer Graphics[: Picture/Image Generation; 1.3.4 [Computer 
Graphics[: Graphics Utilities. General Terms: Algorithms Additional Key Words and Phrases: Raster image, 
bitmaps, bitblt instruction, windows. 1. INTRODUCTION Bitmap displays are in vogue. Despite their drawbacks, 
they are sufficiently inexpensive to be personal equipment rather than shared computer center re- sources. 
Unfortunately, although their use has become widespread, most bitmap displays are controlled by primitive 
software. Bitmap screens are commonly divided into a set of rectangular areas called windows, each of 
which maintains the input and output context for a particular program or graphics application. A bitmap 
display can become several screens at once--different windows can be emulating standard terminals, graphics 
terminals, drafting tables, or even arcade games. The screen areas need not be disjoint--the display 
areas of two programs may overlap, with one area fully visible and the other partially or wholly obscured. 
This paper describes software to manipulate these overlapping windows, or layers, on a single display. 
A program drawing in a layer is fully isolated from other programs drawing on the same screen, even if 
one layer overlaps, and thereby obscures part of, the other. By extending the Author's address: Rob Pike, 
Bell Laboratories, Murray Hill, NJ 07974. Permission to copy without fee all or part of this material 
is granted provided that the copies are not made or distributed for direct commercial advantage, the 
ACM copyright notice and the title of the publication and its date appear, and notice is given that copying 
is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. &#38;#169; ACM 0-89791-109-1/83/007/0331 $00.75 Reprinted From acm 
Transactions On Graphics--April 1983--Vol. 2, No. 2 Rob Pike basic bitmap operators to work with layers, 
which may be regarded as possibly obscured, subdivided bitmaps, each program manipulates its layer just 
as it would manipulate the full screen or any other bitmap in a more conventional environ- ment. Each 
program is removed from considerations regarding, for example, which layers obscure which, and two programs 
may simultaneously draw in their respective layers, even though some of the drawn objects are currently 
obscured from view. 2. COMPARISONS WITH PREVIOUS WORK A number of window systems have been implemented 
by others [4, 5, 7, 8], but the software presented here has some distinguishing characteristics, dictated 
largely by the environment in which it is run. The environment is a bitmap display connected to a multiprogrammed 
timesharing system, with a limited but useful amount of offscreen memory (about a screenful) for storing 
bitmaps. Because the system is multiprogrammed, any program may want to update its window at any time, 
regardless of whether it is bound to the keyboard and mouse or even whether it is fully visible. It is 
also important that the user program be removed from concerns about window update: changes in the window 
configura- tion should be invisible to the program. This precludes the simple "repaint" mechanism, wherein 
a program is instructed to redraw its window whenever its visibility changes, because repainting forces 
the user program to deal with issues better left to lower level software. Like the file system, which 
makes a user file look like a continuously addressable stream of bytes even though it might be scattered 
across the disk, the window system should provide a simple program- ming interface so a program can always 
draw in its window as though it were a contiguous bitmap, regardless of its position or visibility on 
the screen. There are two basic approaches to providing this transparency. The first, exemplified by 
the BRUWIN system [5], is to maintain a display list that is reevaluated when the window configuration 
changes. Although this method typically uses a relatively small amount of memory to hold the display 
list, and makes it easy to rearrange the layout of the windows on the display, it presents a few problems. 
One problem is that the time it takes to redraw a freshly uncovered portion of a window is substantial 
if the window's display list is long. More important, it is only possible to draw what the display list 
can encode. BRUWIN, for example, only supports characters, contradicting the generality of the bitmap 
data structure. Even adding bitmap primitives to the display list is not entirely satisfactory, however. 
Consider using a bitmap paint program for creating a realistic scene: the bitmap containing the scene 
is probably smaller than the display list that describes it, and certainly takes much less time to draw 
on the screen. Display lists introduce an unnecessary level of indirection. The second method, used on 
the LISP Machine [8], is to keep the window image in a contiguous off-screen bitmap and copy data from 
there to the visible portions of the on-screen window. This simple method has the advantage of supporting 
any bitmap operation, but takes a constant large amount of auxiliary memory and can be inefficient--it 
may be necessary to execute each bitmap operation twice: once off-screen and once on-screen. Reprinted 
From acm Transactions On Graphics--April 1983--Vol. 2, No. 2 The layers software is in some sense an 
optimization of this "shadow bitmap" method. By dividing the windows into visible and obscured parts, 
and only keeping obscured parts off-screen, it uses the minimum memory possible {assum- ing no display 
lists) by letting the screen memory itself hold the visible parts. The basic idea is to extend the domain 
of the general bitmap operator bitblt [2] to include windows that are wholly or partially obscured. Layer 
drawing opera- tions, therefore, touch the same number of pixels regardless of visibility, although the 
number of bitblt operations may be larger if there are on-screen and off- screen components. The data 
structures and software are designed to make drawing in a layer as fast as possible, at the expense of 
slower creation and deletion operators (although creation and deletion are still fast). The most common 
layer shuffling operator {making a layer fully visible) is also efficient because it touches only those 
pixels that need to be updated. There are disadvan- tages, of course: the data structures become unwieldy 
with more than about a dozen overlapping layers, and translating a layer across the screen is clumsy, 
but the software has been implemented and works comfortably in our time-sharing environment, where the 
number of layers is fairly small and their average lifetime long. 3. DEFINITIONS The programs described 
herein, written in a pseudo-C dialect, use several simple defined types and primitive bitmap operations. 
This section describes the basic concepts for bitmap operations, but does not attempt to be complete. 
The next section defines bitmaps precisely. A Point is an ordered pair typedef struct{ int x, y; }Point; 
that defines a location in a bitmap such as the screen. This paper uses some typographical conventions 
to clarify the distinctions between abstract objects and their software analogs. Abstract data type~ 
are in roman font, with their first letters capitalized {e.g., Point). Following C notation, functions 
and procedures are written in Helvetica typeface, with a {usually empty) parenthesis set after them (e.g., 
bitblt()). The generalizations of bitmap operators for layers are named by the bitmap operator prefixed 
by an 'T' (e.g., bibtlt(), as opposed to Lbitblt()). Finally, entry point subroutines are in lowercase, 
while subservient routines called by layerop() {see Section 6) have their first letters capitalized (e.g., 
Iblt(), as opposed to Lblt{ )). A Rectangle is defined by a pair of Points spanning a diagonal: typedef 
struct{ Point origin; /* min x,y */ Point corner; /* max x,y */ }Rectangle; Reprinted From acm Transactions 
On Graphics--April 1983--Vol. 2, No. 2 By definition, corner.x >__ origin.x and corner.y _> origin.y. 
Rectangles are half-open: a Rectangle contains the horizontal and vertical lines through origin, and 
abuts, but does not contain, the lines through corner. Two abutting Rectan- gles r0 and rl, with rl. 
origin = (ro. corner, x, r0. origin, y), therefore have no point in common. The same app],ies to lines 
in any direction: a line segment drawn from (x0, y0) to (xl, yl) does not contain (x~, y~). These curious 
definitions simplify drawing objects in pieces, which is essential to the layer primitives. The routine 
rectf(b, r, f) performs a function specified by an integer code f, in a Rectangular r, in a bitmap b. 
The function code is one of CLR: clear Rectangle to zeros OR: set Rectangle to ones XOR: invert bits 
in Rectangle The routine bitblt(sb, r, db, p, f) (bit block transfer) copies a source Rectangle r in 
a bitmap sb to a corresponding Rectangle with origin p in a destination bitmap db. bitblt( ) is therefore 
a form of Rectangle assignment operator, and the function code f specifies the nature of the assignment: 
STORE: dest = source OR : dest l = source CLR : dest &#38; = ~ source XOR: dest A = source For example, 
OR specifies that the destination Rectangle is formed from the bitwise OR of the source and destination 
Rectangles before the bitblt(). Here, ",, 8,,", and ~ are C's bitwise OR, AND, exclusive-OR, and NOT 
operators, and the notation x * - y is equivalent to x = x * y, but is notationally and semantically 
closer to the machine. Often, bitblt() is defined to copy through a two-dimensional mask, to permit stippling 
and related operations. Our original bitblt() had implementation prob- lems (which have since been resolved) 
that forced us not to incorporate a mask into the primitive; for compatibility, our current bitblt() 
also does not have a mask. However, whether a mask is present has no bearing on the ideas presented here. 
bitblt() is a fundemental bitmap operation. It is used to draw characters, save screen rectangles, and 
present menus. Defined with a bit mask, it includes rectf(). For a thorough description, see [2] or [3]. 
bitblt() can be a very expensive operation, however. Generally, the data from the source Rectangle must 
be shifted or rotated and masked before being written to the destination Rectangle, and the amount of 
data processed in a single bitblt( ) can be surprisingly large. 4. BITMAPS A bitmap is a dot-matrix representation 
of a rectangular image. The details of the representation depend on the display hardware, or, more specifically, 
on the arrangement of memory in the display. For bitmaps to mesh well with the software in a display, 
the screen must appear to the program as a bitmap with no special properties other than its visibility. 
Because images (bitmaps) are stored off-screen, off-screen memory should have the same format as the 
screen itself, so that copying images to and from the screen is not a special case in the software. 
Reprinted From acm "l:ransactions On Graphics--April 1983--Vol. 2, No. 2  base rect origin Jx word bdry 
word bdry Fig. 1. Layout of a bitmap. The storage begins and ends on word boundaries, with the last word 
of one scan line followed immediately by the first word of the next scan line. The hatched area, Bitmap. 
rect, stores the image. The simplest way to achieve this generality is to make the screen a contiguous 
array of memory, with the last word in a scan line followed immediately by the first word of the next 
scan line. Under this scheme, bitmaps become something familiar to the programmer: a two-dimensional 
array. Given a two-dimensional array in which to store the actual image, some auxiliary information is 
required for its interpretation. Figure 1 illustrates how a bitmap is interpreted. The hatched region 
is the location of the image. When a bitmap is allocated, the allocation routine, balloc(), assumes the 
bitmap data will correspond to a screen Rectangle, for example, a part of one layer obscured by another, 
balloc() creates the left and right margins of the bitmap to word-align the bitmap with the screen, so 
that word boundaries in the bitmap will be in the same relative positions as in the screen. This technique 
can make a significant performance difference for the common operation of saving and restoring screen 
rectangles. balloc() takes one argument, the on-screen rectangle that corresponds to the bitmap image, 
and returns a pointer to a data structure of type Bitmap. Bitmap is defined thus: typedef struct{ Word 
*base; /* start of data */ unsigned width; /~ width in words */ Rectangle rect; /* image rectangle */ 
}Bitmap; The declaration Word * base states that base points to an object of type Word, Reprinted From 
acre Transactions On Graphics--April 1983--Vol. 2, No. 2 or rather that * base is a Word pointed to by 
base. (Pointers in C can point to objects not allocated dynamically; the layers software depends on this.) 
The elements of the structure are illustrated in Figure 1. width is in Words, which are some convenient 
unit of storage, rect is the argument to balloc() and defines the coordinate system inside the Bitmap 
(the next section, which deals with the Layer data structure, explains why this is done). The storage 
in the Bitmap outside rect--the unhatched portion in Figure 1--is unused, as described above. Typically, 
width is the number of Words that fit h6rizontally within the Bitmap, between the arrows in Figure 1. 
A Bitmap may be contained in another Bitmap, however, if its width is the same as that of the outer Bitmap, 
and base points as usual to the first Word in the Bitmap--the Word containing the pixel at the origin 
of the Bitmap. Although such Bitmaps are not created by balloc(), they have utility--they can represent 
the portion of the screen occupied by a layer, for instance. balloc( ) and its obvious counterpart bfree( 
) hide all issues of storage manage- ment for bitmaps. Because the sizes of bitmaps are widely variable, 
balloc() does garbage compaction, moving all allocated bitmaps to one end of the arena when space runs 
low, thereby ensuring that memory does not fragment. The Bitmap structure is used throughout the layer 
software. Graphics primi- tives operate on points, lines, and rectangles within all Bitmaps, not just 
those on the screen. The screen itself is simply a globally accessible Bitmap structure, called display, 
and is unknown within the graphics primitives. 5. LAYERS A layer is a rectangular portion of the screen 
and its associated image. It may be thought of as a virtual display screen. Layers may overlap (although 
they need not), but the image in the obscured portion of a layer is always kept current. Typically, an 
asynchronous process, such as a terminal program or circuit design system, draws pictures and text in 
a layer, just as it might draw in any bitmap {such as the full screen if it were the only process on 
the display). Because processes are asynchronous, drawing actions can take place at any time in an obscured 
layer, and a graphical object such as a line may be partially visible on the screen and partially in 
the obscured portion of the layer. The layer software isolates a program drawing in a layer on the screen 
from other such programs in other layers, and guarantees that the image on- and off-screen is always 
correct, regardless of the configuration of the layers on the screen. Layers must be differentiated from 
the common notion of windows. 1 Windows are used to save a programming or working environment, such as 
a text editing session; to process "interrupts," such as looking at a file or sending mail; or to keep 
several static contexts, such as file contents, on the screen. Layers are intended to maintain an environment, 
even though it may change because the associated programs are still running. For example, a programmer 
could be editing the program source in one layer while watching for diagnostic messages from a long compilation 
taking place in another, and waiting for intruders into The term "window" in common usage is a misnomer 
(see, e.g., [6]). The meaning is actually closer to that of the older term "viewport." Reprinted From 
acm Transactions On Graphics--April 1983--Vol. 2, No. 2 Layer A Fig. 2. Obscured portions of a layer 
are stored off-screen and linked to the obscured layer. Layer B his corner of the labyrinth in a real-time 
game in a third. There is, of course, nothing profound in the distinction between layers and windows; 
the term "layer" was coined to avoid the more cumbersome phrase "asynchronous window." Nonetheless, the 
difference between layers and windows is significant. Multiple active contexts are natural to use and 
powerful to exploit. Truly asynchronous graphics operations are difficult to support, because the state 
of a layer may change while a graphics operation is underway. The obvious simple solution is to perform 
graphical operations atomically by inhibiting process switches during calls to the layer software. The 
data structures for layers are illustrated in Figures 2 and 3. A partially obscured layer has an obscured 
list: a list of disjoint rectangles in that layer obscured by another layer or layers. In Figure 2, layer 
A obscures layer B. Layer B's obscured list has a single entry, which is marked "obscured by A." If more 
than one layer obscures a rectangle, the rectangle is marked as obscured by the frontmost {unobscured) 
layer intersecting the rectangle. This is illustrated by rectangle 4 in Figure 3. Rectangle 4 is an obscured 
part of both layer B and C, so these layers store their obscured pieces off-screen and mark them blocked 
by layer A. Rectangles 3 and 4 in layer B (Figure 3) may be stored as a single rectangle, as they are 
in Figure 2. They are stored as two because if layer C is later moved to the front of the screen {i.e., 
to the top of the pile of layers), it will obscure portions of both layers A and B. Rectangle 4 in layer 
B would be obscured by C, but rectangle 3 would still be obscured by A. To simplify the algorithms for 
rearrang- ing layers, the layer creation routine does all necessary subdivision when the layer is first 
made, so when layer C is created 2, the obscured rectangle in B is split in two along the edge of the 
new layer. 2 Despite Figure 3, layer C is actually created at the front,but the argument is nonetheless 
true. Reprinted From acm Transactions On Graphics--April 1983--Vol. 2, No. 2 Computer Graphics Volume 
17, Number 3 July 1983 Layer A Layer C I ,3', 4 I 1 I ',2 Layer B Fig. 3. Rectangle 4 is obscured in 
layers B and C, and is maintained off-screen by both. The addition of layer C divides the obscured part 
of layer B into two rectangles, 3 and 4. The Layer structure is typedef struct{ Word *base; /* start 
of data */ unsigned width; /* width in words */ Rectangle rect; /* image rectangle */ Obscured *obs; 
/* linked list of obscured rectangles */ Layer *front; /* adjacent layer in front */ Layer *back; /* 
adjacent layer behind */ } Layer; typedef struct{ Layer *lobs; /* frontmost obscuring Layer */ Bitmap 
*bmap; /* where the obscured data resides */ Obscured *next; /* chaining */ Obscured *prev; } Obscured; 
The first part of the Layer structure is identical to that of a Bitmap. Actually, the Bitmap structure 
has an extra item to it: a NULL obs pointer, so that a Bitmap may be passed to a graphics routine expecting 
a Layer as argument. The next section explains why a Bitmap with a NULL obs pointer looks like a Layer. 
The individual Layers are chained together as a doubly linked list, in order from "front" to "back" on 
the screen. (When they do not overlap, the order is irrelevant.) Besides the link pointers, a Layer structure 
contains a pointer to the list of obscured rectangles and the bounding rectangle on the screen. The obscured 
lists are also doubly linked, but in no particular order. Each element in the Reprinted From acm Transactions 
On Graphics--April 1983--Vol. 2, No. 2 obscured list contains a Bitmap for storing the off-screen image, 
and a pointer to the frontmost Layer that obscures it. As we shall see later, an Obscured element need 
only record which Layer is visible on the screen "in front" of it, not any other obscured Layers that 
also share that portion of the screen. The screen coordinates of the obscured Rectangle are held in the 
rect field of the Bitmap structure pointed to by the Obscured list entry, Obscured. bmap-> rect. All 
coordinates in the layer manipulations are screen coordinates. Note that a Bitmap holding an obscured 
rectangle of a Layer has in its rect field a definition of the screen space it would occupy were it visible. 
This is an appealing model: as a Layer's contents are subdivided into visible and obscured portions, 
so is its coordinate system. More generally, since a Layer structure must contain a rectangle defining 
the portion of the screen it occupies, so must a Bitmap, because Layers are simply a generalization of 
Bitmaps. 6. LAYEROP layerop( ) is the main interface between layers and the bitmap graphics primitives. 
Given a Layer, a Rectangle within the Layer, and a bitmap operator, it recursively subdivides the Rectangle 
into Rectangles contained in single Bitmaps, and invokes the operator on the Rectangle/Bitmap pairs. 
To simplify the operators, layerop() also passes along to the bitmap generator a pointer to a set of 
param- eters. For example, to clear a rectangle in a layer, layerop() is called with the target Layer, 
the rectangle within the layer in screen coordinates, and a procedure (the bitmap operator), to invoke 
rectf(), layerop( ) divides the rectangle into its components in obscured and visible portions of the 
layer, and calls the procedure to clear the component rectangles. A complete example is worked through 
at the end of this section, layerop( ) itself does no graphical operation; it merely controls graphical 
operations done by the bitmap operator handed to it. It is a functional that turns a bitmap operator 
into a layer operator. layerop( ) first clips the target Rectangle to the Layer, then calls the recursive 
routine Rlayerop( ) to do the subdivision: /* * Clip to outer rectangle of layer, then call Rlayerop0 
*/ layerop(Ip, fn, r, otherargs) Layer *lp; procedure (*fn)0; /* Pointer to bitmap operator */ Rectangle 
r; misc otherargs; /* Other arguments used by (*fn)0 */ { r = intersection of r and Ip->rect; if(r not 
null) Rlayerop(Ip, fn, r, otherargs, Ip->obs); } The argument (*fn)() is the pointer to the bitmap-level 
procedure. Rlayerop() recursively chains along the obscured list of the Layer, performing the operation 
Reprinted From acre Transactions On Graphics--April 1983--Vol. 2, No. 2 on the intersection of the argument 
Rectangle and the obscured Bitmap, and passing nonintersecting portions on to be intersected with other 
Bitmaps on the obscured list. When the obscured list is empty, the rectangle must be drawn on the screen. 
The code to test if two rectangles overlap is simple, but not well known: rectXrect(r, s) /* Do r and 
s intersect? */ Rectangle r, s; { return( r.origin.x < s.corner.x &#38;~, s.origin.x < r.corner.x &#38;&#38; 
r.origin.y < s.corner.y &#38;&#38; s.origin.y < r.corner.y ); The &#38;&#38; operator in C is a logical 
AND that does not evaluate the right operand if the left operand is false. Similarly, the = = operator 
used below is the equality operator. Here is Rlayerop( ): /* * Rlayerop --recursively subdivide and intersect 
 * rectangles with obscured bitmaps in layer  */ Rlayerop(Ip, fn, r, otherargs, op) Layer *lp; procedure 
(*fn)0; Rectangle r; mist otherargs; Obscured *op; /* Element of obscured list with which to intersect 
r */ if(op --= NULL) /* This rectangle not obscured */ (*fn)(Ip, r, display, otherargs, op); /* Draw 
on screen */ else if(rectXrect(r, op->bmap->rect) = = FALSE) /* They miss */ Rlayerop(Ip, fn, r, otherargs, 
op->next);/* Chain */ else{ /* They must intersect */ if(r.origin.x < op->bmap->rect.origin.x){ Rectangle 
temp = piece of r left of op->bmap->rect; Rlayerop(Ip, fn, temp, otherargs, op->next); r->origin.x = 
op->bmap->rect.origin.x; } /* etc. for other three sides of rectangle */ /* What's left goes in this 
obscured bitmap */ (*fn)(Ip, r, op->bmap, otherargs, op); }} The Layer pointer and Obscured pointer are 
passed to the bitmap operator (* fn)() because, although they are clearly not needed for graphical operations, 
Reprinted From acm Transactions On Graphics--April 1983--Vol. 2, No. 2 layerop( )'s subdivision is useful 
enough to be exploited by some of the software to maintain the layers themselves; we see an example of 
such usage later on. Note that if layerop( ) is handed a Layer with a NULL obs pointer, or a Bitmap, 
its effect is simply to clip the rectangle and call the bitmap operator. So far, otherargs has been referred 
to in a deliberately vague manner. In practice, layerop( ) works something like the standard C formatted 
output routine printf( ): after the arguments required by layerop( ) (the Layer, bitmap operator, and 
Rectangle), the calling function passes the further arguments needed by the Bitmap operator, layerop() 
passes the address of the first of these arguments through the operator, which therefore sees a pointer 
to a structure containing the necessary arguments. The following example illustrates the action of layerop(). 
Although we will see several examples later, this is the simplest and most common use. Iblt() uses layerop( 
) and bitblt( ) to copy an off-screen Bitmap to a Rectangle within a Layer. The Bitmap may contain, for 
example, a character. The general case of copying a Rectangle from one Layer to another is discussed 
later. Lblt(I, r, db, fp, o) Layer *1; Rectangle r; Bitmap *db; /* Destination Bitmap */ struct{ Bitmap 
*sb; /* Source Bitmap */ int f; /* Function code */ } *fp; Obscured *o; bitblt(fp->sb, r, db, r.origin, 
fp->f); Iblt(I, sb, r, f) Layer *1; Bitmap *sb; Rectangle r; layerop(I, Lblt, r, sb, f); } Notice that 
the bulk of the code is declarations. This code assumes that the source Bitmap is in screen coordinates; 
if it were not, extra arguments to Iblt() woule be passed to the operation routine Lblt( ) to describe 
the necessary coordi- nate transformation. The "extra" arguments to layerop, sb and i, are seen by Lblt 
as elements of a structure pointed to by Ip. As an example, consider drawing a character (the hashed 
rectangle) in Layer B in Figure 4. The first call to Rlayerop( ) has op set to the obscured, off-screen 
element of layer B. op is not NULL, nor is the overlap of r and rp- > rect, so we fall into the last 
block of the large if-else: part of the rectangle must be drawn in this obscured bitmap. The test if(r. 
origin, x < op- > bmap- > rect. origin, x) Reprinted From acre Transactions On Graphics--April 1983--Vol. 
2, No. 2 Layer A Fig. 4. Drawing a rectangle (for example a character) in a layer. Some of the rectangle 
is drawn on the screen, some in the obscured parts of the layer. The portion of the rectangle overlapping 
layer A is drawn Layer B in the obscured bitmap, not on the screen. succeeds, so the right portion of 
the rectangle (to the right of Layer A) is cut off and passed on to a second invocation of Rlayerop(). 
Here, the first if statement succeeds, the visible portion of the rectangle is drawn on the screen, and 
Rlayerop() returns to its first invocation. No further clipping is required, so the left portion of the 
rectangle is drawn in the obscured bitmap and the job is complete. Although it is a complicated operation, 
layerop() is fast compared to the expensive graphics operations it invokes. When layerop( ) is called, 
its overhead is usually unimportant. This is because the data structures to manipulate layers were explicitly 
designed to make drawing as rapid as possible, since that is the most common operation. The less common 
interlayer manipulations such as creating and deleting layers are slower, but their price is still reasonable, 
consid- ering how relatively infrequently they are invoked. 7. UPFRONT There are three basic transformations 
that can in principle be applied to layers: changing the dimensions of a layer (scaling), changing the 
position of a layer on the screen (translation), and changing the front-to-back positions of overlapping 
layers (stacking). The issues of scaling--changing a layer's size, as distinct from changing the scale 
of the coordinate systems inside the layer--and translation are not handled at all by the layer software, 
primarily because the subdivided nature of the data structures makes them difficult to implement. This 
is not to say that scaling and translation are not important transformations. Translation services, in 
particular, must be provided, perhaps by creating a new layer at the destination position, using ]bitblt( 
) (the generalized bitblt( ) operator, discussed later) to copy the data to the new position, and deleting 
the old layer. Note that the data in the layer can be dragged dynamically across the screen, using an 
auxiliary bitmap to hold the data obscured by the dragged rectangle, or even just dragging a rectangle 
Reprinted From acm Transactions On Graphics--April 1983--Vol. 2, No. 2 outline, without dynamically and 
incrementally modifying the layer data structure. Given the relative complexity of the subdivided layer 
structure, this approach is more reasonable, and certainly adequate (it is the way our higher level software 
does translation). The notion of copying the data rather than signaling the application program to recreate 
it is consistent with the layer model: the application program is freed from the responsibility of maintaining 
its layer during rearrangement of the layers on the screen. Scaling, however, does not fit as well into 
the software. Because there is no display list, the data in the different-sized layer cannot be recreated 
perfectly by the lower level software--at best, an intersection of the source and destination layers 
can be maintained. We have no solution to this problem, but do not regard it as critical; the importance 
of the extended bitmap idea is powerful enough for us to prefer to give up low-level support for scaling 
rather than to revert display lists. Also; it is unclear whether a layer should be regarded as a window 
or as a viewport. Does a different sized layer show the same thing at a different scale or a different-sized 
portion of the same thing? For example, if a large layer contains a picture composed of incompressible 
bitmaps, such as a page of characters in a text editor, what would happen to the image if the layer were 
shrunk to half its size? In particular, should the same configuration of smaller characters be drawn, 
or fewer characters of the same size? Either option might be correct under some conditions, and rather 
than make a decision we chose to ignore the issue. Any stacking transformation can be defined as a sequential 
set of one-layer rearrangement operations, moving a single layer to another position (e.g., the front 
or back of the stack of layers). For example, the stack can be inverted by an action similar to counting 
through a deck of cards, upfront( ) is an operator that moves a layer to the front of the stack, making 
it completely visible. It is the only stacking operator in the layer software, because in the one place 
(deleting a layer) where a different operation is required, the desired effect can be achieved with acceptable 
efficiency by calls to upfront(). The action of pulling a layer to the front was chosen because it is 
the most natural. When something interesting happens in a partially obscured layer, the instinctive reaction 
is to pull the layer to the front where it can be studied, upfront( ) also turns out be a useful operation 
during the creation and deletion of layers. upfront( ) has a simple structure. Most of the code is concerned 
with maintaining the linked lists. The basic algorithm exchanges the obscured rectangles in the layer 
with those of the layer obscuring them, swapping the contents of the obscured bitmap for those on the 
screen (Figure 5). Since the obscured rectangle has the same dimensions before and after the swap, the 
exchange can be done in place, and it is not necessary to allocate a new bitmap; we need merely link 
it into the new obscured layer. Obscured rectangles are marked with the frontmost obscuring layer for 
upfront( )'s benefit: the frontmost layer is the layer that occupies the portion of the screen the rectangle 
would occupy were it at the front. /* * upfront --pull layer to the front of the screen */ upfront(Ip) 
Layer *lp; Reprinted From acm Transactions On Graphics--April 1983--Vol. 2, No. 2 Layer A Layer A Layer 
B -Layer B Before ~-~ After Fig. 5. Moving a layer to the front requires interchanging its off-screen 
bitmaps with screen rectangles. Layer *fr; /* a layer in front of Ip */ Layer *beh; /* a layer behind 
Ip */ Obscured *op; for(fr = each layer in front of Ip){ for(op = each obscured portion of Ip){ if(op->lobs 
= = fr){ /* fr obscures op */ screenswap(op-> bitmap, op-> rect); unlink op from Ip; link op into fr; 
} } } move Ip to front of layer list; for(beh = all other Layers from back to front) for(o.p = each 
obscured portion of beh) if(Ip-> rect overlaps op-> bmap-> rect) op->lobs --Ip; /* mark op obscured by 
Ip */ screenswap() interchanges the data in the bitmap with the contents of the rectangle on the screen, 
in place. It is easily implemented, without auxiliary storage, using three calls to bitblt( ) with function 
code OR. Note that because of the fragmentation of the obscured portions done when a new Layer is created, 
if Reprinted From acm Transactions On Graphics--April 1983--Vol. 2, No. 2 Ip- > rect and op- > bmap- 
> rect intersect, the Layer must completely obscure it. Note also that it is upfront() that enforces 
the rule that the frontmost Layer obscuring a portion of a second Layer is the Layer marked as obscuring 
it. Only if these two Layers are interchanged is the screen updated. The last loop is required; it is 
not sufficient to mark obscured rectangles only in the main loop, because rectangles initially behind 
Ip may also need to be marked. 8. DELLAYER It is simpler to delete a Layer than to create one, so we 
discuss deletion first. The algorithm is easy: (1) Pull the layer to the front. It now has no obscured 
pieces, and is a contiguous rectangle on the screen. (2) Color the screen rectangle the background color. 
 (3) Push the layer to the back. All storage needed for the obscured portions of the layer is now bound 
to the layer, since it obscures no other layer. (4) Free all storage associated with the layer. (5) 
Unlink the layer from the layer list.  A special routine, the opposite of upfront(), could be written 
to push the layer to the back, but upfront( ) can be used for the task: /* * dellayer -- delete a layer 
*/ dellayer(Ip) Layer *lp; { Obscured *op; upfront(Ip); background(Ip-> rect); /* Push to back using 
upfront */ while(Ip not rearmost layer) upfront(rearmost layer); /* Free the storage */ for(op --each 
obscured part of Ip){ bfree(op-> bmap); free(op); } unlink Ip from Layer list; } Using successive calls 
to upfront( ) to push a layer to the back 81ves the screen a curious appearance during the deletion, 
dellayer( ) is so simple, though, that it seems unnecessary to give it extra support by providing a more 
efficient "downback()" routine, upfront() does not join disconnected obscured bitmaps Reprinted From 
acm Transactions On Graphics--April 1983--Vol. 2, No. 2 that could be joined because of the deletion, 
although it could. The difficulty of recognizing which bitmaps can be joined is too great to justify 
the usually small storage compaction and execution efficiency that would be gained thereby. Joining would 
be facilitated if the obscured lists of the layers contained infor- mation about which layer caused which 
edge to be cut during creation (e.g., see Rectangle 2 in Figure 3). In retrospect, it would probably 
have been better to use some form of tree structure for the obscured lists, with an indication at each 
node of the layer "responsible" for creating the obscured element or dividing the underlying structure 
into two or more elements. 9. NEWLAYER Making a new layer is the most difficult layer operation, for 
it may require modifying obscured lists of other layers. If the new layer creates any new overlaps, the 
obscured list of the overlapped layer must be restructured so that upiront( ) need not subdivide any 
rectangles to pull the obscured layer to the front. The creation routine, newlayer(), pays the price 
for the simplicity of upfront() and layerop(). The basic structure of newlayer( ) is to build the layer 
at the back, constructing the obscured list by intersecting the layer's rectangle with the obscured rectangles 
and visible portions of the current layers. After allocating storage for the obscured bitmaps, the layer 
is pulled to the fr(~nt, making it completely visible on the screen and forcing the rectangles obscured 
by the new layer to occupy the new storage required by the addition of the new layer. Finally, the screen 
rectangle occupied by the new layer is cleared to complete the operation. Several ancillary routines 
are used by newlayer(), addrect( ) adds rectangles to the obscured list, obs, of the new layer. Since 
the new layer is built at the "back" of the screen, any obscured rectangle of the new layer will be obscured 
by a layer already on the screen, addrect( ) builds the list of unique obscured rectangles, marked by 
whichever layer is currently occupying the screen in each rectangle. To be sure that a rectangle is unique, 
it is sufficient to check only the origin point of that rectangle. The rectangles passed to addrect( 
) are ordered so that the first layer associated with a particular rectangle occupies the screen in that 
rectangle. /* * addrect --add (unique) rectangle to * obscured list of new layer */ Obscured *obs;/* 
Pointer to obscured list for new layer */ addrect(r, Ip) Rectangle r; Layer *lp; /* Layer currently occupying 
r on screen */ { Obscured *op, *newop; for(op = each element of obs) if(op->rect.origin == r.origin) 
return; /* Not unique */ Reprinted From acm Transactions On Graphics--April 1983--Vol. 2, No, 2 newop 
= new Obscured; newop->rect = r; newop->lobs = Ip; link newop into obs list; Because it is called once 
for each rectangle, addrect( ) takes time proportional to the square of the number of obscured rectangles 
in the new layer. But since newlayer( ) will ultimately perform a series of expensive bitblt( )s, the 
time taken by the list operations is insignificant. addobs( ) does recursive subdivision of the obscured 
rectangles that intersect the new layer, calling addrect() when an overlap is established. It is similar 
to layerop() except that it does not chain along the obscured list, and no special action (i.e., storage 
allocation) is required if the rectangles match exactly. As subdivided pieces are added to the obscured 
list of a current layer, the original rectangle must remain in the list until all the subdivided pieces 
are also in the list, whereupon it is deleted. New pieces must therefore be added after the original 
piece. When the topmost call to addobs( ) returns, the subdivision (if any) is complete, and the return 
value is whether the argument rectangle was subdi- vided, newlayer( ) then removes the original rectangle 
from the list if addobs( ) returns TRUE. * addobs --add obscured rectangle to list, subdividing obscured 
 * portions of layers as necessary  */ boolean addobs(op, argr, newr, Ip) Obscured *op; Rectangle argr; 
/* Obscured rectangle */ Rectangle newr; /* Complete rectangle of new layer */ { Layer *lp; /* Layer 
op belongs to */ Obscured *newop; Rectangle r; Bitmap *bp; r = argr; /* argr will be unchanged through 
addobs0 */ if(rectXrect(r, newr)){ /* This is much like layeroP0 */ if(r.origin.x < newr.origin.x){ Rectangle 
temp = piece of r left of newr; addobs(op, temp, newr, Ip); r.origin.x = newr.origin.x; } /* etc. for 
other three sides */ /* r is now contained in rectangle of new layer */ Reprinted From acre Transactions 
On Graphics--April 1983--Vo1.2, No. 2 if(r = = argO{ /* no clip, just bookkeeping */ addrect(r, Ip); 
return FALSE; /* No subdivision */ } addrect(r, Ip); } bp = balloc(r); newop = new Obscured; /* Copy 
the subdivided portion of the image */ bitblt(op->bmap, r, bp, bp->rect.origin, STORE); newop->bmap 
= bp; newop->rect--r; newop->lobs = Ip; /* Layer Ip obscures this part of the new layer */ link op into 
Ip->obs; return TRUE; /* Subdivision */ } newlayer( ) is long but straightforward: Obscured obs; /* 
obscured list of new layer when at back */ /* * newlayer --make a new layer in rectangle r of bitmap 
*bp */ Layer * newlayer(bp, r) Bitmap *bp; Rectangle r; { Layer *lp, *newlp; Obscured *op; /* First 
build, in obs, a list of all obscured rectangles that * will be obscured by the new layer, doing subdivision 
with * addobsO  */ obs = NULL; for(Ip = each layer from front to back){ for(op = each obscured portion 
of Ip){ if(rectXrect(r, op->rect) &#38;&#38; addobs(op, op->rect, r, Ip)){ unlink op from Ip->obs; bfree(op->bmap); 
free(op); } } } /* NOW add the rectangles not currently obscured, but that will * be obscured by new 
layer, by building layer &#38; calling layeroPO */ Reprinted From acm Transactions On Graphics--April 
1983--Vol. 2, No. 2 newlp = new Layer; Bitmap part of newlp = *bp; newlp->obs = obs; /* currently obscured 
... */ for(Ip = each layer from front to back) layerop(Ip, addpiece, Ip->rect); newlp->obs = obs; /* 
... and soon to be */ for(op = each element of obs) op->bmap = balloc(op->rect); link newlp into back 
of layer list; upfront(newlp); rectf(newlp->rect, CLR); /* Clear the screen rectangle */ return newlp; 
newlayer() takes an argument Bitmap, which is typically the screen Bitmap display, but may be any other. 
It is a simple {untried) generalization from Layers within Bitmaps to Layers within Layers, and a true 
hierarchy. addpiece() is a trivial routine to add to the obscured list the rectangles that are currently 
unobscured (i.e. that have only one layer), but that will be obscured by the new layer. addpiece(Ip, 
r, bp, otherargs, op) Layer *lp; Rectangle r; Bitmap *bp; char *otherargs;/* Unused */ Obscured *op; 
 if(op = = NULL) /* This piece occupied by one layer only */ addrect(r, Ip); /* Otherwise it's already 
in obs list */ 10. SCROLLING AND THE GENERAL BITBLT OPERATOR So far, we have only discussed support for 
graphics, not graphical actions themselves. This section, on scrolling, and the next, on line drawing, 
illustrate how difficult graphics can be on a bitmap display, and how layerop( ) can help. For simplicity 
of exposition, this section uses as an example the case of scrolling a layer; although the subdivision 
of rectangles can get complicated in sophisti- cated examples, the issues are all addressed by a simple 
case. Since the general case is not significantly harder to implement, this section presents a general 
Ibitblt( ) operator that extends the bitmap-level bitblt( ) functionality to layers. Scrolling a layer--moving 
a layer full of text up one line--is a special case of a bitblt operation. Scrolling a screen rectangle 
r is a single bitblt: #define NLSZ 16/* height of a text line */ bitblt(display, Rect(r.origin, Pt(r.corner.x, 
r.corner.y-NLSZ)), display, Pt(r.origin.x, r.origin.y+ NLSZ), STORE); Reprinted From acm Transactions 
On Graphics--April 1983--Vol. 2, No. 2 r . . . . . . . . I I B I ~~-~ I b I , , ' ~ ----~d .... 5 __C 
D Fig. 6. To scroll the rear layer, the source rectangles B through D are copied to the destination rectangles 
b through d. This bitblt copies all but the first line of text up one line. Rect( ) and Pt( ) create 
Rectangles and Points from their arguments. Scrolling a layer requires a series of bitblts, which must 
also be done in a certain order. Figure 6 illustrates a simple sample of scrolling a partially obscured 
layer. The scroll is broken into three bitblts, so that the source and destination rectangles each lie 
within a single bitmap. The routiae Ibitblt() has semantics identical to bitblt(), but accepts Layer 
arguments as well as Bitmaps, does the necessary subdivision of obscured rectan- gles, and executes the 
atomic bitblt() calls in the correct order. There are two basic methods for organizing Ibitblt(). The 
first method calls layerop() to create a list of source rectangles by dividing the original rectangle 
((B + C + D) in Figure 6, with (A + B) off-screen and (C + D) visible) into a set of rectangles contained 
within single bitmaps, layerop( ) is then called for each source rectangle to subdivide it so the destination 
rectangles will also be contained in single bitmaps (separating C and D in Figure 6 because c is in a 
different bitmap from d). This has the disadvantage that it calls layerop( ) once for each source rectangle, 
even though simpler, faster code can accomplish the task. The second method, and the one we use, calls 
layerop( ) twice: once to subdivide the source rectangles and once to subdivide the destination rectangles. 
The loop to intersect the source and destination rectangles is then trivial. Ibitblt() uses the defined 
type ListElement to couple the bitmaps to the rectangles: typedef struct{ Rectangle rect; Bitmap *bp; 
} ListElement; The type List chains together ListElements. Two calls to layerop(), one in the coordinate 
system of the source Layer and one in the coordinates of the desti- nation, generate the lists of rectangles 
in sorted order. A simple loop then executes Reprinted From acre Transactions On Graphics--April 1983--Vol. 
2, No. 2 the component bitblt( ) calls: Point delta; /* difference between source and dest. origin */ 
List SrcList; /* list of source rect/bitmap pairs */ List DestList; /* list of destination rect/bitmap 
pairs */ Ibitblt(sl, rect, dl, pt, f) Layer *sl, *dl; Rectangle rect; Point pt; Code f; ListElement 
*s, *d; Rectangle r; delta --pt -rect.origin; layerop(sl, Pass, rect, SrcList); layerop(dl, Pass, raddp(rect, 
delta), DestList); for(s = each element of SrcList) for(d = each element of DestList) { r = intersection 
of d->rect and raddp(s->rect, delta); if(r not null) bitblt(s->bp, rsubp(r, delta), d->bp, r.origin, 
f); raddp( ) adds its second argument, a Point, to the origin and corner of its first argument, a Rectangle; 
rsubp( ) is the corresponding subtraction operator. Pass( ) is a trivial routine for creating the lists. 
The rectangles in the lists are sorted according to an ordering operator lessthan(). Pass(Ip, r, sb, 
ap, op) /* the usual declarations */ struct { List I; } *fp; ListElement *e; for(e = each element of 
fp->l) if(not lessthan(r, e->rect)) { insert the ListElement {sb, r} into fp->l before e; return; } append 
{sb, r} to fp->l Reprinted From acm Transactions On Graphics--April 1983--Vol. 2, No. 2 boolean lessthan(a, 
b) Rectangle a, b; { if((a->origin.y < b->corner.y) &#38;&#38; (b->origin.y < a->corner.y)) return ((a->origin.x 
-b->origin.x)*delta.x) > = 0; else return ((a->origin.y -b->origin.y)*delta.y) > = 0; } The first test 
sees if the two rectangles overlap in y, and if so checks the x ordering; otherwise the y ordering is 
sufficient. The ordering of the lists simplifies doing the bitblts in the correct sequence. For the scrolling 
example, delta.y is positive, and ]essthan( ) orders the rectangles so they are copied in decreasing 
y order (see Figure 6). There are a few efficiency issues. If the source and destination bitmaps are 
distinct in the uppermost call, (e.g., in drawing a character in a layer), the rectangles need not be 
sorted, but Ibitbit( ) sorts them anyway. Also, if one or both of the layers has an empty obscured list, 
I bitblt( ) could recognize these degenerate cases and call bitblt( ) directly if neither layer has obscured 
components, or Iblt( ) if only one does. More seriously, the implementation of Ibitblt( ) presented here 
contains some code with quadratic complexity--specifically, the inner loops of Pass() and bitblt(). (The 
first method described above only executes n times, but calls layerop( ) to (recursively) run down the 
destination lists, so the complexity is the same.) In our application the obscured lists are typically 
short, so this complexity is dwarfed by the expense of the underlying bitblt( ) calls. If this were not 
true, it would be straightforward to rewrite both Pass( ) and Ibitblt( ) to use n log n search algorithms. 
The structure of the code would be the same, however, and the use of layerop( ) to organize the clipping 
would be unaffected. 1 1. LINES Drawing lines on a dot-matrix display is an old problem (see, e.g., [6]). 
Drawing lines in layers is more difficult because the sequence of dots approximating a line must be independent 
of the structure of the layer. To draw a line efficiently in a partially obscured layer, the line-drawing 
algorithm must draw the line as a set of line segments in bitmaps, but the endpoints of the segments 
(which must be in integer coordinates) might not be points on the actual line being approximated. A line 
from (0, 0) to (300, 100) is not approximated by the same points as two lines, one from (0, 0) to (100, 
33) and one from (100, 33) to (300, 100). Of course, if efficiency is not an issue, lines may be drawn 
by generating the set of dots for the approximation and deciding for each dot in which bitmap it will 
be drawn. Here we assume that a line segment may be drawn efficiently in a bitmap (which is true on any 
reasonable display), and that the problem is one of dividing up the original line. Most line-drawing 
algorithms simulate a digital differential analyzer (DDA) to reduce the strength of the rational arithmetic 
in ~y Y=-~'-~x X + yo Reprinted From acm Transactions On Graphics--April 1983--Vol. 2, No. 2 to increments 
and decrements. Different DDAs approximate a given line by different sets of points. Bresenham's algorithm 
[l] has no multiplications or divisions, and uses only integer arithmetic: int e, hx, ~y; e = 2Ay-hx 
; for(i= 1; i<hx; i++){ drawpoint(x, y); if(e>O){ y++; e + = 2Ay-2hx; }else e+ =2hy; x++; } (As mentioned 
earlier, this algorithm is usually implemented efficiently; draw- point( ) is not a subroutine call, 
and the constants come out of the loop.) Here, e is an offset and scaled error term that represents the 
vertical distance between the actual line and the approximating points. Each time the algorithm draws 
a point, e is adjusted to track the local deviation from the actual line. When e is positive, the actual 
line is too far above the dots, and the dots are therefore moved up one unit in y. The line algorithm 
can be restarted as it passes from obscured bitmap to bitmap in the layer by carrying the value of e 
across the boundary. But since layerop( ) calls the bitmap-level operators in a nondeterministic order, 
a simpler solution is to evaluate e at the (x, y) location where the line intersects the bit- map, and 
reset the line algorithm in each component bitmap. We are assuming 0 < Ay/Ax _< 1, a shallow line of 
positive slope. If we further assume that the real line passes through (0, 0), we can analytically specify 
points generated by Bresenham's algorithm. The equation of the actual line is y&#38;x -x&#38;y = O, so 
at a generated point yhx-xAy = r, where r is the residual, that is, the deviation from the actual line, 
measured parallel to the y axis. For computational efficiency, e is offset and scaled from r: e = -2r 
+ 2by -ax. (1) To minimize r, we round to calculate y: [ x2Ay + Ax J Y = 2Ax ' (2) implying, as above, 
r = yhx -xAy. Substituting, e = (2x + 2)Ay - (2y + 1}Ax. (3) Reprinted From acm Transactions On Graphics--April 
1983--Vol. 2, No. 2 Note that if (x, y) = (0, 0), e = 2by -Ax, as in the original algorithm. Equa- tion 
(3) can also be derived directly from the algorithm (and vice versa). Given eqs. (2) and (3), we can 
break a line into a set of segments in bitmaps indepen- dently of the layer's configuration by resetting 
the DDA for each segment. This method generates a unique approximation because the state of the DDA is 
a strict function of x. One problem remains. The clipping routine (see [6]) that divides the line into 
segments individually within bitmaps may clip to a horizontal or a vertical edge of a bitmap. If the 
edge is horizontal, several possible x values may be chosen (see Figure 7). Because lines are half-open, 
the desired value is the least x such that (x, y) is a point on the approximated line, as defined by 
eq. (1). It can be shown that the desired value is I 2yAx -hx x = -(4) Turning these equations into working 
code involves a few obvious transfor- mations to cover all possible cases. The transformations must be 
done carefully, to preserve the half-open property of lines. Given a routine clipline(), which clips 
a line segment to a bitmap and draws it, using a line drawer which loads the DDA on the basis of the 
initial x value, layerop( ) can control drawing a line in a layer: kline(Ip, r, db, fp, op) Layer *lp; 
Rectangle r; Bitmap *db; struct{ Point pO, pl ; /* Endpoints of complete line */ int mode; /* Function 
code */ }*fp; Obscured *op; { clipline(db, r, fp->pO, fp->pl, fp->mode); } Iline(Ip, P, q, f) Layer 
*lp; Point p, q; int f; setline(p, q); /* Initialize global parameters for line() */ layerop(Ip, Lline, 
Ip->rect, p, q, f); } setline( ) initializes the global variables for the line, in particular Ax and 
Ay, and canonicalizes the line by coordinate transformations so that 0 < Ay/hx _< 1. This line algorithm 
has advantages over an unadorned DDA, even for nonoverlapping bitmaps: the sequence of dots generated 
is independent of the direction in which the line is drawn, and portions of a line may be "undrawn." 
Both of these Reprinted From acm Transactions On Graphics--April 1983--V01.2, No. 2 Outside Layer ~pprox. 
points Actual line desired point . ~ ~ ~ Edge of Layer ~ Inside Layer Fig. 7. Lines must be clipped to 
the first point outside a layer or bitmap. advantages stem from the DDA being a strict function of x, 
instead of the distance from the starting point of the line. The point of this section is not that lines 
can be drawn in layers, but how layerop() helps draw them. Given an object to be drawn--a line, a circle, 
an ellipse, a polygon, a spline, etc.--and an operator that can draw the object in a bitmap, layerop() 
will invoke the operator with the clipped rectangle/bitmap pairs that draw the complete object in a layer. 
The implementation of the bitmap operators can be tuned for performance. In our application, we build 
lines out of line segments with a restartable DDA for maximum speed, but draw arcs and circles as sequences 
of points because we do not draw them nearly as often. If circles were important to an application, the 
same sort of DDA restarting could be applied, using layerop( ) to define the clipping rectangles for 
each component of the circle. 1 2. CONCLUSIONS The software described here is in day-to-day use on hundreds 
of "Blit" terminals throughout Bell Labs, providing an asynchronous window interface to the mul- tiprogrammed 
Unix 3 system. Unlike previous window systems, the layers software does not keep display lists [5], force 
"repainting" during window operations [7], or restrict graphical operations in any way. Instead, it supports 
general bitblt- style raster graphics by extending the notion of a bitmap to encompass rectangular regions 
that are subdivided into visible and invisible portions, and provides an elegant interface between the 
simple, traditional bitmap operations and the more general ones. The role of the traditional "window 
manager" is reduced to providing a user interface. In [5] appears the sentence, "Research needs to be 
done to develop a way in which to conveniently store and manipulate graphics data in the context of a 
window manager." layerop( ) and its associated primitives meet this challenge. 3Unix is a trademark of 
Bell Laboratories. Reprinted From acre Transactions On Graphics--April 1983--Vol. 2, No. 2 ACKNOWLEDGMENTS 
Many people helped develop the algorithms discussed herein. The data structures grew out of discussions 
with Ken Thompson; Doug McIlroy guided me through the mazes of integer geometry; Peter Weinberger got 
my line-drawing theorems straight; Andrew Koenig simplified the Bitmap data structure; Andrew Hume spent 
a difficult week getting the details of ibitblt( ) right; John Reiser wrote an astonishing and indispensable 
implementation of bitbit( ); and Brian Kernighan, Dave MacQueen, and Chris van Wyk made helpful constructive 
criticism on an early manuscript, which led me to rethink and rewrite large portions of the software 
as well as of the paper. I thank them all. I am most deeply indebted, however, to Bart Locanthi, who 
wrote the first bitblt(), greatly improved the interface to layerop(), and, most important, designed 
and built a bitmap terminal and placed it on my desk, thereby challenging me to tame the bitmap world. 
REFERENCES I. BRESENHAM, J. E. Algorithm for computer control of a digital plotter. IBM Syst. J. 4, 1 
(1965), 25-30. 2. GUIBAS, L. J., AND STOLFI, J. A language for bitmap manipulation. ACM Trans. Gr. 1, 
3 (July 1982), 191-214. 3. INGALLS, D. H. H. The Smalltalk graphics kernel. Byte 6, 8 (Aug. 1981), 168-194. 
 4. LANTZ, K. A. AND RASHID, R. F. Virtual terminal management in a multiple process environment. In 
Proc. 7th Syrup. Operating Systems Principles (Pacific Grove, Calif., Dec. 10-12), ACM, New York, 1979, 
pp. 86-97. 5. MEYROWITZ, N., AND MOSER, M. BRUWIN: An adaptable design strategy for window manager/ 
virtual terminal systems. In Proc. 8th Syrup. Operating Systems Principles 15, 5 (Pacific Grove, Calif., 
Dec. 14-16), ACM, New York, 1981, pp. 180-189. 6. NEWMAN, W. M., AND SPROULL, R. F. Principles of Interactive 
Computer Graphics, 2nd ed. McGraw-Hill, New York, 1979 7. TESLER, L. The Smalltalk environment. Byte 
6, 8 (Aug. 1981), 90-147.  8. WEINREB,D., AND MOON, D. Introduction to using the window system. Symbolics, 
Inc., 1981. Received February 1983; revised April 1983; accepted April 1983 Reprinted From acm Transactions 
On Graphics--April 1983--Vol. 2, No. 2  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801166</article_id>
		<sort_key>357</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>41</seq_no>
		<title><![CDATA[Solid modeling (Panel Session)]]></title>
		<subtitle><![CDATA[A user perspective]]></subtitle>
		<page_from>357</page_from>
		<doi_number>10.1145/800059.801166</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801166</url>
		<abstract>
			<par><![CDATA[<p>In the past few years there has been a lot of interest and excitement generated by solid modeling. Numerous papers have been presented on the value and benefits of this technology. Many talented individuals have contributed to the state of the art. Solid modeling has played a larger role at SIGGRAPH in the past three years, with tutorials, panel discussions and technical papers. All this material has been presented for the most part by academics, researchers and developers of solid modeling systems.</p> <p>The users of solid modeling systems have yet to be heard from. The reason is that there have been very few of them out there in the past. However, that situation is slowly changing. Now that several CAD/CAM vendors are marketing solid modeling systems, a small cadre of users does exist.</p> <p>This panel will be composed primarily of users of solid modeling systems. They will discuss topics of interest to potential users (and also developers) of solid modelers. Questions to be addressed include:</p> <p>What is the status of solid modeling systems from a user standpoint?</p> <p>Can the present solid modelers be used in a production environment today?</p> <p>Should a solid modeling system replace the present wire frame and surface systems or be integrated with them?</p> <p>What are the problems associated with using a solid modeling system?</p> <p>How good is the user interface?</p> <p>Does the necessary application software exist?</p> <p>The primary intent of this Panel discussion is to give the potential user some idea as to whether the available solid modeling systems are useful for his applications.</p> <p>It is beginning to look like the next generation CAD/CAM system for mechanical design will be a solid modeler. The big question is &#8220;when will this occur?&#8221;</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P85718</person_id>
				<author_profile_id><![CDATA[81100405005]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Frank]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Bliss]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ford Motor Company]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40027810</person_id>
				<author_profile_id><![CDATA[81100488149]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Carl]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Machover]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Machover Associates]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14030007</person_id>
				<author_profile_id><![CDATA[81544264056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Carl]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vogel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[General Motors Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P329939</person_id>
				<author_profile_id><![CDATA[81100315316]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Donald]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Witte]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bendix Kansas City]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PANEL SOLID MODELING: A USER PERSPECTIVE CHAIR: Frank W. Bliss, Ford Motor Company PANELISTS: Carl 
Machover, Machover Associates Carl Vogel, General Motors Corporation Donald R. Witte, Bendix Kansas City 
 CHAIRMAN'S INTRODUCTION: In the past few years there has been a lot of interest and excitement generated 
by solid modeling. Numerous papers have been presented on the value and benefits of this technology. 
Many talented individu- als have contributed to the state of the art. Solid modeling has played a larger 
role at SIGGRAPH in the past three years, with tutorials, panel discussions and technical papers. All 
this material has been presented for the most part by academics, researchers and developers of solid 
modeling systems. The users of solid modeling systems have yet to be heard from. The reason is that 
there have been very few of them out there in the past. However, that situation is slowly changing. Now 
that several CAD/CAM vendors are marketing solid modeling sys- tems, a small cadre of users does exist. 
 This panel will be composed primarily of users of solid modeling systems. They will discuss top~s of 
interest to poten- tial users (and also developers) of solid modelers. Questions to be addressed include: 
 What is the status of solid modeling systems from a user standpoint? Can the present solid modelers 
be used in a production environment today? Should a solid modeling system replace the present wire frame 
and surface systems or be integrated with them? What are the problems associated with using a solid 
modeling system? How good is the user interface? Does the necessary application software exist? The 
primary intent of this Panel discussion is to give the potential user some idea as to whether the available 
 solid modeling systems are useful for his app i ic at ions. It is beginning to look like the next 
gen- eration CAD/CAM system for mechanical design will be a solid modeler. The big question is "when 
will this occur?" Frank W. Bliss of Ford Motor Company is in charge of the Sol~d Modeling Project at 
the Scientific Research Lab. Work has been done in the area of Mass Properties and the generation of 
very realistic color shaded pictures of automotive components. In addition, several commercial solid 
modeling systems have been evaluated. The results of this effort will be discussed. Carl Machover, President 
of Machover Asso- ciates, is a well known CAD/CAM industry consultant. He has over 25 years experi- ence 
in graphics and has consulted with a number of companies concerning solid modeling. He is particularly 
interested in the problems associated with using solid modeling systems. Carl Vogel is a senior designer 
at the Chevrolet Division of General Motors. GM is using its in-house developed solid modeling system 
GMSOLID. Vogel will dis- cuss the use of this system in the Power- train and Chassis component design 
areas. Donald R. Witte is a Staff Engineer at Bendix Kansas City Division. He is using Control Data's 
Synthavision system to gen- erate color shaded pictures of castings. Bendix has effected cost savings 
on supplier-built prototype parts by includ- ing color pictures with the detail drawing of parts. Witte 
will discuss this work. 357
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801167</article_id>
		<sort_key>359</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>42</seq_no>
		<title><![CDATA[Particle systems&#8212;a technique for modeling a class of fuzzy objects]]></title>
		<page_from>359</page_from>
		<page_to>375</page_to>
		<doi_number>10.1145/800059.801167</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801167</url>
		<keywords>
			<kw><![CDATA[Dynamic objects]]></kw>
			<kw><![CDATA[Motion blur]]></kw>
			<kw><![CDATA[Stochastic modeling]]></kw>
			<kw><![CDATA[Temporal aliasing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.5.1</cat_node>
				<descriptor>Fuzzy set</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.3</cat_node>
				<descriptor>Stochastic processes</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003648.10003700</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Stochastic processes</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P299831</person_id>
				<author_profile_id><![CDATA[81100228626]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Reeves]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lucasfilm Ltd]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ABRAHAM, R., AND SHAW, C. DYNAMICS&#8212; The Geometry of Behavior. City on the Hill Press, Santa Cruz, Calif, 1981.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BADLER, N. I., O'ROURKE, J., AND TOLTZIS, H. A spherical human body model for visualizing movement. Proc. IEEE 67, 10 (Oct. 1979).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801255</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BLINN, J. F. Light reflection functions for simulation of clouds and dusty surfaces. Proc. SIGGRAPH '82. In Comput. Gr. 16, 3, (July 1982), 21-29.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807458</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[CSURI, C., HACKATHORN, R., PARENT, R., CARLSON, W., AND HOWARD, M. Towards an inter-active high visual complexity animation system. Proc. SIGGRAPH 79. In Comput. Gr. 13, 2 (Aug. 1979), 289-299.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358553</ref_obj_id>
				<ref_obj_pid>358523</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[FOURNIER, A., FUSSEL, D., AND CARPENTER, L. Computer rendering of stochastic models. Commun. ACM 25, 6, (June 1982), 371-384.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Particle Systems__ Technique for Modeling a Class of Fuzzy Objects WILLIAM T. REEVES Lucasfilm Ltd 
This paper introduces particle systems-a method for modeling fuzzy objects such as fire, clouds, and 
water. Particle systems model an object as a cloud of primitive particles that define its volume. Over 
a period of time, particles are generated into the system, move and change form within the system, and 
die from the system. The resulting model is able to represent motion, changes of form, and dynamics that 
are not possible with classical surface-based representations. The particles can easily be motion blurred, 
and therefore do not exhibit temporal aliasing or strobing. Stochastic processes are used to generate 
and control the many particles within a particle system. The application of particle systems to the wall 
of fire element from the Genesis Demo sequence of the film Star Trek II: The Wrath of Khan [10] is presented. 
Categories and Subject Descriptors: I.3.3 [Computer Graphics]: Picture/Image Generation; I.3.5 [Computer 
Graphics]: Computational Geometry and Object Modeling; 1.3.7 [Computer Graphics]: Three-Dimensional 
Graphics and Realism General Terms: Algorithms, Design Additional Key Words and Phrases: Motion blur,stochastic 
modeling, temporal aliasing, dynamic objects 1. INTRODUCTION Modeling phenomena such as clouds, smoke, 
water, and fire has proved difficult with the existing techniques of computer image synthesis. These 
"fuzzy" objects do not have smooth, well-defined, and shiny surfaces; instead their surfaces are irregular, 
complex, and ill defined. We are interested in their dynamic and fluid changes in shape and appearance. 
They are not rigid objects nor can their motions be described by the simple affine transformations that 
are common in computer graphics. This paper presents a method for the modeling of fuzzy objects that 
we call particle systems. The representation of particle systems differs in three basic ways from representations 
normally used in image synthesis. First, an object is represented not by a set of primitive surface elements, 
such as polygons or patches, that define its boundary, but as clouds of primitive particles that define 
its volume. Second, a particle system is not a static entity. Its particles change form and move with 
the passage of time. New particles are "born" and old Author's address: William T. Reeves, Lucasfilm 
Ltd, P.O. Box 2009, San Rafael, CA 94912. Permission to copy without fee all or part of this material 
is granted provided that the copies are not made or distributed for direct commercial advantage, the 
ACM copyright notice and the title of the publication and its date appear, and notice is given that copying 
is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. &#38;#169;ACM 0-89791-109-1/83/007/0359 $00.75 Reprinted From acm Transactions 
On Graphics-April 1983-Vol. 2,No. 2  William T. Reeves particles "die." Third, an object represented 
by a particle system is not deterministic, since its shape and form are not completely specified. Instead, 
stochastic processes are used to create and change an object's shape and appearance. In modeling fuzzy 
objects, the particle system approach has several important advantages over classical surface-oriented 
techniques. First, a particle (for now, think of a particle as a point in three-dimensional space) is 
a much simpler primitive than a polygon, the simplest of the surface representations. Therefore, in the 
same amount of computation time one can process more of the basic primitives and produce a more complex 
image. Because a particle is simple, it is also easy to motion-blur. Motion-blurring of fast-moving objects 
for the removal of temporal aliasing effects has been largely ignored in computer image synthesis to 
date. A second advantage is that the model definition is procedural and is controlled by random numbers. 
Therefore, obtaining a highly detailed model does not necessarily require a great deal of human design 
time as is often the case with existing surface-based systems. Because it is procedural, a particle system 
can adjust its level of detail to suit a specific set of viewing parameters. As with fractal surfaces 
[5], zooming in on a particle system can reveal more and more detail. Third, particle systems model objects 
that are "alive," that is, they change form over a period of time. It is difficult to represent complex 
dynamics of this form with surface-based modeling techniques. Modeling objects as collections of particles 
is not a new idea. Fifteen years ago, the earliest computer video games depicted exploding spaceships 
with many little glowing dots that filled the screen. Point sources have been used as a graphics data 
type in many three-dimensional modeling systems (e.g., the early Evans and Sutherland flight simulators), 
although there are few real references to them in the literature. Roger Wilson at Ohio State [4] used 
particles to model smoke emerging from a smokestack. There were neither stochastic controls nor dynamics 
in his model. Alvy Ray Smith and Jim Blinn used particles to model star creation and death in galaxies 
for the Cosmos series [11]. Alan Norton [9] used particles to generate and display three-dimensional 
fractal shapes. Jim Blinn [3] discussed light reflection functions for simulating light passing through 
and being reflected by layers of particles. His technique was used to produce images of the rings of 
Saturn. Blinn did not address the fuzzy object modeling problem which is the topic of this paper. Volumetric 
representations have also been proposed as viable alternatives to surface representations. Solid modeling 
[13] is a form of volumetric representation, as is the work of Norm Badler and Joe O'Rourke on "bubbleman" 
[2]. The use of stochastic modeling relates our work to the recent advances in fractal modeling [5]. 
Section 2 decribes the basic framework of particle systems in more detail. Section 3 examines how particle 
systems were used to produce the fire element in the Genesis Demo sequence from the movie Star Trek II: 
The Wrath of Khan [10]. Section 4 presents several other applications of particle systems, and Section 
5 discusses ongoing and future research in this area. 2. BASIC MODEL OF PARTICLE SYSTEMS A particle system 
is a collection of many minute particles that together represent a fuzzy object. Over a period of time, 
particles are generated into a system, move and change from within the system, and die from the system. 
Reprinted From acm Transactions On Graphics-April 1983-Vol. 2, No. 2  where MeanPartssa is the mean 
per screen area, VarPartssa,its variance, and ScreenArea the particle system's screen area. This method 
controls the level of detail of the particle system and, therefore, the time required to render its image. 
For example, there is no need to generate 100,000 particles in an object that covers 4 pixels on the 
screen. To enable a particle system to grow or shrink in intensity, the designer is able to vary over 
time the mean number of particles generated per frame (i.e., MeanPartsfand MeanPartssa,are, as used above, 
functions of frame number). Currently, we use a simple linear function MeanPartsf= InitialMeanParts+ 
DeltaMeanPartsx (f-fo) or InitialMeanPartssa+ - MeanPartssaf= DeltaMeanPartssax (f fo), where f is the 
current frame, fothe first frame during which the particle system is alive, InitialMeanPartsthe mean 
number of particles at this first frame, and DeltaMeanParts its rate of change. The variance controls, 
VarPartsfand VarPartssaf,are currently constant over all frames. More sophisticated quadratic, cubic, 
or perhaps even stochastic variations in both the mean and variance parameters would be easy to add. 
To control the particle generation of a particle system, therefore, the designer specifies fo and either 
the parameters InitialMeanParts,DeltaMeanParts,and VarParts, or the parameters InitialMeanPartsa,, DeltaMeanPartsa,, 
and VarPartssa 2.2 Particle Attributes For each new particle generated, the particle system must determine 
values for the following attributes: (1) initial position, (2) initial velocity (both speed and direction), 
 (3) initial size, (4) initial color, (5) initial transparency, (6) shape, (7) lifetime.  Several 
parameters of a particle system control the initial position of its particles. A particle system has 
a position in three-dimensional space that defines its origin. Two angles of rotation about a coordinate 
system through this origin give it an orientation. A particle system also has a generationshape which 
defines a region about its origin into which newly born particles are randomly placed. Among the generation 
shapes we have implemented are: a sphere of radius r, a circle of radius r in the x-y plane of its coordinate 
system, and a rectangle of length l and width w in the x-y plane of its coordinate system. Figure 1 shows 
a typical particle system with a spherical generation shape. More complicated generation shapes based 
on the laws of nature or on chaotic attractors [1] have been envisioned but not yet implemented. Reprinted 
From acm Transactions On Graphics-April 1983-Vol. 2, No. 2 Fig. 1. Typical particle system with spherical 
generation shape. The generation shape of a particle system also describes the initial direction in which 
new particles move. In a spherical generation shape, particles move outward away from the origin of the 
particle system. In a circular or rectangular shape, particles move upward from the x-y plane, but are 
allowed to vary from the vertical according to an "ejection" angle, which is another parameter (see Figure 
3). The initial speed of a particle is determined by InitialSpeed= MeanSpeed + Rand() x VarSpeed, where 
MeanSpeedand VarSpeed are two other parameters of the particle system, the mean speed and its variance. 
To determine a particle's initial color, a particle system is given an average color,' and the maximum 
deviation from that color. Particle transparency and particle size are also determined by mean values 
and maximum variations. The equations are similar to the one given above for initial speed. A particle 
system has a parameter that specifies the shape of each of the particles it generates. The particle shapes 
implemented so far are spherical, rectangular, and streaked spherical. The latter is used to motion-blur 
particlesa very important feature when modeling fast-moving objects. We discuss streaking particles 
in more detail in Sections 2.5 and 3. The number of possible attribute control parameters and their variants 
is endless. We are presenting those that we have found to be most useful and interesting. 2.3 Particle 
Dynamics Individual particles within a particle system move in three-dimensional space and also change 
over time in color, transparency, and size. 1In more detail, average red, green, and blue values are 
specified. Reprinted From acm Transactions On Graphics-April 1983-Vol. 2, No. 2 William T. Reeves To 
move a particle from one frame to the next is a simple matter of adding its velocity vector to its position 
vector. To add more complexity, a particle system also uses an acceleration factor to modify the velocity 
of its particles from frame to frame. With this parameter the model designer can simulate gravity and 
cause particles to move in parabolic arcs rather than in straight lines. A particle's color changes over 
time as prescribed by the rate-of-color-change parameter. The transparency and size of particles are 
controlled in exactly the same way. In our implementation, these rates of change are global for all particles 
in a particle system, but one can easily imagine making this parameter stochastic too. 2.4 Particle Extinction 
When it is generated, a particle is given a lifetime measured in frames. As each frame is computed, this 
lifetime is decremented. A particle is killed when its lifetime reaches zero. Other mechanisms, if enabled, 
arrange for particles to be killed because they can contribute nothing to the image. If the intensity 
of a particle, calculated from its color and transparency, drops below a specified threshold, the particle 
is killed. A particle that moves more than a given distance in a given direction from the origin of its 
parent particle system may also be killed. This mechanism can be used to clip away particles that stray 
outside a region of interest.2 2.5 Particle Rendering Once the position and appearance parameters of 
all particles have been calculated for a frame, the rendering algorithm makes a picture. The general 
particle rendering problem is as complicated as the rendering of objects composed of the more common 
graphical primitives, such as polygons and curved surfaces. Parti cles can obscure other particles that 
are behind them in screen depth. They can be transparent and can cast shadows on other particles. Furthermore, 
particles can coexist in a scene with objects modeled by surface-based primitives, and these objects 
can intersect with the particles. In our existing system, two assumptions allow us to simplify the rendering 
algorithm. First, we assume that particle systems do not intersect with other surface-based modeling 
primitives, and hence our rendering algorithm need only handle particles. Objects modeled using other 
techniques are composited together with particle system objects in a postrendering compositing stage. 
In order for a particle system to intersect or be behind other objects, the rendering system will split 
the image of a particle system into subimages based on clipping planes defined in the model coordinate 
space. These subimages are then combined with other images in the compositing stage. The other simplifying 
assumption made in our current rendering system is that each particle can be displayed as a point light 
source.3 With this assumption, 2 Note that this clipping is performed in modeling space-to a given plane 
for example. Clipping to the viewing frustum occurs later in the rendering stage and is discussed below. 
3 Explosions and fire, the two fuzzy objects we have worked with the most, are modeled well with this 
assumption. Other fuzzy objects, such as clouds and water, are not. Section 5 discusses rendering algorithms 
for these objects. Reprinted From acm Transactions On Graphics-April 1983-Vol. 2, No. 2 Particle Systems 
determining hidden surfaces is no longer a problem. Each particle adds a bit of light to the pixels that 
it covers. A particle behind another particle is not obscured but rather adds more light to the pixels 
covered. The amount of light added, and its color depend on the particle's transparency and color. Currently, 
the amount of light added does not depend on the distance between the particle and the viewing position, 
but that would be an easy extension. The viewing transforma tion, the particle's size, and its shape 
determine which pixels are covered. All particle shapes are drawn antialiased in order to prevent temporal 
aliasing and strobing. Light may be added to a pixel by many particles, so the rendering algorithm clamps 
the individual red, green, and blue intensities at the maximum intensity value of the frame buffer instead 
of letting them wrap around. With this algorithm and assumptions, no sorting of the particles is needed. 
They are rendered into the frame buffer in whatever order they are generated. Shadows are no longer a 
problem, since particles do not reflect but emit light. 2.6 Particle Hierarchy Our system has a mechanism 
that supports the formation and control of particle system hierarchies. The model designer creates a 
particle system in which the particles are themselves particle systems. When the parent particle system 
is transformed, so are all of its descendant particle systems and their particles. The parent particle 
system's mean color and its variance are used to select the mean color and variance of the offspring 
particle systems using the same equations presented earlier. The number of new particle systems generated 
at a frame is based on the parent's particle generation rate. The other parameters of the parent similarly 
affect those of its children. The data structure used to represent the hierarchy is a tree. A hierarchy 
can be used to exert global control on a complicated fuzzy object that is composed of many particle systems. 
For example, a cloud might be composed of many particle systems, each representing a billowing region 
of water particles. A parent particle system could group these all together and control the cloud's 
global movement and appearance as influenced by the wind and terrain. 3. USING PARTICLE SYSTEMS TO MODEL 
A WALL OF FIRE AND  EXPLOSIONS The Genesis Demo sequence [14] from the movie Star Trek II: The Wrath 
of Khan [10] was generated by the Computer Graphics project of Lucasfilm Ltd. The sequence depicts the 
transformation of a dead, moonlike planet into a warm, earthlike planet by an experimental device called 
the Genesis bomb. In a computer-simulated demonstration, the bomb hits the planet's surface and an expanding 
wall of fire spreads out from the point of impact to eventually "cleanse" the entire planet. The planet's 
surface begins to buckle, mountains grow, and oceans, vegetation, and an atmosphere form to produce an 
earthlike environment. The wall-of-fire element in the Genesis Demo was generated using a two-level hierarchy 
of particle systems. The top-level system was centered at the impact point of the genesis bomb. It generated 
particles which were themselves particle systems. Figure 2 illustrates the positions of these second-level 
particle systems and how they formed expanding concentric rings on the surface of the planet. Reprinted 
From acm Transactions On Graphics-April 1983-Vol. 2,No. 2 Fig. 2. Distribution of particle systems on 
the planet's surface. The number of new particle systems generated in each ring was based on the circumference 
of the ring and a controlling density parameter. New particle systems were spaced randomly around the 
ring. Particle systems overlapping others in the same or adjacent rings gave the ring a solid, continuous 
look. The second-level particle systems began generating particles at varying times on the basis of their 
distance from the point of impact. By varying the starting times of the particle systems, the effect 
of an expanding wall of fire was produced. The second-level particle systems were modeled to look like 
explosions. Figure 3 shows an example. The generation shape was a circle on the surface of the planet. 
Each particle system was oriented so that particles, generated at random positions within the circle, 
flew upward away from the planet's surface. The initial direction of the particles' movement was constrained 
by the system's ejection angle to fall within the region bounded by the inverted cone shown in Figure 
3. As particles flew upward, the gravity parameter pulled them back down to the planet's surface, giving 
them a parabolic motion path. The number of particles generated per frame was based on the amount of 
screen area covered by the particle system. The individual particle systems were not identical. Their 
average color and the rates at which the colors changed were inherited from the parent particle system, 
but varied stochastically. The initial mean velocity, generation circle radius, ejection angle, mean 
particle size, mean lifetime, mean particle generation rate, and mean particle transparency parameters 
were also based on their parent's Reprinted From acm Transactions On Graphics-April 1983-Vol. 2, No. 
2 parameters, but varied stochastically. Varying the mean velocity parameter caused the explosions to 
be of different heights. All particles generated by the second-level particle systems were predominately 
red in color with a touch of green. Recall from Section 2.5 that particles are treated as point light 
sources and that colors are added, not matted, into a pixel. When many particles covered a pixel, as 
was the case near the center and base of each explosion, the red component was quickly clamped at full 
intensity and the green component increased to a point where the resulting color was orange and even 
yellow. Thus, the heart of the explosion had a hot yellow-orange glow which faded off to shades of red 
elsewhere. Actually, a small blue component caused pixels covered by very many particles to appear white. 
The rate at which a particle's color changed simulated the cooling of a glowing piece of some hypothetical 
material. The green and blue components dropped off quickly, and the red followed at a slower rate. Particles 
were killed when their lifetime expired, when their intensity fell below the minimum intensity parameter, 
or if they happened to fall below the surface of the planet. A quickly moving object leaves a blurred 
image on the retina of the human eye. When a motion picture camera is used to film live action at 24 
frames per second, the camera shutter typically remains open for 1/50 of a second. The image captured 
on a frame is actually an integration of approximately half the motion that occurred between successive 
frames. An object moving quickly appears blurred in the individual still frames. Computer animation has 
traditionally imaged scenes as individual instants in time and has ignored motion blur. The resulting 
motion often exhibits temporal aliasing and strobing effects that are disconcerting to the human eye. 
Motion blur is a complex topic that is beginning to appear in the literature [7, 12]. The particles in 
our wall-of-fire element are motion-blurred. Three-dimensional positions are calculated for a particle 
at the beginning of a frame and about halfway through the frame, and an antialiased straight line is 
drawn between the Reprinted From acm Transactions On Graphics-April 1983-Vol. 2,No. 2 William T. Reeves 
corresponding screen coordinate positions in the frame buffer.4 Antialiased lines are used to prevent 
staircasing (moving jaggies) and strobing (popping on and off) effects. To be perfectly correct, screen 
motion due to movement of the camera should be considered when calculating where to blur a particle. 
One can also argue that simulating the imperfect temporal sampling of a movie camera is not ideal and 
that motion blur should really simulate what.happens in the human eye. This is a good area for future 
research. In the finished sequence, the wall of fire spread over the surface of the planet both in front 
of and behind the planet's limb (outer edge). The rendering algorithm generated two images per frame-one 
for all particles between the camera's position and the silhouette plane of the planet, and one for all 
particles on the other side of this clipping plane. These two elements were composited with the barren 
moonlike planet element and the stars element in back-to-front order-stars, background fires, planet, 
and foreground fires. Because the wall of fire was modeled using many small light-emitting particles, 
light from the fire should have reflected off the planet's surface. Our current implementation of particle 
systems does not handle light reflection on surface based objects. To achieve this effect, Lucasfilm 
team member Tom Duff added an additional strong local light source above the center of the rings of fire 
when he rendered the planet's surface. This produced the glow that circles the ring of fire on the planet's 
surface. (This glow is visible in Figure 5.) Figure 4 is a frame showing the initial impact of the Genesis 
bomb. It was generated from one very large particle system and about 20 smaller ones about its base. 
About 25,000 particles exist in this image. Figure 5 occurs partway through the first half of the sequence. 
It contains about;200 particle systems and 75,000 particles. Figure 6 shows the ring of fire extending 
over and beyond the limb of the planet. It is formed from about 200 explosions and 85,000 particles. 
Figure 7 shows the wall of fire just before it engulfs the camera; in Figure 8 the camera is completely 
engulfed. Both employ about 400 particle systems and contain over 750,000 particles. The textures in 
Figure 8 are completely synthetic and yet have a "natural" and highly detailed appearance that is uncommon 
in most computer graphics images. These images are interesting statically, but they only really come 
alive on the movie screen. It is interesting to note that this is also the case for many of the best 
traditional (i.e., non-computer-generated) special effects shots where motion blur is an important factor. 
A few points concerning random numbers are of interest from a production point of view. The random number 
routine we use is based on [6], and generates numbers uniformly in the range [0.0, 1.0]. It is an incremental 
algorithm based on updating a table of seed values. To checkpoint a production, all that need be saved 
is this random number table-we do not save all the parameters of 750,000 particles. To restart a computation 
at frame n, the closest preceding frame p is found that cannot contribute particles to frame n (this 
is determined from the lifetime parameters of all the active particle systems). Frame p + l's random 
number table is then read, and particle generation can begin from there. No 4 A particle's trajectory 
is actually parabolic, but the straight-line approximation has so far proved sufficient. Reprinted From 
acm Transactions On Graphics-April 1983-Vol. 2, No. 2 Particle Systems s m Fig. 4. Initial explosion. 
Fig. 5. Expanding wall of fme. Reprinted From acm Transactions On Graphics-April 1983-Vol. 2. NO. 2 
m -William T. Reeves Fig. 6. . . Llll over limb of planet. Fig. 7. Wall of fwe about to engulf camera. 
Reprinted From acm Transactions On Graphics-April 1983-Vol. 2. No. 2 Particle Systems * m particles are 
drawn until the simulation reaches frame n, so this backing up and restarting usually takes only a few 
minutes. Particles moving off screen or being extinguished for any reason do not affect the randomness 
of other particles. This is because all stochastic decisions con-cerning a particle are performed when 
it is generated. After that, its motion is deterministic. If stochastic elements were to be used to perturbate 
the dynamics of a particle (e.g., to simulate turbulence), more care would have to be taken when checkpointing 
a frame and killing particles. In that case, it would probably be better to use a more deterministic 
and reproducible random number generator. 4. OTHER APPLICATIONS OF PARTICLE SYSTEMS 4.1 Fireworks We 
are currently using particle systems to model fireworks. The fireworks differ from the Genesis Demo in 
that the control parameters of the particle systems vary more widely, and streaking is more predominate. 
Figure 9 shows two red explosions superimposed. One explosion is tall, thin, and near the end of its 
lifetime, and the other is short, fat, and building up to full steam. Figure 10 shows several green explosions 
dying off and blue spherical explosion starting up. Figure 11 contains overlapping, multicolored explosions 
formed with different generation shapes and ejection angles. Again, these images only really come alive 
when projected at 24 frames per second. Reprmted From acm Transactions On Graphics-April 1983-Vol. 2, 
No. 2 371 I -William T. Reeves , .I, .s _I Fig. 9. Two red fireworks Fig. 10. Green and blue fireworks. 
Reprinted From acm Transactions On Graphics-April 1983-Vol. 2, No. 2 372  Particle Systems - 4.2 Line 
Drawing Explosions Particle systems are being used to model exploding objects in a computersimulated 
tactical display for a scene from the movie Return of the Jedi [8]. In this case, the particle systems 
are implemented on a line-drawing display. In order to simulate motion blur, the particles are drawn 
as very small straight lines instead of as points. The texturing effects that are evident in the previous 
examples are lost on a line-drawing display, and yet the motion still looks real and the sequence gives 
the viewer the impression that something is exploding. This is because the model is dynamic-it moves 
well. 4.3 Grass To model grass, we use an explosive type of particle system, similar to that used in 
the Genesis Effect. Instead of drawing particles as little streaks, the parabolic trajectory of each 
particle over its entire lifetime is drawn. Thus, the time-domain motion of the particle is used to make 
a static shape. Grasslike green and dark green colors are assigned to the particles which are shaded 
on the basis of the scene s light sources. Each particle becomes a simple representation of a blade of 
grass and the particle system as a whole becomes a clump of grass. Particle systems randomly placed on 
a surface and overlapping one another are used to model a bed or patch of grass. Figure 12 is a picture 
entitled whitesand by Alvy Ray Smith of Lucasfilm. The grass elements of this image were generated as 
described above. The plant Reprinted From acm Transactions On Graphics-April 1983 - Vol 2, No. 2 373 
 elements were generated using a partially stochastic technique similar to particle systems. 5. ONGOING 
RESEARCH IN PARTICLE SYSTEMS A logical extension of this research will be to use particle systems to 
model fuzzy objects in which the individual particles can not be rendered as point light sources, but 
must be rendered as individual light-reflecting objects. To this end, we have begun to investigate the 
modeling of clouds. Clouds are difficult for several reasons. First, the shape and form of clouds are 
complex, depending on many factors such as wind direction, temperature, terrain, and humidity. The atmospheric 
literature abounds with cloud models that are simple in concept but computationally difficult, since 
most are based on partial differential equations. Second, clouds are difficult because they can throw 
shadows on themselves. This property is very important in making a cloud look like a cloud. Third, the 
number of particles needed to model a cloud will be very large. This will require an efficient rendering 
algorithm. 8. CONCLUSIONS We have presented particle systems, a method for the modeling of a class of 
fuzzy objects, and have shown how they were used in making the fiie element of the Genesis Demo sequence 
for the movie Star Trek II: The Wrath of Khan. Particle systems have been used as a modeling tool for 
other effects and appear promising for the modeling of phenomena like clouds and smoke. Reprinted From 
acm Transactions On Graphics-April 1983 - Vol. 2, No. 2 374 Particle Systems Particles, especially when 
modeled as point light sources or as streaks of light, have proved efficient to render-they are merely 
antialiased lines. Because they are so simple, they lend themselves to a hardware or firmware implementation. 
With a hardware antialiased line-drawing routine, the computation of our wallof-fire element would have 
been two to three times faster. Particle systems are procedural stochastic representations controlled 
by several global parameters. Stochastic representations are capable of producing minute detail without 
requiring substantial user design time. The textures in the fire sequence could not have been modeled 
with other existing methods. Fire images, scanned in from a photograph or painted, could have been texture 
mapped, but they would still have been static. Another advantage of a procedural representation is its 
ability to adapt to several different viewing environments. For example, procedural representations can 
generate only as much detail as is needed in a frame, potentially saving significant amounts of computation 
time. Having finally come to grips with spatial aliasing, it is now time for computer image synthesis 
to being to investigate and solve temporal aliasing problems. The Genesis Demo is the first "big screen" 
computer-synthesized sequence to include three-dimensional dynamic motion blur. The particles in a particle 
system can easily be motion-blurred because they are so simple. A great deal of work remains to be done 
in this area-blurring particles is much easier than blurring curved surface patches. Particle systems 
can model objects that explode, flow, splatter, puff up, and billow. These kinds of dynamics have not 
been produced with surface-based representations. The most important aspect of particle systems is that 
they move: good dynamics are quite often the key to making objects look real. 7. ACKNOWLEDGMENTS The 
author gratefully acknowledges the suggestions and encouragement of all members of the graphics project 
at Lucasfilm Ltd, especially those who worked on the Genesis Demo sequence: Loren Carpenter, Ed Catmull, 
Pat Cole, Rob Cook, David DiFrancesco, Tom Duff, Rob Poor, Tom Porter, and Alvy Ray Smith. The crusade 
for motion blur and antialiasing in computer image synthesis is a goal of the entire graphics project 
and Lucasfilm as a whole. One of the referees deserves credit for pointing out several extensions and 
improvements to the motion blurring discussion. Finally, thanks to Ricki Blau for editorial and photographic 
assistance. REFERENCES 1. ABRAHAM, R., AND SHAW, C. DYNAMICS_The Geometry of Behavior. City on the Hill 
Press, Santa Cruz, Calif., 1981. 2. BADLER, N. I., O'RouRKE, J., AND TOLTZIS, H. A spherical human body 
model for visualizing movement. Proc. IEEE 67, 10 (Oct. 1979). 3. BLINN, J. F. Light reflection functions 
for simulation of clouds and dusty surfaces. Proc. SIGGRAPH '82. In Comput. Gr. 16, 3, (July 1982), 21-29. 
 4. CSURI, C., HAcKATHORN, R., PARENT, R., CARLSON, W., AND HOWARD, M. Towards an inter  active high 
visual complexity animation system. Proc. SIGGRAPH 79. In Comput. Gr. 13, 2 (Aug. 1979), 289-299. 5. 
FOURNIER, A., FUSSEL, D., AND CARPENTER, L. Computer rendering of stochastic models. Commun. ACM 25, 
6, (June 1982), 371-384. Reprinted From acm Transactions On Graphics-April 1983-Vol. 2, No. 2 William 
T. Reeves 6. KNUTH, D. E. The Art of Computer Programming, vol. 2. Addison-Wesley, Reading, Mass., (1969), 
p. 464. 7. KOREIN, J., AND BADLER, N. I. Temporal anti-aliasing in computer generated animation. To 
appear in Proc. SIGGRAPH '83 (July 1983). 8. LUCASFILM. Returnof the Jedi (film), May 1983. 9. NORTON, 
A. Generation and display of geometric fractals in 3-D. Proc. SIGGRAPH '82. In Comput. Gr. 16, 3 (July 
1982), 61-67. 10. PARAMOUNT. Star Trek II: The Wrath of Khan (film), June 1982. 11. PBS. CarlSagan's 
Cosmos Series. (television series), Public Broadcasting System, 1980. 12. POTMESIL, M., AND CHAKRAVARTY, 
I. Modeling motion blur in computer-generated images. To appear in Proc. SIGGRAPH '83 (July 1983). 13. 
REQUICHA, A. A. G., AND VOELCKER, H. B. Solid modelling: A historical summary and contemporary assessment. 
IEEE Comput. Gr.Appl. (March 1982). 14. SMITH, A. R., CARPENTER, L., CATMULL, E., COLE, P., COOK, R., 
POOR, T., PORTER, T. AND REEVES, W. Genesis Demo Documentary (film), June 1982, Lucasfilm Ltd.  Received 
February 1983; revised April 1983; accepted April 1983 Reprinted From acm Transactions On Graphics-April 
1983-Vol. 2, No. 2 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801168</article_id>
		<sort_key>377</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>43</seq_no>
		<title><![CDATA[Temporal anti-aliasing in computer generated animation]]></title>
		<page_from>377</page_from>
		<page_to>388</page_to>
		<doi_number>10.1145/800059.801168</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801168</url>
		<abstract>
			<par><![CDATA[<p>The desirability of incorporating temporal anti-aliasing, or motion blur, into computer generated animation is discussed and two algorithms for achieving this effect are described. The first approximates continuous object movement and determines intervals during which each object covers each pixel. Hidden surface removal is then performed, allowing the calculation of visible object intensity functions and subsequent filtering. The second form of algorithm detailed involves supersampling the moving image and then filtering the resulting intensity function to &#8220;multiply-expose&#8221; each output picture. The effects of filter types and the relationship of the algorithms to forms of spatial anti-aliasing are discussed.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Filtering</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Hidden line/surface removal</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P331610</person_id>
				<author_profile_id><![CDATA[81339510180]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Korein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer and Information Science, Moore School D2, University of Pennsylvania, Philadelphia, Pennsylvania]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15038213</person_id>
				<author_profile_id><![CDATA[81452608047]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Norman]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Badler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer and Information Science, Moore School D2, University of Pennsylvania, Philadelphia, Pennsylvania]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Badler, Norman, Joseph O'Rourke, Hasida Toltzis. "A Spherical Representation of a Human Body for Visualizing Human Movement," Proceedings of the IEEE, 67(10), October 1979, pp. 1397-1403.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>908845</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Blinn, James F., Computer Display of Curved Surfaces, University of Utah Doctoral Dissertation, Computer Science Dept., 1978.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>362739</ref_obj_id>
				<ref_obj_pid>362736</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bouknight, W. J. "A Procedure for Generation of Half-tone Three-dimensional Computer Graphics Representations," Communications of the ACM, 13(9), September 1970, pp. 527-536.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360839</ref_obj_id>
				<ref_obj_pid>360825</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bui-Tuong, Phong, "Illumination for Computer-Generated Pictures," Communications of the ACM, 18(6), June 1975, pp. 311-317.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807360</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Catmull, Edwin. "A Hidden-Surface Algorithm with Anti-Aliasing," Computer Graphics, 12(3), August 1978, pp. 6-10.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907952</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Crow, Franklin. "The Aliasing Problem in Computer-Synthesized Shaded Images," University of Utah Doctoral Dissertation, 1976.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Crow, Franklin. "A Comparison of Antialiasing Techniques," IEEE Computer Graphics and Applications, 1(1), January 1981, pp. 40-48.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Gouraud, H., "Continuous Shading of Curved Surfaces," IEEE Transactions on Computers, C-20(6), June 1971, pp. 623-628.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806783</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Gupta, Satish and Robert Sproull. "Filtering Edges for Grey-Scale Displays," Computer Graphics, 15(3), August 1981, pp. 1-5.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988414</ref_obj_id>
				<ref_obj_pid>988412</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Lipscombe, James. "Reversed Apparent Movement and Erratic Motion with Many Refreshes per Minute," Computer Graphics, 14(4), March 1981, pp113-118.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988463</ref_obj_id>
				<ref_obj_pid>988460</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Knowlton, Kenneth. "Computer-Aided Definition, Manipulation, and Depiction of Objects Composed of Spheres," Computer Graphics, 15(1), April 1981, pp. 48-71.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801252</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Norton, Alan, Alyn Rockwood and Philip Skolmoski. "Clamping: A Method of Antialiasing Textured Surfaces by Bandwidth Limiting in Object Space," Computer Graphics, 16(3), July 1982, pp. 1-8.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806818</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Potmesil, Michael and Indranil Chakravarty. "A Lens and Aperture Camera Model for Synthetic Image Generation," Computer Graphics, 15(3), August 1981, pp. 297-305.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801169</ref_obj_id>
				<ref_obj_pid>964967</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Potmesil, Michael and Indranil Chakravarty. "Motion Blur in Computer Generated Images," Computer Graphics, 17(3), July 1983.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801167</ref_obj_id>
				<ref_obj_pid>964967</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Reeves, William. "Particle Systems - A Technique for Modelling a Class of Fuzzy Objects," Computer Graphics, 17(3), July 1983.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Szabo, Nicholas. "Digital Image Anomalies: Static and Dynamic," in Bruce Schachter (ed.), Computer Image Generation, John Wiley and Sons, New York, 1983.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Thomas, Frank and Ollie Johnston. Disney Animation: The illusion of Life, Abbeville Press, New York, 1981.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Watkins, G.S. "A Real-Time Visible Surface Algorithm," University of Utah Computer Science Dept., UTEC-CSc-70-101, June 1970.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Whitaker, Harold and John Halas. Timing for Animation, Focal Press Ltd., London, 1981.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Williams, Lance. "Overview of 3D Animation," Tutorial: Computer Animation Techniques, SIGRAPH '79.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Williams, Lance. "Overview of 3D Animation," Seminar: Three Dimensional Computer Animation, SIGRAPH '81.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Temporal Anti-Allaslng in Computer Generated Animation Jonathan Koreln Norman Badler Department of 
Computer and Information Science Moore School D2 University of Pennsylvania Philadelphia, Pennsylvania 
19104 Abstract The desirability of incorporating temporal antl-aliaslng, or motion blur, into computer 
generated animation is discussed and two algorithms for achieving this effect are described. The first 
approximates continuous object movement and determines intervals during which each object covers each 
plxel. Hidden surface removal is then performed, allowing the calculation of visible object intensity 
functions and subsequent filtering. The second form of algorithm detailed involves supersampling the 
moving image and then filtering the resulting intensity function to "multlply-expose" each output picture. 
The effects of filter types and the relationship of the algorithms to forms of spatial antl-aliaslng 
are discussed. CR Categories and Subject Descriptors: 1.3.3 [Computer Graphics]: Picture/Image Generation 
- dlsplay algorithms; 1.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism -animation, 
visible llne/surface algorithms. I. Introduction In computer generated animation, a frame is typically 
generated by sampling an application model at an instant of time. The image created is then used, on 
output, to represent a small interval of time in the full animated sequence. While this form of sampling 
is satisfactory with scenes of low and moderate action, it has unpleasant effects when rapidly moving 
objects are present. At high speeds, movement begins to have a stilted, jerky appearance. An ax swung 
into a log may seem to simply appear in the wood without descending towards it. Rain is perceived as 
flashing dots, rather than streaks, and wagon wheels may start Permission to copy without fee all or 
part of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. &#38;#169; ACM 0-89791-109-1/83/007/0377 
$00.75 spinning slowly backward. An analogy with a movie camera taking pictures of live action is appropriate. 
If the camera shutter speed is set extremely high little real world motion will be directly represented 
within each individual frame. The situation is typically the same in computer generated animation: the 
image is sampled at an instant of time. A movie camera normally opens its shutter for a short interval 
of time, and movement is represented as a slight blur of the image of the moving object. It is this slight 
blur which ameliorates the effects of sampling on the final film: the jerkiness is smoothed out, and 
quickly moving objects are displayed as streaks. Traditional animators have confronted the problem in 
various ways. In cel animation speed lines, illustrated in figure I, are drawn to suggest the blur of 
the movement. Repeated instances of a rapidly moving object often trail in its path, as shown in figure 
lb. Broken spokes may be added to wheels to force the viewer to make the desired correspondences [17,19]. 
Williams [21] reports that Industrial Light and Magic has made efforts to alleviate the jerkiness typically 
 accompanying stop-motion model animation through the use of motion blur. Methods for achieving similar 
motion effects within a single frame are needed in computer generated animation. While the graphics literature 
is rich in papers on spatial antl-aliasing (eg. [5,6,9]), little research has been done investigating 
temporal anti-aliasing. Williams [20,21] gives short but enlightening discussions on the subject, and 
some of his unpublished notes contributed to the ideas in thls paper. Szabo [16] also briefly surveys 
common temporal anomalies. Lipscomb [I0] investigated the temporal aliaslng occuring from differing update 
and display rates, a form which creates partlcularly irksome visual effects in real time refresh vector 
systems. Norton, Rockwood and Skolmoski [12] extend their efficient spatial antl-aliasing algorithm for 
surface textures to the temporal dimension. Two new papers not yet available to the present authors also 
attempt to simulate motlon blur and should be 1 published in tandem with the current research . Potmesll 
and Chakravarty have evidently expanded their sophisticated camera model [13] to include motion blur 
as a special effect [14], and Reeves [15] has implemented a method of streaking dots representing stars 
and fire for use in the second Star Trek movie. The goal of the current paper is to examine methods 
of temporal anti-aliaslng which respect and augment the visible surface problem. It is necessary to determine 
not only which objects i. Peter Tanner, personal communication. (a) (b) Figure 1 project onto a given 
pixel, but when they do so. A high resolution temporal intensity function may then be derived on the 
basis of visible object attributes; this function is in turn convolved with an appropriate averaging 
filter. Two methods for determining this high resolution intensity function are discussed. The first 
approximates object movement with continuous functions and, within the limits of this approximation, 
determines the precise intervals during which each pixel center is covered by an unoceluded object. The 
resulting intensity function is then derived using continuous approximations of the object attributes 
required by the shading routines. The second method uses a standard rendering algorithm to repeatedly 
 "supersample" the moving scene and determine a discrete approximation of the needed pixel intensity 
function. In the camera analogy the continuous algorithm may be compared to lengthening the interval 
during which the shutter is open while supersampling is similar to taking multiple instantaneous exposures. 
It should be noted that neither method attempts to integrate temporal and spatial antl-allasing; this 
important topic is discussed in a subsequent section. 2. Continuous Algorithms This class of algorithms 
is distinguished by its attempt to represent object movement, and object attribute changes in general, 
as continuous functions. The plxel intensity function (which is not continuous) is determined on the 
basis of these object change functions. This may be contrasted to a strict supersampling approach, in 
which a standard rendering algorithm uses static object attributes to determine the pixel intensity 
function at discrete points in time. The goal of the procedure is to accurately represent the motion 
blur one would expect in a photograph of moving objects, though different filters may be used to achieve 
different effects. The algorithm will be presented in a highly parallel, pixel oriented form, primarily 
to simplify the exposition. The actual implementation is scan line based and uses some simple forms of 
coherence to increase efficiency. The outline below describes the creation of a single image. The "filtered 
interval" refers to the temporal interval which the output frame will represent (le. the interval over 
which the filter is non-zero), and the terms "cover" and "project onto" are used interchangeably. For 
each object: l.Determlne a temporal transformation function for each dynamic object attribute. 2.Determine 
the areas which the moving object covers during the filtered interval. For each pixel: 1.Determine 
which objects cover that plxel at some time in the filtered interval. 2.Determlne the sublnterval(s) 
during which each of these objects projects onto that pixel. 3.Perform hidden surface removal by eliminating 
subintervals associated with occluded objects. 4.Determlne the plxel intensity function on the basis 
of the remaining subintervals and the corresponding object attribute functions. 5.Filter the resulting 
plxel intensity function. In determining the feasibility of this type of solution, we limited the implementation 
to images made from objects composed of spheres which are projected as disks [I,II]. A similar algorithm 
for temporally anti-aliaslng polygons, polyhedra, and curved surfaces approximated by polyhedra is described 
in the appendix. 2.1 Determining Continuous Movement Functions In any animation system, it must be 
possible to determine the values of object attributes (eg. color, polygon vertices, circle center and 
radius) at various points in time. Ideally, the functions used to determine these values could be used 
as the temporal attribute functions directly. But while these functions may sometimes be explicitly 
 represented, they are more often buried implicitly in the structure of the animating procedures. Even 
 in the case of explicit representation, the form of the functions used and parameters modified are 
 often too complex for efficient analysis. Thus these more exact functions must generally be substituted 
with interpolations between sampled values of the functions. The animating program is expected to maintain 
correspondences between objects so that the anti-allaslng program can tell which object in one frame 
is 'the same' object in a previous frame. Interpolation functions are then computed to define each dynamic 
attribute value continuously, and these functions are used to create a blur representing this approximation 
of an object's transformation between sampled points. In the general case, the filtered interval need 
not correspond to the interval between successive scene samples, and neither of these need correspond 
to the interval between two output frames. The following description, however, will make these assumptions 
for the sake of clarity; relaxing them does not significantly complicate the algorithm. Two important 
decisions must be made: what attributes should be interpolated, and with what form of function? The former 
question not only requires a separation of dynamic from static attributes, but also determines the level 
in the output pipeline at which temporal antl-allasing becomes involved. In the simple case of spheres 
projected as disks it is possible to work with image space attributes. For shaded polygons, some object 
space calculations are necessary and more complex objects may require an algorithm based completely in 
object space. Care must also be taken to understand the geometric significance of the interpolated attributes. 
Interpolating the coefficients of each term of an expanded polynomial has very different results than 
the interpolation of the disk center and radius. In general, interpolation of geometrically meaningful 
parameters leads to more intuitive and predictable, though less general, results. The forms of the interpolation 
functions are dictated by the usual trade-offs between accuracy and efficiency. The implementation described 
uses only linear interpolation functions: simplicity and speed were emphasized. The major sacrifice 
made is that the path each disk travels on the image plane is straight. This approximation of curved 
movement by plecewise linear blurs is satisfactory only if rapidly arcing motions are sampled frequently. 
Higher order interpolation functions could alleviate this effect (William's notes suggest B-splines), 
but would substantially increase the cost of subsequent processing. For disks, the attributes for which 
interpolation functions are formed are specifically: - the x coordinate of the disk center;  - the 
y coordinate of the disk center;  - the depth of the disk from the view plane;  - the disk radius; 
 - the disk intensity.  Each of these attributes will now be considered piecewise linear functions 
of time.  2.2 Determining the Covering Intervals In the current algorithm a pixel is considered as 
a point on the image plane (alternatives will be discussed later). Accurately determining the pixel intensity 
function requires finding the subintervals of the filtered interval during which a given object covers 
the pixel. This in turn requires the determination of the times when an object boundary crosses over 
that pixel. Because this is a relatively expensive operation, it must only be performed when necessary. 
Both bounding boxes and exact boundaries of object paths are used to reduce the number of such calculations 
in the current implementation. When linear attribute interpolations are used, finding these areas is 
quite simple. The bounding box of the movement path between a pair of disks is simply the bounding box 
of the pair of disks. The scan line algorithm implemented maintains sorted active scan line and active 
pixel lists so that only object paths whose bounding box covers a pixel are ever considered in subsequent 
processing. Determining the moving disk boundary exactly is almost equally simple. Disk movement on 
the image plane is completely determined by the radius interpolation function, which controls scaling, 
and the x and y disk center interpolation funetlons, which control translation. If the radial scaling 
is greater than the center translation, as shown in figure 2a, then the boundary of the moving disk over 
the interval is simply the circle surrounding the larger of the two sampled disks. Testing for inclusion 
in the boundary is done by testing for inclusion in this circle. If the center translation component 
of the movement is greater than the radlal scaling, a boundary of the shape shown in figure 2b will arise. 
This shape is always bounded by arcs from the sampled circles and llne segments from the outer pair of 
lines tangent to both of these circles. Testing for a plxel's inclusion in this boundary is simply a 
matter of testing for its inclusion in either of the sampled circles or in the isosceles trapezoid connecting 
the four tangent points defined by these lines. When the points at which the disk attributes are sampled 
are more closely spaced than the endpolnts of the interval to be filtered, testing forincluslon in the 
path of the disk requires testing for inclusion in numerous boundaries of the type Just discussed. A 
filtered interval may also truncate a sampled interval. In this case the circles defined by the interpolation 
functions at that time, rather than at the time actually sampled, are used in the inclusion tests and 
in deriving the trapezoid. While the boundary serves to define which pixels are, at some time in the 
filtered interval, covered by a disk, it does not derive the subinterval during which they are actually 
covered by the disk. When the filtered interval corresponds to the interval between attribute Figure 
2a. When scaling dominates image motion, the larger of the two circles bounds the movement path. Figure 
2b. When translation dominates image motion, parts of each circle and the two outer lines tangent to 
each circle bound the movement path.  samples, determining this subinterval is especially simple. If 
a plxel is within the disk at the start and/or end of the interval, the subinterval starts and/or stops 
there as well. Otherwise, the subinterval endpolnt(s) can be determined by intersecting the circle surrounding 
the moving disk with the pixel location. The following terms will be used in describing this equation, 
which must be solved for time t: px, py : the plxel location; ax * t + bx : the interpolation function 
for the x coordinate of the disk center; ay * t + by : the interpolation function for the y coordinate 
of the disk center; ar * t + br : the interpolation function for the disk radius. The actual equation 
follows, with the general equation for a circle listed below it to indicate its derivation. (px -(ax 
* t + bx)) 2 + (py -(ay * t + by)) 2 = (ar * t + br) 2 2 (x -h ) + ( y -k )2 2 r If the filtered 
interval is greater than the sampling interval, this quadratic may have to be evaluated more than once. 
It should also be noted that the use of image space coordinates does not account for perspective foreshortening, 
and thus the time intervals derived will only be an approximation of three-dlmenslonal linear movement 
is most situations. It is possible to label each disk as moving or non-moving on the basis of some 
movement threshold, and only blur those disks which exceed that threshold. This occasionally gives 
an unpleasant all-or-nothing effect, as objects just under the threshold will appear as still disks 
and those just above the threshold appear as blurs. Nonetheless, it is a useful efficiency measure. 
The supersampllng method turns out to be more conducive to tailoring the amount of anti-aliaslng to 
be done to the amount of movement present.  2.3 Eliminating Occluded Intervals For a given plxel, 
we now have the intervals during which every object projects onto it. These intervals, however, do 
not account for occlusion by other objects also projecting onto that plxel. To determine the intervals 
in which each object is actually visible, temporal depth functions must be derived I . i. There is 
a similar problem in area-sampllng for visible surfaces [5].  For disks, the depth interpolation function 
may be used directly. The intervals so far derived can be pared down to reflect only intervals during 
which a specific disk projects to the pixel unoccluded : only "visible disk intervals' will remain. 
Figure 3a shows the depth functions of three disks (a,h and c) over the subintervals during which they 
are determined to project onto a specific plxel. It can be seen that in interval [a ,a ], disk a occludes 
disk b. Furthurmore, in s e interval [c ,i], disk c covers disk b, while s  afterwords disk b covers 
disk c. After taking depth into account, the new intervals, shown in figure 3b, represent only the times 
that a disk is actually projecting onto a pixel unimpeded. The problem of removing the occluded intervals 
is quite similar to determining the continuous visiblity of polygons on a single scan line [3,18[. Time 
replaces the scan line, the subinterval endpoints replace the intersection of the scan line with polygon 
edges, and the depth interpolation function replaces the depth of the polygon for that scan line. The 
current implementation starts with the earliest, shallowest interval and determines the end of its visible 
portion by finding intersections (if any) with other intervals, after which the process is iterated with 
the new shallowest interval. It is important to note that methods utilizing coherence between scan lines 
in spatial hidden surface algorithms apply between adjacent pixels in the temporal case. Disk B  Increasing 
 Depth |- From I ~ Disk A Viewer I~ I I I  f 1 1 I | I I I I! I I a a c i s e s Figure 3a. The intervals 
associated with disks A,B and C before hidden surface removal. nre Depth | From | Disk A Viewer I 
 Figure 3b. The intervals after hidden surface removal. 2.4 Determining the Pixel Intensity Function 
 At this point a set of time intervals, each specifying a single visible object, have been determined. 
An intensity function must then be derived for each of these intervals using the attribute functions 
defined for the corresponding objects. The complexity of these calculations is directly related to the 
complexity of the shading algorithm used; for disks, we may simply use the intensity interpolation function 
already calculated. Using these functions, a piecewise linear pixel intensity function is easily derived. 
The appendix illustrates the more sophisticated techniques necessary when dealing with shaded polygons. 
 The intensity function may now be filtered and the final pixel intensity calculated. The effects of 
various filter types are discussed later: the results shown in figures 4a through 4f and in figure 4h 
were achieved with a simple rectangular filter. Figure 4a shows a single disk expanding and moving diagonally. 
In figure 4b two moving disks are piercing two still disks. The two disks in figure 4c are both moving 
from their left to right, and thus the shallower yellow disk almost always occludes the purple disk. 
This may be contrasted to the opposing movements occurring in figure 4d: the deeper disk is now visible 
for a much greater period of time towards the ends of the motion. In figure 4e eight disks are expanding 
and moving from the center of the screen in different directions. The yellow disks begin motion deeper 
than the pink disks but soon become more shallow. A still disk covering an expanding disk gives figure 
4f the semblance of an eclipse. Figure 4g displays unblurred photos of bubbleman throwing a ball to compare 
with the various anti-aliased sequences shown. Figure 4h was created with the standard box filter, while 
the filter used to generate figure 4i emphasized later movement. 3. Supersampling Algorithms In spatial 
anti-aliasing, determining an approximation of the image intensity function is often accomplished by 
simply sampling the image at a greater resolution than that of the output display [7]. Thus for each 
pixel the intensity might be determined in four or more different locations, rather than just one. A 
filter may then be applied to derive the actual intensity of the output pixel. The same technique can 
be applied in temporal antl-aliasing. The animation system generates not one, but multiple intensity 
buffers for a single output frame, each corresponding to a slightly different point in time. The intensities 
of each plxel in the sequence of buffers form a pixel intensity function which has been digitized at 
a greater resolution than the output frame rate, and this function may then be filtered to create the 
 actual output image. This approach is simple to implement and has the major advantage of being adaptable 
to any form of image; it is irrelevant what object representations or rendering techniques are being 
used. It is hard to overestimate the 381   calculates the number of supersamples needed for that 
particular frame. It is also possible to have the anti-aliasing program choose the filter type on the 
basis of this estimate. This type of program offers an appealing combination of adaptabilty, efficiency, 
and ease of implementation. The final modification which might be envisioned in a supersampling program 
would be utilizing object specific movement, much as was suggested for the continuous algorithm. The 
animating program determines per object movement, rather than a 'worst case' measure of frame movement. 
The anti-aliasing program then sends back the number of supersamples of each object to generate. The 
animation system returns depth as well as intensity images for each object. The anti-aliasing program 
must perform the hidden surface removal, much as was done in the continuous algorithm, by using the depth 
samples as piecewise constant functions. Once occluded intervals have been eliminated, the final pixel 
intensity function can be derived using the sampled intensities. These functions are then filtered and 
the blurred image output. Computational efficiency may then be achieved, at the expense of storage 
efficiency, by subsampling slowly moving or still objects. Because their images are stored, the hidden 
surface removal algorithm will automatically determine when moving objects, whose projections are sampled 
at a higher rate, will occlude them. This is quite similar to the use of multiple image planes in cel 
animation, though it allows much more freedom since the depth of an object is computed separately for 
each pixel. This method of utilizing frame-to-frame coherence is attractive, but it is expensive in the 
storage needed and is ineffective in scenes with changing light sources or shadows and reflections. 
4. Filters Various filter types may be used to achieve different special effects in the final image. 
The standard box filter tends to create an image in which objects are fainter at their extremes in the 
direction of motion, where they cover pixels for a shorter duration. Gaussian and triangular filters 
exaggerate this effect. Moving objects can be made to trail faintly away by specifying a filter which 
 increases with time (see figure 4i). Moving objects may be emphasized by considering the length of their 
covering intervals; this might be a desirable effect~ as quickly moving objects thin in the direction 
of motion tend to all but disappear when a box filter is used. 5. Spatial Anti-Aliasing Because of 
the similarity of the problems, it is highly desirable that spatial and temporal antl-aliasing be treated 
in an integrated fashion. The paper by Norton, Rockwood and Skolmoski [12] is an excellent example of 
this integration, though their approach treats explicitly defined textures and does not attempt to deal 
with the hidden surface removal problem. The continuous algorithm described in the current paper intentionally 
avoids the complexities of this synthesis, though it may point towards future solutions. Once each pixel 
is treated as an area, covering intervals become covering volumes in a space in which the image forms 
two dimensions and time the third. The problem may then be restated: l.Find the covering volumes of 
each object. 2.1f covering volumes intersect determine the visible volume by comparing the corresponding 
objects" depth functions. 3.Derive the intensity function of the remaining volumes and apply a three-dimensional 
filter to this function. The challenge presented by this outline is considerable, but not insurmountable; 
we feel the design of an efficient algorithm would be a significant advance in the state of the art. 
 As it stands, the algorithms presented in this paper do not integrate spatial and temporal anti-aliaslng. 
The temporal supersampling approach may be used with any form of spatial anti-aliasing, while the temporal 
continuous algorithm may only be used with spatial supersampling methods. 6. Efficiency While no detailed 
efficiency analyses were done, it is clear that both methods of temporal anti-aliasing are expensive. 
The times required to generate both the supersampled and continuously anti-aliased bubbleman pictures 
shown here were between three and ten CPU minutes on a VAX 780. While these times were obtained using 
very unoptimized Pascal code and could be improved substantially, it is unlikely they could match the 
ten to thirty seconds needed to create the unblurred pictures. The possibility of decreasing representational 
complexity in conjunction with the use of temporal anti-aliasing algorithms is being studied. 7. Discussion 
 This investigation into the temporal anti-aliasing of raster images has led to some definite conclusions 
and suggestions for future work. Two classes of algorithms have been introduced: those which use continuous 
approximations of object movement, and those which use discrete object positions sampled at a high rate. 
The choice between them, as is often the case, depends on the application. If complex object types or 
rendering techniques are to be used, supersampling has a clear advantage. If considerable anti-aliasing 
is to be done or a smooth blur is essential, a well designed continuous algorithm streamlined for efficiency 
is preferable. The integration of continuous spatial and temporal anti-aliasing is seen as the most important 
area for future research. 8. Appendix : The Temporal Anti-aliaslng of Polygonal Objects The general 
outline of the continuous algorithm discussed for disks is readily adapted for use with other simple 
object representations. In particular, polyhedral objects projected with a variety of shading techniques 
may be effectively anti-aliased within this framework. Again, numerous implementation alternatives are 
available; the one chosen is a compromise between accuracy and efficiency. The algorithm described works 
primarily in object space, though some portions use image coordinates. It will be assumed that each vertex 
is sampled in object space at appropriate time intervals and that a correspondance is made between the 
same vertex at succesive sampling times. Information about surface normals must also be supplied, though 
the location of this information depends on the type of shading used. If the model is strictly polyhedral, 
the surface normal is associated with each polygon; if the model is actually polyhedra used to represent 
a curved surface, the normal is associa ted with each vertex [4,8]. First, the temporal object interpolation 
functions must be considered. Linear interpolation functions are calculated for each vertex position 
and for each of the surface normals, whether they be associated with polygons or vertices. The line segment 
joining corresponding vertex points in succesive sampled scenes plays an important role in the algorithm 
and will he called the 'vertex path' (see figure 6). Again, we will examine the rendering of one pixel 
at this point, though the algorithm may be adapted to scan llne methods. It will also be assumed, again 
to simplify the presentation, that each polygon is sampled only at the beginning and end of the filtered 
interval. Relaxing this assumption is necessary in practical applications, but while this increases the 
complexity of the procedures no new conceptual difficulties are introduced. First the interval during 
which each polygon covers a given pixel (the "covering interval') must be found. As with disks, two estimates 
of the image area swept by a moving object are used to eliminate most polygons from consideration. This 
is the only portion of the algorithm which operates in image space. The cruder approximation is again 
the bounding box of two adjacent sampled instances of a primitive object, now a polygon; the more sophisticated 
estimate is once more the convex hull of these samples. While the latter test is more selective, it will 
not exclude all uncovered plxels, as will the boundary derived in the moving disk case. The remaining 
routines work in object space in order to give more accurate covering interval and surface normal estimates 
when perspective viewing is used. We define the "view ray' as the line, in object space, going through 
the center of projection and a given pixel location on the view plane. To determine the intervals during 
which the pixel is covered by a given polygon we must first find if it is covered at the beginning of 
the filtered interval and then find when it is subsequently covered and uncovered. The former can be 
determined by intersecting the sampled polygon with the view ray; the latter by determining the intersection 
of the view ray and the polygon's moving edges. These steps are detailed below. 1) Find out if the pixel 
was covered at the start of the filtered interval. This may be known from calculations performed for 
the previous frame; otherwise is is necessary to determine if the view ray intersects the initial sampled 
polygon. 2) Determine the intersection of the the view ray and the polygon's moving edges. The solution 
is most useful if put in terms of depth along the view ray (d), length along the edge (i), and time 
(t), rather than object space coordinates. This is done by solving the vector equation R 0 + d(R I - 
R 0) = E + i(E -E ) 0 1 0 where + t(E -E ) E0 = E00 10 00 and E =E +t(E -E ) 1 01 Ii 01 and, as 
illustrated in figure 6, d~the view ray   /,5. 0 oov ng the vertex pathj,~ ~ ,f Ell Figure 6. The 
intersection I between theview ray and the moving edge is most usefully determtnedin terms of t, the 
timeof intersection, d, the depth along the view ray, and I, the length along the edge.  - R is the 
center of projection, 0  - R is the pixel point in the view plane, 1  - E00 and E01 are an edge's 
two vertices in the first sample polygon, - El0 and E are the corresponding vertices Ii from the 
next sample polygon, - t is a time parameter used to locate the moving vertices, which are used to 
define the moving edge, -1 is the length along this moving edge, and - d is the depth of the view ray 
from the center of projection. Determining t, d and 1 at the point of intersection from the vector quantities, 
which are known, can be found after straightforward manipulation to require the solution of one quadratic 
and two linear equations. If this system has a real solution for which t and 1 are between zero and one 
and d is greater than one, an intersection of the moving edge and the view ray occurs. 3) As edge intersections 
are found, they are ordered by time. With each intersecting edge, the status of the pixel's being covered 
or uncovered by the polygon changes. We may then determine each of the covered intervals within any 
sampled interval since we have already determined whether or not the pixel is covered at the beginning 
of each sampled interval. At this point a depth function must be derived for each of the covering intervals. 
The parameter d, depth from the center of projection, is the most convenient value to use for this purpose. 
It is inherently determined when calculating the intersection of the view ray and moving edges, and is 
easily found when intersecting the view ray with sampled polygons. If these depth samples are linearly 
interpolated, the visible surface problem presented is the same as in the disk case. As a shading function 
we will use a variation of the type derived by Bui-Tuong Phong [4]: i = f(L.N) + s(H.N) n where -i 
is the reflected intensity; -L is the unit vector to the light source; -N is the unit surface normal 
vector; - H is the unit vector halfway between the viewer and the light source; -f is the diffuse reflection 
coeflclent; - s is the specular reflection coeficient; - n is the specular reflection exponent. 
In our case, all vectors are functions of time, hut we will restrict H and L to be constant. The determination 
of the value of N varies depending on whether or not a curved surface is being simulated. If a curved 
surface is not being represented, the normals of the polygons at the sampled times provide values which 
can be used in determining the intensity function. If a curved surface is being approximated, the values 
of the sampled normals used must be chosen on the basis of where the view ray intersects the polygon. 
When a covering interval border has been determined by intersecting the view ray directly with a polygon, 
the normal at that time may be determined by methods adapted from Bui-Tuong Phong [4] and Gouraud [8]. 
An edge normal function is derived by linearly interpolating the given vertex normals. The plane containing 
the view ray and its scan llne is then intersected with the polygon edges. The normals evaluated for 
the edges at these intersection points are in turn fitted with a linear interpolation function. Finally, 
this function may be evaluated at the location of the intersection of the polygon and view ray to determine 
a final "curved surface' normal. The method for calculating normals when a covering interval border 
is found through the intersection of the view ray with a moving edge is a variation on this theme. A 
temporal interpolation function is fitted to vertex normals given at adjacent sampling times. These functions 
may then be evaluated to find the vertex path normals at the time the view ray intersects the moving 
edge. A spatial linear interpolation function, based on the edge length parameter i, is then fitted 
to these vertex normal values. This function may then be evaluated at the point of view ray intersection 
since the value of 1 at that point has already been calculated. In either modeling case, we derive accurate 
samples of the surface normal at the point at which the viewing ray intersects it, despite the fact that 
this point may be moving both with respect to the world and the surface. We may then estimate and filter 
the intensity function in a number of ways. The simplest method is to evaluate the intensity function 
with the average surface normal to determine a constant intensity over each covered interval1; using 
the average of the two normals sampled within each covering interval suffices if a linear normal interpolation 
function is assumed. If more accuracy is needed, the intensity function may be calculated at several 
points within each covering interval using such a normal interpolation function. In either case, the 
resulting intensity function may be filtered discretely [6] or I. as Blinn [2] did when spatially anti-aliasing 
perturbed surface normals  continuously, again with the choice reflecting accuracy and efficiency considerations. 
The most accurate solution would be obtained by representing lntensity as a continuous function of the 
surface normal, rather than sampling the intensity for specific values of the surface normal. Performing 
a continuous convolution wlth this function, however, requires symbolic integration (when done for arbitrary 
specular reflection exponents) and/or numerical integration techniques and will not be discussed. The 
method and results of this algorithm are similar to those found in the disk case. The popularity of polygonal 
representations makes this algorithm more generally useful; as is often true, however, solving a difficult 
problem within a simple framework has lead to the proper form of solution in a more complex manifestation. 
 9. Acknowledgements Special thanks to Jim Korein for our invaluable discussions, Jane Rovins and Jane 
Korein for their artistic judgements, and Lance Williams for his scribbled insights. We also wish to 
thank Jeri Brown, Barbara Woolford and Jim Lewis of NASA, whose support and appreciation made this research 
possible. 10. References 1. Badler, Norman, Joseph O'Rourke, Hasida Toltzis. "A Spherical Representation 
of a Human Body for Visualizing Human Movement," Proceedings of the IEEE, 67(10), October 1979, pp. 1397-1403. 
 2. Blinn, James F., Computer Display of Curved Surfaces, University of----~ Docto~ Dissertation, Computer 
Science Dept., 1978.  3. Bouknight, W. J. "A Procedure for Generation of Half-tone Three-dimenslonal 
Computer Graphics Representations," Communications of  the ACM, 13(9), September 1970, pp. 527-536. 
 4. Bui-Tuong, Phong, "lllumination for  Computer-Generated Pictures," Communications of the ACM, 18(6), 
June 1975, pp. 311-317.  5. Catmull, Edwin. "A Hidden-Surface Algorithm  with Anti-Aliasing," Computer 
Graphics, 12(3), August 1978, pp. 6-10.  6. Crow, Franklin. "The Aliasing Problem in Computer-Synthesized 
Shaded Images,"  University of Utah Doctoral Dissertation, 1976.  7. Crow, Franklin. "A Comparison 
of Antialiasing  Techniques," IEEE Computer Graphics and Applications, I(i), January 1981, pp. 40-48. 
 8. Gouraud, H., "Continuous Shading of Curved  Surfaces," IEEE Transactions on Computers, C-20(6), 
June 1971, pp. 623-628.  9. Gupta, Satish and Robert Sproull. "Filtering  Edges for Grey-Scale Displays," 
Computer Graphics, 15(3), August 1981, pp. I-5. i0. Lipscombe, James. "Reversed Apparent Movement and 
Erratic Motion with Many Refreshes per Minute," Computer Graphics, 14(4), March 1981, ppl13-118. ii. 
Knowlton, Kenneth. "Computer-Aided Definition, Manipulation, and Depiction of Objects Composed of Spheres," 
Computer Graphics, 15(I), April 1981, pp. 48-71. 12. Norton, Alan, Alyn Rockwood and Philip Skolmoski. 
"Clamping: A Method of  Antialiasing Textured Surfaces by Bandwidth Limiting in Object Space," Computer 
Graphics, 16(3), July 1982, pp. I-8.  13. Potmesil, Michael and Indranil Chakravarty. "A Lens and Aperture 
Camera Model for Synthetic Image Generation," Computer  Graphics, 15(3), August 1981, pp. 297-305. 
 14. Potmesil, Michael and Indranil Chakravarty. "Motion Blur in Computer Generated Images," Computer 
Graphics, 17(3), July 1983.  15. Reeves, William. "Particle Systems -A Technique for Modelling a Class 
of Fuzzy  Objects," Computer Graphics, 17(3), July 1983.  16. Szabo, Nicholas. "Digital Image Anomalles: 
Static and Dynamic," in Bruce Schachter (ed.), Computer Image Generation, John Wiley and Sons, New York, 
1983.  17. Thomas, Frank and Ollie Johnston. Disney Animation: The illusion of Life, Abbeville Press, 
New York, 1981.  18. Watkins, G.S. "A Real-Time Visible Surface Algorithm," University of Utah Computer 
Science Dept., UTEC-CSc-70-101, June 1970.  19. Whitaker, Harold and John Halas. Timing for Animation, 
Focal Press Ltd., London,--I~I~  20. Williams, Lance. "Overview of 3D Animation," Tutorial: Computer 
Animation Techniques, SIGRAPH "79.  21. Williams, Lance. "Overview of 3D Animation,"  Seminar: Three 
Dimensional Computer Animation, SIGRAPH '81.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801169</article_id>
		<sort_key>389</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>44</seq_no>
		<title><![CDATA[Modeling motion blur in computer-generated images]]></title>
		<page_from>389</page_from>
		<page_to>399</page_to>
		<doi_number>10.1145/800059.801169</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801169</url>
		<abstract>
			<par><![CDATA[<p>This paper describes a procedure for modeling motion blur in computer-generated images. Motion blur in photography or cinematography is caused by the motion of objects during the finite exposure time the camera shutter remains open to record the image on film. In computer graphics, the simulation of motion blur is useful both in animated sequences where the blurring tends to remove temporal aliasing effects and in static images where it portrays the illusion of speed or movement among the objects in the scene.</p> <p>The camera model developed for simulating motion blur is described in terms of a generalized image-formation equation. This equation describes the relationship between the object and corresponding image points in terms of the optical system-transfer function. The use of the optical system-transfer function simplifies the description of time-dependent variations of object motion that may occur during the exposure time of a camera. This approach allows us to characterize the motion of objects by a set of system-transfer functions which are derived from the path and velocity of objects in the scene and the exposure time of a camera.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Camera model]]></kw>
			<kw><![CDATA[Digital optics]]></kw>
			<kw><![CDATA[Image restoration]]></kw>
			<kw><![CDATA[Motion blur]]></kw>
			<kw><![CDATA[Point-spread function]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Photometry</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14194891</person_id>
				<author_profile_id><![CDATA[81100561593]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Potmesil]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39022602</person_id>
				<author_profile_id><![CDATA[81100000871]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Indranil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chakravarty]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Schlumberger-Doll Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>578664</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Andrews, H. C. and Hunt, B. R., Digital Image Restoration, Prentice Hall Inc., New Jersey, 1977]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Blinn, J., F., "Systems Aspects of Computer Image Synthesis and Animation", SIGGRAPH 1982 Tutorial Notes]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Dainty, J. C., and Shaw, R., Image Science, Academic Press, New York, 1974]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Goodman, J. W., Introduction to Fourier Optics, McGraw-Hill, Inc., New York, 1968, Chapter 4,5]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Newell, M. E., Newell, R. G., and Sancha, T. L., "A New Approach to the Shaded Picture Problem", Proceedings of the ACM National Conference, 1972]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806818</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Potmesil, M. and Chakravarty, I., "A Lens and Camera Model for Synthetic Image Generation", ACM Computer Graphics (Proc. SIGGRAPH 1981), 15, (3), 297-305, August 1981]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357300</ref_obj_id>
				<ref_obj_pid>357299</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Potmesil, M. and Chakravarty, I., "Synthetic Image Generation with a Lens and Aperture Camera Model", ACM Transactions on Graphics, 1, (2), 85-108, April 1982]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>108781</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Pratt, W. K., Digital Image Processing, Wiley-Interscience, New York, 1978]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801293</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Reynolds, C. W., "Computer Animation with Scripts and Actors", ACM Computer Graphics (Proc. SIGGRAPH 1982), 16, (3), 289-296, July 1982]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Roth, S., "Ray Casting for Modeling Solids", Computer Graphics and Image Processing, 18, (1), 109-144, January 1982]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Sawchuk, A. A., "Space-Variant Image Motion Degradations and Restorations", Proc. IEEE, 60, (7), 854-861, July 1972]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Sawchuk., A. A., "Space-Variant Image Restoration by Coordinate Transformation", JOSA, 64, (2), 138-144, February 1974]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Shack., R. V., "The Influence of Image Motion and Shutter Operation on the Photographic Transfer Function", Applied Optics, 3, (10), 1171-1181, October 1964]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801276</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Shelley., K. L., and Greenberg, D. P., "Path Specification and Path Coherence", ACM Computer Graphics (Proc. SIGGRAPH 1982), 16, (3), 157-166, July 1982]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Whitted., T., "An Improved Illumination Model for Shaded Display", Comm. ACM, 3, (6), June 1980, 343-349]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Modeling Motion Blur in Computer-Generated Images Michael Potmesil 1 Bell Laboratories lndranil Chakravarty 
2 Schlumberger-Doll Research  Abstract This paper describes a procedure for modeling motion blur in 
computer-generated images. Motion blur in photography or cinematography is caused by the motion of objects 
during the finite exposure time the camera shutter remains open to record the image on film. In computer 
graphics, the simulation of motion blur is useful both in animated sequences where the blurring tends 
to remove temporal aliasing effects and in static images where it por- trays the illusion of speed or 
movement among the objects in the scene. The camera model developed for simulating motion blur is described 
in terms of a generalized image-formation equation. This equation describes the relationship between 
the object and corresponding image points in terms of the optical system-transfer function. The use of 
the optical system-transfer function simplifies the description of time- dependent variations of object 
motion that may occur dur- ing the exposure time of a camera. This approach allows us to characterize 
the motion of objects by a set of system-transfer functions which are derived from the path and velocity 
of objects in the scene and the exposure time of a camera. CR Categories and Subject Descriptors: 1.3.7 
[Computer Grapbiesl: Three-Dimensional Graphics and Realism, 1.4.4 [Image Processingl: Restoration General 
Terms: Algorithms Key Words and Phrases: Camera Model, Motion Blur, Point-Spread Function, Image Restoration, 
Digital Optics Permission to copy without fee all or part of this material is granted provided that the 
copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the 
title of the publication and its date appear, and notice is given that copying is by permission of the 
Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific 
permission. &#38;#169; ACM 0-89791-109-1/83/007/0389 $00.75 1.0 Introduction The generation of computer-synthesized 
images with a high degree of realism has been the topic of much current research. The increase in realism 
in recent years can be attributed mainly to improved shading techniques and to more versatile 3D object 
modeling techniques that can represent complex objects with great detail. The objective of this paper 
is to report an image synthesis tech- nique which incorporates the optical effects of a camera in imaging 
a scene. These optical effects of a camera add another dimension of realism in computer generated images. 
In this paper we attempt to model motion blur, an effect caused by the movement of objects during the 
exposure time of the camera, and used often to portray to the viewer the illusion of the motion of objects. 
In a previous paper [6,7] the authors developed a technique for modeling the effects of a lens and aperture 
in a camera model for computer-synthesized images. This paper is both an extension and a generalization 
of the pre- vious paper to incorporate the effects of the camera shutter. In addition, this paper presents 
the image-generation process in terms of a cascaded optical, system- transfer function which specifies 
the relationship between object and image points. This formalism allows us to express easily the time-dependent 
variations of an object's position and provides a more comprehensive approach to synthesizing the effects 
of motion blur and other camera effects. The appearance of motion blur in images can occur due to a number 
of reasons. The primary cause of this blurring is due to the movement of an object's position in the 
scene during the exposure time of the camera. By exposure time we mean the time during which the camera 
shutter remains open and the film acts as an integrating medium to accumulate the total radiant energy 
of the objects in the scene. There are two principal reasons for motion blur: 1. Address: Bell Laboratories, 
Holmdel, NJ 07733 2. Address: Schlumberger-Doll Research Center, Old Quarry Road, Ridgefield, CT 06877 
  1. Movements of Objects - The motion of objects in the scene is the most common cause for image blurring. 
The motion of objects can be classified into three categories: motion of the camera and a static scene, 
motion of objects with a static camera, and finally the simultaneous motion of both camera and objects 
in the scene. 2. Movement of the Shutter -Film is exposed in a cam- era by the movement of the shutter 
across the film plane. The finite opening and closing time of the shutter, the direction of movement 
of the shutter as well as changes in the shape of the aperture caused by the movement of the shutter 
may all modify the appearance of motion blur in the image [13]. In this paper we will not be concerned 
with modeling the blur arising solely from the functioning of the cam-era shutter.  The problem of 
characterizing the degradation caused by an optical system has been the topic of extensive research in 
image processing [1]. The removal of camera degradation to recover the original image based on some a 
priori knowledge of the degradation phenomenon is called image restoration. In synthesizing motion blur 
the prob- lem is almost inversed, that is, the objective is to generate an appropriate degradation function 
given an idealized description of the scene. Although one can draw upon the techniques developed for 
estimating the optical system- transfer function from a degraded image, the synthesis of an optical system-transfer 
function from a scene descrip-tion must take into account the individual motion of objects in the scene, 
the camera path, the occlusion rela- tionships that may vary between the objects during the exposure 
time, and any optical effects included as part of the camera model. The generation of motion blur in 
computer-synthesized images, as described in this paper, consists of two stages: 1) a hidden-surface 
program generates inten- sity sample points of an instantaneous image identifying points which are in 
motion and giving the image path of the projected motion; and 2) a post-processor which blurs the moving 
points by convolving them with the optical system-transfer functions derived from the image path and 
merges them with the stationary points into a final raster image. A ray-tracing program with a recursive 
shader [15] generates image point samples, keeping intensity contribu- tions due to surface reflections, 
transparencies and shadows separate. This allows moving objects reflected in mirrors, transmitted through 
transparent surfaces, and the shadows of such objects to be blurred. For proper merger of mov- ing objects 
with stationary objects, intensity samples of the stationary surfaces, hiding or hidden by the moving 
sur-faces, are also computed. The sample points which contain intensity reflected from moving objects 
are convolved with an optical system- transfer function, derived from the path and velocity of the motion, 
and the exposure time. The blurred moving objects and the stationary objects are then merged in a time-and-depth 
buffer into the final image. In this buffer the visibility of a surface point is determined by its depth, 
and its intensity is modified by the amount of exposure time it remains visible. A hidden-surface program 
which processes projected surfaces in a back-to-front order [5] can directly convolve each moving object 
with its optical system-transfer function as it is stored into the output image. 2.0 Image Formation 
Model The camera model which underlies the image-formation process, as modeled here, consists of two 
stages. First, a 3D scene is projected by a geometric transforma- tion into a 2D image-irradiance plane. 
The image-irradiance plane is then further transformed by the optical system-transfer function, also 
called a point-spread func- tion (PSF), into an actual raster image called the image-output plane. The 
image-output plane simulates a film by acting as a medium for accumulating the radiant energy of objects 
during the exposure time. The 3D scene is defined in a homogeneous coordinate system O(x,y,z,w) and 
contains a set of 3D objects represented by ei(x,y,z,w) , i=l,2,--,n as shown in Fig- ure i. This representation 
contains both geometrical and optical properties of the objects. This is referred to as the object-space 
coordinate system. A geometrical transfor- mation function q (x',y'; e t (x,y,z,w)) transforms object 
descriptions from O(x,y,z,w) to O'(x',y') (irnage-irradiance plane) eliminating hidden surfaces of objects 
and generating the radiant intensity for each visible sur- face point: f (x',y') = q (x',y'; e (x ,y 
,z ,w ) ) (2-1) A discrete formulation of the image-irradiance function is ei(x. y, z, w) -..~ ....... 
~~~5 %x O'   ~y'\ Xf Figure 1 Projection of 3D scene in object space O(x,y,z,w) into image-irradianc 
plane O'(x',y') equivalent to uniformly sampling the function f(x',y') using a comb function (Figure 
2) such that (2-2) M N f(iax',jay') = Z Z ~(iax',jay') f(x',y') i--O j-O M N .Z Z 8(x'-iBx', y'-jAy') 
 i=O j=O Figure 2 2D comb function of samples of the image-irradiance function in O'(x',y') where ax' 
and ay' are the sampling intervals in x' and y' directions, respectively. We will assume that the sampling 
rate is adequate so that we can reconstruct f(x',y') from f (iax',jay') by using a suitable interpolating 
function. The transformation from the image-irradiance plane O'(x',y') to the image-output plane O"(x",y") 
(raster image) incorporates the degradation of the optical system and simulates the imaging medium. We 
denote this transformation as h (x",y"; x',y'oC(x',y')), and thus we can express the raster image g (x",y") 
= h (x",y"; x',y',f (x',y')). (2-3) In order to quantify the transformation h(x",y";x',y', f(x',y')) 
we will assume the following properties: 1. The radiant intensity distribution in both the image-irradiance 
plane and the image-output plane is either positive or zero. This implies that the image- irradiance 
function (which is a measure of this energy distribution), and any transformations on the image-irradiance 
function may either conserve the energy or distribute it differently. Thus f (x',y') >/ O, and g(x",y") 
>/ O. (2-4) 2. The radiant intensity distribution is additive in both the image-irradiance plane and 
the image-output plane. This superposition property implies that given two points fl(x',y') and f2(x',y') 
and their corresponding mapping gl(x",y") and g2(x",y") then (2-5) f l(x',y') + f 2(x',y') = gl(x",y" 
) + g2(x",y"). Based on these two assumptions we may write the transfor- mation from the image-irradiance 
plane O'(x',y') to the output-image plane O"(x",y") in the spatial domain by: (2-6) co g (x ",y ") = 
f f n (x",y";x',y' (x',y')) ax' ay' --co ~oa where h (x",y"; x',y'J (x',y')) is the optical system-transfer 
function (PSF) describing the energy distribution between points in the image-irradiance plane O' and 
the output-image plane 0". This is the most general form for describing the image-formation process. 
A number of simplifications can be made to the gen- eralized image formation equation (2-6) described 
above. If the additive components in the image-irradiance plane relate to the additive components in 
the image-output plane then the system is said to be linear in which case equation (2-6) can be written 
in the more familiar form of (2-7) oo oo g(x",y") f f h(x",y";x',y') f(x',y') dx' dy' = lOO ~ The description 
of the optical system-transfer function in terms of the all four coordinate variables (x",y",x',y') reflects 
space-variance of the transfer function, that is, the transfer function is allowed to vary with position 
in both the object space and image space. We define such a transfer function as a space-variant PSF (SVPSF). 
If we restrict the transfer function to be independent of position, that is, the PSF applies uniformly 
to all points in the object and image space then it is defined as space-invariant (SIPSF) which can be 
expressed as h (x",y";x',y') = h (x"-x',y"-y'). (2-8) In the space-invariant PSF case, the transformation 
from the image-irradiance plane O' to the image- output plane O" can be expressed with the convolution 
integral: (2-9) co oo g(x",y") = f f h(x"-x',y"-y') f(x',y') dx' dy'. --co l~ We often denote the convolution 
operation in the spatial domain by the symbol * so that we may equivalently write equation (2-9) as g(x",y") 
= h (x",y")*f(x',y'). Finally in the discrete formulation g (x",y") can be represented by the convolution 
summation (2-10)M N g(iax",jay") = ~ ~ h (iax'-iax',fay"-fay') f (izxx',jay') i=o j=o The equivalent 
operation in the frequency domain can be written as the product of the fourier transforms of the image-irradiance 
plane and the PSF: G(u,v) = H(u,v) F(u,v). (2-11) For incoherent image formation systems, such as the 
ones we are trying to model, we only need to consider the squared magnitude of the PSF [4] in the convolution 
equa- tions described above. The reason for this assumption is that the intensity (radiant energy) is 
dependent only on the amplitude and not the phase of a electro-magnetic field distribution. In many cases 
it is desirable to simplify the compu- tation of convolution by factoring the 2D PSF into two 1D PSFs. 
This property is called the separability property of the PSF and can be expressed as follows for the 
space- invariant case as: h(x'-x",y'-y") = h](x'-x") h2(y'-y"). (2-12) This implies that for an orthogonal 
coordinate space, the horizontal and vertical transformations can be performed sequentially and independently 
of each other. In many instances modeling the camera effects may results in several PSFs cascaded together 
so that the overall transfer function achieves the desired effect. Given two PSFs h](x",y";x',y') and 
hE(X",y";x',y') the overall transfer function h l2(x',y";x',y') can be expressed as the convolution of 
the individual transfer functions hl(X",y";x',y')*h2(x",y";x',y'). Alternately, in the fre-quency domain 
this can be viewed as the product of the two individual transfer functions. In summary the image formation 
model can be described by a two stage process as shown in Figure 3. The first is a geometrical transformation 
which projects the object space onto an image-irradiance plane resolving the hidden surfaces and calculating 
the radiant intensity for each discrete visible point. We call this process the scene imaging system. 
An optical system-transfer function (PSF) is then used to transform the image-irradiance plane to the 
image-output plane. The PSF describes the relationship of the energy distributions between the image-irradiance 
plane and image-output planes incor-porating the effects of the lens, aperture, and shutter. The output-image 
plane thus acts as an integrating medium accumulating the total radiant energy during the exposure time 
of the camera. 3.0 Effects of Object Motion and Camera Shutter 3.1 Point-Spread Function of Object Motion 
The problem of blurring the output image function g(x",y") due to object motion during the exposure time 
[1,11,12,13] is described in this section. Let the path of the object motion be described in the 3D object 
coordinate system O (x,y,z,w) as a parametric function of time: r(t) = [ x(t) y(t) z(t) w(t) ], (3-1) 
which is projected into the 2D object coordinate system O'(x',y') as: r'(t) = [ x'(t) y'(t) ] (3-2) by 
the geometrical transformation q (x',y'; x,y,z,w) as in equation (2-1): r'(t) = q (x',y'; r(t)). (3-3) 
We assume that all the projected points in f(x',y') of an object are moving along a single projected 
path r'(t), that is, r(t) is projected into a space-invariant r'(t) in O'(x',y'). By this we mean that 
the blurring is applied uni- formly to all points along the projected path of motion. Suppose that the 
motion of the object can be stopped at some instant t, and this instantaneous image can be expressed 
as g(x",y",t). Then the recorded image-output function g(x",y") during the fixed exposure time interval 
[0,T] is: T g(x",y") = J" g(x",y", t) dt. (3-4) 0 This integration is physically performed by the film 
or other recording medium. It can be described as the sum-mation at image output point (x",y") of the 
intensities of all object points that are mapped onto this image point during the exposure interval. 
Assuming that energy is con- served by the imaging system at any instant during the exposure, energy 
radiated by an area element dx' dy' of O'(x',y') and collected by an area element dx" dy" of 0 (x",y") 
is described by: g(x",y",t) dx" dy" = f(x',y') dx' dy' (3-5) at instant t. Substituting the path description 
(3-2) into (3-5) and combining it with (3-4) we obtain the recorded image function: 2D Comb Sample Function 
Raster Image Scene i e (x, y, z, w) Imaging OpticalSystem qlx', y'; x, y, z) / h(x', y', x , y')  
 f(x', y') g(x", V") Figure 3 Image-formation system model 3.2 Point-Spread Function of Camera Shutter 
In the previous section we obtained a PSF due to object motion assuming instantaneous shutter opening 
at time t = 0 and closing at time t = T. Most actual shutters, however, are mechanical devices which 
vary the shape of the aperture during their opening and closing times and therefore further modify the 
system PSF. Shack [13] derived the shutter PSF's for the two most common types of shutter. The focal-plane 
shutter is a narrow slit in a curtain which moves across the film frame, slightly in front of the film 
plane, usually in the x" (horizontal) direction. The aperture remains open for the duration of the exposure. 
The exposure is controlled by the speed of the curtain motion and the width of the slit. The PSF for 
this shutter type is in the frequency domain [13]: 1 lul 0<lul<d H(u,v) = d (3-16) o lul>a where d 
is the ratio of the width of the curtain slit moving in the x" direction and the radius of the aperture 
opening. The between-the-lens shutter consists of a usually circular or star-shaped opening made of several 
blades, placed between lens elements, and similar in construction to the aperture diaphragm. The size 
of the shutter diaphragm increases as the shutter opens and decreases as the shutter closes. The aperture 
diaphragm of the lens remains open all the time. The PSF of this shutter type is substantially more complicated 
[13] than (3-10), and can be best approximated by a PSF of a circular aperture [6,7] whose radius varies 
with time. The PSF's due to motion of objects and camera may be cascaded with the PSF due to the shutter 
into a single system-transfer function [Figure 5]. 4.0 Synthetic Image Generation 4.1 Hidden-Surface 
Elimination A ray-tracing hidden-surface processor is used to compute the intensity point samples in 
O'(x',y') from a description of a 3D scene and the camera geometry. The processor uses Whitted's recursive-illumination 
model [15] to compute intensity-point samples with the following shading equation: f(iax',jzxy') = I 
a + I d + Is + lr + It (4-1) where I a = the ambient light intensity, 1 d = the the diffuse reflection, 
I s = the specular reflection, I r = the reflected light intensity, and I t = the transmitted light intensity. 
This equation can be recursively redefined for I r and The intensity change due to a shadow cast by an 
opaque object is: Ishadow = --I d --I s   (4-2) The hidden-surface program generates a raster image 
with a pin-hole camera model, and optionally the separate image intensity samples f(iax',jzxy') [6,7]. 
Each sample consists of the following information obtained from a node of the shading tree at O'(iax',jzxy'): 
1. the type of intensity information contained in the node: opaque, reflected, transmitted, hidden, or 
sha-dow. 2. the (i,j) coordinates of the sampled point, 3. the red, green, and blue intensity values, 
Ire d, lgreen, and Ibtue, contributed by the measured object point, 4. the z' depth (along the camera's 
optical axis) of the measured object point (not used for shadows), and 5. the object identification 
number of the measured point (or object identification number of object cast- ing shadow).  The list 
of samples of an image frame is then passed to a motion-blur processor together with a list of motion 
paths of the individual objects, time of the frame exposure t frame, and the exposure duration Tfram 
e. In an actual ani- mation system, this information would be provided by an animation processor [2,9,14] 
which controls the motions of the objects and the camera. J Aperture and hl*f h2*hl*f -h3*h2*hl*f _] 
Motion PSF: Shutter PSF: Focus PSF: F  -J h2(x", y": x', y') h3(x", y": x', y') -J h l(x'', y"; x', 
y', z') Figure 5 Cascaded system of point-spread functions 394 4.2 Addition of Motion Blur Motion blur 
is generated by a processor shown in the block diagram in Figure 6. This processor separates sam-ple 
points of a moving object with the same path r'(t) into a raster image f. The motion blur PSF h(x",y";xy) 
(equation 3-9) is computed from the object path r'(t), the exposure time Iframe, and the exposure length 
Tframe as a raster image b. The images f and h are then convolved into a blurred image f*h. This convolution 
can be per-formed either directly in the spatial domain, or optionally images f and h can be converted 
by FFT into F and H, respectively, in the frequency domain, multiplied into F'H, and then converted back 
into f*h by an inverse FFT. Finally, all blurred images of the moving objects are merged with the image 
of the stationary objects into the output raster image. Path r'(t) f all I l ExposureTime tf~me ExposureLength~frame 
 Samples PSF m--w1 r ...... "1 I ComputeFFT !iI IIi Compute FFT !I L~-- ~J L . . . . . .J f stationary 
r" | I Compute I I I Inverse FFT ,,.J Merge [  Samples Figure 6 Block diagram of the motion-blur processor 
To merge the stationary image with the blurred images of the moving objects, we need to compute in each 
blurred image the fraction of the exposure length Tframe that the moving object overlaps each pixel. 
From this information we can also determine the amount of time Tframe that each pixel in the stationary 
image is visible. This is accomplished by adding a fourth band (to the red, green and blue intensity 
bands) to each image f. This band contains the exposure length Tframe in every pixel that the instantaneous 
image of the moving object overlaps and 0 elsewhere. This "time" band is convolved with the PSF h exactly 
as the three image intensity bands. In image f*h this band, therefore, contains the fraction of time 
Tframe that the moving object overlaps each pixel. The hidden-surface processor also generates intensity 
sam-ples of stationary objects hiding or hidden by the moving objects. (Intersections with hidden-surfaces 
are otherwise used by this program for processing solid objects [10].) The stationary image is an image 
of the scene without the moving objects, rather than an image with "black holes" in places where the 
moving objects are missing. The blurred images of the moving objects and the image of the station-ary 
objects are merged in a time-and-depth buffer. In this buffer the visibility of each pixel in the images 
being merged is determined by its depth, and its intensity by the length of time it remains visible. 
If a stationary surface is hiding a moving surface, the time exposure and therefore the intensity of 
the moving surface are reduced to 0. Oth-erwise, if one or more moving surfaces are hiding a sta-tionary 
surface, the intensity of the stationary surface is reduced by the amount of time it is invisible which 
is determined from the exposure time value computed in each blurred image f*h. If the sum of the computed 
exposure times of the moving surfaces exceeds Tframe at a given pixel, then the intensity of the blurred 
moving surfaces is also appropriately reduced, and the stationary surface becomes, of course, completely 
invisible. 5.0 Results The image sample functions are generated from a 3D scene description by a ray-tracing 
program previously described in [6,7]. Moving objects are assigned paths from which the motion-blur PSF's 
are computed. The motion-blur generation, as described in the previous section (Fig-ure 6), has been 
implemented in both the spatial and fre-quency domains. The first example (Figure 7) illustrates the 
use of multiple PSFs to describe the motions of several objects in a scene. Figure 7(a) shows an instantaneous 
image of a magnet and ten metallic balls suspended in air. In Figure 7(b) the balls accelerate along 
their individual paths toward the magnet during an exposure time T. In Figure 7(c) the exposure time 
has been doubled to 2T with a corresponding increase in the motion blur. In Figure 7(d) the PSF's of 
the balls have been modified to simulate a multiple exposure. During the total exposure time 2T the shutter 
was opened five times at 0.4T intervals, and each time remained open for 0.2T. Note in the last three 
images that the intensity of the moving objects diminishes as the velocity of the objects increases. 
In the second example motion blur is applied to an image containing reflections and refractions of moving 
objects. The instantaneous image (Figure 8(a)) consists of a cube with an image of a mandrill mapped 
on its six sides and three transparent spheres. These four objects are reflected in a planar mirror. 
The finite-exposure image  [11] Sawchuk, A. A., "Space-Variant Image Motion Degradations and Restorations", 
Proc. IEEE, 60, (7), 854-861, July 1972 [12] Sawchuk, A. A., "Space-Variant Image Restoration by Coordinate 
Transformation", JOSA, 64, (2), 138- 144, February 1974 [13] Shack, R. V., "The Influence of Image Motion 
and Shutter Operation on the Photographic Transfer Function", Applied Optics, 3, (10), 1171-1181, October 
1964 [14] Shelley, K. L., and Greenberg, D. P., "Path Specification and Path Coherence", ACM Computer 
Graphics (Proc. SIGGRAPH 1982), 16, (3), 157-166, July 1982 [15] Whitted, T., "An Improved Illumination 
Model for Shaded Display", Comm. ACM, 3, (6), June 1980, 343-349  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801170</article_id>
		<sort_key>401</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1983</article_publication_date>
		<seq_no>45</seq_no>
		<title><![CDATA[Ramifications of CAD/CAM on the automotive supplier community (Panel Session)]]></title>
		<page_from>401</page_from>
		<page_to>402</page_to>
		<doi_number>10.1145/800059.801170</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801170</url>
		<categories>
			<primary_category>
				<cat_node>J.6</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>D.2.9</cat_node>
				<descriptor>Productivity</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10002952</concept_id>
				<concept_desc>CCS->Information systems->Data management systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003490.10003491</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Management of computing and information systems->Project and people management</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P334348</person_id>
				<author_profile_id><![CDATA[81100614872]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wayne]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hamann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ford Motor Company]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PANEL RA~MIFICATIONS OF CAD/CAM ON THE AUTOMOTIVE SUPPLIER COM~FUNITY CHAIR: Wayne Hamann, Ford Motor 
Company PANELISTS: W. Frankish, Engineering Service, Inc. M. Glowacz, Revere Mold T. E. Kelble, Dana 
Corporation M. J. Linsell, Ford Motor Company G. A. MacAlpine, Time Engineering J. F. Yevitch, General 
Motors Corporation James Yevitch, General Motors Corp., and CHAIRMAN'S INTRODUCTION Mick Linsell, Ford 
Motor Company will pro- The automotive companies are striving for vide perspective from the automotive 
industry. a totally integrated CAD/CAM environment as a major contribution to improved pro- Bill Frankish, 
Engineering Service, Inc. ductivity and product. To the extent that and Gordon McAlpine, Time Engineering, 
 suppliers are involved in the will provide the design and tooling design engineering/manufacturing 
process, they services perspective. are an essential element of this integra- tion process. Many of 
these suppliers Michaelene Glowacz, Revere Mold &#38; already have CAD/CAM systems, most do not. Engineering 
will give a tooling suppliers From the perspective of the automotive perspective. industry, the challenge 
is to encourage the procurement and utilization of CAD/CAM Tom Kelble, Dana Corporation, will provide 
 systems while concurrently factoring in a component supplier's perspective. traditional sourcing practices. 
Sup- pliers, on the other hand, are often Panelists will include a brief synopsis on reluctant to make 
major purchases of the extent of their involvement in CAD/CAM equipment without a commitment CAD/CAM, 
what CAD/CAM systems they use, if from their automotive customers that the they have interchanged data 
with an auto- equipment will be utilized. There have motive customer, as well as their views on been 
instances where suppliers have pro- the issues discussed above. cured CAD/CAM systems only to find 
that they were underutilized. PANELISTS' ABSTRACTS: With the migration to electronic format, T. E. 
Kelble, Dana Corporation the problem of data compatibility becomes a major challenge to the supplier 
who must Dana Corporation is a Fortune 500 company communicate with a variety of data sources headquartered 
in Toledo, Ohio. We provide from different automotive customers. products and services to everything 
from Where data translation occurs today the vehicular industry to shelter and between supplier and 
customers it gen- erally involves simple geometry only using security markets. My division is the standard 
formats developed by the major Spicer Axle Division, which is part of the automotive companies. IGES 
(Initial vehicular segment. We provide driving Graphics Exchange Specification) has capa- axles to 
the automotive and light truck bility for much broader translation capa- industries. Our customers include 
Ameri- bility; however, utilization to date has can Motors, Chrysler, Ford and General Motors. been 
very limited. The Spicer Axle Division has been using In summary, the two major questions to be CAD 
as a productive tool in our Product addressed in this panel are: Engineering area since 1977. In 1977 
we installed a Gerber Interactive Design Sys- What is the optimum strategy to develop tem that is used 
for product design, automotive supplier CAD/CAM capability?. drafting, and finite element modeling. 
In late 1981 the Gerber system was being What is the optimum strategy to achieve fully used on three 
shifts, and we needed electronic communication of additional equipment. A division task engineering/manufacturing 
(CAD/CAM) data force was formed to review what our between suppliers and their customers? CAD/CAM 
objectives should be. After a list of objectives was determined, we To provide a broad perspective for 
discus- evaluated seven CAD/CAM vendors. Of the sion, the panel members represent a diver- seven vendors, 
we benchmarked the three sity of interests: 401 that were best suited to our needs and objectives. 
Of the three we benchmarked, we ultimately chose the GE/Calma systems. We are currently installing a 
configura- tion that will allow us to also integrate CAD/CAM in our manufacturing areas. One of our division 
objectives is to establish a common data base that can be accessed by all areas. Part of our division 
objectives is also the ability to exchange data with both our customers and suppliers. This exchange 
of data will allow for shortened lead times for new product development, as well as higher quality parts. 
We are looking for- ward to exchanging data with both groups of individuals. At the present time, we 
have exchanged 3D wire frame data with two of our customers. This project was done for another Dana division 
which produces heavy truck axles. The programs were successful in both cases. It becomes a challenge 
to fully utilize the CAD data base for its maximum productivity capabil- ities. One definite area of 
benefit is clearly between the customer/supplier. It is our intent to develop these links from one data 
base to another. Bill Frankish, Engineering Serv~e, Incorporated The author will present a brief slide 
presentation on E.S.I.'s experience in CAD/CAM, discussing the kinds of transla- tion tools which E.S.I. 
utilizes to inter- change data between the Ford/Lundy and GM/CGS graphics systems. Within this con- text, 
two questions will be addressed: i. What is the optimum strategy to develop automotive supplier CAD/CAM 
capa- bility? 2. What is the optimum strategy to achieve electronic communication of Engineering/Manufacturing 
(CAD/CAM) data between suppliers and their customers? M. J. Linsell, Ford Motor Company Ford Motor 
Company is committed to the implementation of Computer Aided Design and Manufacturing (CAD/CAM) in all 
activi- ties of the Company. The benefits of increased productivity, improved quality, better and more 
sophisticated products and shorter product lead times are well recog- nized. Equally well recognized 
is the fact that the same benefits that accrue to Ford from the use of CAD/CAM technology can be realized 
by outside suppliers. To join in the electronic exchange of data, however, suppliers must be able to 
inter- face with the hardware and software used by Ford internally. Over 430 graphics workstations 
are installed at Ford. Some 360 are the Ford developed Product Design Graphics System which runs on 
Prime minicomputers and is now being marketed by Prime. The majority of the remainder are Computervision 
sys- tems. Data has already been exchanged with suppliers for several pilot programs, but on a hand 
carried tape basis. Ford is establishing a framework for future exchange by pursuing the following actions: 
 Limit internal proliferation in the pro- curement of CAD/CAM equipment. In the near term, the best way 
to facilitate data exchange is to commonize equipment to the maximum extent practical. Adopt IGES as 
a Corporate standard. Ford will adopt IGES as a standard format for communication of geometric data. 
Develop common, Corporate standards for the use of graphics data as a means of conveying product design 
information. Ford is implementing procedures worldwide to commonize design and drafting standards. Advise 
suppliers of our CAD/CAM directions and provide information on comparability. Ford will benefit in both 
lead time and cost if suppliers make effective use of CAD/CAM technology. Information will be provided 
that will allow them to achieve compatability with Ford. Also, trial pro- grams will continue to be undertaken 
with selected suppliers to exchange CAD/CAM data on specific components. These will help identify changes 
needed, both in the company and at the supplier, to support data sharing. Determine how geometric drawing 
data and alphanumeric release data should be asso- ciated. Work is currently underway to develop the 
specifications for a new engineering releasing system and will include consideration of the interface 
with drawing data. 402 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1983</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
</content>
</proceeding>
