<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>08/04/2009</start_date>
		<end_date>08/06/2009</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[New Orleans]]></city>
		<state>Louisiana</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES11411</series_id>
		<series_title><![CDATA[ACM Siggraph Video Game Symposium]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>1581073</proc_id>
	<acronym>Sandbox '09</acronym>
	<proc_desc>Proceedings of the 2009 ACM SIGGRAPH Symposium</proc_desc>
	<conference_number>4</conference_number>
	<proc_class>symposium</proc_class>
	<proc_title>Video Games</proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-1-60558-514-7</isbn13>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2009</copyright_year>
	<publication_date>08-04-2009</publication_date>
	<pages>173</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>We are very excited to introduce the papers and panels for the fourth annual Sandbox Symposium, which is now the Game Papers track of the SIGGRAPH conference! This is our third year of printed proceedings, which features an innovative blend of papers on topics such as kinesthetics and movement, user interface design, persuasive games, and programming techniques and tools. We had over 80 high-quality submissions and chose 20 papers to be presented at the conference.</p> <p>Additions this year include a social game that involves business card swapping; an onsite GameJam!, or 24-hour game design competition; IndieCade, an exhibit of independent game development; and Real-Time Rendering, a series of live demos of video games and real-time simulations. There are also two game development courses: Advances in Real-Time Rendering in 3D Graphics and Games I and II. And finally, Will Wright, game designer and creator of Spore and The Sims, is providing the Tuesday keynote address of the conference, "Playing with Perception."</p> <p>This year's Game Papers track features papers on both the theoretical and practical aspects of game development and design, and includes proofs of concept, case studies, critiques and explorations. The authors of these papers show a diversity of expertise and breadth of experiences that echoes the games community itself. They come from both academia and industry, or bridge the gap between; they have backgrounds in literature, pedagogy, health, physics, art history, cinematography, design, ecology, activism, programming and psychology. Many submissions this year centered on supporting physical movement through play and on the intersection between the real and virtual, suggesting that the definition of games and what games can do is growing and expanding. We hope to continue to serve as a site of scholarship on all aspects of games-practice, theory, design and participation.</p> <p>The first paper session, on Tuesday morning, is "Supporting Social and Persuasive Play," which presents four papers that highlight games that aim to persuade, raise awareness, and educate through participatory game play. Tuesday afternoon's paper session, "Game Mechanics and Design Projects" brings together four papers that describe design and prototyping principles and techniques for games and social networking.</p> <p>Wednesday morning's "Kinesthetic Movement in Games I" features papers that describe how to design and analyze game interfaces to better support virtual and physical game experiences.</p> <p>The two curated panels feature game designers, thinkers and researchers in a moderated discussion on cutting edge issues in the fields. Wednesday's panel, "Instigating Change: Models for Positive Games" is a discussion of games that entertain and also have unique positive social aspects that can empower forms of critical play and encourage people to take positive action. Thursday's panel, "The Art History of Games," explores ideas surrounding games as an art form, the role of technology in the development of game and the relationship among games, the established art world, "fine art" forms, and the cultural traditions of art. Please note that there are three other games-focused panels as well, "Getting a Job in CG for Entertainment," "Keeping a Job in CG for Entertainment" and "Simulated Physics in Games."</p> <p>Thursday morning's paper session, "Kinesthetic Movement in Games II" reveal insights into game mechanics and play for games with real, virtual and potentially physical components. The final paper session, Thursday afternoon's "3-D and the Cinematic in Games" brings together four papers on cutting-edge cinematography, lighting, and fluid simulation techniques in game development. other panels as well, "Getting a Job in CG for Entertainment," "Keeping a Job in CG for Entertainment" and "Simulated Physics in Games."</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<chair_editor>
		<ch_ed>
			<person_id>P1570291</person_id>
			<author_profile_id><![CDATA[81100099437]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[Drew]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Davidson]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
			<role><![CDATA[Program Chair]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
		<ch_ed>
			<person_id>P1570292</person_id>
			<author_profile_id><![CDATA[81320489774]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>2</seq_no>
			<first_name><![CDATA[Tracy]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Fullerton]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[University of Southern California]]></affiliation>
			<role><![CDATA[Program Chair]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
		<ch_ed>
			<person_id>P1570293</person_id>
			<author_profile_id><![CDATA[81365593292]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>3</seq_no>
			<first_name><![CDATA[Karen]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Schrier]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[Columbia University]]></affiliation>
			<role><![CDATA[Program Chair]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
		<ch_ed>
			<person_id>P1570294</person_id>
			<author_profile_id><![CDATA[81335497917]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>4</seq_no>
			<first_name><![CDATA[Stephen]]></first_name>
			<middle_name><![CDATA[N.]]></middle_name>
			<last_name><![CDATA[Spencer]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[University of Washington]]></affiliation>
			<role><![CDATA[Editor]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>2009</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<section>
		<section_id>1581074</section_id>
		<sort_key>10</sort_key>
		<section_seq_no>1</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Supporting social and persuasive play]]></section_title>
		<section_page_from>7</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1570295</person_id>
				<author_profile_id><![CDATA[81365593292]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Karen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schrier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1581075</article_id>
		<sort_key>20</sort_key>
		<display_label>Pages</display_label>
		<article_publication_date>08-04-2009</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Designing history]]></title>
		<subtitle><![CDATA[the path to participation nation]]></subtitle>
		<page_from>7</page_from>
		<page_to>14</page_to>
		<doi_number>10.1145/1581073.1581075</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1581075</url>
		<abstract>
			<par><![CDATA[<p>This paper describes the iterative design process of a cross media learning environment for high school students, the heart of which is an online collectible card game centered on U.S. Constitutional issues. The method of design and iterative process undertaken by the several media, industry, technology, curriculum and content partners associated with this project is described in detail by the core game design team from the USC School of Cinematic Arts Game Innovation Lab and Activision Blizzard. The project goals were to design a game that encourages historical thinking and learning through its core mechanics and which aligns with standards for middle and high school history courses. The progress of the design against these goals is described in terms of a detailed iterative process and measured with preliminary evaluation and testing results.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[game design]]></kw>
			<kw><![CDATA[game innovation]]></kw>
			<kw><![CDATA[iterative process]]></kw>
			<kw><![CDATA[learning and games]]></kw>
			<kw><![CDATA[playtesting]]></kw>
			<kw><![CDATA[prototyping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>K.3.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.1</cat_node>
				<descriptor>Education</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor>Modeling methodologies</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010406</concept_id>
				<concept_desc>CCS->Applied computing->Enterprise computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010343</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Modeling methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010344</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Model verification and validation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010489</concept_id>
				<concept_desc>CCS->Applied computing->Education</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1570296</person_id>
				<author_profile_id><![CDATA[81320489774]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tracy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fullerton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570297</person_id>
				<author_profile_id><![CDATA[81440615340]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Laird]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Malamed]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Activision Blizzard]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570298</person_id>
				<author_profile_id><![CDATA[81440616736]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nahil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sharkasi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570299</person_id>
				<author_profile_id><![CDATA[81440610979]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jesse]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vigil]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[The Corporation for Public Broadcasting. <i>American History and Civics Initiative: A Request for Proposals</i>. Washington, D.C. 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Federation of American Scientists. <i>Summit on educational games: Harnessing the power of video games for learning</i>. Washington, DC, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Salen, K. and Zimmerman, E. <i>Rules of Play</i>. MIT Press, Cambridge, MA, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Gee, J. P. <i>Good video games and good learning: Collected essays on video games, learning and literacy</i>. New York: Peter Lang, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1183324</ref_obj_id>
				<ref_obj_pid>1183316</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Fullerton, T., Chen, J., Santiago, K. Et Al. "That Cloud Game: Dreaming (and Doing) Innovative Game Design" Sandbox SIGGRAPH proceedings, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Fullerton, T. <i>Game Design Workshop: A Playcentric Approach to Creating Innovative Games</i>. Morgan Kaufmann, Burlington, MA, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Wikipedia. <i>A Mind Forever Voyaging</i> entry. Retrieved February 27, 2009, from http://en.wikipedia.org/wiki/A_Mind_Forever_Voyaging]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Symcox, L. "Thinking Historically: Critical Engagement with the Past." <i>The Social Studies Professional</i>. Sacramento, CA, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Cruz, E. <i>Bloom's Revised Taxonomy</i>. Retrieved February 27, 2009, from http://coe.sdsu.edu/eet/Articles/bloomrev/index.htm]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Squire, K., Jenkins, H. "Harnessing the power of games in education." <i>Insight</i>, 3 (5), 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Designing History: The Path to Participation Nation Tracy Fullerton* Laird M. Malamed** Nahil Sharkasi* 
Jesse Vigil* tfullerton@cinema.usc.edu lairdo@activision.com sharkasi@usc.edu jesse@psychicbunny.com 
*University of Southern California, School of Cinematic Arts, Interactive Media Division **Activision 
Blizzard Abstract This paper describes the iterative design process of a cross media learning environment 
for high school students, the heart of which is an online collectible card game centered on U.S. Constitutional 
issues. The method of design and iterative process undertaken by the several media, industry, technology, 
curriculum and content partners associated with this project is described in detail by the core game 
design team from the USC School of Cinematic Arts Game Innovation Lab and Activision Blizzard. The project 
goals were to design a game that encourages historical thinking and learning through its core mechanics 
and which aligns with standards for middle and high school history courses. The progress of the design 
against these goals is described in terms of a detailed iterative process and measured with preliminary 
evaluation and testing results. Keywords: Game design, learning and games, game innovation, iterative 
process, prototyping, playtesting. 1. Introduction: A Call for Proposals Games as learning environments 
are currently of strong interest across many important educational domains. The specific area of interest 
for the educational game project discussed here is the teaching of American history content, historical 
thinking skills and civic engagement. These areas were defined as requiring critical attention in a 2005 
call for proposals issued by the Corporation for Public Broadcasting (CPB) in which it was stated that 
young people's knowledge of even the fundamentals of American history and civics has been on a steady 
and well-documented decline for a generation or more and, by most standards, has remained at an unacceptable 
level for some time. [CPB 2005] Cited as examples of this crisis were findings by the American Council 
of Trustees and Alumni that of students surveyed at the 55 most elite colleges only 60 percent were able 
to correctly place the Civil War within a fifty year spread; and 63 percent did not know what the Emancipation 
Proclamation actually granted. Also, according to the most recent National Assessment of Educational 
Progress the nation s report card only a quarter of twelfth-graders could name two ways in which the 
U.S. Constitution limits government power. Copyright &#38;#169; 2009 by the Association for Computing 
Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org. 
Sandbox 2009, New Orleans, Louisiana, August 4 6, 2009. &#38;#169; 2009 ACM 978-1-60558-514-7/09/0008 
$10.00 The Corporation for Public Broadcasting, in recognition of this crisis, established an American 
History and Civics Initiative with the intent to award $20 million in grants to forge unique and sustainable 
partnerships to create innovative, multi-platform projects that measurably improve learning. The grants 
would be awarded in several stages, with up to ten prototype awards followed by a smaller number of full 
production awards. Eligibility for the awards was open to any partnership of public or private, non-profit, 
educational, or commercial entity with the stipulation that each partnership must incorporate a broadcast 
via public television, encouraging, though not requiring the use of new interactive technologies (the 
Web, interactive computer programs, video games, cell phones, etc.).  2. Games and Learning The Federation 
of American Scientists has suggested that educational games can provide a fundamental difference from 
traditional educational instruction. This difference is accentuated by the fact that games are based 
on challenge, reward, learning through doing and guided discovery, in contrast to the tell and test methods 
of traditional instruction. [Federation of American Scientists 2006] Commercial game designers have known 
that games are inherently successful learning environments for some time. Even in a non-educational setting, 
game designers Katie Salen and Eric Zimmerman posit that games employ a model of interactivity that emphasizes 
several levels of cognitive thinking and interpretive participation. [Salen &#38; Zimmerman 2004] Their 
model of meaningful play suggests that the agency provided by the player s ability to initiate and perform 
a range of explicit actions can be the basis for a transformative experience, and a potentially powerful 
learning experience. As a model for educational game designers currently seeking to access this potential 
for transformative play, game and literacy scholar James Paul Gee presents several key learning principles 
used for building good computer and videogames. These learning principles can be categorized into three 
sections empowering learners, problem solving, and understanding that are embodied through various functions 
of games. [Gee 2007] Gee writes: people learn skills, strategies, and ideas best when they see how they 
fit into an overall larger system to which they give meaning; any experience is enhanced when we understand 
how it fits into a larger meaningful whole. 2.1 Answering the Call In light of the potential for games 
as a powerful learning environment, a partnership formed to answer the CPB call for proposals. The companies 
assembled included Los Angeles public broadcasting station KCET; Activision, a premiere publisher of 
commercial games; the USC Game Innovation Lab, a research lab focused on developing experimental game 
play including learning games; noted historians Gary Nash (UCLA) and Linda Symcox (CSULB); The Center 
for Civics Education; Dominic Kinsley of Young Minds Inspired, creator of teaching programs and curriculum 
based on popular media, and several other media partners. In addition, the playcentric process emphasizes 
repeated prototyping and playtesting of ideas throughout the design process. As part of the playcentric 
design process, the team typically prototypes original game systems in simple form using paper cards, 
storyboards, or, later in the process, simple software mock-ups. Playtesting begins the earliest stages 
of the project and continues throughout the entire production. The partners all believed the failure 
of many educational games was twofold. Either they lacked good gameplay or production value, or they 
lacked solid instructional design. It was hoped a partnership of game design experts from both the commercial 
industry and curriculum side by side with experienced historians and teachers would surmount those issues 
and create an instructionally sound game that was also deeply engaging.  3. Methodology: Playcentric 
Design In addition to the game design expertise provided by the Activision partners, the game team at 
the USC Game Innovation Lab contributed the playcentric design methodology, which is an iterative process 
of design focused on early paper playtesting to foster development of new mechanics or unique reuses 
of known mechanics. [Fullerton et al. 2006] Playcentric design is design and technology at the service 
of the player experience. This process has been the core methodology taught at USC for a number of years 
and for further reference is described in detail in Game Design Workshop: A Playcentric Approach to Creating 
Innovative Games by Tracy Fullerton, one of the authors of this paper. [Fullerton 2008] The process first 
stresses an understanding of how of the formal aspects of the game system supports and integrates with 
the dramatic aspects to create an overall aesthetic experience that is the basis of the player experience. 
In the experimental games created in the Game Innovation Lab, including Cloud, flOw, The Night Journey, 
etc., the uniqueness of player experience begins with consideration of player actions: objectives, procedures, 
mechanics, and uses these mechanics to develop the foundation of the emotional or learning experience. 
The mechanics of the game are considered an important and core function of the message. In order to design 
a game, then, about American history, the mechanics of learning and practicing historical thinking would 
need to be at the heart of the design. Figure 1: First paper prototype for Participation Nation 3.1 
Setting Learning Goals Another key element of the playcentric process is the idea of designing to meet 
user experience goals. For this project, the experience goals were deeply related to learning and engagement 
with historical content coupled with the needs of busy classroom teachers. Early effort, even in the 
proposal phase, was put into getting input from teachers who might potentially use the project in their 
curriculum as to exactly on what those goals should focus. KCET, the managing partner in drafting the 
overall proposal, did several focus group tests among local high school teachers to pinpoint areas of 
American history best served by a game for learning. Overwhelmingly, the answer was 11th grade Constitutional 
history. The complexities of the topic, the wide range of historical moments, opinions, conflicting values 
and ideals, and subtleties of thought required to understand and analyze Constitutional issues were thought 
by all the teachers approached to be the most important subject matter we could address. If we could 
create a game that interested and engaged students with this topic, it would aid an important and difficult 
task in the school system. Our content experts, historians Gary Nash and Linda Symcox, had, fortunately, 
been involved in the creation of the U.S. National Standards for history education, and were able to 
help us develop a matrix of key historical concepts and events surrounding Constitutional history that 
could serve as the learning moments for each set of concepts. These included moments ranging from the 
creation of the Constitution itself to important tests of its powers, such as the Civil War and the Civil 
Rights movement. Armed with had a general area of focus, we needed an initial game concept that would 
address this focus and make the best use of the special resources made available by our team, specifically, 
our relationship with Activision.  4. Early Game Concepts While Activision delivered dozens of interactive 
entertainment titles yearly, none of its titles focused on education. However, several individuals at 
the company, notably VP of Communications Maryanne Lataif and VP Studios Laird Malamed, recognized the 
mission as outlined by CPB in the call for proposals as an important objective to support, both for Activision 
as a company and for the commercial game industry as a whole. The idea that new media in general, and 
specifically games, might serve as a key differentiator from past efforts to engage students with history 
and civics was important enough from a cultural point of view to justify the participation of Activision 
in the project even though the overall goals for the project were not specifically commercial. As a partner, 
Activision offered to provide game design expertise, user experience personnel, supplementary funding, 
and a game engine from one of their existing products. 4.1 The Call of Duty Game Engine An early review 
of Activision s titles revealed that the only games in their portfolio to have strong embedded historical 
content were enthusiast strategy simulations (Call to Power 1 and 2 and the Total War series). Both product 
lines focused on macroeconomic, nation-state resource management and continent-level conflict over hundreds 
of years. Given the goal of bringing civics to life in a personal manner for students, these game engines 
were immediately rejected. Thinking the chances of immersion were greater with firsthand experience, 
a First Person engine specifically the Call of Duty PC game engine was selected as the platform. Past 
Call of Duty games place players in the middle of the some of the greatest battles in World War II. In 
fact, focus group participants have identified Call of Duty as being a catalyst for interest and learning 
about World War II. While very few of the historical events identified in the content matrix by Nash 
and Symcox involved battles, we believed the engine would be ideal for bringing historical moments directly 
to life in front of students eyes. Because a fundamental aspect of the partnership included the idea 
of Activision donating the use of one of its game engines, the concept team spent quite a bit of time 
trying to make this work creatively. Various realities intervened to alter this decision, including the 
availability of gaming hardware in schools and the suitability of first person mechanics for achieving 
the learning goals. Walking through our process of determining the best technology and game mechanics 
for the project is instructional, however, and many elements of the early designs did survive in the 
final prototype, so we will survey these concepts and the factors that lead us to abandon them in favor 
of the final mechanic and platform. 4.2 Future Imperfect Our initial design inspiration drew upon the 
1985 Infocom interactive fiction game, A Mind Forever Voyaging (AMFV) written by Steve Meretzky. In AMFV 
the government has decided to implement wide social and economic policies, and the player is sent into 
calculated futures to determine the impact of the proposed legislation by living within the simulated 
changed world. Wikipedia contains a summary of the game s content. [Wikipedia 2009] Taking a cue from 
watching the impact of changes on future society, our game design envisioned a fictional town. Players 
play themselves (since the game is first person). They have to go to school, do errands, go shopping 
and socialize. Players have the ability to turn on and off various elements of the Constitution and its 
Amendments, and thus could live within a simulated world without its protections. Turning off the right 
to free assembly could then result in arrest if the player attends a rally or gathering. If search and 
seizure is eliminated, players can be harassed by police officers for no reason. The hope was that by 
allowing students to experience a virtual world without the protections we rely upon, they would better 
appreciate and understand their own rights. Ultimately, this game mechanic was viewed as too esoteric 
and too removed from real history to meet the project s overall goals of aligning with high school curriculum. 
This is a very difficult issue, which continued to come up during the design process: In a game about 
history, how much play can we allow between actual events and the possibilities of the game space? Obviously, 
the player must be able to have some influence on the outcome of a scenario, but it was critical to our 
learning goals that they learn and understand where those differences occurred. Engaging this issue facilitated 
a shift from open-ended simulation to reproducing the historical turning points identified by our partners. 
 4.3 Past Imperfect One early decision that helped focus our efforts was to use the integration of Central 
High School as subject matter for the playable prototype. Given the contemporary media coverage, large 
number of first hand photographs and voice recordings, we felt we could adequately reproduce the fateful 
days in of school integration in September 1957 using a first person engine In effect, we would script 
history using a virtual world with virtual characters. The world would still look like a game (polygonal 
and simplified where necessary), but recognizable as Little Rock AR. Players would be free roaming individuals 
within the drama and would not be able to impact the events. At this stage, however, there was no game 
idea, only a history simulation a 3D playback of what actually happened (or our interpretation and reduction 
of those events). To become a game concept, we developed the notion of history being lost historical 
records, artifacts and knowledge of the past were simply vanishing. By turning the loss of history into 
a dramatic device, we sought to create an objective within the recreated simulation and instill student/players 
with a connection to the events. The game scenario would report the disappearance of history and challenge 
the player to travel back in time to document and save history using a special time camera that could 
record video, audio and images. If players missed part of the event during their travels, they could 
return and capture the missing elements. In fact, some scenarios would require the player to travel back 
multiple times because various events would be happening simultaneously or in different locations. Once 
the player retrieved the historical documents with their time camera, the game system would link those 
digital images to actual source materials. For example, if the player photographed Elizabeth Eckford 
being harassed by another student, the actual primary source of that event would be unlocked for the 
player. Players would then write historical essays/blogs combining text and media assets to demonstrate 
their understanding and knowledge of the events they witnessed. A particular goal of this version of 
the game concept was to create in students an understanding of primary sources as records of history. 
This goal strongly resonated with the team and the learning experts, and has survived into the final 
version of the game to become a key component of the play mechanic, even though most of the other aspects 
of the design have evolved and changed. This version of the game concept was detailed in a design summary 
document, and storyboards were commissioned to illustrate the play mechanics. It was this concept that 
was included in the first, written proposal to CPB in late 2005. Figure 2: Storyboard for first person, 
3D game concept This first person 3D version of the game concept idea was re­evaluated after the initial 
proposal was submitted. Primary was a concern about the use of the Call of Duty game engine. In continuing 
to gather data, KCET found the average school in the low-income school districts we wanted to reach typically 
had one or two low-end computer systems available per class. The PC requirements to run the Call of Duty 
engine would likely be too high for the majority of these schools and exclude many of the students we 
were trying to reach. The first person mechanic had several associated issues. The genre mostly appeals 
to experienced gamers, typically males, and for the inexperienced can be potentially disorientating. 
Attempting to attract non-game players (male or female), this could be a barrier to engagement. Finally, 
the game engine would be difficult for the envisioned student team at USC to learn and program to its 
fullest. To effectively use the engine, the original development team would be needed to support the 
effort. This was unrealistic given that team s focus on other titles.  4.4 Calling all browsers During 
2006, the CPB initiative was in a state of hiatus due to personnel changes at a decision-making level. 
During this time, the KCET, USC and Activision team members continued talking casually about other possible 
approaches to the idea, but did no detailed design work. When the CPB initiative was re-launched in January 
of 2007, however, we were asked to re-visit our proposed ideas and come in for a pitch meeting with revised 
and/or expanded concepts. By this time, our focus had turned to developing a web-based game. Utilizing 
a web browser (another decision that ultimately carried through to the final design) would address our 
concerns about computing horsepower in the classrooms. This decision also would allow easily for a variety 
of platforms (PC, Mac, Linux) to be supported. And, an online product could support a new concept that 
became more and more important to the proposed learning environment: a social network in which students 
and teachers could communicate, compete, cooperate and creatively interact with the game content. In 
this preliminary pitch, we envisioned a web version of the treasure hunt mechanic described above, utilizing 
the mission structure retained from the previous concept. Although we knew we could find a more elegant 
solution if we were to be funded for a full design phase, this version of the game design was presented 
to CPB in February 2007, as part of a full learning environment that included an online community, a 
graphic novel, a series of short video webisodes and a suite of online teacher s tools and curriculum 
organized and aligned with California 11th grade history standards. This presentation was well received 
by the rest of the partners on the project and ultimately by CPB as a research and prototyping grant 
was awarded in mid-2007.  5. Final Design Direction After approval by CPB to move forward with the research 
and prototyping phase, all aspects of the project were opened for re­evaluation and a full team of media 
consultants, curriculum planners and game designers (graduate students) were brought on board to flesh 
out the preliminary goals. At this stage, we had the time and resources to truly address the design challenge 
that we had set from the standpoint of our core learning goals. We knew we wanted students to interact 
with history in tangible ways, and we knew to be successful, the project would have to surpass the high 
educational requirements. Our evaluation consultant Richard Wainess pointed out that creative and critical 
thinking were key outcomes that we needed to plan for in our player experience. Otherwise, the game would 
not be adopted by school districts (let alone receive approval to continue from CPB). We needed to find 
or create a mechanic that would support not only the learning of standard facts about history, but also 
the ability for students to play with that history and form their own ideas and conclusions. One of our 
team historians, Linda Symcox, had written an article about teaching the process of thinking historically. 
In it, she quotes historian Tom Holt, who makes a case for the use of primary sources in K-12 classrooms. 
She says, He refuses to approach history as the memorization of someone else's facts. Rather, he argues 
that the study of history must be an experience in authentic problem solving. This can only be achieved 
by poring over primary sources. Holt concludes that, rather than teaching [students] to be consumers 
of stories, someone else's facts, we might better develop their critical faculties, letting them create 
stories of their own. [Symcox 2004] The team examined a range of potential game mechanics that might 
support just this type of play behavior. One concept that generated traction in our discussions was that 
of a collectable card game. Our own experiences with such games evoked Holt s description of students 
pouring over primary resources. Collecting, organizing and interpreting the power of cards as part of 
the meta experience of such game systems seemed a perfect model for students to learn about historical 
figures, events and laws. We envisioned an environment where students could battle each other with ideas, 
learning and understanding facts, applying them in game, analyzing and evaluating possible strategies 
as they related to history, and creating their own particular strategies. These activities also followed 
Bloom s Taxonomy of cognitive learning skills, which encourages a rising expectation of learning outcomes, 
from factual knowledge, comprehension, application, analysis, and synthesis to evaluation. [Cruz, 2009] 
 5.1 Choosing CCGs In addition to the considerations mentioned above, Collectible Card Games (CCGs) seemed 
a better choice than virtual worlds, or role playing games because of their established popularity among 
our target age group (middle to high school students), the archetypal representation of characters and 
their attributes, and the seemingly effortless way which players absorb and memorize the game s content, 
characters, and rules as an effect of repeated play. 5.1.1 Prior Art Collectible Card Games examined 
as prior art included MAGIC: The Gathering, Pokemon, Harry Potter, and Star Trek. Harry Potter and Star 
Trek were good reference examples of new content being applied to an existing mechanic, and Star Trek 
s timeline mechanic held serious potential for us to adapt. 5.1.2 Advantages of CCGs Players of CCGs 
have incredible knowledge of the characters, attributes, and values on each card. Like baseball fans 
memorizing statistics on baseball cards, CCGs players often memorize content of the cards, both general 
and specific, in order to formulate their strategy and defeat opponents in battle. We set out to turn 
these aspects of CCGs to the advantage of learning history. Our hope was apply historical content to 
this tiered information structure, so that a student need not know all the specific historical facts 
about a figure in order to know that figure s place in the system.  5.2 A Casual Mechanic with Deep 
Gameplay Realizing the applicability of an online card game to our goals, work began on a simple to learn 
yet hard to master card game mechanic that could support many different moments in Constitutional history. 
It needed to be flexible to encompass more than 200 years of history, specific enough to give students 
a deep experience of the material, and accessible enough for inexperienced players and teachers to quickly 
adopt. An iterative process beginning with index cards to internally test basic mechanics evolved into 
full paper prototypes and finally basic digital prototypes. These were not only tested by the team, but 
with groups of teachers and students. (See section 8 on Evaluation.) With each iteration, we found more 
specific ways to align the play experience with the learning goals. The first challenge we addressed 
in early iterations was how to allow students to experience history in a non-linear, non­deterministic 
way. By strictly adhering to the narrative of history we risked creating a static experience, and by 
allowing too much freedom in the system we risked teaching false history. Applying a level of abstraction, 
and taking a historiographical approach to the content gave us to better way engage with primary source 
documents and meet the learning goals of teaching analytical and research skills. We began by seeking 
out reoccurring archetypal roles that could be applied to any moment in Constitutional history. This 
led us to see the historic content more as grouped elements with specific attributes, and less as a linear 
narrative. This interpretation of history was very similar to the basic character systems found in collectible 
card games, so we used a variation of these models to begin prototyping.  5.3 Concept driven prototypes 
In the course of developing Participation Nation, we created more than a dozen paper prototypes, iterating 
on the basic concept of collectible card games, heavily emulating their structure. Our cast of historical 
figures from the civil rights movement and the 1957 school integration in Little Rock, Arkansas were 
given a Class (Leader, Lawmaker, Citizen, etc ), numerical values for Strengths (Power, Agency, and Resources), 
and assigned to a team (Forces of Change or Status Quo). Historical figures (or Actors, as we named them) 
from opposing sides would battle over Issues, comparing their Class, Power, Agency, and Resource values 
based on the criteria of the issue in question. In addition to the Actors, we also included Support Cards 
that represented other historical events and concepts that were key to teaching this Constitutional crisis, 
which also added numerical values to cards in the battle. 5.3.1 Response to Early Prototypes (Problems) 
The response to this prototype at the December 2007 full team meeting, including all of our content and 
curriculum partners, was that the mathematical system driving the game was too transparent, and a student 
could simply play the numbers and win the game without engaging in the historical content at all. This 
is indeed the shortcoming of many educational games that teach you how to play the game, rather teach 
you the intended subject matter. Figure 3: Cards from an early prototype; note numeric battle system 
on Terrence Robert s card at bottom left. Another critique of our early prototypes involved replayability. 
We still struggled with the challenge of making the game system  flexible while not betraying historical 
facts. A major factor in this challenge was incorporating the element of time and tying figures and gameplay 
to a liner timeline. Our desire was to portray the idea of continually drawing on the wisdom and power 
of the past as we write and interpret history. We wanted some freedom in the system to 1) interpret historical 
events and 2) to use historical figures as archetypes anachronistically. However, our CCG-style battles 
had players changing history by recreating confrontations that never happened. For example, The Arkansas 
National Guard could defeat the NAACP in battle and render them powerless for the remainder of the game. 
This loose interpretation of historic events was clearly at odds with our learning goals.  5.3.2 Revisions 
 Hiding the Numbers Addressing the concern that we had created a math game, rather than a history game, 
we decided to replace the numerical values for Power, Agency, and Resources with history keywords (later 
named Civic Principles). When the player sent his or her cards to battle he or she would be associating 
these terms with the issue (or question) at the center of the battle. The cards were still weighted according 
to Class, but the numeric values were completely removed from the interface. This allowed us to turn 
the problem of gaming the system to our advantage. Even if students play Participation Nation as a simple 
word matching game, they would still be memorizing keywords and associating them with historic figures, 
ultimately an activity we want to promote. 5.3.3 Revisions Replayability In addressing replayability, 
i.e. how to make a historic moment replayable without teaching false history, we relied again on abstraction. 
We removed any attachment to a timeline, and re­framed the issues at the center of the battle, so that 
the player and the opponent were both playing to the question, rather than directly against each other. 
The paper prototype process facilitated rapid revisions and testing of each new mechanic.  5.4 Final 
Revisions In the final Participation Nation prototype, the player assumes the role of either the Forces 
of Change or the Status Quo in a debate over three points of argument concerning the overall constitutional 
issue of school integration. The player uses his or her deck of historical figures, laws and values to 
make a stronger, more relevant argument than his or her opponent. Each card has an inherent strength 
(Class) and relevancy. Matching a historical figure to its relevant point of argument generates a stronger 
argument in the debate. Primary sources add strength to an argument, teaching not only the importance 
of using facts to support an argument, but also the process of analyzing and evaluating primary sources 
(more information below). 5.4.1 "Battle" becomes "debate" Debate is a time-tested classroom activity 
that engages students in critical thinking on history. Participation Nation, evolves this academic exercise 
into an innovative and challenging game mechanic. Through play, students not only to increase their knowledge 
of facts of history, but also to employ this knowledge in the construction of complex arguments surrounding 
the essential questions of constitutional crisis. The game mechanic asks students not only to recall 
the figures, terms and concepts of a particular point in history, but also to organize these elements, 
and draw connections between them, facilitating a broader understanding across different periods in history. 
 Figure 4: Debate prototype for Participation Nation By framing a traditional collectible card game battle 
as a debate, the players use their deck to address a question rather than battling the opponent s deck. 
This helps avoid the false history problem The game is then not about the Arkansas National Guard directly 
confronting the NAACP (which may never have happened), but rather about evaluating each historical figure 
s influence on the Constitutional validity of school integration. Winning the game is not simply about 
memorizing the facts of history, but using those facts to formulate a powerful argument. 5.4.2 Mechanic 
of Primary Sources In the project learning goals, there was an emphasis on historiography, critical analysis 
of primary sources, and research and writing skills. We introduced primary source documents (photographs, 
video, audio, documents, etc.) into the system as supports that would strengthen the player s argument 
and add more strategic game play. Primary Sources, tightly associated with key figures, laws, or values, 
became special wild cards that would block, unblock, add power, replace or call up the cards in play 
and in the decks. Primary Sources are heavily weighted in the game, and can easily change the course 
of the debate prompting players to learn how to access and use them to win the game. This mechanic motivates 
research and critical analysis, and emphasizes the importance of using appropriate support, documents, 
and facts in an argument. 5.4.3 Bonuses for understanding history, replicating it The game needed to 
motivate interest in history without requiring prior expertise. In the game, players are not penalized 
for making weak arguments, but are substantially rewarded for the strength and relevancy of their argument. 
It rewards players for accuracy and strategy in their interpretation of history, and also gives special 
awards for recreating historical moments through game play.  6. Description of the Final Prototype 
Participation Nation is an online-enabled, 2D digital implementation of a customizable card game. Like 
other games in its genre, it focuses on testing the collection and organization of the player s cards 
against those of an opponent.  Figure 5: Final digital prototype for Participation Nation Game levels 
focus on a central debate between the Forces of Change and the Status Quo. Each debate is comprised of 
three key questions central to the debate topic at hand. Those questions are each represented by a play 
line, a piece of game geography that can be secured by the strategic placement of cards from the player 
s deck. In the metaphor of the debate, securing a line counts as being conceded a point in the debate. 
Winning the game requires being awarded two of the three possible points in the debate, accomplished 
when players present evidence supporting their historical point of view from the cards in their hand. 
Cards are evaluated based on their relevancy to the topic at hand and the strength of the argument they 
make, similar to imagining a tug-of-war. The more relevant evidence introduced on one side of the debate, 
the closer that side gets to being awarded the point. Momentum can be reversed and ground can be recovered 
if the opposition renders some aspect of the argument invalid or strengthens its own argument. Adding 
an additional layer of complexity, wins and losses are graded by the system based on strength of overall 
argument during play and the efficiency and effectiveness of the debate. This is intended to further 
incentivize replayability and improved performance, since a higher score and best ending await only the 
players with mastery of both the history and key debate skills. 6.1 History as Strategy Primary sources 
are unlocked when historical concepts those sources are associated with get introduced into play. Many 
can block the effect of an opponent s card, refuting the argument in the metaphor of the debate. Likewise, 
some strengthen an argument already in play by adding additional evidence. Certain cards have even more 
unique abilities. For example, Little Rock Mayor Woodrow Mann s telegram to President Eisenhower is a 
primary source card that brings the very powerful Eisenhower card from anywhere in the deck to the player 
s hand. That primary source is only unlocked when Mann s card is introduced into the debate. This sequence 
is furthermore part of a hidden combo that rewards the player for demonstrating an understanding of history 
by putting certain cards into play in a historically accurate sequence.  7. Integration with the Larger 
Project The debate game functions as a central component of the online destination for Participation 
Nation. The entire initiative comprises a web portal and social networking site with content surrounding 
eighteen chapters of Constitutional history, each with a supporting comic book chapter, video webisodes, 
an interactive database of primary source material, and three levels of the debate game for each historical 
chapter. Users create unique profiles that track scores in the game, progress through curriculum, rankings 
within a class, school, or community, and special badges earned for exemplary performance. The various 
project sections reinforce and support each other. For example, the graphic novel and game share art 
assets and an overall artistic aesthetic. Though not implemented in the prototype, the game has been 
designed to award special badges and higher scores to players replicating the historical events presented 
in the graphic novel. Additionally, to encourage players to explore the history behind the game and engage 
the material on a critical level, every card in the debate game can be improved through scholarly activity 
in a section of the site called Learn and Connect, a graphical searchable database of primary sources 
relating to the historical figures and values in the game debates. Figure 6: Graphic Novel  8. Preliminary 
Evaluation The final Participation Nation prototype was piloted in several classrooms to good response. 
This prototype included a playable version of the debate game with content for the Little Rock level, 
a single chapter of the graphic novel, with aligned content from Little Rock, a working version of Learn 
and Connect with primary sources for the Little Rock content, webisodes and clickable mock-ups of other 
pages in the Little Rock chapter. The classrooms included two in Long Beach Unified School District with 
24 and 35 students respectively. Another classroom in Lake Havasu Arizona with 25 students was also a 
pilot site. These larger group sites were in addition to a number of individual playtests conducted at 
the USC Game Innovation lab as part of the ongoing development effort. Much of the data collected from 
these tests is currently not publishable under confidentiality agreements; however, in general, the responses 
of both teachers and students to the prototype were extremely encouraging. Teachers expressed enthusiastic 
attitudes toward probable adoption; and felt that the content of the program was excellently aligned 
with their district and state curriculum standards. Figure 7: Students playing prototype Students also 
expressed enthusiasm toward adoption, but more importantly, displayed behaviors that were perfectly aligned 
with the learning goals for the game mechanics. For example, students read the debate cards carefully, 
using the knowledge they found there to consider their game strategies. One student was overheard asking 
his classmate, Was Eisenhower for or against integration? As was pointed about by the evaluator, this 
is an excellent question to glean from playing the debate game and reading the primary sources. In a 
test at the Game Innovation Lab, where the design team asked participants to think aloud about the choices 
that they were making, one player made a good move using the card Federalism in the debate. When asked 
why he had played Federalism to the question Should the Federal government intervene in the crisis at 
Little Rock? he replied that he had not know the term before playing the game, but had come to understand 
the idea by reading the debate cards. Overall, both teachers and students reacted positively to the game 
and its learning potential. One teacher commented, The students were thoroughly engaged and were anxious 
to discuss historical topics and issues. My students who normally are not very enthusiastic were sucked 
in to the game/debate and graphic novel. This is, of course, exactly the outcome we had hoped for with 
the design of the game mechanics.  9. Conclusion The prototype of Participation Nation was delivered 
to CPB in November of 2008 and is awaiting a decision as to further funding for the production of the 
full eighteen chapters of historical content. It is the feeling of the team that preliminary evaluation 
upholds our design decisions and points to a positive learning effect from the use of the game within 
a standard history curriculum. Watching the pilot tests and playtests has reinforced our sense that this 
core game mechanic, chosen after the evaluation of so many other possibilities, creates precisely the 
cognitive process required to engage students in the historical thinking process through gameplay. By 
strategizing and playing well, they in fact practice the very skills needed to learn, understand, apply, 
analyze and evaluate historical knowledge. Within the social network of the game and learning platform, 
they practice these skills on their own and with their peers, proving true a comment by Henry Jenkins 
and Kurt Squire that Ultimately, educational game design is not just about creating rules or writing 
computer codes; it is a form of social engineering, as one tries to map out situations that will encourage 
learners to collaborate to solve compelling problems. [Squire and Jenkins 2003]  References THE CORPORATION 
FOR PUBLIC BROADCASTING. American History and Civics Initiative: A Request for Proposals. Washington, 
D.C. 2005. FEDERATION OF AMERICAN SCIENTISTS. Summit on educational games: Harnessing the power of video 
games for learning. Washington, DC, 2006. SALEN, K. and ZIMMERMAN, E. Rules of Play. MIT Press, Cambridge, 
MA, 2004. GEE, J. P. Good video games and good learning: Collected essays on video games, learning and 
literacy. New York: Peter Lang, 2007. FULLERTON, T., CHEN, J., SANTIAGO, K. ET AL. That Cloud Game: Dreaming 
(and Doing) Innovative Game Design Sandbox SIGGRAPH proceedings, 2006. FULLERTON, T. Game Design Workshop: 
A Playcentric Approach to Creating Innovative Games. Morgan Kaufmann, Burlington, MA, 2008. WIKIPEDIA. 
A Mind Forever Voyaging entry. Retrieved February 27, 2009, from http://en.wikipedia.org/wiki/A_Mind_Forever_Voyaging 
SYMCOX, L. Thinking Historically: Critical Engagement with the Past. The Social Studies Professional. 
Sacramento, CA, 2004. CRUZ, E. Bloom's Revised Taxonomy. Retrieved February 27, 2009, from http://coe.sdsu.edu/eet/Articles/bloomrev/index.htm 
SQUIRE, K., JENKINS, H. Harnessing the power of games in education. Insight, 3 (5), 2003.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1581076</article_id>
		<sort_key>30</sort_key>
		<display_label>Pages</display_label>
		<article_publication_date>08-04-2009</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Societal impact of a serious game on raising public awareness]]></title>
		<subtitle><![CDATA[the case of FloodSim]]></subtitle>
		<page_from>15</page_from>
		<page_to>22</page_to>
		<doi_number>10.1145/1581073.1581076</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1581076</url>
		<abstract>
			<par><![CDATA[<p>This paper presents an evaluation of the societal impact of a simulation-based Serious Game. FloodSim was developed with the aim of raising awareness of issues surrounding flooding policy and citizen engagement in the UK. The game was played by a large number of users (N=25,701) in a period of 4 weeks. Quantitative and qualitative analyses (on a reduced data set) were carried out in order to explore the impact of FloodSim play in raising the general public awareness around flooding in the UK. The results suggest FloodSim was hugely successful in generating general public interest and there was evidence that (a) FloodSim increased awareness at a basic level and (b) that despite the simplicity of the simulation, players perceived FloodSim to be an accurate source of information about flood risk and prevention. This suggests that serious games such as FloodSim have potential to engage the public and raise awareness of societal issues. However, FloodSim only raised awareness at a basic level. It is suggested that more needs to be done to endow serious games with pedagogical principles and more care should be given to the accuracy of the information they convey. The appropriateness of games as an educational medium for raising awareness of complex, real-life issues should also be carefully considered. This study throws some light on the potential of simulation-based Serious Games to offer experiential learning, engage users with serious topics while raising public awareness and understanding of social issues such as flooding and related policymaking. Future research is outlined consisting of identifying the problems and challenges in designing and developing serious games while considering pedagogical principles.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[serious games]]></kw>
			<kw><![CDATA[societal impact]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.6.7</cat_node>
				<descriptor>Environments</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>J.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010366.10010367</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation support systems->Simulation environments</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010455</concept_id>
				<concept_desc>CCS->Applied computing->Law, social and behavioral sciences</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1570300</person_id>
				<author_profile_id><![CDATA[81384593021]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Genaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rebolledo-Mendez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Coventry University, Coventry, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570301</person_id>
				<author_profile_id><![CDATA[81436601959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Katerina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Avramides]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Sussex, Brighton, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570302</person_id>
				<author_profile_id><![CDATA[81314488313]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sara]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[de Freitas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Coventry University, Coventry, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570303</person_id>
				<author_profile_id><![CDATA[81440620456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Memarzia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PlayGen Ltd, London, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1279223</ref_obj_id>
				<ref_obj_pid>1279012</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Conati, C. and C. Merten (2007). "Eye-Tracking for User Modeling in Exploratory Learning Environments: an Empirical Evaluation." &amp;lt;u&amp;gt;Knowledge Based Systems&amp;lt;/u&amp;gt; &#60;b&#62;20&#60;/b&#62;(6): 557--574]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Craig, S., A. Graesser, et al. (2004). "Affect and learning: An exploratory look into the role of affect in learning." &amp;lt;u&amp;gt;Journal of Educational Media&amp;lt;/u&amp;gt; &#60;b&#62;29&#60;/b&#62;: 241--250.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Csikszentmihalyi, M. (1990). &amp;lt;u&amp;gt;Flow: The Psychology of Optimal Experience&amp;lt;/u&amp;gt;. New York, Harper and Row.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1040838</ref_obj_id>
				<ref_obj_pid>1040830</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[D'Mello, S., S. Craig, et al. (2005). &amp;lt;u&amp;gt;Integrating affect sensors in an intelligent tutoring system&amp;lt;/u&amp;gt;. "Affective Interactions: The Computer in the Affective Loop Workshop" In conjunction with the International conference on Intelligent User Interfaces. p. 7--13.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[de Freitas, S. (2008). Serious Virtual Worlds: a scoping study. JISC. London, Serious Games Institute, Coventry University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Dieleman, H. and D. Huisingh (2006). "Games by which to learn and teach about sustainable development: exploring the relevance of games and experiential learning for sustainability." &amp;lt;u&amp;gt;Journal of Cleaner Production&amp;lt;/u&amp;gt; &#60;b&#62;14&#60;/b&#62;: 837--847.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1480553</ref_obj_id>
				<ref_obj_pid>1480531</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Freitas, S. d. and T. Neumann (2008). "The use of 'exploratory learning' for supporting immersive learning in virtual environments" &amp;lt;u&amp;gt;Computers and Education&amp;lt;/u&amp;gt; &#60;b&#62;52&#60;/b&#62;(2): 343--352.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Iuppa, N., G. Weltman, et al. (2004). &amp;lt;u&amp;gt;Bringing Hollywood Storytelling Techniques to Branching Storylines for Training Applications&amp;lt;/u&amp;gt;. 3rd Narrative and Interactive Learning Environments, Edinburgh, Scotland, p. 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Johnson, W. L., N. Wang, et al. (2007). &amp;lt;u&amp;gt;Experience with Serious Games for Learning Foreign Languages and Cultures&amp;lt;/u&amp;gt;. SimTecT Conference, Australia. p.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1101300</ref_obj_id>
				<ref_obj_pid>1101149</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Kapoor, A. and R. W. Picard (2005). Multimodal affect recognition in learning environments. &amp;lt;u&amp;gt;Proceedings of the 13th annual ACM international conference on Multimedia&amp;lt;/u&amp;gt;. Hilton, Singapore, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>267996</ref_obj_id>
				<ref_obj_pid>267985</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Laurel, B. (1997). Interface Agents: Metaphors with Characters. &amp;lt;u&amp;gt;Software Agents&amp;lt;/u&amp;gt;. J. M. Bradshaw. London, AAAI Press / The MIT Press: 67--78.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Lester, J. C., S. A. Converse, et al. (1997). &amp;lt;u&amp;gt;Animated pedagogical agents and problem-solving effectiveness: A large-scale empirical evaluation&amp;lt;/u&amp;gt;. 8th International Conference on Artificial Intelligence in Education, Kobe, Japan, IOS Press. p. 23--30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>371560</ref_obj_id>
				<ref_obj_pid>371552</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Lester, J. C., S. G. Towns, et al. (2000). Deictic and Emotive Communication in Animated Pedagogical Agents. &amp;lt;u&amp;gt;Embodied Conversational Agents&amp;lt;/u&amp;gt;. J. Cassell, S. Prevost, J. Sullivant and E. Churchill. Boston, MIT Press: 123--154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Luckin, R. (1998). 'Ecolab': Explorations in the Zone of Proximal Development,. &amp;lt;u&amp;gt;School of Cognitive &amp; Computer Sciences&amp;lt;&amp;lt;/u&amp;gt;. Brighton, UK, University of Sussex.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Malone, T. and M. Lepper (1987). Making learning fun. &amp;lt;u&amp;gt;Aptitude, Learning and Instruction: Conative and Affective Process Analyses&amp;lt;/u&amp;gt;. R. Snow and M. Farr, Lawrence Erlbaum: 223--253.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Papert, S. (1983). &amp;lt;u&amp;gt;The Children's Machine&amp;lt;/u&amp;gt;, Basic Books.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Rebolledo-Mendez, G., S. de Freitas, et al. (2008). &amp;lt;u&amp;gt;A model of motivation for virtual world avatars&amp;lt;/u&amp;gt;. Eight International Conference on Intelligent Virtual Agent, IVA 2008, Tokyo, Japan, Springer-Verlag p. 535--536.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Tudge, J. R. H. (1992). "Processes and consequences of peer collaboration: A vygotskian analysis." &amp;lt;u&amp;gt;Child development&amp;lt;/u&amp;gt; &#60;b&#62;63&#60;/b&#62;: 1364--1379.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>337537</ref_obj_id>
				<ref_obj_pid>336595</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Yoon, S.-Y., B. M. Blumberg, et al. (2000). &amp;lt;u&amp;gt;Motivation Driven Learning for Interactive Synthetic Characters&amp;lt;/u&amp;gt;. Fourth International Conference on Autonomous Agents, Barcelona.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1581077</article_id>
		<sort_key>40</sort_key>
		<display_label>Pages</display_label>
		<article_publication_date>08-04-2009</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Using influence and persuasion to shape player experiences]]></title>
		<page_from>23</page_from>
		<page_to>30</page_to>
		<doi_number>10.1145/1581073.1581077</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1581077</url>
		<abstract>
			<par><![CDATA[<p>In this paper we present a pilot study on the use of <i>influence</i> to guide player behavior in an interactive storytelling environment. We discuss the need for authoring support in drama management systems and present <i>computational models of influence</i> as a method to meet that need. To investigate some of the benefits of using influence, we implement an authoring tool and web-based mixed-media choose-your-own-adventure-style storytelling system, using it to conduct a pilot study. The pilot study is based on hand-authored static content, but the results indicate promise for generalizing to dynamic content. Further, we provide an analysis of the data that indicates the potential effectiveness of <i>influence statements</i> on affecting player decisions and comment on players' perceptions of self-agency with or without those statements.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor>Modeling methodologies</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.3.5</cat_node>
				<descriptor>Web-based services</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003260.10003282</concept_id>
				<concept_desc>CCS->Information systems->World Wide Web->Web applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010343</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Modeling methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003260.10003304</concept_id>
				<concept_desc>CCS->Information systems->World Wide Web->Web services</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1570304</person_id>
				<author_profile_id><![CDATA[81318491081]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Roberts]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570305</person_id>
				<author_profile_id><![CDATA[81100274403]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Merrick]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Furst]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570306</person_id>
				<author_profile_id><![CDATA[81100494006]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dorn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570307</person_id>
				<author_profile_id><![CDATA[81435595392]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Charles]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Isbell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ariely, D. 2008. <i>Predictably Irrational: The Hidden Forces That Shape Our Decisions</i>. Harper Collins.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1329367</ref_obj_id>
				<ref_obj_pid>1329125</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bhat, S., Roberts, D. L., Nelson, M. J., Isbell, C. L., and Mateas, M. 2007. A globally optimal algorithm for TTD-MDPs. In <i>Proceedings of the Sixth International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS07)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Cialdini, R. B. 1998. <i>Influence: The Psychology of Persuasion</i>. Collins.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280797</ref_obj_id>
				<ref_obj_pid>280765</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Doyle, P., and Hayes-Roth, B. 1998. Agents in annotated worlds. In <i>AGENTS '98: Proceedings of the Second International Conference on Autonomous Agents</i>, 173--180.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Fleiss, J. L., Levin, B., and Paik, M. C. 2003. <i>Statistical Methods for Rates and Proportions</i>, 3 ed. John Wiley &amp; Sons.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Laurel, B. 1986. <i>Toward the Design of a Computer-Based Interactive Fantasy System</i>. PhD thesis, Drama Department, Ohio State University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Likert, R. 1932. A technique for the measurement of attitudes. <i>Archives of Psychology 140</i>, 1--55.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Mateas, M., and Stern, A. 2005. Structuring content in the Fa&amp;#231;ade interactive drama architecture. In <i>Proceedings of Artificial Intelligence and Interactive Digital Entertainment (AIIDE05)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1805762</ref_obj_id>
				<ref_obj_pid>1805750</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Mateas, M. 1999. An Oz-centric review of interactive drama and believable agents. In <i>AI Today: Recent Trends and Developments. Lecture Notes in AI 1600</i>, M. Woodridge and M. Veloso, Eds. Springer, Berlin, NY. First appeared in 1997 as Technical Report CMU-CS-97-156, Computer Science Department, Carnegie Mellon University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2175754</ref_obj_id>
				<ref_obj_pid>2175737</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Medler, B., and Magerko, B. 2006. Scribe: A general tool for authoring interactive drama. In <i>Proceedings of the Third International Conference on Technologies for Interactive Digital Storytelling and Entertainment</i>, 139--150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1160808</ref_obj_id>
				<ref_obj_pid>1160633</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Mott, B. W., and Lester, J. C. 2006. U-director: a decision-theoretic narrative planning architecture for storytelling environments. In <i>Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS06)</i>, ACM Press, New York, NY, USA, 977--984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1160769</ref_obj_id>
				<ref_obj_pid>1160633</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Nelson, M. J., Roberts, D. L., Isbell, C. L., and Mateas, M. 2006. Reinforcement learning for declarative optimization-based drama management. In <i>Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS06)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Riedl, M. O., Stern, A., Dini, D., and Alderman, J. 2008. Dynamic experience management in virtual worlds for entertainment, education, and training. <i>International Transactions on Systems Science and Applications, Special Issue on Agent Based Systems for Human Learning 4</i>, 2.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Roberts, D. L., and Isbell, C. L., 2008. A survey and qualitative analysis of recent advances in drama management. <i>International Transactions on Systems Science and Applications, Special Issue on Agent Based Systems for Human Learning 4</i>, 2.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Roberts, D. L., Isbell, C. L., Riedl, M. O., Bogost, I., and Furst, M. L. 2008. On the use of computational models of influence for interactive virtual experience management. In <i>Proceedings of the First International Conference on Interactive Digital Storytelling (ICIDS08)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>925491</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Weyhrauch, P. 1997. <i>Guiding Interactive Drama</i>. PhD thesis, School of CS, CMU, Pittsburgh, PA. Tech Rept CMU-CS-97-109.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Young, R. M., Riedl, M. O., Branly, M., and Jhala, A. 2004. An architecture for integrating plan-based behavior generation with interactive game environments. <i>Journal of Game Development 1</i>, 1, 54--70.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1581078</article_id>
		<sort_key>50</sort_key>
		<display_label>Pages</display_label>
		<article_publication_date>08-04-2009</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Game design strategies for collectivist persuasion]]></title>
		<page_from>31</page_from>
		<page_to>38</page_to>
		<doi_number>10.1145/1581073.1581078</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1581078</url>
		<abstract>
			<par><![CDATA[<p>A fundamental feature of serious games is persuasion, an attempt to influence behaviors, feelings, or thoughts. Much of the existing research on serious games and, more generally, on persuasive technology (PT), does not address the important links between persuasion and culture. It has tended to originate from Western, individualist cultures, and has focused on how to design for these audiences. In this paper, we describe the design of one of two versions of a serious game we developed about quitting smoking titled <i>Smoke?</i> which is targeted at collectivist players. We show how the design was informed by persuasive strategies we identified from the cross-cultural psychology literature, intended for use in games for players of collectivist cultures: HARMONY, GROUP OPINION, MONITORING, DISESTABLISHING, and TEAM PERFORMANCE. We then discuss the results of a quantitative investigation of the effects of both game versions on both individualist and collectivist players.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[culture]]></kw>
			<kw><![CDATA[game design]]></kw>
			<kw><![CDATA[persuasion]]></kw>
			<kw><![CDATA[serious games]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Software psychology</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010216.10010217</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Philosophical/theoretical foundations of artificial intelligence->Cognitive science</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10011748</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Empirical studies in HCI</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1570308</person_id>
				<author_profile_id><![CDATA[81100115736]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rilla]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Khaled]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carleton University, Ottawa, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570309</person_id>
				<author_profile_id><![CDATA[81100069860]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pippin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carleton University, Ottawa, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570310</person_id>
				<author_profile_id><![CDATA[81100135223]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Biddle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carleton University, Ottawa, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570311</person_id>
				<author_profile_id><![CDATA[81100312099]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ronald]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fischer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Victoria University of Wellington, Wellington, New Zealand]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570312</person_id>
				<author_profile_id><![CDATA[81100588708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Noble]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Victoria University of Wellington, Wellington, New Zealand]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Aaker, J., and Maheswaran, D. 1997. The effect of cultural orientation on persuasion. <i>The Journal of Consumer Research 24</i>, 3 (December), 315--328.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1214315</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bogost, I. 2007. <i>Persuasive Games: The Expressive Power of Videogames</i>. The MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bramley, D., Riddell, T., Whittaker, R., Corbett, T., Lin, R.-B., Wills, M., Jones, M., and Rodgers, A. 2005. Smoking cessation using mobile phone text messaging is as effective in M&amp;#227;ori as non-M&amp;#227;ori. <i>The New Zealand Medical Journal 118</i>, 1216 (June).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Burke, J. A., Salazaar, A., Daughety, V., and Becker, S. L. 1992. Activating interpersonal influence in the prevention of adolescent tobacco use. <i>Health Communication 4</i>, 1, 1--17.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1316476</ref_obj_id>
				<ref_obj_pid>1316471</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Damron, R. L., and Halleck, G. B. 2007. Generating cross-cultural training data for the university game. <i>Simulation &amp; Gaming 38</i>, 4, 556--568.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[de Mooij, M. 2005. <i>Global Marketing and Advertising: Understanding Cultural Paradoxes</i>. Sage Publications.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Dijkstra, A., de Vries, H., Kok, G., and Roijackers, J. 1999. Self-evaluation and motivation to change. <i>Psychology and Health 14</i>, 747--759.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Evers, V. 2001. <i>Cultural Aspects of User Interface Understanding: An Empirical Evaluation of an E-Learning Website by International User Groups</i>. PhD thesis, Institute of Educational Technology, The Open University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Festinger, L. 1954. A theory of social comparison. <i>Human Relations 7</i>, 117--140.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>640634</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Fogg, B. J. 2003. <i>Persuasive Technology: Using Computers to Change What We Think and Do</i>. Morgan Kaufmann Publishers.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Frasca, G., Battegazzore, S., Olhaberry, N., Infantozzi, P., Rodriguez, F., and Balbi, F., 2003. September the 12th. http://www.newsgaming.com/games/index12.htm.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Gudykunst, W. B., and Matsumoto, Y. 1996. Cross-cultural variability of communication in personal relationships. In <i>Communication in Personal Relationships Across Cultures</i>, W. B. Gudykunst, S. Ting-Toomey, and T. Nishida, Eds. Sage Publications, CA, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Hall, E. 1976. <i>Beyond Culture</i>. Anchor Press/Doubleday.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Higgin, T. 2009. Blackless Fantasy: The Disappearance of Race in Massively Multiplayer Online Role-Playing Games. <i>Games and Culture 4</i>, 1, 3--26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Hofstede, G. 1996. <i>Cultures and organisations: Software of the mind</i>. McGraw-Hill Education.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Hofstede, G. J. 2008. One game does not fit all cultures. In <i>Why do games work? In search of the active substance</i>, L. de Caluw, G. J. Hofstede, and V. Peters, Eds. Kluwer, 69--77.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Khaled, R., Noble, J., and Biddle, R. 2005. Developing culturally-aware persuasive technology. In <i>Proceedings of the 7th International Workshop on Internationalisation of Products and Systems</i>, E. D. Gado, Ed.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1228213</ref_obj_id>
				<ref_obj_pid>1228175</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Khaled, R., Barr, P., Fischer, R., Biddle, R., and Noble, J. 2006. Factoring culture into the design of a persuasive game. In <i>Proceedings of OzCHI 2006</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Kleinjan, M., van den Eijnden, R. J. J. M., Dijkstra, A., Brug, J., and Engels, R. C. M. E. 2006. Excuses to continue smoking: The role of disengagement beliefs in smoking cessation. <i>Addictive Behaviours 31</i>, 2223--2237.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345238</ref_obj_id>
				<ref_obj_pid>345190</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Marcus, A., and Gould, E. 2000. Crosscurrents: cultural dimensions and global web user-interface design. <i>Interactions 7</i>, 4, 32--46.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Markus, H., and Kitayama, S. 1991. Culture and the self: Implications for cognition, emotion, and motivation. <i>Psychological Review 98</i>, 224--253.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Niaura, R., Abrams, D. B., Shadel, W. G., Rohsenow, D. J., Monti, P. M., and Sirota, A. D. 1999. Cue exposure treatment for smoking relapse prevention: A controlled clinical trial. <i>Addiction 94</i>, 5, 685--695.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Patterson, J. 1992. <i>Exploring M&amp;#227;ori Values</i>. Dunmore Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Pavlou, P., and Chai, L. 2002. What drives electronic commerce across cultures? A cross-cultural empirical investigation of the theory of planned behaviour. <i>Journal of Electronic Commerce Research 3</i>, 4, 240--253.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Schoeffel, P., Meleisea, M., David, R., Kalauni, R., Kalolo, K., KingiTemaleti, P., Taumoefolau, Vuetibau, L., and Williams, S. P. 1994. Spare the rod? Conflicting cultural models of the family and approaches to child socialisation in New Zealand. Tech. Rep. 1, Centre of Pacific Studies, University of Auckland.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Schwartz, S. 1992. Universals in the content and structure of values: Theoretical advances and empirical tests in 20 countries. In <i>Advances in experimental social psychology</i>, M. P. Zanna, Ed., vol. 25. New York: Academic Press, 1--65.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Statistics New Zealand, 2002. 2001 Census. http://www.stats.govt.nz/, 25 June.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Tafarodi, R. W., and Walters, P. 1999. Individualism-collectivism, life events, and self-esteem: a test of two trade-offs. <i>European Journal of Social Psychology 29</i>, 5 (June), 797--814.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Triandis, H. C., Bontempo, R., Villareal, M. J., Asai, M., and Lucca, N. 1988. Individualism and collectivism: Cross-cultural perspectives on self-ingroup relationships. <i>Journal of Personality and Social Psychology 54</i>, 2, 323--338.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Triandis, H. C. 1995. <i>Individualism and Collectivism (New Directions in Social Psychology)</i>. Westview Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[United States Army, 2002. America's army: The official army game. http://www.americasarmy.com/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[V&amp;#246;hringer-Kuhnt, T. 2001. <i>The Influence of Culture on Usability</i>. Master's thesis, Department of Educational Sciences and Psychology, Freie Universit&amp;#228;t Berlin, Berlin, Germany.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Webster, A. 2001. <i>Spiral of Values</i>. Alpha Publications.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Game Design Strategies for Collectivist Persuasion Rilla Khaled*, Pippin Barr , Robert Biddle Ronald 
Fischer§, James Noble¶ Carleton University Victoria University of Wellington Ottawa, Canada Wellington, 
New Zealand Abstract was informed by a set of design strategies we identi.ed from the cross-cultural 
psychology literature relating to the bipolar dimen-A fundamental feature of serious games is persuasion, 
an attempt sion of individualism collectivism. We de.ne individualism and to in.uence behaviors, feelings, 
or thoughts. Much of the existing collectivism as follows: research on serious games and, more generally, 
on persuasive tech- Individualist societies are those in which the ties between individu­nology (PT), 
does not address the important links between persua­als are loose: people are expected to look after 
themselves and their sion and culture. It has tended to originate from Western, individu­immediate family 
only. Individual interests outweigh group inter­alist cultures, and has focused on how to design for 
these audiences. ests, and individualists tend to be self-motivated and goal-oriented, In this paper, 
we describe the design of one of two versions of a using guilt and loss of self-respect as motivators 
[Hofstede 1996]. serious game we developed about quitting smoking titled Smoke? Individualists also exhibit 
more attitude-behaviour consistency than which is targeted at collectivist players. We show how the design 
collectivists, perform their duties if it is advantageous in terms of was informed by persuasive strategies 
we identi.ed from the cross­bene.t, and have a self-identity that is de.ned independently of cultural 
psychology literature, intended for use in games for players speci.c collectives [Triandis 1995]. of 
collectivist cultures: HARMONY, GROUP OPINION, MONITOR-ING, DISESTABLISHING, and TEAM PERFORMANCE. We 
then dis- In contrast, collectivist societies are ones in which, from birth on­cuss the results of a 
quantitative investigation of the effects of both wards, people are integrated into strong, cohesive 
ingroups, which game versions on both individualist and collectivist players. are groups of people about 
whose welfare a person is concerned, and separation from whom leads to anxiety. These ingroups protect 
CR Categories: K.8.0 [PERSONAL COMPUTING]: General their members in exchange for unquestioning loyalty. 
Group inter-Games; H.1.2 [INFORMATION SYSTEMS]: User/Machine ests outweigh individual interests, and 
individuals strive to main­Systems Software psychology. tain social harmony, adapt skills and virtues 
necessary for being a good group member and maintaining tradition. Shame and loss of Keywords: Game 
design, serious games, persuasion, culture. face are typical motivators [Hofstede 1996]. Collectivists 
exhibit less attitude-behaviour consistency than individualists, enjoy doing what is right for their 
collective, and have self identities that are 1 Introduction strongly linked to attributes of their group 
[Triandis 1995]. A central feature of many serious games is persuasion, an attempt to In reality, the 
issue of cultural identity is more complex than this in.uence behaviors, feelings, or thoughts. Serious 
games share im-categorisation indicates, and individuals may have multiple cultural portant common ground 
with persuasive games, videogames that identities. The individualism collectivism dimension does, how­mount 
procedural rhetorics effectively [Bogost 2007], and persua-ever, serve as a useful tool for conceptualising 
the general belief sive technology (PT), any interactive computing system designed and behavioural patterns 
of particular groups. to change people s attitudes or behaviors [Fogg 2003]. Attitudes In the upcoming 
sections of this paper, we overview existing related and behaviours are often heavily in.uenced by culture 
[Hofstede research, we present our game design strategies for collectivist per­1996; Triandis 1995], 
which we view as shared learned patterns suasion alongside the collectivist version of our game Smoke? 
andof beliefs and behaviours that govern how an individual interacts we discuss the results of a quantitative 
evaluation of the effects of with others and the environment. Very little research on serious the collectivist 
and individualist versions of Smoke? on players. games or persuasive technologies, however, focuses 
on the impor­tant role that culture plays in persuasion. Additionally, this research has tended to originate 
from Western cultures, and provides little 2 Background direction for how to design for non-Western cultures. 
The notion of culture is touched on in a number of serious games. In this paper, we present the design 
of one of two versions of a For example, September 12th uses a simulation mechanic to ex­web-based Flash 
game we developed called Smoke? about smok­press one view of the spread of terrorism contextualised within 
a ing cessation, targeted at M¯aori1 players. We discuss how its design Middle Eastern setting [Frasca 
et al. 2003]. The game has limited interactive possibilities, thus it is not possible to divorce the 
rhetoric *e-mail: rilla.khaled@gmail.com of the spread of terrorism from its cultural context: conceivably 
its e-mail: pippin.barr@gmail.com makers intended to create exactly this connection in the minds of e-mail: 
robert biddle@carleton.ca players. America s Army is another game with an underlying mes­ §e-mail: ronald..scher@vuw.ac.nz 
¶e-mail: kjx@ecs.vuw.ac.nz sage closely tied to culture [United States Army 2002], but focuses rather 
on value transference to its players, as it re.ects army cul­ 1M¯aori are the indigenous people of New 
Zealand. ture and emphasises the importance of army values. Within the Copyright &#38;#169; 2009 by the 
Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of 
this work for personal or serious games literature, researchers have also examined represen­classroom 
use is granted without fee provided that copies are not made or distributed tations and conceptualisations 
of race in games such as Everquest for commercial advantage and that copies bear this notice and the 
full citation on the and World of Warcraft [Higgin 2009]. What appears to be under­first page. Copyrights 
for components of this work owned by others than ACM must be investigated at present are games designed 
to harness underlying honored. Abstracting with credit is permitted. To copy otherwise, to republish, 
to post on motivations of their target players cultures to facilitate persuasion. servers, or to redistribute 
to lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, 
ACM Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org. Even in more general non-game-based PT, 
culture is rarely a de-Sandbox 2009, New Orleans, Louisiana, August 4 6, 2009. &#38;#169; 2009 ACM 978-1-60558-514-7/09/0008 
$10.00 sign factor. Almost all of the explicit PT research originates from countries classi.ed as individualist, 
and focuses on tools destined for individualist audiences. In the domain of smoking cessation tools, 
we encountered just one project addressing culture, STOMP: Stop Smoking With Mobile Phones. STOMP was 
initiated in New Zealand (NZ), and focused on providing its users with cessation­related support via 
mobile phone SMS messages [Bramley et al. 2005]. Its researchers designed one set of messages for NZ 
Euro­pean participants, and another for M¯aori participants. Along with standard quitting advice, the 
M¯aori set of messages made use of M¯aori language and referenced M¯aori traditions. Although cul­ture 
played an important role in STOMP, language cues were the only means of cultural differentiation between 
the two culturally­targeted services [Bramley et al. 2005]. Its researchers did not look at how culture 
could have been harnessed to inform tool interaction. In contrast, the simulation literature features 
more work directly pertaining to cross-cultural differences. Certain researchers have looked at how people 
of particular kinds of cultures are likely to respond to simulation games [Hofstede 2008], while others 
have developed (mainly analog) simulation games to serve as intercul­tural training tools [Damron and 
Halleck 2007]. While the goal of these games is generally to educate participant-viewers about cul­tures 
and culturally-rooted behaviours, they tend not to incorporate culture in a more general role as a rule 
system from which other issues, such as health, are explored. In the HCI domain, cultural dimensions, 
aspects of culture that can be measured relative to other cultures, especially those proposed by the 
sociologist Geert Hofstede, have proved to be popular tools for reasoning about culture and technology 
[Hofstede 1996]. One of the early uses of Hofstede s dimensions was Aaron Marcus s anal­ysis of websites, 
in which he demonstrated how aspects of web­site design from different countries supported what the dimensions 
proposed about their national cultures [Marcus and Gould 2000]. Other HCI investigations relying on cultural 
dimensions includes Vanessa Ever s research into cross-cultural understanding of inter­face metaphors 
[Evers 2001], and Thomas V¨ohringer-Kuhnt s in­vestigation into the in.uence of culture on people s perceptions 
of usability [V¨ohringer-Kuhnt 2001]. Although there are a number of such research initiatives focused 
on cultural aspects of HCI, this literature does not focus on the relationship between culture and persuasion. 
It has tended to focus on productivity applications over persuasive ones. Applications under investigation 
have therefore not been designed to explicitly shape people s attitudes through use. Culture is a pervasive 
factor in day-to-day life, and by extension plays a part in shaping the design preferences of game and 
PT de­signers, and the strategies they use and embed in their products. In making design decisions, designers 
constantly make culturally­based value judgements about what they personally .nd persuasive, what they 
believe their target audience .nds persuasive, and which persuasion motivations to foreground within 
their designs. Some­times designers make these decisions consciously, and at other times unconsciously, 
but undoubtedly, decisions are always made. While it may be exceptionally dif.cult for designers to identify 
cul­tural preferences that they have unknowingly embedded into their technologies, these ideals and values 
will resurface when the tech­nologies are used by audiences the designers were not considering. As the 
anthropologist Edward Hall explains: The only time one is aware of the control system is when things 
don t follow the hidden program. This is most frequent in intercul­tural programs [Hall 1976]. Hall was 
referring here to the hidden internalised cultural pro­grams people operate with, that form integral 
parts of our person­alities [Hall 1976]. At the same time, his words seem to carry par-  Figure 1: The 
concentration-sandwich minigame from Smoke? ticular resonance for game players and technology users who 
have felt dissatisfaction caused by mismatched assumptions about their identity, their knowledge base, 
their behaviour, or their typical in­teraction patterns. This consequence should concern designers of 
serious games and general PT, as making users uncomfortable is unlikely to be conducive to attitude and 
behavioural change. In recognising and identifying cultural differences between users, and differences 
in perception of persuasiveness, we are in a stronger position to identify the best ways to trigger persuasion 
in games and tools. Referring again to Hall s hidden program , if the cultural as­sumptions of a serious 
game or PT match those of its users, their attention is then able to focus on the informational content 
related directly to attitude and behaviour change. In short, this might in­crease their potential effectiveness. 
 3 Collectivism focused persuasive design strategies To investigate the role of culture in persuasion, 
we developed two versions of a serious game promoting smoking cessation titled Smoke? One of the versions 
was designed for M¯aori players (the NZM game), as M¯aori are traditionally classi.ed as collectivist 
[Patterson 1992]. The other version was designed for NZ Euro­pean players (the NZE game), who are classi.ed 
as individualist [Hofstede 1996; Triandis 1995]. Both versions feature a character, MC, who can be male 
or female, and who has just decided to quit smoking. The game represents the next six weeks of MC s life, 
and the player can choose to help or hinder MC s attempt to quit. The design of the NZM game was informed 
by a set of collectivist persuasive design strategies, which we discuss next. We note that the design 
of the NZE game, which we addressed in another pub­lication [Khaled et al. 2006], was not informed by 
these strategies. Table 1 summarises the differences between the two game versions. Our collectivism 
focused persuasive design strategies were synthe­sised from a number of sources. These include the cross-cultural 
psychology literature on the behavioural and motivational differ­ences between individualists and collectivists, 
cross-cultural con­sumer psychology research, interviews with NZ social marketing practitioners of both 
M¯aori and NZ European origin, and Fogg s PT strategies [Fogg 2003]. We performed a conceptual textual 
analysis on Fogg s strategies to determine cultural biases, using as an an­  Minigames Individual context 
Social context Relationship Lesser involvement and Greater involvement with NPCs emotional dependence 
and emotional depen­ dence Advantages Major focus on effects Focus on self and ef­ of quitting for MC 
fects on MCs family and friends Disadvantages Major focus on effects Focus on self and ef­ of smoking 
for MC fects on MCs family and friends Table 1: Key differences between NZE and NZM games alytical context 
the literature .ndings on motivational differences between individualists and collectivists. The analysis 
indicated that Fogg s PT strategies are motivationally better suited towards indi­vidualist cultures 
[Khaled et al. 2005]. Many of these strategies are designed for use in the context of in­group goals. 
They are premised on the assumption that ingroup members are supportive of these goals and would willingly 
share personal information with other members. We suggest that this is likely to be the case for collectivist 
users. While any number of circumstances may lead to ingroup members deciding that they do not want to 
pursue an ingroup goal, a fundamental characteristic of collectivists is the prioritisation of group 
goals over personal ones [Hofstede 1996; Markus and Kitayama 1991; Triandis 1995]. 3.1 The HARMONY strategy 
Antecedents Collectivists often choose to live in close proxim­ity with their ingroup members [Patterson 
1992; Triandis 1995]. Socialisation in collectivist cultures emphasises interdependence and the importance 
of acting to preserve harmony and facilitate co-operation [Gudykunst and Matsumoto 1996; Hofstede 1996; 
Markus and Kitayama 1991; Triandis 1995]. The drive to retain a harmonious atmosphere even impacts on 
con.ict resolution as collectivists will avoid direct confrontation unless it is absolutely necessary 
[Gudykunst and Matsumoto 1996; Markus and Kitayama 1991; Triandis 1995]. Related to harmony is the drive 
to act in the best interests of the group. Both our interview data and the cross­cultural literature 
support the notion that collectivists give prece­dence to ingroup interests and responsibilities over 
personal ones [Gudykunst and Matsumoto 1996; Hofstede 1996; Triandis 1995]. Description The HARMONY strategy 
involves presenting social density cues to users. The cues serve to suggest to users that they are in 
a socially dense environment with members of their ingroup, in order to promote harmonious actions that 
support group goals. Application: social minigames The NZM and NZE versions of Smoke? both feature minigames, 
one of which tests MC s ability to concentrate. While the NZE minigames are designed to draw atten­tion 
to MC in an individual context, the NZM minigames are de­signed to highlight MC s progress (or lack of 
progress) in a highly visible social context. This context is peopled with others he cares about and 
whose opinions matter to him. The NZM concentra­tion minigame requires the player to help MC make sandwiches 
for other family members, who watch and react to MC (.gure 1). In this scenario, we intended players 
to feel motivated by their re­sponsibility towards other members of MC s ingroup2. Consequences The HARMONY 
strategy reminds users of how they can act in the group s interests, and assist in maintaining group 
harmony. Users are then motivated to make decisions and act based on supporting the group s goals as 
far as possible. There are in­evitably contexts in which users decide to go against the wishes of the 
ingroup. This strategy only applies to situations in which indi­viduals have already committed to supporting 
ingroup goals. 3.2 The GROUP OPINION strategy Antecedents Collectivists place more importance on the 
opin­ions of others in their ingroup than do individualists when making decisions [de Mooij 2005; Hofstede 
1996; Markus and Kitayama 1991; Triandis 1995]. They are more likely to be concerned about acting in 
the best interests of the group over their own interests, and group interests might change according 
to the situation [de Mooij 2005; Hofstede 1996; Markus and Kitayama 1991; Triandis 1995]. As a result, 
the opinions of others often have a strong shaping in­.uence on their decisions [Aaker and Maheswaran 
1997; Pavlou and Chai 2002]. Research has also shown that collectivists tend to be less effective if 
they feel they are performing an activity in isola­tion, while being more effective in group situations 
[Hofstede 1996; Triandis 1995]. Accessing the opinions of other ingroup members is not always feasible, 
however, and can be time consuming. Description The GROUP OPINION strategy involves providing users with 
the opinions of other ingroup members or users simi­lar to them3 at moments when they are required to 
make important decisions related to their own goals. Application: the moods + opinions box In the NZM 
version of Smoke? the moods + opinions box contains iconic representa­tions of three family members and 
one friend. As players progress through the game narrative and interactions, each of the charac­ter icons 
facial expressions may change in accordance with their current mood. Their mood is determined by how 
they individually perceive MC s choices. In addition to expressing moods, each char­acter icon can express 
opinions on MC s actions. The offering of an opinion is indicated by the character icon .ashing. These 
opin­ions typically appear when players are required to make a decision on behalf of MC. By clicking 
on a character icon, players can view that character s opinion about what MC should do. Consequences 
The GROUP OPINION strategy partially simu­lates the experience of making a decision by consulting other 
in­group members. Being reminded of the existence of the group when making decisions may help an isolated 
group member feel more supported and speed up the decision making process. Further, if the decision is 
one that concerns a group interest, the decision maker is better informed about how to make a decision 
that best suits every­one s interests. Finally, social comparison theory shows that people of any culture 
tend to be naturally interested in the opinions and progress of others as they are constantly benchmarking 
their own behaviours against those of others [Festinger 1954].  3.3 The MONITORING strategy Antecedents 
Collectivists are accustomed to the constant pres­ence of other members of their ingroup [de Mooij 2005; 
Hofstede 1996; Markus and Kitayama 1991; Triandis 1995]. This puts the ingroup in the position of observing 
other group members actions 2This scenario was directly inspired by comments from participants in a 
3The similarity might be based on shared social networks, careers, M¯aori focus group about perceptions 
of smoking. schools, socio-economic backgrounds, or interests. Figure 2: Gran s opinion of MC taking 
Arihia s phone number and behaviour in what is effectively passive surveillance. In West­ern cultures, 
surveillance has negative connotations, while monitor­ing of one s own behaviours is perceived as acceptable. 
In contrast, collectivists are adjusted to surveillance by their ingroup members. Given their interdependence 
with the group members, their identi­ties are informed by the ingroup identity. Further, collectivists 
are more effective in group situations as compared to ones in which they work alone [Hofstede 1996; Triandis 
1995]. In addition, the constant presence of the ingroup acts as a reminder of expected behaviour because 
behaviour in collectivist cultures tends to be situationally dependent, and motivated by collective responsibility 
[de Mooij 2005; Markus and Kitayama 1991; Triandis et al. 1988]. Finally, collectivists are known to 
measure their own performance through the assessments of other ingroup members [Hofstede 1996; Markus 
and Kitayama 1991; Triandis et al. 1988]. Description The MONITORING strategy involves tracking be­haviour 
that users wish to change and making this information available to other trusted group members. In appropriate 
situations, the group members act as mentors, using the information to support users and keep them motivated 
to change their behaviour. Application: Jade s status The NZM version of Smoke? promi­nently features 
one character that the NZE version does not: Jade, MC s younger sister. Jade spends large amounts of 
time with MC and views MC as a role model. Jade has her own status screen, which details her likelihood 
of being adversely affected by smok­ing. Jade s status is directly related to MC s actions because what 
MC does in Jade s company is re.ected in Jade s future attitudes and smoking likelihood. While this is 
a simpli.ed cause-and-effect relationship, we anticipated that awareness of being monitored by Jade would 
trigger players into making responsible choices. Consequences The MONITORING strategy harnesses the situ­ationally 
dependent nature of collectivist behaviour [Gudykunst and Matsumoto 1996; Markus and Kitayama 1991; Triandis 
et al. 1988]. The context becomes one in which users are aware that they are being monitored by other 
ingroup members, and they act ac­cordingly. The role of a mentor is to serve as a positive in.uence for 
the person being monitored, and to track and re.ect this person s progress. In addition, although users 
are being monitored by oth­ers, the close level of identi.cation between users and their mentors means 
that the monitoring is more akin to monitoring one s own behaviour. An ethical issue related to any surveillance 
concerns in­dividuals or groups obtaining information about other individuals or groups. The issue is 
mitigated, however, if the individual under watch welcomes the information sharing, which is more likely 
to be the case for collectivists and their ingroup member mentors. 3.4 The DISESTABLISHING strategy 
Antecedents The use of praise in individualist cultures is a widespread parenting practice [Triandis 
1995]. It is linked to the notion of self-esteem, a prominent concept in individualist cultures, but 
less important and relevant in collectivist ones [de Mooij 2005; Markus and Kitayama 1991; Tafarodi and 
Walters 1999; Triandis 1995]. Praise is less common in collectivist cultures, and has been associated 
with leading to excessive pride in individuals [Schoeffel et al. 1994; Triandis 1995]. In terms of operant 
conditioning for teaching behaviour, rather than the use of positive reinforcement, a noted characteristic 
of collectivist cultures is the use of punishment [Schoeffel et al. 1994; Triandis 1995]. While positive 
reinforce­ment focuses on encouraging behaviours, punishment focuses on weakening behaviours. Description 
The DISESTABLISHING strategy concerns training users out of practising speci.c actions or behaviours 
that they do not want to perform. It focuses on the weakening of undesirable behaviours by triggering 
self-selected reminder cues when the un­desirable behaviour is detected. Application: telling off MC 
s best friend Research on smok­ing cessation campaigns has shown that smokers often feel targeted by 
negatively-framed messages and begin adopting disengagement beliefs [Kleinjan et al. 2006]. We were wary 
of making players feel they were being reprimanded. To avert this reaction, the NZM game features an 
aunt character who is an advocate for smoke-free lifestyles and critical of other smokers in the NZM 
game, but who does not overtly direct her criticism at MC. For example, in one par­ticular scene when 
MC s best friend begins smoking at a family pic­nic, the aunt reproaches the best friend for smoking 
in Jade s pres­ence. We hoped that in witnessing this exchange, players would be able to think more objectively 
about negative consequences with­out MC featuring as the target of disapproval. A further justi.ca­tion 
for this indirect approach is that collectivist cultures tend to be high context, meaning that people 
are closely observant of each other and read much underlying meaning into peoples words and actions [Gudykunst 
and Matsumoto 1996; Hall 1976; Markus and Kitayama 1991; Triandis et al. 1988]. Accordingly, we hoped 
that players might observe the experiences of the other characters, and apply their learnings to MC and 
potentially even themselves. Consequences The DISESTABLISHING strategy focuses on weakening undesirable 
behaviours. The cross-cultural psychology literature shows that collectivists learn proper conduct by 
focusing on improper conduct [Markus and Kitayama 1991; Triandis 1995]. In addition, collectivist cultures 
tend to be tight, meaning that they feature many rigid rules regarding conduct [Triandis 1995]. From 
a Western perspective, this strategy may seem ethically ambiguous [Fogg 2003]. Rather than arguing for 
the need to punish, however, we are arguing for the need to focus on undesirable behaviours that users 
themselves wish to weaken. Furthermore, it is possible to do so without punishment, but by relying on 
reminder cues. It is important that users are given the option of establishing the form and frequency 
of the reminder they wish to receive, along with the ability to disable or modify the reminders at any 
point. Figure 3: Convincing MC s best friend to quit  3.5 The TEAM PERFORMANCE strategy Antecedents 
Collectivists self identities are to a large extent in­formed by the ingroup identity [Hofstede 1996; 
Markus and Ki­tayama 1991; Triandis 1995]. A close level of interdependence with the group also means 
that members are motivated to act in ways that uphold the ingroup s reputation, or at the very least, 
min­imise bringing shame on the ingroup. As surveillance is currently used in the workplace or in public 
areas, it tends to track the be­haviour of individuals, or perhaps a group of individuals, but gen­erally 
results in either the promise of rewards or punishment for the individual [Fogg 2003]. Collectivists, 
however, not only tend to be more effective in group situations, but are more concerned with contributing 
towards ingroup goals than individual goals [Hofstede 1996; Markus and Kitayama 1991; Triandis 1995]. 
Description The TEAM PERFORMANCE strategy concerns track­ing the behaviour of individuals constituting 
a group. It rewards or reprimands all members of the group on the basis of the actions of each individual 
that are related to a group goal. Application: the convince challenge In the NZM version of Smoke? if 
players are largely successful in keeping MC smoke-free, they are presented with an additional challenge: 
to convince MC s best friend to consider quitting smoking by choosing the correct combination of four 
cessation arguments. Figure 3 shows a screen­shot from this challenge. In a real world scenario, we are 
aware that the success or failure of MC convincing the best friend to quit would in fact have the greatest 
impact on the best friend, but, in a collectivist culture, other ingroup members would also share in 
the best friend s successes and failures, as they would impact on the group overall. In the spirit of 
group solidarity, if players succeed in presenting a successful combination of cessation arguments to 
the best friend, the best friend will consider quitting. As a result, MC will bene.t from a boost in 
mental well-being, re.ecting a connec­tion between her well-being and the best friend s well-being. Consequences 
The TEAM PERFORMANCE strategy shifts the locus of the persuasion goal to a group level. It reminds individuals 
who identify with an ingroup that their combined actions have con­sequences for the goal outcomes of 
the whole group. By seeing how their actions can lead to bene.ts or disadvantages for the ingroup, users 
might be more motivated to continue these actions. As such, it is important that the technology clearly 
identi.es to its users which of their individual behaviours affect the group outcome. This strat­egy 
also requires clear de.nitions of groups and group members so that individuals know whose actions affect 
them and vice versa.  4 Evaluation In order to validate the strategies that informed the design of 
the NZM version of Smoke? we decided to investigate the relative ef­fectiveness of the two versions of 
Smoke? on different audiences. In terms of explicit outcomes, our objective was to determine whether 
individualist players would .nd the NZE version of Smoke? more persuasive than the NZM version and whether 
collectivist players would .nd the NZM version more persuasive than the NZE ver­sion. Given the timeframe 
of the study, we decided to focus only on players short term attitude changes resulting from the game. 
As such, we conducted a quantitative evaluation of the effects of the two versions of Smoke? on individualist 
and collectivist audiences. 4.1 Experiment design In the cross-cultural psychology literature, pre-and 
post-surveys predominate as a method of measurement of attitude change fol­lowing phenomena exposure. 
We adopted this approach ourselves by presenting players recruited from our university with a survey 
before and after playing through a randomly allocated version of Smoke? Focusing on cultural orientation 
and game version, our ex­periment relied on a between-participants 2x2 factorial design: in­dividualism 
versus collectivism and NZE version versus NZM ver­sion. We ran the experiment on 141 participants between 
the ages of 17 and 25. 71 participants played the NZE game and the remainder were assigned the NZM game. 
71 participants self-identi.ed their ethnicity as NZ European, and 46 self-identi.ed with the M¯ aori 
or Paci.c peoples ethnic group. As discussed earlier, NZ Europeans are likely to be individualistic in 
their outlook, while M¯ aori are more likely to be collectivistic (although acculturation complicates 
the issue of M¯ aori culture categorisation). The Paci.c peoples cat­egory is a heterogeneous group, 
including Samoans, Tongans, and Niueans [Statistics New Zealand 2002]. Paci.c cultures are also classi.ed 
as collectivistic [Schoeffel et al. 1994; Webster 2001]. The remainder of the participants identi.ed 
with a range of ethnic­ities, including Chinese and Indian. China and India are generally regarded as 
collectivist cultures [Hofstede 1996; Triandis 1995]. In terms of smoking behaviour, 43 of the participants 
described them­selves as current smokers, 32 described themselves as ex-smokers, and the remainder described 
themselves as non-smokers.  4.2 Measuring cultural orientation Our pre-survey contained a question on 
self-identi.ed ethnicity. We grouped the ethnicity responses to .t within the categories of NZ European 
, M¯ aori or Paci.c , other Europeans , and all others . As the term ethnicity can be ambiguous, one 
section of the pre­survey contained items from Schwartz s values survey [Schwartz 1992], from which we 
created two variables corresponding to the subjects degree of individualism and collectivism. Those who 
were categorised as NZ Europeans and other Europeans had higher total means for individualism and lower 
total means for collectivism than those categorised as M¯aori or Paci.c , and all others , who had higher 
total means for collectivism and lower total means for individualism. For the rest of our analyses, then, 
we used the in­dividualism and collectivism variables to identify subjects cultural orientations. Subjects 
with high individualism or low collectivism scores were considered individualist, while those with low 
individ­ualism or high collectivism were considered collectivist.  4.3 Measuring attitudes towards smoking 
Both the pre-and post-surveys contained items about subjects be­liefs regarding smoking, which were drawn 
from surveys used in previous studies on smoking cessation [Burke et al. 1992; Dijkstra et al. 1999]. 
The items were grouped to determine means mea­suring participants positive beliefs about smoking, their 
negative beliefs about smoking, their intention to quit if they were smok­ers, their resistance to smoking 
if they were non-smokers (i.e. how strongly they felt that smoking was undesirable), and .nally, their 
temptation to smoke. We were then able to compare how the values of the means changed across pre-and 
post-survey responses. In ad­dition, the pre-survey contained items concerning the role smoking played 
in the participants lives. 4.4 Regression model of attitudes towards smoking by cultural orientation 
and game version To explore the relationship between game version, cultural orienta­tion, and pre-and 
post-survey responses, we used a multiple regres­sion model estimated by the ordinary least squares method. 
Our model featured the following variables as regressors: status: the mean variable relating to subjects 
smoking behaviour; proximity: the mean variable relating to the importance of smoking in subjects lives; 
version: a categorical variable describing the game version that subjects played; one of either individualism 
or collectivism: mean variables representing subjects degree of individualism or collectivism; and interaction: 
a crossproduct interaction term rep­resenting the joint effect of version and one of either individual­ism 
or collectivism. We then regressed each of the .ve post-survey means of positive beliefs, negative beliefs, 
resistance to smoking, intention to quit, and temptation to smoke on our model, including the corresponding 
pre-survey mean as an additional regressor.  4.5 Signi.cant interactions Intention to quit smoking We 
investigated the following hy­potheses by regressing post-survey means for smokers intention to quit 
against regressors of status, proximity, smokers pre-survey intention to quit, version, individualism, 
and interaction: H1a: Individualist players will have a greater positive change in intention to quit 
smoking after playing the NZE version of Smoke? than after playing the NZM version. H1b: Collectivist 
players will have a greater positive change in intention to quit smoking after playing the NZM version 
of Smoke? than after playing the NZE version. For each t ratio calculated in our model, we determined 
a relevant p­value, the probability of obtaining results at least as extreme as the ones observed, supposing 
in fact that cultural orientation and game version together, represented as the interaction regressor, 
had no effect on intention to quit. We summarise the details of the intention to quit regression model 
in table 2. In the intention to quit model, the effect of interaction, the regressor term representing 
the joint effect of version and individualism, is signi.cant at the p< 0.05 level using a one-sided test. 
Individualist smokers who played the NZE version of Smoke? demonstrated a greater positive change between 
pre-and post­survey intention to quit than those who played the NZM version. This result supports H1a. 
Collectivist smokers who played the NZM version of Smoke? demonstrated a greater positive change between 
pre-and post- Regressor ß Std. Error t ratio Constant 1.988 * .795 2.501 status -.046 .038 -1.213 proximity 
.229 * .060 3.794 pre-intention to quit .888 * .056 15.946 version -2.039 * 1.051 -1.939 individualism 
-.418 * .145 -2.893 interaction .479 * .199 2.140 n = 43, R2 = .902 * Signi.cant at the p< 0.05 level. 
Table 2: Dependent variable: post-survey intention to quit survey intention to quit than those who played 
the NZE version. This result supports H1b. Both these .ndings support the notion that persuasion effectiveness 
is increased by cultural relevance. Interestingly, both game ver­sions were more successful in shifting 
attitudes of collectivist play­ers than of individualist players. This may indicate that the indi­vidualist 
smoker subjects were less receptive overall to persuasion about quitting smoking than the collectivist 
smoker subjects. Resistance to smoking We investigated the following hypothe­ses by regressing post-survey 
means for subjects resistance to smoking against regressors of status, proximity, pre-survey resis­tance 
to smoking, version, individualism, and interaction: H2a: Individualist subjects will have a greater 
positive change in resistance to smoking after playing the NZE version of Smoke? than after playing the 
NZM version. H2b: Collectivist subjects will have a greater positive change in re­sistance to smoking 
after playing the NZM version of Smoke? than after playing the NZE version. We summarise the details 
of the resistance to smoking regression model in table 3, calculating p-values from the t ratios, as 
we did for the intention to quit model. Regressor ß Std. Error t ratio Constant 1.241 * .553 2.242 status 
.012 .024 .494 proximity -.049 * .035 -1.414 pre-resistance to smoking .786 * .069 11.368 version -1.199 
* .660 -1.817 individualism -.058 .092 -.632 interaction .220 * .125 1.758 n = 107, R2 = .626 * Signi.cant 
at the p< 0.05 level. Table 3: Dependent variable: post-survey resistance to smoking In the resistance 
to smoking model, the effect of interaction, the regressor term representing the joint effect of version 
and individu­alism, is signi.cant at the p< 0.05 level using a one-sided test. Individualist subjects 
who played the NZE version of Smoke? demonstrated a lesser positive change between pre-and post-survey 
resistance to smoking than those who played the NZM version. This result does not support H2a. Collectivist 
subjects who played the NZM version of Smoke? demonstrated a greater positive change between pre-and 
post­survey resistance to smoking than those who played the NZE ver­sion. This result supports H2b. 
At the same time, we note that individualist players of the NZE ver­sion of Smoke? demonstrated signi.cantly 
more attitude shift than collectivist players of the NZE version. This appears to indicate that while 
the NZM version seems to be more effective in increasing all players resistance to smoking, the NZE version 
is comparatively more effective on individualist rather than collectivist players. We hypothesise that, 
despite our intention to design equally persuasive game versions, irrespective of cultural factors, aspects 
of the NZM version may simply have been more persuasive than the NZE ver­sion. Effective persuasive game 
design is a rather new area of re­search and needs to mature before general design principles regard­ing 
persuasion are established. Interestingly, while resistance through collective support, as in the NZM 
version of Smoke? may seem more like a collectivistic con­cept, it is in fact a central feature of organisations 
like Weight Watchers and Alcoholics Anonymous. Both have been very suc­cessful in Western (individualist) 
cultures. This may indicate that for increasing attitudes related to different types of resistance, in­dividualists 
may bene.t from collective support as much as collec­tivists, if not more. We hypothesise that because 
it is less part of their everyday interaction, it has additional persuasion for them. Temptation to smoke 
We investigated the following hypotheses by regressing post-survey means for temptation to smoke against 
re­gressors of status, proximity, pre-survey temptation to smoke, ver­sion, collectivism, and interaction: 
H3a: Individualist subjects will have a greater negative change in temptation to smoke after playing 
the NZE version of Smoke? than after playing the NZM version. H3b: Collectivist subjects will have a 
greater negative change in temptation to smoke after playing the NZM version of Smoke? than after playing 
the NZE version. We summarise the details of the temptation to smoke regression model in table 4. Regressor 
 ß Std. Error t ratio Constant -.350 * .225 -1.554 status -.003 .028 -.123 proximity .070 .171 .412 pre-temptation 
to smoke .756 * .072 10.514 version .673 * .269 2.506 collectivism .158 * .045 3.521 interaction -.171 
* .062 -2.739 n = 66, R2 = .705 * Signi.cant at the p< 0.05 level. Table 4: Dependent variable: post-survey 
temptation to smoke In the temptation to smoke model, the effect of interaction, the regressor term representing 
the joint effect of version and collec­tivism, is signi.cant at the p< 0.05 level using a one-sided test. 
Individualist subjects who played the NZE version of Smoke? demonstrated a lesser negative change between 
pre-and post­survey temptation to smoke than those who played the NZM ver­sion. This result does not 
support H3a. Collectivist subjects who played the NZM version of Smoke? demonstrated a lesser negative 
change between pre-and post­survey temptation to smoke than those who played the NZE version. This result 
does not support H3b. So, interestingly, neither of our hypotheses regarding temptation to smoke were 
supported by the regression model .ndings. This out­come deserves some consideration. The smoking cessation 
litera­ture has reported in depth on how cue exposure increases urges to smoke [Niaura et al. 1999]. 
We suggest that when subjects played a game version that re.ected their cultural orientation, that is, 
the culturally-congruent condition, they were better able to engage with the game content by the context 
of their personal lives. Encounter­ing smoking cues may thus have elicited smoking-related thoughts and 
general curiosity about smoking in players, leading to a smaller drop in temptation to smoke. In the 
culturally-incongruent condi­tion, we hypothesise that players were less able to contextualise the game 
to their personal lives and were, therefore, less likely to be tempted by smoking cues. In turn, this 
response corresponded to a greater drop in temptation to smoke. Potentially, then, culturally­relevant 
cue exposure is a factor designers should account for when designing tools to help people deal with cue-stimulated 
cravings: it may increase the potency of cue exposure effects. As an additional interpretation, in our 
previous research we noted that NZ Europeans may view smoking as an activity with a more in­dividual 
locus, while M¯aori may view it as having a more collective locus [Khaled et al. 2006]. In the culturally-incongruent 
conditions, smoking was portrayed in a more collectively-oriented manner for individualist players and 
a more individually-oriented manner for collectivist players. Given the direct mismatch between real 
life and game portrayals, smoking may have seemed more foreign and incongruent as an activity, and therefore 
less appealing. In turn, this may have led to greater increases in temptation reductions.  4.6 Discussion 
The subject matter of Smoke? certainly impacted on how we ap­plied the design strategies and also how 
they would have been per­ceived. For example, smoking is considered harmful to others in both individualist 
and collectivist cultures alike. For research rea­sons, however, in the NZE game we focused less on social 
con­cerns, even though we are aware that in ethical terms, individualists are equally conscientious of 
their effects on others. In addition, we were only able to indirectly evaluate our strategies, through 
play­ers perceptions of the NZE and NZM games as cohesive artifacts. Without doubt, different implementations 
would have yielded dif­ferent results, and we hope to explore further potential implemen­tations of the 
strategies in future work.  5 Conclusions Serious games are intended to in.uence attitudes and behaviours, 
yet many of these attitudes and behaviours are in.uenced, in turn, by culture [Hall 1976; Hofstede 1996; 
Triandis 1995]. Harness­ing the culturally relevant motivations underlying people s actions, both in 
an individual and social context, has been shown in other domains to lead to greater persuasion. By accounting 
for culture in design, we are more likely to produce serious games and other PTs with increased sensitivity, 
realism, and effectiveness. In this paper, we outlined the design of one version of a culturally­relevant 
serious game we developed called Smoke? for collectivist, M¯ aori players. Its design was informed by 
a set of .ve persuasive design strategies for collectivist persuasion that we identi.ed from the cross-cultural 
psychology literature on individualism and col­lectivism. These strategies are: HARMONY: presenting social 
den­sity cues to users; GROUP OPINION: providing users with opinions of other ingroup members; MONITORING: 
sharing a user s track­ing information with a support group; DISESTABLISHING: training users out of practising 
behaviours they do not wish to perform; and TEAM PERFORMANCE: rewarding or reprimanding a group of users 
for the actions of an individual user. We also developed a version of Smoke? for individualist, NZ Eu­ropean 
players. We conducted a comparative, quantitative evalua­tion of the two games effects on both individualist 
and collectivist players. We were able to .nd some support for the idea that the culturally-matched conditions 
yielded greater persuasion. Due to the complexity of the multiple factors involved in the experiment, 
including potential cue exposure effects and social perceptions of smoking, it was not possible to reach 
a clear outcome. Our future research plans include designing additional games for collectivist players 
based on these strategies, but focusing on a lesser number of factors in order to explore their effects 
on persuasion and play.  References AAKER, J., AND MAHESWARAN, D. 1997. The effect of cultural orientation 
on persuasion. The Journal of Consumer Research 24, 3 (December), 315 328. BOGOST, I. 2007. Persuasive 
Games: The Expressive Power of Videogames. The MIT Press. BRAMLEY, D., RIDDELL, T., WHITTAKER, R., CORBETT, 
T., LIN, R.-B., WILLS, M., JONES, M., AND RODGERS, A. 2005. Smoking cessation using mobile phone text 
messaging is as ef­fective in M¯aori as non-M¯aori. The New Zealand Medical Jour­nal 118, 1216 (June). 
BURKE, J. A., SALAZAAR, A., DAUGHETY, V., AND BECKER, S. L. 1992. Activating interpersonal in.uence in 
the prevention of adolescent tobacco use. Health Communication 4, 1, 1 17. DAMRON, R. L., AND HALLECK, 
G. B. 2007. Generating cross­cultural training data for the university game. Simulation &#38; Gaming 
38, 4, 556 568. DE MOOIJ, M. 2005. Global Marketing and Advertising: Under­standing Cultural Paradoxes. 
Sage Publications. DIJKSTRA, A., DE VRIES, H., KOK, G., AND ROIJACKERS, J. 1999. Self-evaluation and 
motivation to change. Psychology and Health 14, 747 759. EVERS, V. 2001. Cultural Aspects of User Interface 
Understand­ing: An Empirical Evaluation of an E-Learning Website by In­ternational User Groups. PhD thesis, 
Institute of Educational Technology, The Open University. FESTINGER, L. 1954. A theory of social comparison. 
Human Relations 7, 117 140. FOGG, B. J. 2003. Persuasive Technology: Using Computers to Change What 
We Think and Do. Morgan Kaufmann Publishers. FRASCA, G., BATTEGAZZORE, S., OLHABERRY, N., INFAN-TOZZI, 
P., RODRIGUEZ, F., AND BALBI, F., 2003. Septem­ber the 12th. http://www.newsgaming.com/games/ index12.htm. 
GUDYKUNST, W. B., AND MATSUMOTO, Y. 1996. Cross-cultural variability of communication in personal relationships. 
In Com­munication in Personal Relationships Across Cultures, W. B. Gudykunst, S. Ting-Toomey, and T. 
Nishida, Eds. Sage Publi­cations, CA, USA. HALL, E. 1976. Beyond Culture. Anchor Press/Doubleday. HIGGIN, 
T. 2009. Blackless Fantasy: The Disappearance of Race in Massively Multiplayer Online Role-Playing Games. 
Games and Culture 4, 1, 3 26. HOFSTEDE, G. 1996. Cultures and organisations: Software of the mind. McGraw-Hill 
Education. HOFSTEDE, G. J. 2008. One game does not .t all cultures. In Why do games work? In search of 
the active substance, L. de Caluw, G. J. Hofstede, and V. Peters, Eds. Kluwer, 69 77. KHALED, R., NOBLE, 
J., AND BIDDLE, R. 2005. Developing culturally-aware persuasive technology. In Proceedings of the 7th 
International Workshop on Internationalisation of Products and Systems, E. D. Gado, Ed. KHALED, R., BARR, 
P., FISCHER, R., BIDDLE, R., AND NOBLE, J. 2006. Factoring culture into the design of a persuasive game. 
In Proceedings of OzCHI 2006. KLEINJAN, M., VAN DEN EIJNDEN, R. J. J. M., DIJKSTRA, A., BRUG, J., AND 
ENGELS, R. C. M. E. 2006. Excuses to con­tinue smoking: The role of disengagement beliefs in smoking 
cessation. Addictive Behaviours 31, 2223 2237. MARCUS, A., AND GOULD, E. 2000. Crosscurrents: cultural 
dimensions and global web user-interface design. Interactions 7, 4, 32 46. MARKUS, H., AND KITAYAMA, 
S. 1991. Culture and the self: Im­plications for cognition, emotion, and motivation. Psychological Review 
98, 224 253. NIAURA, R., ABRAMS, D. B., SHADEL, W. G., ROHSENOW, D. J., MONTI, P. M., AND SIROTA, A. 
D. 1999. Cue exposure treatment for smoking relapse prevention: A controlled clinical trial. Addiction 
94, 5, 685 695. PATTERSON, J. 1992. Exploring M¯aori Values. Dunmore Press. PAVLOU, P., AND CHAI, L. 
2002. What drives electronic com­merce across cultures? A cross-cultural empirical investigation of the 
theory of planned behaviour. Journal of Electronic Com­merce Research 3, 4, 240 253. SCHOEFFEL, P., 
MELEISEA, M., DAVID, R., KALAUNI, R., KALOLO, K., KINGITEMALETI, P., TAUMOEFOLAU, VUETIBAU, L., AND WILLIAMS, 
S. P. 1994. Spare the rod? Con.icting cultural models of the family and approaches to child socialisation 
in New Zealand. Tech. Rep. 1, Centre of Paci.c Studies, University of Auckland. SCHWARTZ, S. 1992.Universalsinthecontentandstructureofval­ues: 
Theoretical advances and empirical tests in 20 countries. In Advances in experimental social psychology, 
M. P. Zanna, Ed., vol. 25. New York: Academic Press, 1 65. STATISTICS NEW ZEALAND, 2002. 2001 Census. 
http://www. stats.govt.nz/, 25 June. TAFARODI, R. W., AND WALTERS, P. 1999. Individualism­collectivism, 
life events, and self-esteem: a test of two trade-offs. European Journal of Social Psychology 29, 5 (June), 
797 814. TRIANDIS, H. C., BONTEMPO, R., VILLAREAL, M. J., ASAI, M., AND LUCCA, N. 1988. Individualism 
and collectivism: Cross-cultural perspectives on self-ingroup relationships. Jour­nal of Personality 
and Social Psychology 54, 2, 323 338. TRIANDIS, H. C. 1995. Individualism and Collectivism (New Di­rections 
in Social Psychology). Westview Press. UNITED STATES ARMY, 2002. America s army: The of.cial army game. 
http://www.americasarmy.com/. V¨OHRINGER-KUHNT, T. 2001. The In.uence of Culture on Us­ability. Master 
s thesis, Department of Educational Sciences and Psychology, Freie Universit¨at Berlin, Berlin, Germany. 
WEBSTER, A. 2001. Spiral of Values. Alpha Publications.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1581079</section_id>
		<sort_key>60</sort_key>
		<section_seq_no>2</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Visuals, 3-d and the cinematic in games]]></section_title>
		<section_page_from>39</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1570313</person_id>
				<author_profile_id><![CDATA[81100099437]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Drew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Davidson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1581080</article_id>
		<sort_key>70</sort_key>
		<display_label>Pages</display_label>
		<article_publication_date>08-04-2009</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Inferred lighting]]></title>
		<subtitle><![CDATA[fast dynamic lighting and shadows for opaque and translucent objects]]></subtitle>
		<page_from>39</page_from>
		<page_to>45</page_to>
		<doi_number>10.1145/1581073.1581080</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1581080</url>
		<abstract>
			<par><![CDATA[<p>This paper presents a three phase pipeline for real-time rendering that provides fast dynamic light calculations while enabling greater material flexibility than deferred shading. This method, called <i>inferred lighting</i>, allows lighting calculations to occur at a significantly lower resolution than the final output and is compatible with hardware multisample antialiasing (MSAA). In addition, inferred lighting introduces a novel method of computing lighting and shadows for translucent objects (alpha polygons) that unifies the pipeline for processing lit alpha polygons with that of opaque polygons. The key to our method is a discontinuity sensitive filtering algorithm that enables material shaders to "infer" their lighting values from a light buffer sampled at a different resolution. This paper also discusses specific implementation issues of inferred lighting on DirectX 10, Xbox 360, and PlayStation 3 hardware.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[alpha-polygon shadows]]></kw>
			<kw><![CDATA[deferred shading]]></kw>
			<kw><![CDATA[fast dynamic lighting]]></kw>
			<kw><![CDATA[real-time rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1570314</person_id>
				<author_profile_id><![CDATA[81100637638]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kircher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Volition, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570315</person_id>
				<author_profile_id><![CDATA[81440599313]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lawrance]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Volition, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1230117</ref_obj_id>
				<ref_obj_pid>1230100</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bavoil, L., Callahan, S. P., Lefohn, A., Comba, J. A. L. D., and Silva, C. T. 2007. Multi-fragment effects on the gpu using the k-buffer. In <i>I3D '07: Proceedings of the 2007 symposium on Interactive 3D graphics and games</i>, ACM, New York, NY, USA, 97--104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378468</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Deering, M., Winner, S., Schediwy, B., Duffy, C., and Hunt, N. 1988. The triangle processor and normal vector shader: a vlsi system for high performance graphics. In <i>SIG-GRAPH 1988</i>, ACM, New York, NY, USA, 21--30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Engel, W., 2008. Diary of a graphics programmer: Light pre-pass renderer. Online, accessed Jan. 27th, 2009. http: //diaryofagraphicsprogrammer.blogspot.com/2008/03/light-pre-pass-renderer.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1404441</ref_obj_id>
				<ref_obj_pid>1404435</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Filion, D., and McNaughton, R. 2008. Effects &amp; techniques. In <i>ACM SIGGRAPH 2008 classes</i>, ACM, New York, NY, USA, 133--164.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Hargreaves, S., and Harris, M., 2004. Deferred shading. Online, accessed Jan. 26th, 2009. http://developer.nvidia.com/object/6800_leagues_deferred_shading.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Heidmann, T. 1991. Real shadows, real time. <i>Iris Universe 18</i>, 23--31.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Kwoon, H. Y. 2004. The theory of stencil shadow volumes. In <i>ShaderX2: Introductions and Tutorials with DirectX 9</i>. Wordware Publishing, Inc., 197--278.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Lee, M., 2008. Prelighting. Online, accessed Feb. 10th, 2009. http://www.insomniacgames.com/tech/articles/0209/files/prelighting.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134067</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Molnar, S., Eyles, J., and Poulton, J. 1992. Pixelflow: high-speed rendering using image composition. In <i>SIGGRAPH 1992</i>, ACM, New York, NY, USA, 231--240.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97901</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Saito, T., and Takahashi, T. 1990. Comprehensible rendering of 3-d shapes. <i>SIGGRAPH Comput. Graph. 24</i>, 4, 197--206.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1283909</ref_obj_id>
				<ref_obj_pid>1283900</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Segovia, B., Iehl, J. C., Mitanchey, R., and P&amp;#233;roche, B. 2006. Non-interleaved deferred shading of interleaved sample patterns. In <i>GH '06: Proceedings of the 21st ACM SIGGRAPH/EUROGRAPHICS symposium on Graphics hardware</i>, ACM, New York, NY, USA, 53--60.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Shishkovtsov, O. 2005. Deferred shading in s.t.a.l.k.e.r. <i>In GPU Gems 2</i>. Addison-Wesley, ch. 9, 143--166.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Thibieroz, N. 2004. Deferred shading with multiple render targets. In <i>ShaderX2: Shader programming Tips and Tricks with DirectX9</i>. Wordware Publishing, Inc., 251--269.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Valient, M., 2007. Deferred rendering in killzone 2. Online, accessed Jan. 26th, 2009. Develop Conference http://www.guerrilla-games.com/publications/dr_kz2_rsx_dev07.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Inferred Lighting: Fast dynamic lighting and shadows for opaque and translucent objects Scott Kircher* 
Alan Lawrance Volition, Inc. Volition, Inc.  Figure 1: Inferred lighting allows this complex scene with 
213 active lights to be rendered at 1280x720 resolution, with 8x MSAA, at 23fps (43.5ms) on a GeForce 
8800GTX. Abstract This paper presents a three phase pipeline for real-time rendering that provides fast 
dynamic light calculations while enabling greater material .exibility than deferred shading. This method, 
called in­ferred lighting, allows lighting calculations to occur at a signif­icantly lower resolution 
than the .nal output and is compatible with hardware multisample antialiasing (MSAA). In addition, in­ferred 
lighting introduces a novel method of computing lighting and shadows for translucent objects (alpha polygons) 
that uni.es the pipeline for processing lit alpha polygons with that of opaque poly­gons. The key to 
our method is a discontinuity sensitive .ltering algorithm that enables material shaders to infer their 
lighting val­ues from a light buffer sampled at a different resolution. This paper also discusses speci.c 
implementation issues of inferred lighting on DirectX 10, Xbox 360, and PlayStation 3 hardware. *e-mail: 
scott.kircher@volition-inc.com e-mail: alan.lawrance@volition-inc.com Copyright &#38;#169; 2009 by the 
Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of 
this work for personal or classroom use is granted without fee provided that copies are not made or distributed 
for commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. 
To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific 
permission and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or 
e-mail permissions@acm.org. Sandbox 2009, New Orleans, Louisiana, August 4 6, 2009. &#38;#169; 2009 
ACM 978-1-60558-514-7/09/0008 $10.00 CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics 
and Realism Color, shading, shadowing, and texture Keywords: real-time rendering, fast dynamic lighting, 
alpha­polygon shadows, deferred shading 1 Introduction Dynamic lighting is an important graphical aspect 
of many enter­tainment software titles. In particular, the lighting situation in so­called open world 
games can change drastically between day and night scenes. During night scenes, it is often desirable 
to have a very large number of dynamic lights for cars, neon signs, and so on. Even seemingly static 
lights, such as street lights, must be able to light dynamic objects, and so their lighting cannot be 
solely pre­computed. The method described in this paper is not restricted to open world games, however. 
Any game featuring a very large num­ber of dynamic lights could potentially bene.t. Traditionally, games 
have relied on a technique that is commonly called forward rendering for handling multiple dynamic lights. 
In forward rendering, the interactions between each light and each ob­ject in the scene must be explicitly 
accounted for. For example, the rendering engine typically must query the scene to determine which objects 
a particular light affects. In many cases, a pass over at least a portion of the scene geometry is required 
for each light. These queries and rendering passes can be quite expensive, and must be performed regardless 
of whether or not the light casts shadows. An increasingly popular technique called deferred shading 
allows the scene geometry to be rendered only once, and then non­shadow casting lights are applied as 
screen-space operations [Saito and Takahashi 1990; Shishkovtsov 2005; Filion and McNaughton 2008]. This 
solves the light-object interaction complexity problem, but makes it dif.cult to implement advanced material 
shaders, as effectively one material shader must be able to handle all desired shading effects. In addition 
deferred shading is incompatible with lighting alpha polygons. Deferred shading renderers must revert 
to forward rendering to light translucent objects [Shishkovtsov 2005]. This paper presents a dynamic 
lighting method, called inferred lighting that combines the strengths of forward rendering with those 
of deferred shading. Our method enables a very large num­ber of non-shadow casting dynamic lights, while 
allowing material shaders to be almost as .exible as in forward rendering. In ad­dition, inferred lighting 
introduces a new method for lighting al­pha polygons, allowing the same lighting techniques to be used 
on translucent and opaque objects. This method even makes translu­cent objects compatible with stencil 
shadow volumes [Heidmann 1991], which has traditionally not been possible. 1.1 Related Work The rendering 
technique now called deferred shading is sometimes credited to Deering et al. [1988], but it was more 
directly intro­duced by Saito and Takahashi [1990] for non-photo-realistic ren­dering. Saito and Takahashi 
rendered objects into a geometry buffer (G-buffer) storing normals, depth, and other geometric properties, 
and then performed 2D image space techniques to extract con­tours, silhouettes, and other information. 
Deferred shading ap­peared again in the PixelFlow hardware architecture proposed by Molnar et al. [1992]. 
The advent of multiple-render-target (MRT) technology in 2002 made deferred shading an effective real-time 
rendering technique [Thibieroz 2004; Hargreaves and Harris 2004]. Since then it has appeared in several 
game engines including S.T.A.L.K.E.R. [Shishkovtsov 2005], Killzone 2 [Valient 2007], and StarCraft 2 
[Filion and McNaughton 2008]. Standard deferred shading turns lighting into a simple screen-space operation 
and thereby makes non-shadow casting lights very cheap. However, because all data needed for shading 
must be stored in the G-buffers, increasing material complexity means more storage space required for 
G-buffer data, and more memory bandwidth con­sumed reading and writing that data. This places severe 
limits on the kinds of materials that can be rendered with deferred shading on current game console hardware. 
In addition, deferred shading is fundamentally incompatible with lit alpha polygons. Short of the K-buffer 
hardware modi.cations proposed by Bavoil et al. [2007], the G-buffer cannot store more than one set of 
geometry data per pixel. Thus, multiple layers of lighting are not possible. Translu­cent geometry must 
be lit using a separate forward rendering pass. So-called light pre-pass rendering, as proposed by Engel 
[2008] mitigates the material complexity problem of deferred shading. In his approach, the G-buffer contains 
very little information (i.e., only normals and depth). Lighting calculations are then partially per­formed 
as screen-space operations into another light buffer, and then a second scene geometry pass is rendered 
using the lighting buffer as input for a full material shader. Light pre-pass rendering is also being 
used by Insomniac Games [Lee 2008], and is closely related to our proposed method. However, light pre-pass 
rendering still requires alpha polygons to be lit in a separate forward render­ing pass, and requires 
that the light buffer be full resolution to avoid signi.cant lighting artifacts. Inferred lighting and 
light pre-pass rendering were developed concurrently and independently. Figure 2: (Top Left:) The 800x450 
normals G-buffer. (Top Right:) The 800x450 DSF data G-buffer. (Bottom Left:) The 800x450 L­buffer after 
rendering lighting. (Bottom Right:) The .nal 1280x720 8xMSAA output framebuffer.  2 The Inferred Lighting 
Pipeline The pipeline for inferred lighting consists of three distinct stages, a geometry pass, light 
pass and material pass. There are no addi­tional passes necessary for translucent geometry. 2.1 Geometry 
Pass The .rst pass stores geometric properties to screen-space buffers called G-buffers. The information 
stored in these buffers will de­pend on the needs of the light pass, but at a minimum it will consist 
of normals, depth, and discontinuity sensitive .lter (DSF) data. Vi­sualization of the view-space normals 
and DSF G-buffers are shown at the top left and right of Figure 2, respectively. The normals use 16 bits 
per component, but only the X and Y com­ponents are stored since the Z component can be reconstructed 
in the shader. A mapping from full sphere to half sphere is used to avoid storing a Z sign bit. For a 
right-handed coordinate system, the mapping for a view-space normal n is ' n + (0, 0, 1) n = (1) ||n 
+ (0, 0, 1)|| where only the X and Y components of n' are stored in the normal buffer. The reverse mapping, 
used when the normal buffer is read, is z =1 - ||(nx',ny')||2 , (2) ''2 n = (2znx, 2zny, 2z - 1). (3) 
The mapping has a singularity at n = (0, 0, -1), but this would be a normal facing directly away from 
the viewer, and should never need to be encoded. DSF data is stored as two 16 bit values, linear depth 
of the pixel and an ID value. Details on how the DSF data is used in the material pass is explained in 
§3. Other data can be stored in the G-buffer, such as specular power, motion vectors (for per-object 
motion blur), or a material ID to al­low for BRDF lighting. Details on using additional G-buffer data 
is beyond the scope of this paper, but it is expected that users of in­ferred lighting will tailor G-buffer 
usage to suit their speci.c needs. Creating additional G-buffers should be done with care, as they are 
memory intensive and incur a performance cost.  2.2 Light Pass The second pass calculates the contribution 
of the ambient and dy­namic lights that affect the scene. Lighting calculations are per­formed in screen-space 
using the G-buffer as input, and the results are written to the light buffer (L-buffer) for use in the 
material pass. Since lighting is done entirely in screen-space, there are no geom­etry passes necessary 
for non-shadow casting lights. As a conse­quence of the fact we use a lower resolution G-buffer and L-buffer 
than our .nal output, the viewpoint will have to get closer to a sur­face before normal map detail will 
become apparent in the lighting. Implementations can trade off between speed and quality by adjust­ing 
the G-buffer/L-buffer resolution. The L-buffer consists of four 16 bit channels. The diffuse lighting 
in stored in the RGB channels, and specular lighting in the alpha channel. The specular lighting value 
is encoded as the accumulated intensity of the specular highlight. The specular color is recon­structed 
in the material pass by scaling the stored intensity by the color of the diffuse lighting. For each light 
processed in the light pass, a full screen quadrilat­eral polygon is rendered that executes the lighting 
shader for every screen pixel. Since a given light may only affect a portion of the screen, there are 
two optimizations available to skip pixels not af­fected by light. The .rst is to apply a scissor rectangle 
that bounds the screen-space in.uence of the light. The second is to utilize the stencil buffer to determine 
where the light volume intersects visible scene geometry. We use the depth-fail stencil volume technique, 
as described in [Kwoon 2004]. These optimizations should be used to­gether, but it can be faster to skip 
the stencil optimization for lights that take up a very small amount of screen space. 2.3 Material Pass 
The .nal pass renders the scene using full material shaders but sam­ples lighting values from the L-buffer 
rather than doing its own light calculations. Alpha objects must be sorted and rendered af­ter opaque 
objects, but are lit in the same manner. The material pass is rendered at the framebuffer resolution, 
which is higher than the resolution used for the L-buffer. This requires the material pass to up-sample 
from the L-Buffer and perform special .ltering, which is described in §3.  3 Discontinuity Sensitive 
Filtering The inferred lighting pipeline as described in §2 allows the geom­etry pass and light pass 
to occur at a lower resolution than the ma­terial pass. This saves both memory and pixel shading costs. 
How­ever, the lower lighting resolution will be visible along the edges of objects in the scene, resulting 
in unacceptable aliasing. This is especially noticeable when the foreground and background objects receive 
drastically different lighting values as in Figure 3. Our solution to this problem is to perform discontinuity 
sensitive .ltering (DSF) of the L-buffer as it is read during the material pass. During the geometry 
pass, one 16 bit channel of the DSF buffer is .lled with the linear depth of the pixel, the other 16 
bit channel is .lled with an ID value that semi-uniquely identi.es continuous regions. This ID is described 
in more detail in §3.1. Figure 3: (Top:) Using an 800x450 single-sampled L-buffer with a 1280x720 8xMSAA 
framebuffer, with no DSF, results in badly aliased lighting. (Bottom:) DSF solves this problem. Figure 
4: DSF sampling uses only L-buffer samples that match the surface being rendered in the material pass. 
In this example, the dotted grid represents the high-resolution framebuffer. The solid grid represents 
the low resolution L-buffer. When material shading pixel p, which is part of the gray surface, sample 
d is thrown out, and only samples a, b, and c are used to light p. During the material pass, the pixel 
shader computes the locations of the four L-buffer texels that would normally be accessed if regular 
bilinear .ltering were used. These four locations are point sampled from the DSF buffer. The depth and 
ID values retrieved from the DSF buffer are compared against the depth and ID of the object being rendered. 
The results of this comparison are used to bias the usual bilinear .ltering weights so as to discard 
samples that do not belong to the surface currently rendering (see Figure 4). These biased weights are 
then used in custom bilinear .ltering of the L­buffer. Since the .lter only uses the L-buffer samples 
that belong to the object being rendered, the resulting lighting gives the illusion of being at full 
resolution. This same method works even when the framebuffer is multisampled (hardware MSAA), however 
sub­pixel artifacts can occur, due to the pixel shader only being run once per pixel, rather than once 
per sample. Such sub-pixel artifacts are typically not noticeable. In the rare case where all four L-buffer 
samples are unusable the pixel shader falls back on regular bilinear .ltering, possibly result­ing in 
a small lighting artifact. Our method of computing lighting at a lower resolution and then up-sampling 
with the aide of a discontinuity sensitive .lter is sim­ilar in spirit to the interleaved sampling and 
discontinuity .ltering methods proposed by Segovia, et al. [2006]. However, we feel our DSF method is 
simpler, more ef.cient (does not require walking the buffer to .nd discontinuities), and more suited 
to game rendering. 3.1 Object and Normal-Group IDs The ID value stored in the second channel of the 
DSF buffer con­sists of two parts. The upper 8 bits are an object ID, assigned per object (renderable 
instance) in the scene. Since 8 bits only allows 256 unique object IDs, scenes with more than this number 
of ob­jects will have some objects sharing the same ID. In theory this can lead to low probability DSF 
artifacts. In practice, we have never observed such artifacts. The lower 8 bits of the channel contain 
a normal-group ID. This ID is pre-computed and assigned to each face of the mesh. Anywhere the mesh has 
continuous normals, the ID is also continuous. In our implementation, a normal is continuous across an 
edge if and only if the two triangles that abut the edge share the same normals at both vertices of the 
edge. By comparing normal-group IDs the discontinuity sensitive .lter can detect normal discontinuities 
without actually having to recon­struct and compare normals. Both the object ID and normal-group ID must 
exactly match the material pass polygon being rendered be­fore the L-buffer sample can be used (depth 
must also match within an adjustable threshold). We pre-compute normal-group IDs by .rst constructing 
the dual graph of the mesh in question. We then remove any edges in the dual graph across which the mesh 
has discontinuous normals, and randomly assign normal-group IDs to each connected component of the resulting 
graph. These IDs are assigned to the corresponding faces of the original mesh. In our implementation, 
we pack the normal-group IDs into the fourth component of the mesh tangent vectors.  4 Lighting Alpha 
Polygons In addition to allowing the L-buffer resolution to be less than the output framebuffer resolution, 
DSF also enables a novel method of lighting alpha polygons. The main idea is that alpha polygons are 
rendered during the geometry pass using a stipple pattern, so that their G-buffer samples are interleaved 
with opaque polygon sam­ples. The light pass will automatically light those stippled pixels. No special 
case processing during the light pass is necessary, as long as the lighting operations are one to one 
(i.e., do not involve blurring of the L-buffer). In the material pass the DSF for opaque polygons will 
automatically reject stippled alpha pixels, and alpha polygons are handled by .nding the four closest 
L-buffer samples in the same stipple pattern, again using DSF to make sure the sam­ples were not overwritten 
by some other geometry. Figure 5 shows (left) an example of how the stipple pattern for a translucent 
object (the cockpit window glass) interleaves with the opaque object samples in the L-buffer. It also 
shows (right) the .nal result after the material pass. Since the stipple pattern is a 2 × 2 regular pattern, 
the effect is that the alpha polygon gets lit at half the resolution of opaque objects. Opaque objects 
covered by one layer of alpha have only slightly reduced lighting resolution (one out of every four samples 
cannot be used). For alpha polygons it is possible to simply use a discontinuity sen­sitive bilinear 
.lter (as in §3), with the sample locations taking into account the lower resolution sampling pattern. 
However, we use a small radius cone .lter on the nearest four stipple samples, as we found it gave slightly 
higher quality results. DSF data should be used to throw out samples that don t belong to the surface 
in ques­tion, regardless of .ltering method. Figure 5: (Left) In the L-buffer, alpha polygon samples 
are inter­leaved with opaque polygon samples. (Right) During the material pass, DSF and knowledge of 
the stipple pattern resolves appropri­ate lighting values. Figure 6: A helicopter blade casting a shadow 
on 3 overlapping layers of alpha polygons. Inferred lighting allows alpha polygons to be lit in essentially 
the same way as opaque polygons. Multiple layers of lit alpha objects can be achieved by assigning a 
different stipple pattern to each layer. We use a 2×2 regular pattern, which means there are four available 
patterns. Thus, up to four lay­ers of overlapped lighting are possible (including the opaque back­ground). 
Typically, by this point the background is so obscured by layers of alpha that loss of lighting information 
is not catastrophic. Different sampling patterns can be used. For example, a 4 × 4 pat­tern would allow 
up to 16 layers, but at greatly reduced resolution. Figure 6 shows an example of multiple layers of alpha 
objects being lit by inferred lighting. When using multiple stipple patterns, it must be decided what 
pat­tern to use for each layer. Currently we either statically assign the patterns with user input, or 
assign them dynamically based on the depth of the objects. Neither method is optimal, but both suf.ce 
for scenes without too many dynamic, overlapping alpha objects. Our method of lighting alpha polygons 
can accommodate any light­ing/shadow algorithm that is one to one in terms of L-buffer pixels. Thus, 
it can even be used with stencil shadow volumes, which are otherwise incompatible with translucent geometry. 
 5 Platform Speci.c Implementation Issues We have implemented inferred lighting under DirectX 10 as 
well as two current generation game consoles, the Xbox 360 and the PlayStation 3. The technique works 
equally well on all three plat­forms, but there are platform speci.c implementation issues, espe­cially 
on the game consoles. The only issue on DirectX 10 is that the depth surface must be resolved to an R24X8 
texture before it can be read. All other surfaces can be read from directly. 5.1 Xbox 360 The Xbox 360 
has 10MB of high performance video memory known as EDRAM. Special care must be taken when setting up 
the G-buffers and L-buffer in video memory, as none of the G-buffers can overlap. The L-buffer can share 
EDRAM with G-buffers, but ensure it doesn t overlap with the depth G-buffer as this can be used in the 
light pass if using the stencil optimization. It is not possible to read directly from EDRAM, so screen 
buffers must be resolved to textures. The format of the DSF buffer (two 16 bit channels) in EDRAM is 
.xed point with a range of -32 to 32. The corresponding texture format is .xed point with a range of 
0 to 1. This requires the shader to scale the shader output of 0 to 1 to -32 to 32. To maintain 16 bit 
precision, the texture used for the resolve needs to be created with a custom format that has a range 
of -1 to 1. When sampling from this texture in a shader, the results must be scaled to a 0 to 1 range. 
The Xbox 360 uses the same pixel center convention as DirectX 1 though 9, which requires special handling 
in shaders. This can be avoided by setting the render state D3DRS HALFPIXELOFFSET to true. This will 
ensure the Xbox 360 uses the same pixel center convention as DirectX 10 and the PlayStation 3. 5.2 PlayStation 
3 The PlayStation 3 does not support a surface format with two 16 bit channels. It is possible to work 
around this limitation by using an ARGB8 surface format and packing two 16 bit values into four 8 bit 
values. When binding the surface to a sampler, a texture for­mat remap must be used to ensure the texture 
is read correctly as CELL GCM TEXTURE Y16 X16. Although this solves the problem of storing the normals 
and DSF data as 16 bits per component, it does cause an issue with blending normals. An example is alpha 
blending a normal map decal onto a surface. Since all four channels of the normals G-Buffer are used 
to contain the packed 16 bit values, there is no channel available to write the alpha value. One solution 
to this problem is to treat the decal object as an alpha object. This will ensure lighting is computed 
for both the decal and the underlying surface, and the two will be blended together in the material pass. 
The downsides are that we consume an alpha layer, and the lighting on the decal will be at a lower resolution. 
Multiple render target support on the PlayStation 3 requires that all color targets are the same number 
of bits per pixel. This isn t a problem for the standard G-buffer set-up of 32 bits for normals and DSF 
data, but it s a restriction to keep in mind when designing G-buffers for cross-platform compatibility. 
Finally, reading from the depth buffer on the PS3 requires a texture format remap of BARG and use of 
the texture sampling function texDepth2D precise().  6 Additional Results We have demonstrated several 
inferred lighting results throughout the above sections. Additional results and timing information fol­low. 
Figure 7 shows a scene with 287 lights and 287 objects running with inferred lighting (but without MSAA) 
on a PlayStation 3. The GPU time is 5.03ms (199fps). Such tightly packed scenes are nearly Figure 7: 
A simple scene with 287 lights and 287 objects running at 199fps (5.03ms) on a PlayStation 3. Figure 
8: A scene with 101 objects and 81 omni lights running at 34fps (29.4ms) with inferred lighting on the 
Xbox 360. This same scene runs at 6fps (167ms) with forward rendering. the worst case for forward rendering. 
For such scenes, forward rendering complexity would be O(nl), where n is the number of objects and l 
is the number of lights. This is signi.cantly worse than inferred lighting s complexity of O(n + l) (which 
is the same as the complexity of deferred shading and light-prepass rendering). Figure 8 shows a scene 
with 101 objects and 81 omni lights running at 34fps (29.4ms) with inferred lighting on the Xbox 360. 
The same test scene using an optimized forward renderer, which has been in development for several years, 
runs at 6fps (167ms). Although this test is a nearly worst case example for forward rendering, it clearly 
shows how well inferred lighting handles a large number of non­shadow casting lights. Figure 9 shows 
the usefulness of using a lower-resolution L-buffer with DSF in the material pass. There are 122 non-shadowing 
lights in the scene. Both images use a 1280x720 framebuffer with 8x MSAA. The top picture uses a low 
resolution (800x450) L-buffer with DSF, and renders at 40 frames per second (25ms) on a GeForce 8800GTX. 
The bottom picture uses a full resolution (1280x720) L­buffer without DSF, and renders at 31 frames per 
second (32ms). In addition to the lower frame rate, notice that the bottom picture exhibits more aliasing 
on, for example, the handrails. This is due to the fact that the G-buffer was not multisampled. Having 
a mul­tisampled G-buffer is not even possible on most platforms (since the buffer must be resolved before 
it is read), and even if it were, it would further reduce performance. Also, since the bottom picture 
does not use DSF, many full-alpha objects had to be replaced with alpha-tested objects, resulting in 
yet more aliasing. As just alluded to, our alpha lighting technique can be applied to full-alpha chain 
link fences and other traditionally alpha-tested ob­jects like barbed wire and foliage (see Figure 10). 
This is use­ful because alpha-testing is currently incompatible with hardware  Figure 9: A scene with 
122 dynamic lights. Using an 800x450 L­buffer with a 1280x720 8x MSAA framebuffer and DSF (Top) results 
in both better quality anti-aliasing and faster framerate (40fps vs 31fps) than using a 1280x720 L-buffer 
with no DSF (Bottom). MSAA. To achieve the best results, we recommend doing an alpha­test in the geometry 
pass, in addition to the stipple pattern, to ensure that no stippled lighting samples are written in 
regions of complete transparency.  7 Conclusion Inferred lighting allows for a very large number of 
non-shadow casting dynamic lights while supporting complex material shaders, a uni.ed pipeline for lighting 
translucent and opaque objects, and hardware MSAA compatibility. The distinguishing feature of inferred 
lighting is the DSF algorithm which allows lighting to occur at a lower resolution than the .nal output 
and provides a novel approach to lighting alpha polygons without additional scene processing. 7.1 Limitations 
and Future Work While inferred lighting works very well in the case where there is a large number of 
lights or other operations occurring at the L-buffer resolution, it does incur a higher base cost (when 
there are no, or very few, lights) than deferred shading or light-prepass rendering. Further optimizations 
to the DSF algorithm will help ameliorate this. For example, we have plans to reduce the number of samples 
taken from the L-buffer, by leveraging the bilinear .lter available in hardware. Also, while the alpha 
lighting method solves the traditional problem of how to apply all lights and shadows in a scene to translucent 
objects, it is limited to a small number of lighting layers. Moreover, assigning stipple patterns in 
an optimal way (either dynamically or statically) is an open problem worthy of further research. Acknowledgements 
Special thanks to Tomas Arce who was in­strumental in the initial design of inferred lighting, and to 
Ja­son Lowe who implemented the .rst PS3 version of the algo­rithm. Thanks also to Mike Flavin, Bart 
Wyatt, Andy Cunningham, and John Buckley for their feedback about the method and paper. Thanks also to 
Adam Pletcher, Jason Childress, and Chris Cla.in for putting together many of the test scenes that appear 
in this paper.  References BAVOIL, L., CALLAHAN, S. P., LEFOHN, A., COMBA, J. A. L. D., AND SILVA, 
C. T. 2007. Multi-fragment effects on the gpu using the k-buffer. In I3D 07: Proceedings of the 2007 
symposium on Interactive 3D graphics and games, ACM, New York, NY, USA, 97 104. DEERING, M., WINNER, 
S., SCHEDIWY, B., DUFFY, C., AND HUNT, N. 1988. The triangle processor and normal vector shader: a vlsi 
system for high performance graphics. In SIG-GRAPH 1988, ACM, New York, NY, USA, 21 30. ENGEL, W., 2008. 
Diary of a graphics programmer: Light pre-pass renderer. Online, accessed Jan. 27th, 2009. http: //diaryofagraphicsprogrammer.blogspot.com/ 
2008/03/light-pre-pass-renderer.html. FILION, D., AND MCNAUGHTON, R. 2008. Effects &#38; techniques. 
In ACM SIGGRAPH 2008 classes, ACM, New York, NY, USA, 133 164. HARGREAVES, S., AND HARRIS, M., 2004. 
De­ferred shading. Online, accessed Jan. 26th, 2009. http://developer.nvidia.com/object/6800_ leagues_deferred_shading.html. 
HEIDMANN, T. 1991. Real shadows, real time. Iris Universe 18, 23 31. KWOON, H. Y. 2004. The theory of 
stencil shadow volumes. In ShaderX2: Introductions and Tutorials with DirectX 9. Word­ware Publishing, 
Inc., 197 278. LEE, M., 2008. Prelighting. Online, accessed Feb. 10th, 2009. http://www.insomniacgames.com/tech/ 
articles/0209/files/prelighting.pdf. MOLNAR, S., EYLES, J., AND POULTON, J. 1992. Pixel.ow: high-speed 
rendering using image composition. In SIGGRAPH 1992, ACM, New York, NY, USA, 231 240. SAITO, T., AND 
TAKAHASHI, T. 1990. Comprehensible rendering of 3-d shapes. SIGGRAPH Comput. Graph. 24, 4, 197 206. SEGOVIA, 
B., IEHL, R., AND P´ J. C., MITANCHEY, EROCHE, B. 2006. Non-interleaved deferred shading of interleaved 
sam­ple patterns. In GH 06: Proceedings of the 21st ACM SIG-GRAPH/EUROGRAPHICS symposium on Graphics 
hardware, ACM, New York, NY, USA, 53 60. SHISHKOVTSOV, O. 2005. Deferred shading in s.t.a.l.k.e.r. In 
GPU Gems 2. Addison-Wesley, ch. 9, 143 166. THIBIEROZ, N. 2004. Deferred shading with multiple render 
tar­gets. In ShaderX2: Shader programming Tips and Tricks with DirectX 9. Wordware Publishing, Inc., 
251 269. VALIENT, M., 2007. Deferred rendering in killzone 2. Online, accessed Jan. 26th, 2009. Develop 
Con­ference http://www.guerrilla-games.com/ publications/dr_kz2_rsx_dev07.pdf.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1581081</article_id>
		<sort_key>80</sort_key>
		<display_label>Pages</display_label>
		<article_publication_date>08-04-2009</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[An interactive simulation system for flying Japanese kites]]></title>
		<page_from>47</page_from>
		<page_to>53</page_to>
		<doi_number>10.1145/1581073.1581081</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1581081</url>
		<abstract>
			<par><![CDATA[<p>This paper presents an interactive simulation system for flying kites, especially of Japanese types, and discusses the development of a flying kite game with a haptic interface device on a personal computer. The kite interacts with fluid and the computational costs associated with accurate calculation of its behavior are high. The system can precisely calculate the forces induced by air current exerted on the kite based on the graphs of lift and drag coefficients of rectangles with several aspect ratios established by experiments for wings and kites. Hence we can reproduce realistic behaviors of the kite even though it uses a coarse mesh for fluid simulation. The haptic interface device is connected to the simulation system to reflect the drag force applied by the operator and for force feedback from the kite though the kite string to the operator; we have developed a game that gives the feeling of flying a real kite.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[fluid simulation]]></kw>
			<kw><![CDATA[haptic interface device]]></kw>
			<kw><![CDATA[kite flying]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Gaming</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010257.10010293.10010318</concept_id>
				<concept_desc>CCS->Computing methodologies->Machine learning->Machine learning approaches->Stochastic games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010070.10010099</concept_id>
				<concept_desc>CCS->Theory of computation->Theory and algorithms for application domains->Algorithmic game theory and mechanism design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1570316</person_id>
				<author_profile_id><![CDATA[81440597416]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Taichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Shizuoka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570317</person_id>
				<author_profile_id><![CDATA[81342494827]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Makoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fujisawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nara Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570318</person_id>
				<author_profile_id><![CDATA[81539969956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kenjiro]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Miura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Shizuoka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Abbott, I., and Von Doenhoff, A. 1959. <i>Theory of Wing Sections</i>. Dover Publications.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ashley, H., and Landahl, M. 1985. <i>Aerodynamics of Wings and Bodies</i>. Dover Publications.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Azuma, A. 1992. <i>Science of RC Airplane and Kite</i>. Denpa Jikkensha (Japanese).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1205650</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bourg, D. 2001. <i>Physics for Game Developers</i>. O'Reilly Media, Inc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>196548</ref_obj_id>
				<ref_obj_pid>195966</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Cooke, J. M., Zyda, M. J., Pratt, D. R., and McGhee, R. B. 1992. Npsnet: flight simulation dynamic modeling using quaternions. <i>Presence: Teleoper. Virtual Environ. 1</i>, 4, 404--420.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073281</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Feldman, B. E., O'Brien, J. F., and Klingner, B. M. 2005. Animating gases with hybrid meshes. In <i>Proceedings of SIGGRAPH 2005</i>, ACM Press, New York, NY, USA, 904--909.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276438</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Goldenthal, R., Harmon, D., Fattal, R., Bercovier, M., and Grinspun, E. 2007. Efficient simulation of inextensible cloth. In <i>Proceedings of SIGGRAPH 2007</i>, ACM Press, New York, NY, USA, 49.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073299</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Guendelman, E., Selle, A., Losasso, F., and Fedkiw, R. 2005. Coupling water and smoke to thin deformable and rigid shells. In <i>Proceedings of SIGGRAPH 2005</i>, ACM Press, New York, NY, USA, 973--981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Ito, T., and Komura, K. 1979. <i>Science of Kite</i>. Shogaku-kan (Japanese).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Katz, J., and Plotkin, A. 2004. Low-speed aerodynamics. <i>Journal of Fluids Engineering 126</i>, 293.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1179223</ref_obj_id>
				<ref_obj_pid>1179196</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Klingner, B. M., Feldman, B. E., Chentanez, N., and O'Brien, J. F. 2006. Fluid animation with dynamic meshes. In <i>SIGGRAPH '06: ACM SIGGRAPH 2006 Papers</i>, ACM, New York, NY, USA, 820--825.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015745</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Losasso, F., Gibou, F., and Fedkiw, R. 2004. Simulating water and smoke with an octree data structure. In <i>Proceedings of SIGGRAPH 2004</i>, ACM Press, New York, NY, USA, 457--462.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015711</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Mitani, J., and Suzuki, H. 2004. Making papercraft toys from meshes using strip-based approximate unfolding. In <i>SIGGRAPH 2004</i>, 259--263.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276433</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Mori, Y., and Igarashi, T. 2007. Plushie: an interactive design system for plush toys. In <i>SIGGRAPH '07: ACM SIGGRAPH 2007 papers</i>, ACM, New York, NY, USA, 45.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[O-dako. Gddweb: Shirone o-dako. http://gddweb.org/odako.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Ramakrishnananda, B., and Wong, K. C. 1999. Animating bird flight using aerodynamics. <i>The Visual Computer 15</i>, 10, 494--508.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Regenie, V., et al. 1992. The f-18 high alpha research vehicle: A high-angle-of-attack testbed aircraft.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360645</ref_obj_id>
				<ref_obj_pid>1399504</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Robinson-Mosher, A., Shinar, T., Gretarsson, J., Su, J., and Fedkiw, R. 2008. Two-way coupling of fluids to rigid and deformable solids and shells. In <i>SIGGRAPH '08: ACM SIGGRAPH 2008 papers</i>, ACM, New York, NY, USA, 1--9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Rogallo, F., Sleeman, W., and Croom, D. 1965. Resume of recent parawing research. <i>NASA Technical Reports</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Rogallo, F. 1968. Nasa research on flexible wings. <i>Annals of the New York Academy of Sciences 154</i>, 2 International Congress on Subsonic Aeronautics, 953--961.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Smith, H. 1992. <i>The Illustrated Guide to Aerodynamics</i>. Tab Books.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311548</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Stam, J. 1999. Stable fluids. In <i>Proceedings of SIGGRAPH 1999</i>, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 121--128.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Wang, Z. 2008. Aerodynamic efficiency of flapping flight: analysis of a two-stroke model. <i>Journal of Experimental Biology 211</i>, 2, 234.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Withers, P. C. 1981. An aerodynamic analysis of bird wings as fixed aerofoils. <i>Journal of Experimental Biology 90</i>, 143--162.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882360</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Wu, J., and Popovi&amp;#263;, Z. 2003. Realistic modeling of bird flight animations. In <i>SIGGRAPH '03: ACM SIGGRAPH 2003 Papers</i>, ACM, New York, NY, USA, 888--895.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 AnInteractiveSimulationSystemfor FlyingJapaneseKites TaichiOkamoto * MakotoFujisawa KenjiroT.Miura ShizuokaUniversity 
NaraInstituteofScienceandTechnology ShizuokaUniversity  (a) Kite festival (b) Various kites (c) Kite 
simulator (multi-shots) Figure 1: Japanesekitesand theirsimulator. Abstract Thispaperpresentsan interactivesimulationsystemfor 
.yingkites, especially of Japanese types, and discusses the development of a .ying kite game with a haptic 
interface device on a personal com­puter. The kite interacts with .uid and the computational costs as­sociated 
with accurate calculation of its behavior are high. The systemcanpreciselycalculate theforces inducedby 
aircurrent ex­erted on the kite based on the graphs of lift and drag coef.cients of rectangles with several 
aspect ratios established by experiments for wings and kites. Hence we can reproduce realistic behaviors 
of the kite even though it uses a coarse mesh for .uid simulation. The haptic interface device is connected 
to the simulation system to re.ect the drag force applied by the operator and for force feed­back from 
the kite though the kite string to the operator; we have developed agame thatgives thefeeling of .ying 
arealkite. CR Categories: I.3.7[ComputerGraphics]: Three-Dimensional Graphics andRealism Animation;I.6.8[Simulation 
andModel­ing]: Types ofSimulation Gaming; Keywords: kite .ying, .uidsimulation,haptic interfacedevice 
* e-mail:f0730024@ipc.shizuoka.ac.jp e-mail:fujis@is.naist.jp e-mail:tmkmiur@ipc.shizuoka.ac.jp Copyright 
&#38;#169; 2009 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies 
of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM 
Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org. Sandbox 2009, New Orleans, Louisiana, August 
4 6, 2009. &#38;#169; 2009 ACM 978-1-60558-514-7/09/0008 $10.00  1 Introduction Rapidadvanceshavebeenmadeinphysical-basedsimulation 
tech­niques in the .eld of computer graphics for complicated .uid mo­tionsanddeformationsof objects,andthequalityofCG 
animations using these techniques used for cinemas and commercial .lms is very high, approaching that 
of video images of real phenomena. Several studies have applied these techniques to hobbies, such as 
nuigurumi(Japanese ragdoll making)[Mori andIgarashi2007] and origami(Japanesepaper folding)[Mitani and 
Suzuki 2004], which many people enjoy in their daily lives. The goal of our research is to simulate the 
.ying of Japanese kites, a traditional Japanese hobby,asan interactivegame.Theshapesof theJapanesekitesare 
usually very simple, made up of rectangles or combinations of rect­angles. Japanese kites have artistic 
value because of the traditional paintingson them,asshown inFig.1(a)and(b). The kite has a very long 
history; it was invented more than 2,500 years ago, and ful.lled people s dream of .ying before the inven­tion 
of the airplane. New types of the kite have been developed recently, such as Gayla-kite and bio-kite. 
Kites are .own at cere­monies and festivals and are used in sports and games. Although kites have long 
been enjoyed by people, to our knowledge, there havebeen nopreviousphysical-basedinteractive simulations 
ofkite .ying. This is because the angle of attack can change to a much greater extent than the wing of 
an airplane and the rigidity of the kite is low and its shape can be deformed easily. Furthermore, it 
is very dif.cult to calculate the interactions between a thin plate and air.oweven in the .eldof computational 
.uiddynamics. In this paper, we propose an interactive simulation method using the graphs of the lift 
and drag coef.cients of wings and kites es­tablished by experiments for performance tests. It is necessary 
to use a .ne grid to accurately calculate the behavior of air.ows and the pressure on the upper and lower 
surface for a thin object us­ing discrete computational methods, such as FDM and FEM, and the computational 
time tends to be quite long. We obtain the lift anddrag coef.cients,theworkingpointsofthe lift anddragforces, 
from theshapeofthekiteandtheangleofattack using experimental data.Itisnot necessary toknowthebehavioroftheair.owaround 
the kite in detail for our method. Hence, we can adopt a coarse­structuredgridinstead of anunstructured.negriddeformed 
tothe shape of the kite that requires high computational cost. Our .uid simulator calculates the forces 
exerted by .uid on the kite and ob­tains the lift and drag forces from their coef.cients based on the 
graphs. The tensile force generated by the kite string is calculated byamass-spring model andthedeformationofthekite 
isevaluated by assuming that the kite is made of a wireframe. These calcula­tionscanbeperformedinreal 
time. One of the pleasures of .ying kites is to watch their free move­mentin.ight,and anotheris tocontrolthe.ightbypullingthekite 
string, changing the strength and frequency of drag according to theair.ow toachievestable .ight.Wecombinedthereal-timekite 
simulatorwithahaptic interfacedevice todevelop auser interactive game.Thehaptic interfacedeviceaddsforcefeedbackfor 
inputde­vices and we can control the .ight of the kite and give the player thefeeling ofholding theendsof 
thekitestring.Ourultimategoal is todevelop an interactivekite .yinggamewithahaptic interface device. 
Thegoalsof ourresearch were todevelopthefollowing: A fast computation method for the lift and drag forces 
using experimentaldata ofkites.  Application of our method for a complicated kite shape by subdividing 
it into a combination of rectangles.  Amethod tocontrol akiteusing ahapticinterfacedevice.  Therestofthispaperconsistsofthefollowing 
sections.Thefollow­ing section describes previous work and Sections 3 and 4 address ourkite.ying simulation 
techniquesand userinteaction.Section5 shows the results of our simulation. The .nal section of the paper 
presents concludingremarks.  2 RelatedWork Flying objects other than kites include airplanes and birds. 
Dur­ing .ight, the air .ows around the wing of an airplane and the forces inducedby theair.owhavebeenstudied 
indetail[Abbott and Von Doenhoff 1959; Ashley and Landahl 1985; Smith 1992; Wang 2008]. Simulations based 
on these studies have been per­formed[Cookeetal.1992;Bourg2001]. Theaspectratioof the aerofoil of the 
airplane is larger than that of a kite, and the maxi­mum angle of attack of the airplane is generally 
less than 20deg although the kite may have larger angles. The delta wing mainly used for military planes 
has a small aspect ratio and the perfor­mancecharacteristicsforrelatively largeanglesof attackhavebeen 
studied[Regenie et al.1992;Katz andPlotkin2004]. However, air­plane wings have a certain thickness and 
are not made of thin ma­terial,such aspaperorvinyl.The liftforcethat causestheairplane to elevate has 
been studied in detail as the airplane .ies with thrust forceand thedragforceagainst the thrustforce 
isnot importantfor airplanedesign. Theparawinghasbeenstudiedfor thehangglider[Rogalloetal. 1965; Rogallo 
1968]. The speci.cation of the parawing is like a parachute the cloth of which is .xed more loosely than 
that of the kite. RamakrishnanandaandWong[1999] producedbird .ightanima­tionsbased onaerodynamics.They 
calculated thegravitydistribu­tion on the wings of the bird from the wing shape and predicted the .ight 
trajectorybased on theaerodynamics. WuandPopovi´c [2003]modeled a bird as an articulated skeleton with 
feathers and evaluatedthe lift anddragforcesexerted onthewingsby simpli.ed aerodynamics. They used experimental 
data obtained by Withers [1981]for the lift and the dragcoef.cients. We adopted simpli.ed aerodynamicssimilar 
to thoseofWuandPopovi´c[2003] and used experimentaldata ofkites obtainedbyAzuma[1992] andIto and Komura[1979]. 
As a physically-based simulation, .uid dynamics have been used to animate complicated behaviors of water, 
gas, and .ames. In re­centyears, techniquesforanimationof complicatedinteractionsbe­tween .uidand solidobjectshavebeenstudied. 
Stam[1999]pro­posed a semi-Lagrangian method and a stable and fast .uid calcu­lation method usingFourierdomain 
on a regular cubic mesh, which contributed to improvementofthequalityand speed of .uidsimu­lation. Losasso 
et al. [2004] used an unstructured grid the size of which is .ner around free .uid and solid surfaces 
with less com­putational cost. However, the boundary regions between the ob­jects and .uid are expressed 
by hexahedrons and good results are not always obtained. In a study to improve the quality of simula­tionof 
.uid-solid interactionsusing unstructuredgrids,Feldmanet al. [2005]proposed a method to use a regularly 
hexahedral hybrid mesh but with a tetrahedral mesh around solid objects. Klingner et al. [2006] proposed 
a method in which a tetrahedral grid is gen­erated according to solid shapes at each time step for interactions 
with moving solid objects. The generation of tetrahedral meshes requiresadditionalprocessing time.Guendelmanetal.[2005] 
and Robinson-Mosher et al. [2008] paid attention to thin objects and proposed methods to interact with 
thin shells. It is very dif.cult to accuratelyprocessinteractionswithsolidsinreal time.  3 KiteFlyingSimulator 
Inthissection,wewilldescribeourkitemodelandthe treatment of theforcesexerted tothekite. 3.1 Kite Model 
Somekiteshaveartisticallycomplicatedshapesandtheboxkitehas a 3-dimensional shape. The kite train consists 
of several sub-kites, but the kites that can be made at home are rectangular, diamond­shaped, or other 
simple shapes. We restricted the scope of our dis­cussion to traditionalJapanesekites(see,forexample,[O-dako]). 
The traditionalJapanesekitehasaframemadeofbamboocovered withJapanesepaper,andisrectangular,diamond-shaped,oryakko, 
which is a rectangle with sleeves. One or two tails may sometimes be attached at the lower end of the 
kite to improve the stability of .ight.The tailsareused tobalance theweight on theright andleft sidesand 
tostabilizethekitetodecrease itsangleof attack.Dueto thesimplicityofitsshape,anaccuratesimulationcanbeperformed 
based on experimentaldatafor simple shapes. We use a rectangle of width bandheight c asashapemodelfor 
the simplest Japanese traditional kite as shown in Fig.2(a). We use a combination of this simple model 
to treat a kite with a more com­plicated shape as explained in Section 3.7. Figure 2(b) illustrates the 
forces exerted on the kite viewed from its side. In addition to the lift and drag forces generated by 
the air.ow around the kite, it has its own gravity, and tensile forces from the kite string and kite 
tail(s). If the sum of these forces is upward, the kite will .y. The lift and drag forces and their working 
points have a marked effect on theattitudeofthekite,whichisdeterminedby rotational motion about thecenterofgravityof 
thekite.  b NKHV  1.4 CKTHNQY FTCI 1.2 c 1  VGPUKQP VGPUKQP 0.8 %.  UVTKPI VCKN 0.6 (C) (b) Figure 
2: (a)Thebasickite model.(b)Forcesextertedon thekite (side view). 3.2 LiftandDragForce Themostimportantfactorstodeterminethekitemotionarethe 
lift and the drag forces generated by the air.ow. When an aerofoil is placed in the air.ow, the air.ow 
velocity becomes different at the upper and the lower surfaces of the aerofoil and the air.ow gener­ates 
a pressure difference. The kite can gain lift force by inclining 0.4 0.2 0  a FGI Figure 4: Variation 
ofCL with a. AR =0.68(redline)and AR = 1.48(green line) 0 5 10 15 20 25 30 35 40 45 its attitude and 
obtain drag force proportional to the projected area on theplaneperpendicular to theair.ow.It isnecessarytoknow 
the behavior of the air.ow around the kite to calculate the lift and the dragforcesaccurately.Thekite 
isgenerallymadeofpaperorvinyl and the simulation between these thin shells and .uid requires a speciallyshapedgridorFEM.However, 
these incurhigh computa­tional costs.Forexample,Robinson-Mosheret al.[2008]tookfrom several minutes toseveralhoursforthecalculationof 
oneframe. Weadoptedsimpli.ed aerodynamics toobtain theforcesexerted on thekite inreal time.The liftforce 
L and the drag force D exerted onanobject in the .uidaregivenby: 1 L = CL.S(U × n)× U , (1) 2 0 5 10 
15 20 25 30 35 40 45  1 a FGI D = CD .S|U |U , (2) 2 Figure 5: Variation ofCD with a. AR =0.68(redline)and 
AR = where CL and CD arethe lift anddrag coef.cients, .is thedensity 1.48(green line) of air, S is the 
projected area of the kite, U is the relative velocity of the air.ow to the kite, and n is the normal 
vector of the kite surface. The lift and drag coef.cients are determined by the angle of attack, which 
is the attitude of the kite relative to the air.ow. The pressure difference between the upper and lower 
surfaces of the kite is distributed continuously along the centerline of the kite as illustratedinFig.3(a).TheexperimentaldataobtainedbyAzuma 
[1992]substitutethisdistributionwiththeresultantforceofthe lift anddragforcesand theworkingpoint asshown 
inFig.3(b). FKHHGTGPVKCN RTGUUWTG  TGUWNVCPV HQTEG QHFKUVTKDWVKQP QP NKHV CPF FTCI WRRGT CPF NQYGT 
UKFGU #KTHNQY  YQTMKPI RQKPV CPING QH CVVCEM  0 10 20 30 40 50 60 70 80 90 Figure 3: Force calculation.(a)Differentialpressuredistribution, 
a FGI (b)Resultantforceand itsworkingpoint. Figure 6: Variation of r between the front end of the kite 
to the point of theapplicationof the force/kiteheight with a. Weusedthe threegraphsshowninFigs.4-6inthisresearch,which 
showgraphsof the CL and CD to the angle of attack, respectively. The working point of the resultant 
force of the lift and drag forces isgivenby thedistance rc from the front end to the working point where 
c is the kite height and the ratio r is obtained by the graph shown inFig.6providedbyItoandKomura[1979]. 
Figures4 and5 show thecoef.cientswhen theaspect ratios AR = b/c are equivalent to 0.68 and 1.48, respectively. 
For other aspect ratios, we used a linear interpolation of these data. For a> 45°, we use the calculation 
method for CL and CD reported by Wang [2008]givenby: CL(a)= CL(45°)sin2a, (3) CD (a)= CD (45°)(1-cos2a). 
(4) We obtain the working point exerting the lift force L and the drag force D fromFig.6[Ito andKomura1979]for 
all aspect ratios. 3.3 LiftandDragForcein3D The lift and drag forces are calculated from the pressure 
distribu­tion along the centerline of the kite. To calculate the motion of the kite in 3-dimensions, 
we need the pressure distribution on the whole 2-dimensional surface of the kite. However, it is very 
dif.­cult toestimate thedistributionbecause itdependson theair.owfor an object with a complicated shape. 
As we assumed a rectangular shapefor thekite,weexpect that thepressuredistribution in thedi­rectionvertical 
tothecenterlineofthekitewouldbesimilar tothat along the centerline. Here, we established a coordinate 
system for thekiteasshown inFig.7(a) whoseorigin isat thecenterofgravity of the kite. We assumed that 
the pressure distribution along the x­axis varies with respect to the air.ow in the z-x plane in the 
same manner as that alongthe y-axis with respect to the air.ow in the y-z plane.Thepitch motionabout 
the x-axis is calculated from the lift anddragforcesandtheirworkingpoint obtained on theplanealong the 
centerline(x =0 plane). Similarly, the roll motion about the y-axis is evaluated by use of the y =0 plane. 
We use the air.ow projectedonto the y-z plane tocalculatethepitchmotionaboutthe x-axis and the air.ow 
projected onto the z-x plane to calculate the roll motion about the y-axis. For yaw motion about the 
z-axis, if the kite is assumed to be .at, no lift or drag force is generated. Generally, the kite is 
de.ected by theair.owfrom thefrontandgeneratesaforceagainst theyaw motion. This force is necessary to 
stabilize the attitude of the kite and if it is ignored, the kite will swing right and left continuously. 
Weapplyanarti.cialdampingforce tostoptheyawmotion. [ CZKU TGUWNVCPV HQTEG QH Z CZKU \ CZKU  \ Z RNCPG 
 C D E Figure 7: Extension into 3D.The lift and the drag forces exert along the center line of the 3D 
kite.We use the air.ow projected onto the y-z plane to calculate the pitch motion about the x-axis and 
the air.ow projected onto the z-x plane to calculate the roll motion about the y-axis.  [ \ RNCPG  
3.4 Kite String and Tail One of the tensile forces for the kite is through the kite string con­troledbytheplayerandtheotherisbythekite 
tailwhichisdraged by theair.ow.Thekitestringplaysaroleof adirect interfacewith the player and the tail 
is to balance the right and left weight and to stabilize the kite to decrease its angle of attack. We 
use a mass­spring modelforthekitestring andthe tailasshowninFig.8.Each massissubjectedto itsgravity,adragforcebytheair.owand 
aten­sile force between two connected masses. The drag force is given by Eq.(2). We use a suitable value 
for CD by assuming a cylinder oraplaneas itsshapeand S isaprojected areaof themassand U is a relative 
velocity of the air.ow to it. The tensile force by the spring F spring isgivenby: F spring = ks.ls + 
kdv, (5) where ks is a spring constant, .l is an extension ofthe spring, kd is a damping factor and v 
is a relative velocity of the mass at the end ofthe springto that at its start. Furthermore if the mass 
is connected to thekitewithaspring, itget anactionforcefrom thekiteand the kite get its reaction force. 
We limit the extension of the kite string and tailsby applythemethodproposedbyGoldenthal et al.[2007] 
into one dimensional case and shorten the length of the string and those of the tails not to make the 
simulation of the kite string and tails unrealistic. MKVG URTKPI C D  Figure 8: Mass-spring model ofthekitestring 
andthekite tail.(a) String,(b)Tail.  3.5 SimulationProcedure The simulation procedure is basically simple 
and it is similar to those of general physically-based simulations. However, we have steps to limit the 
extension of the kite string and tails and some special treatmentisnecessarybecausethe tensionsofthestring 
and tails willbedecreased very much orbecome even 0after their short­ening operations. Thesimulationprocedure 
isasfollows: Initialize kite position, fluid simulator. Set time interval dt. Loop: Translate the end 
of the string according to the user operation. Calculate Tstring and Ttail . Calculate L and D. Calculate 
force F and moment M exerted to the kite. F is given by: F = L + D + G + Tstring + Ttail Translate and 
rotate the kite. Translate the joints of the string and the tails according to the kite motion. Shorten 
the string and the tails. Go to Loop.  In the above procedure, Ln and Dn are the lift and drag forces 
at time step n, respectively and G is thegravityofthekite.The tensile forceofthekitestringto thekiteT 
n tail is string andthat ofthe tail T n calculatedbeforetheshortening operations.This treatment enables 
quicker response to the user interaction because the kite string and tailsexertsstrongerforces tothekite. 
 3.6 De.ectionofKite We model the kite as a .at rectangle. The Japanese kite usually uses a bamboo framework, 
which can be bent easily by a weak force. With deformation of the framework skeleton, the whole kite 
bends about the y-axis. This deformation improves the stability of the kite. Although the force of restitution 
is substituted for the damping force, de.ection of the kite enhances visual realism. We calculate de.ection 
of the framework along the x-axis to represent de.ectionof thekite.Thede.ection dinFig.8(a)isgivenby: 
P 2 1 d =(l-2x)(l2 +2lx-2x ),I = ah3 , (6) 48EI12 where P is the tensile force from the kite string, 
E is the Young s modulus, I is the inertia moment ofthe wireframe, x is thedistance from they-axis, lis 
the initiallength, and band hare the width and height, respectively, of the sectional area of the skeleton. 
After obtaining d, we scale the kite to keep the initial width of the framework. We calculate the projected 
length l ' as shown in Fig.9(b). S inEqs. (1)and(2) isgivenby S = Sinit(l ' /l). The change of S induces 
the vibration of the kite along the y-axis as a real kite. Because if S is decreased, the lift and drag 
forces also decrease, then the tensile force of the kite string is weaken and d becomes smaller. Figure 
9: Kitede.ection.(a)De.nition ofd,(b)Fix thecenterand calculate theprojected length l ' . 3.7 KitewithComplexShape 
We have successfully simulated the motion of a rectangular kite. However, some Japanese kites have a 
more complex shape. For example, theshapeof theyakkocanberegarded asacombination of rectangles positioned 
in parallel with the axes. In this section, wediscuss simulation of akite made of several rectangles. 
Weassume thattheshapeofthekiteconsistsof several rectangles. Forcalculationof thepitch rotationmoment,wesubdivide 
thekite intoseveral rectangleseach of whichhasadifferentheightfrom the adjacent rectangles. For each 
triangular shape, we calculate the lift anddragforcesand thepointofforceapplicationusing thegraphs shown 
inFigs.4-6. For calculation of the roll rotation moment, we subdivide the kite intoseveral rectangleseach 
of whichhasadifferent widthfrom the adjacent rectangles. We calculate the lift and the drag forces and 
the point of force application in a manner similar to that described for the pitch rotation moment. We 
sum these forces and moments and obtain theforceand moment at and aroundthecenterofgravity of thekite. 
 C D Figure 10: Complex Shape. (a)We subdivide the kite into several rectangles for each of which has 
a different width from the adja­cent rectangles for the pitch rotation motion. (b)We calculate the liftanddrag 
forcesand theworkingpoint of the force forrollrota­tionmotioninamannersimilar tothatdescribedforpitch 
rotation motion.  4 UserInteraction The external causes of kite motion are the air.ow and kite string 
operation by the .yer. The behavior of the air.ow is controlled by agrid-based .uidsimulatorand thepositionof 
theendpointof thekitestring assumed tobeheldby the .yer(redpoint inFig.13) is controlled by a haptic 
interface device with force feedback to enhance the reality of the simulation. 4.1 FluidSimulation People.ykitesatdifferentlocations,e.g.,a.atarea,ahill 
top,ora mountainous region under various weather conditions,for example with weak or strong winds, or 
steady or unsteady winds. We can changethegeographicaldata toemulatedifferentlocationsandthe behavior 
of the kite naturally becomes dependent on them by use of the .uid simulator. Thegoverning equationsfor 
incompressibleviscous .uidaregiven by: .· u =0, (7) .u 1 .p+ ..2 u + f , (8) = -(u ·.)u - .t . where 
.is thekinematicviscosityand f is the externalforce, which is theresultantforceofgravityand theforceexertedby 
useropera­tion. The above equations are called Navier-Stokes equations. We adopt the methodproposedbyStam[1999] 
to solveEqs.(7) and (8) and determine the behavior of the air.ow. The direction and strength of the air.ow 
are controlled by changing the value of f at certain grid points, for example those on a speci.ed plane. 
The velocity of the air.ow at an arbitrary point is calculated by trilin­ear interpolation of the eight 
grid points of the cell that includes thepoint.Thevelocityof theair.ow u at theconnectionpointbe­tween 
thekiteand thekitestringisusedfor U inEqs.(1) and(2) to calculatethe lift anddragforces. We take into 
account the effects of swirl and separation of the air­.owaround thekiteusing thegraphsobtainedby experiment.The 
air.ow behind the kite can be assumed to have already passed the kiteanddoesnot comeback tothekiteinashort 
timeanddoesnot directly affect the kite motion. Hence, we neglect any effects from thekiteon theair.ow 
inour .uidsimulation.  4.2 KiteManipulation Theusercancontroltheair.owbyexerting aforceonacertaingrid 
point.However,forkite .ying inreal situations, theoperatorpulls the kite string back and forth or right 
and left to control its .ight. Thekitestringissubjectedtoa tensileforcedraggedbythekiteand the strength 
of the force changes depending on the strength of the air.ow and the size of the kite. To allow the user 
to have a feeling similar to real kite string control, we use a haptic interface device that enables 
3D input and output. The user can pull the kite string using the haptic interface device and obtain feedback 
force from the simulator. Both haptic and visual feedback provide an illusion of realkite .ying. The 
user operation is transferred to the simulator by displacing the position of the end point of the kite 
string proportional to device displacement. The force exerted on the end of the kite string is transferred 
tothedeviceoperatoras ßF F spring where F spring is in Eq.(5) and ßF is a coef.cient that can be changed 
by the user. By changing ßF , theforcefeedbackby thehaptic interfacedevicecan beadjustedforusersof variousagesfromchildren 
toadults.  5 Results We have implemented a kite .ying simulation system based our method,andpresent 
theresultsof experimentswhere theusercon­trolled thekiteusing ahaptic interfacedevice. Figure 11 shows 
models of rectangular, diamond-shaped, and yakko type kites with textures. The rectangular kite in Fig.11(a) 
consists of one rectangle and two tails for .ying simulation. It has another geometrical model for rendering 
which consists of 12 × 12 rectangles to express the effect of the de.ection described in Section 3.6. 
The diamond-shaped kite in Fig.11(b) has two mod­els with a tail to calculate the lift and drag forces. 
One of them consists of 5 rectangles along the y-axis and the other consists of 5 rectangles along the 
x-axis for pitching and rolling, respectively. Ithas thesamemodelforrendering astherectangularkiteanditis 
smoothed out by mapping the diamond-shaped texture. The yakko kite in Fig.11(c) has two tails and two 
different models for .ying simulation. One of them consists of a single rectangle for its body and another 
rectangle for each sleeve, for a total of three rectan­glesforpitching andtheotherconsitsof tworectanglesforrolling. 
The yakko kite has the same rendering model as the other kites to express itsde.ections. Figure 12 shows 
differences of the behavior of the kite when we changedtheenvironmentfor theair.owby.uidsimulation.For 
the kite in Fig.12(a), the air is horizontally .owing from left right and the kite .ied stably as the 
video accompanying this paper shows. On the other hand, the kite in Fig.12(b) .ied unstably because the 
air.ow was going upward at the right bottom corner naturally due to the slope inserted in the environment 
of .uid simulation. Figure13 shows thecontrol of thekiteby ahaptic interfacedevice (Phantom Omni; SensAble 
Technologies). The operator controls the position of the end point of the kite string and feels the tensile 
force of the kite string. The kite responses quickly enough to the operatorcontrol and when theoperatorpulls 
thestring, therelative velocityoftheair.owincreasesandthekitereceivesastronger lift force.Asaresult, 
thekite .ieshigherjust as in thereal world. The processing time including .uid simulation is very fast 
thanks totheuseof experimentaldataforthe lift anddragforcecalculation and a relatively coarse mesh of16 
×16×16gridsandisabout300 fps on aPC withPentium4,3GHz and2GB memory. C 4GEVCPIWNCT MKVG D &#38;KCOQPF 
UJCRGF MKVG E ;CMMQ MKVG Figure 11: Typical typesofJapanesekites.   6 ConclusionsandFutureWork In this 
paper, we have proposed a real-time kite .ying simulation systemusingtheexperimentaldataofthe.yingkite,a 
lift anddrag force calculation method based on aerodynamics, real-time .uid simulation, and tensile force 
calculation based on a mass-spring model. Furthermore, we have developed an interactive .ying kite game 
with a haptic interface device. Simulation of the interaction between thinobjectsand.uidhashighcomputational 
costs,but we sped up the calculation time by using the experimental data of the coef.cients of lift and 
drag and limiting the interaction to be only from .uid to solid. We concentrated our efforts on Japanese 
kites and picked a rect­angular kite as the basic type. We have extended our simulation method to be 
applicable to more complicated shapes by treating them as combinations of rectangles. We took into account 
the de­.ectionofthekiteframework andthekite tails.Wecouldreproduce the changes in the air.ow using a 
computational .uid solver based onthegridcausedbythe terrainandthedifferencesinaltitude. The data are 
available for only two different aspect ratios of the rectangle to calculate the coef.cients of lift 
and drag, and we used linear interpolation to calculate them for an arbitrary aspect ratio. For aspect 
ratios less than 1, the graphs become speci.c shapes. It is desirable to collect more data, and the interpolation 
method shouldbereconsidered infuturestudies. As future work, to make our system more enjoyable, we will 
im­prove our system to include two or more kites, which will .ght witheach other; this iscalledkenka-dako(.ghtingkites) 
and will berealizedonacomputer.Anotherdirectionof researchis toapply our system to design the shape of 
the kite and evaluate differences in performance and stability associated with modi.cations of its shape. 
 References ABBOTT, I., AND VON DOENHOFF, A. 1959. Theory of Wing Sections. DoverPublications. ASHLEY, 
H., AND LANDAHL, M. 1985. Aerodynamics of Wings andBodies. DoverPublications. AZUMA, A. 1992. Science 
ofRCAirplane andKite. DenpaJikken­sha(Japanese). BOURG, D. 2001. Physics forGameDevelopers. O ReillyMedia, 
Inc. COOKE, J. M., ZYDA, M. J., PRATT, D. R., AND MCGHEE, R. B. 1992. Npsnet: .ight simulation dynamic 
modeling using quaternions. Presence: Teleoper.VirtualEnviron.1,4,404 420. FELDMAN, B. E., O BRIEN, J. 
F., AND KLINGNER, B. M. 2005. Animating gases with hybrid meshes. In Proceedings of SIG-GRAPH2005,ACMPress,NewYork,NY,USA,904 
909. GOLDENTHAL, R., HARMON, D., FATTAL, R., BERCOVIER, M., AND GRINSPUN, E. 2007. Ef.cient simulation 
of inextensible cloth. In Proceedings of SIGGRAPH 2007, ACM Press, New York,NY,USA,49. GUENDELMAN, E., 
SELLE, A., LOSASSO, F., AND FEDKIW, R. 2005. Coupling water and smoke to thin deformable and rigid shells. 
In Proceedings of SIGGRAPH 2005, ACM Press, New York,NY,USA,973 981. ITO, T., AND KOMURA, K. 1979. Science 
of Kite. Shogaku-kan (Japanese). KATZ, J., AND PLOTKIN, A. 2004. Low-speed aerodynamics. Journal ofFluidsEngineering126,293. 
KLINGNER, B. M., FELDMAN, B. E., CHENTANEZ, N., AND O BRIEN, J. F. 2006. Fluid animation with dynamic 
meshes. In SIGGRAPH 06:ACMSIGGRAPH2006Papers,ACM,New York,NY,USA,820 825. LOSASSO, F., GIBOU, F., AND 
FEDKIW, R. 2004. Simulating water and smoke with an octree data structure. In Proceedings of SIGGRAPH 
2004, ACM Press, New York, NY, USA, 457 462. MITANI,J., AND SUZUKI,H. 2004. Makingpapercraft toysfrom 
meshes using strip-based approximate unfolding. In SIGGRAPH 2004,259 263. MORI,Y., AND IGARASHI,T.2007.Plushie: 
an interactivedesign system for plush toys. In SIGGRAPH 07: ACM SIGGRAPH 2007papers,ACM,NewYork,NY,USA,45. 
O-DAKO. Gddweb : Shirone o-dako. http://gddweb.org/odako.html. RAMAKRISHNANANDA, B., AND WONG, K. C. 
1999. Animating bird .ight using aerodynamics. The Visual Computer 15, 10, 494 508. REGENIE,V., ETAL. 
1992. Thef-18high alpharesearch vehicle: Ahigh-angle-of-attack testbed aircraft. ROBINSON-MOSHER, A., 
SHINAR, T., GRETARSSON, J., SU, J., AND FEDKIW, R. 2008. Two-way coupling of.uids to rigid and deformable 
solids and shells. In SIGGRAPH 08: ACM SIG-GRAPH2008papers,ACM,NewYork,NY,USA,1 9. ROGALLO, F., SLEEMAN, 
W., AND CROOM, D. 1965. Resume of recentparawing research. NASATechnicalReports. ROGALLO, F. 1968. Nasa 
research on .exible wings. Annals ofthe NewYorkAcademy ofSciences154,2InternationalCongress on SubsonicAeronautics,953 
961. SMITH, H. 1992. The Illustrated Guide to Aerodynamics. Tab Books. STAM, J. 1999. Stable .uids. In 
Proceedings of SIGGRAPH 1999, ACM Press/Addison-Wesley Publishing Co., New York, NY,USA,121 128. WANG, 
Z. 2008. Aerodynamic ef.ciency of.apping.ight: analy­sis of a two-stroke model. Journal ofExperimentalBiology211, 
2,234. WITHERS, P. C. 1981. An aerodynamic analysis of bird wings as .xed aerofoils. Journal ofExperimentalBiology90,143 
162. WU,J., AND POPOVIC´,Z. 2003. Realisticmodeling ofbird .ight animations. In SIGGRAPH 03:ACMSIGGRAPH2003Papers, 
ACM,NewYork,NY,USA,888 895.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1581082</article_id>
		<sort_key>90</sort_key>
		<display_label>Pages</display_label>
		<article_publication_date>08-04-2009</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[The Bespoke 3DUI XNA Framework]]></title>
		<subtitle><![CDATA[a low-cost platform for prototyping 3D spatial interfaces in video games]]></subtitle>
		<page_from>55</page_from>
		<page_to>61</page_to>
		<doi_number>10.1145/1581073.1581082</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1581082</url>
		<abstract>
			<par><![CDATA[<p>This paper presents the Bespoke 3DUI XNA Framework, an open-source software platform for research in 3D user interaction. The Bespoke 3DUI XNA Framework distinguishes itself from other platforms, in that it provides 3D user interface machinery in a game development framework. This combination leverages low-cost, widely available game technologies, enabling researchers to investigate 3DUI techniques, and providing game developers a foundation for prototyping 3DUIs in commercial video games.</p> <p>The paper explores the functionality and utility of the software library and describes how researchers and game makers can leverage the platform to investigate 3D user interfaces in the context of prototypical interactive experiences.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[3DUI]]></kw>
			<kw><![CDATA[framework]]></kw>
			<kw><![CDATA[game development]]></kw>
			<kw><![CDATA[user interface]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1570319</person_id>
				<author_profile_id><![CDATA[81440619172]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Varcholik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570320</person_id>
				<author_profile_id><![CDATA[81100283513]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Joseph]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[LaViola]]></last_name>
				<suffix><![CDATA[Jr.]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570321</person_id>
				<author_profile_id><![CDATA[81410595526]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Charles]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hughes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bespoke Software 2008. The Bespoke Open Sound Control Library. Available from: http://www.bespokesoftware.org/osc/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>835847</ref_obj_id>
				<ref_obj_pid>580521</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bierbaum, A, Just, C., Hartlink, P., Meinert, K., Baker, A. and Cruz-Neira, C. 2001. VR Juggler: a virtual platform for virtual reality application development. In <i>Virtual Reality, 2001. Proceedings. IEEE</i>, 89--96.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>993837</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bowman, D. Kruijff, E., LaViola, J. and Poupyrev, I. 2004. <i>3D User Interfaces: Theory and Practice</i>. Addison Wesley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1544280</ref_obj_id>
				<ref_obj_pid>1544197</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Charbonneau, E. Miller, A. Wingrave, C. and LaViola, J. 2009. RealDance: An Exploration of 3D Spatial Interfaces for Dancing Games. In <i>Proceedings of the IEEE Symposium on 3D User Interfaces</i>, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1536559</ref_obj_id>
				<ref_obj_pid>1536513</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Chertoff, D. Byers, R. and LaViola, J. 2009. An Exploration of Menu Techniques using a 3D Game Input Device. In <i>Proceedings of the Foundations of Digital Games</i>, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Delta3D 2009. Delta3D: Open Source Gaming &amp; Simulation Engine. Available from: http://www.delta3d.org/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Epic Games 2008. Unreal Technology. Available from: http://www.unrealtechnology.com/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>261549</ref_obj_id>
				<ref_obj_pid>261540</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Fruend, Y. and Schapire, R. E. 1997. A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting. <i>Journal of Computer and System Sciences 55</i>, 119--139.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Fristrom, J. 2008. Postmortem: Torpex Games' Schizoid. Available from: http://www.gamasutra.com/view/feature/3796/postmortem_to rpex_games_schizoid.php.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1032359</ref_obj_id>
				<ref_obj_pid>1032275</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Goslin, M., Mine, M. 2004. The Panda3D Graphics Engine. <i>Computer 37</i>, 112--114.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Hutchison, D. 2008. Introducing DLP 3-D TV. Available from: http://dlp.com/downloads/Introducing%20DLP%203D%20H DTV%20Whitepaper.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[ID Software 2009. id Technology Licensing. Available from: http://www.idsoftware.com/business/technology/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Irawati, S. Sangchul, A., Jinwook, K. and Heedong, K. 2008. VARU Framework: Enabling Rapid Prototyping of VR, AR and Ubiquitous Applications. In <i>Virtual Reality Conference, 2008. VR '08. IEEE</i>, 201--208.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>858134</ref_obj_id>
				<ref_obj_pid>857202</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Kato, H., Billinghurst, M. 1999. Marker Tracking and HMD Calibration for a Video-Based Augmented Reality Conferencing System. In <i>Proceedings of the 2nd IEEE and ACM International Workshop on Augmented Reality</i>, San Francisco, CA, 1999, 85--94.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187631</ref_obj_id>
				<ref_obj_pid>1187619</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Katzourin, M., Ignatoff, D., Quirk, L,. LaViola, J., Jenkins, O. 2006. Swordplay: Innovating Game Development through VR. In <i>Proceedings of the IEEE computer graphics and applications</i>, Nov./Dec. 2006, 15--19.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>774467</ref_obj_id>
				<ref_obj_pid>774464</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Kelso, J. Satterfield, S. G., Arsenault, L. E., Ketchan, P. M., and Friz, R. D. 2003. DIVERSE: A Framework for Building Extensible and Reconfigurable Device-Independent Virtual Environments and Distributed Asynchronous Simulations. <i>Presence: Teleoperators &amp; Virtual Environments 12</i>, 19--36.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1246876</ref_obj_id>
				<ref_obj_pid>1246870</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Kessler, G. D., Bowman, D. A., and Hodges, L. F. 2000. The Simple Virtual Environment Library: An Extensible Framework for Building VE Applications. <i>Presence: Teleoperators &amp; Virtual Environments 9</i>, 187--208.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Microsoft 2008. Microsoft Research Faculty Summit 2008: DemoFest. Available from: http://research.microsoft.com/enus/um/redmond/events/fs2008/demofest.aspx.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Microsoft 2009. XNA Developer Center. Available from: http://msdn.microsoft.com/en-us/xna/default.aspx.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Natural Point 2008. TrackIR: 6DOF Head Tracking. Available from: http://www.naturalpoint.com/trackir/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1363203</ref_obj_id>
				<ref_obj_pid>1363200</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Oda, O., Lister, L., White, S., Feiner, S. 2007. Developing an augmented reality racing game. In <i>Proceedings of the Proceedings of the 2nd international conference on INtelligent TEchnologies for interactive enterTAINment</i>, Cancun, Mexico, 2007 ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Peek, B. 2009. Managed Libary for Nintendo's Wiimote. Available from: http://www.codeplex.com/WiimoteLib.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122753</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Rubine, D. 1991. Specifying gestures by example. In <i>International Conference on Computer Graphics and Interactive Techniques</i>, 329--337.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>641636</ref_obj_id>
				<ref_obj_pid>641633</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Schmalstieg, D., Fuhrmann, A., Hesina, G., Szalavari, Z., Encarnasalo, L. M., Gervautz, M. and Purgathofer, W. 2002. The Studierstube Augmented Reality Project. <i>Presence: Teleoperators &amp; Virtual Environments 11</i>, 33--54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>638554</ref_obj_id>
				<ref_obj_pid>638553</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Stanley, K. O. and Miikkulainen, R. 2002. Evolving Neural Networks through Augmenting Topologies. <i>Evolutionary Computation 10</i>, 99--127.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Texas Instruments 2009. 3D TV - DLP HDTV. Available from: http://www.dlp.com/hdtv/3-d_dlp_hdtv.aspx.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Troillard, C. 2008. Wiimote Image. Available from: http://www.osculator.net/wiki/uploads/Main/pry-wiimote.gif.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Varcholik, P., Barber, D., Nicholson, D. 2008. Interactions and Training with Unmanned Systems and the Nintendo Wiimote. In <i>Proceedings of the 2008 Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC '08)</i>, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1601220</ref_obj_id>
				<ref_obj_pid>1601159</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Varcholik, P., LaViola, J., Nicholson, D. 2009. TACTUS: A Hardware and Software Testbed for Research in Multi-Touch Interaction. In <i>Proceedings of the Human-Computer Interaction International (HCI International)</i>, San Diego, CA, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Varcholik, P., Merlo, J. 2008. Gestural Communication with Accelerometer-based Input Devices and Tactile Displays. In <i>Proceedings of the 26th Army Science Conference</i>, Orlando, FL, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Bespoke 3DUI XNA Framework: A Low-Cost Platform for Prototyping 3D Spatial Interfaces in Video Games 
Paul D. Varcholik* Joseph J. LaViola Jr. Charles Hughes Media Convergence Laboratory School of Electrical 
Engineering School of Electrical Engineering Institute for Simulation &#38; Training and Computer Science 
and Computer Science University of Central Florida University of Central Florida University of Central 
Florida Abstract This paper presents the Bespoke 3DUI XNA Framework, an open-source software platform 
for research in 3D user interaction. The Bespoke 3DUI XNA Framework distinguishes itself from other platforms, 
in that it provides 3D user interface machinery in a game development framework. This combination leverages 
low­cost, widely available game technologies, enabling researchers to investigate 3DUI techniques, and 
providing game developers a foundation for prototyping 3DUIs in commercial video games. The paper explores 
the functionality and utility of the software library and describes how researchers and game makers can 
leverage the platform to investigate 3D user interfaces in the context of prototypical interactive experiences. 
Keywords: 3DUI, user interface, framework, game development 1 Introduction 3D user interfaces [Bowman 
2004] give users the ability to spatially interact with 3D virtual worlds because they provide nat­ural 
mappings from human movement to interface controls. These interfaces, common in virtual and augmented 
reality applications, give users, rich, immersive, and interactive experiences that can mimic the real 
world or provide magical, larger than life interac­tion metaphors [Katzourin 2006]. With the latest generation 
of video game hardware, 3D user interfaces are emerging as the gaming interaction paradigm of the future. 
The popularity of devices such as the Sony Eye-Toy, the Nintendo Wii, and 3D DLP HDTV are making it possible 
for gamers to interface with video games using 3D spatial input, and for developers to leverage concepts 
from virtual and augmented reality such as head tracking and stereoscopic vision. However, developing 
software for these interfaces can be a daunting process for newcomers due to hardware inaccessibility 
and a lack of 3DUI software tools. Thus, investigators must overcome a number of technical challenges 
to develop a 3DUI platform before they can begin research in this area. Through an extensive literature 
review, and collaboration with 3DUI experts, we have compiled a set of requirements necessary for a 3DUI 
research platform. These requirements can be categorized into three areas: 1) high-level non-functional 
requirements; 2) primary components essential for basic 3DUI research; and 3) secondary elements necessary 
for longer-term research efforts. These categories include: * pvarchol@ist.ucf.edu jjl@cs.ucf.edu ceh@cs.ucf.edu 
Copyright &#38;#169; 2009 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for components of this work owned by others than ACM 
must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from 
Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org. Sandbox 2009, New Orleans, 
Louisiana, August 4 6, 2009. High-Level Non-Functional Requirements Commercial-off-the-shelf (COTS) 
hardware  Open-source software  Modern programming language, accessible to novice software developers 
 Broad application  Extensibility  Primary Components 2D/3D graphics rendering  6DOF head tracking 
 3D motion controller (ala Nintendo Wiimote)  Stereoscopic rendering  Secondary Components Scene 
management  Content pipeline with support for common scene elements (e.g. 3D models, animation, terrain) 
 3D audio  Network support (e.g. multiplayer services)  2D/3D Gesture recognition  Physics  Motion 
tracking (of multiple, non-head points (e.g. hands, body or object)  Chroma-key extraction  Head-mounted 
displays  2D UI widgets (e.g. menus, buttons)  Recording user experience  Cross-platform communication 
 Notably, the requirements call for a generic platform for creating virtual environment but with a specific 
set of technologies for interacting with those worlds. In short, these requirements describe a game engine 
geared toward the input devices and displays involved in 3DUI development. This paper presents the Bespoke 
3DUI XNA Framework, an extensible, open-source software library developed to meet many of these requirements, 
thereby enabling the rapid creation of sophisticated 3DUIs. A detailed discussion is provided on design 
decisions, compatible hardware, software architecture and the employment of the framework. The work presented 
has been adopted by a number of university colleagues, members of the open-source community, and as the 
development platform for a graduate course in 3DUIs at the University of Central Florida. This paper 
discusses feedback obtained from users of the framework as insight into the usefulness of the system. 
 2 Related Work The idea of providing an open-source framework for supporting 3D spatial interaction 
research and development is not new. There have been many different software frameworks and toolkits, 
such as the SVE toolkit [Kessler et al. 2000], VR Juggler &#38;#169; 2009 ACM 978-1-60558-514-7/09/0008 
$10.00 [Bierbaum et al. 2001], DIVERSE [Kelso et al. 2003], Studierstube [Schmalstieg et al. 2002], 
ARToolKit [Kato 1999], and the VARU Framework [Irawati et al. 2008], that aid in developing spatial 3D 
interfaces. However, these frameworks and toolkits were designed with more conventional VR and AR applications 
in mind. Thus, they lack the video game specific development tools that the Bespoke 3DUI XNA Framework 
provides. There are several open source game toolkits available that make it easier to build video games 
by providing important infrastructure components such as 3D rendering, asset management, sound, event 
handling, scene graph support, and physics simulation. Examples of these types of toolkits include the 
Microsoft XNA Game Studio [Microsoft 2009], Panda3D [Goslin 2004], and Delta3D [Delta3D 2009]. Although 
these game development environments provide sophisticated tools and support for developing video games, 
they generally do not focus on 3DUI and virtual reality-based games. One development framework that is 
closest in spirit to the Bespoke 3DUI XNA Framework is Goblin XNA [Oda 2007]. Goblin XNA is a framework 
for research on 3D user interfaces, including mobile augmented reality and virtual reality, with an emphasis 
on games. It is written in C# and based on the Microsoft XNA platform [Microsoft 2009]. Goblin XNA has 
many similarities to the Bespoke 3DUI XNA Framework in terms of using XNA as its underlying platform 
and supporting head tracking and 3D spatial interfaces. However, Goblin XNA is primarily focused on augmented 
reality games while the Bespoke 3DUI XNA Framework targets virtual reality-based games using 3D TVs and 
monitors and 3D motion controllers. To the best of our knowledge, the Bespoke 3DUI XNA Framework is one 
of the first development environments to provide comprehensive support for both 3D spatial interfaces 
and video game creation.  3 The Bespoke 3DUI XNA Framework The Bespoke 3DUI XNA Framework is organized 
as a collection of .NET assemblies, application samples, and documentation; and is packaged within a 
Windows installer for easy distribution and inclusion of third-party dependencies. The Framework is written 
in the C# programming language and targets the .NET 3.0 Framework and .NET 2.0 runtime. As the name implies, 
the Bespoke 3DUI XNA Framework is built on top of the Microsoft XNA platform. XNA is a set of software 
libraries, tools, and community resources focused on enabling game developers to be successful on Microsoft 
gaming platforms [Microsoft 2009]. We chose C# and XNA to match our requirement of a system that utilized 
a modern programming language, and one accessible to novice software developers. We have anecdotal evidence, 
from many years of instruction on software development, that C# does indeed meet the criteria of a modern 
language that is approachable by novice programmers; and yet the environment is powerful enough to support 
real-time interactive simulation and games. Moreover, since XNA s initial release in December 2006, thousands 
of games have been created with the platform; including commercial games that have been released on Microsoft 
s Xbox LIVE Arcade service [Fristrom 2008]. We can confidently state that the XNA platform delivers a 
solid development environment for students and hobbyists as well as for professional game makers. The 
Bespoke 3DUI XNA Framework is an open-source product, distributed with a complete set of source code 
and a significant number of application samples. For non-3DUI-specific game development, the Bespoke 
3DUI XNA Framework has no hardware requirements beyond those of XNA (which are primarily concerned with 
a graphics card that supports 3D acceleration and pixel shaders). When developing 3DUI applications, 
the Framework supports particular hardware for stereoscopic rendering, 6DOF head tracking, and 3D motion 
control. Figure 1 pictures a development workstation and typical hardware employed for creating games 
and 3DUIs with the Bespoke 3DUI XNA Framework. The specific hardware components and their corresponding 
Framework elements are described in the sections below. Figure 1: Development workstation 3.1 Stereoscopic 
Rendering For stereoscopic rendering, we targeted the 3D DLP HDTV technology from Texas Instruments [Texas 
Instruments 2009]. This technology, incorporated into television sets by Mitsubishi and Samsung, generates 
independent views for the left and right eyes. The television is optically synchronized with a pair of 
shutter glasses worn by the viewer, and supplies images at 60Hz per eye (equivalent to 120Hz). To present 
the viewer with a 3D image, the video source must supply the television with an image stream following 
Texas Instrument s 3D Image Format [Hutchison 2008]. This format calls for the left and right eye images 
to be masked with alternate checkerboard patterns and then combined into a single image before transmission 
to the television. The 3D DLP technology correspondingly samples the interleaved image, reconstructing 
the left and right eye images for subsequent display. The Bespoke 3DUI XNA Framework supports the 3D 
DLP HDTV technology through: 1. A stereoscopic camera component 2. Independent render targets 3. Shader-based 
image masking  The stereoscopic camera component defines the concept of a left and right eye to correctly 
present the virtual camera s view and projection matrices from the corresponding perspective. The eyes 
are separated by a user-configurable interpupillary distance. Rendering a complete frame in 3D requires 
that the scene be drawn twice: first from one eye s perspective and then the other. Each perspective 
is rendered, not directly to the screen, but to independent render targets whose outputs are bitmaps 
that are fed to the image masking process. The image masking is offloaded to the graphics processing 
unit (GPU) with a simple pixel shader written in the High-Level Shader Language (HLSL) and using the 
shader model 3.0 specification. The masking shader interleaves the left and right eye images and presents 
the final image to the display. By implementing render targets and shader-based image masking, the Bespoke 
3DUI XNA Framework is able to transform any scene into 3D. The masking process runs at 800 to 200 fps 
for resolutions of 800 by 600 to 1920 by 1080, respectively, thereby having only negligible influence 
on overall frame rates. There are a number of other considerations when rendering to this technology. 
First, the PC must connect to the television using an HDMI or DVI video cable. This is required, because 
the resolution must be 1920 by 1080 (1080p) to render in 3D. Additionally, video drivers must be configured 
so that they do not scale the display, nor should an application scale the image (e.g. by embedding the 
game in a resizable window). Any scaling will misalign the masking during the television s sampling and 
image reconstitution. Therefore, applications must run in full-screen mode. At the Interactive Systems 
and User Experiences Lab (IS&#38;UE) at the University of Central Florida, we have employed ten Samsung 
50 3D DLP HDTVs with Shutter Glasses from TriDef (pictured in Figure 1). Each television costs approximately 
$1,500 with an additional $200 for a 2-pack of 3D shutter glasses, software, and IR sync cable/emitter. 
 3.2 Head Tracking Head tracking refers to the ability to track the position and orientation of the 
user s head [Bowman 2004]. This information can be used as input, commonly as a means to manipulate the 
virtual camera. We have adopted a commercial-off-the-shelf, 6DOF optical head tracking solution from 
Natural Point called the TrackIR [Natural Point 2008]. Pictured in Figure 2, the TrackIR mounts atop 
the display and detects a triangular array of infrared points within a 46° field of view. The IR points 
can be actively transmitted to the TrackIR, or the TrackIR can emit IR for use with a retro-reflective 
clip. The device operates at 120fps. Figure 2: NaturalPoint TrackIR and retro-reflective clip [Natural 
Point 2008] The TrackIR is programmatically accessible through a Component Object Model (COM) assembly 
provided free of charge from Natural Point. COM components can be accessed through the .NET Interop service 
and treated as native code elements by the Bespoke 3DUI XNA Framework. To make the device accessible 
to XNA, a small wrapper class encapsulates the functionality and makes the TrackIR behave similarly to 
the native XNA keyboard, mouse, and gamepad input devices. Additionally, the Bespoke 3DUI XNA Framework 
provides a virtual camera that is bound to the TrackIR input; giving game developers an instant option 
for a first-person, head-tracked camera. The framework also exposes a user-configurable sensitivity value 
for non-isomorphic mapping. The TrackIR is sold for ~$130, making it a fairly low-cost solution for 6DOF 
head tracking.  3.3 3D Motion Controller With the recent popularity of the Nintendo Wii, 3D motion control 
has garnered considerable attention from simulation and game developers. Game makers now have strong 
commercial motivation to explore spatial interaction, and can do so cost effectively with commercial-off-the-shelf 
input devices such as the Nintendo Wiimote. Though initially designed for use on the Nintendo Wii, the 
Wiimote has been adapted for use on the PC. The Wiimote connects to the PC using the Bluetooth communication 
protocol and transmits data at 100fps. The device includes accelerometers for detecting forces applied 
to three axes (illustrated in Figure 3). Figure 3: Nintendo Wiimote [Troillard 2008] The open-source 
community has embraced the Wiimote, having reverse engineered the messaging protocol, and has provided 
a number of software libraries for PC-to-Wiimote communication. We have adopted one of these libraries, 
Brian Peek s Managed Wiimote Library [Peek 2009], for use with the Bespoke 3DUI XNA Framework. Brian 
Peek s library is a general-purpose package that allows a Wiimote to be accessed by any .NET assembly, 
not simply XNA. The Bespoke 3DUI Framework wraps the Wiimote functionality and makes it available in 
a manner consistent with other XNA input devices. The Wiimote also includes seven input buttons, a digital 
directional pad, a speaker, vibration system, and an infrared camera, all of which are accessible through 
the Wiimote components incorporated into the Bespoke 3DUI XNA Framework. The Wiimote data is associated 
with a player enumeration for supporting up to four simultaneous Wiimotes; and the 3-axis accelerometer 
data is augmented with timestamp information for use with the gesture recognition system (described further 
in the next section) [Varcholik 2008]. While the Wiimote is a very capable and inexpensive spatial input 
device, it has a few limitations. First, the Wiimote is incapable of providing directionality with the 
accelerometers alone. For example, if one were to place the Wiimote upright on a table, in its natural 
position with the buttons facing the ceiling, the accelerometers alone could not detect the angle of 
rotation around the vertical axis. Likewise, the accelerometers cannot detect if the Wiimote is closer 
to or farther away from the user. In other words, the Wiimote s accelerometers cannot detect specific 
pointing angles or distance. This is where the Wiimote s infrared camera comes into play. With two neighboring 
IR source points, the Wiimote can determine distance from the IR source and relative orientation. Another 
consideration is that the device must be in motion for the accelerometers to be used for gesture recognition. 
While it is possible to create one non-moving gesture per orientation, a gesture recognition system would 
not be able to disambiguate static poses. In summary, the data collected from the Wiimote s accelerometers 
corresponds to forces applied to three axes: longitudinal roll (motion along the X-axis), pitch (motion 
along the Y-axis), and the upright/upside-down orientation of the Wiimote (vertical motion along the 
Z-axis). The Wiimote is a relative motion device, in that there is no innate sense of absolute position 
in space. To root the Wiimote, at least with respect to distance and directionality, the IR camera must 
be employed. However, the developer must consider the strong likelihood of camera occlusion, particularly 
when using the Wiimote as a full 3D motion input device.  3.4 Gesture Recognition One of the key features 
of the Bespoke 3DUI XNA Framework is the gesture recognition system; a machine-learning platform that 
enables game makers to create an arbitrary set of 3D gestures and map them to gameplay elements. The 
recognition system is separated into two primary components: the trainer and the classifier. The training 
component collects accelerometer data from the Nintendo Wiimote and associates the data with a user-defined 
label identifying the gesture. A gesture sample is the accelerometer data collected between the press 
and release of the Wiimote s B button (the trigger button). Any number of samples can be used to train 
a gesture, but our experimentation shows good accuracy (> 94%) with as few as ten samples per gesture 
[Varcholik 2008]. A set of 29 features is used to linearly separate the gestures in the set. These features 
are extensions of work by Rubine [1991] on 2D symbol recognition, and are further described in [Varcholik 
2008]. Trained gestures can be serialized for reuse. Notably, no computed feature or weighting information 
is stored during serialization only the accelerometer data. This allows for alternate machine learning 
algorithms without invalidating previously trained gestures. The classification component records an 
unlabeled sample and attempts to match it against a trained gesture. Classification can be accomplished 
in 10ms or less, even on modest computing hardware (our development machine was a laptop with a 1.8Hhz 
Intel Core2 Duo and 2GB of RAM). The gesture recognition system supports both one- and two­handed gestures. 
For two-handed gestures, the user can train the data simultaneously, utilizing the same B button start/stop 
mechanic for each hand. The system uses a linear classifier to make its recognition decisions. We have 
also experimented with an AdaBoost implementation [Freund and Schapire 1997], and an artificial neural 
network (ANN) evolved using NeuroEvolution of Augmenting Topologies (NEAT) [Stanley and Miikkulainen 
2002]. Our unpublished experiments show slightly better accuracy for AdaBoost over the linear classifier, 
but at the expense of higher training cost. The ANN implementation, with the topology and node weights 
evolved using NEAT, had the worst accuracy and highest training cost. The experiment was conducted using 
three gesture sets and with two sample sizes per set. We hypothesize that the ANN/NEAT implementation 
performed poorly because of the small amount of training data (ten and twenty samples per gesture). These 
results are preliminary and not rigorously analyzed so we cannot draw any formal conclusions. Informally, 
we note that the linear classifier, included with the Bespoke 3DUI XNA framework, can train in almost 
real-time with ten gestures in the set and ten samples per gesture; and can classify in real time with 
accuracy greater than 94%. We have employed this system for computer mediated, human-to-human communication 
[Varcholik 2008] and for human-robot communication [Varcholik 2008]. The recognition system has also 
been extended to support 2D symbol recognition for multi-touch surface interaction [Varcholik 2009]. 
 3.4 Game Engine Features What is distinct about the Bespoke 3DUI XNA Framework is that it provides 
3D user interface machinery in a game development framework. This combination enables academic researchers 
to investigate 3DUI techniques while leveraging game technologies, and provides game developers a foundation 
for prototyping 3DUIs in commercial video games. Thus far we ve discussed some of the 3DUI specific functionality 
of the framework. In this section, we describe some components of the library that are commonly found 
in game engines. As previously stated, the Microsoft XNA platform is a set of software libraries and 
tools designed to enable the creation of games on Microsoft gaming platforms. The software libraries 
support 2D/3D graphics rendering, 3D sound, multi-player networking, an extensible content pipeline, 
and include a comprehensive set of game-related helper classes. However, XNA should not be classified 
as an actual game engine. A game engine generally includes higher-level constructs such as scene management, 
object interaction, animation, graphical user interfaces, and artificial intelligence. Often, a game 
engine provides rendering through a graphics API such as Direct3D or OpenGL. A game engine is commonly 
considered middleware in that it sits between the rendering platform and the game code itself. XNA is 
built atop Microsoft DirectX, and in this respect acts as middleware between a game and the rendering 
system. However, XNA remains fairly low-level in the functionality that it provides, and is thus better 
compared to OpenGL than to games engines such as id Software s Quake III [id Software 2009] or Epic Games 
Unreal [Epic Games 2008]. The Bespoke 3DUI XNA Framework extends the XNA platform to provide game engine 
features including those listed in Table 1. Feature Description Virtual Cameras An abstraction of the 
View and Projection matrices required for rendering, coupled with specialized attributes (e.g. chase 
camera, stereoscopic camera, orthographic camera). Input Devices Componentized abstractions of input 
devices including: mouse; keyboard; gamepad; Wiimote; and the TrackIR, which generally provide frame-to-frame 
history for relative tracking. Actors Scene object abstractions for dynamic and non-dynamic objects that 
generally have an associated 3D model. Animation Animated actor. Menus &#38; GUI 2D elements for game 
state transitions and in-game graphical user interfaces (e.g. buttons, fonts/strings, and menus). Scene 
Loading XML serialization of scene elements (e.g. actors, cameras, UI elements) for non­programmatic 
scene population. Allows for in-game scene reloading. Scene Management An abstraction of the active scene 
that provides an entry and access point for game objects. Encapsulates the primary render and update 
threads. Particle System 3D particles for producing effects (e.g. explosions, smoke, projectiles) Terrain 
2D texture tiling over a height map Skybox 5-sided texture mapped cube for scene background floor removed. 
Post Processing Effects system applied after the scene has been rendered but before final display. Common 
effects: bloom, god rays, blur Windows Forms Allows an XNA application to be embedded within a Windows 
Form. World Editor Enables the dynamic compilation of XNA assets for WYSIWYG scene creation and asset 
previewing. Cross-platform Communication Generic network communication mechanism useful for serializing 
input device data to remote clients. Table 1: Game engine features A few of these components are particularly 
noteworthy: the scene management system, the world editor, and the cross­platform communication component. 
The scene management system provides an organizational element that is otherwise absent from XNA. Specifically, 
the scene manager acts as a collection point for all of the objects in the game and triggers the rendering 
of and interaction between those objects. Scene objects are actors, 2D UI elements, terrain, sky boxes, 
and sprites. These elements are added to the scene management system programmatically or via the XML 
scene loader. Scene objects can be disabled so that they are not considered during the update and render 
threads, or can be made invisible so that they are updated but not rendered. The world editor, pictured 
in Figure 4, is a WYSIWYG scene creation tool built on top of the Bespoke 3DUI XNA Framework. As such, 
the editor uses the same rendering system as games authored using the framework. This is essential for 
accurately previewing the look of a 3D model as it will appear in-game. The scene author first populates 
a content library a categorized list of source assets that can be compiled into game-ready content. 
Source assets come from a digital content creation (DCC) package in a format that generally contains 
more information about the asset than can be used in-game. Compiling the source asset into a game-ready 
format means that we process the asset through the XNA content pipeline, extracting only that information 
that is useful at runtime and writing it into a usable format. Ordinarily, XNA asset compilation is performed 
along with source code. Moving this process to the world editor allows the user to compile large sets 
of assets out-of-band. Content libraries can be serialized and loaded into the world editor for easy 
reuse. Once an asset is included in the content library and compiled, it can be added to a scene. Scene 
objects can be selected through a mouse click, or through a categorized scene explorer window. Once selected, 
objects can be transformed through chorded keyboard-mouse movements in a manner similar to popular 3D 
modeling packages such as 3D Studio Max or Maya. An object s attributes can also be manipulated through 
a Visual Studio-like properties window. Indeed, the world editor borrows heavily from the dockable window 
scheme of Visual Studio. Scene s can be serialized to an XML document for in-game loading. Figure 4: 
World Editor The last game engine feature, discussed in more depth, is cross-platform communication. 
XNA supports the PC, Xbox 360, and the Microsoft Zune. The Bespoke 3DUI XNA Framework extends support 
to non-Microsoft environments such as MacOS or Linux through the Bespoke Open Sound Control (OSC) Library 
[Bespoke Software 2008]. OSC is an open, lightweight, message­based protocol that enables, for example, 
input device data to be transmitted over the network. The Bespoke OSC implementation uses UDP/IP as the 
transport protocol, and includes support for unicast, broadcast, and multicast. A remote application, 
therefore, need only support the OSC protocol and network communication via UDP; and need not be hosted 
on a Microsoft operating system or written in C#. Applications of the Bespoke 3DUI XNA Framework can 
use this mechanism for transmitting Wiimote and head-tracking data to remote clients. As a final note 
on hardware and operating systems we note that, while XNA supports the Xbox 360 and Microsoft Zune, only 
subsets of the .NET Framework Class Library (FCL) are available on those platforms. Additionally, of 
the three XNA-supported platforms, only the PC is capable of accessing external hardware. As such, the 
Bespoke 3DUI XNA Framework must operate on a PC in order to support devices such as the Nintendo Wiimote 
and the NaturalPoint TrackIR.  4 Case Studies The Bespoke 3DUI XNA Framework has been utilized as the 
development platform for a graduate course in 3DUIs at the University of Central Florida. In the context 
of that course, a number of students have published papers on games and techniques developed using the 
Framework. This section discusses two of these projects, pictured in Figure 5: RealDance [Charbonneau 
2009] and an exploration of menu techniques using a 3D game input device (the Nintendo Wiimote) [Chertoff 
2009]. These projects, and several others, were demonstrated at the 2008 Microsoft Faculty Summit [Microsoft 
2008]. Figure 5: Projects created with the Framework: RealDance (left), Menu Techniques (right) RealDance, 
is a rhythm and dance game in the spirit of games like Dance Revolution and Guitar Hero. The user is 
equipped with four Nintendo Wiimotes strapped to the arms and legs. A green icon signals a punch, and 
a purple icon a kick; with the icons displayed on the left or right side of the screen to indicate which 
arm/hand to use. A video of the song is played in the background and scoring is based on how accurately 
the user matches his motions with the song. In addition to the game itself, the two student developers 
designed a song creation system to associate motion icons with a song, and created adjustable straps 
to secure the Wiimotes to the arms and legs. They also employed a novel menu navigation system, using 
gesture recognition, whereby the user punches to the left and right to move through the menu, and claps 
their hands to make a selection. The game includes four popular songs with motion tracks that vary in 
difficulty. The Menu Techniques project examined the efficacy of linear versus radial menus when using 
a 3D spatial input device such as the Nintendo Wiimote. To this end, the two-student development team 
created a simple game environment that asked the user to uncover a hidden artifact through the successive 
selection and application of digging tools accessed through a hierarchical menu. The students ran a within-subjects 
experiment with twenty subjects to compare their menu techniques. These case studies represent both 
the entertainment and the research capacity of the Bespoke 3DUI XNA Framework. Note that each project 
was developed in approximately four weeks (the amount of time each two-person group had for their final 
project in the 3DUI course).  5 Framework Evaluation As of this writing, the Bespoke 3DUI XNA Framework 
has been publicly available for roughly sixteen months and is in its fourth major revision. These revisions 
have been created in direct response to feedback we have received on the Framework. Specifically, we 
have solicited feedback from the students of the two semesters of the 3DUI course and we have created 
an online survey for collecting feedback from external adopters. This feedback is presented here informally, 
but generally states that the Framework is useful, interesting, and easy to use. Figure 6 shows the aggregated 
feedback, of eighteen respondents, to a seven-level Likert scale concerning their overall reaction to 
the Framework. Figure 6: Feedback responses Other questions allowed for free-form responses on the strengths 
and weaknesses of the Framework and what users would like to see added. In general, users of the Framework 
thought that the platform provided many of the common components that are required for game development; 
and that having the Framework let developers concentrate on their game/research instead of the underlying 
technology of a game engine. Indeed, comments on the strengths of the Framework overwhelmingly support 
the contention that this platform helps users to quickly get games up­and-running. Feedback on the weaknesses 
of the system revealed two major themes: sparse documentation, and the lack of a physics system. Documentation 
is provided through a Microsoft help file (.chm) and a variety of code samples. The negative feedback 
indicates the need for more effort in documenting the system and providing more detailed tutorials. Indeed, 
many of the feedback respondents requested cookbook-style tutorials that would quickly demonstrate the 
use of a common feature. A physics system is a game engine component that simulates Newtonian physics 
and interactions between game objects. XNA does not provide a native physics system, and this is absent 
from the Bespoke 3DUI XNA Framework as well. Simple bounding sphere collision detection is included in 
the Framework, but more sophisticated collision detection and dynamic simulation is left to the user 
to implement. This is a focus for future work on the system. 6 Conclusions and Future Work With the 
latest generation of video game hardware, 3D user inter­faces are emerging as the gaming interaction 
paradigm of the future. Researchers and game makers require tools to enable the development of interesting 
3DUI techniques. Many commercial and open-source video game creation packages exist as do several 3DUI 
frameworks. However, there are few development platforms that combine 3D user interface machinery in 
a game development framework. The Bespoke 3DUI XNA Framework provides this combination and enables academic 
researchers to investigate 3DUI techniques while leveraging game technologies, and provides game developers 
a foundation for prototyping 3DUIs in commercial video games. This paper has described the Framework: 
its organization, 3DUI functionality, and game engine features; and the requirements that drove the development 
of the system. We have discussed a graduate course in 3DUI that has adopted the Framework, and have detailed 
two student-created projects that have used the Framework for entertainment and 3DUI research. Furthermore, 
we have informally presented an evaluation of the Framework that has revealed strengths and weaknesses 
of the system, and has provided direction for future work. Future work on the Framework will include 
improved documentation and tutorials, and will integrate a full-fledged physics system for collision 
detection and dynamic simulation. Additionally, the Framework does not satisfy all of the requirements 
that we identified for a 3DUI game development research platform. In particular, we intend to add support 
for augmented reality research through chroma-key extraction, head­mounted displays, and fiducial tracking. 
We will also add the ability to record the user s experience during a simulation, and include support 
for hand and body motion tracking. Finally, we will add support for the Wii Balance Board and the upcoming 
WiiMotion Plus hardware peripherals that offer even more options for exploring user interfaces and gameplay 
mechanics. The Bespoke 3DUI XNA Framework is available for download at http://www.bespokesoftware.org/3DUI. 
 7 Acknowledgements This work is supported in part by IARPA, SAIC, and by the National Science Foundation 
under award number DRL0638977.  References BESPOKE SOFTWARE 2008. The Bespoke Open Sound Control Library. 
Available from: http://www.bespokesoftware.org/osc/. BIERBAUM, A, JUST, C., HARTLINK, P., MEINERT, K., 
BAKER, A. And CRUZ-NEIRA, C. 2001. VR Juggler: a virtual platform for virtual reality application development. 
In Virtual Reality, 2001. Proceedings. IEEE, 89-96. BOWMAN, D. KRUIJFF, E., LAVIOLA, J. And POUPYREV, 
I. 2004. 3D User Interfaces: Theory and Practice. Addison Wesley. CHARBONNEAU, E. MILLER, A. WINGRAVE, 
C. AND LAVIOLA, J. 2009. RealDance: An Exploration of 3D Spatial Interfaces for Dancing Games. In Proceedings 
of the IEEE Symposium on 3D User Interfaces, 2009. CHERTOFF, D. BYERS, R. AND LAVIOLA, J. 2009. An Exploration 
of Menu Techniques using a 3D Game Input Device. In Proceedings of the Foundations of Digital Games, 
2009. DELTA3D 2009. Delta3D: Open Source Gaming &#38; Simulation Engine. Available from: http://www.delta3d.org/. 
EPIC GAMES 2008. Unreal Technology. Available from: http://www.unrealtechnology.com/. FRUEND, Y. AND 
SCHAPIRE, R.E. 1997. A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting. 
Journal of Computer and System Sciences 55, 119­ 139. FRISTROM, J. 2008. Postmortem: Torpex Games' Schizoid. 
Available from: http://www.gamasutra.com/view/feature/3796/postmortem_to rpex_games_schizoid.php. GOSLIN, 
M., MINE, M. 2004. The Panda3D Graphics Engine. Computer 37, 112-114. HUTCHISON, D. 2008. Introducing 
DLP 3-D TV. Available from: http://dlp.com/downloads/Introducing%20DLP%203D%20H DTV%20Whitepaper.pdf. 
ID SOFTWARE 2009. id Technology Licensing. Available from: http://www.idsoftware.com/business/technology/. 
IRAWATI, S. SANGCHUL, A., JINWOOK, K. AND HEEDONG, K. 2008. VARU Framework: Enabling Rapid Prototyping 
of VR, AR and Ubiquitous Applications. In Virtual Reality Conference, 2008. VR '08. IEEE, 201-208. KATO, 
H., BILLINGHURST, M. 1999. Marker Tracking and HMD Calibration for a Video-Based Augmented Reality Conferencing 
System. In Proceedings of the 2nd IEEE and ACM International Workshop on Augmented Reality, San Francisco, 
CA, 1999, 85-94. KATZOURIN, M., IGNATOFF, D., QUIRK, L,. LAVIOLA, J., JENKINS, O. 2006. Swordplay: Innovating 
Game Development through VR. In Proceedings of the IEEE computer graphics and applications, Nov./Dec. 
2006, 15-19. KELSO, J. SATTERFIELD, S.G., ARSENAULT, L.E., KETCHAN, P.M., AND FRIZ, R.D. 2003. DIVERSE: 
A Framework for Building Extensible and Reconfigurable Device-Independent Virtual Environments and Distributed 
Asynchronous Simulations. Presence: Teleoperators &#38; Virtual Environments 12, 19-36. KESSLER, G.D., 
BOWMAN, D.A., AND HODGES, L.F. 2000. The Simple Virtual Environment Library: An Extensible Framework 
for Building VE Applications. Presence: Teleoperators &#38; Virtual Environments 9, 187-208. MICROSOFT 
2008. Microsoft Research Faculty Summit 2008: DemoFest. Available from: http://research.microsoft.com/en­us/um/redmond/events/fs2008/demofest.aspx. 
MICROSOFT 2009. XNA Developer Center. Available from: http://msdn.microsoft.com/en-us/xna/default.aspx. 
NATURAL POINT 2008. TrackIR: 6DOF Head Tracking. Available from: http://www.naturalpoint.com/trackir/. 
ODA, O., LISTER, L., WHITE, S., FEINER, S. 2007. Developing an augmented reality racing game. In Proceedings 
of the Proceedings of the 2nd international conference on INtelligent TEchnologies for interactive enterTAINment, 
Cancun, Mexico, 2007 ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications 
Engineering). PEEK, B. 2009. Managed Libary for Nintendo's Wiimote. Available from: http://www.codeplex.com/WiimoteLib. 
RUBINE, D. 1991. Specifying gestures by example. In International Conference on Computer Graphics and 
Interactive Techniques, 329-337. SCHMALSTIEG, D., FUHRMANN, A., HESINA, G., SZALAVARI, Z., ENCARNASALO, 
L.M., GERVAUTZ, M. AND PURGATHOFER, W. 2002. The Studierstube Augmented Reality Project. Presence: Teleoperators 
&#38; Virtual Environments 11, 33-54. STANLEY, K.O. AND MIIKKULAINEN, R. 2002. Evolving Neural Networks 
through Augmenting Topologies. Evolutionary Computation 10, 99-127. TEXAS INSTRUMENTS 2009. 3D TV - DLP 
HDTV. Available from: http://www.dlp.com/hdtv/3-d_dlp_hdtv.aspx. TROILLARD, C. 2008. Wiimote Image. Available 
from: http://www.osculator.net/wiki/uploads/Main/pry-wiimote.gif. VARCHOLIK, P., BARBER, D., NICHOLSON, 
D. 2008. Interactions and Training with Unmanned Systems and the Nintendo Wiimote. In Proceedings of 
the 2008 Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC '08), 2008. VARCHOLIK, 
P., LAVIOLA, J., NICHOLSON, D. 2009. TACTUS: A Hardware and Software Testbed for Research in Multi-Touch 
Interaction. In Proceedings of the Human-Computer Interaction International (HCI International), San 
Diego, CA, 2009. VARCHOLIK, P., MERLO, J. 2008. Gestural Communication with Accelerometer-based Input 
Devices and Tactile Displays. In Proceedings of the 26th Army Science Conference, Orlando, FL, 2008. 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>National Science Foundation</funding_agency>
			<grant_numbers>
				<grant_number>DRL0638977</grant_number>
			</grant_numbers>
		</article_sponsors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1581083</article_id>
		<sort_key>100</sort_key>
		<display_label>Pages</display_label>
		<article_publication_date>08-04-2009</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Heuristics for continuity editing of cinematic computer graphics scenes]]></title>
		<page_from>63</page_from>
		<page_to>69</page_to>
		<doi_number>10.1145/1581073.1581083</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1581083</url>
		<abstract>
			<par><![CDATA[<p>We present a set of heuristics for editing footage of 3D computer graphics cinematic sequences into a coherent movie clip which obeys the conventions of continuity editing. Our approach mimics the decision processes of an editor assembling a clip out of filmed footage involving multiple camera setups. Given a set of stylistic rules, our software applies a number of heuristics to produce a final result satisfying those rules, as well as the fundamental rules of continuity editing. The main contribution of this paper is in the formulation of editing heuristics which take into account stylistic rules, enabling different edits of the same scene into cinematic clips of various styles. We demonstrate the use of these heuristics on three scenes taken from actual film clips.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[computational cinematics]]></kw>
			<kw><![CDATA[computer animation]]></kw>
			<kw><![CDATA[film editing]]></kw>
			<kw><![CDATA[film grammar]]></kw>
			<kw><![CDATA[virtual characters]]></kw>
			<kw><![CDATA[virtual cinematography]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1570322</person_id>
				<author_profile_id><![CDATA[81365594825]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kaveh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kardan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Hawaii]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570323</person_id>
				<author_profile_id><![CDATA[81100150660]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Henri]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Casanova]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Hawaii]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Amerson, D. and Kime, S. 2000, Real-time Cinematic Camera Control for Interactive Narratives. In <i>AAAI'00</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Arijon, D. Grammar of the Film Language. Silman-James Press, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>291101</ref_obj_id>
				<ref_obj_pid>291080</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bares, W. H.; and Lester, J. C. 1999, Intelligent Multi-Shot Visualization Interfaces for Dynamic 3D Worlds. In <i>Intl. Conf. on Intelligent User Interfaces</i>, pages 119--126.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bares, W. H.; Thainimit, S.; and McDermott, S., A Model for Constraint-Based Camera Planning. In <i>Smart Graphics. Papers from the 2000 AAAI Spring Symposium</i>, pages 84--91, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Bordwell, D., Staiger, J., and Thompson, K., The Classical Hollywood Cinema. Routledge, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1892897</ref_obj_id>
				<ref_obj_pid>1892875</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Christianson, D. B.; Anderson, S. E.; He, L.-W.; Salesin, D. H.; Weld, D. S.; and Cohen, M. F. 1996, Declarative camera control for automatic cinematography. In <i>Proceedings of the Thirteenth National Conference on Artificial Intelligence</i>, 148--155.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Cozic, L.; Davis, S. B.; and Jones, H., Interaction and Expressivity in Video Games: Harnessing the Rhetoric of Film. In <i>Technologies for Interactive Digital Storytelling and Entertainment</i>, pages 232--239, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Elson, D. K., and Riedl, M. O., 2007, A Lightweight Intelligent Virtual Cinematography System for Machinima Production. In <i>Artificial Intelligence and Interactive Digital Entertainment 2007</i>, pages 8--13.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Friedman, D. A.; and Feldman, Y. A., 2004, Knowledge-based cinematography and its applications. In <i>Proceedings of ECAI</i>, pages 256--262.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Halper, N.; Helbing, R.; and Strothotte, T. 2001, A Camera Engine for Computer Games: Managing the Trade-Off Between Constraint Satisfaction and Frame Coherence, In <i>Proc. Eurographics 2001</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237259</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[He, L.; Cohen, M.; and Salesin, D., 1996, The Virtual Cinematographer: A Paradigm for Automatic Real-time Camera Control and Directing, <i>Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques</i>, pages 217--224.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1401875</ref_obj_id>
				<ref_obj_pid>1401843</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Kardan, K., and Casanova, H., 2008, Virtual Cinematography of Group Scenes Using Hierarchical Lines of Actions. In <i>Proceedings of the 2008 ACM SIGGRAPH Symposium on Video Games</i>, pages 171--178.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Katz, S., Film Directing: Shot by Shot: Visualizing from Concept to Screen, Michael Wiese Productions, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1043558</ref_obj_id>
				<ref_obj_pid>1042444</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Li, T. Y.; and Xiao, X. Y., An Interactive Camera Planning System for Automatic Cinematographer. In <i>Conference on Multimedia Modeling</i>, pages 310--315, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Mascelli, J. V., The Five C's of Cinematography: Motion Picture Filming Techniques. Silman-James Press, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Murch, W., In the Blink of an Eye. Silman-James Press, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Oliveros, D. A. M. 2004. Intelligent Cinematic Camera for 3D Games, MSc. Thesis, University of Technology, Sydney Australia.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>337513</ref_obj_id>
				<ref_obj_pid>336595</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Tomlinson, B.; Blumberg, B., and Delphine, N., 2000, Expressive Autonomous Cinematography for Interactive Virtual Environments. In <i>Proc. of the 4th International Conf. on Autonomous Agents</i>, pages 317--324.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Heuristics for Continuity Editing of Cinematic Computer Graphics Scenes Kaveh Kardan Henri Casanova 
University of Hawaii* Abstract We present a set of heuristics for editing footage of 3D computer graphics 
cinematic sequences into a coherent movie clip which obeys the conventions of continuity editing. Our 
approach mimics the decision processes of an editor assembling a clip out of .lmed footage involving 
multiple camera setups. Given a set of stylistic rules, our software applies a number of heuristics to 
produce a .nal result satisfying those rules, as well as the fundamental rules of continuity editing. 
The main contribution of this paper is in the formulation of editing heuristics which take into account 
stylistic rules, enabling different edits of the same scene into cinematic clips of various styles. We 
demonstrate the use of these heuristics on three scenes taken from actual .lm clips. CR Categories: I.3.7 
[Computer Graphics]: Three-Dimensional Graphics and Realism; J.5 [Arts and Humanities] Keywords: computer 
animation, virtual cinematography, .lm editing, .lm grammar, virtual characters, computationalcinematics 
 Introduction In computer graphics, the need to generate cinematic sequences is present in numerous 
applications. From narrative productions such as feature .lms and in-game cinematics to computer-assisted 
storytelling, pre-visualization for .lms, and 3D chat systems, there is a prevalent demand for means 
of automating the production of computer-generated cinematics. Even in simple 3D scenes, the process 
of creating and placing cameras to follow the action of a scene and then editing the resulting footage 
is a time­consuming one. And in fact, there are cases, such as cinematic playback of computer games actions, 
where it is not possible to perform these actions manually, as the "script" of the scene is not known 
ahead of time, and will vary from player to player and game to game. *e-mail: {kaveh | henric}@hawaii.edu 
Copyright &#38;#169; 2009 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for components of this work owned by others than ACM 
must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from 
Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org. Sandbox 2009, New Orleans, 
Louisiana, August 4 6, 2009. &#38;#169; 2009 ACM 978-1-60558-514-7/09/0008 $10.00 While the work presented 
herein is relevant for all the aforementioned application domains, in this paper we choose to focus on 
applications that involve the generation of cinematic sequences to enable new gameplay and storytelling 
avenues. Such sequences could be generated from high level authorial or user input, leading to the creation 
of customized movie experiences. The fundamental element required for such new entertainment media is 
the ability to generate cinematics that follows the established conventions of cinema, includingcinematography 
and editing, aiming for an immersive and engaging user experience. In this paper, we tap into these established 
conventions in the .eld of .lm editing and derive heuristics to help automate this process while leading 
to correct editing. Once these heuristics have been implemented as part of an automated cinematic sequence 
generation system, this system can then be used to investigate which rules and parameters lead to particular 
editing styles. For instance, the stylistic inputs to the system can be modi.ed in an attempt to produce 
results that match historical and authorial styles. The main contribution of this paper is the explicit 
enumeration of editing heuristics and stylistic rules resulting in various possible edits of a given 
scene, each of which is visually acceptable yet stylistically different. We have implemented these heuristics 
and have investigated their ability to produce acceptable cinematic sequences based on a wide range of 
test cases. In this paper we show obtained results for three dialogue scenes that correspond to actual 
.lm clips. The rest of this paper is organized as follows. In Section 2 we review related work. In Section 
3 we detail the rationale and the design for our approach. In Section 4 we describe and justify a series 
of editing heuristics. In Section 5 we describe four stylistic rules that are used as parameters to the 
editing heuristics. Section Section 6 brie.y describes our implementation. Section 7 presents evaluation 
results. Finally, Section 8 concludes the paper with a summary of our results and a discussion of future 
work directions. 2 Related Work Texts on cinematography and editing such as [Murch] [Mascelli] [Arijon] 
[Katz] provide information in terms of generalized rules, or as formulae (idioms) which apply in speci.c 
cases. Their work is rendered dif.cult by the fact that the art of editing is not merely one of applying 
a set of standardized rules to the editing process of a scene or .lm. Like any art form, the art comes 
from understanding and then consciously breaking rules for effect where desired. [Christianson] and 
[He] implement Arijon s idioms regarding camera placement into a software framework for doing virtual 
cinematography of 3D scenes. Editing in these idioms is hard coded in terms of actor motion into, out 
of, and within the frame. The placement of virtual cameras within 3D scenes has been addressed in [Bares1999] 
[Bares2000] [Li] [Oliveros].[Friedman] and [Elson] apply knowledge bases in reproducingthe .lmmaking 
process, but are more concerned with camera placement and do not address the issue of overlapping actions, 
which require editing decisions. [Tomlinson] implements the camera as an agent within the scene, and 
handles editing by mathematically weighting which actor should be shown, based on the actor s importance 
and recent screen presence. We also limit ourselves to situations where a scene of events already exists 
in its entirety. Much work has been done in cinematography and editing of real-time systems [Amerson][Halper] 
[Cozic], such as games in the process of being played. We do not address this situation, but rather take 
advantage of the extra knowledge provided by the predetermined events in order to produce cinematically 
correct edited clips.  Design In this paper, we limit the scope of the stagings of our 3D computer 
graphics scenes to static scenes of dialog between arbitrary numbers of actors. Dialog scenes are the 
most common scenes in narrative cinema, the resulting edits are relatively easy to judge aesthetically, 
and there is a large body of work on how to edit such scenes. Furthermore, since such scenes contain 
a fixed audio track of the dialog, it prevents the editing system from retiming events or shuffling them 
back and forth in time. Though a real editor might well make use of such manipulations, our system strictly 
adheres to the fixed audio track. 3.1 Overall Approach In traditional .lmmaking, the creation of a .lm 
proceeds along a pipeline involving preproduction, production, and post­production. For our purposes, 
the roles of interest are those of the director, the cinematographer, and the editor. Fig. 1 shows how 
we represent each of these roles as modules in our system. The director module places the actors around 
the stage (also known as blocking) and produces a shot list. This shot list indicates what the director 
considers worth .lming in the events that unfold in the scene. The director's shot list may contain gaps 
in time between shots as well as temporal overlaps between shots. The cinematography system then generates 
all possible camera setups based on actor grouping and lines of action. The design of the cinematography 
system is fully explained in a previous paper [Kardan]. The editor generates an edited shot list which 
is then rendered based on the appropriate camera setups. In this paper, we describe the editing step 
of the process, which involves taking the director's initial shot list, a number of stylistic rules, 
the available camera setups, and producing a .nal edited shot list. This list indicates which camera 
setup is to be used at each point in time. Continuity editing is a style of .lm editing developed in 
Hollywood early in the 20th century. By 1920, the principles of continuity editing had been established 
[Bordwell] and were prevalent in the productions of the .lm studios. By the 1930's, the rules of continuity 
editing had been re.ned to the extent that they have remained essentially unchanged to this date. The 
goal of continuity editing is to make the edits in a cinematic sequence as invisible as possible. To 
accomplish this, heuristics have been developed by .lm editors to determine where to cut from one shot 
to another in order to minimize the audience's awareness of the transition, and to not break their immersion 
in the story. Continuity editing and its heuristics provide the basis for our work.  3.2 Scene Representation 
The scene is the top level data structure in our system. A scene takes place in a single location, with 
a number of actors. Scenes are made up of events, such as one actor talking to or looking at one or more 
actors. In this work we only consider static scenes, in which neither the camera(s) nor the actors are 
moving. Events are the atomic units of acting in our system. An event is represented as a continuous 
interval of time, with a start and end time. A scene then consists of a list of possibly overlapping 
events. In addition to temporal information, events may be assignedimportance/intensity values, which 
can be used by the editing system to decide which of several overlapping events' shot should be used, 
and which framing to use for a given shot. The editing process, in our system, consists in converting 
the list of events in a scene into an edited shot list.  Editing Heuristics Our approach is to mimic 
the decision process of a real editor while working on a scene. We treat the input to the editor module 
as a set of shots (the shot list) provided by the director. We make these heuristics explicit, rather 
than relying on more abstract mathematical weightings or constraint systems. This allows us to tweak 
the rules which feed into the heuristics and speak about the decisions made by the system in producing 
the .nal edit. In this way, our system is somewhat similar in goal to an expert system, in that we want 
to be able to look and what it did and understand why it made the choices it did.  Figure 2. Shot, camera, 
and event data structures. Each shot contains data as shown in Fig. 2. The editor's task is to reduce 
these shots to a single, temporally continuous, sequence of shots along with an associated camera setup 
for each shot. We describe here eight editing heuristics that are used in movie editing and that we have 
implemented in our system. The heuristics are applied in the order presented below. 4.1 Selecting Shot 
Camera Setups A number of camera setups with different framings, possibly ranging from long shots to 
extreme closeups, are created for each scene event, from which the editor selects a setup for each shot 
in the timeline. The selection is made based on speci.ed stylistic rules having to do with preferred 
camera framings, which can be modi.ed by the intensity of the event. One stylistic rule, for example, 
is whether each scene should start with an establishing shot which shows all the actors in the scene. 
Other rules can specify that one framing should not follow another in subsequent shots of an actor. For 
example, in the editing of the classical Hollywood period (1930-1950), a closeup of an actor could not 
follow a long shot of the actor without .rst going through an intervening medium shot. 4.2 Flattening 
Shot List When two or more shots overlap in time, the editor needs to decide which shot to use in the 
.nal edit. This is done using the importance associated with the events of the shots. The shot with the 
highest importance is the one selected. This can result in the other shots being entirely removed from 
the timeline, or clipped (trimmed or split) by the more important shot. 4.3 Filling Shot List Gaps When 
there is a gap between two adjacent shots, the editor needs to determine how to .ll this gap. A value 
in the range [0, 1] is used to determine the cut point between the two shots in the gap. A value of 0.5 
would equally extend both shots to .ll the gap. In dialog scenes, the choice of this value has a strong 
impact on the feeling of the .nal edit. It is unusual to stay on the shot of a person who has .nished 
speaking, unless there are non-verbal motivations for doing so. The natural tendency is to cut to the 
listener, who, perhaps after a pause, will reply. In most cases, this value tends to be small, allowing 
the shot of the listener to .ll most if not all of the gap. 4.4 Removing Quick Shots Shot length is 
one of the commonly used metrics when determining trends in .lm style and editing, since it is easily 
measured and analyzed. We provide the notion of a minimum shot length to handle cases where a shot's 
duration, due to the duration of the associated event, or due to timing changes made by other editing 
heuristics, is too small to warrant a cut in the .nal shot list. Such a shot would be jarring, .ashing 
onto and off the screen at a faster pace than .ts the desired editing style. As faster and more frenetic 
editing has reduced shot lengths in .lms over the years, audiences have become used to making sense of 
more disjointed imagery, but there is still a need for preventing cuts which are inappropriately quick 
for a given style or pacing. When a shot duration is found to be below the speci.ed threshold, one of 
two actions can be taken. Either the shot is removed and the adjoining shots are extended to .ll in the 
resulting gap, or the shot is extended. The current implementation always removes the shot, but one can 
imagine the decision as to which is the more appropriate action could be taken based on the the relative 
importance of the shots. 4.5 Creating L-cuts The choice of a cut point between speaking actors is not 
limited to the pause between speaking events. Often, an editor will cut away from the speaker to the 
listener before the former is done speaking. The second shot in this instance will show the second actor 
listening, then replying. This sort of cut, sometimes referred to as an L-cut due to the positioning 
of the relevant video and audio tracks in a timeline, is commonly used, resulting in a smoother --less 
noticeable --transition between shots as not both the audio and visuals are transitioning simultaneously. 
 4.6 Inserting Reaction Shots Conversely, when a shot is considered too long for the desired edit pacing, 
a reaction shot may be inserted to split the shot into several smaller clips. We do this if the duration 
of a shot exceeds a given maximum shot length. In this case, a shot of the listener is cut into the shot 
of the speaker, even if the listener is not associated with any events at that time. In practice, an 
editor will attempt to use a clip of the listener in which something is happening, be it a gesture, a 
smile, or other facial expression, even if the clip is from another point in time, another take, or even 
another scene. This conceals the true purpose of the cut by providing a visual excuse for the edit. 
 4.7 Reframing Quick Shots A useful heuristic, when the editor encounters a sequence of short shots which 
would otherwise be removed from the timeline, is to reframe the shots into a single longer shot by using 
an alternate camera framing. For example, if two actors are engaged in a rapid exchange of dialog, it 
is possible that each of the shots of an actor speaking has a smaller duration than desired. In such 
a case, it would be undesirable to simply remove some or even all of these rapid dialog shots. Instead, 
the editor combines these shots into a single shot, and searches the camera setups for a setup which 
frames both actors. In this way, a series of very quick one­shots of the actors is replaced with a longer 
two-shot of both actors. 4.8 Merging Continuous Shots There may be cases, possibly due to the application 
of other heuristics, where two similar shots of an actor are adjacent on the timeline. This re.ects shots 
corresponding to two camera setups which are close to each other. In such a case, there will be a disconcerting 
jump between the two shots. The merging heuristic detects such cases and replaces one shot with the other, 
resulting in a single longer shot without any visual discontinuity. Stylistic Rules The stylistic rules 
are parameters that are used by the heuristics described above in making their choices. In the same spirit 
as for the heuristics, we make these parameters explicit in terms that an editor could understand. This 
allows us to make speci.c changes and see the results, as well as try to derive the required input to 
achieve a certain style of editing. Doing so involves grouping the rules into "editing styles" which 
can then be packaged for use by higher level modules of a future system. The existing system features 
a dozen or so stylistic editing rules which fall into the following categories. 5.1 Framing The cinematography 
module will create multiple camera setups for framings of each shot in the director's shot list. The 
framing rules determine what range of framings should be generated, and what the preferred framing is 
for shots. Some styles will prefer closeups, while other will use a medium shot as the default framing. 
In addition, rules exist to specify whether each scene should begin with an establishing shot, and whether 
there are any constraints on framings of the same actor in consecutive shots. A further rule determines 
whether framings should indicate the relative distances between actors. 5.2 Shot Durations Shot durations 
are fundamental to the pacing of a scene. Rules determine the minimum and maximum shot durations an edit 
should produce. If shots are shorter than the minimum duration, they will be removed by the appropriate 
heuristic,and if they are longer than the maximum duration, they will be split by insertions of reaction 
shots. An additional rule determines how to .ll gaps between events, which typically denote pauses in 
a conversation. 5.3 Cutting Style If a shot meets the preconditions of being cut using an L-cut, this 
rule determines when and how such a cut is made. By separating the visual and audio transitions between 
two shots, a smoother .ow is achieved, and is often experienced as being less noticeable by the audience. 
 5.4 Reframing This rule speci.es whether to merge a series of quick shots of individual actors into 
a longer one by using an alternate camera setup which shows all the actors involved in a group shot. 
This rule has a signi.cant effect on the .nal edit, as it can greatly alter the duration of shots.  
  6 Implementation Our implementation consists of a software framework based on custom software communicating 
with Maya, an existingcommercial 3D animation application developed by Autodesk. The bulk of our software 
is written in Common Lisp on OS X, with a socket interface to Maya. Common Lisp calls are translated 
to MEL (Maya Extension Language) command strings which are then sent to Maya via a socket. This allows 
our system to execute any Maya action, such as creating and placing cameras, animating actors, and rendering 
frames of animation. Values from Maya are return to our software via sockets as well. The remainder of 
our software consists of C++ plugins written for Maya to optimize operations which are unacceptably slow 
when running in MEL. This software architecture avoids the need to develop a custom 3D graphics engine, 
and gives us access to the large set of functionality present in an existing commercial 3D animation 
system. The two main graphical entities of our 3D scenes are cameras and actors. Cameras have position 
and orientation, as well a focal length and aspect ratio. Actors are modeled as hierarchical skeletal 
animation rigs, with rigid body parts, and can perform simple procedural animation. Currently, these 
procedures consist of a mouth animation for speaking, and a look-at animation for orienting the head. 
Actor animations are generated procedurally based on the events of a scene. These animations are helpful 
to illustrate the events of the scene, and make the viewer understand why a framing or editing decision 
was made by the system. In essence they help comprehension of the scene by doing very rudimentary acting, 
in term allowing us to evaluate the correctness and appropriateness of the generated cinematography. 
 7 Results &#38; Evaluation The scenes, and corresponding event lists, used for evaluating our systems 
are created by manually encoding existing .lm clips. Timing and staging information is extracted from 
the several sample clips and encoded to serve as test cases for the system. This provides us with a dialog 
track for the scene, and also allows us to compare the generated results with established acceptable 
results. Note that events could also be generated using a higher­level scripting system. A story generator 
or machinima system, for example, could create scenes and events procedurally. Game engines could also 
generate in-game cinematics on the .y based on scripts and user actions. Finally instant messaging and 
chat software could also be used to generate events, based on the text and emotes input by multiple participants. 
In what follows, we discuss results obtained with scenes manually encoded from video clips. The generated 
movies for these examples are available at www2.hawaii.edu/~kaveh/Research/Papers/Siggraph2009/. 7.1 
Simple Two-actor Dialog This scene from the television show "Buffy The Vampire Slayer" is a simple conversation 
between two actors. The conversation is straightforwardly back and forth, each actor taking turn in speaking. 
As we can see from the director's shot list for the two actors in Fig. 3, there is no overlapping dialog, 
and minimal gaps in the dialog. The steps taken by the editor mainly consist of .lling in gaps and inserting 
reaction shots (diamonds) based on the stylistic rules. In Edit 1 long shots are tolerated only one reaction 
shot is inserted. This result is somewhat close to the actual edit of the .lm clip, though that was not 
our goal in choosing stylistic rules. In the Edit 2 a more frenetic editing pace is speci.ed, and as 
a result the longer shots have been broken up with more reaction shots. L-cuts have also been added in 
Edit 2 to reduce the reliance of the speaking events on the back-and­forth editing.  7.2 Two-actor Dialog 
With Pauses The following example is from a tense conversation in the feature .lm "Michael Clayton". 
Unlike the previous scene, the conversation is peppered with tense and meaningful pauses, as can be seen 
in the director's shot list for the actors in Fig. 4. We illustrate two resulting edits by our system, 
based on the conversation event timings with different stylistic rules. Though our edits are "correct" 
(i.e. not demonstrably incorrect), our system falls short of the actual clip in conveying the sense of 
tension and subtle actors interactions. This is due to our events not being tagged with the myriad of 
non-verbal actions being taken by the actors, and by the very rudimentary nature of the acting of our 
CG actors. This indicates how primitive, although correct, the decisions of our system are compared to 
those of a real editor.  7.3 Clustered Actors The third example is from Charlie Wilson s War , and 
involves two groups of actors in conversation. By using the framing style (5.1) where framing indicates 
relative distance between actors, the edit accomplishes the task of showing that the two groups are far 
apart, as can be seen in Fig. 5a. Also in this scene one shot is marked as having a higher intensity 
than the rest. As a result, the editor chooses a closer framing for that shot, resulting in a (desired) 
visual jump closer to the actor as his dialog becomes more intense. This is shown in Fig. 5b. 7.4 Complex 
Overlapping Dialog Scene The .nal example we show is from the .lm "The Big Lebowski" and features a long 
dialog scene with three actors constantly speaking over each others' lines. As can be seen from the director's 
shot list in Fig. 6, the speaking events involve many temporal overlaps which need to be resolved by 
the editing system. Edit 1 produces a clip with many short shots and visual jumps, leading to a frenetic 
scene pacing. Edit 2 shows a more restrained pace with fewer rapid cuts while maintaining the goal of 
showing what is in the director's shot list. This is accomplished mainly through the application of the 
reframing heuristic described in Section 4.7. When there is a burst of rapid dialog between two actors, 
this heuristic replaces several quick cuts of each actor with a longer shot of the two actors, resulting 
in a smoother edit.  Conclusions and Future Work We have proposed a set of heuristics for editing of 
.lm clips that attempts to mimic the thought process of an actual editor, using editing heuristics informed 
by a set of stylistic rules. Our system can generate a variety of visually acceptable edited sequences 
given different stylistic rules for static scenes involving dialogs between actors. Though our system 
is far from being able to produce edited clips that would rival in quality with those of a real editor 
editing a subtle scene, it can serve a number of useful functions. It can serve as an editor's apprentice, 
generating a quick .rst cut of a given scene, for the editor to examine and tweak as deemed appropriate. 
For highly constrained styles, such as sitcoms and talk shows, the output from our system may be adequate 
for use as is. Our system can also be used in places where scenes are generated on the .y (game scene 
playbacks, 3D chat) or procedurally (game cinematics, storytelling software). Our system currently deals 
with static scenes. Adding editing decisions based on actor movements within the frame is a natural next 
step. Many of the standard cinematic idioms rely on cutting on an action, such as an actor entering or 
exiting the frame. Good editors base editing choices on the smallest of visual cues: glances, reactions, 
even blinks. Their decisions are informed by the context of the scene in question and how it .ts into 
the larger work. Our system needs better tagging of events, to denote both more subtle actions such as 
reactions, and blinks, while at the same time we need a wider variety of events, such as meaningful looks, 
gestures, smiles, and facial expressions. Editors must also ensure that the edit they produce conveys 
the appropriate emotional content and invokes the desired responses in the audience. A set of heuristics 
which associate editing and camera choice to emotional response would be useful in attempting to better 
manage the viewer's emotional journey through the .lm. More contextual information, such as the role 
and place of the scene being edited within the scope of the larger narrative can provide information 
to help determine the editing style and pacing. Editors have the freedom to shift and rearrange shots 
in time, effectively building events and sequences which did not take place during .lming. It would be 
very interesting if an automated editing system could manipulate shots in such a powerful way to enhance 
the story experience of the viewer. The stylistic rules could be packaged into full-.edged styles, such 
as those of Classical Hollywood, contemporary action .lms, music videos, situation comedies, and so forth. 
The system can also be used to attempt to reverse engineer the editing of such styles, effectively deriving 
the implicit heuristics which underly each .lmic style. Acknowledgements We would like to thank Avatar 
Reality and the Blue Mars project for their support of portions of this research.  References AMERSON, 
D. and KIME, S. 2000, Real-time Cinematic Camera Control for Interactive Narratives. In AAAI'00. ARIJON, 
D. Grammar of the Film Language. Silman-James Press, 1991. BARES, W.H.; and LESTER, J.C. 1999, Intelligent 
Multi-Shot Visualization Interfaces for Dynamic 3D Worlds. In Intl. Conf. on Intelligent User Interfaces, 
pages 119-126. BARES, W.H.; THAINIMIT, S.; and McDERMOTT, S., A Model for Constraint-Based Camera Planning. 
In Smart Graphics. Papers from the 2000 AAAI Spring Symposium, pages 84-91, 2000. BORDWELL, D., STAIGER, 
J., and THOMPSON, K., The Classical Hollywood Cinema. Routledge, 1998. CHRISTIANSON, D. B.; ANDERSON, 
S. E.; He, L.-W.; Salesin, D. H.; Weld, D. S.; and Cohen, M. F. 1996, Declarative camera control for 
automatic cinematography. In Proceedings of the Thirteenth National Conference on Arti.cial Intelligence, 
148-155. COZIC, L.; DAVIS, S.B.; and JONES, H., Interaction and Expressivity in Video Games: Harnessing 
the Rhetoric of Film. In Technologies for Interactive Digital Storytelling and Entertainment, pages 232-239, 
2004. ELSON, D. K., and RIEDL, M. O., 2007, A Lightweight Intelligent Virtual Cinematography System for 
Machinima Production. In Arti.cial Intelligence and Interactive Digital Entertainment 2007, pages 8-13. 
FRIEDMAN, D.A.; and FELDMAN, Y.A., 2004, Knowledge­based cinematography and its applications. In Proceedings 
of ECAI, pages 256-262. HALPER, N.; HELBING, R.; and STROTHOTTE, T. 2001, A Camera Engine for Computer 
Games: Managing the Trade-Off Between Constraint Satisfaction and Frame Coherence, In Proc. Eurographics 
2001. HE, L.; COHEN, M.; and SALESIN, D., 1996, The Virtual Cinematographer: A Paradigm for Automatic 
Real-time Camera Control and Directing, Proceedings of the 23rd Annual Conference on Computer Graphics 
and Interactive Techniques, pages 217-224. KARDAN, K., and CASANOVA, H., 2008, Virtual Cinematography 
of Group Scenes Using Hierarchical Lines of Actions. In Proceedings of the 2008 ACM SIGGRAPH Symposium 
on Video Games, pages 171-178. KATZ, S., Film Directing: Shot by Shot: Visualizing from Concept to Screen, 
Michael Wiese Productions, 1991. LI, T.Y.; and XIAO, X.Y., An Interactive Camera Planning System for 
Automatic Cinematographer. In Conference on Multimedia Modeling, pages 310-315, 2005. MASCELLI, J. V., 
The Five C's of Cinematography: Motion Picture Filming Techniques. Silman-James Press, 1998. MURCH, W., 
In the Blink of an Eye. Silman-James Press, 2001. OLIVEROS, D.A.M. 2004. Intelligent Cinematic Camera 
for 3D Games, MSc. Thesis, University of Technology, SydneyAustralia. TOMLINSON, B.; BLUMBERG, B., and 
DELPHINE, N., 2000, Expressive Autonomous Cinematography for Interactive Virtual Environments. In Proc. 
of the 4th International Conf. on Autonomous Agents, pages 317-324.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1581084</section_id>
		<sort_key>110</sort_key>
		<section_seq_no>3</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Game mechanics and design principles]]></section_title>
		<section_page_from>71</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1570324</person_id>
				<author_profile_id><![CDATA[81320489774]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tracy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fullerton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1581085</article_id>
		<sort_key>120</sort_key>
		<display_label>Pages</display_label>
		<article_publication_date>08-04-2009</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Game design principles for engaging cooperative play]]></title>
		<subtitle><![CDATA[core mechanics and interfaces for non-mimetic simulation of fire emergency response]]></subtitle>
		<page_from>71</page_from>
		<page_to>78</page_to>
		<doi_number>10.1145/1581073.1581085</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1581085</url>
		<abstract>
			<par><![CDATA[<p>Core mechanics are the activities that players repeat to play a game, the central aspects of play constrained by rules. Interfaces mediate play experiences, impacting engagement with core mechanics. We design core mechanics for gathering, integrating, and sharing information, based on team coordination practices of fire emergency responders. We connect these mechanics with interfaces that impact player engagement. Mechanics and interfaces combine into a <i>non-mimetic</i> simulation game, which eschews fire and smoke, in favor of re-creating information flows and team structures.</p> <p>We describe the iteration of mechanics and interface components as shaped by practice, pilot games, participatory redesign sessions, and long-term user studies. The result is integrated core mechanics that we develop from work practice and interface components that support engagement with them. From this data, we construct game design principles for engaging cooperative play: information distribution, modulating visibility, providing the right information in the right time, making predictable, and understandable representations for shared mental models.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[communication]]></kw>
			<kw><![CDATA[cooperation]]></kw>
			<kw><![CDATA[coordination]]></kw>
			<kw><![CDATA[core mechanics]]></kw>
			<kw><![CDATA[emergency response]]></kw>
			<kw><![CDATA[interface design]]></kw>
			<kw><![CDATA[work practice]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1570325</person_id>
				<author_profile_id><![CDATA[81309499161]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zachary]]></first_name>
				<middle_name><![CDATA[O.]]></middle_name>
				<last_name><![CDATA[Toups]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570326</person_id>
				<author_profile_id><![CDATA[81100203284]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andruid]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kerne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570327</person_id>
				<author_profile_id><![CDATA[81430595127]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hamilton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>857656</ref_obj_id>
				<ref_obj_pid>857189</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bederson, B. B. and Boltman, A. 1999. Does animation help users build mental maps of spatial information? In <i>Proc. IEEE INFOVIS</i>, 28--36.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1455736</ref_obj_id>
				<ref_obj_pid>1455735</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Blattner, M., Sumikawa, D., and Greenberg, R. 1989. Earcons and icons: Their structure and common design principles. In <i>Human Computer Interaction 4</i>, 1, 11--44.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Cannon-Bowers, J. A., Salas, E., and Converse, S. 1993. Shared mental models in expert team decision making. In Castellan, N. J., Ed. <i>Individual and Group Decision Making: Current Issues</i>. Lawrence Earlbaum Associates, 221--246.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1394465</ref_obj_id>
				<ref_obj_pid>1394445</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Denef, S., Ramirez, L., Dyrks, T., and Stevens, G. 2008. Handy navigation in ever-changing spaces: An ethnographic study of firefighting practices. In <i>Proc. ACM Designing Interactive Systems</i>, 184--192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Gentner, D. and Stevens, A. L., Eds. 1983. <i>Mental Models</i>. Lawrence Earlbaum Associates.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Granlund R., Johansson, B., Persson, M. 2001. C3Fire: A micro-world for collaboration training in the ROLF environment. In <i>Proc. Simulation and Modeling, Simulation Theory and Practice</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1520702</ref_obj_id>
				<ref_obj_pid>1520340</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Hamilton, W. A., Toups, Z. O., and Kerne, A. 2009. Synchronized communication and coordinated views: qualitative data discovery for team game user studies. In <i>Proc. Ext. Abs. ACM SIGGCHI</i>, 4573--4578.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Hutchins, E. 1995. <i>Cognition in the Wild</i>. MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Itten, J. 1997. <i>The Art of Color: The Subjective Experience and Objective Rationale of Color</i>, revised ed. Wiley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1086144</ref_obj_id>
				<ref_obj_pid>1086057</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Kerne, A. 2005. Doing interface ecology: The practice of metadisciplinarity. In <i>ACM SIGGRAPH Art and Animation Catalog</i>, 181--185.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1124804</ref_obj_id>
				<ref_obj_pid>1124772</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Landgren, J. 2006. Making action visible in time-critical work. In <i>Proc. ACM SIGCHI</i>, 201--210.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Macmillan, J., Entin, E. E., and Serfaty, D. 2004. Communication overhead: The hidden cost of team cognition. In {Salas and Fiore 2004}, 61--82.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Malone, T. W. 1981. Toward a theory of intrinsically motivating instruction. In <i>Cognitive Science 5</i>, 4, 333--369.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2187809</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Norman, D. A. 2002. <i>The Design of Everyday Things</i>. Basic Books.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>293208</ref_obj_id>
				<ref_obj_pid>293172</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Page, E. H. and Smith, R. 1998. Introduction to military training simulation: A guide for discrete event simulationists. In <i>Proc. ACM Winter Simulation Conference</i>, 53--60.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>357320</ref_obj_id>
				<ref_obj_pid>357318</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Reeves, W. T. 1983. Particle systems - A technique for modeling a class of fuzzy objects. In <i>ACM Trans. Graphics 2</i>, 2, 91--108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37406</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Reynolds, C. W. 1987. Flocks, herds and schools: A distributed behavioral model. In <i>Computer Graphics (Proc. ACM SIGGRAPH 87)</i>, 25--34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Salas, E. and Fiore, S. M., Eds. 2004. <i>Team Cognition: Understanding the Factors that Drive Process and Performance</i>. American Psychological Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1215723</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Salen, K. and Zimmerman, E. 2004. <i>Rules of Play: Game Design Fundamentals</i>. MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Sch&amp;#246;n, D. A. 1983. <i>The Reflective Practitioner: How Professionals Think in Action</i>. Basic Books.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237774</ref_obj_id>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Simon, H. A. 1996. <i>The Sciences of the Artificial</i>, 3&#60;sup&#62;rd&#60;/sup&#62; ed. MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97923</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Sims, K. 1990. Particle animation and rendering using parallel computation. In <i>Computer Graphics (Proc. ACM SIGGRAPH 90</i>), 405--413.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Song, A. A. and Kleinman, D. L. 1994. A distributed simulation system for team decisionmaking. In <i>Proc. AI, Simulation, and Planning in High Autonomy Systems, Distributed Interactive Environments</i>, 129--135.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Suchman, L. 1987. <i>Plans and Situated Actions</i>. Cambridge University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1240734</ref_obj_id>
				<ref_obj_pid>1240624</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Toups, Z. O. and Kerne, A. 2007. Implicit coordination in firefighting practice: Design implications for teaching fire emergency responders. In <i>Proc. ACM SIGCHI</i>, 707--716.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531725</ref_obj_id>
				<ref_obj_pid>1531674</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Toups, Z. O., Kerne, A., Hamilton, W., and Blevins, A. 2009. Emergent team coordination: From fire emergency response practice to a non-mimetic simulation game. In <i>Proc. ACM Group</i>, 341--350.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>78223</ref_obj_id>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Tufte, E. R. 1990. <i>Envisioning Information</i>. Graphics Press LLC.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[United States Department of Homeland Security. 2004. <i>National Incident Management System</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>983611</ref_obj_id>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Ware, C. 2004. <i>Information Visualization: Perception for Design</i>, 2&#60;sup&#62;nd&#60;/sup&#62; ed. Morgan Kaufman.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>985778</ref_obj_id>
				<ref_obj_pid>985692</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Xiaodong, J., Hong, J. I., Takayama, L. A., and Landay, J. A. 2004. Ubiquitous computing for firefighters: Field studies and prototypes of large displays for incident command. In <i>Proc. ACM SIGCHI</i>, 678--686.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Game Design Principles for Engaging Cooperative Play: Core Mechanics and Interfaces for Non-Mimetic 
Simulation of Fire Emergency Response Zachary O. Toups, Andruid Kerne, William Hamilton Interface Ecology 
Lab | Department of Computer Science and Engineering | Texas A&#38;M University {zach, andruid, bill}@ecologylab.net 
 Abstract Core mechanics are the activities that players repeat to play a game, the central aspects of 
play constrained by rules. Interfaces mediate play experiences, impacting engagement with core mechanics. 
We design core mechanics for gathering, integrating, and sharing information, based on team coordination 
practices of fire emergency responders. We connect these mechanics with interfaces that impact player 
engagement. Mechanics and interfaces combine into a non-mimetic simulation game, which eschews fire and 
smoke, in favor of re-creating information flows and team structures. We describe the iteration of mechanics 
and interface components as shaped by practice, pilot games, participatory re­design sessions, and long-term 
user studies. The result is integrated core mechanics that we develop from work practice and interface 
components that support engagement with them. From this data, we construct game design principles for 
engaging cooperative play: information distribution, modulating visibility, providing the right information 
in the right time, making predictable, and understandable representations for shared mental models. Keywords: 
Core mechanics, interface design, cooperation, communication, coordination, work practice, emergency 
response. CCS: H.5.2 User Interfaces.  Introduction Core mechanics are essential aspects of play, constrained 
by rules, that game players repeat [Salen and Zimmerman 2004]. In digital games, user interfaces mediate 
this experience, shaping the way players perceive and interact with game structures. The affordances 
[Norman 2002] of the interface signal action opportunities to players, indicating the way to play. Information 
provided by the interface impacts players decisions. We design core mechanics from prior design implications 
for teaching team coordination [Toups and Kerne 2007] and non­mimetic simulation principles [Toups et 
al. 2009]. These elucidate the need, in fire emergency response practice, to gather, integrate, and share 
information among team members. Because the implications and principles do not specify a need to model 
fire and smoke, we develop a non-mimetic simulation game to teach team coordination. Players take on 
roles that reflect those of fire emergency response and gather, integrate, and share information from 
a virtual environment, like firefighters. Non-mimetic simulation is a novel form, eschewing the concrete 
aspects of the Copyright &#38;#169; 2009 by the Association for Computing Machinery, Inc. Permission 
to make digital or hard copies of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions 
from Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org. Sandbox 2009, New 
Orleans, Louisiana, August 4 6, 2009. &#38;#169; 2009 ACM 978-1-60558-514-7/09/0008 $10.00 simulated 
environment, such as fire and smoke, and focusing scarce simulation resources on human-centered processes 
from a grounding in practice [Toups et al. 2009]. We develop games because they are engaging and provide 
intrinsic motivation to learn [Malone 1981]. The core mechanics of team games differ from those of single-player 
games. Human-human interaction becomes essential, adding communication and coordination to the array 
of options already available. In our game design to teach team coordination skills, these mechanics are 
essential. Design implications for teaching team coordination, uncovered from ethnographic field work 
[Toups and Kerne 2007], shape interface components that contribute to engagement in the core mechanics, 
grounding the design in practice. Non-mimetic simulation principles guide game mechanic design [Toups 
et al. 2009]. Essential to practicing team coordination skills in the non-mimetic simulation game is 
the information distribution among roles and players, which requires players to engage in distributed 
cognition [Hutchins 1995] by perceiving, integrating, transforming, and sharing information in order 
to make sense of the game environment. We analyze interface components that contribute to engagement 
in the core mechanics of a team game, leading players to cooperate. From this analysis, we develop cooperative 
game design principles for core mechanic and interface design: distributing information, modulating visibility, 
information timing, making predictable, and representations for shared mental models. We describe relevant 
background: simulation, game design, interface design, team coordination, and grounding in practice. 
We discuss the current iteration of our non-mimetic simulation game for teaching team coordination. We 
describe core mechanics derived from work practice that encourage players to cooperate through processes 
of gathering, integrating, and sharing information. For each mechanic, we describe the interface elements 
that support engagement by examining the iteration of the design with pilot games, participatory re-design 
sessions, and sustained user studies. We develop and discuss game design principles for core mechanics 
and interfaces in cooperative games. 2 Background and Prior Work We connect background from diverse 
sources. We construct simulations, considering prior work. Game design and interface design are essential. 
The team coordination literature includes distributed cognition and team cognition. Finally, we explore 
the ethnographic grounding for the game design, looking at fire emergency response work practice. 2.1 
Simulation Simulations are operational environments that enable participants to practice skills in a 
safe setting [Page and Smith 1998]. Traditionally, simulations capture, in some level of fidelity, the 
real world. Non-mimetic simulation differs from traditional simulations in that it eschews concrete aspects 
of the real-world environment. Rather than expend resources attempting to re­create reality, non-mimetic 
simulation focuses on human-centered aspects of practice, such as information flows, as discovered through 
examinations of work practice [Toups et al. 2009]. Several notable simulations for team coordination 
and emergency response exist. In the Distributed Dynamic Decision simulation, participants manipulate 
virtual entities and discuss decisions about resource allocation [Song and Kleinman 1994]. In C3Fire, 
participants are presented with a map of terrain and direct virtual units to respond to emergencies [Granlund 
et al. 2001].  2.2 Game Design Salen and Zimmerman frame games in terms of rules and play [2004]. Rules 
are the mathematical and logical structures that define the boundaries of the game, a set of restrictions 
on free action. Rules constrain play, the freedom to make choices and act within the rules. Game mechanics 
are experiences of rules and play together. The core mechanics of a game are the sets of actions that 
are repeated to play [Salen and Zimmerman 2004]. In a game of Tic-Tac-Toe, for example, the core mechanic 
is claiming territory on the board by marking an X or O . According to Salen and Zimmerman, player actions 
should have clear, discernable outcomes to facilitate understanding the game system [2004]. 2.3 Interface 
Design Interfaces are border zones [Kerne 2005] between humans, machines, games, and information systems. 
Interfaces mediate the ways in which humans can act and perceive information within a computer system. 
Interaction design principles urge making visible by providing information to guide the user [Norman 
2002]. Color [Itten 1997; Tufte 1990], animation [Ware 2004; Bederson and Boltman 1999], information 
visualization techniques [Ware 2004; Tufte 1990], and sonifications [Blattner et al. 1989] provide the 
user with information, enhancing understanding. As in games, feedback indicates the outcome of actions 
taken in an interface, which is essential for building mental models [Norman 2002]. Mental models are 
structures describing objects and people that enable inference and prediction of outcomes in an environment 
[Gentner and Stevens 1983]. 2.4 Team Coordination Distributed cognition takes a holistic view of the 
information flow in a working environment, considering how cognitive processes are embodied in humans 
and artifacts, and that they change through time [Hutchins 1995]. Distributed cognition examines the 
ways in which individuals transform and transport information across media. We take distributed cognition 
as our lens for examining fire emergency response work practice. Team cognition considers teams as functional 
cognitive units [Salas and Fiore 2004]. When team members share mental models, they can work together 
smoothly, lessening the need to communicate [Cannon-Bowers et al. 1993]. Implicit coordination occurs 
when team members operate effectively with little communication, leading to a beneficial reduction in 
cognitive and technological bandwidth, and time [MacMillan et al. 2004].  2.5 Fire Emergency Response 
Practice Our design is grounded in human-centered aspects of fire emergency response work practice. Fire 
emergency responders (FERs) work in multiple distributed teams, providing perspectives from in and around 
the fireground [Denef et al. 2008; Toups and Kerne 2007; Landgren 2006; USDHS 2004]. Teams observe and 
communicate to make sense of the situation, find and rescue victims, and put out fires. FERs in the role 
of firefighter act in teams in and around the fireground. An incident commander (IC), the highest ranking 
individual at an incident, directs the teams. The IC is positioned away from the fireground, observing 
it in context and using a variety of information artifacts [Toups and Kerne 2007, Xiaodong et al. 2004]. 
Information flows are complex and multi-way, with firefighters at the fireground providing information 
from multiple perspectives and taking situated action [Suchman 1987] while the IC coordinates them. Toups 
and Kerne develop design implications for teaching team coordination from ethnographic investigation 
of fire emergency response practice [2007]. Each team member s unique perspective, background, and information 
access create information differential. FERs mix communication modalities: they prefer to use face-to-face 
communication whenever possible, because it is fast, rich, and easy to understand. Some situations make 
face-to-face communication impossible, and so half-duplex radios are used instead. The use of audible 
cues enables firefighters to perceive more about their environment and sense information remotely through 
background sounds over the radio. These design principles are embodied in our game design to teach team 
coordination skills through non-mimetic simulation of fire emergency response work. Because the design 
implications do not specify a need to model fire and smoke, Toups et al. develop non-mimetic (a.) seeker 
local view (b.) coordinator overview Figure 1. Screenshots of non-mimetic simulation game for teaching 
team coordination. The seeker s view (a.) is highly detailed, showing other seekers, threats, goals, 
and walls; it is also local. The coordinator s view (b.) is an overview of the game world that is less 
detailed, showing seekers, threats, bases, and regions containing goals. In the pictures, Player3 (orange) 
is starting to collect a cooperative goal that requires all three seekers, while under attack by a flock 
of threats. The coordinator is tracking Player2 (blue) and observing the team act. The two sides of the 
team can communicate using radio. simulation principles [2009]. Information distribution develops information 
differential to reflect a grounding in distributed cognition. Participants need diverse perspectives 
that inter-relate so that they are reliant one another for the information they need to succeed; further, 
these perspectives should require information transformation to be effective. Dividing information along 
participant roles reflects FER work practice. Real-time stress ensures that participants must make quick 
decisions about what information to share, and how to share it. Game Design We develop game designs for 
non-mimetic simulation of fire emergency response. The game designs reflect human-centered aspects of 
response work practice through core mechanics that engage participants in processes of information transformation 
and communication. In this section, we describe the most recent iteration of our game design, including 
the game entities, player roles, and communication. Players act in teams structured like FER teams. Information 
access and action capabilities reflect FER practice, following the non-mimetic simulation principle of 
participant roles [Toups et al. 2009]. A team of four human players works together to play. Three players 
take on the role of seeker and search a virtual environment to find goals while avoiding threats. A time 
limit, along with the goal collection mechanics and the behavior of threats creates real-time stress. 
The coordinator assists the team by observing and communicating about a virtual world overview with different 
information from the seekers. The team members communicate information in order to work together effectively. 
3.1 Game Entities The objective of the game is to find and collect all of the goals in the virtual world 
within a time limit. Seekers collect goals by facing them for several seconds. Some goals are cooperative, 
requiring two or three seekers to gather simultaneously. The game includes a scoring mechanic, and cooperative 
goals are more valuable than single-seeker goals. Threats are virtual world entities that hunt down the 
seekers. Contact with a threat reduces a seeker s hit points (HP). Once the seeker s HP reach zero, that 
seeker must locate a base, a safe area that restores HP. A seeker with zero HP cannot collect goals. 
 3.2 Roles Each seeker sees a local view in a high level of detail, limited to an arc in front of their 
avatar (Figure 1, a.). The arc shows walls, threats, and goals, but not bases. The seeker head-up display 
(HUD), arranged around the viewing arc, includes information about location and orientation, distance 
to nearby threats, HP remaining, and the colors and names of teammates. A coordinator assists the group 
of seekers by observing the virtual world from an overview (Figure 1, b.). The coordinator s view is 
less detailed than that of the seekers. Coordinators can see the locations of seekers, threats, and bases. 
They can see regions that contain goals, but not their exact locations. The interface allows the coordinator 
to monitor the team s progress in the game, track the status of individual players, and observe the game 
world as a whole. Seekers are analogs for firefighters: they can act in the game world and can see a 
local, detailed perspective. The coordinator s role is like that of the IC: observing and directing the 
team.  3.3 Communication To facilitate voice communication between players, each player has a push-to-talk 
(PTT) radio. The radio functions through a monaural wireless headset and is controlled by the game software. 
Some status changes for seekers include sound effects, to provide additional information about the game. 
Sound effects are routed through the radio, as well as individual s headsets. If a player s PTT is active, 
then that player s game sound effects will be heard by other listening players. We thus fulfill the audible 
cue design principle [Toups and Kerne 2007] to enable remote sensing through the radio for players. 
4 Evaluation Method The data that inform the core mechanics and interface design principles comes from 
a series of studies and iterative designs that span four years. A series of early pilot studies rapidly 
iterated the game design, with later pilot studies refining it significantly. A sustained user study, 
in which 40 unique participants played 8 games each over the course of four weeks followed. Integrating 
feedback from the sustained user study, we conducted a participatory design phase, in which the authors 
played the game with an expert FER who has 30 years of experience. Finally, we examined data from an 
ongoing FER student user study, in which students at a local fire school play the game during their off 
hours. In all of the user studies, participants play the game on a set of laptop computers with the ability 
to communicate remotely, activated by key press (PTT). In the early pilot studies, players communicated 
using voice-over-IP with wireless headsets. This became problematic due to lag issues and an inability 
to record the players utterances. In later games, hardware was developed to route handheld radio voice 
through the computer, while recording it. Keyboard and mouse are the inputs to the game. In the early 
pilot studies, three conditions were used: all players sitting around a room and able to speak to each 
other freely; coordinator in a separate room, reachable only by VOIP with seekers co-located; and all 
players in separate rooms, communicating by VOIP. In subsequent studies, most games are played in one 
of two conditions, with both conditions forming a single session: seekers co-located with coordinator 
separate; and all players isolated. In the seekers co-located condition, seekers are seated around a 
table and able to speak to one another face-to-face. They may use the radio to contact the coordinator. 
In the other configuration, all players must use the radio to communicate. The configurations Table 
1. Summary of evaluations and resulting changes to the team game design. One session is a set of two 
games (one with seekers co­located, the other all isolated), played by 4 participants. evaluation sessions 
unique participants resulting changes early pilot studies 12 8* cooperative goal mechanic; HP mechanic; 
block-and-grid coordinates later pilot studies 3 12 making goals invisible in the coordinator interface; 
discernable patterns for threats sustained user study 40 40 goal collection status indicator; PTT status 
indicator FER expert participatory design 4 4 making threats visible to seekers; seeker location context 
indicator; PTT status audio FER student user study 13 16 remove PTT warning from PTT status indicator 
 * participants included one author participants consisted of the authors and expert FER, Cary Roccaforte 
 reflect the design principle of mixing communication modalities [Toups and Kerne 2007]. In the sustained 
and FER student user studies, participants play a set of four sessions (8 games) on the same team over 
the course of a month. The sustained user studies introduce a tutorial game in which all players are 
co-located for the first session that explains how to play and indicates the information distribution 
between the coordinator and seekers. The role of coordinator rotates each session, so all players have 
the opportunity to experience the role. This decision was made at the direction of our FER expert, as 
each student in fire school has the opportunity to experience IC roles. Participants are given time before, 
in between, and after the games to reflect on effective strategies [Schön 1983]. Participants are compensated 
(30 USD gift card and food at each session). Data is collected for all user studies through questionnaires, 
game logs, and audio. Questionnaires gather background, such as participants experience with video games 
and teams (emergency response, for example), as well as asking participants to recount specific events 
of teamwork during play. Logs are recorded on the game server and capture every aspect of the game state 
at 10Hz. All audio passing through the game system and spoken in the laboratory is recorded, including 
during the reflective periods. In­game audio and log data are synchronized using timestamps and played 
back together for analysis that includes quantitative evidence of improved teamwork skills through coding 
audio, as well as qualitative instances of coordination [Hamilton et al. 2009]. The design process is 
iterative, incorporating feedback and observations from previous game versions into the new. Newer designs 
improve in their ability to encourage participants to cooperate and engage team coordination skills. 
Designing Core Mechanics and Interfaces We develop core mechanics around activities of gathering, integrating, 
and sharing information to engage in distributed cognition while under real-time stress. FERs gather 
information about the incident and integrate it into their understanding to effectively fight fire and 
rescue victims. They share relevant portions of their understanding with one another to assist others 
in making sense of the environment, enabling strategy formation and protecting lives. These activities 
are undertaken by FERs as discovered through investigations of work practice [Toups and Kerne 2007, Landgren 
2006]. Interfaces mediate interaction with the game. Their design impacts the ways and extent to which 
participants engage in the core mechanics. We connect each core mechanic to interface components, and 
describe the components evolution based on data and feedback from users experiences. A summary of data 
sources can be found in Table 1. 5.1 Gathering: Locating Goals FERs search the fireground for victims 
and dangers. They use pre­plans (documents created from an inspection of local buildings), schematics, 
and maps, as well as direction from outside to plan actions at an incident. Gathering information is 
an essential activity in fire emergency response [Denef et al. 2008; Toups and Kerne 2007]. The objective 
of the game is to collect all of the goals in the terrain before time runs out. Central to the objective 
is locating goals and gathering intelligence about nearby terrain and the type of goal. Cooperative goals 
require multiple seekers to collect, so seekers must synchronize activities to be in the same place at 
the same time without being captured by threats. 5.1.1 Making Invisible: Hiding Goal Information In early 
designs, the exact location of each goal was visible in the coordinator s view (Figure 2, a.). The result, 
observed in a series of pilot studies, was that the coordinator and seekers did not have to collaborate 
to gather information. There was no need to gather information, as the coordinator had access to all 
that was necessary, and could share with the team. Typically, this took the form of top-down orders, 
directing seekers exactly where to go. Based on the need for seekers to engage in gathering information 
about the environment as part of distributed cognition, we altered how information about goals is distributed 
among team members. Instead of allowing the coordinator to see the exact location of each goal, goal 
locations are made fuzzy (Figure 2, b.). The coordinator can only see map regions that contain goals, 
so that it is possible to direct the seekers in general, but not tell them exactly what to do. To balance 
information distribution, and stimulate communication, we made information invisible in the coordinator 
interface. The goal mechanic design was iterated, along with the game s interfaces. Originally, all goals 
required only a single seeker to collect. We added cooperative goals. Through further iterations, a piece 
of information was removed from the coordinator s interface: the number of seekers required to collect 
a goal. This instance of making invisible again drives initiative from the seekers in the distributed 
cognition process. 5.1.2 Making Visible: Revealing Cooperative Goals To further distribute information 
among team members, the seekers have a detailed view when they get near a goal. They are able to see 
how many players are necessary to collect the goal (Figure 1, a.; Figure 3 the goal requires three seekers, 
as indicated by the three white rings around it). This information is hidden from the coordinator (Figure 
1, b; Figure 2, b), who may need to assist the seekers in grouping together. In the sustained and FER 
student user studies, this led to players developing strategies for scouting out goals. In this instance, 
we make visible.  5.1.3 Making Visible: Goal Collection Attribution Once the cooperative goal mechanic 
was introduced, players had difficulty understanding who was currently collecting a goal. Goal collection 
is signified through a set of rings that indicate the goal s status, and the number of seekers required 
for its collection (Figure 3, a.). In the original design, each ring was filled by a seeker (Figure 3, 
b.). This created confusion, as some players would incorrectly position their avatars around the goal. 
The goal would indicate that two seekers had collected part of the goal. A third seeker would be positioned 
incorrectly, but would incorrectly believe they were contributing and that another seeker (a.) precise 
goal locations and coordinates Figure 2. Goals in the original (a.) and current (b.) coordinator interfaces. 
Originally, goal locations are clearly marked and coordinates are specified in decimal numbers. This 
interface reduces communication and diminishes the role of seekers in the team. In the current version, 
information is made invisible in the coordinator's interface and moved to the seeker interface. Yellow 
highlighted regions contain goals; a block and grid coordinate system replaces the decimal system. was 
not. Problems appeared in the long-term user study, after cooperative goals were introduced. To correct 
players confusion about who was contributing to a goal s collection, we color the collection status according 
to a seeker s color (Figure 3, c.). In addition, we draw a line from each collecting seeker to the goal, 
linking that player to the goal to indicate their action (Figure 1, a.). This feedback is a change to 
making visible, and in later user studies, no further confusion has occurred.  5.1.4 Summary Seekers 
can see only a local view, which allows them, when they get close enough, to spot the exact locations 
of goals and the number of players required to collect them. The coordinator can only see which regions 
still contain uncollected goals. Fundamentally, these interfaces distribute information within the team. 
Coordinators are presented with a broad overview that lacks detail, while seekers can change their local 
views (by moving) and gather detailed information about the proximate environment. In iterating the design, 
we alter information distribution between players, making it more even and preventing one part of the 
team from knowing too much. We hide information, so that players must seek it out in the game environment, 
practicing gathering information. In this case, distributing information involves sometimes making invisible, 
contrary to traditional interface design [Norman 2002], and making visible in others. Modulating visibility 
is important in designing information distribution to ensure that participants need to work together. 
 5.2 Integrating: Evading Threats Firefighters integrate observations of the fireground with their knowledge 
of how fire works and their shared mental models of teammates [Gentner and Stevens 1983; Cannon-Bowers 
et al. 1993], to effectively fight fire and work together [Toups and Kerne 2007]. This allows them to 
predict future events and plan accordingly, reducing communication overhead and improving implicit coordination 
[MacMillan et al. 2004]. As firefighters searching for victims avoid fires, so seekers must avoid threats 
in the non-mimetic simulation game. They build up mental models of the locations of bases and offline 
areas to protect themselves from threats and remember goal locations. 5.2.1 Discernable Patterns: Avoiding 
Threats Threats supply real-time stress. One problem with early game designs is that threats were too 
difficult to avoid. Seekers were out when a threat came into contact with them. Threats were faster than 
seekers, and once they targeted a seeker, a threat would pursue until the seeker was out. From the early 
pilot studies, we addressed the problem of threats being too dangerous. We introduced the hit point (HP) 
mechanic. This allowed seekers to sustain several hits from a threat, making it easier to stay in the 
game. This iteration enabled us to add more interesting behaviors to threats, as we could include more 
threats in each game. After the late pilot studies, we applied particle physics [Reeves 1983] to the 
threats, and used flocking [Reynolds 1987] and particle choreography [Sims 1990] techniques to give them 
behaviors. This creates varied challenges for the seekers to overcome, and assists players in predicting 
what threats will do, increasing the player s ability to predict future outcomes using mental models 
[Gentner and Stevens 1983]. despite the HP mechanic and discernable patterns. Threats were invisible 
to seekers. The seeker HUD includes a proximity display to indicate when a threat was getting close, 
by filling up a meter with threat symbols (Figure 1, a.) corresponding to the inverse of the distance 
to the nearest threat. The intention of this design decision was to make the coordinator direct seekers 
around threats, distributing information. However, because seekers could not directly see threats, attacks 
felt random. In most cases, the coordinator could not communicate to seekers about threats fast enough. 
They were overwhelmed by the rapid onslaught of information. In some groups, the coordinator simply gave 
up on communicating to the team about threats. In this integrated design of core mechanic and interface, 
the timing of the information distribution did not, in practice, result in successful game play. The 
desired mechanic: coordinator would tell the seekers where the threats were, and seekers would avoid 
them, rarely materialized. To improve seekers ability to evade the fast-moving threats, and make the 
game experience less random, we made the threats visible in the seeker interface. In the ongoing user 
study with FER students, this has improved play. Seeker players do not feel like they were taken out 
of the game randomly. Because seekers cannot see behind them and cannot move faster than the threats, 
they still challenge players. By providing the seekers with the right information at the right time, 
we reduce the ephemeral information burden on coordinators. This makes coordination less cumbersome and 
frustrating for both coordinators and seekers. Another design choice might have been to slow the threats 
down. We did not choose this design because FERs in practice must respond to rapidly changing fire condition 
threats.  5.2.3 Summary Players experience threats through macro/micro views [Tufte 1990]. The coordinator 
can discuss threat behavior in macro, because s/he can see the flocks of threats moving about the map. 
The flocks are coherent, and move predictably. It becomes unnecessary to speak about them in micro, because 
seekers can see the local threats and react. This makes the game feel less random to players, and more 
predictable, but no less challenging. Here we see that making visible is essential. We suggest that making 
predictable is important as well, contributing to participants mental models [Salen and Zimmerman 2004; 
Gentner and Stevens 1983]. The discernable patterns enable participants to make plans while taking situated 
action [Suchman 1987]. Because some information is ephemeral, its value to the team (b.) original collection 
status (c.) collection status with color attribution 5.2.2 Information Timing: Threat Locations Information 
timing must be tuned to promote distributed cognition. In our sustained user studies and participatory 
design sessions, we observed seekers having difficulty avoiding threats, Figure 3. Goal collection status 
indicators. Un-collected cooperative goal (a.) shows three empty rings, one for each seeker necessary 
to collect the goal. Original collection status indicators (b.) do not show who is collecting the goal. 
Current collection status indicators (c.) indicate how much each seeker has contributed to the collection 
of the goal through color. extremely brief. Information timing must provide players with the right information 
in the right time. This reduces the burden on players who must communicate about it, easing frustration 
and making the game more fun.  5.3 Representations for Shared Mental Models One of the most essential 
aspects of fire emergency response practice is sharing information at an incident. Firefighters in the 
fireground have to act as the eyes and ears to the incident commander (IC) outside. The IC must make 
sense of the information from the firefighters and combine it with a contextualized overview that includes 
observing the fireground from a distance and consulting and maintaining information artifacts. Sensemaking 
enables the IC to formulate the best strategy and communicate orders for firefighters to accomplish it. 
Communication in fire emergency response is rich and multi-way. Communication between coordinator and 
seekers and between the seekers themselves is a core mechanic of the non­mimetic simulation game. Players 
need to share information about goals, walls, bases, threats, and each other to coordinate their actions, 
form shared mental models, and engage in distributed cognition. Communication is stimulated by information 
distribution. Representations must be designed to support shared mental models. In early game designs, 
the seekers did not communicate: there was no need to. The coordinator knew the exact location of every 
goal and exactly how many players were needed to collect each (1). Players had difficulty understanding 
how the radio worked, and thus shunned its use. This did not reflect fire emergency response practice. 
5.3.1 Sharing Location In the early pilot studies, location was difficult to communicate within the team 
because locations were given as a pair of detailed coordinates in the X / Y plane (e.g. 123.83, 475.20; 
Figure 2, a.). As seekers moved, the numbers changed rapidly. We observed the coordinator directing seekers 
using the blocks drawn on the background of the map ( move two blocks east, one block north ) instead 
of the coordinates. Based on this observation and the need for locations to be easily referenced, we 
introduced a block-and-grid interface. We divided the terrain into five columns and five rows. Each column 
is numbered (1-5) and each row is lettered (A-E) so that coordinates consist of letter, number combinations 
(Figure 2, b.). In later user studies, this was observed to improve participants ability to communicate 
location with each other, as the letter­number combinations were used extensively. 5.3.2 Collaborative 
Navigation In sustained user studies and participatory re-design, it became clear that seekers had difficulty 
navigating to locations specified by the coordinator. While the coordinator could provide directions, 
this was often a cumbersome process, made more difficult by seekers moving constantly. During a series 
of participatory re-design sessions with an FER expert (30 years experience), we augmented the status 
/ compass HUD element in the seeker interface. Instead of showing the seeker avatar icon rotating to 
indicate direction, the icon is held facing forward. Around the edges of the icon, the next nearest block 
and grid locations are displayed (Figure 4). If a player needed to move from location 2, C to location 
2, B, the player needs only rotate until the B is in front of the avatar icon and move forward. To enhance 
understanding of this interface element, we also clearly demark the edges of the blocks on the map, so 
as seekers move, they can see the boundaries.  5.3.3 Monitoring Communication Status Players had difficulty 
developing an understanding of how the radio worked. Because the radio is half-duplex, only one player 
can communicate at a time. Players would cross-talk (a common problem in real-life teams), and thus have 
difficulty understanding each other. Another issue faced by teams when using the radio is connection 
lag when using push-to-talk (PTT). There is a 500ms 1,500ms lag between when the PTT button is keyed, 
and when receiving radios pick up the transmission. The result was that players would frequently fail 
to get the first parts of their messages to their teammates, who would either misunderstand or be unable 
to understand the communication. To address these issues, we introduce a radio status visualization to 
all of the game interfaces (Figure 5). The status visualization depicts an icon of a radio. Whenever 
there is voice communication on the line, the status visualization lights up in red, indicating it is 
unsafe to talk. Whenever a player keys their radio, the visualization turns yellow for a second, then 
turns green. In addition, a set of waves animate from the antenna on the radio to indicate it is transmitting. 
An audio interface is also used to help delay players briefly before they start talking. The sonification 
lasts for approximately one second and stops early if the player releases the PTT key. This modification 
was made based on the suggestion of an expert FER, who indicated that radios used in the field work in 
this way. Based on recent FER student user studies, we plan to make invisible a part of the PTT status 
indicator. In real life, FERs do not have a warning light to indicate to them that someone may be trying 
to use the radio. According to an expert FER, cross-talk is common with half-duplex radios and FERs must 
learn to deal with it effectively in the field. As part of modulating visibility, we plan to remove the 
PTT warning from the PTT status indicator.  Figure 4. Seeker location and status indicator. The seeker 
avatar icon in the background shows the current state of the seeker; Figure 5. Radio status indicator 
animation and sound. The overlaid is location and hit points. Letters and numbers around the animation 
and sound help participants understand that the radio edge indicate nearby locations. takes about a second 
to connect to the radios of other players. 5.3.4 Summary To better reflect the field of fire emergency 
response, where the firefighters are the eyes and ears of the IC and where the IC provides direction 
[Toups and Kerne 2007], we make the seeker interface more detailed to provide good representations on 
which to build shared mental models. We simplify map references by using a block-and-grid system. Although 
this reduces the granularity of the map for communication and navigation, it simplifies what players 
must communicate about. Crafting visualizations and sonifications for the radio assists players in building 
a mental model of its use. Players become better able to time their transmissions and avoid speaking 
when they will be unheard by the team. 6 Cooperative Game Design Principles From the iterative design 
of our non-mimetic simulation team game, we describe game design principles from the core mechanics and 
interface components. Information distribution applies the non-mimetic simulation design principle of 
information distribution to game mechanics, impacting the way participants play the game [Toups et al. 
2009]. Modulating visibility is essential to distributing information between players, encouraging communication 
and cooperation. The timing of information distribution must also be considered, as it can impact the 
way in which players are able to communicate with each other and engage game mechanics. Predictability 
impacts players abilities to form mental models and maintain situation awareness, contributing to team 
members ability to work together. 6.1 Information Distribution Previously described as an interface design 
implication for teaching team coordination skills [Toups and Kerne 2007] and a non-mimetic simulation 
principle [Toups et al. 2009], we consider information distribution as a game mechanic. Information is 
shared between players through their interfaces, often modulating visibility, but it impacts the way 
they play the game. Creating information distribution involves determining the information necessary 
to play the game and effectively sharing it between participants, so that each player has access to a 
different piece of the information picture. Information distribution is accomplished through different 
participant roles [Toups et al. 2009]. Players must be reliant on each other to complete the game. Information 
distribution encourages engagement with the core mechanic of team communication, and requires participants 
in different roles to gather and integrate different types of information in different representations. 
6.2 Modulating Visibility Despite the interaction design mantra of making visible [Norman 2002], we find 
that making invisible can be just as important when designing cooperative game interfaces. As part of 
information distribution [Toups and Kerne 2007], some information is withheld from players and provided 
to others. Throughout the design process, we find that developing the proper balance of visible/invisible 
information in team members interfaces is important, as it impacts their sources of information (the 
interface versus other players). The timing of information is essential in the selection of whether information 
should be made visible or invisible. Slow-changing information (such as goal locations, which never change) 
is a good candidate for making invisible. In games where communication is a core mechanic, team members 
must have something to communicate about. Creating deficiencies in one interface that are fulfilled by 
another is one way of accomplishing this. 6.3 The Right Information in the Right Time Part of creating 
information distribution and real-time stress [Toups et al. 2009] involves rapid information change. 
Information in games may be ephemeral. The temporality of information must be considered when players 
need to communicate about it. Short-lived information that must be acted upon quickly should not have 
to be communicated using slow channels, such as radio. Players will be unable to react in time and may 
perceive the game mechanics as unfair. The user interface must provide the right information in the right 
time. While we made threats visible to avoid too-fast information timing, we were able to make goal details 
invisible to the coordinator, because of their slow timing. 6.4 Making Predictable Mental models enable 
players to understand and manipulate the game in their heads [Salen and Zimmerman 2004; Gentner and Stevens 
1983]. When mental models are shared, players are able to cooperate more effectively, because their mental 
models predict things in the same way [Cannon-Bowers et al. 1993]. Game mechanics must be consistent 
[Salen and Zimmerman 2004], they must provide some level of predictability, to enable mental model formation. 
Threat behaviors were introduced to make the threats predictable. Flocks are clear as the threats move 
around the playing field, although the patterns that the flocks follow may not be. Despite the complexity, 
players know that threats move together, and that they will react to seekers in a certain way. This allows 
the coordinator to predict when threats will be a problem for seekers, and warn them accordingly. 6.5 
Communicable Representations The way information is presented in a game s interface impacts the way players 
are able to use it. For players to engage in team processes of distributed cognition, they must be able 
to construct a shared understanding of the game system and be able to communicate about it. Essential 
to building effective interfaces for team coordination games is creating representations that are easily 
understood and referenced while under the real-time stress of game play. The block-and-grid coordinate 
system is one mechanism for this: it makes it easier for players to communicate about location in a way 
that is meaningful and staisfices [Simon 1996] for the situation. Information to be shared should be 
easy to communicate, in order to reduce communication overhead. 7 Conclusion We have developed game design 
principles for cooperative game play. We described the core mechanics and interfaces for a non­mimetic 
simulation game of fire emergency response work practice that focuses on learning team coordination skills. 
Based on prior design implications for teaching team coordination and non-mimetic simulation principles, 
the game eschews fire and smoke in favor of human-centered aspects of firefighting, such as information 
flows. Core mechanics center around players ability to gather, integrate, and share information. We examined 
interface components that contribute to engagement in the core mechanics using pilot studies, sustained 
user studies, and participatory re­design sessions to evaluate them. Use of the game design principles 
directs play such that players must coordinate to succeed. Information must be distributed, so that team 
members work together to build the information picture. Modulating visibility, rather than simply making 
visible, is essential for distributing information and encouraging communication. Designing for information 
timing helps team members enjoy the game without the burden of trying to communicate information that 
will be stale and rarely useful. Predictability of game elements helps eliminate a sense of randomness 
and creates accountability for the game system, assisting players in formulating shared mental models 
that help them to coordinate. Representations of distributed information can be designed in consideration 
of the clues that different players receive to facilitate communication and promote the formation of 
shared mental models. Through the non-mimetic simulation of fire emergency response, we are constructing 
a system for teaching team coordination to FERs. We hypothesize that a non-mimetic simulation of fire 
emergency response may also prove effective for teaching team coordination to other types of teams. Future 
research will investigate how well this system teaches team coordination to FERs and other teams, such 
as programmers. We work through a science of design, constructing educational game software. The implications 
from such software will be useful for constructing engaging, fun games for learning in other domains. 
The principles presented here should be considered for any cooperative team game in which the goal is 
to encourage teamwork and interaction among players. 8 Acknowledgements We are grateful for the support 
of Fire Chief Cary Roccaforte in providing his expertise in FER work practice and shaping the game designs. 
This work supported by the NSF, IIS-0803854 and IIS-0905594. 9 References BEDERSON, B.B. AND BOLTMAN, 
A. 1999. Does animation help users build mental maps of spatial information? In Proc. IEEE INFOVIS, 28-36. 
BLATTNER, M., SUMIKAWA, D., AND GREENBERG, R. 1989. Earcons and icons: Their structure and common design 
principles. In Human Computer Interaction 4, 1, 11-44. CANNON-BOWERS, J.A., SALAS, E., AND CONVERSE, 
S. 1993. Shared mental models in expert team decision making. In Castellan, N.J., Ed. Individual and 
Group Decision Making: Current Issues. Lawrence Earlbaum Associates, 221-246. DENEF, S., RAMIREZ, L., 
DYRKS, T., AND STEVENS, G. 2008. Handy navigation in ever-changing spaces: An ethnographic study of firefighting 
practices. In Proc. ACM Designing Interactive Systems, 184-192. GENTNER, D. AND STEVENS, A.L., Eds. 1983. 
Mental Models. Lawrence Earlbaum Associates. GRANLUND R., JOHANSSON, B., PERSSON, M. 2001. C3Fire: A 
micro-world for collaboration training in the ROLF environment. In Proc. Simulation and Modeling, Simulation 
Theory and Practice. Hamilton, W.A., Toups, Z.O., and Kerne, A. 2009. Synchronized communication and 
coordinated views: qualitative data discovery for team game user studies. In Proc. Ext. Abs. ACM SIGGCHI, 
4573-4578. HUTCHINS, E. 1995. Cognition in the Wild. MIT Press. ITTEN, J. 1997. The Art of Color: The 
Subjective Experience and Objective Rationale of Color, revised ed. Wiley. KERNE, A. 2005. Doing interface 
ecology: The practice of metadisciplinarity. In ACM SIGGRAPH Art and Animation Catalog, 181-185. LANDGREN, 
J. 2006. Making action visible in time-critical work. In Proc. ACM SIGCHI, 201-210. MACMILLAN, J., ENTIN, 
E.E., AND SERFATY, D. 2004. Communication overhead: The hidden cost of team cognition. In [Salas and 
Fiore 2004], 61-82. MALONE, T.W. 1981. Toward a theory of intrinsically motivating instruction. In Cognitive 
Science 5, 4, 333-369. NORMAN, D.A. 2002. The Design of Everyday Things. Basic Books. PAGE, E.H. AND 
SMITH, R. 1998. Introduction to military training simulation: A guide for discrete event simulationists. 
In Proc. ACM Winter Simulation Conference, 53-60. REEVES, W.T. 1983. Particle systems A technique for 
modeling a class of fuzzy objects. In ACM Trans. Graphics 2, 2, 91-108. REYNOLDS, C.W. 1987. Flocks, 
herds and schools: A distributed behavioral model. In Computer Graphics (Proc. ACM SIGGRAPH 87), 25-34. 
SALAS, E. AND FIORE, S.M., Eds. 2004. Team Cognition: Understanding the Factors that Drive Process and 
Performance. American Psychological Association. SALEN, K. AND ZIMMERMAN, E. 2004. Rules of Play: Game 
Design Fundamentals. MIT Press. SCHÖN, D.A. 1983. The Reflective Practitioner: How Professionals Think 
in Action. Basic Books. SIMON, H.A. 1996. The Sciences of the Artificial, 3rd ed. MIT Press. SIMS, K. 
1990. Particle animation and rendering using parallel computation. In Computer Graphics (Proc. ACM SIGGRAPH 
90), 405-413. SONG, A.A. AND KLEINMAN, D.L. 1994. A distributed simulation system for team decisionmaking. 
In Proc. AI, Simulation, and Planning in High Autonomy Systems, Distributed Interactive Environments, 
129-135. SUCHMAN, L. 1987. Plans and Situated Actions. Cambridge University Press. TOUPS, Z.O. AND KERNE, 
A. 2007. Implicit coordination in .re.ghting practice: Design implications for teaching .re emergency 
responders. In Proc. ACM SIGCHI, 707-716. TOUPS, Z.O., KERNE, A., HAMILTON, W., AND BLEVINS, A. 2009. 
Emergent team coordination: From fire emergency response practice to a non-mimetic simulation game. In 
Proc. ACM Group, 341-350. TUFTE, E.R. 1990. Envisioning Information. Graphics Press LLC. UNITED STATES 
DEPARTMENT OF HOMELAND SECURITY. 2004. National Incident Management System. WARE, C. 2004. Information 
Visualization: Perception for Design, 2nd ed. Morgan Kaufman. XIAODONG, J., HONG, J.I., TAKAYAMA, L.A., 
AND LANDAY, J.A. 2004. Ubiquitous computing for firefighters: Field studies and prototypes of large displays 
for incident command. In Proc. ACM SIGCHI, 678-686.     
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>NSF</funding_agency>
			<grant_numbers>
				<grant_number>IIS-0803854IIS-0905594</grant_number>
			</grant_numbers>
		</article_sponsors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1581086</article_id>
		<sort_key>130</sort_key>
		<display_label>Pages</display_label>
		<article_publication_date>08-04-2009</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Experimental evaluation of teaching recursion in a video game]]></title>
		<page_from>79</page_from>
		<page_to>86</page_to>
		<doi_number>10.1145/1581073.1581086</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1581086</url>
		<abstract>
			<par><![CDATA[<p>We present <i>EleMental</i>: The Recurrence, a novel game that provides computer science students the opportunity to write code and perform interactive visualizations to learn about recursion through depth-first search of a binary tree. We designed the game to facilitate maximum transfer of learning to writing real programs, while also providing for interactive visualizations. We conducted a study with computer science majors to measure the impact of the game on learning and on attitudes toward educational games. Our results demonstrate the enthusiasm students have for learning games and provide insight into how such games should be constructed.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[education]]></kw>
			<kw><![CDATA[evaluation]]></kw>
			<kw><![CDATA[game development]]></kw>
			<kw><![CDATA[motivation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>K.3.2</cat_node>
				<descriptor>Computer science education</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003457.10003527.10003531.10003533</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education->Computing education programs->Computer science education</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003527.10003531.10003533</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education->Computing education programs->Computer science education</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1570328</person_id>
				<author_profile_id><![CDATA[81385597894]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Amanda]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chaffin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Charlotte, Charlotte, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570329</person_id>
				<author_profile_id><![CDATA[81440608833]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Katelyn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Doran]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Charlotte, Charlotte, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570330</person_id>
				<author_profile_id><![CDATA[81440607905]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Drew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hicks]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Charlotte, Charlotte, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570331</person_id>
				<author_profile_id><![CDATA[81100476451]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tiffany]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barnes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Charlotte, Charlotte, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ACM/IEEE-CS Joint Curriculum Task Force. Computing Curricula 2001. Accessed February 13, 2008. http://www.acm.org/education/curric_vols/cc2001.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Aspray, W., and Bernat, A. Recruitment and retention of underrepresented minority graduate students in computer science: Report of a workshop, March 4--5, 2000. Washington, DC: Computing Research Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1463674</ref_obj_id>
				<ref_obj_pid>1463673</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Barnes, T., E. Powell, A. Chaffin, H. Lipford. Game2Learn: Improving the engagement and motivation of CS1 students. ACM GDCSE 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1268821</ref_obj_id>
				<ref_obj_pid>1268784</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Barnes, T., H. Richter, E. Powell, A. Chaffin, A. Godwin. (2007). Game2Learn: Building CS1 learning games for retention, Proc. <i>ACM Conference on Innovation and Technology in Computer Science Education</i> (ITiCSE2007), Dundee, Scotland, June 25--27, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1121498</ref_obj_id>
				<ref_obj_pid>1121341</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Bayliss, J., and S. Strout. Games as a "flavor" of CS1. In SIGCSE 2006. ACM Press, New York, NY, 500--504.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1083474</ref_obj_id>
				<ref_obj_pid>1083431</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Beaubouef, T. and J. Mason. Why the high attrition rate for computer science students: some thoughts and observations. SIGCSE Bull. 37, 2 (Jun. 2005), 103--106.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>775347</ref_obj_id>
				<ref_obj_pid>775339</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Becker, K. 2001. Teaching with games: The Minesweeper and Asteroids experience. The Journal of Computing in Small Colleges Vol. 17, No. 2, 2001, 22--32.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Bioware Corporation. Aurora Neverwinter Toolset. Accessed Jan. 2, 2008. http://nwn.bioware.com/builders/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Darkwynter Studio. The DarkWynter 3D Game Engine. Accessed March 27, 2008. http://darkwynter.com/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1508980</ref_obj_id>
				<ref_obj_pid>1508865</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Eagle, M., T. Barnes. Experimental evaluation of an educational game for improved learning in introductory computing. ACM SIGCSE 2009, Chattanooga, TN, March 3--8, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Enterbrain Corporation. RPGMaker XP. Accessed Jan. 2, 2008. http://www.enterbrain.co.jp/tkool/RPG_XP/eng/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Garris, Ahlers, and Driskell. Games, motivation, and learning: a research and practice model. <i>Simulation and Gaming</i>, Vol. 33, No. 4, 2002, 441--467.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Gee, J. P. What video games have to teach us about learning and literacy. <i>Compute Entertain of Educational Technology</i>. 30, 4 (1999), 311--321.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>950591</ref_obj_id>
				<ref_obj_pid>950566</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Jenkins, Henry, Eric Klopfer, Kurt Squire, and Philip Tan. Entering the Education Arcade. <i>Source Computers in Entertainment</i>, Volume 1, Issue 1 (Oct. (October 2003), 20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Lepper, M. R., and Malone, Th. W. 1987. Intrinsic motivation and instructional effectiveness in computer-based education. In R. E. Snow and M. J. Farr (Eds.), Aptitude, learning, and instruction: Vol. 3. <i>Conative and affective process analyses</i> (pp. 255--286). Hillsdale, NJ: Lawrence Erlbaum.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Losh, Elizabeth. In Country with Tactical Iraqi: Trust, Identity, and Language Learning in a Military Video Game. <i>Virtualpolitik</i>. 2006. August 3, 2006. http://virtualpolitik.org/DAC2005.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1352260</ref_obj_id>
				<ref_obj_pid>1352135</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Maloney, J. H., K Peppler., Y Kafai, M. Resnick, N. Rusk, Programming by choice: urban youth learning programming with scratch, in Proc. of the 39th SIGCSE <i>Technical Symposium on Computer Science Education</i>, pp 267--371, Portland, OR, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Microsoft Corporation. XNA Game Studio Express. Accessed March 27, 2008. http://xna.com]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Mullins, P, Whitfield, D, and Conlon, M. Using Alice 2.0 as a First Language. To appear in CCSC 24th Annual Eastern Conference 2008 (Hood College, Frederick, Maryland, October 10 and 11, 2008).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1121500</ref_obj_id>
				<ref_obj_pid>1121341</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Parberry, Ian, Max B. Kazemzadeh, Timothy Roden. The art and science of game programming, Proc. 37th SIGCSE <i>Technical Symposium on Computer science education</i>, March 03--05, 2006, Houston, Texas, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1029564</ref_obj_id>
				<ref_obj_pid>1029533</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Bierre, Kevin J. and Andrew M. Phelps. The use of MUPPETS in an introductory java programming course, SIGITE 2004, October 28--30, 2004, Salt Lake City, UT, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1121407</ref_obj_id>
				<ref_obj_pid>1121341</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Polack-wahl, J. and Anewalt, K. Undergraduate research: Learning strategies and undergraduate research. SIGCSE 2006:209--213.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1209307</ref_obj_id>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Prensky, M. Digital Game-Based Learning, New York, McGraw Hill, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Shaffer, David, Kurt R. Squire, Richard Halverson, and James P. Gee. Video Games and the Future of Learning. Academic Advanced Distributed Learning Co-Lab. December 10, 2004. August 3, 2006. http://www.academiccolab.org/resources/gappspaper1.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Vesgo, J. Continued Drop in CS Bachelor's Degree Production and Enrollments as the Number of New Majors Stabilizes. Computing Research News, Vol. 19, No. 2., March 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Zweben, S. 2006--2007 Taulbee Survey. Computing Research News, 20, 3 (May 2008).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Experimental Evaluation of Teaching Recursion in a Video Game Amanda Chaffin, Katelyn Doran, Drew Hicks, 
and Tiffany Barnes Department of Computer Science University of North Carolina at Charlotte 9201 University 
City Blvd., Charlotte, NC, USA 1-704-687-8577  tbarnes2@uncc.edu ABSTRACT We present EleMental: The 
Recurrence, a novel game that provides computer science students the opportunity to write code and perform 
interactive visualizations to learn about recursion through depth-first search of a binary tree. We designed 
the game to facilitate maximum transfer of learning to writing real programs, while also providing for 
interactive visualizations. We conducted a study with computer science majors to measure the impact of 
the game on learning and on attitudes toward educational games. Our results demonstrate the enthusiasm 
students have for learning games and provide insight into how such games should be constructed.  Categories 
and Subject Descriptors K.3.2 [Computers and education]: Computer and information science education. 
 computer science education. General Terms Design, Human Factors  Keywords Game development, education, 
motivation, evaluation. 1. INTRODUCTION Even as employment opportunities for computer scientists rise, 
the number of students enrolling and graduating from college with computer science (CS) degrees is on 
a steady decline [Vesgo 2007]. Most of the attrition in the CS degree happens early on, typically in 
the first two CS courses, and is as high as 40% in some cases [Beaubouef &#38; Mason, 2005]. Some of 
the reasons for this attrition are: lack of preparation in the necessary logic and math skills, miscommunications 
between instructors and students, poorly designed instructional blocks and topic coverage, poorly designed 
programming projects and code examples, and under prepared or poor localized language skilled instructors 
[Beaubouef &#38; Mason, 2005]. Many researchers have discussed the possible benefits for using games 
in education and we believe they may be particularly helpful in improving computer science education. 
Games provide Copyright &#38;#169; 2009 by the Association for Computing Machinery, Inc. Permission to 
make digital or hard copies of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions 
from Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org. Sandbox 2009, New 
Orleans, Louisiana, August 4 6, 2009. &#38;#169; 2009 ACM 978-1-60558-514-7/09/0008 $10.00 a way to 
create and share educational content while also making students feel their computing education is more 
relevant. We hope to leverage the increased motivation games inherently provide to increase student attitudes 
toward computing and help them learn at the same time. Games have been used to teach students a variety 
of subjects ranging from training a soldier the Arabic language and culture to teaching high school students 
the events that led up to the American Revolution [Losh 2006; Jenkins 2006]. In computer science, building 
video games provides an integrative experience that uses a wide range of computing skills along with 
professional and soft skills [Bayliss &#38; Stout, 2006; Parberry et al, 2005; Becker, 2001]. Our Game2Learn 
project extends this approach, involving computing students in creating educational games that are in 
turn used for teaching younger computing students [Barnes 2007].  2. BACKGROUND Games are increasingly 
recognized for their inherent motivation, as inspiration for improving educational applications [Barnes 
et al. 2008; Garris et al. 2002; Parberry et al. 2005]. However, there is little consensus on what makes 
for an effective instructional game [Gee 1999]. Lepper and Malone have investigated the most important 
factors in making educational games fun, listing as most game design books do the importance of challenge, 
or the balance between ease and difficulty, in engaging learners in games, but highlight the need to 
design in activities that help learners address and revise their misconceptions [1987]. Garris, Ahlers, 
and Driskell [2002] have classified the factors that are important to games effectiveness for learning, 
and have identified motivation to play and play again as a key feature of the best instructional games. 
Their framework reveals that is important to weave game engagement in with the types of feedback most 
useful in learning [Garris et al. 2002]. We have therefore striven to design educational games that balance 
play and learning time, make strong ties between in-game motivation and learning outcomes, and that can 
be used as learning tools in a standard introductory computing course. Games have long been used to engage 
students in learning computer science. Becker [2001] found that programming Minesweeper and Asteroids 
in the first two computer science courses can help students better understand object inheritance [Becker, 
2001]. Bayliss and Strout teach introductory computer science using games at the Rochester Institute 
of Technology (RIT), and have found improved performance and decreased feelings of intimidation by peers 
[Bayliss, Stout, 2006]. Games are also used as advanced projects for students in capstone projects. At 
the University of North Texas, most computer science majors complete the successful capstone course, 
where they work in teams to create games of their own choosing using realistic tools and practices [Parberry 
et al, 2005]. This work encouraged us to include real programming tasks in our games. Developed at RIT, 
MUPPETS (Multi-User Programming Pedagogy for Enhancing Traditional Study) is a game where students develop 
and interact with visible 3D objects in the game world [Bierre &#38; Phelps, 2004]. Using Java, students 
can edit existing code, create new code, compile and run their code and have direct feedback in the form 
of compilation errors or, in the case of the code being correct, have the changes appear in the game. 
In the introductory course, students program their robots to compete in a class-wide battle. The MUPPETS 
approach is very similar to our own, but we build in more scaffolding and instruction into our games. 
Alice 2.0, created at Carnegie Mellon University, allows students to program using pull-down menus in 
a 3D environment, similar to the MUPPETS system. Instead of creating robots to battle in an arena, however, 
Alice 2.0 is mostly a story-telling game where students create stories to be played out. Mullins et al 
did a comparison study between students who used Alice 2.0 to program and students who programmed in 
a more traditional C++ programming environment. In the study, retention rates seemed to be slightly higher 
in classes using Alice, even if the instructor changed to using Alice partway into the semester, than 
the more traditional classes. The paper reports that students who took the course were more likely to 
feel like they can solve problems than those who took traditional courses (Mullins, et al. 2008). Scratch, 
a visual programming language, was created at the Michigan Institute of Technology (MIT) by the Lifelong 
Kindergarten Group in collaboration with UCLA and teaches students how to program in a visual learning 
environment. Students can create custom scripts, with some advanced programming concepts (sequential 
execution, threading, user interaction, conditionals, etc.) as well as more basic programming concepts 
(variables, print statements, etc). From following the students for 18 months, they noticed increased 
gains in Boolean logic, variables, and random numbers, to name a few. Maloney, et al, also asked thirty 
random students several qualitative questions and the biggest surprise was their finding that their students 
did not realize they were programming at all with Scratch. As they had likened Scratch to a notepad, 
it is rather unsurprising they would not quite realize what they are doing with the program (Maloney, 
et al 2008). At UNC Charlotte, undergraduate students have built and evaluated several Game2Learn games 
for teaching computing. Saving Sera [built in RPG Maker by Enterbrain Corporation] and The Catacombs 
[built in Neverwinter Nights by Bioware Corporation] are role-playing games (RPGs) designed to teach 
control logic such as if-then statements and for loops [Barnes 2007, 2008]. User studies of these games 
showed positive learning gains, and after we made some changes to add more feedback, positive attitudes 
about using such games as homework. However, when we used these games with CS1 students, we found that 
they were not focused enough and caused some students confusion. Therefore, we now design our games to 
concentrate most of the gameplay on learning rather than world exploration. Squee and The Tournament, 
two games for teaching functions, have also shown positive learning gains, though this work is currently 
unpublished. From Squee, we learned that a complex interface can overshadow the benefits of a good metaphor, 
and with The Tournament, we learned that a three­dimensional (3D) RPG with interactive dialog can make 
the presentation of learning material more engaging than a lecture. Wu s Castle is a 2D RPG where players 
must build armies of snowmen to escape a mirror universe [Eagle 2008]. Of course, students learn to use 
arrays and for loops to construct these armies. We have conducted extensive studies with Wu s Castle 
and have shown playing the game before doing a programming assignment to learn loops and arrays can increase 
overall learning [Eagle 2008, 2009], and we now use this game in UNC Charlotte s introductory programming 
course. From this game, we have learned the importance of full logging of game interactions and learning 
outcomes, which can indicate what parts of the game are working best. We also found that interactive 
instruction with small steps can help lead the players through the learning process while still keeping 
the players engaged and feeling as if they are playing a game .  3. GAME2LEARN DESIGN &#38; ENGINE Designing 
educational games requires a different focus than general game design; otherwise, we may fall into the 
trap of designing fun games with no learning value [Barnes 2007]. During the course of the Game2Learn 
project, we discovered it is best for the game designers to select first the target concept they want 
to teach. Then the designers create the code that best illustrates the target concept. Only after the 
concept and the target code are designed do the designers begin to develop a game that wraps the concept 
and the code in a game mechanic that works as a metaphor for the entire game as well as the coding concept. 
This methodology constrains the game design and game engine choice to best match the target computing 
concept and metaphor. After the overall game concept is defined, the designers then tailor the game instructions 
to the students to support students in writing code and learning concepts. Game instructions include 
both how to play and write in-game code as well as educational instruction about the game content. Scaffolding 
code, that is, pre-written code provided to help students get started, is designed at this point as well, 
although we try to limit the overall amount provided to reduce complexity for introductory students. 
We used this Game2Learn methodology to design EleMental: The Recurrence to teach recursion. Our previous 
games focused on teaching introductory concepts; here we wanted to provide a learning game for more advanced 
computing students, combined with the capability to write, compile, and run a program within the game 
environment. EleMental was created using a game engine called DarkWynter that was created by a team of 
students in UNC Charlotte s Game Design and Development program [DarkWynter Studio]. Unlike commercial 
game engines, the DarkWynter engine s code base is open source for academic purposes. The engine is coded 
using XNA s Game Studio 2.0, and integration with the C# s Code Dom allows us flexibility to create games 
that could compile any .Net language [Microsoft Corporation]. The DarkWynter engine allows for real-time 
terrain modification that we planned leveraged for interactive visualization of depth first search (DFS) 
in a binary tree. With an obvious data structure (a tree), a standard algorithm to code (DFS), and the 
majority of the implementation needed for gameplay built into the engine (real time terrain modification), 
we were quickly able to finalize the code we wanted players to produce. Since the game allows coding 
in C#, but our students learn C++ or Java, we designed the game code to be relatively free of C# quirks. 
To avoid teaching students more about C# than necessary, we wrote scaffolding code for all the coding 
challenges. Finally, we created a parsing method to check both the students code and their output to 
insure that their code matched what we wanted them to produce. After completing the parsing section, 
we then turned our attention to the design of the game.  4. ELEMENTAL GAME DESIGN The overall design 
of the game involves completing three programming puzzles, helped by Ele, a programmable avatar shown 
in Figure 1, Thoughts shown in Figure 2 for visualizing data to collect, and Cera, an in-game mentor, 
who instructs students as they progress through the game, with dialog as shown in Figure 3. Figure 2: 
Thought Icon Figure 1: Ele Icon In the game, students first take a brief pretest to determine their understanding 
of recursion. Then students complete a basic hello world program to get used to the compiler interface. 
Then students walk their character using depth first search traversal to collect Thoughts from the leaves 
of a binary tree. In level 2, students must correctly code the traversal for the left side of DFS. After 
their code is written, Ele walks through the tree using the student code while Cera explains what the 
code is doing. In level 3, students code both the right and left DFS for the binary tree and navigate 
Ele through the binary tree using the keyboard and mouse. This time, visualization is provided for the 
stack calls the recursive algorithm makes. Once the player finishes the game, they take the final survey 
to complete their journey. Figure 3: Cera's Dialogue Box  4.1 Code Descriptions Students write three 
programs in the game: Hello World, finish an incomplete DFS, and write a DFS with more of the code removed. 
Each program has scaffolding code, such as that in Table 1, as well as its own set of instructions. In 
the Hello World puzzle, we explain to the students how the interface works and give them the brief overview 
of the plot: the students are trapped in their own minds and must code their way back to normality. We 
also explain the only real difference between C# and C++ or Java code we are going to ask them to write, 
is Console.WriteLine as opposed to C++ s cout command or Java s System.Out.Println. Finally, we instruct 
them to compile and run their code, as well as what to do if their code does not compile. In the second 
puzzle, we provide the students with the scaffolding code to perform a DFS on a tree. Unlike the Hello 
World code, this code is more complex since it performs a DFS on an actual tree within the game engine. 
Instead of having the students write the entire program, we simply have them write the traversal code 
for the left node, which can be modeled after that for the right, as shown in the scaffold code given 
in Table 1. In level 3, the students write all the code for the recursive portion of the DFS. Table 1: 
Level 2 DFS scaffold code  4.2 Traversal and Instruction After the student finishes the code portion 
in each level, they walk through the tree in DFS order themselves or watch their programmed AI do so. 
In the first level, the students walk through the tree as Cera provides instructions and hints on how 
to walk through the tree. For example, Cera says the following Do you recognize what we're doing? We 
are traveling to each of the furthest islands in a set order. Can you think of a name for this type of 
traversal? A moment later, when the student reaches the new node, Cera says, Yes! I've got it! We are 
using a depth-first traversal. For this traversal we travel as far down each branch as possible." The 
letter stored at each node is shown in the first­person view, while a mini-map displays only the tree 
structure and the student s progress. Figure 4: Level 2 AI walkthrough In the second level, after the 
students write the left portion of the DFS, they watch the programmed AI traverse the tree from an overhead 
view shown in Figure 4. As the traversal proceeds, dialogue is shown explaining how recursive calls are 
made to direct the character, as in Table 2. Table 2: Level 2 DFS instructional dialogue As the code 
runs, DepthFirstSearch() is called multiple times. The call is made  repeatedly until it returns a 
value!"  "Because the previous node returned a value,  the Thought can proceed."  "The Thought has 
received values for two leaves. Now, it can make its way back by passing th ese r etur ned values to 
the previous calls!"   In level 3, students traverse the tree once again. However, this time; we added 
a stack and telephone metaphor to the game. Cera explains to the student how recursive calls to DFS are 
pushed onto the stack when the student visits that node and are popped off when the student returns from 
the node. The stack is shown visually as in Figure 5.   Figure 5: Visualization of a DFS stack showing 
each node Table 3 shows sample dialogue from the telephone metaphor we use in the game. We use the idea 
that the students are calling one another for assistance with their code and each student has to call 
another student until someone returns with an answer. Then, each student returns answers to the person 
who first called them. Table 3: Level 3 telephone call metaphor sample "Think of recursion like this: 
You need help with your programming homework so you call  Bobby (B), but Bobby is confused too."  
"Bobby calls up his friend Eric (E) to see if he can help out, but Eric can t. Eric calls  Jason (J) 
to ask him for help."  "Now, Jason returns a response to Eric and the call to Jason pops off."  "Jason 
is out of the picture now, but Eric likes to be sure of things so he makes another call." "Eric calls 
Katie. Katie s answer returns to Eric and her call pops."  "Now Eric has no calls left to make and 
can return Bobby's call with a response."  "Now Bobby's call is completed and he can return a value 
to you.  "So now, your calls have returned a value to you and have popped off. You can continue your 
  calls by calling Cody(C)."    5. METHOD We performed an exploratory evaluation of the prototype 
with 43 students who were enrolled in or had already completed Data Structures and Algorithms between 
October 2008-February 2009. The purpose of the study was to gather formative feedback and determine the 
impact and feasibility of using such games for homework. Participants signed an informed consent form, 
took a demographic survey and a pre-test of recursion-related computing concepts, played about 40 minutes 
of the game, took a post-test, and took a survey about their experience. The demographic survey includes 
information on the participant s race, gender, year in school, major, and gaming habits. The post-test 
is similar to the pre-test, with the numbers and variable names changed in each question (so they are 
isomorphs but not identical). Each session took about one hour. The post-surveys were used to gather 
what testers thought of using games like ours as homework, which quests they preferred, and how balanced 
the game was in play to coding time. Our primary outcome measurements were the quantitative learning 
gains and the qualitative responses about whether 1) the games were enjoyable, 2) subjects felt they 
could learn with them, and 3) subjects would prefer to learn using a game such as these. Gameplay logs 
were analyzed for the time spent on each game level, as well as the number of correct and incorrect attempts 
at the code challenges. We planned to use demographic and pre­test/post-test results to categorize responses 
to see if different groups responded differently to the game.  6. RESULTS &#38; DISCUSSION The study 
was conducted with 43 participants. The first twenty­seven took only a post-test, while the final 16 
participants took both the pre-test and post-test. In this discussion we focus on the pre-and post-test 
group. Of these 16, 13 were 18-25 years old and 3 were 25-30. Thirteen were male and three were female. 
Two of the students were of Asian descent, 2 were African American, 2 were Hispanic, 9 were Caucasian, 
and one preferred not to respond. We had 1 freshman, 1 sophomore, 10 juniors, 3 seniors, and 1 post Baccalaureate 
student. Fourteen were in computer science-related majors, 1 was in computer engineering, and 1 was in 
graphic design with a minor in computer science. The students were also asked what programming languages 
that they had used previously. As Java and C++ are taught in our first series of programming courses 
at UNCC, it comes as no surprise that these were the dominant languages among the students. We also asked 
the students what types of games they like to play when they have the time. Interestingly enough for 
our project, 8 of the students like first person shooters, which is what the DarkWynter game engine was 
originally, and 8 of the students like RPGs, which is what the engine has become. Three of the students 
never play video games while most play less than 3 hours a week, 1 plays 3-10 hours a week, 2 play 11-20 
hours a week and 2 play more than 20 hours a week. Four of the students reported being hardcore gamers, 
5 reported being casual gamers, and the rest said they were neither. Because of the small sample size, 
we did not perform comparisons among the subgroups; however, as we add participants, we will look at 
the subgroups to determine differences, if there are any. 6.1 Quantitative Results We conducted a pre-to 
posttest comparison for students taking both tests (N=16), and the average scores for each question on 
these tests is shown in Table 2. We note that all scores increased except for Q3. We suspect this may 
be a result of students assuming the pre-and posttest questions were identical. We can mitigate the answer 
the same multiple choice answer by randomizing the placement of the answers by student. Posttest scores 
are significantly higher (M = 1.9375, SD = 1.12361) than the pretest scores (M =3.1250, SD = 1.82117; 
t(15) = -3.048, p < .05). With a p value of approximately 0.008, we can show there is a significant improvement 
in the posttest scores compared to the pretest scores. The effect size, using Cohen s d, is .78, which 
is a large effect size, with an a error rate of .05 and a power of .95. On average, the posttest scores 
were approximately .78 standard deviations higher than the pretest. Table 2: Pre and posttest results 
(N= 16) Q1 Q2 Q3 Q4 Q5 Total % Pretest 0 .5 .63 .38 .44 1.94 of 5 38% Posttest .63 .63 .56 .5 .81 3.13 
of 5 63% Figure 5 shows the pretest and posttest results, with participants, ordered by increasing post-test 
scores, on the x axis and scores (out of 5 points) on the y. Each participant has two columns in the 
chart the light blue column representing their pretest score and the purple representing their posttest 
score.  Figure 4: Pre and Posttest Scores per Participant Since some students did not take the pretest, 
our study became an abbreviated Solomon s research design that controls for test effects that cause learning 
from a pretest to the posttest. This 4­group design contains a treatment and control group with both 
pretests and posttests and has treatment and control groups with posttests only. In our case, we have 
two treatment groups, Group P with a pretest and posttest and Group NP with just a posttest (N=27). Since 
our purpose is learning, we have omitted both groups where there is no treatment. We ran an independent 
samples t-test across the two groups, the ones that took the pre and posttests (P) and the no-pretest 
(NP) group. The means and standard deviations of the two groups were very close to one another, P (M 
= 3.23, SD = 1.83275) and NP (M = 3.25, SD = 1.45213), indicating that there was probably not an interaction 
for the P group stemming from taking the pretest before the posttest. Table 3 shows the averages and 
standard deviations over the game logs for each student in group P, which include the overall time spent 
(minutes: seconds) on the game, as well as the times and the number of tries students took on each challenge. 
On average, students spent 75% of their game time actually working on code challenges. There were no 
significant correlations between time spent in the game and posttest scores. For 6 of the 11 students 
with valid logs, their time to solve DFS 2 decreased as compared to that for DFS 1, and for 2 students 
the time was very similar. While the number of tries to complete DFS2 went up, we believe the decrease 
in time indicates some learning, since DFS 2 was more challenging. In the future, we plan to do a more 
detailed analysis of learning times and tries for all study participants. Table 3: Game Log Averages 
Challenge Average Standard Dev. Game Time 32:28 15:02 Hello World 6:57 4:53 HW Tries 1.23 0.44 DFS1 Time 
10:02 4:10 DFS1 Tries 2.46 2.26 DFS2 Time 10:32 10:41 DFS2 Tries 4.00 24.68 Figure 6 shows the results 
of our analysis of the 11 complete log files for a correlation between time spent playing the game and 
code attempts with their test scores. The x-axis lists the total time time for each student, with three 
bars for each student whose heights represent their pre-(blue) and posttest (purple) scores and number 
of attempts (yellow). There was not a significant interaction between time and attempts. However, there 
is a trend that students who did well on the posttest did not spend as much time in the game or have 
as many attempts as their peers.  Figure 5: Time in Game and Attempts vs. Test Scores; Participants 
Ordered by Time Spent   6.2 Survey Results While we only have valid pre-and posttests for 16 students, 
42 students gave us survey responses after playing the game. We have averaged the results from the survey 
questions, where one equals strongly disagree and five equals strongly agree. Out of the 42 students, 
34 strongly agreed or agreed that they enjoyed playing the game. Thirty-five respondents agreed that 
they enjoyed creating and compiling their own code inside the game (88% response). Thirty-four of the 
students agreed the game was helpful in learning computer science concepts and 32 students thought the 
game has a good balance between play and quest time. Thirty-four students agreed that the second level 
with the AI walkthrough was more helpful in learning DFS than the other two levels. Finally, 32 students 
disagreed that they miscoded on purpose to see what would happen and 29 students disagreed that they 
guessed at the solutions. Table 4 shows student responses to the question What aspects did you like best 
about the game? As the 42 students were not limited to a single categorical response, they could list 
more than one answer in this section, and we categorized them into 67 distinct responses. One student 
responded, I liked the whole experience of walking through the paths and then typing the code and then 
doing it again...I also liked the example as the tree is traversed, which we categorized as in game coding, 
gameplay, and visualization. Another student stated, I like the fact that I write my own code to make 
the game work. While the gameplay was rated the lowest, we believe that is because the students were 
actually taken with the novelty of in-game coding and the visualization aspects of the game. As for the 
educational aspects, one of the students said, It is a great new way to teach computer science concepts 
to new students. Table 4: Favorite aspects of EleMental Response Categories By respondents By categories 
In-Game Coding 29% (12 of 42) 18% (12 of 67) Visualization 62% (26 of 42) 39% (26 of 67) Education 38% 
(16 of 42) 24% (16 of 67) Hints 19% (8 of 42) 12% (8 of 67) Game Play 11% (5 of 42) 7% (5 of 67) We 
categorized 42 responses to the item What improvements would you suggest for EleMental? as shown in Table 
5. Several students suggested we include player orientation in the mini map for navigation, improve the 
instruction language, and to fix bugs, such as those that allow students fall off the bridges. The three 
students who suggested we add audio (though it is included) did not use headphones. Two students asked 
for more levels with harder code quests and more game play. Table 5: Suggested game improvements Response 
Categories By respondents By categories Add Audio 7% (3 of 42) 6% (3 of 54) Improve Mini Map 26% (13 
of 42) 24% (13 of 54) More levels &#38; challenges 10% (5 of 42) 9% (5 of 54) Improve Instructions 26% 
(13 of 42) 24% (13 of 54) Improve Visualizations 9% (4 of 42) 8% (4 of 54) Improve Engine 33% (16 of 
42) 29% (16 of 54) Finally, we asked the students what they thought about game assignments versus traditional 
coding assignments. Out of the 42 responses, 74%, stated that they would rather play games like this 
one than code a traditional assignment, while 19% prefered traditional assignments and the remaining 
students were neutral. One student wrote, I believe game assignments are better than traditional ones 
because [they] make you think more outside the box. Another student expressed Game assignments are better 
because a lot of people enter computer science wanting to do things like gaming, and are disappointed 
because all they get to do in the first few assignments is write code to calculate tax on different stuff. 
Finally, one of our students said, Game assignments are better suited for my learning style most traditional 
assignments have felt kind of useless because they really don t [sic] do anything besides run At best 
a traditional assignment gets you to match your output to the teacher...In this game, at least the code 
did something visual.  7. CONCLUSIONS &#38; FUTURE WORK Creating a game to teach students recursion 
-a rather notorious subject in the computer science field -is a challenge. We had the standard team communication 
issues, but since our student game programmers included novices, we also had to ensure everyone learned 
recursion enough to design and code the game. Comparing this experience to our prior efforts in game 
creation, we found it much easier to design and create a game for programming with our own game engine 
on the XNA platform than with a commercial game engine such as Unreal Engine. One of the goals of the 
Game2Learn project is to enhance computer science education of the student-developers of the games, and 
this game enriched several areas. In order to ensure players wrote the expected code, student developers 
had to learn how .Net compiler works and how to parse code. The process improved our knowledge of recursion, 
compilers, abstraction, polymorphism, and designing complex architectures to handle game interactions. 
By using the agile cycle of development, we learned how to iterate rapidly through the software cycle 
efficiently and with good communication. Perhaps most importantly, student developers learned good code 
is no substitute for good design. Our study results show students achieved statistically significant 
learning gains when they played EleMental: The Recurrence. Although the sample size was smaller than 
we would have liked, the differences in the scores were large enough to show both statistical significance 
and a large effect size, suggesting that EleMental: The Recurrence implements effective teaching strategies 
in a game environment. With the good use of visualization, frequent feedback, and an in-game compiler 
that students can write their own code to work in our game, this game demonstrates many characteristics 
we feel are important for educational games for computing. Most of the students were enthusiastic about 
learning with this game and the possibility of using more such games in learning complex computing topics. 
We plan to use the results of this study to engage computer science professors in using our student-created 
and scientifically validated learning games in their courses. Based on our findings in this study, we 
plan several improvements for EleMental. To better demonstrate the task of depth-first search and the 
advantages of using recursion, we plan to replace level 1 ( Hello world ) with a small brute-force program 
to perform tree traversal. In all levels, we will also include a visualization of how the student s programmed 
traversal would work, even when it is incorrect. We may also remove the first­person perspective for 
walking through the binary tree, since a top-down view seems to be most beneficial. Based on student 
feedback, we will add a better indicator on the mini-map to show both position and orientation for the 
player, and integrate the experience points (XP) better with the game to provide more motivation. We 
designed the stack and telephone metaphor to work together to help students connect local and global 
behavior for the recursive DFS. However, based on feedback from our data structure and algorithms professor, 
this area is most difficult for students and could use more elaboration in the game. In our future studies, 
we also plan to integrate the game as a standard assignment in our algorithms and data structures class, 
to ensure better study samples and provide all students with the benefit of an alternative form of learning. 
 8. ACKNOWLEDGMENTS This work was partially supported by the National Science Foundation Grants No. 
CNS-0552631 and CNS-0540523, IIS­0757521, and the UNC Charlotte Diversity in Information Technology Institute. 
 9. REFERENCES ACM/IEEE-CS JOINT CURRICULUM TASK FORCE. Computing Curricula 2001. Accessed February 
13, 2008. http://www.acm.org/education/curric_vols/cc2001.pdf ASPRAY, W., AND BERNAT, A. Recruitment 
and retention of underrepresented minority graduate students in computer science: Report of a workshop, 
March 4-5, 2000. Washington, DC: Computing Research Association. BARNES, T., E. POWELL, A. CHAFFIN, H. 
LIPFORD. Game2Learn: Improving the engagement and motivation of CS1 students. ACM GDCSE 2008. BARNES, 
T., H. RICHTER, E. POWELL, A. CHAFFIN, A. GODWIN. (2007). Game2Learn: Building CS1 learning games for 
retention, Proc. ACM Conference on Innovation and Technology in Computer Science Education (ITiCSE2007), 
Dundee, Scotland, June 25-27, 2007. BAYLISS, J., AND S. STROUT. Games as a "flavor" of CS1. In SIGCSE 
2006. ACM Press, New York, NY, 500-504. BEAUBOUEF, T. AND J. MASON. Why the high attrition rate for computer 
science students: some thoughts and observations. SIGCSE Bull. 37, 2 (Jun. 2005), 103-106. BECKER, K. 
2001. Teaching with games: The Minesweeper and Asteroids experience. The Journal of Computing in Small 
Colleges Vol. 17, No. 2, 2001, 22-32. BIOWARE CORPORATION. Aurora Neverwinter Toolset. Accessed Jan. 
2, 2008. http://nwn.bioware.com/builders/ DARKWYNTER STUDIO. The DarkWynter 3D Game Engine. Accessed 
March 27, 2008. http://darkwynter.com/ EAGLE, M., T. BARNES. Experimental evaluation of an educational 
game for improved learning in introductory computing. ACM SIGCSE 2009, Chattanooga, TN, March 3-8, 2009. 
ENTERBRAIN CORPORATION. RPGMaker XP. Accessed Jan. 2, 2008. http://www.enterbrain.co.jp/tkool/RPG_XP/eng/ 
GARRIS, AHLERS, AND DRISKELL. Games, motivation, and learning: a research and practice model. Simulation 
and Gaming, Vol. 33, No. 4, 2002, 441-467. GEE, J. P. What video games have to teach us about learning 
and literacy. Compute Entertain of Educational Technology. 30, 4 (1999), 311-321. JENKINS, HENRY, ERIC 
KLOPFER, KURT SQUIRE, AND PHILIP TAN. Entering the Education Arcade. Source Computers in Entertainment, 
Volume 1, Issue 1 (Oct.. (October 2003), 20. LEPPER, M. R., AND MALONE, TH. W. 1987. Intrinsic motivation 
and instructional effectiveness in computer-based education. In R. E. Snow and M. J. Farr (Eds.), Aptitude, 
learning, and instruction: Vol. 3. Conative and affective process analyses (pp. 255-286). Hillsdale, 
NJ: Lawrence Erlbaum. LOSH, ELIZABETH. In Country with Tactical Iraqi: Trust, Identity, and Language 
Learning in a Military Video Game. Virtualpolitik. 2006. August 3, 2006. http://virtualpolitik.org/DAC2005.pdf 
MALONEY, J. H., K PEPPLER., Y KAFAI, M. RESNICK, N. RUSK, Programming by choice: urban youth learning 
programming with scratch, in Proc. of the 39th SIGCSE Technical Symposium on Computer Science Education, 
pp 267--371, Portland, OR, 2008. MICROSOFT CORPORATION. XNA Game Studio Express. Accessed March 27, 2008. 
http://xna.com MULLINS, P, WHITFIELD, D, AND CONLON, M. Using Alice 2.0 as a First Language. To appear 
in CCSC 24th Annual Eastern Conference 2008 (Hood College, Frederick, Maryland, October 10 and 11, 2008). 
 PARBERRY, IAN, MAX B. KAZEMZADEH, TIMOTHY RODEN. The art and science of game programming, Proc. 37th 
SIGCSE Technical Symposium on Computer science education, March 03-05, 2006, Houston, Texas, USA. BIERRE 
, KEVIN J. AND ANDREW M. PHELPS. The use of MUPPETS in an introductory java programming course, SIGITE 
2004, October 28-30, 2004, Salt Lake City, UT, USA. POLACK-WAHL, J. AND ANEWALT, K. Undergraduate research: 
Learning strategies and undergraduate research. SIGCSE 2006: 209 213. PRENSKY, M. Digital Game-Based 
Learning, New York, McGraw Hill, 2001. SHAFFER, DAVID, KURT R. SQUIRE, RICHARD HALVERSON, AND JAMES P. 
GEE. Video Games and the Future of Learning. Academic Advanced Distributed Learning Co-Lab. December 
10, 2004. August 3, 2006. http://www.academiccolab.org/ resources/gappspaper1.pdf VESGO, J. Continued 
Drop in CS Bachelor's Degree Production and Enrollments as the Number of New Majors Stabilizes. Computing 
Research News, Vol. 19, No. 2., March 2007. ZWEBEN, S. 2006-2007 Taulbee Survey. Computing Research News, 
20, 3 (May 2008).  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>National Science Foundation</funding_agency>
			<grant_numbers>
				<grant_number>CNS-0552631CNS-0540523IIS-0757521</grant_number>
			</grant_numbers>
		</article_sponsors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1581087</article_id>
		<sort_key>140</sort_key>
		<display_label>Pages</display_label>
		<article_publication_date>08-04-2009</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Cardboard semiotics]]></title>
		<subtitle><![CDATA[reconfigurable symbols as a means for narrative prototyping in game design]]></subtitle>
		<page_from>87</page_from>
		<page_to>93</page_to>
		<doi_number>10.1145/1581073.1581087</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1581087</url>
		<abstract>
			<par><![CDATA[<p>In this paper, we propose the technique of cardboard semiotics. We explain the importance of symbolic analysis as a tool for building narrative prototypes in videogames. Borrowing from the participatory design work in the early 1990s, we suggest a means for adapting and extending this work based on the implicit participation of gamers' immediate-level stories (i.e., the gameplay with narrative implications). Our paper first introduces the concept of semiotics and explains how cardboard semiotics can function as an applied technique within the domain of videogame design and development. Next, we propose a theoretical basis for our work using a simple three act narrative structure and explore some basic concepts from narrative game design. Finally, we conclude with some simple examples of how cardboard semiotics might function in a design environment.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>K.6.1</cat_node>
				<descriptor>Systems analysis and design</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.6.1</cat_node>
				<descriptor>Systems development</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor>Modeling methodologies</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003457.10003490.10003491.10003495</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Management of computing and information systems->Project and people management->Systems analysis and design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003490.10003491.10003496</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Management of computing and information systems->Project and people management->Systems development</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010343</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Modeling methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1570332</person_id>
				<author_profile_id><![CDATA[81384590786]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rudy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McDaniel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570333</person_id>
				<author_profile_id><![CDATA[81309497358]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Erik]]></first_name>
				<middle_name><![CDATA[Henry]]></middle_name>
				<last_name><![CDATA[Vick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rochester Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570334</person_id>
				<author_profile_id><![CDATA[81539631256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jacobs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rochester Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570335</person_id>
				<author_profile_id><![CDATA[81440622695]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Telep]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1208681</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bateman, C. (Ed.). 2007. <i>Game Writing: Narrative Skills for Videogames</i>. Boston: Charles River Media.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1306829</ref_obj_id>
				<ref_obj_pid>1306813</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Buchanan, J. W, Augustin, M Chuang, G Delgado, A, Ortega, A, and Seaver, J. 2008. Game Sketching. <i>Proceedings of the 2&#60;sup&#62;nd&#60;/sup&#62; international conference on Digital Interactive Media in Entertainment and Arts</i>. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Campbell, J. 1949. <i>The Hero with 1,000 faces</i>. Boston, MA. Pantheon Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Chandler, D. 2007. <i>Semiotics: The Basics</i>. New York: Routledge.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Crawford, C. 2005. <i>On Interactive Storytelling</i>. Berkeley, CA: New Riders Games.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Demarle, M. 2007. Nonlinear Game Narrative. In C. Bateman (Ed.), <i>Game Writing: Narrative Skills for Videogames</i>, Boston: Charles River Media, 71--84.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Eco, U. 1976. <i>A Theory of Semiotics</i>. Bloomington: Indiana University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Ehn, P., and Kyng, M. 2003. Cardboard computers: mocking-it-up or hands-on the future. In N. Wardrip-Fruin and N.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Montfort (Eds.), <i>The New Media Reader</i>, Cambridge: The MIT Press, 651--662.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Frasca, G. 2003. Ludologists love stories, too: notes from a debate that never took place. <i>Levelup 2003 Conference</i>, from www.ludology.org/articles/Frasca_LevelUp2003.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Freytag, G. 1900. Technique of the Drama: An Exposition of Dramatic Composition and Art (E. J. MacEwan, Trans. 3rd ed.). Chicago: Scott, Foresman and Company.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Fullerton, T. 2008. <i>Game Design Workshop: Second Edition: A Playcentric Approach to Creating Innovative Games</i>. Burlington, MA: Morgan Kaufmann.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Gordon, A. S., and Iuppa, N. V. 2003. Experience Management Using Storyline Adaptation Strategies. <i>Proceedings of the 1&#60;sup&#62;st&#60;/sup&#62; International Conference on Technologies for Digital Storytelling and Entertainment</i>, Darmstadt, Germany.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Ince, S. 2006. <i>Writing for videogames</i>. London: A&amp;C Black.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Jacobs, S. 2007. <i>The Basics of Narrative</i>. In C. Bateman (Ed.), <i>Game Writing: Narrative Skills for Videogames</i>, Boston: Charles River Media, 25--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1207478</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Koster, R. 2004. <i>A Theory of Fun for Game Design</i>. Phoenix, AZ: Paraglyph.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>317285</ref_obj_id>
				<ref_obj_pid>317279</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Myers, D. 1999. Simulation as Play: A Semiotic Analysis. <i>Simulation &amp; Gaming</i>, 30, 2, 147--162.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Reich, R. 1991. <i>The Work of Nations</i>. New York: Alfred A. Knopf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1209229</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Rollings, A. &amp; Morris, D. 2004. <i>Game Architecture and Design: A New Edition</i>. Berkeley, CA: New Riders.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>975180</ref_obj_id>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Rouse III, R. 2005. <i>Game Design Theory and Practice</i> (2nd ed.). Plano, TX: Wordware Publishing.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1215723</ref_obj_id>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Salen, K. and Zimmerman, E. 2004. <i>Rules of play: Game design fundamentals</i>, Cambridge, MA: The MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>975461</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Sheldon, L. 2004. <i>Character Development and Storytelling for Games</i>. Boston: Thomson Course Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>995045</ref_obj_id>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Wardrip-Fruin, N., and Harrigan, P. (Eds.). 2004. <i>First Person: New Media as Story, Performance, and Game</i>. Cambridge: The MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Cardboard Semiotics: Reconfigurable Symbols as a Means for Narrative Prototyping in Game Design Rudy 
McDaniel Erik Henry Vick Peter Telep University of Central Florida Stephen Jacobs University of Central 
Florida Rochester Institute of Technology Abstract In this paper, we propose the technique of cardboard 
semiotics. We explain the importance of symbolic analysis as a tool for building narrative prototypes 
in videogames. Borrowing from the participatory design work in the early 1990s, we suggest a means for 
adapting and extending this work based on the implicit participation of gamers immediate-level stories 
(i.e., the gameplay with narrative implications). Our paper first introduces the concept of semiotics 
and explains how cardboard semiotics can function as an applied technique within the domain of videogame 
design and development. Next, we propose a theoretical basis for our work using a simple three act narrative 
structure and explore some basic concepts from narrative game design. Finally, we conclude with some 
simple examples of how cardboard semiotics might function in a design environment. 1. Introduction In 
this paper, we examine semiotics as a tool for the rapid conceptual prototyping of narrative elements 
and gameplay in games. We call this concept cardboard semiotics because it involves the construction 
of a general framework with generic symbolic units that can easily be knocked down and built back up 
with new content as necessary. We use cardboard both as a metaphor for the process and as a nod to the 
participatory user­centered design processes followed with computer systems design in the early 1990s. 
For the sake of focus, we consider in this paper the types of games that are strongly narrative (e.g., 
RPGs and some FPS games) with detailed plots and characters that change over time, though we also maintain 
that these techniques are useful for other types of games as well. Following this general idea of cardboard 
semiotics, cardboard units are filled in with interchangeable narrative and gameplay scenarios that meet 
the needs of particular symbolic functions. To Copyright &#38;#169; 2009 by the Association for Computing 
Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org. 
Sandbox 2009, New Orleans, Louisiana, August 4 6, 2009. &#38;#169; 2009 ACM 978-1-60558-514-7/09/0008 
$10.00 explain how this process might work, we use this paper to outline some basic semiotic concepts 
and consider how these methods can be useful for improving the stories found in modern games. We suggest 
that although innovations in gameplay and game mechanics continue to push the cutting edge in game development, 
interactive narrative has not yet caught up, nor has the smooth integration of the two developed as a 
regular feature of games. This is in part due to the massive complications involved in trying to address 
what DeMarle [2007] describes as the immediate-level story, or the story that is crafted when the player 
engages with a game (as opposed to the high-level dramatic story outlined by the game s authors). This 
is of course somewhat of a holy grail challenge, since computers are nowhere near a human s ability to 
craft interesting real-time stories, though such stories can certainly emerge when humans play games 
together online. As Rouse III [2005] notes, a designer that truly cares about interactive storytelling 
must accept the fact that authorial control must be relinquished. This must occur in order to make the 
game flexible enough to tell the player s story as well as the designer s story. We can parse work from 
other disciplines for some insight into this process. For example, narratology, or the science of story, 
has been recently applied to both games and interactive media in several scholarly works [Bateman 2007; 
Gordon and Iuppa 2003; Ince 2006; Wardrip-Fruin and Harrigan 2004; Sheldon 2004]. However, with the recent 
distractions in the field of interactive storytelling stemming from issues like the ludology vs. narratology 
debate [see Frasca 2003], it is no wonder that little work has been done in improving the quality of 
story and its integration into play in even AAA-quality commercial games. For example, Fallout 3, IGN 
s 2008 Xbox 360 Game of the Year, has a relatively bland high-level story and numerous problems in its 
immediate-level stories. One small example is the fact that NPC characters show only limited environmental 
awareness and do not react to events in the game that should be meaningful to them from a dramatic perspective. 
In Fallout 3 s gameworld, an organization of NPCs known as the Brotherhood of Steel professes a hatred 
for super mutants, and yet when the hero just happens to show up with a super mutant as a member of her 
quest party, then the NPCs have no reaction to this development. This gameplay situation has dramatic 
implications, but they remain unexplored. Unfortunately, these types of issues are not yet resolved in 
many contemporary games. In order to present game developers with additional, rapid choices for dealing 
with high-level storytelling, we modify the cardboard computing model of Ehn and Kyng [2003] to accommodate 
a mechanism for interchangeable storytelling and gameplay using semiotics. In essence, this involves 
the construction of a theoretical model in which narrative symbols are used as interchangeable placeholders 
for dramatic moments in a given gameworld. Following Crawford [2005], this addresses the issue that the 
fundamental quest in interactive storytelling is not the development of a central storyline, but rather 
the creation of a suitable storyworld in which dramatic action can unfold. This world can then be given 
entirely different dramatic possibilities when a set of flexible narrative symbols is chosen and these 
symbols are manipulated when devising a suitable testbed for the game. To clarify our model for cardboard 
semiotics, we will first review the concept of cardboard prototyping and some basic terminology from 
the field of semiotics and describe some ideas from this body of work. Next, we will construct a model 
for integrating semiotic placeholders as tools for preliminary storyboards that can later be fully developed 
into traditional storyboard sequences. This constitutes a move from the abstract domain of the narrative 
concept to the specific domain of a narrative symbol used to express that concept in the game. It also 
fits with existing methods of game design and physical prototyping (e.g., evolutionary prototypes, testbeds, 
and iterative design models see Rollings and Morris [2004]) by allowing for a testbed of preliminary 
ideas which may or may not be incorporated into the final game. Finally, we will apply this model to 
an example in two different configurations in the final section of this paper.  2. Theoretical Foundations 
Cardboard computing is an approach devised by Ehn and Kyng [2003] in which designers and users co-create 
interfaces through the use of reconfigurable cardboard mock-ups, sometimes augmented with additional, 
more primitive technologies (such as slideshow projectors). They claim four reasons for advertising 
such a methodology, noting that the mockups are cheap, understandable (with no confusion between the 
mock-up and the real-thing ), fun to work with, and affording of hands-on, user­driven experience. As 
interactive gaming is by definition user co­created, we maintain that the same process can be beneficial 
to the creation of better stories in games. We extend this idea by using semiotics as a mechanism for 
generating narrative units in games. As Koster [2005] reminds us, games use puzzles as a significant 
part of their design, and much of the fun of gaming stems from us as pattern recognizers trying to find 
out how to assemble these puzzles into meaningful sequences to solve larger puzzles and/or assemble a 
coherent narrative. In much the same way, we can use semiotics as a means to reverse engineer dramatic 
puzzles in order to find the most compelling narrative patterns in order to guide the player through 
the process of putting a particular dramatic occurrence back together. Videogame designers often use 
variations of the cardboard computing methodology to test mechanics and formal elements of the games 
they are developing before opening the editor to start the coding process. Fullerton devotes a full chapter 
to Physical Prototyping in her text Game Design Workshop: Second Edition [2008]. In addition, Buchanan 
has been researching and promoting a hybrid of cardboard computing in a digital sandbox that he calls 
Game Sketching [Buchanan et al. 2007]. These methods all speak to the need to develop gameplay and narrative 
elements well in advance of pursuing the actual digital development of the game itself. No matter what 
the applied methodology of narrative and gameplay design, the authors believe that the use of semiotics 
can greatly inform the process. In terms of the content used in our cardboard mockups, we suggest using 
semiotics as a generative mapping device for narrative possibilities. Simply put, semiotics is the study 
of signs and symbols. Chandler [2007] explains that signs take the form of words, images, sounds, odours, 
flavours, acts, or objects, but such things have no intrinsic meaning and become signs only when we invest 
them with meaning (13). Because of the breadth of potential choices for symbols and the apparent arbitrariness 
of their meaning-making relationships with real world concepts, much literature on the subject is riddled 
with ambiguity, enigma, and argument. Different approaches to semiotics abound, as do differing formulations 
of how exactly the relationship unfolds between an author, her chosen symbol, the concept that symbol 
represents, and the way that symbol is interpreted by an audience. Well known semioticians include Ferdinand 
de Saussure, Charles Peirce, Roman Jakobson, Louis Hjelmslev, and Umberto Eco. In his book A Theory of 
Semiotics [1976], Eco maintains that the study of semiotics concerns anything that can be taken as a 
sign (7) that it is the discipline studying everything which can be used to tell a lie (7) and that the 
objects we see and recognize as ordinary forms are only the surface appearances of the more complex networks 
of underlying elementary units. Since much of gaming often involves a willing suspension of disbelief 
on the part of the player, Eco s conceptualization of semiotics as a tool for telling lies is particularly 
compelling as an argument to persuade game creators of its validity. While interesting, much of this 
history and theoretical posturing seems somewhat abstract and detached from the practical and applied 
techniques of videogame development. We can, however, see some use in adapting the basic terminology 
and concepts from this field to our present consideration of better narrative development in games. To 
begin with, we can consider the difference between a game concept and the symbol or symbolic grouping 
of content used to communicate that concept. An understanding of semiotic analysis is useful for that 
task. From a very general perspective, semiotics as a field is concerned with distinctions between a 
symbolic signifier and the idea or concept to which that signifier refers, which is known as the signified. 
Under this construction, it is clear that the relationship between the signifier and the signified is 
somewhat tenuous (as Eco [1976] warns earlier in this paper), and that various people will use arbitrarily 
defined signifiers to refer to the same signified concepts. One obvious example of this is language itself, 
which is obviously distinct according to culture, but which still refers in essence to the natural world 
and to humankind s relation to this natural environment and to one another. As such, our ability to understand 
and manipulate the symbolic nature of information has become a marketable job skill in the modern economy. 
Robert Reich s The Work of Nations [1991] is a well-known text that speaks to the necessity of symbolic 
analysts who contribute not to material goods of the economy, but rather to manipulations of symbols 
 data, words, oral and visual representations (177). As professionals working in a multidisciplinary, 
rapid-paced field that is constantly changing and evolving, game designers are usually by necessity skilled 
symbolic analysts. As an established strategy for considering representational media, it is not surprising 
that semiotics has previously been linked to pursuits in game design. Salen and Zimmerman [2004] note 
that games can represent meaning, but also that they are representations of meaning. They represent meaning 
through their symbolic depictions of characters, stories, settings, ideas, and behaviors and are representations 
of meaning when we consider them as representational wholes (364). With this duality there is a potential 
for incongruence. A game may succeed at representing narrative elements when those elements are considered 
independently, but fail when these disparate narrative components are considered in their entirety as 
a single representational unit or overall overarching plot. The fact that games representations happen 
with interaction from the player, and often as a result of these interactions, complicates the semiotic 
process even further. As Salen and Zimmerman explain, these relationships between characters, events, 
and environments are at least as complex as those which occur when a reader immerses herself in a good 
book. An important goal in the pursuit of a semiotic methodology for game designers is to articulate 
some of these symbols that are important for both practitioners and theorists. Fortunately, prior work 
presents some common symbols that are of use to us during story development. For example, on a practical 
level, character archetypes first introduced by Jung and applied to mythological structures by Campbell 
[1949] are examples of semiotic symbols that can be used to define pieces of plot and/or gameplay (Hero, 
Mentor, Threshold Demon, etc.). For theorists, Myers [1999] has performed a similar semiotic analysis 
considering the differences between algorithmic and experiential simulations and comparing theories of 
play to theories of simulation. This very brief overview of cardboard computing and semiotics introduces 
the framework we will use to apply our ideas for modular and interchangeable narrative prototyping for 
games. In the next section, we consider a simple arrangement upon which to place narrative symbols according 
to a particular structure that suits the dramatic needs of a gameworld.  3. A Simple Three Act Semiotic 
Structure Using a symbolic-analytic approach, we can devise a base semiotic structure within which to 
define narrative symbols for games. One simple possibility follows narrative structures outlined by Aristotle, 
Campbell, Field and others and was theorized initially by Freytag [see Jacobs 2007] in which stories 
are broken down into three act structures. Here, a system follows in which Act I introduces the hero 
and takes up approximately 25% of game time for the primary quest. Act II is the bulk of the game at 
50% and constitutes the hero s trials and tribulations as she moves towards the narrative climax in the 
game. In Act III, the tension is resolved through the player s actions and the endgame occurs. This also 
constitutes 25% of the game. These three acts contain the major plot points for our abstract interactive 
story. At this point, we can proceed literally following the work of Ehn and Kyng [2005], such as by 
building a tri-fold poster board capable of displaying interchangeable symbols for each dramatic act. 
This approach may be valuable simply in bringing people together to share ideas about the story and co-create 
the narrative in a shared physical environment. This method for visualizing , analyzing, and making structural 
changes in story is often used in film, especially animated film, in which storyboarded sequences of 
the entire film are used for discussion and illustration of points. Or, in a fashion more likely to be 
feasible in modern development processes, the chart can be deconstructed and broken down into individual 
units for consideration during meetings or via online discussion. What is important here is that the 
story be given the same organic treatment as the gameplay, as is ensured when an iterative design or 
rapid application development process is used during production. If a particular narrative symbol is 
not a powerful factor in the evolution of the plot or does not contribute to a better understanding of 
a particular character or her growth in the game, it should be replaced early on in the testbed, just 
as an incompatible or unpleasant gameplay mechanic would be discarded or refashioned into something useful. 
In some instances, then, the overall backbone or overarching plot for the narrative structure is not 
as important as the narrative placeholders in which semiotics can be applied. For example, the three 
act structure for game narrative is a very rudimentary adaptation of the classical Freytag Pyramid structure 
[Freytag 1900] which is over a decade old and itself stems from Aristotle s centuries-old Poetics. More 
suitable plot structures are found in works such as Ince [2006] in which he outlines various interactive 
story structures such as: linear narratives (plot points in a linear sequence, as we describe in the 
three act example above),  linear story with non-linear gameplay (plot points in a sequence, but multiple 
ways to get there DeMarle [2007] calls this technique gated storytelling ),  branching stories (one 
initial plot point leads to several branching hierarchies of plot points), or  parallel stories (one 
initial plot point branches to two series of plot points, which each proceed linearly). Admittedly, some 
of Ince s categories overlap to an extent, but again, it is not the underlying structures which are important, 
but rather the technique we apply to each narrative plot point. We can perhaps proceed most easily in 
explaining the cardboard semiotics concept by using a simple example. Returning to our base three act 
structure for game design, we can assign placeholder events to each particular point in the structure. 
Let us consider Event 1 to represent the introduction of the hero. Event 2 will represent the hero s 
final and most ambitious battle. Event 3 will represent the hero s final fate as evidenced by the dramatic 
conclusion to the story. Collectively, these three events represent the high-level story for our game. 
We must then define our primary symbols (high-level plot points, playable characters, and environmental 
locations) and then the supporting symbols that connect the immediate-level story to the high-level story. 
We might consider some guiding questions to help generate our primary symbols. Following a semiotic approach, 
then, involves the asking of these questions in relation to the high-level story: What is the dramatic 
purpose of this symbol?  What in-game actions, environmental effects (art, particle effects, and sound 
effects), and NPC behaviors can help strengthen this symbolism as the symbol evolves during gameplay? 
 What connotations exist as the result of this symbol? How can these expectations be strengthened or 
minimized throughout the rest of the game?  Returning to our Fallout 3 example from earlier in the paper, 
we can situate the Super Mutant party member as a narrative symbol. This symbol produces meaning when 
placed in a procedural role in the game. For example, one dramatic purpose of this symbol inside Fallout 
3 s gameplay is to reflect an unexpected use of character in the story, namely that a bestial, mutated 
character can in fact display some remembrance of its humanity and contribute positively to the party. 
The actions and effects that strengthen this symbolism include all of the actions this NPC exhibits that 
contradict what the player initially knows about Super Mutants (that they will destroy all humans on 
site and exhibit only the most primitive urges to hunt and feed). However, the positive gains from this 
positioning are at once lost when other human NPC characters in the game (the Brotherhood of Steel) do 
not react to this unusual circumstance, thus weakening the overall story and the symbolic usefulness 
of this dramatic character. Because of this capacity for trouble when moving a narrative symbol into 
a procedural act, a second level of semiotic analysis is necessary. This involves considering each symbol 
not only in terms of the overarching story, but also in terms of the other narrative symbols which already 
exist. In this case, it would mean recognizing that the Super Mutant symbol needs to be better aligned 
with the Brotherhood of Steel symbols in order to better support the existing narrative atmosphere and 
to remind the player of the unique circumstances in which she is playing. This is just a simple example. 
To adopt a semiotic approach in more detail, we must consider the salient symbols present in the immediate-level 
story. This is the narrative possibility space surrounding each of the high-level narrative plot points. 
These heuristic questions are even more direct: Which symbols should be included in each Act?  What 
do these symbols denote?  What do these symbols connote?  How do these symbols contribute to the high-level 
story?  How do these symbols relate to other symbols?  With these questions in mind as guiding principles 
for a basic high-level story (remember, symbolic mappings can be quickly changed to generate innovations 
in high-level story based on a primary template and some sort of equivalency table), one can set about 
moving from dramatic to ludic space in the immediate-level domain.  4. From Dramatic Structure to Game 
Structure Once we have developed the high level story sufficiently (and in this sense, we are not limited 
to formal narratives) we must move towards designing an experience based around the message the story 
conveys. To do this, we need to map the existing dramatic structure onto a structure that facilitates 
gameplay. Salen and Zimmerman [2004] define gameplay as the navigation of a suite of choices, where each 
choice leads to an action that has a discernable outcome and ultimately leads the play to the next step 
in the critical path of the game. Further, if we examine the play of a game across the dimension of interactivity, 
we can view the game in three distinct ways: 1. As a composition of small, game events couched in larger 
game units 2. As a composition of these large game units 3. As the game as a whole.  These three views 
allow us to take a modular approach to game design and to compose the following game structure: 1. Game 
exposition the introduction of the game as a whole 2. Game problem setup identification of the termination 
conditions of the game s primary plot problem 3. Game Unit description (repeat for all units) a. Game 
unit exposition b. Game unit problem setup   c. Game event (choice) description (repeat for all events 
in the unit) i. Event exposition ii. Provide direction towards solutions iii. Elicit choice / action 
iv. Discernable event outcome d. Discernable outcome for the game unit  4. Discernable outcome for 
the game problem This structure is very similar to the dramatic structure proposed above, and provides 
a roadmap for combining the high-level dramatic elements with immediate-level game events and game objects. 
Using this structure, we can use the high level descriptions of the three acts in the preceding sections 
to describe our overall game and begin to create our explicit structure: 1. Game Exposition introduction 
of the game world, definition of the setting 2. Game problem setup the hero s nemesis acts in such 
a way as to provide cause for the final battle (end of act II) 3. Game Unit 1: Introduction of the Hero 
 4. Game Unit n: Hero s final battle 5. Discernable outcome for the game problem: Hero s final fate. 
 Of course, any real game will likely have more than two game units. Using this cardboard semiotics 
concept allows us to play with various plot points combining them into units, reordering them to create 
different units or alter the flow of the game, elevating any particular plot point to become the defining 
problem of a game unit itself, and so on. In this manner, we can develop a full critical path from a 
simple three act dramatic structure. For example, using the basic narrative from the preceding section, 
a plot point may be defined as finding suitable companions to aid in the quest. Using this as a basis, 
we can replace the entire plot point with boxes (placeholders) that match the game unit structure suggested 
above. In other words, we need a unit exposition box, a unit problem box, some number of game event / 
plot point boxes, and an outcome box: 1. Hero discovers he needs help to complete the quest. 2. Hero 
must find the best companions that match his character and style. 3. various plot points 4. Hero moves 
to the next unit with his companions in tow.  This process is designed to be very simplistic. Keeping 
these high-level box descriptors simple allows us to mix and match with other like-function boxes to 
create new units and twists to the story. For example, using cardboard semiotics, we can replace the 
exposition box above with any of these expositions below: Hero becomes injured and can t complete a task 
 Hero discovers his nemesis has 3 bodyguards  Hero is given multiple tasks of equal importance all with 
a similar time limit.  Similarly, we could replace the unit problem box with any of these: Hero must 
choose the lesser of evils and team up with a villain  Hero must protect three young children, each 
of which has a special ability he does not have.  Hero discovers five teenagers following him and imitating 
his actions.  By mixing and matching, we can try various game units without a significant increase in 
development or design costs. Once we have decided on the final configuration, we can simply refine the 
concepts in each box to near-final versions. For example, using our original set of boxes for this level: 
1. Hero is given a quest by a broken, old beggar. The beggar s body is twisted and his outbursts of ranting 
indicate his mind is twisted as well. The beggar tells the Hero that he was an aspiring hero once, and 
that he had faced a great evil. He goes on to say that he was blinded by pride and entered the battle 
alone. He motions at his broken body and mutters That was the last time I was whole. He screams and 
begins to mutter. Further conversation is impossible. 2. The Hero glances around and sees a few people 
looking his way. Some stand in the shadows, some stand boldly in the light, staring at him openly.  
We can then define various game events that lead the Hero to successfully ally with some and not with 
others. The resolution box ultimately depends on the game events that occur, so we can leave that as 
an exercise for the reader. Our primary task in creating game events to flesh out the levels is somewhat 
different than that of creating plot points in a narrative, given that players must have enough freedom 
to choose their own actions and solutions to problems. However, given our approach to cardboard semiotics 
outlined above, creation of game events simply becomes an act of replacement and refinement. Simply put, 
we may take any particular plot point and deconstruct it into an expository box, a directional box, a 
choice/action box and an outcome box. For example, given the developing story outline for the preceding 
section, a plot point may be that of procuring transportation . We may use cardboard semiotics to replace 
this plot event with the following four boxes: 1. Hero needs to travel a great distance. 2. Hero is 
under time pressure, and has a moral choice to make.  3. Hero may do nothing, steal, or obtain a slow 
form of transportation at great cost. 4. If the Hero does nothing he advances to another plot line, 
closing this line for the present. If the Hero steals, discernable changes to his character status must 
be made. If the Hero takes the honest, but slow form of transport, time pressure must mount, and changes 
to his character must be made.  We can continue down this path, picking symbols and representations 
that further elucidate the event. For example: 1. The Hero trips over a cobblestone and falls. When he 
stands and dusts off his knees, he sees the edge of a map under the stone. He examines the map and sees 
a path leading to his ultimate destination. A time, very near in the future, is scribbled next to the 
destination. 2. The Hero hears a car horn behind him. He also hears the sound of a horse at a slow walk 
somewhere to his left. As he stands there, a passerby glances at him meaningfully and whispers Get a 
move on, Chum. 3. As the player turns around, a man gets out of a car behind him. Leaving the motor 
running and the door open, the man runs towards a horse stable on the side of the road. Near the doorway 
to the stable, a sign reads: Horses for sale! Today only! Ask inside! 4. If the player turns and leaves 
the area, the map crumbles to dust, accompanied by the sound of a mournful bell tolling once. If the 
player jumps into the car and drives off, people on the street turn and watch. Some glare, some avert 
their eyes, and some show a hand sign meant to ward off evil; however, the car moves very quickly and 
he rapidly leaves the people behind. If he chooses to buy a horse, he must negotiate a high sales price, 
but on mounting the horse, passersby smile and wave at him.  In this manner, we can turn simple, generic 
plot points and other dramatic elements into a near-production quality critical path and game description. 
By using cardboard semiotics, we can structure plot points into game events or game units.  5. Conceptual 
Possibilities / Visual Aids The process above is one such way to prototype some possible stories and 
critical paths using cardboard semiotics. We could also consider the problem from a production perspective 
by using particular mapping structures as cognitive aids or visual aids to help the team during brainstorming. 
For example, assuming our goal is to create an innovative new story for a First Person Shooter (FPS), 
we can begin by defining our primary symbols in a visual sequence as a chart or diagram. We might start 
with a basic set of generic symbols: one playable hero character, one master villain, three environmental 
locations, and a basic plot to overthrow the villain. The rest of the story depends on our manipulation 
of these primary symbols and our filling in of generic placeholders with particular instances. We can 
then develop the following hierarchy of symbols. Each environmental symbol is sustained by character 
and object symbols which constitute its overall effect on the player s interpretation of the game story. 
In essence, each of these sub­symbols helps to generate the player s immediate-level story as they visit 
that location. In Figure 1, we have created a saloon symbol as a visual aid which would represent a saloon 
location in our FPS. The saloon environment is populated by eight other symbols NPC symbol relationships 
are designated by normal lines, while object relationships are designated by dotted lines. Consider the 
various permutations afforded by this particular configuration (see Table 1). The symbolic structure 
shown generates an 8x4 grid, each symbol on a new row with three placeholders, and each placeholder populated 
by an additional symbolic reference. Collectively, each sample arrangement below contributes to a different 
narrative outcome in the game. From that initial chart, each configuration is instantiated for n number 
of cases as devised by the game designer. These symbols can then be further examined for dramatic power 
and potential gameplay fun in the same fashion as the example we considered in section 4. Figure 1: 
Saloon Symbol Symbol Instance 1 Instance 2 Instance 3 Bartender Grizzly Clean-Cut Alien Card Player 1 
Shark Mentor Villain Card Player 2 Love Interest Sleeping Alien Bar Patron 1 Sleeping Chatty Absent Bar 
Patron 2 Drunk Furtive Insane Painting Tumbleweed Black Spot No 2 The Scream Jukebox Shot Up Classy High-Tech 
Floor Sawdust Wood Marble Table 1: Symbol Instances From this conceptual model, more elaborate physical 
mockups (perhaps using a tabletop model or roleplay scenario) can be constructed as visual aids that 
will assist the game designer in understanding what the signs are and what they need to look like for 
a given ludic or narrative moment in the gameworld. If a particular sign does not meet the needs of a 
level or dramatic plot point, it can be polished, reimagined, or deleted without requiring a significant 
amount of additional development work.  6. Conclusion In this article, we suggested a theoretical construct 
for a method of game design that is based on using interchangeable narrative symbols. Using a methodology 
we call cardboard semiotics, we explained how a series of guiding questions can be used to consider the 
relationships between immediate-level and high-level storytelling in gameworlds, and further, how those 
relationships can be expanded into functional game descriptions and critical paths. We think this is 
significant because it capitalizes on the importance of the human element in game design human creativity 
(for coherent narrative drama) is thus far unchallenged by computational algorithms. Future work in this 
area will involve the testing of this methodology to game design situations in which students produce 
original games that should break out of existing clichéd narratives and stand on their own as unique 
creative contributions to the field.  References BATEMAN, C. (Ed.). 2007. Game Writing: Narrative Skills 
for Videogames. Boston: Charles River Media. BUCHANAN, J.W, AUGUSTIN, M CHUANG, G DELGADO, A, ORTEGA, 
A, AND SEAVER, J. 2008. Game Sketching. Proceedings of the 2nd international conference on Digital Interactive 
Media in Entertainment and Arts. ACM. CAMPBELL, J. 1949. The Hero with 1,000 faces. Boston, MA. Pantheon 
Press. CHANDLER, D. 2007. Semiotics: The Basics. New York: Routledge. CRAWFORD, C. 2005. On Interactive 
Storytelling. Berkeley, CA: New Riders Games. DEMARLE, M. 2007. Nonlinear Game Narrative. In C. Bateman 
(Ed.), Game Writing: Narrative Skills for Videogames, Boston: Charles River Media, 71-84. ECO, U. 1976. 
A Theory of Semiotics. Bloomington: Indiana University Press. EHN, P., AND KYNG, M. 2003. Cardboard computers: 
mocking-it­up or hands-on the future. In N. Wardrip-Fruin and N. Montfort (Eds.), The New Media Reader, 
Cambridge: The MIT Press, 651-662. FRASCA, G. 2003. Ludologists love stories, too: notes from a debate 
that never took place. Levelup 2003 Conference, from www.ludology.org/articles/Frasca_LevelUp2003.pdf. 
FREYTAG, G. 1900. Technique of the Drama: An Exposition of Dramatic Composition and Art (E. J. MacEwan, 
Trans. 3rd ed.). Chicago: Scott, Foresman and Company. FULLERTON, T. 2008. Game Design Workshop:Second 
Edition: A Playcentric Approach to Creating Innovative Games. Burlington, MA: Morgan Kaufmann. GORDON, 
A. S., AND IUPPA, N. V. 2003. Experience Management Using Storyline Adaptation Strategies. Proceedings 
of the 1st International Conference on Technologies for Digital Storytelling and Entertainment, Darmstadt, 
Germany. INCE, S. 2006. Writing for videogames. London: A&#38;C Black. JACOBS, S. 2007. The Basics of 
Narrative. In C. Bateman (Ed.), Game Writing: Narrative Skills for Videogames, Boston: Charles River 
Media, 25-42. KOSTER, R. 2004. A Theory of Fun for Game Design. Phoenix, AZ: Paraglyph. MYERS, D. 1999. 
Simulation as Play: A Semiotic Analysis. Simulation &#38; Gaming, 30, 2, 147-162. REICH, R. 1991. The 
Work of Nations. New York: Alfred A. Knopf. ROLLINGS, A. &#38; MORRIS, D. 2004. Game Architecture and 
Design: A New Edition. Berkeley, CA: New Riders. ROUSE III, R. 2005. Game Design Theory and Practice 
(2nd ed.). Plano, TX: Wordware Publishing. SALEN, K. AND ZIMMERMAN, E. 2004. Rules of play: Game design 
fundamentals, Cambridge, MA: The MIT Press. SHELDON, L. 2004. Character Development and Storytelling 
for Games. Boston: Thomson Course Technology. WARDRIP-FRUIN, N., AND HARRIGAN, P. (Eds.). 2004. First 
Person: New Media as Story, Performance, and Game. Cambridge: The MIT Press.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1581088</article_id>
		<sort_key>150</sort_key>
		<display_label>Pages</display_label>
		<article_publication_date>08-04-2009</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Game design for social networks]]></title>
		<subtitle><![CDATA[interaction design for playful dispositions]]></subtitle>
		<page_from>95</page_from>
		<page_to>102</page_to>
		<doi_number>10.1145/1581073.1581088</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1581088</url>
		<abstract>
			<par><![CDATA[<p>The author argues that tasks of designing games for online social networks, such as Facebook, can benefit from understanding the project as a practice where techniques and methods of game design are embedded into interaction design and service design tasks. Research into motivations and emotional dispositions of social media use, and analyzing existing popular games in said networks, help in identifying game mechanics that tap into user practices. Through a number of Facebook games as case studies, the author extracts a set of design principles into a design framework where interaction, social, service, and game design meet. The framework aims to support the inherent sociability, spontaneity, narrativity, and playfulness that permeate online social networks.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Facebook]]></kw>
			<kw><![CDATA[Twitter]]></kw>
			<kw><![CDATA[game design]]></kw>
			<kw><![CDATA[interaction design]]></kw>
			<kw><![CDATA[social media]]></kw>
			<kw><![CDATA[social networks]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>K.6.1</cat_node>
				<descriptor>Systems analysis and design</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.3.5</cat_node>
				<descriptor>Web-based services</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.4.3</cat_node>
				<descriptor>Computer-supported collaborative work</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003130.10003131.10003570</concept_id>
				<concept_desc>CCS->Human-centered computing->Collaborative and social computing->Collaborative and social computing theory, concepts and paradigms->Computer supported cooperative work</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003260.10003282</concept_id>
				<concept_desc>CCS->Information systems->World Wide Web->Web applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003490.10003491.10003495</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Management of computing and information systems->Project and people management->Systems analysis and design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003260.10003304</concept_id>
				<concept_desc>CCS->Information systems->World Wide Web->Web services</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003567.10003570</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing and business->Computer supported cooperative work</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1570336</person_id>
				<author_profile_id><![CDATA[81100104941]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[J&#228;rvinen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IT University of Copenhagen, Denmark]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Benkler, Y. 2006. <i>The Wealth of Networks</i>. Yale University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bj&amp;#246;rk, S. &amp; Holopainen, J. 2005. <i>Patterns in Game Design</i>. Charles River Media.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bogost, I. 2004. Asynchronous Multiplay: Futures for Casual Multiplayer Experience. Other Player Conference, IT University of Copenhagen, Denmark.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Brathwaite, B. 2007. Facebook's Parking Wars' Play Dynamics. http://bbrathwaite.wordpress.com/2007/12/27/facebooks-parking-wars-play-dynamics/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Cook, D. 2008. The Princess Rescuing Application. <i>Lost Garden</i>. http://lostgarden.com/2008/10/princess-rescuing-application-slides.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Elster, J. 1999. <i>Strong Feelings. Emotion, Addiction, and Human Behavior</i>. MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Follett, J. 2007. Engaging User Creativity: The Playful Experience' at UXMatters. http://www.uxmatters.com/mt/archives/2007/12/engaging-user-creativity-the-playful-experience.php]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Graham, J. 2006. 'Flickr of idea on a gaming project led to photo website'. Usa Today, Feb 27&#60;sup&#62;th&#60;/sup&#62;, 2006. http://www.usatoday.com/tech/products/2006-02-27-flickr_x.htm]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Hyatt, N. 2008. What's wrong with Facebook games? http://nabeel.typepad.com/brinking/2008/01/whats-wrong-wit.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[J&amp;#228;rvinen, A. 2008. <i>Games without Frontiers. Theories and Methods for Game Studies and Design</i>. Doctoral dissertation for University of Tampere, Finland. http://acta.uta.fi/english/teos.php?id=11046]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Kim, A. J. 2008. Putting the Fun in Functional. Applying game mechanics to Functional Software. http://www.slideshare.net/amyjokim/putting-the-fun-in-functiona?type=powerpoint]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Kollock, P. 1999. The Economies of Online Collaboration: Gifts and Public Goods in Cyberspace. In Smith, M. A. &amp; Kloock, P. (eds.) <i>Communities in Cyberspace</i>. Routledge.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Lantz, F. 2009. 'Play in the Age of Social Software.' Game Developers Conference, March 27, 2009. https://www.cmpevents.com/GD09/a.asp?option=C&amp;V=11 &amp;SessID=937]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>971109</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Laurel, B 2003. (Ed.) <i>Design Research. Methods and Perspectives</i>. MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Mayer, A. 2009a. Ten Reasons that Social Games May Eat the Casual Market. <i>Inside Social Games: Tracking Innovation at the Convergence of Games and Social Platforms</i>. http://www.insidesocialgames.com/2009/02/10/ten-reasons-that-social-games-may-eat-the-casual-market/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Mayer, A. 2009b. 'Successful Social Games Are All About Status' at Inside Social Games: Tracking Innovation at the Convergence of Games and Social Platforms. http://www.insidesocialgames.com/2009/02/24/successful-social-games-are-all-a out-status/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Mihaly, M. 2008. Casual Games on Facebook http://forge.ironrealms.com/2008/01/11/casual-games-on-facebook/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1407577</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Porter, J. 2008. <i>Designing for the Social Web</i>. New Riders.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Porter, J. 2009. Game Mechanics for Interaction Design: An Interview with Amy Jo Kim. http://bokardo.com/archives/game-mechanics-for-interaction-design-an-interview-with-amy-jo-kim/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1457202</ref_obj_id>
				<ref_obj_pid>1457199</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Rao, V. 2008. 'Playful Mood: the Construction of. Facebook as a Third Place.' MindTrek Conference 2008, Tampere, Finland. http://portal.acm.org/citation.cfm?id=1457199.1457202&amp;coll=Portal&amp;dl=GUIDE&amp;CFID=24746181&amp;CFTOKEN=85617762]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Remo, C. 2009. DICE 09: Valve's Newell On 'Using Your Customer Base To Reach New Customers'. <i>Gamasutra</i>. http://www.gamasutra.com/php-bin/news_index.php?story=22378]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1407667</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Saffer, D. 2007. <i>Designing for Interaction: Creating Smart Applications and Clever Devices</i>. New Riders.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1215723</ref_obj_id>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Salen, K &amp; Zimmerman, E. 2004. <i>Rules of Play. Game Design Fundamentals</i>. MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Yee, N. 2001. The Norrathian scrolls: A study of EverQuest (version 2.5) http://www.nickyee.com/eqt/home.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Game Design for Social Networks: Interaction Design for Playful Dispositions Aki Järvinen1 IT University 
of Copenhagen, Denmark / mygamestudies.com ABSTRACT The author argues that tasks of designing games 
for online social networks, such as Facebook, can benefit from understanding the project as a practice 
where techniques and methods of game design are embedded into interaction design and service design tasks. 
Research into motivations and emotional dispositions of social media use, and analyzing existing popular 
games in said networks, help in identifying game mechanics that tap into user practices. Through a number 
of Facebook games as case studies, the author extracts a set of design principles into a design framework 
where interaction, social, service, and game design meet. The framework aims to support the inherent 
sociability, spontaneity, narrativity, and playfulness that permeate online social networks. CR Categories: 
D.2.10 [Software] Design Methodologies Keywords: Game design, interaction design, social media, social 
networks, Facebook, Twitter 1. INTRODUCTION: Social network games as anemerging area of game business 
and development As online social networks, such as Facebook, Myspace, or Twitter, have become increasingly 
popular, an increasing number of games have found their way into these platforms. Facebook applications 
attract millions of users per monthly basis, and game applications frequently reach the top 10 popularity 
lists of the platform [see http://www.appdata.com]. Furthermore, social media experts are claiming that 
social media games are threatening the market of so-called casual games, due to, e.g., their virality, 
accessibility, and scalability [Mayer 2009a]. Furthermore, it can be argued that online applications 
and services incorporate playful, game-like qualities, even if they are not explicitly presented and 
marketed as games. Flickr, the photosharing service, originated from a game concept at a start-up company 
[Graham 2006], and its origins arguably show in the service s design features: e.g., how users are motivated 
to interact and build their reputation among the community. As the paper will demonstrate, Facebook and 
other social media entertainment applications often tread the fine line between trivia, socializing, 
play, and gaming. Facebook has drawn this line in the water by separating the application category Just 
for Fun from the category of Gaming . In terms of design practices, these observations point towards 
a junction where interaction design projects embed game design tasks, and vice versa. 1 E-mail: aki.jarvinen@itu.dk 
Copyright &#38;#169; 2009 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for components of this work owned by others than ACM 
must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from 
Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org. Sandbox 2009, New Orleans, 
Louisiana, August 4 6, 2009. &#38;#169; 2009 ACM 978-1-60558-514-7/09/0008 $10.00 From the vantage point 
of game design, it becomes engulfed by interaction, or service design tasks. In practice this means that 
the context of use, or in this case, play, has to be taken into account in the design in more complex 
ways. The author has worked on both fields, game design and interaction design, and recently on projects 
(such as an online sports fantasy league platform; http://www.liigaporssi.fi, and an MMO targeted at 
children; http://www.peliakatemia.fi) where the two practices and methods of the two have been joined. 
Thus, the research emerges from both theoretical interest, but also from practical experiences. Valentina 
Rao [2008] has studied the playful mood that, e.g., Facebook applications encourage, noting that individual 
use them both for entertainment purposes and socialization tools, and often they are considered as unsatisfactory 
experiences, which, on the other hand, forces developers to reconsider whether they are designing and 
developing games, or something on the borderline of social media and games. The two principal research 
questions are: First, how can interaction design inform game design practices in the context of designing 
games for social networks? Second: How can such observations and findings, based on an understanding 
of user motivations, be formalized into design principles that would solve design problems and inspire 
new design solutions in this particular design space? The paper explores the overlapping design spaces 
by identifying prominent game design principles in popular Facebook games. These principles outline high-level 
design solutions that evidently tap into the user motivations and their practices in present online social 
networks, as their popularity suggests. In addition, how such principles are embedded into the designs 
of other online applications, is discussed. The paper uses Facebook applications as primary examples, 
but the results are also discussed in the context of other online social platforms, such as Twitter. 
 1.1 Design Research into Games Answering the research questions requires bridging design research [Laurel 
2005] from two areas: designing online applications, i.e. practices of interaction design, and designing 
games, i.e. practices of game design. The perspective presented here is the one of research into designs 
, which means that the research focuses on the practices of interaction and game design practices as 
objects of study, and treats the results of those practices as design objects or design events. In practice, 
this entails deconstructing existing design objects (Facebook games, in this case), and indentifying 
design solutions from them, and abstracting them into principles so that the results would inform future 
design tasks. The method of analyzing the designs takes advantage of methods that the author has developed 
previously for analyzing games as particular systems. [Järvinen 2008] These methods enable us to identify 
formal properties of games as systems, and for the research question of this paper, they are expanded 
into studying games in the context of social networks, and how design solutions contribute to player 
experiences.  1.2 Definitions: Service/Interaction/Social Designmeets Game Design Wikipedia defines 
game design as the process of designing the content and rules of a game. The term is also used to describe 
both the game design embodied in an actual game as well as documentation that describes such a design. 
This paper argues that game design is a subset of interaction design. Therefore we need to define interaction 
design . Dan Saffer gives us a definition that goes as follows: Interaction design is the art of facilitating 
interactions between humans through products and services. Saffer goes on to include to his definition 
interactions between humans and products, which are able to respond to human actions, i.e. devices and 
services with microprocessors. [Saffer 2007] Thus, game design is a subset of interaction design that 
focuses on facilitating interactions of player and games as particular entertainment systems. This seems 
particularly true in terms of game development for social networks. In addition, it is useful to relate 
these fields to so-called service design. Saffer defines it as follows: A service is a chain of activities 
that form a process and have value for the end user. Saffer relates service design to the design of systems 
by stating that in service design projects, the system is the service. He goes on to say that service 
design focuses on context, i.e. the entire system of use . [Saffer 2007] Game design in this context 
is, on one hand, succumbing to the constraints of the social network service, and on the other hand, 
using the service s social functionalities to the benefit of your design. I suggest that there is one 
more area of design, or at least a term, that we need to relate game design to. Joshua Porter has introduced 
the term social design to emphasize the social aspects of particular interaction and service design projects: 
Social design is the conception, planning, and production of web sites and applications that support 
social interaction. [Porter 2008] One could conclude, then, that social design is a subset of service 
design, where social functionalities e.g., communication, sharing are the design drivers. For the practice 
of designing games for social networks, the consequences are: The game design part of the design has 
to be embedded as a subsystem into the larger system of the social media or network service. In practice 
this often means that the developer does not design, nor own, the service itself, but takes advantage 
of the service API. In effect, the API brings along a number of design constraints, but also possibilities. 
In any case, the community context, as with service design, becomes part of the design. 2. Game Mechanics 
for Social Networks The notion of applying game design techniques to the design of online applications 
is gaining prominence. Amy Jo Kim is a designer who has promoted Putting fun into functional approach 
where the design of game mechanics is applied as an interaction design method [Kim 2008]. Jonathan Follet 
[2007] is another user experience designer who has promoted the benefits of designing playful experiences. 
Daniel Cook [2008] is a game designer who has put forward the idea of building princess applications 
, i.e. taking advantage of structures like goal hierarchies and skill progression in designing applications. 
Cook takes the high level goal of the classic video game Super Mario Bros. and uses Mario s (i.e. the 
player s) journey through the game as a motivational structure that could be applied to the use patterns 
of any application. The approach introduced here will continue this line of thinking by placing it in 
the contexts of academic design research and game studies. I argue that these kinds of efforts call for 
more rigorous and concisely defined terminology. Kim s notion of game mechanics as a collection of tools 
and systems that an interactive designer can use to make an experience more fun and compelling [Porter 
2009], works as a starting point. In her work, Kim has also identified certain core gameplay mechanics, 
i.e. player actions, such as collecting and exchange. However, I argue that in order to translate the 
premise into actual design projects, the tools and systems need to be analyzed in more detail. The design 
space of game design for social networks is certainly not exhausted yet, especially as new social media 
platforms emerge almost monthly without doubt, an increasing number of design elements and patterns 
can be identified and tested within this design space. This can be easier if we lay down certain vocabulary 
and conceptual framework from which to follow the development of social networks for purposes of game 
design. 2.1 Designing Game mechanics: A Method ofTriangulation I will introduce a method of triangulation, 
which helps in designing game play into social networks. It starts with a more concise definition of 
game mechanics . In theoretical conceptualizations of game design, the design of so­called core mechanics 
has been widely acknowledged as being of fundamental importance in creating play [Salen &#38; Zimmerman 
2004, Järvinen 2008]. Core mechanics has been defined as the actions that players repeatedly take in 
a game [Salen &#38; Zimmerman 2004]. Instead of understanding game mechanics as a generic set of game 
design elements, I suggest a narrower yet more practical definition: Individual game mechanics can be 
thought of as verbs that game designer give the players to act in the world of the game. The verbs as 
mechanics are linked with the goals of the game, i.e. they are the means to reach the ends. Core mechanics 
are, thus, combinations of individual game mechanics that are used to accomplish certain goals imposed 
at the player. In effect, these relations are the building blocks for designing play. [Järvinen 2008.] 
Whereas in a single player video game, the core mechanics might create a feedback loop between the player 
and the software as a system, in multiplayer games, the system becomes more complex, as it will govern 
the actions of multiple players and their relations. In social network games, the is system the social 
network as a whole, consisting of the service (e.g., the Facebook platform), individual players, and 
the community as large. Game play in social networks is a feedback loop of player actions that try to 
accomplish goals, and are given feedback through the network, either through the system itself, or individual 
players, or community as a whole. The dynamic within these elements can be conceptualized as a triangle 
with the three elements, around which the user experience starts to emerge as play:  Image 1: Verbs 
 Goals Network play model. The model gives us a tentative idea of the scope and focus of designing games 
for social networks, i.e. what are the elements through which play can be created in this context. In 
which kind of rhythm and reciprocity should this dynamic be put into action is another game design question 
that we will tackle later. Before that, a more substantial, user-centered question is: What are suitable 
verbs and goals that speak to a user s motivations of engaging with social networks in the first place? 
 2.2 Motivations for online social networks and games In order to bridge interaction design with game 
design techniques, it is useful to take the motivations of, first, social media use, and second, game 
play into account. The advantage designers can gain from thinking about user motivations is that they 
can proceed to design opportunities for the users to act in ways that become expressions of the users 
motives. Therefore, games for social networks should target motivations of using those networks, and 
stylize them into playful interactions that give the players a feeling that they are expressing their 
motives --consciously or unconsciously. In his study of social networks, Yochai Benkler (2006) has identified 
the following motivations for social media use: Social connectedness, psychological well-being, gratification, 
and material gain. Peter Kollock (1999) has defined four motivations for contributing in online communities: 
Reciprocity, reputation, increased sense of efficacy, and attachment to and need of a group. When these 
motivations are addressed in terms of playing games in social networks, besides the obvious social aspects 
of play, it can also be leveraged to for increasing well-being and sense of efficacy. On the other hand, 
people playing multiplayer role-playing games are motivated by aspects having to do with achievements 
in the game (e.g., competing, and advancing), and immersing themselves into the game s world (e.g., discovering, 
customizing, enjoying the story aspects). Social aspects matter as well, e.g., working in a tem, player 
relationships, and socializing in general. [Yee 2001.] I argue that these two sets provide a useful starting 
point for synthesizing a framework for thinking about game design for social networks. It combines a 
number of the above features, however, by filtering them through the emotional disposition of playfulness, 
as discussed by Rao [2008]. Therefore the motivations for game play in social networks may become more 
casual (random, fleeting, effort-aversive) than the ones of, e.g. players of MMORPGs by average. According 
to emotion theorist Jon Elster [1999] emotions transform into emotional dispositions through their long-term 
consequences, i.e. repeated experience of an emotion that is triggered in connection with a particular 
event, object, or agent, becomes an emotional disposition towards it. Playful disposition, and variations 
in it, can thus be seen as long-term consequences of emotions experienced during the play of social media 
games. If we look back at the notion of user s behavior as an expression of their motives, and designing 
for it, the challenge is how to design for playful dispositions. Another interaction design practice, 
namely the design of user personas [Saffer 2007] to guide the design, can be useful here, as personas 
can be used to conceptualize different types of goals that the players have, such as end goals, experience 
goals, and life goals. The Verbs -Goals Network model helps in determining how such goals figure in 
the design of a game for the broader contexts of social network use.  2.3 Five Design Drivers Now we 
are ready to establish a framework of motivations and dispositions regarding social network use, which 
in turn can be formulated into a number of design drivers. Rao [2008] identifies three qualities to the 
playfulness that characterizes Facebook use: Physicality, Spontaneity, and Inherent Sociability. My research 
shows that the narrative aspect of the network is important, and therefore I will add Narrativity to 
the list. Furthermore, as a particular game design feature, Asynchronicity, as discussed by Ian Bogost 
[2009], needs to be considered as another quality. I will use this four-fold distinction as a framework 
for further identifying principles that would support designing for the playful dispositions. (We will 
come across more examples of how these qualities are evident in the case study section on Facebook games.) 
2.3.1 Symbolic Physicality Rao identifies the symbolic ways that Facebook games add physical depth to 
playful interactions , such as poking, drinking beer, hi-fiving, etc. These features essentially try 
to add human warmth of actual physicality to the non-physical online space. 2.3.2 Spontaneity The apparent 
silliness and/or simplicity of Facebook games, such as complicated set of actions being simplified into 
a click of a single button, is there to support the inherent spontaneity of user behavior in online social 
networks. Many of the above-mentioned manners of symbolic physicality draw from this quality as well: 
a one-click poke functionality encourages spontaneous acts of symbolic physicality. In the context of 
networked play, spontaneity is closely related to both accessibility and familiarity: by modeling acts 
and themes familiar from everyday life (such as giving gifts or parking a car) or popular culture (gangsters), 
eliminates the need to explain various rules, as players are familiar with the conventions and behavioral 
schemas associated with their game counterparts. 2.3.3 Inherent sociability Playfulness is intrinsically 
connected to social situations and cannot exist without them , according to Rao [2008]. Again, the above-mentioned 
features highlight this in addition, Rao lists fast rewards for player actions, abundance of positive 
feedback, no negative consequences for exploration, and ablity to build on someone else s work as design 
solutions that support the inherent sociability. These features are, by and large, similar to ones identified 
from the design of casual games in general. In terms of designing games, the inherent sociability opens 
up possibilities for intuitive teaming of players, since networked individuals might have a particular 
social context where they know each other. Nabeel Hyatt [2008] has indeed pointed out how social network 
games can rely heavily on social context (namely school, department, and residence loyalties) to provide 
a framework for alliances, gameplay and motivation. It is no coincidence, thus, that games for social 
networks have been generally grouped, in business and design discourse, under the label social games 
. Even if it can be pointed out that games have always been fundamentally social, the fact that these 
online networks do not meaningfully exist without the users social ties does make the term appropriate. 
In terms of design, it accounts for taking such inherent sociability as a starting point for concept 
creation and design. 2.3.4 Narrativity When studying how popular social network games' play dynamics 
work, it becomes evident how fundamental it is for the concept that various player actions and play results 
are not only communicated but stylized into particular narrative rhetoric across the network. Therefore 
narrativity warrants to be defined as an important design driver. It has consequences for the visibility 
of the game concept in the network. Facebook games embrace narrative by using it in propagating word 
of the game and its players across the network. This happens using the standard functionalities individual 
users news feed and notifications, in particular first, in order to keep existing players engaged, and 
second, in trying to elicit curiosity towards the game from people in the network who are not yet playing. 
It is useful to make a distinction between functional means of communicating game results and player 
progress, and narrative techniques to do that. The differences are specifically in the use of rhetoric 
that draws from the metaphors and fiction that the game designers have constructed in relation to the 
game s rules. Consequently, a social network game about mobsters might address the player, and communicate 
its results and events across the network, with parlance that is associated with particular rhetoric 
of fiction, familiar from as crime and gangster movies, comics, and novels. 2.3.5 Asynchronicity Finally, 
the last high-level design driver is one that can be used to guide design solutions regarding the tempo 
of the game, i.e. how design solutions concerning the other drivers come together in the social play 
of networks. This ties directly in with the verbs-goals­network response model presented earlier, and 
how it s poles function as the triangle where network play emerges. Game designer and academic Ian Bogost 
[2004] lists four features of asynchronous play it supports multiple players playing in sequence, not 
in tandem , it requires a persistent state which all players affect, and which in turn affects all players 
, and finally, it is organized around the breaks between players: opponent turns in Scrabble often mean 
bathroom breaks, email checks [Bogost 2004] Yet, according to Bogost, this kind of asyncronicity needs 
not be the game s defining characteristic. However, it would seem that most games in social networks 
do center around such breaks, it is just that the quality and quantity of the breaks are based on the 
nature and/or constraints of the system i.e. breaks in play in social networks that center around instant 
messaging or micro-blogging, such as Twitter, would create different variety of asynchronous play than 
Facebook, which supposedly has a slower, more structured rhythm of use.   2.4 Interaction Design for 
Playfulness Concluding from the definitions and observations made thus far, we can tentatively define 
game design for social networks as Interaction design for social playfulness . This means designing for 
inherently casual yet highly engaged disposition to play around in the social network, with the general 
means afforded by the platform, and the extended affordances that applications, such as games, bring 
with them. Yet, designing for playfulness also means that the focus of the design result should privilege 
emotional engagement rather than highly intricate and innovative gameplay even if these two are not 
necessarily in contradiction. Mihaly and Mayer echo this observation: Good games on FB are as much about 
communication and/or self-expression as they are about gameplay [Mihaly 2008], and For users of these 
applications where am I at? can be almost as important as what s next? [Mayer 2009b]. Rao concludes 
her research by stating that Facebook Applications seem to appeal to the sphere of emotions (fun and 
playful mood) rather than actions (gameplay) . She elaborates that instead of modeling and stylizing 
actions concretely for gameplay as verbs, which is what real games do, these games rely on compressing 
that action into a few clicks (at most), and then narrating the resulting action through a dramatic tale 
, as Rao puts it. As a consequence, minimal engagement produces high rewards. [Rao 2008]. One could summarize 
this difference into a comparative principle: Whereas video game designers create skill-based justifications 
for resolutions of events, i.e. whether an action was successful or not; social network game designers 
create story-or community-based justifications for the resolutions of events in their games. The illustration 
below models the different aspects of interaction design for playfulness. It starts with the asynchronicity 
that permeates play in social networks: Play takes place in turns, or in individual time units ('ticks') 
per player which then get acknowledged by the game as a system facilitating networked play. That is why 
it is pictured as an orange, cyclical path of game play along which players repeatedly go through. Furthermore, 
their progress, network standing, and reputation evolve parallel to this cyclical process of core play 
mechanics.  Image 2: Design framework for social network play dynamics visualized. In the above visualization, 
the clouds between individual play moments specify certain, predictable consequences for player experience. 
Therefore, the idea of the framework is that design solutions affecting and producing such transitions 
can be put into specific focus, and some perhaps emphasized over the others, thus giving the play the 
game facilitates potentially a different flavor. (The design solutions are visualized as the drops marked 
GD in connection with each driver.) Now we will move on to see, how these consequences of interaction 
design for playfulness can be seen in a sample of Facebook games and their designs. 3. Case studies 
in Facebook Games As this case study sample is focused on game applications produced for Facebook, a 
certain set of functionalities will be in place due to the platform itself. These include, for instance, 
access to a list of friends who are playing , the invite friends feature upon installation, and the notification 
functionality, which is used to alert players to change of turns, request for help form friends, and 
other dynamics of the game. In the context of this paper, such standard functionalities can be interpreted 
as being part of how the users can express their motivation for socializing through Facebook. They are 
also functionalities that encourage the users into network proliferation [Brathwaite 2007], i.e. the 
game developers can try to leverage their player base into marketing the game to their network of friends. 
The selection of six games here is based on two factors: Either the given game is among the most popular 
in Facebook (Lil Green Patch, Mafia Wars, Who Has the Biggest Brain?), or it is interesting in terms 
of its game design (Parking Wars, PackRat, PhotoGrab). I will shortly describe each game and analyze 
their design features. 3.1 Parking Wars: Risk and Reward In Parking Wars2 players own a street with 
a number of parking lots, and a set of cars of various color. The task is to earn money by keeping one 
s own cars parked and by ticketing fellow players who park on illegal zones on one s street. Money accumulates 
as time passes, which makes Parking Wars appear as an asynchronous waiting game. However, soon after 
one starts playing, the game becomes a resource management game in the sense that the player is not able 
to park your cars on the slots that their color permits you to. Thus, players have to take risks by parking 
on illegal zones (parking a red card on a slot reserved for blue ones, for example) in the fear that 
the other player catches her and gives a ticket. As one gains money in the game, new cars are unlocked, 
which makes the task more difficult, but at the same time, one has more resources for making money. I 
would argue that Parking Wars is a relatively sophisticated game design when compared with most Facebook 
applications. Game designer and academic Brenda Brathwaite [2007] has analyzed its dynamics. Her discussion 
of how the game encourages repeat visits is the most interesting design detail in the game. These repeat 
visits work on a risk-reward principle: in case the player has parked her car on an illegal zone, or 
finds another player who has parked illegally on her street, she can, instead of ticketing or leaving 
with her car immediately, wait in the hope of accumulating more money. This is a risk, though, as the 
other player might leave the illegal zone, or her car might be ticketed at any moment. What makes this 
risk-reward structure in Parking Wars interesting is how it is tied to the asynchronous nature of the 
game, and thus to common practices of using the social network, i.e. checking back Facebook few times 
a day per average. Getting engaged with Parking Wars might very well mean that a user s visiting frequency 
starts to accelerate, in direct proportion to one s friends activity in the game. Fittingly, area/code 
designer Frank Lantz [2009] described the game in a recent Game Developer Conference talk as a form of 
gardening punctuated with 'gotcha' moments . In these ways, Parking Wars design melds asycnhronicity, 
sociability, and spontaneity into an addictive mix. Slapping tickets on fellow players can also be seen 
as an instance of symbolic physicality that makes the game more emotional. We had accidentally designed 
a social MMO , Lantz [2009] stated. This also led to the need to add features to the game, so that it 
would evolve with players' needs: achievements in the form of different badges, new types of cars, etc. 
2 Parking Wars by area/code. http://www.facebook.com/apps/application.php?sid=e518c5ec77fe98b2 bcffac8227d89996&#38;id=31435010008&#38;ref=s 
 Parking Wars is an interesting example of how a game design has been appropriated to many of the inherent, 
motivating features of the social network it is being played in.  3.2 Lil Green Patch: Reciprocity and 
Communality In Lil Green Patch3, each player is given a small garden patch, where they can collect various 
kinds of plants by exchanging them with other players. In the process, the application motivates players 
by promising that For every 10 friends receiving a plant you save 1 square foot of rain forest from deforestation 
. This is achieved by sponsorship deals, where the sponsors donate money according to overall player 
activity. Green bucks are a currency in the game with which one can unlock new plants and tools which 
help in tending the garden. Fellow players can also tend your garden, by weeding it, for example. The 
main design feature is the shared goal for a common cause, which is extraneous to the game itself, even 
if facilitated by the exchange mechanic, which necessitates player response, rather than system response. 
This asynchronous feature facilitates reciprocity, which can be seen as an expression of the inherent 
sociability that Lil Green Patch supports. Fighting global warming is also made more accessible and spontaneous 
by tapping into cultural and seasonal conventions (Valentine s day etc) regarding the plants and the 
ritual of exchange and remembering friends. These design features can be seen as supporting both narrativity 
and spontaneity.  3.3 Mafia Wars: One-Click Choices within a Reputation System Mafia Wars4 describes 
itself with the following slogan: Start a Mafia Family with your friends, do Crime Jobs for cash, buy 
Powerful Weapons, and Fight!!! Mafia Wars, and many similar games by its developer Zynga, is a strategy 
game designed to the mould of games like Civilization: The player gathers resources and accomplishes 
tasks in order to progress ahead on a tree-like structure of new goals, gameplay elements and environments. 
Civilization designer Sid Meier has famously defined a game as a series of interesting choices . Formally 
it can be said that Mafia Wars follows this design philosophy, even if the interestingness of its choices 
can be debated. As it is, Mafia Wars demonstrates a number of design solutions outlined earlier: it compresses 
seemingly complex actions and events e.g. Corner-Stole Hold Up -into single clicks of a button. This 
is a design solution that adheres to Rao s earlier observation that in Facebook games, the story of the 
action is more important than the act itself. The consequence is that the act, the verb, can be minimized 
into a simple one-click choice; a gang war in the press of a button. Another aspect that Mafia Wars highlights 
is how it incorporates a design of a reputation system into its gameplay. This is a feature 3 Lil Green 
Patch by Ashish Dixit and David King. http://www.facebook.com/apps/application.php?sid=3e931187c0ff99b1 
e72ac7bbd4482989&#38;id=7629233915&#38;ref=s 4 Mafia Wars by Zynga. http://www.facebook.com/apps/application.php?sid=3f2022455da3e7ef 
7166d1508f85d9cf&#38;id=10979261223&#38;ref=s that Amy Jo Kim addresses as a game-like feature that has 
found its way into many online services [Porter 2009]. Mafia Wars reputation system is a goal hierarchy 
as well, and it is communicated to the player through a metaphor that is modeled after the power structure 
of the Mafia: The player starts as a regular thug, trying to earn the reputation of a Capo, and so on. 
There is an existing reputation system that has been modeled into the purposes of the game, and this 
narrative aspect gives playing the game a touch of make-believe. In a reputation system like this, constantly 
communicating the player s status or standing in the game is of utmost importance. As Andrew Mayer has 
noted, these kinds of social network games are constantly telling you where you re at, what you need, 
and what s next. [Mayer 2009b] When this is linked to the one-click choices, together they support spontaneous 
playfulness. The reputation as a reward is more important than the activity described in the one-button 
choices, and thus they can be as sparse as they are. However, Mafia Wars embeds the inherent sociability 
of the network into its simple goals: Most of the goals cannot be achieved without friends from one s 
network. As the screenshot above illustrates, the game employs a deliberate rhetoric of with your help 
in both complementing the players based on their actions, and in requesting them to join an action with 
a fellow player. 3.4 Who Has the Biggest Brain? This game5 recycles Nintendo s highly popular Brain Training 
game series into a Facebook game application. It provides various types of cognitive challenges, and 
uses of the metaphor of brain size as an indicator of player progression. The game is among the top 30 
most popular Facebook applications with approximately 4 million monthly active players. The game also 
applies a pattern in its title that seems to be frequent in development of casual games: Who has the 
biggest Brain? communicates its high level goal directly in its title, at the same time referencing the 
hugely popular television game show Who wants to be a Millionaire? Based on the games popularity, it 
seems that the longing of being smart, and being recognized as one in the game s leaderboard, is almost 
as desirable as being a millionaire. The game s design also incorporates achievements handed out to the 
players as various brain labels : Based on your play of the brain-teasing games, it tells you whether 
you have a nerdy or an alien brain. These labels can be easily shared and discussed as personality types, 
which likens them to various Which movie are you? and other Just for fun tests that populated the Internet 
even before the popularity of present online social networks. Such achievements are another design solution 
for supporting the inherent sociability of the network, where exchanges and comparisons with your friends 
are spontaneous to conduct. 3.5 PhotoGrab: User-made levels 5 Who Has the Biggest Brain? by Playfish. 
http://www.facebook.com/apps/application.php?sid=0940691730d1d70 4dfc50e41efaca2f3&#38;id=8827826004&#38;ref=s 
 PhotoGrab6 is marketed as BrainGames meets Photo-Sharing . It is is developed by Shufflebrain, a company 
of Amy Jo Kim s, who was referenced earlier on her work on bridging game design and interaction design 
techniques. The game is based around photographs, which users can share and turn into games that test 
players quickness in pattern recognition. This game application draws from inherent sociability by tapping 
into online photo sharing that has become very popular. The game makes itself accessible via the fact 
that almost everyone has photos, and everyone is familiar with the process of trying to spot details 
in an image. In summary, the verbs of the game are familiar, intuitive for human cognitive processes, 
and they are connected to goals with cognitive benefits, i.e. the brain game aspect. Another noteworthy 
feature is that PhotoGrab give the users creativity an expression through the possibility of making one 
s own games out of photographs picked from the user s Facebook albums. This is a design feature that 
takes advantage of user­generated content, but also, it is interesting how it takes advantage of the 
user as a resource of information beyond her Facebook profile credentials. PhotoGrab is an example of 
how any content that the user makes available in the social platform can be appropriated into the purposes 
of a social network game, if spontaneous, playful game mechanics can be designed around the content. 
  4. Design elements for playful dispositions Based on the theoretical discussion, and the case studies, 
I will summarize notes about tentative design principles for social network games. The playful qualities 
of spontaneity, sociability, symbolic physicality, narrativity, and asynchronicity are the driving, yet 
high-level principles. If we go into more detail, first, it seems that games that manage to bridge a 
number of the playful qualities of social networks are successful, at least in the short run: If a game 
concept includes design features that support both spontaneity, asynchronicity, and symbolic physicality 
that take use of the narrativity and inherent sociability of the platform, it is at least formally promising. 
However, implementing the features can just as easily make or break the game therefore, it is not only 
the design of the metaphors, verbs, goals, but their communication and implementation that is important. 
How the network as an instance of individuals and community responds to the user s playful activities, 
is a fundamental design task in a social network game project. The author suggests that the Verbs Goals 
 Network model is helpful in designing these aspects. Whereas interaction design techniques help in grasping 
the social possibilities of the network, game design techniques are needed to create competition, challenge, 
and tension into those acts of socializing, even if they would be more playful than with most other game 
contexts. The logging in/out of the social network as a risk-reward structure is an example of what clever 
game design solutions can bring to the table. 6 PhotoGrab Beta by ShuffleBrain. http://www.facebook.com/apps/application.php?sid=adb3bea6b53a40dd 
8bdec0a2bc92ed8b&#38;id=15807092154&#38;ref=s Through the case studies, this paper has documented a number 
ways that high level design principles for playfulness can be implemented in order to support the playful 
dispositions that majority of social network users seem to embrace. We can already identify a number 
of design patterns that can be used to inspire design solutions. Below, a visualization of them is presented 
in analogous fashion to the earlier visualization of the high-level design drivers. The patterns are 
meant to inspire concrete design solutions for the gaps in asynchronous social play and, to highlight 
the aspect of game design that it is a form of system design where the dynamics of the system create 
play experiences, which are always more than the sum of the system s parts. Therefore, use of such patterns 
should always be complemented with iterative design practices where play-testing is fundamentally important. 
 Image 3: Design patterns for supporting the design drivers. 5. Conclusion for Future Research and Development 
As social networks emerge that move from Facebooks symmetrical reciprocity of friendship, and onto a 
more free-form structures, as with Twitter s asymmetrical follower structure, this has consequences for 
game design tasks as well. Asynchronicity, or at least its rhythm and implementation, need to be reconsidered. 
Another considerable design space is that of application APIs and the information feeds they create. 
Social networks with an existing user base might be applicable for game concepts, which feed information 
to and from the network and its users. The research presented here suggests that analyzing the API of 
a social network helps in identifying the playability of the platform. On the other hand, there are weak 
signals of so-called crowdsourcing becoming more prominent feature in game applications as well. In practice 
this could mean tapping into creativity of users by motivating them with game-like goal structures. Jane 
McGonigal s work in the Institute for the Future with social games, such as Superstruct and Free Space 
(see http://lab.signtific.org), presents early examples of these kinds of social game concepts which 
create their own temporary, experimental social networks, where the verbs are essentially qualitative 
means of expressing and imagining future developments. To conclude, this paper presents work in progress 
and introduces a beginning for a collection of design patterns for games in social networks. The above 
collection, and the five high-level design drivers present raw material for future work. One option would 
be start formalizing them according to a model that, e.g., Björk and Holopainen [2005] have done in their 
study of patterns for game design. In this context, this paper presents an initial harvest of patterns 
that the author hopes not only to formalize, but also to validate through prototypes and applications. 
 6. REFERENCES BENKLER, Y. 2006. The Wealth of Networks. Yale University Press. BJÖRK, S. &#38; HOLOPAINEN, 
J. 2005. Patterns in Game Design. Charles River Media. BOGOST, I. 2004. Asynchronous Multiplay: Futures 
for Casual Multiplayer Experience. Other Player Conference, IT University of Copenhagen, Denmark. BRATHWAITE, 
B. 2007. Facebook s Parking Wars Play Dynamics. http://bbrathwaite.wordpress.com/2007/12/27/facebooks­parking-wars-play-dynamics/ 
COOK, D. 2008. The Princess Rescuing Application. Lost Garden. http://lostgarden.com/2008/10/princess-rescuing­application-slides.html 
ELSTER, J. 1999. Strong Feelings. Emotion, Addiction, and Human Behavior. MIT Press. FOLLETT, J. 2007. 
Engaging User Creativity: The Playful Experience at UXMatters. http://www.uxmatters.com/mt/archives/2007/12/engaging­user-creativity-the-playful-experience.php 
GRAHAM, J. 2006. Flickr of idea on a gaming project led to photo website . Usa Today, Feb 27th, 2006. 
http://www.usatoday.com/tech/products/2006-02-27­flickr_x.htm HYATT, N. 2008. What's wrong with Facebook 
games? http://nabeel.typepad.com/brinking/2008/01/whats-wrong­wit.html JÄRVINEN, A. 2008. Games without 
Frontiers. Theories and Methods for Game Studies and Design. Doctoral dissertation for University of 
Tampere, Finland. http://acta.uta.fi/english/teos.php?id=11046 KIM, A.J. 2008. Putting the Fun in Functional. 
Applying game mechanics to Functional Software. http://www.slideshare.net/amyjokim/putting-the-fun-in­functiona?type=powerpoint 
KOLLOCK, P. 1999. The Economies of Online Collaboration: Gifts and Public Goods in Cyberspace. In Smith, 
M.A. &#38; Kloock, P. (eds.) Communities in Cyberspace. Routledge. LANTZ, F. 2009. Play in the Age of 
Social Software. Game Developers Conference, March 27, 2009. https://www.cmpevents.com/GD09/a.asp?option=C&#38;V=11 
&#38;SessID=937 LAUREL, B 2003. (Ed.) Design Research. Methods and Perspectives. MIT Press. MAYER, A. 
2009a. Ten Reasons that Social Games May Eat the Casual Market. Inside Social Games: Tracking Innovation 
at the Convergence of Games and Social Platforms. http://www.insidesocialgames.com/2009/02/10/ten-reasons­that-social-games-may-eat-the-casual-market/ 
MAYER, A. 2009b. Successful Social Games Are All About Status at Inside Social Games: Tracking Innovation 
at the Convergence of Games and Social Platforms. http://www.insidesocialgames.com/2009/02/24/successful­social-games-are-all-about-status/ 
MIHALY, M. 2008. Casual Games on Facebook http://forge.ironrealms.com/2008/01/11/casual-games-on­ facebook/ 
PORTER, J. 2008. Designing for the Social Web. New Riders. PORTER, J. 2009. Game Mechanics for Interaction 
Design: An Interview with Amy Jo Kim. http://bokardo.com/archives/game-mechanics-for­interaction-design-an-interview-with-amy-jo-kim/ 
RAO, V. 2008. Playful Mood: the Construction of. Facebook as a Third Place. MIndTrek Conference 2008, 
Tampere, Finland. http://portal.acm.org/citation.cfm?id=1457199.1457202&#38;co ll=Portal&#38;dl=GUIDE&#38;CFID=24746181&#38;CFTOKEN=856 
17762 REMO, C. 2009. DICE 09: Valve's Newell On 'Using Your Customer Base To Reach New Customers'. Gamasutra. 
http://www.gamasutra.com/php­bin/news_index.php?story=22378 SAFFER, D. 2007. Designing for Interaction: 
Creating Smart Applications and Clever Devices. New Riders. SALEN, K &#38; ZIMMERMAN, E. 2004. Rules 
of Play. Game Design Fundamentals. MIT Press. YEE, N. 2001. The Norrathian scrolls: A study of EverQuest 
(version 2.5) http://www.nickyee.com/eqt/home.html   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1581089</section_id>
		<sort_key>160</sort_key>
		<section_seq_no>4</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Kinesthetic movement in games I]]></section_title>
		<section_page_from>103</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1570337</person_id>
				<author_profile_id><![CDATA[81320489774]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tracy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fullerton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1581090</article_id>
		<sort_key>170</sort_key>
		<display_label>Pages</display_label>
		<article_publication_date>08-04-2009</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A high performance visual profiler for games]]></title>
		<page_from>103</page_from>
		<page_to>110</page_to>
		<doi_number>10.1145/1581073.1581090</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1581090</url>
		<abstract>
			<par><![CDATA[<p>Video games are software products with the purpose to entertain its players. Unfortunately, the performance of video games can suddenly decrease; this phenomenon is called a <i>frame drop</i>, and causes the amount of fun experienced by players to drop. To avoid this behavior, usually the process of creating a video game involves trying to improving the performance of a game, usually aided by use of a <i>performance profiler</i>. We present a performance profiler designed to find causes of frame drops and other bottlenecks in video games. Current performance profilers are not suitable for video games because they are often slow while collecting data, so the interactive element of video games is lost and recreating events that cause frame drops is next to impossible. Furthermore, they accumulate information over relatively large periods of time making temporary drops in performance invisible and their causes difficult to find.</p> <p>This article describes a tool called GamePro. GamePro is a performance profiler that consists of two components: a data logger and a data presenter. The data logger is fast during run-time, has a powerful <i>snapshot</i> feature that collects timed data, and can inspect native and scripting methods. The data presenter is able to show causes of sudden drops in performance and other bottlenecks in software. Visualization is used to present the data and to enable the developers to find performance issues efficiently and effectively.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[development tools]]></kw>
			<kw><![CDATA[frame drops]]></kw>
			<kw><![CDATA[performance]]></kw>
			<kw><![CDATA[profiling]]></kw>
			<kw><![CDATA[video games]]></kw>
			<kw><![CDATA[visualization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>D.2.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10011007.10011074.10011099.10011102.10011103</concept_id>
				<concept_desc>CCS->Software and its engineering->Software creation and management->Software verification and validation->Software defect analysis->Software testing and debugging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011074.10011099.10011102.10011103</concept_id>
				<concept_desc>CCS->Software and its engineering->Software creation and management->Software verification and validation->Software defect analysis->Software testing and debugging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1570338</person_id>
				<author_profile_id><![CDATA[81440622164]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michiel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Roza]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Eximion BV]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570339</person_id>
				<author_profile_id><![CDATA[81336492704]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schroders]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Eximion BV]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570340</person_id>
				<author_profile_id><![CDATA[81100431196]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Huub]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[van de Wetering]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technische Universiteit Eindhoven]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Angus, T., 2009. rtprof. Online resource. http://rtprof.sourceforge.net/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[AutomatedQA, 2009. AQtime. Online resource. http://www.automatedqa.com/products/aqtime/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bruls, M., Huizing, K., and van Wijk, J. J. 2000. Squarified treemaps. In <i>Proceedings of Joint Eurographics and IEEE TCVG Symposium on Visualization (TCVG 2000)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Deelen, P., van Ham, F., Huizing, C., and van de Wetering, H. 2007. Visualization of dynamic program aspects. In <i>Proceedings of the 4th IEEE International Workshp on Visualizing Software for Understanding and Analysis</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Electric Software, 2009. GlowCode. Online resource. http://www.glowcode.com/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Eximion, 2009. Kalydo, an online game console. Online resource. http://www.kalydo.com.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Eximion, 2009. The Kalydo engine. Online resource. http://www.dreamcreateplay.com.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1190039</ref_obj_id>
				<ref_obj_pid>1190036</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Holten, D., van Wijk, J. J., and Martens, J.-B. 2006. A perceptually based spectral model for isotropic textures. <i>ACM Transactions on Applied Perception</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>949654</ref_obj_id>
				<ref_obj_pid>949607</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Johnson, B., and Shneiderman, B. 1991. Tree-maps: A space-filling approach to the visualization of hierarchical information structures. In <i>Proc. of the 2nd international IEEE Visualization Converence</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>309407</ref_obj_id>
				<ref_obj_pid>308706</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Panzer, J. 1999. Automatic code instrumentation. <i>C/C++ Users Journal</i> (January).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Roza, M. W. A. 2008. <i>GamePro: A high performance visual profiler for real-time applications</i>. Master's thesis, Technische Universiteit Eindhoven.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Software Verification, 2009. Performance Validator. Online resource. http://www.softwareverify.com/cpp/profiler/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>857663</ref_obj_id>
				<ref_obj_pid>857189</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Van Wijk, J., and van de Wetering, H. 1999. Cushion treemaps: Visualization of hierarchical information. In <i>Proceedings IEEE Symposium on Information Visualization (InfoVis'99)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1106568</ref_obj_id>
				<ref_obj_pid>1106328</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Wattenberg, M. 2005. Baby names, visualization, and social data analysis. In <i>Proc. IEEE Symp. Information Visualization</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A high performance visual pro.ler for games Michiel Roza* Mark Schroders Huub van de Wetering Eximion 
BV Eximion BV Technische Universiteit Eindhoven Abstract Video games are software products with the purpose 
to entertain its players. Unfortunately, the performance of video games can sud­denly decrease; this 
phenomenon is called a frame drop, and causes the amount of fun experienced by players to drop. To avoid 
this be­havior, usually the process of creating a video game involves trying to improving the performance 
of a game, usually aided by use of a performance pro.ler. We present a performance pro.ler designed to 
.nd causes of frame drops and other bottlenecks in video games. Current performance pro.lers are not 
suitable for video games be­cause they are often slow while collecting data, so the interactive el­ement 
of video games is lost and recreating events that cause frame drops is next to impossible. Furthermore, 
they accumulate informa­tion over relatively large periods of time making temporary drops in performance 
invisible and their causes dif.cult to .nd. This article describes a tool called GamePro. GamePro is 
a perfor­mance pro.ler that consists of two components: a data logger and a data presenter. The data 
logger is fast during run-time, has a power­ful snapshot feature that collects timed data, and can inspect 
native and scripting methods. The data presenter is able to show causes of sudden drops in performance 
and other bottlenecks in software. Vi­sualization is used to present the data and to enable the developers 
to .nd performance issues ef.ciently and effectively. CR Categories: D.2.5 [Software]: Software Engineering 
Testing and Debugging Keywords: development tools, frame drops, pro.ling, perfor­mance, video games, 
visualization 1 Introduction The purpose of video games is to entertain. The amount of fun ex­perienced 
by the player is usually lowered when the performance of a game .uctuates; one of the reasons is that 
changes in performance can cause the player to lose control over the game. This is speci.­cally problematic 
if the performance suddenly drops dramatically. Developers of video games should therefore write software 
with performance that .uctuates as little as possible; this is hard espe­cially since video games tend 
to use a lot of resources. To achieve this, developers of video games use tools that measure performance 
*michiel@eximion.nl mark@eximion.nl h.v.d.wetering@tue.nl Copyright &#38;#169; 2009 by the Association 
for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for 
personal or classroom use is granted without fee provided that copies are not made or distributed for 
commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. 
To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific 
permission and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or 
e-mail permissions@acm.org. Sandbox 2009, New Orleans, Louisiana, August 4 6, 2009. &#38;#169; 2009 
ACM 978-1-60558-514-7/09/0008 $10.00 of software, called performance pro.lers. These tools show devel­opers 
which parts of their software limit the performance of the en­tire application, also known as bottlenecks. 
Numerous of such tools exist, however, no tools exist that can quickly inform the developer when a frame 
drop occurs, where it occurs in the code and what happens during this sudden drop in performance. Furthermore, 
as video games are usually large projects, many pro.lers do not clearly show what happens in the software 
because they have limited ways of presenting large amounts of gathered pro.le data. For convenience, 
performance of a video game will be calculated in the amount of consecutive images shown per second, 
also known as frame rate.A frame drop is de.ned as a sudden drop in frame rate below a certain threshold. 
Note that a low frame rate isn t as bad as a frame drop, as long as it stays above a certain threshold. 
This threshold varies from person to person and from game to game, but in general a frame rate of at 
least 30 frames per second is acceptable as long as the frame rate does not vary too much. This article 
describes the creation of a tool which we call GamePro, which enables developers to learn what causes 
these performance issues. We use visualization to enable the developers to .nd perfor­mance issues ef.ciently. 
1.1 User questions GamePro is mainly (but not exclusively) tested by pro.ling the Ka­lydo engine [Eximion 
2009b] that powers all games on the online game console Kalydo [Eximion 2009a]. The target users of Game-Pro 
are developers that want to improve the performance of their real-time software, as well as .nd causes 
of frame drops. The target users have access to and understand the source code of the inspected software 
and know how to resolve issues that GamePro points out. To determine what the tool should be able to 
point out, develop­ers were asked what they wanted to know about their software, and what was missing 
from existing pro.lers. The following user ques­tions about real-time software were formulated based 
on that in­formal survey: What are the biggest bottlenecks in the inspected software? What are causes 
of sudden drops in performance? What parts of the software use large amounts of memory? Does the soft­ware 
contain memory leaks, and if so, what causes those leaks?  1.2 Performance pro.lers Performance pro.lers 
are tools that measure performance of soft­ware. Current performance pro.lers are not suitable for analyz­ing 
the performance of a video game, because they use a lot of resources. As a result, the pro.led video 
game becomes too slow to remain interactive and recreation of events that cause frame drops becomes cumbersome. 
Also, current pro.lers only show the overall average or total duration of parts of software, but not 
their behavior over time. As a result, .nding causes of frame drops is hard. GamePro is a performance 
pro.ler that inspects dynamic software aspects, like method timing, memory usage, and memory leaks. 
1.2.1 Online vs. of.ine Pro.lers can basically be subdivided into online pro.lers that present collected 
data during run-time, and of.ine pro.lers that con­sist of a separate data collector that stores data 
at run-time and a data presenter that shows this data after completion of the pro.led software. Both 
methods have their advantages and disadvantages. An online pro.ler can help the user to associate behavior 
with pro­.ling information; such as the pressing of a button that triggers method calls the user may 
or may not expect. Examples of online pro.lers are GlowCode [Electric Software 2009] and rtprof [An­gus 
2009]. An of.ine pro.ler uses less resources during run-time of the pro.led software and also enables 
the user to pro.le full­screen applications, as no screen space is needed for presenting the collected 
data during run-time. Furthermore, it allows the user to study the data in their own time, without the 
need for an extra eye on the presented data. An example of an of.ine pro.ler is Auto­matedQA AQtime [AutomatedQA 
2009]. Another notable of.ine tool is TraceVis by Pieter Deelen [Deelen et al. 2007] that visual­izes 
dynamic software aspects other than performance. Whether a pro.ler is online or of.ine is not mutually 
exclusive. Performance Validator [Software Veri.cation 2009] is a pro.ler that can be used both online 
and of.ine. GamePro is an of.ine pro.ler for reasons of performance and the possibility to analyze data 
for software in any other language, on any other operating system or using another compiler. Also, the 
user running the software (e.g. the tester) can be different than the user analyzing the data. It is 
possible to analyze the data on another computer. This allows collection of data on a slow system to 
check the bottlenecks on slower systems, while analyzing the data on a faster one for better response 
of the analysis tool.  1.2.2 Sampling vs. instrumenting Pro.lers work by collecting dynamic data on 
the inspected software during run-time. This can be done using two different methods: sampling pro.lers 
check which method or stack is active at prede­.ned time samples, and instrumenting pro.lers alter the 
machine code in such a way, that the pro.ler gains control over the software at certain events. Sampling 
pro.lers are overall faster because over­head needed for collecting data is less, but they are less precise 
as method calls that occur in between samples can be missed. GamePro is an instrumenting pro.ler because 
it needs accurate tim­ing for computing method durations and it needs to collect extra information like 
memory usage.  1.3 Structure In the next section we describe the data model used, and section 3 describes 
its collection and manipulation. Section 4 explains the visualization of this data. Section 5 describes 
the interaction, and section 6 describes an evaluation of the tool. Section 7 contains conclusions and 
future work. For a more detailed discussion we refer to the master s thesis [Roza 2008] of the .rst author. 
 2 Data model An execution of an application is called a run. A run consists of one or more threads. 
Inside each thread the following events oc­cur sequentially: method entries and exits, and memory allocation, 
deallocation, and reallocation. A list of these events in sequence is called a trace. GamePro Data Logger 
is an instrumenting pro.ler and is designed to collect traces. GamePro Data Logger collects the method 
entry and exit events for not only native methods, but also for script methods; as scripting is commonly 
used in video games. 2.1 Traces A trace is de.ned as a list of typed time-stamped events. The event types 
and their attributes are: Method entry/exit with method id, and  Memory allocation/deallocation with 
allocation id and size.  Two values are interesting for the inspection of performance of soft­ware: 
the duration of a method call including the duration of all subsequent calls from that method (which 
is, conveniently, the time between an entry and an exit of the same method call) called inclu­sive duration; 
which can be used to learn how expensive method calls are, and the duration of a method call excluding 
the duration of all subsequent calls from that method, called exclusive duration; which can be used to 
learn more about where improvements can be made when inspecting expensive calls. Memory events are used 
for detecting memory leaks, which are parts of memory that are no longer referenced, but are not freed 
either, and memory hogs, which are parts of software that use a lot of memory. We focus on the size of 
the memory allocated and deallocated, and the location of memory allocations in order to .nd the causes 
of memory leaks. To achieve this, we use the following scheme: memory allocation events are matched by 
memory deallo­cation events by their unique allocation identi.er. If such a deallo­cation event does 
not exist in any of the traces, the memory usage has increased.  2.2 Pro.le trees Traces are collected, 
but not stored due to sheer size. Instead the data collector stores a pro.le tree (Va,Ea) for each trace 
a. Va is the set of call stacks occurring during the execution of trace a, including the empty stack. 
Each event corresponds to a call stack that is de.ned as the list m0 · m1 · ... · mn of nested active 
method calls mi. An edge (s, t) in Ea exists if a method entry m in a exists such that t = s · m; s is 
called the parent of t and t a child of s. Figure 1 shows a node link diagram of a pro.le tree. The label 
on an edge (s, t ) is the method m with t = s · m. We see that a method can occur on multiple edges. 
A vertex is represented in this .gure by the list containing the method identi.ers used as edge labels 
on its root path and ordered from the root to the vertex. Figure 1: A pro.le tree where a label on an 
edge represents the method called by the parent call stack. 3.1 Comparing Figure 2: The global design 
of GamePro including its data .ow. tered, visualized, or both, it contains too much information for the 
developer to quickly comprehend. GamePro Analysis is able to .lter, modify, compare and visualize the 
data described in section 2.2. The following sections describe the comparing and one of the modifying 
methods of GamePro. To .nd causes of a frame drop at time t we consider two snapshots St0 and St1 for 
t0 <t = t1, and construct an interval snapshot Id = St0 - St1 . The difference in snapshots St0 - St1 
is de.ned as the vertex-wise difference of all stored attributes resulting in the pro.le tree of the 
run as if it started on time t0 and ended at t1. So, Id contains information on the program behavior 
around the time of the frame drop. After similarly constructing an interval snapshot In of a time interval 
[s0,s1] without a frame drop, Id and In can be compared to .nd the causes of the frame drop by constructing 
I n - I d, where I is an interval snapshot where the attributes are normalized with the length of the 
interval. This comparison can give negative values. Furthermore, it can be used to compare snap­shots 
of two different runs of the same application to check out an optimization or to determine the performance 
of new features. 3.2 Flattening Method information is spread out over multiple vertices, or call stacks, 
of the pro.le tree. The .attening operation creates a graph from the pro.le tree with a vertex for each 
called method and each edge representing all calls from its source method to its target method. The corresponding 
attributes in the call stacks are accu­mulated in the graph vertices. Since the graph vertices are methods 
they can directly be pinpointed in the source code and help the de­veloper to understand the pro.ling 
data. The resulting graph is used in many of the visualizations in section 4.   4 Visualization Pro.le 
data has two major aspects: a structural aspect represented by the pro.le tree, and a temporal aspect 
represented by a sequence of snapshots. To present this data to the user in an orderly manner we use 
visualization. The basic visualization techniques used are presented in section 4.1, and the views built 
on these techniques are presented in section 4.2. 4.1 Techniques GamePro uses adaptations of standard 
visualization techniques to suit the pro.le data. In section 4.1.1 we present a delta stack graph and 
in section 4.1.2 we present an inverted cushion treemap fol­lowed by a split view treemap in section 
4.1.3. 4.1.1 Delta stack graph A delta stack graph is a modi.cation of the standard stack graph, which 
in term is derived from the bar graph. It can be used to visualize some of the temporal aspects of the 
data. A stack graph [Wattenberg 2005] is a graph where multiple func­tions f : V . R are literally stacked 
vertically per input value v. Colors are used to identify the functions, so we use a unique color for 
every shown function. A stack graph can only visualize posi­tive values, while our data can contain negative 
values (e.g. when comparing data sets). We therefore introduce a delta stack graph, in which we show 
two stack graphs: one for all positive values, and one for all negative values. The negative-valued stack 
graph is shown upside down directly beneath the positive stack graph. The concept of a delta stack graph 
is shown in .gure 3. Figure 4 shows a more involved example that shows a clear distinction between posi­tive 
and negative contributions of stacked functions distinguishable by color. Figure 4: A screenshot of 
the delta stack graph in action showing multiple, differently colored functions containing negative values. 
 4.1.2 Inverted cushion treemaps Squari.ed Cushion Treemaps [Bruls et al. 2000] are used to show the 
structural aspects of our data. A treemap [Johnson and Shnei­derman 1991] is a space-.lling method for 
displaying hierarchical data using nested rectangles, where each rectangle represents a ver­tex in the 
hierarchy. The area of a rectangle is proportional to the given size of the tree node it represents. 
The leaves of the tree are required to have a positive size, the size of every other node is the accumulated 
sizes of its children. Cushion treemapping [van Wijk and van de Wetering 1999] uses special shading to 
render rectangles to improve readability of structural information. Our tree data contains a color, a 
label and numeric attributes for every vertex. We can use a treemap to show the distribution of any one 
of these attributes over the pro.le tree. To use a treemap we de.ne the size of a tree vertex as the 
absolute value of one of the attributes. To visualize the distinction between negative and positive attributes 
we introduce cushion tree maps where negative values are drawn using inverted cushions. Where in an ordinary 
cushion treemap a cushion is drawn with intensity 0 = i = 1 per pixel, we replace this intensity for 
positive attributes by i An inverted cushion treemap is 1 2 , and + 2 for negative attribues by shown 
in .gure 6. 1 2 - 2 i . detecting the aforementioned ratio, and identifying the vertices that Figure 
5: A pro.le tree, where the inclusive duration is shown in the vertices. After alteration the tree has 
consistent accumulated sizes: every leaf contains exclusive duration, and all other vertices contain 
inclusive duration. The new leaves have a green color. Figure 6: A tree containing both positive and 
negative leaves. The size of the leaves is their absolute value, and the size of non-leaf vertices are 
the sum of their children s sizes. 4.1.3 Split view treemap If the tree data contains both positive 
and negative values, often the ratio between positive and negative vertices is interesting when pro.ling. 
However, this information is hard to determine from a treemap of a tree containing a mix of positive 
and negative attributes (see .gure 6). We therefore split the tree in two trees and display them separately 
in adjoining treemaps: We .rst make two copies of the original tree and remove all leaves with a positive 
value from one tree, and all leaves with a negative value from the other tree. This operation is illustrated 
in .gure 7 and a split view treemap is shown in .gure 8. The resulting treemap is suitable for quickly 
are signi.cant to the positive or negative part of the tree. Our structural data may still be inconsistent 
with an ordinary treemap, because the size of a vertex may not be equal to the ac­cumulated sizes of 
its children. For example, if we use exclusive method duration as attribute to show in the treemap, the 
size of a vertex then only equals the sum of its children s sizes if its own exclusive duration attribute 
is zero. We make the pro.le tree vertex sizes consistent by adding a child leaf to every vertex with 
children; the newly created leaf has its parent s attribute and color and all attribute data is now available 
in leaves, so the sizes of non-leaf nodes can simply be assigned the ac­cumulated sizes of their children. 
A pro.le tree that has been made consistent in this way, is shown in .gure 5. In the remainder of this 
  4.2 Views This section describes the views used in GamePro. Each view shows different aspects of the 
pro.le data. Figure 9 displays a screenshot of GamePro Analysis showing all its views. The views are 
triggered by several possible selections. First of all a data set needs to be selected, and a second 
data set can be selected for comparing. Secondly, one of the following attributes is selected for visualization 
in, for instance, a treemap: inclusive or exclusive method duration (total and average per call), inclusive 
or exclusive memory usage, number of calls, and standard deviation of call du­rations. Thirdly, a time 
interval can be selected for .ltering the Figure 8: A split view tree map consisting of two adjoining 
treemaps for negative and positive values, respectively. The effect of inverted cushions, and the ratio 
between positive and negative can be clearly seen. data, and possibly an additional time interval needs 
to be selected for comparison of snapshots. And .nally, a set of methods can be selected as a .lter on 
the data set. In the remainder of this section we describe the most important views in GamePro Analysis. 
4.2.1 Method matrix view The method matrix view enables the developer to directly explore the numerical 
attributes in the data sets. The view (see .gure 9) shows a table containing properties of the complete 
set of methods of the selected data set. Every row represents a method and contains the attributes of 
that method. Every column represents one attribute, where the head of the column contains a descriptive 
name. 4.2.2 Time line view The time line view contains a delta stack graph as described in sec­tion 
4.1.1, where time is always represented horizontally. The view shows the frame rate of a run to .nd frame 
drops and the course of attributes of methods in time to identify frame drop causes, and allows selection 
of time intervals for viewing or comparison. Representing the data of snapshots directly would lead to 
a non­decreasing graph (except when showing memory usage, see .gure 11) because snapshots are cumulative. 
This makes it hard for the user to e.g. see if and when method calls suddenly use a signi.­cant amount 
of time. So, whenever an attribute other than memory usage is selected, the time line view depicts the 
difference between consecutive snapshots (see .gure 10). If no method is selected the view shows the 
frame rate (see .gure 10) as a function of time. A large variance in frame rate indicates that the inspected 
software has performance issues: under normal Figure 10: The time line view, showing the frame rate of 
a game. Figure 11: The time line view, showing the behavior over time of the memory usage (in bytes) 
of a single method. If multiple methods are selected, they are represented by their own colors, and the 
selected attributes are stacked. If a method and its children are selected, the children are visualized 
on top of the method. If applicable to the selected attribute, the inclusive at­tributes are shown for 
the children and the exclusive attribute for the method. Methods are stacked in order of inclusive duration 
in the .nal snapshot. An example of this behavior can be seen in .gure 12. The pivot timestamps are shown 
as red vertical lines below the delta stack graph and show by their density where frame drops occur. 
See .gure 11 or 12 for an example.  4.2.3 Method view The method view shows a collection of methods 
in a single level treemap where the sizes are given by the selected attribute and the color by the method. 
If no method is selected the method view shows all methods in the data set with their (exclusive, if 
applicable) attributes; see left hand of .gure 14. This view can be used to quickly .nd expensive methods. 
If a single method m is selected, all methods called by m are shown. This view can be used to navigate 
through the call tree and see the  Figure 14: On the left, the method view showing the exclusive du­ration 
of all methods, showing which methods are time-consuming. On the right, the method view showing the inclusive 
duration dis­tribution of multiple selected methods.  4.2.4 Pro.le tree view The pro.le tree view shows 
the call tree using the treemap of section 4.1.2. The view is used to quickly identify call stacks that 
e.g. use a signi.cant amount of time. A difference computed from snapshots in two data sets or over two 
intervals in the same data set can also be explored in this view. A screenshot of the pro.le tree view 
can be seen in .gure 15, where we see a pro.le tree where one method is selected and inclusive duration 
is used as attribute.  5 Interaction This section describes how the user can manipulate the data, selec­tion 
and the focus of the different views. The data can be modi­.ed before presenting in order to generate 
less visual noise from insigni.cant methods, and to help developers understand the data better. In the 
method matrix view, the user can select an attribute Figure 15: The pro.le tree view, showing inclusive 
duration of the pro.le tree. The selected parts are brighter than others. by selecting a column, select 
methods by selecting rows, and resize columns by dragging the line between two columns. To hide a col­umn, 
the user can simple drag a column until it is invisible. The user can select one or two intervals by 
dragging over the interval that he or she wants to select in the time line view. In the pro.le tree view, 
methods are selected by selecting vertices; in which case the last method mn of their call stack is selected. 
To select chil­dren of the currently selected method, these child methods can be selected in the method 
view. To select its parents, methods in the parents view can be selected. The focus of the matrix view 
can be altered by using the scroll bars. If the user desires to focus on a part of the shown interval, 
it is possible to zoom and pan using the time line view to alter the shown interval.  6 Evaluation Figure 
16: A screenshot of the work-in-progress game Robo Res­cue which contained performance issues. To determine 
the usefulness of GamePro, GamePro was tested by developers of the Kalydo engine [Eximion 2009b]. This 
section contains an evaluation of the .nal version of GamePro. Note that GamePro is currently capable 
of pro.ling not only Kalydo games, but any application developed using Microsoft Visual C++. 6.1 Test 
case This section describes a real-world example of solving a bottleneck and a frame drop in a game using 
GamePro. The working title of the Figure 18: A screenshot of GamePro Analysis showing the time line 
view and the pro.le tree view after selecting interesting inter­vals. We .rst inspect the largest frame 
drop between 0:28 and 0:30 and determine its cause. We thus select the interval containing the frame 
drop, and compare it to the complete run, seen in Figure 18. One method clearly takes way more time than 
average during this inter­val, namely the method called TODEApi::collideBetween-Spaces. This method is 
part of the collision detection of the Ka­lydo engine [Eximion 2009b]. The method matrix view shows that 
the amount of calls to this method is large. The developer investigated this and discovered that the 
problem was caused by a special effect that appears whenever the player picks up an object. This effect 
consists of star-shaped objects that appear, which follow the player until it reaches them and disappear. 
The developer created these object unnecessarily with physical proper­ties, which meant that the physics 
system treated these objects as being physically present, and was therefore checking whether they were 
colliding. 6.1.2 Finding causes of bad performance Next we inspect the overall low and .uctuating performance. 
Firstly we notice in the method matrix view (.gure 17) that the number of calls to several mathematical 
methods is huge. Click­ing on any of these methods, we determine the source of these calls, by .nding 
the most signi.cant parents of these methods, which can be seen as the methods that have the largest 
size in the parents view, and selecting these. This reveals that they originate from the method TSceneGraph::processInside-BoundingVolume, 
which calls a given method for all objects in­side a given bounding volume. How often this method is 
called, de­pends on how the world is subdivided. It turned out that the world was subdivided into far 
too many cells, which resulted in every op­eration on the scene graph being more expensive than needed. 
This is solved by making the subdivision automatic. 6.1.3 Finding causes of memory leaks Robo Rescue 
also exhibited a memory leak, resulting in a notice­able increase in memory usage every minute. To .nd 
this leak, we use .lters to show only the vertices containing memory leaks. One of these methods is shown 
in .gure 19. We can .nd out that the method script::createFlowerParticleSystem calls this method for 
the creation of a temporary array of game ob­jects. Fixing this memory leak turned out to be trivial 
by deleting the temporary array after use. Figure 19: The time line view, showing memory usage of TArray<TGameObject 
*>::operator new. 6.1.4 Results The developer .xed the performance issues and the memory leak, and we 
created a new pro.le for the game. Figure 20 shows Game-Pro presenting that pro.le. Before pro.ling, 
the game ran at about 35 frames per second at 100% cpu usage, which means one frame costs around 28.6 
milliseconds of cpu time. Afterwards, the game ran at 65 frames per second at 14% cpu usage, which means 
one frame costs around 2.2 milliseconds. The performance is almost doubled, and the ef.ciency is increased 
by more than a factor 13. We also notice that the frame rate is much more stable. Also, the memory leak 
is removed. The cause of the frame drop was found in about ten minutes. The cause of the bad performance 
was also found in about ten minutes. We improved the performance of the game greatly by using GamePro 
for only twenty minutes. The cause of the memory leak was found in about .ve minutes. We conclude that 
using GamePro, we can ef.ciently .nd causes of performance and memory issues.  6.2 Comparison to other 
tools We compared GamePro to other existing pro.lers by performance during run-time, usability, and ease 
of use. This is done by pro.l­ing the game Oggo s Oddysey [Eximion 2009a] using different pro­.lers. 
The pro.lers selected were AQtime [AutomatedQA 2009], GlowCode [Electric Software 2009] and Performance 
Validator [Software Veri.cation 2009]. Other pro.ling tools that are speci.c Figure 20: A screenshot 
of GamePro Analysis showing the data set created from the optimized version of Robo Rescue. to games 
(such as those available to game console developers) were not available to the authors at the time of 
writing. We mainly com­pare performance pro.ling, because that is the most powerful fea­ture of GamePro. 
Also, some tools do not contain memory pro.ling capabilities. GamePro has a snapshot feature, which all 
the others lack. This makes it next to impossible to .nd causes of frame drops in the other pro.lers. 
Furthermore, GamePro is the only tested pro­.ler which is able to pro.le script methods. The following 
table compares GamePro to the aforementioned pro.lers while pro.ling the game Oggo s Oddysey. Pro.ler 
name Performance penalty Memory GamePro 1.33 174 MB AQtime 5.3 28 MB GlowCode 1.33 147 MB PV 19 94 MB 
 Clearly, GamePro s performance penalty is at most as small as the fastest pro.ler we tested. Unfortunately, 
the memory usage is sig­ni.cant; which is not a problem unless memory requirements are tight. The memory 
usage is due to the fact that GamePro takes snapshots, while other pro.lers do not. Last but not least, 
Game-Pro is the only pro.ler used for pro.ling Kalydo games since its completion effectively replaced 
AQtime as the tool of choice.  7 Conclusions and future work GamePro Data logger is a fast and powerful 
collector of pro.le data. It can inspect native and script methods as well as memory alloca­tions and 
deallocations with only minor changes in source code. Its snapshot functionality is a powerful way of 
collecting information on frame drops. In GamePro Analysis, developers can .nd bottle­necks, frame drops, 
and their causes quickly. Locations of mem­ory leaks are easily detectable. Data sets are easily comparable 
so changes in performance between runs are also easily detectable. We introduced the use of negative 
values in stack graphs by de.n­ing the delta stack graph, and in treemaps by introducing negative highlights. 
Pro.lers enable developers to locate causes of performance and memory issues. A notable difference between 
GamePro and other tools is the fact that frame drops can be identi.ed; also their causes can be identi.ed 
and the performance penalty is low. Other pro.lers turned out not to be very useful when pro.ling games. 
The conclu­sion is that GamePro is a useful tool and offers many improvements over existing tools when 
pro.ling video games. GamePro is there­fore able to shorten the time-to-launch of a game by enabling 
the developer to quickly improve performance and remove causes of frame drops. Suggestions to enhance 
GamePro include: experiment with bor­dered treemaps, researching the use of textures in the current visu­alizations 
as in [Holten et al. 2006], make a better use of the natural hierarchy of classes in C++, research of 
other visualizations that can show perhaps a combination of methods and time in a concise way, and show 
a screenshot of the inspected application for every snap­shot, so the developer knows what was happening 
in the application at the time we are taking snapshots.  References ANGUS, T., 2009. rtprof. Online 
resource. http://rtprof. sourceforge.net/. AUTOMATEDQA, 2009. AQtime. Online resource. http:// www.automatedqa.com/products/aqtime/. 
BRULS, M., HUIZING, K., AND VAN WIJK, J. J. 2000. Squari.ed treemaps. In Proceedings of Joint Eurographics 
and IEEE TCVG Symposium on Visualization (TCVG 2000). DEELEN, P., VAN HAM, F., HUIZING, C., AND VAN DE 
WETER-ING, H. 2007. Visualization of dynamic program aspects. In Proceedings of the 4th IEEE International 
Workshp on Visualiz­ing Software for Understanding and Analysis. ELECTRIC SOFTWARE,2009.GlowCode.Onlineresource. 
http: //www.glowcode.com/. EXIMION, 2009. Kalydo, an online game console. Online resource. http://www.kalydo.com. 
EXIMION, 2009. The Kalydo engine. Online resource. http: //www.dreamcreateplay.com. HOLTEN, D., VAN WIJK, 
J. J., AND MARTENS, J.-B. 2006. A perceptually based spectral model for isotropic textures. ACM Transactions 
on Applied Perception. JOHNSON, B., AND SHNEIDERMAN, B. 1991. Tree-maps: A space-.lling approach to the 
visualization of hierarchical infor­mation structures. In Proc. of the 2nd international IEEE Visu­alization 
Converence. PANZER, J. 1999. Automatic code instrumentation. C/C++ Users Journal (January). ROZA, M. 
W. A. 2008. GamePro: A high performance visual pro.ler for real-time applications. Master s thesis, Technische 
Universiteit Eindhoven. SOFTWARE VERIFICATION, 2009. Performance Validator. Online resource. http://www.softwareverify.com/cpp/ 
profiler/. VAN WIJK, J., AND VAN DE WETERING, H. 1999. Cushion treemaps: Visualization of hierarchical 
information. In Pro­ceedings IEEE Symposium on Information Visualization (InfoVis 99). WATTENBERG, M. 
2005. Baby names, visualization, and social data analysis. In Proc. IEEE Symp. Information Visualization. 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1581091</article_id>
		<sort_key>180</sort_key>
		<display_label>Pages</display_label>
		<article_publication_date>08-04-2009</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Presence-enhancing real walking user interface for first-person video games]]></title>
		<page_from>111</page_from>
		<page_to>118</page_to>
		<doi_number>10.1145/1581073.1581091</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1581091</url>
		<abstract>
			<par><![CDATA[<p>For most first-person video games it is important that players have a high level of feeling presence in the displayed game environment. Virtual reality (VR) technologies have enormous potential to enhance gameplay since players can experience the game immersively from the perspective of the player's virtual character. However, the VR technology itself, such as tracking devices and cabling, has until recently restricted the ability of users to really walk over long distances.</p> <p>In this paper we introduce a VR-based user interface for presence-enhancing gameplay with which players can explore the game environment in the most natural way, i. e., by real walking. While the player walks through the virtual game environment, we guide him/her on a physical path which is different from the virtual path and fits into the VR laboratory space. In order to further increase the VR experience, we introduce the concept of <i>transitional environments</i>. Such a transitional environment is a virtual replica of the laboratory environment, where the VR experience starts and which enables a gradual transition to the game environment. We have quantified how much humans can unknowingly be redirected and whether or not a gradual transition to a first-person game via a transitional environment increases the user's sense of presence.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[presence]]></kw>
			<kw><![CDATA[virtual reality]]></kw>
			<kw><![CDATA[walking interface]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1570341</person_id>
				<author_profile_id><![CDATA[81313483772]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Frank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Steinicke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of M&#252;nster, M&#252;nster, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570342</person_id>
				<author_profile_id><![CDATA[81317491340]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gerd]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bruder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of M&#252;nster, M&#252;nster, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570343</person_id>
				<author_profile_id><![CDATA[81100061158]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Klaus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hinrichs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of M&#252;nster, M&#252;nster, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570344</person_id>
				<author_profile_id><![CDATA[81100501447]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Anthony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Steed]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University College London, London, United Kingdom]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>931187</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Arthur, K. W., Arthur, K. W., and Arthur, K. W. 2000. Effects of field of view on performance with head-mounted displays. Tech. rep., Dissertation Abstracts International.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Barfield, W., and Hendrix, C. 1995. The effect of update rate on the sense of presence within virtual environments. <i>Virtual Reality: The Journal of the Virtual Reality Society 1</i>, 1, 3--16.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Berthoz, A. 2000. <i>The Brain's Sense of Movement</i>. Harvard University Press, Cambridge, Massachusetts.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bertin, R. J., Isra&amp;#235;l, I., and Lappe, M. 2000. Perception of two-dimensional, simulated ego-motion trajectories from optic flow. <i>Vis. Res. 40</i>, 21, 2951--2971.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>835876</ref_obj_id>
				<ref_obj_pid>580130</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Bouguila, L., and Sato, M. 2002. Virtual Locomotion System for Large-Scale Virtual Environment. In <i>Proceedings of Virtual Reality</i>, IEEE, 291--292.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1544265</ref_obj_id>
				<ref_obj_pid>1544197</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Bruder, G., Steinicke, F., and Hinrichs, K. H. 2009. Arch-explore: A natural user interface for immersive architectural walkthroughs. In <i>Proceedings of IEEE Symposium on 3D User Interfaces (3DUI)</i>, IEEE Press, 75--82. accepted for publication.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Dichgans, J., and Brandt, T. 1978. Visual vestibular interaction: Effects on self-motion perception and postural control. In <i>Perception. Handbook of Sensory Physiology, Vol. 8</i>, Springer, Berlin, Heidelberg, New York, R. Held, H. W. Leibowitz, and H. L. Teuber, Eds., 755--804.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>835733</ref_obj_id>
				<ref_obj_pid>554230</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Dinh, H. Q., Walker, N., Song, C., Kobayashi, A., and Hodges, L. F. 1999. Evaluating the importance of multi-sensory input on memory and the sense of presence in virtual environments. <i>Proceedings of the IEEE Virtual Reality</i>, 222.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Ellis, S. R. 1997. Presence of mind: A reaction to thomas sheridan's "further musings on the psychophysics of presence". In <i>Presence: Teleoperators and Virtual Environments</i>, MIT Press, vol. 5, 247--259.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1228637</ref_obj_id>
				<ref_obj_pid>1228627</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Friedman, D., Brogni, A., Antley, A., Guger, C., Antley, A., Steed, A., and Slater, M. 2006. Sharing and analyzing data from presence experiments. <i>Presence: Teleoperators and Virtual Environments 15</i>, 5, 599--610.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2153640</ref_obj_id>
				<ref_obj_pid>2153634</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Groenda, H., Nowak, F., R&amp;#246;ssler, P., and Hanebeck, U. D. 2005. Telepresence Techniques for Controlling Avatar Motion in First Person Games. In <i>Intelligent Technologies for Interactive Entertainment (INTETAIN 2005)</i>, 44--53.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1246713</ref_obj_id>
				<ref_obj_pid>1246708</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[IJsselsteijn, W., de Ridder, H., Freeman, J., Avons, S. E., and Bouwhuis, D. 2001. Effects of stereoscopic presentation, image motion, and screen size on subjective and objective corroborative measures of presence. <i>Presence: Teleoperators and Virtual Environments 3</i>, 10, 298--311.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Insko, B., Meehan, M., Whitton, M., and Brooks, F. 2001. Passive Haptics Significantly Enhances Virtual Environments. In <i>Proceedings of 4th Annual Presence Workshop</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Interrante, V., Riesand, B., and Anderson, L. 2007. Seven League Boots: A New Metaphor for Augmented Locomotion through Moderately Large Scale Immersive Virtual Environments. In <i>Proceedings of Symposium on 3D User Interfaces</i>, IEEE, 167--170.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1179162</ref_obj_id>
				<ref_obj_pid>1179133</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Iwata, H., Hiroaki, Y., and Tomioka, H. 2006. Powered Shoes. <i>SIGGRAPH 2006 Emerging Technologies</i>, 28.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Jenkins, H. 2004. Game design as narrative architecture. <i>First Person: New Media as Story, Performance, Game MIT Press</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1152451</ref_obj_id>
				<ref_obj_pid>1152399</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Kohli, L., Burns, E., Miller, D., and Fuchs, H. 2005. Combining Passive Haptics with Redirected Walking. In <i>Proceedings of Conference on Augmented Tele-Existence</i>, ACM, vol. 157, 253--254.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Lappe, M., Bremmer, F., and van den Berg, A. V. 1999. Perception of self-motion from visual flow. <i>Trends. Cogn. Sci. 3</i>, 9, 329--336.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>515391</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Laramee, F. D. 2002. <i>Game Design Perspectives</i>. MA: Charles River Media.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>835964</ref_obj_id>
				<ref_obj_pid>832289</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Meehan, M., and Razzaque, S. 2003. Effect of latency on presence in stressful virtual environments. In <i>Proceedings of the IEEE Virtual Reality 2003</i>, 141--148.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566630</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Meehan, M., Insko, B., Whitton, M., and Brooks, F. P. 2002. Physiological measures of presence in stressful virtual environments. <i>ACM Transactions on Graphics 21</i>, 645--652.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>551862</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Murray, J. H. 1997. <i>Hamlet on the Holodeck The Future of Narrative in Cyberspace</i>. MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Peck, T., Whitton, M., and Fuchs, H. 2008. Evaluation of reorientation techniques for walking in large virtual environments. In <i>Proceedings of Virtual Reality</i>, IEEE, 121--128.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1124097</ref_obj_id>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Razzaque, S. 2005. <i>Redirected Walking</i>. PhD thesis, University of North Carolina, Chapel Hill.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1215723</ref_obj_id>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Salen, K., and Zimmerman, E. 2004. <i>Rules of Play, Game Design Fundamentals</i>. MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Sanchez-Vives, M. V., and Slater, M. 2005. From presence to consciousness through virtual reality. <i>Nature Reviews Neuroscience 6</i>, 332--339.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2167319</ref_obj_id>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Schell, J. 2008. <i>The Art of Game Design: A book of lenses</i>. Morgan Kaufmann Pub.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Slater, M., Usoh, M., and Steed, A. 1994. Depth of presence in immersive virtual environments. In <i>Presence: Teleoperators and Virtual Environments</i>, vol. 3, 130--144.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>210084</ref_obj_id>
				<ref_obj_pid>210079</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Slater, M., Usoh, M., and Steed, A. 1995. Taking steps: The influence of a walking metaphor on presence in virtual reality. In <i>ACM Transactions on Computer-Human Interaction (TOCHI)</i>, vol. 2, 201--219.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Slater, M., Steed, A., McCarthy, J., and Marinelli, F. 1998a. The influence of body movement on presence in virtual environments. <i>Human Factors: The Journal of the Human Factors and Ergonomics Society 40</i>, 3, 469--477.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Slater, M., Steed, A., McCarthy, J., and Marinelli, F. 1998b. The virtual ante-room: Assessing presence through expectation and surprise. In <i>Eurographics Workshop on Virtual Environments</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Steed, A., Benford, S., Dalton, N., Greenhalgh, C., MacColl, I., Randell, C., and Schn&amp;#228;delbach, H. 2002. Mixed-reality interfaces to immersive projection systems. In <i>Immersive Projection Technology Workshop</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1450611</ref_obj_id>
				<ref_obj_pid>1450579</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Steinicke, F., Bruder, G., Jerald, J., Frenz, H., and Lappe, M. 2008. Analyses of human sensitivity to redirected walking. In <i>ACM Symposium on Virtual Reality Software and Technology (VRST)</i>, ACM Press, 149--156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1549869</ref_obj_id>
				<ref_obj_pid>1549820</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Steinicke, F., Steed, G. B. A., Hinrichs, K., and Gerlach, A. 2009. Does a gradual transition to the virtual world increase presence? In <i>Proceedings of the IEEE Virtual Reality (VR2009)</i>, IEEE Press, 203--210.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>836073</ref_obj_id>
				<ref_obj_pid>523977</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Uno, S., and Slater, M. 1997. The sensitivity of presence to collision response. <i>Virtual Reality Annual International Symposium (VRAIS)</i>, 95.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311589</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Usoh, M., Arthur, K., Whitton, M., Bastos, R., Steed, A., Slater, M., and Brooks, F. 1999. Walking &amp;gt; Walking-in-Place &amp;gt; Flying, in Virtual Environments. In <i>International Conference on Computer Graphics and Interactive Techniques (SIGGRAPH)</i>, ACM, 359--364.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1246906</ref_obj_id>
				<ref_obj_pid>1246899</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Usoh, M., Catena, E., Arman, S., and Slater, M. 1999. Using presence questionaires in reality. <i>Presence: Teleoperator in Virtual Environments 9</i>, 5, 497--503.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Usoh, M., Arthur, K., Whitton, M., Bastos, R., Steed, A., Brooks, F., and Slater, M. 2006. The visual cliff revisited: A virtual presence study on locomotion. In <i>International Workshop on Presence</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Vinayagamoorthy, V., Brogni, A., Gillies, M., Slater, M., and Steed., A. 2004. An investigation of presence response across variations in visual realism. In <i>7th International Conference on Presence</i>, 148--155.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1246762</ref_obj_id>
				<ref_obj_pid>1246761</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Witmer, B. G., and Singer, M. J. 1998. Measuring presence in virtual environments: A presence questionnaire. In <i>Presence</i>, vol. 7, 225--240.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Presence-Enhancing Real Walking User Interface for First-Person Video Games Frank Steinicke* , Gerd 
Bruder , Klaus Hinrichs Visualization and Computer Graphics Research Group Department of Computer Science 
University of M¨unster Einsteinstraße 62, 48149 M¨unster, Germany Abstract For most .rst-person video 
games it is important that players have a high level of feeling presence in the displayed game environment. 
Virtual reality (VR) technologies have enormous potential to en­hance gameplay since players can experience 
the game immersively from the perspective of the player s virtual character. However, the VR technology 
itself, such as tracking devices and cabling, has un­til recently restricted the ability of users to 
really walk over long distances. In this paper we introduce a VR-based user interface for presence­enhancing 
gameplay with which players can explore the game en­vironment in the most natural way, i. e., by real 
walking. While the player walks through the virtual game environment, we guide him/her on a physical 
path which is different from the virtual path and .ts into the VR laboratory space. In order to further 
increase the VR experience, we introduce the concept of transitional envi­ronments. Such a transitional 
environment is a virtual replica of the laboratory environment, where the VR experience starts and which 
enables a gradual transition to the game environment. We have quanti.ed how much humans can unknowingly 
be redirected and whether or not a gradual transition to a .rst-person game via a transitional environment 
increases the user s sense of presence. CR Categories: H.5.1 [Information Interfaces and Presentation]: 
Multimedia Information Systems Arti.cial, augmented, and vir­tual realities; Keywords: Virtual reality, 
walking interface, presence 1 Introduction In recent years video games have become more and more popu­lar 
for entertainment as well as serious gaming such as advergames or educational games. In the context of 
video games, .rst-person refers to a graphical perspective rendered from the viewpoint of the player 
s character [Jenkins 2004]. Video games often utilize so­phisticated rendering technology as well as 
multimodal interaction *e-mail: fsteini@uni-muenster.de e-mail: gerd.bruder@uni-muenster.de e-mail: khh@uni-muenster.de 
§e-mail: A.Steed@cs.ucl.ac.uk Copyright &#38;#169; 2009 by the Association for Computing Machinery, 
Inc. Permission to make digital or hard copies of part or all of this work for personal or classroom 
use is granted without fee provided that copies are not made or distributed for commercial advantage 
and that copies bear this notice and the full citation on the first page. Copyrights for components of 
this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, 
to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or 
a fee. Request permissions from Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org. 
Sandbox 2009, New Orleans, Louisiana, August 4 6, 2009. &#38;#169; 2009 ACM 978-1-60558-514-7/09/0008 
$10.00 Anthony Steed§ Virtual Environments and Computer Graphics Group Department of Computer Science 
University College London Gower Street, London, WC1E 6BT, United Kingdom and information such as surround-sound 
audio feedback and hap­tic peripherals, which support vibration or force feedback. Usually, mouse, keyboard, 
joystick or game controller are used as interface devices. In video game terminology, the term gameplay 
is used to describe the overall experience of playing the game excluding fac­tors like graphics and sound. 
It has been claimed that one way to enhance gameplay is to immerse users within the virtual world, so 
that they have a high sense of presence in the environment [Murray 1997]. Since the beginning of the 
early 90 s developments in technology and software of the VR and games industry are strongly related 
and mutually affect each other [Murray 1997]. Since VR systems are able to present the game world from 
the viewpoint of the gamer s character, they have great potential as an enabling technology for immersive 
gameplay. With immersive VR technologies natural 3D user interfaces and navigation by real walking can 
be implemented easily. For example, players can immerse in the game environment by wearing head-mounted 
displays (HMDs) while tracking systems provide information about the position/orientation of the user 
and the input devices. Although VR research has gone through a long history of re.nement, there are still 
only few commercially avail­able games which make use of VR. Usually, before players can use VR technology 
they need to be instructed and trained with the con­trol and input devices, for instance, they need to 
understand how 3D hand-based input devices are used for virtual walking metaphors. Furthermore, real 
walking has been shown to be a more natural lo­comotion technique than any other navigation metaphor, 
including virtual .ying or navigation based on walk-like gestures [Usoh et al. 1999a]. An obvious approach 
to implement real walking is to transfer user s tracked head movements to changes of the virtual camera 
in the VE by means of a one-to-one mapping, i. e., a one meter movement in the tracked space is mapped 
to a one meter movement of the virtual camera in the corresponding direction. This technique has the 
drawback that the users movements are restricted by a limited range of the tracking sensors and a rather 
small workspace in the real world. Therefore, concepts for virtual locomotion are needed that enable 
walking over large distances in the virtual world while remaining within a relatively small space in 
the real world. Vari­ous prototypes of interface devices have been developed to prevent a displacement 
in the real world [Bouguila and Sato 2002; Iwata et al. 2006]. Although these hardware systems represent 
enormous technological achievements, they are still very expensive and will not be generally accessible 
in the foreseeable future. It is known from perceptive psychology that vision often dominates proprioceptive 
and vestibular sensation when they disagree [Dich­gans and Brandt 1978; Berthoz 2000]. When humans can 
use only vision to judge their motion through a virtual scene they can suc­cessfully estimate their momentary 
direction of self-motion but are not as well in perceiving their paths of travel [Lappe et al. 1999; 
Bertin et al. 2000]. Therefore, since users tend to unwittingly com­pensate for small inconsistencies 
during walking it is possible to guide them along paths in the real world which differ from the path 
perceived in the virtual world. For example, intentionally rotat­ing the virtual camera to one side causes 
the user to unknowingly compensate by walking a circular arc in the opposite direction. This redirected 
walking enables users to explore a virtual world that is considerably larger than the tracked working 
space by guid­ing the user on a physical path which .ts into the VR laboratory space [Steinicke et al. 
2008; Razzaque 2005] (see Figure 1). Within the academic work on immersive virtual reality, presence 
is an important metric that is used to compare systems. Whilst various de.nitions of presence exist, 
we use the de.nition of Sanchez-Vive and Slater, where presence leads to the participant behaving as 
if the virtual world represented real situations [Sanchez-Vives and Slater 2005]. According to [Slater 
et al. 1998b] users might feel a higher sense of presence in the VE if it is presented as persistent 
space that can be entered and exited, and moreover, if the transfer into the VE involves some notion 
of travel or detachment from the real world. In theme parks a similar concept is used successfully. For 
example, prior to a ride in a roller coaster passengers have to cross dungeons or fairy tale worlds to 
mentally prepare for the experi­ence. Therefore, in order to improve a VR-based game experience it seems 
reasonable to provide users with a virtual replica of their real environment (usually the laboratory) 
such that they can accus­tom themselves to walking in an immersive VR system [Steinicke et al. 2009]. 
After a certain time period, the user may enter the real game world, for example via a virtual door. 
Although it seems to be a promising approach to use a gradual transition be­tween real and game world, 
it has not been veri.ed if the usage of such metaphors as introduced by such a transitional environment 
increases the sense of presence in a game context. The remainder of this paper is structured as follows. 
Section 2 re­views work related to our paper. In Section 3 we propose a new user interface, which allows 
players to travel without restriction through a geospatial game environment by means of redirected walking. 
Section 4 describes the experiment that we have conducted to quan­tify how much users can be redirected. 
Section 5 explains the evalu­ation in which we have analyzed whether a transitional environment increases 
the user s sense of presence or not. Section 6 discusses the results and Section 7 concludes the paper. 
 2 Related Work In 1991 Virtuality released a VR gaming system called the 1000CS, which was an immersive 
HMD platform with a tracked 3D joy­stick. During the 90 s several VR games have been proposed for different 
VR setups, e. g., Nintendo s Virtual Boy. Another exam­ple of the connection between VR and gaming is 
DisneyQuest pro­viding an indoor interactive theme park which contains several VR attractions [Schell 
2008]. Furthermore, the recent success of input devices such as Nintendo s Wii or Apple s iPhone for 
3D gaming underlines some common themes of the VR and gaming domain. These gaming and VR devices, exploit 
the human sense of proprio­ception, that is body state awareness, so that certain types of action can 
be performed using mimicry of real actions. Indeed, it is dif.cult to create a high-.delity VR experience, 
primar­ily because of technical limitations. These limitations often inter­fere with even basic requirements 
like support for omni-directional and unlimited walking. Therefore, locomotion in a large virtual world 
by real walking through a limited physical setup is in the fo­cus of many research groups. As mentioned 
above redirected walk­ing [Razzaque 2005] provides a promising solution to the problem of limited tracking 
space and the challenge of providing users with the ability to explore an IVE by walking. With this approach 
the user is redirected via manipulations applied to the displayed scene, causing users to unknowingly 
compensate scene motion by repo­sitioning and/or reorienting themselves. Different approaches to Figure 
1: Redirected walking scenario: a user walks in the real environment a different path with a different 
length in comparison to the perceived path in the virtual world. redirect a user in an IVE have been 
proposed. An obvious approach is to scale translational movements, for example, to cover a vir­tual distance 
that is larger than the distance walked in the physical space [Interrante et al. 2007]. With most reorientation 
techniques, the virtual world is imperceptibly rotated around the center of a sta­tionary user until 
he/she is oriented in such a way that no physical obstacles are in front of him/her [Peck et al. 2008; 
Razzaque 2005; Kohli et al. 2005]. Then the user can continue to walk in the desired virtual direction. 
Alternatively, reorientation can also be applied while the user walks [Groenda et al. 2005; Razzaque 
2005]. For instance, if the user wants to walk straight ahead for a long distance in the virtual world, 
small rotations of the camera redirect him/her to walk unconsciously an arc in the opposite direction 
in the real world. When redirecting a user, the visual sensation is consistent with motion in the IVE, 
but proprioceptive sensation re.ects mo­tion in the physical world. However, if the induced manipulations 
are small enough, the user has the impression of being able to walk in the virtual world in any direction 
without restrictions [Steinicke et al. 2008; Peck et al. 2008; Razzaque 2005]. Quanti.ed analyses of 
how much redirection is possible in the context of game design have not been undertaken. Walking is not 
only the most natural way of traveling, it is a more presence-enhancing metaphor than other navigation 
metaphors in­cluding .ying or navigating by walk-like gestures [Slater et al. 1995; Usoh et al. 1999a]. 
There has been vigorous debate about how to best measure presence [Ellis 1997; Bar.eld and Hendrix 1995; 
Friedman et al. 2006; Meehan et al. 2002]. Due to the sub­jective nature of presence, it sounds reasonable 
to measure presence with respect to a subject s self-reported sense of presence. For this purpose a few 
standard questionnaires are available [Witmer and Singer 1998; Usoh et al. 1999b; Slater et al. 1995]. 
In the past different approaches have been presented and examined in how far they increase a user s sense 
of presence. Obviously, pres­ence can be supported by exclusion of real world cues since these might 
interfere or be inconsistent with the presented VE. Further­more, presence can be enhanced by incorporating 
a virtual repre­sentation of the user into the environment (a virtual body ) [Slater et al. 1994; Slater 
et al. 1995], especially providing actual limb motion [Usoh et al. 2006]. In addition, multimodal feedback 
in a VE increases the sense of presence, in particular if not only haptic and tactile feedback [Insko 
et al. 2001] but also audio and olfactory stimuli correspond to events in the VE [Dinh et al. 1999]. 
Moreover, properties of the visual display have an impact on the user s sense of presence [Vinayagamoorthy 
et al. 2004], such as a wide .eld of view [Arthur et al. 2000], realistic physical simulations [Uno and 
Slater 1997], stereoscopic display [IJsselsteijn et al. 2001], low la­tency [Meehan and Razzaque 2003; 
Bar.eld and Hendrix 1995] and also dynamic shadows of objects in a virtual environment con­  (a) (b) 
(c) (d) Figure 2: Images from a presence-enhancing game scenario: (a) photo of the laboratory environment, 
sceenshots of (b) the transitional environment, (c) the transitional environment with a portal to the 
actual game environment, and (d) the immersive game with a portal back to the transitional environment. 
tribute to a user s sense of presence. Apart from the intrinsic properties of the display, some work 
has demonstrated that the staging of the experience and introduction of a user to the system can impact 
her subsequent sense of presence. Some projects have already used different concepts to provide a seamless 
transition from the real to the virtual world and in the op­posite direction [Slater et al. 1998a]. Sometimes 
curtains in front of the laboratory are used, which block out the view to the physical surrounding [Steed 
et al. 2002]. Hence a con.ict between the pre­sented virtual world and the real surrounding is prevented. 
Slater et al. have proposed different virtual presentations in an HMD which transfer a user to another 
virtual world, when a HMD is put on [Slater et al. 1994]. After taking off the last virtual HMD, the 
user is transferred to the VE from where she was transferred before. In the games literature, the term 
immersion is used in a way similar to the way presence is used in the VR .eld. In the VR .eld, im­mersion 
tends to refer to a technology, whereas in the games .eld, immersion tends to refer to the ability of 
players to suspense disbe­lief and accepts what they perceive as reality [Laramee 2002]. The view that 
games necessarily require immersion has been called the immersive-fallacy by Salen &#38; Zimmerman [Salen 
and Zimmerman 2004], who note that games take many forms that are not necessar­ily technologically immersive. 
Never the less, immersive VR does lend itself well to novel types of interface, and novel forms of game, 
so no doubt there will be more cross-over in the future.  3 Presence-Enhancing Geocaching Game Geocaching 
is a high-tech treasure hunting game played throughout the world by adventure seekers equipped with GPS 
devices. The ba­sic idea is to locate hidden containers, called geocaches 1, outdoors and then share 
your experiences online. We developed an immersive version of the real-world geocaching for which we 
have integrated redirected walking and transitional environments. 3.1 Immersive Game Interface Setup 
The physical environment within which the player moves during the game is a 10m×7m darkened laboratory 
room. The players wear an HMD (3DVisor Z800, 800x600@60 Hz, 40. diagonal .eld of view (FoV)) on which 
the virtual game is rendered. On top of the HMD an infrared LED is .xed (see Figure 1). We tracked the 
position of this LED within the room with an active optical tracking sys­tem (Precise Position Tracking 
of World Viz), which provides sub­ 1 There are more than 700,000 active geocaches around the world. millimeter 
precision and sub-centimeter accuracy. The update rate was 60 Hz providing real-time positional data 
of the active mark­ers. For three degrees of freedom (DoF) orientation tracking we use an InertiaCube 
2 (InterSense) with an update rate of 180 Hz. The InertiaCube is also .xed on top of the HMD. In the 
experiments we used an Intel computer (host) with dual-core processors, 4 GB of main memory and an nVidia 
GeForce 8800 for system control and logging purposes. Players are equipped with an HMD back­pack consisting 
of a laptop PC with a GeForce 7700 Go graphics card (see Figure 1). A Nintendo Wii remote controller 
serves as input device and acoustic feedback is displayed via headphones at­tached to the HMD. The game 
is rendered using OpenGL and our own software. During the game the room is entirely darkened in order 
to reduce the user s perception of the real world. All comput­ers, including the laptop on the back of 
the user, are equipped with wireless LAN adapters. The entire weight of the backpack is about 8 kg which 
is quite heavy. However, no wires disturb the immer­sion and no assistant must walk beside the user to 
keep an eye on the wires.  3.2 Transitional Environment Before the actual game starts, the user enters 
a virtual replica of the real environment, i. e., a realistic 3D model of the laboratory. This is not 
a novel concept and has been introduced by Slater et al. [Slater et al. 1998b] After putting on the HMD, 
users see the 3D model of the laboratory space which is used as transitional en­vironment (see Figure 
2(a) and (b)). Since the virtual model is a one-to-one replica of the real laboratory users could walk 
around and touch virtual respectively real objects such as walls, doors, or cabinets. Within this transitional 
environment users can open a context menu in analogy to a menu in a game by pressing a but­ton on the 
Wii remote device. Via the menu subjects can mod­ify the game settings such as sound volume, enabling/disabling 
of head-up-displays etc. Furthmore, users can open a virtual portal to the actual game environment. Such 
a virtual portal is displayed on one wall of the laboratory space (see Figure 2(c)). Players can see 
the 3D game environment through the semi-transparent virtual portal. Now, when passing the virtual portal 
users are transferred to the game environment and can play the game. As mentioned above the virtual wall 
onto which the virtual portal is displayed is located in correspondence with the physical wall, which 
would prevent a walk-through. Therefore, we apply redirected walking techniques, in particular motion 
compression approaches (cf. Sec­tion 3.3), which allow users to walk through the virtual portal with 
no obstacle obscuring the path in the physical world [Bruder et al. 2009; Steinicke et al. 2009]. When 
players want to return to the virtual replica, for example to change the settings of the game, they can 
open a virtual portal back to the transitional environment by pressing a button on the Wii re­mote device. 
Figure 2(d) illustrates such a virtual portal in the game environment. During the game players can switch 
between virtual game environment and virtual replica room at any time by means of virtual portals.  
3.3 Virtual Portals In order to transfer players from the transitional to the game en­vironment such 
that they believe to be in a new environment, we need a plausible way of travel. Inspired by TV series 
or movies, for instance, Stargate, but also 3D games such as the .rst-person ac­tion video game Portal 
by Valve Corporation (released in 2007), we have decided to use the concepts of virtual portals. For 
our setup we want to provide a compelling visualization and appearance of the portals that indicate the 
way from the transitional environment to the virtual world and vice versa. Therefore, we visualize the 
portal on one of the walls of the transitional environment instead of visualizing them as .oating objects 
within the room. As illus­trated in Figure 2(c) the 3D game environment is visible through the semi-transparent 
portal. The portal can be placed in any loca­tion in the virtual world. By masking the area of the portal 
in the OpenGL stencil buffer, a different virtual world can be rendered in the fragments showing the 
portal. Clipping against the portal is re­quired to prevent that objects of the world behind the portal 
stick out of the portal. Optionally, we blend a bumping effect to indicate that a virtual world with 
a different context is behind the portal (see Figure 2(d)). As mentioned above, at the beginning of the 
experiment physical walls of the lab and virtual walls of the transitional environment are aligned in 
correspondence. Hence, a virtual portal on the wall could not be entered without additional effort. Therefore, 
after the user has indicated to open a portal in the transitional environment, for example by pressing 
a button on an input device, we apply motion compression approaches. We scaled the movements with a factor 
of 1.2. Thus, one meter in the physical space is mapped to 1.2 meters in the transitional world. According 
to Steinicke et al. [Steinicke et al. 2008] such a manipulation cannot be noticed reliably by a walking 
user. Hence, when subjects move to the virtual portal, they have only walked 80% of the distance in the 
physical world and are still far away from the laboratory wall. Now the user can pass the portal, where 
a physical wall has been before. Thus we are able to display a portal on the virtual wall in the transitional 
environment through which subjects can enter the virtual world. When subjects re-enter the transitional 
environment via the portal, we apply the same concept again. When the player passes the virtual portal 
of the virtual replica room, she enters an urban 3D model (see Figure 2(c) and (d)). Figure 3 shows a 
typical view of a player in the virtual geocaching game. We use a geo-referenced 3D model of our local 
city in which users can walk virtually. The objective of the virtual geocaching game is to go to preassigned 
GPS positions by means of walking. In order to support the user to orient in the virtual city model, 
a virtual compass and the user s current position can be rendered on a head­up-display to indicate the 
user s virtual position and orientation in real-time. In order to avoid problems of collision with walls 
in the physical space, we have to redirect players (cf. Section 4).  4 Psychophysical Walking Experiment 
The objective of this experiment is to support omni-directional and unlimited walking for .rst-person 
games, in particular games with a geospatial context. As mentioned in Section 1 we need to redirect players 
in the laboratory environment on a circular arc (while they Figure 3: The view of a player in the virtual 
geocaching game. The application was also used for the psychophysical experiment and the evaluation of 
transitional environments. believe to walk straight) in order to keep them in the tracked space. In order 
to ensure that players do not observe that they get manip­ulated, we have performed a psychophysical 
experiment in which subjects had to discriminate the walk direction, i. e., subjects had to judge whether 
the walked curve is bent to the left or to the right. The visual stimulus we used in our experiment was 
generated by the geocaching game (see Figure 3). 9 male and 3 female (age 19-50, Ø : 25.54) subjects 
participated in the study. Most subjects are students or members of the depart­ments (computer science, 
mathematics, psychology, geoinformat­ics, and physics). All have normal or corrected to normal vision; 
8 wear glasses or contact lenses. 2 have no game experience, 4 have some, and 6 have much game experience. 
 4.1 Material and Methods of Discrimination Experi­ment In contrast to [Steinicke et al. 2008] we use 
the method of constant stimuli in a two-alternative forced-choice (2AFC) task rather than simple yes/no-judgements 
which potentially involve bias. In the method of constant stimului, the applied gains are not related 
from one trial to the next, but presented randomly and uniformly dis­tributed. The subject choses between 
one of two possible responses, e. g. Was the physical path curved to the left or to the right ; re­sponses 
like I can t tell. were not allowed. In this version, when the subject cannot detect the signal, he/she 
must guess, and will be correct on average in 50% of the trials. The gain at which the subject responds 
left in 50% of the trials is taken as the point of subjective equality (PSE), at which the subject perceives 
the phys­ical and the virtual movement as identical. As the gain decreases or increases from this value 
the ability of the subject to detect the difference between physical and virtual movement increases. 
We de.ne the detection threshold (DTs) for gains larger than the PSE to be the value of the gain at which 
the subject has 75% prob­ability of choosing the right answer correctly and the detection threshold for 
gains smaller than the PSE to be the value of the gain at which the subject chooses the left response 
in only 25% of tri­als (since the correct response right was then chosen in 75% of the trails). In this 
experiment we analyze sensitivity to curvature gains which enforce the user to walk a curve in order 
to stay on a straight path. A curvature gain gC[w] applied to a virtual direction of walk w de­notes 
the resulting bend of the path in the real world. For example, when the user moves straight ahead, a 
curvature gain that causes   -p-/ 30 -p-/ 36 -p-/ 45 -p-/ 60 -p-/ 90 -p /180 0 p /180 p / 90 p / 60 
p / 45 p / 36 p / 30 gC[w] Figure 4: Pooled results of the discrimination of path curva­ture. The x-axis 
shows the applied curvature gain which bends the walked path either to the left (gC[w] < 0) or to the 
right (gC[w] > 0), the y-axis shows the proportion of subjects left responses. reasonably small iterative 
camera rotations to one side forces the user to walk along a curve in the opposite direction in order 
to stay on a straight path in the virtual world. This curve is determined by a circular arc with radius 
r, and gC := 1 . In case no curvature is r applied it is r =8 . gC =0, whereas if the curvature causes 
the user to rotate by 90. clockwise after p 2 meters the user has covered a quarter circle with radius 
r =1 . gC =1. On the same level as the subject s eye we added a green dot to the scene, which turned 
red when the subjects had walked 7m to­wards it. While the subjects walked, we rotated the scene to ei­ther 
side with a velocity linked to the subject s movement velocity. The scene rotated by 5, 10,15,20 and 
30 degrees after 5m walking distance. This corresponds to a curvature radius of approximately 57.3, 28.65, 
19.10, 14.32 and 9.55m respectively. Hence, the cur­vature gains were given by gC[w]=± p 45 ,± p 90 ,± 
p . 30 , ± p 60 ,± p 180 We presented the gains each 8 times in a randomized order. The rotation of the 
virtual camera started after subjects had walked the 2m start-up phase. After subjects walked a total 
distance of 7m in the virtual world, the screen turned white and the question of the discrimination task 
appeared. The subject s task was to de­cide whether the physical path was curved to the left or to the 
right by pressing the corresponding left or right button on the Wii controller. Then the subject was 
guided to a new starting position.  4.2 Results of Discrimination Experiment Figure 4 shows the mean 
probability for the response that the phys­ical path was curved to the left against the curvature gains 
gC[w]. Error bars indicate the standard error. The PSE for the pooled data is p =0.002. At this PSE the 
subjects have in fact walked a 1423 circular arc with a radius of 453.14m, and rotated by less than one 
degree after 5m. For individual subjects the PSE varied be­tween p =-0.019 and p =0.052 (8 subjects with 
PSE -162.51 60.90 above or equal, 3 less than 0.0022). The detection thresholds are given by the stimulus 
intensity at which subjects correctly detect the bending of the path 75% of the time. Detection thresholds 
were gC[w]=±0.045, i. e., gC[w]=- 69p .23 for leftward bended paths and gC[w]=+69p .23 for rightward 
bended paths. At these threshold val­ues, subjects walked physically a circular arc with a radius of 
ap­proximately 22.03m. Within this range of these detection thresh­olds subjects cannot estimate reliably 
if they walk straight or a curve. In summary, if the laboratory space covers an area of approximately 
40m ×40m, it becomes possible to guide the user on a circular arc in the physical world, while the user 
walks straight without restric­tion in the VE. Hence, a physical environment within an amusement park 
can be constructed where several gamers can walk unlimitedly in any directions as long as they are redirected 
according to the re­sults mentioned above.  5 Evaluation of Transitional Environment Now that we are 
able to allow players to walk through an arbitrary environment we wanted to verify whether or not a gradual 
transition to the virtual game environment further increase the user s sense of presence. Therefore, 
we have performed an evaluation based on the subject s self-reported sense of presence according to the 
SUS presence questionnaire. 7 male and 3 female subjects (age 23-53, Ø: 32.6) participated. 3 subjects 
had no game experience, 4 subjects had some, and 3 sub­jects had much game experience. Four of the subjects 
had experi­ence with walking in VR environments using HMD setups. Sub­jects were allowed to take breaks 
at any time. Some subjects ob­tained class credit for their participation. The total time per sub­ject 
including pre-questionnaire, instructions, training, experiment, breaks, and debrie.ng took 2 hour and 
was performed within two days. Half of the subjects have performed the experiment .rst with and then 
without transitional environment, whereas the others have per­formed the experiment in reversed order. 
 5.1 Material and Methods for Evaluation of Transi­tional Environments In order to verify if a transitional 
environment increases the sub­ject s sense of presence we have conducted the experiment with two conditions. 
With the .rst condition (RW) subjects started in the virtual city model when they turned on the HMD, 
so they started the virtual geocaching game directly from the real world. With the second condition (TW) 
subjects were transferred to the virtual city model via a transitional world. Hence, after having been 
equipped with the HMD, subjects saw a realistic model of the laboratory space used as transitional environment 
and could enter the virtual city model after opening and passing a virtual portal. In contrast to the 
situation in the virtual city model subjects could talk with the experimenter while they were walking 
through the transitional environment. We allowed this communication to indicate that they are still not 
in the virtual world. Since the virtual model is a 1-to-1 copy of the real laboratory users could walk 
around and touch vir­tual respectively real objects such as walls, doors, or cabinets. After approximately 
5 minutes we told subjects that they had to press a particular button on the Wii remote device in order 
to open a portal to the virtual world. The portal was displayed on one wall of the laboratory space (see 
Figure 2(c)). After walking through the portal (as described in Section 3.3) sub­jects were transferred 
to the virtual city model and the portal closed behind them. When a subject pressed the home button on 
the Wii remote device another portal to the virtual replica room appeared and the subject could walk 
through this portal back to the virtual laboratory (see Figure 2(c)). Sounds related to the virtual city 
model (ambient city noise) were turned off in the virtual replica and subjects could talk to the experimenter 
again. The virtual world was the same in both conditions no matter whether subjects entered di­rectly 
or after they moved through the transitional environment .rst. We ensured that subjects stayed the same 
timespan (10 minutes) in the immersive virtual environment for both conditions.  5.2 Results for Evaluation 
of Transitional Environ­ments 5.2.1 Subjective Presence In order to analyze if the gradual transition 
to the virtual world in­creases the subject s sense of presence we evaluated the subjects self reports. 
We used the Slater-Usoh-Steed (SUS) presence ques­tionnaire, which has been developed over a number of 
years in several experiments at the University College London [Usoh et al. 1999b]. The questions are 
based on variations of three themes, i. e., sense of being in the VE, the extent to which the VE becomes 
the dominant reality, and the extent to which the VE is remembered as a place. Subjects had to rate each 
of six questions on a 1-to-7 Likert scale (where 1 means no presence, and 7 means high presence). In 
the following scond refers to a user s self-reported sense of presence under condition cond, with cond 
= TW for the condition with and cond = TW for the condition without transitional environment. Subjective 
evaluation of the condition without transitional environ­ment shows that subjects had only a slight sense 
of presence sRW . This is indicated by the average score of 3.63 (s = 0.70) of the SUS questionnaires; 
no high rates, i. e., 6 or 7, were chosen by the subjects. We compare this score with the transitional 
world condi­tion later. As mentioned above we were not focused on the absolute sense of presence, but 
on the sense of presence in comparison to situations when subjects were in the transitional environment 
.rst. Thus, we want to examine whether the scores for the SUS question­naires for both conditions vary 
signi.cantly and the low degree of presence in both conditions is dispensable. In comparison to the .rst 
condition, subjective evaluation of the vir­tual geocaching with the transitional environment condition 
shows that subjects still had a slight, but statistically increased sense of presence sTW . The virtual 
geocaching were identical in both con­ditions, but subjects had to walk through a transitional environment 
in the condition TW. The increase of the subject s sense of pres­ence sTW is underlined by the average 
score of 4.31 (s = 0.57) of the SUS questionnaires; 3 subjects answered three questions with 6. This 
is an increase of the subject s sense of presence of 19%. On average each subject increases her SUS scores 
by 0.68 (s = 0.59). Figure 5 shows the results for condition RW and TW for all sub­jects. No subject 
reported a lower sense of presence when entering the transitional environment .rst.  5.2.2 Behavioral 
Presence We have also considered the subjects behaviors by means of videos, which we have captured during 
the experiment, in a post­session a few days later. Due to our redirection concepts subjects could walk 
unlimitedly in the VE. We reviewed their way of walk­ing. We have measured their speed, but also considered 
other no­ticeable problems while they were walking, such as unnatural walk­ing, walking with arms reached 
out, stumbling and so on. One naive observer had to view different video sequences of the experiment 
days after the experiment took place. This observer did not know if the shown sequence was from the experiment 
with condition RW or condition TW, i. e., if the shown subjects had been in the tran­sitional environment 
.rst or if they had started in the virtual world directly. The observer had to classify the way of walking 
by means of considering walking speed, patterns, reliability, overall impres­sion and relation between 
walk and view direction. Each aspect Figure 5: Results of the SUS questionnaires for individual subjects 
S0,...,S9 for condition RW and TW. had to be classi.ed according to three levels such as very slow ­-slow 
-normal, or unnatural -almost natural -natural etc. The evaluation of the ways of walking of the different 
subjects shows that subjects partly feel uncomfortable and insecure, while walk­ing in the HMD environment. 
The viewer classi.ed the walk speed with this condition with 1.75 on average, which corresponds to very 
slow to slow walking. The pattern of walking was evaluated as 2.1, where 1 corresponds to walking with 
caution, and 3 corresponds to safe walking. The overall impression of the walk to the viewer was 2.3 
on average, which corresponds to almost natural walking. Two subjects reached out their arms almost constantly. 
When con­sidering the view direction the viewer noticed that subjects looked to the direction they walk 
most of the time. He classi.ed this by 2.0, where 1 corresponds to always looking at heading direction 
and 3 corresponds to free look-around. 4 subjects tried to talk to the experimenter during the experiment, 
although we told them that talking to the experimenter while they are in the VE is not possible. In the 
same way to the results for the condition RW we reviewed the subjects behaviors on the captured videos 
for the condition TW. The evaluation of the way of walking for the different subjects shows that subjects 
feel more comfortable and safe, while walking in the HMD environment, when they have entered via a transitional 
environment. The same viewer classi.ed the walk speed in this condition with 2.1 on average, which corresponds 
to slow walk­ing. The pattern of walking was evaluated as 2.3, where 1 corre­sponds to walking with caution, 
and 3 corresponds to safe walking. The overall impression of the walk to the viewer was 2.5 on av­erage, 
which corresponds to an impression between almost natural walking and natural walking. When considering 
the view direction subjects steered less to the direction they walk in contrast to con­dition RW. The 
viewer classi.ed this by 2.6, which corresponds to rather free look-around during walking. Hence, after 
approximately 5 minutes walking in the transitional environment, which subjects have entered before they 
use the HMD, they seem to move faster and more natural in comparison to the situation when they start 
the VR experience directly in an entirely virtual world. 3 subjects tried to talk to the experimenter 
while in the virtual geocaching game. This number has decreased although in this condition subjects were 
allowed to talk to the experimenter during the time when they were in the transitional environment. 6 
subjects started to talk to the ex­perimenter when they re-entered the transitional environment after 
the virtual geocaching. When entering the VE through the virtual portal, all subjects walked carefully 
and decreased speed. 4 reached out their arms when walking through the portal. In summary, from an extraneous 
perspective subjects move more comfortable and safe through the VE when they have visited a transitional 
environment .rst.   6 Discussion According to the experiment described in Section 4, if the labora­tory 
space covers an area of approximately 40m ×40m, it becomes possible to guide the player on a circular 
arc in the physical world, whereas the user can walk straight in the VE unlimitedly. We add a security 
area within the tracking region. When the player has left this security area we redirect her with respect 
to the angle of the intersection of her current walk direction and the border of the tracking region 
either to the left or to the right such that she will return to the security area. We have performed 
further questionnaires in order to determine the users fear of colliding with physical objects. The subjects 
revealed their level of fear on a four point Liker-scale (0 corresponds to no fear, 4 corresponds to 
a high level of fear). On average the evalua­tion approximates 0.6 which shows that the subjects felt 
safe even though they were wearing an HMD and knew that they were being manipulated. Further post-questionnaires 
based on a comparable Likert-scale show that the subjects only had marginal positional and orientational 
indications due to environmental audio (0.6), visible (0.1) or haptic (1.6) cues. The evaluation of transitional 
environments has shown that such a gradual transition to the game environment has the potential to in­crease 
the player s sense of presence. Subjective comments show that subjects of this experiment feel a higher 
sense of presence with the condition TW, i. e., when they have entered the virtual world via a transitional 
environment. A statistical analysis has shown that the increase of the SUS scores from 3.63 to 4.31 is 
statistically sig­ni.cant (. = 0.05). As mentioned above the strongest impact of the usage of a transitional 
world could be manifested by the subjec­tive measurements as well as subjective comments after the experi­ments. 
For instance, one subject remarked: After walking through the wormhole, I really got the feeling of being 
transferred to another world. This was a typical comment of the subjects. The metaphor of a virtual portal 
supports their notion of being transferred to another world. Some subjects noticed that acoustics were 
very important when they left the transitional world and entered the virtual one. In the transitional 
world we neglected acoustics, whereas in the virtual world ambient city noise was displayed. The behavioral 
measures show that subjects move faster, more safely and naturally through the VE, when they entered 
via a transi­tional environment. According to the evaluation of the observer no subject moved more slowly 
or unsafely after they have walked in the transitional world. Two subjects remarked that it was de.nitely 
easier for them to orient themselves and that they had a better feel­ing for distance estimation in the 
VE. In general, subjects have re­marked that estimation and performance of motions have improved after 
they have visited the transitional environment. This is also underlined by the increased walking speeds 
of the subjects for con­dition TW, i. e., when they have visited the transitional environment .rst. We 
were surprised about the positive feedback about the application of virtual portals. The subjective comments 
have shown that sub­jects really preferred the usage of portals, which transferred them to the virtual 
world. 7 Conclusion and Future Work In this paper we have presented a presence-enhancing user inter­face 
for immersive .rst-person gameplay. For this setup we have quanti.ed how much players can be guided on 
a circular arc, and we have analyzed the usage of transitional environments via which player could enter 
a virtual world. The results of the psychophysi­cal experiments have shown that players can be guided 
on circular arc with a radius of 22.03m whereas they believe themselves to be walking straight. Within 
the range of these detection thresholds subjects cannot estimate reliably if they walk straight or on 
a curve. In the evaluation of transitional environments we focused on the question whether the usage 
of transitional environments affects the user sense of presence in the virtual environment or not. The 
self­reports of the participants show a signi.cant increase of the sub­ject s sense of presence. Furthermore, 
subjects seem to move more safely and naturally when they have accustomed themselves to the VR system 
setup in a familiar environment .rst. We have shown that transitional environments enhance the level 
of feeling present in the game. The bene.ts were shown for an example game, and we are con.dent that 
the results even hold for other .rst-person games. Many factors may have a certain impact on the results 
of the pre­sented experiment and further experiments need to be conducted. Moreover, in future experiments 
we will consider whether the us­age of a transitional environment improves distance estimation or user 
movement in general. It has been have shown that distance estimation in virtual environments which are 
known from the real world is better than distance estimation in unknown environments. Perhaps such skills 
could be transferred to the virtual world, when users enter a game environment via a virtual replica 
of a known environment. To summarize the results, we suggest using redirected walking and transitional 
environments for immersive games since they have great potential to enhance gameplay, while portals to 
the virtual world can further improve the VR experience.  References ARTHUR, K. W., ARTHUR, K. W., 
AND ARTHUR, K. W. 2000. Effects of .eld of view on performance with head-mounted dis­plays. Tech. rep., 
Dissertation Abstracts International. BARFIELD, W., AND HENDRIX, C. 1995. The effect of update rate on 
the sense of presence within virtual environments. Virtual Reality: The Journal of the Virtual Reality 
Society 1, 1, 3 16. BERTHOZ, A. 2000. The Brain s Sense of Movement. Harvard University Press, Cambridge, 
Massachusetts. BERTIN, R. J., ISRA ¨ EL, I., AND LAPPE, M. 2000. Perception of two-dimensional, simulated 
ego-motion trajectories from optic .ow. Vis. Res. 40, 21, 2951 2971. BOUGUILA, L., AND SATO, M. 2002. 
Virtual Locomotion System for Large-Scale Virtual Environment. In Proceedings of Virtual Reality, IEEE, 
291 292. BRUDER, G., STEINICKE, F., AND HINRICHS, K. H. 2009. Arch­explore: A natural user interface 
for immersive architectural walkthroughs. In Proceedings of IEEE Symposium on 3D User Interfaces (3DUI), 
IEEE Press, 75 82. accepted for publication. DICHGANS, J., AND BRANDT, T. 1978. Visual vestibular inter­action: 
Effects on self-motion perception and postural control. In Perception. Handbook of Sensory Physiology, 
Vol.8, Springer, Berlin, Heidelberg, New York, R. Held, H. W. Leibowitz, and H. L. Teuber, Eds., 755 
804. DINH, H. Q., WALKER, N., SONG, C., KOBAYASHI, A., AND HODGES, L. F. 1999. Evaluating the importance 
of multi­sensory input on memory and the sense of presence in virtual environments. Proceedings of the 
IEEE Virtual Reality, 222. ELLIS, S. R. 1997. Presence of mind: A reaction to thomas sheri­dan s further 
musings on the psychophysics of presence . In Presence: Teleoperators and Virtual Environments,, MIT 
Press, vol. 5, 247 259. FRIEDMAN, D., BROGNI, A., ANTLEY, A., GUGER, C., ANTLEY, A., STEED, A., AND SLATER, 
M. 2006. Sharing and analyzing data from presence experiments. Presence: Tele­operators and Virtual Environments 
15, 5, 599 610. GROENDA, H., NOWAK, OSSLER, AND HANEBECK,F., R¨ P., U. D. 2005. Telepresence Techniques 
for Controlling Avatar Motion in First Person Games. In Intelligent Technologies for Interactive Entertainment 
(INTETAIN 2005), 44 53. IJSSELSTEIJN, W., DE RIDDER, H., FREEMAN, J., AVONS, S. E., AND BOUWHUIS, D. 
2001. ffects of stereoscopic presentation, image motion, and screen size on subjective and objective 
cor­roborative measures of presence. Presence: Teleoperators and Virtual Environments 3, 10, 298 311. 
INSKO, B., MEEHAN, M., WHITTON, M., AND BROOKS, F. 2001. Passive Haptics Signi.cantly Enhances Virtual 
Environ­ments. In Proceedings of 4th Annual Presence Workshop. INTERRANTE, V., RIESAND, B., AND ANDERSON, 
L. 2007. Seven League Boots: A New Metaphor for Augmented Loco­motion through Moderately Large Scale 
Immersive Virtual Envi­ronments. In Proceedings of Symposium on 3D User Interfaces, IEEE, 167 170. IWATA, 
H., HIROAKI, Y., AND TOMIOKA, H. 2006. Powered Shoes. SIGGRAPH 2006 Emerging Technologies, 28. JENKINS, 
H. 2004. Game design as narrative architecture. First Person: New Media as Story, Performance, Game MIT 
Press. KOHLI, L., BURNS, E., MILLER, D., AND FUCHS, H. 2005. Combining Passive Haptics with Redirected 
Walking. In Pro­ceedings of Conference on Augmented Tele-Existence, ACM, vol. 157, 253 254. LAPPE, M., 
BREMMER, F., AND VAN DEN BERG, A. V. 1999. Perception of self-motion from visual .ow. Trends. Cogn. Sci. 
3, 9, 329 336. LARAMEE, F. D. 2002. Game Design Perspectives. MA: Charles River Media. MEEHAN, M., AND 
RAZZAQUE, S. 2003. Effect of latency on presence in stressful virtual environments. In Proceedings of 
the IEEE Virtual Reality 2003, 141 148. MEEHAN, M., INSKO, B., WHITTON, M., AND BROOKS, F. P. 2002. Physiological 
measures of presence in stressful virtual environments. ACM Transactions on Graphics 21, 645 652. MURRAY, 
J. H. 1997. Hamlet on the Holodeck The Future of Narrative in Cyberspace. MIT Press. PECK, T., WHITTON, 
M., AND FUCHS, H. 2008. Evaluation of re­orientation techniques for walking in large virtual environments. 
In Proceedings of Virtual Reality, IEEE, 121 128. RAZZAQUE, S. 2005. Redirected Walking. PhD thesis, 
University of North Carolina, Chapel Hill. SALEN, K., AND ZIMMERMAN, E. 2004. Rules of Play, Game Design 
Fundamentals. MIT Press. SANCHEZ-VIVES, M. V., AND SLATER, M. 2005. From pres­ence to consciousness through 
virtual reality. Nature Reviews Neuroscience 6, 332 339. SCHELL, J. 2008. The Art of Game Design: A book 
of lenses. Morgan Kaufmann Pub. SLATER, M., USOH, M., AND STEED, A. 1994. Depth of presence in immersive 
virtual environments. In Presence: Teleoperators and Virtual Environments, vol. 3, 130 144. SLATER, M., 
USOH, M., AND STEED, A. 1995. Taking steps: The in.uence of a walking metaphor on presence in virtual 
reality. In ACM Transactions on Computer-Human Interaction (TOCHI), vol. 2, 201 219. SLATER, M., STEED, 
A., MCCARTHY, J., AND MARINELLI, F. 1998a. The in.uence of body movement on presence in virtual environments. 
Human Factors: The Journal of the Human Fac­tors and Ergonomics Society 40, 3, 469 477. SLATER, M., STEED, 
A., MCCARTHY, J., AND MARINELLI, F. 1998b. The virtual ante-room: Assessing presence through ex­pectation 
and surprise. In Eurographics Workshop on Virtual Environments. STEED, A., BENFORD, S., DALTON, N., GREENHALGH, 
C., MACCOLL, I., RANDELL, C., AND SCHN ¨H. ADELBACH, 2002. Mixed-reality interfaces to immersive projection 
systems. In Immersive Projection Technology Workshop. STEINICKE, F., BRUDER, G., JERALD, J., FRENZ, H., 
AND LAPPE, M. 2008. Analyses of human sensitivity to redirected walking. In ACM Symposium on Virtual 
Reality Software and Technology (VRST), ACM Press, 149 156. STEINICKE, F., STEED, G. B. A., HINRICHS, 
K., AND GER-LACH, A. 2009. Does a gradual transition to the virtual world increase presence? In Proceedings 
of the IEEE Virtual Reality (VR2009), IEEE Press, 203 210. UNO, S., AND SLATER, M. 1997. The sensitivity 
of presence to collision response. Virtual Reality Annual International Sympo­sium (VRAIS), 95. USOH, 
M., ARTHUR, K., WHITTON, M., BASTOS, R., STEED, A., SLATER, M., AND BROOKS, F. 1999. Walking > Walking­in-Place 
> Flying, in Virtual Environments. In International Conference on Computer Graphics and Interactive Techniques 
(SIGGRAPH), ACM, 359 364. USOH, M., CATENA, E., ARMAN, S., AND SLATER, M. 1999. Using presence questionaires 
in reality. Presence: Teleoperator in Virtual Environments 9, 5, 497 503. USOH, M., ARTHUR, K., WHITTON, 
M., BASTOS, R., STEED, A., BROOKS, F., AND SLATER, M. 2006. The visual cliff revis­ited: A virtual presence 
study on locomotion. In International Workshop on Presence. VINAYAGAMOORTHY, V., BROGNI, A., GILLIES, 
M., SLATER, M., AND STEED., A. 2004. An investigation of presence re­sponse across variations in visual 
realism. In 7th International Conference on Presence, 148 155. WITMER, B. G., AND SINGER, M. J. 1998. 
Measuring presence in virtual environments: A presence questionnaire. In Presence, vol. 7, 225 240. 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1581092</article_id>
		<sort_key>190</sort_key>
		<display_label>Pages</display_label>
		<article_publication_date>08-04-2009</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Understanding visual interfaces for the next generation of dance-based rhythm video games]]></title>
		<page_from>119</page_from>
		<page_to>126</page_to>
		<doi_number>10.1145/1581073.1581092</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1581092</url>
		<abstract>
			<par><![CDATA[<p>We present an experimental study exploring how to best guide users when playing RealDance, a next generation dancing game prototype. It uses four Nintendo Wii remotes, attached to the wrists and ankles, to create a 3D spatial interface utilizing the entire body to more closely mimic real dancing. Since RealDance requires a player to use both arms and legs, the player needs to know which of their four limbs to use, where they are expected to move, and when they are expected to move in the dance sequence. To understand the best way to present this information, we implemented three visual interface methods: Timeline, Motion Lines, and Beat Circles, that are based on existing rhythm video games but extended to support RealDance's 3D interaction requirements.</p> <p>Our study explores each visual interface's effectiveness in conveying dance sequence information and assisting the player in providing a rewarding experience. Our evaluation is based on points scored in the game, and post-questionnaires used to solicit reactions about each visual interface including which was preferred and why. The results of the study show that players had significantly higher scores when using Motion Lines and Beat Circles than with the Timeline. The results also indicate that players found Motion Lines and Beat Circles significantly easier to follow than Timeline and icon position significantly less confusing than the Timeline interface. From these results, we believe that Motion Lines and Beat Circles are more appropriate visual interfaces than the traditional Timeline interface for full body, rhythm dance games.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[3D spatial interaction]]></kw>
			<kw><![CDATA[dance gaming]]></kw>
			<kw><![CDATA[dance-based rhythm games]]></kw>
			<kw><![CDATA[games]]></kw>
			<kw><![CDATA[user evaluation]]></kw>
			<kw><![CDATA[visual interfaces]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Evaluation/methodology</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003122</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->HCI design and evaluation methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1570345</person_id>
				<author_profile_id><![CDATA[81435609148]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Emiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Charbonneau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570346</person_id>
				<author_profile_id><![CDATA[81384608507]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570347</person_id>
				<author_profile_id><![CDATA[81100365043]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chadwick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wingrave]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570348</person_id>
				<author_profile_id><![CDATA[81100283513]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Joseph]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[LaViola]]></last_name>
				<suffix><![CDATA[Jr.]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1142248</ref_obj_id>
				<ref_obj_pid>1142215</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Aylward, R., and Paradiso, J. A. 2006. Sensemble: a wireless, compact, multi-user sensor system for interactive dance. In <i>NIME '06: Proceedings of the 2006 conference on New interfaces for musical expression</i>, IRCAM --- Centre Pompidou, Paris, France, France, 134--139.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085949</ref_obj_id>
				<ref_obj_pid>1085939</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Blaine, T. 2004. The convergence of alternate controllers and musical interfaces in interactive entertainment. In <i>NIME '05: Proceedings of the 2005 conference on New interfaces for musical expression</i>, National University of Singapore, Singapore, Singapore, 27--33.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>993837</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bowman, D. A., Kruijff, E., LaViola Jr., J. J., and Poupyrev, I. 2004. <i>3D User Interfaces: Theory and Practice</i>. Addison-Wesley, Boston.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bureau, D. N., 2007. Labanotation, September. http://www.dancenotation.org/DNB/index.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1544280</ref_obj_id>
				<ref_obj_pid>1544197</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Charbonneau, E., Miller, A., Wingrave, C. A., and LaViola, J. J. 2009. Poster: Realdance: An exploration of 3d spatial interfaces for dancing games. IEEE Computer Society, Los Alamitos, CA, USA, vol. 0, 141--142.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>835983</ref_obj_id>
				<ref_obj_pid>832289</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Chua, P. T., Crivella, R., Daly, B., Hu, N., Schaaf, R., Ventura, D., Camill, T., Hodgins, J., and Pausch, R. 2003. Training for physical tasks in virtual environments: Tai chi. In <i>VR '03: Proceedings of the IEEE Virtual Reality 2003</i>, IEEE Computer Society, Washington, DC, USA, 87.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>986102</ref_obj_id>
				<ref_obj_pid>985921</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Desurvire, H., Caplan, M., and Toth, J. A. 2004. Using heuristics to evaluate the playability of games. In <i>CHI '04: CHI '04 extended abstracts on Human factors in computing systems</i>, ACM, New York, NY, USA, 1509--1512.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1055081</ref_obj_id>
				<ref_obj_pid>1054972</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[H&amp;#228;m&amp;#228;l&amp;#228;inen, P., Ilmonen, T., H&amp;#246;ysniemi, J., Lindholm, M., and Nyk&amp;#228;nen, A. 2005. Martial arts in artificial reality. In <i>CHI '05: Proceedings of the SIGCHI conference on Human factors in computing systems</i>, ACM, New York, NY, USA, 781--790.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Holm, S. 1979. A simple sequentially rejective multiple test procedure. <i>Scandinavian Journal of Statistics 6</i>, 2, 65--70.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1129019</ref_obj_id>
				<ref_obj_pid>1129006</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Hoysniemi, J. 2006. International survey on the dance dance revolution game. <i>Comput. Entertain. 4</i>, 2, 8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1401850</ref_obj_id>
				<ref_obj_pid>1401843</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Khoo, E. T., Merritt, T., Fei, V. L., Liu, W., Rahaman, H., Prasad, J., and Marsh, T. 2008. Body music: physical exploration of music theory. In <i>Sandbox '08: Proceedings of the 2008 ACM SIGGRAPH symposium on Video games</i>, ACM, New York, NY, USA, 35--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1447520</ref_obj_id>
				<ref_obj_pid>1447515</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[LaViola, J. 2008. Bringing vr and spatial 3d interaction to the masses through video games. <i>IEEE Computer Graphics and Applications 28</i>, 5, 10--15.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>376905</ref_obj_id>
				<ref_obj_pid>376890</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Moeslund, T. B., and Granum, E. 2001. A survey of computer vision-based human motion capture. <i>Comput. Vis. Image Underst. 81</i>, 3, 231--268.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1077253</ref_obj_id>
				<ref_obj_pid>1077246</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Sweetser, P., and Wyeth, P. 2005. Gameflow: a model for evaluating player enjoyment in games. <i>Comput. Entertain. 3</i>, 3, 3--3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1581082</ref_obj_id>
				<ref_obj_pid>1581073</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Varcholik, P. 2009. The bespoke 3dui xna framework: A low-cost platform for prototyping 3d spatial interfaces in video games. In <i>Sandbox 2009: ACM SIGGRAPH Video Game Proceedings</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Understanding Visual Interfaces for the Next Generation of Dance-Based Rhythm Video Games EmikoCharbonneau* 
Andrew Miller ChadwickWingrave JosephJ. LaViolaJr.§ Universityof CentralFlorida Abstract Wepresentanexperimental 
studyexploringhowto best guide users when playingRealDance, anextgenerationdancing game proto­type. It 
uses four Nintendo Wii remotes, attached to the wrists andankles,tocreatea3Dspatialinterface utilizingthe 
entire body to moreclosely mimicreal dancing. SinceRealDance requiresa playerto usebotharmsandlegs,the 
player needstoknowwhichof theirfour limbstouse, wherethey areexpectedtomove,andwhen theyareexpectedtomoveinthe 
dance sequence.Tounderstandthe bestwayto present this information, we implementedthree visual interface 
methods:Timeline, MotionLines,and Beat Circles, that arebased onexistingrhythmvideo gamesbutextendedtosupport 
RealDance s 3Dinteraction requirements. Ourstudyexplores each visual interface seffectivenessinconvey­ing 
dance sequence information and assisting the player in pro­vidingarewardingexperience. Ourevaluationis 
based on points scored in the game, and post-questionnaires used to solicit reac­tions about each visual 
interface including whichwas preferredand why. Theresultsof thestudy show that playershad signi.cantly 
higherscoreswhen using MotionLines andBeat Circlesthanwith theTimeline. Theresults also indicate thatplayers 
found Motion Linesand Beat Circlessigni.cantly easiertofollowthanTimeline and icon positionsigni.cantly 
less confusing than theTimeline in­terface. From theseresults,we believe that MotionLines andBeat Circles 
are more appropriate visual interfaces than the traditional Timelineinterface forfull body,rhythmdance 
games. CR Categories: K.8.0 [PersonalComputing]: General Games; H.5.2 [InformationInterfaces andPresentation]:UserInterfaces 
Evaluation/methodology Keywords: dance-basedrhythmgames,dance gaming,3D spatial interaction, visual interfaces, 
user evaluation, games 1 Introduction Rhythm music games, sometimes referredtoassight-reading mu­sic 
games, feature gameplay that incorporates eye and body co­ordination with music. Toscore well,aplayer 
must translate vi­sual cuesinto actionsandperformthemattheappropriatetimeand in rhythm. While the actions 
usually correspond tobuttons on an analog game controller, themed experiences through custom hard­ware 
controllerssuchasdancingonagamepadorplayingaguitar shaped controller are popular as well. This is likely 
attributable *e-mail: miko@eecs.ucf.edu e-mail:amiller@eecs.ucf.edu e-mail:cwingrav@eecs.ucf.edu §e-mail:jjl@eecs.ucf.edu 
 Copyright &#38;#169; 2009 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for components of this work owned by others than ACM 
must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from 
Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org. Sandbox 2009, New Orleans, 
Louisiana, August 4 6, 2009. &#38;#169; 2009 ACM 978-1-60558-514-7/09/0008 $10.00 Figure 1: The RealDance 
system in action. The Motion Lines in­terface is pictured. to thenoveltyofincreased physicalexertioninthe 
caseof Dance Dance Revolution (DDR)andagreater senseof making musicin the caseof bandgames such as Guitar 
Hero and Rock Band. Theadventofthe NintendoWii remote(Wiimote)has brought3D spatial interfaces [Bowman 
et al. 2004] to the consumer market andthe forefrontof gaming[LaViola 2008].However,onlyafew rhythm music 
gameshavemadeeffectiveuseof3D spatialinter­action.Forexample, DDR uses theWiimoteand Nunchuk to add other 
actionstoits four arrowpatterns,but theironlyfunctionisa simpleshakeofthecontrollerthat canbe completedin 
almostany positionandin anydirection. While this doesrequire the playerto use their entirebody,itisnotmuchdifferentthen 
pressingabutton. We postulate that taking advantageoftheWiimote s potentialfor full body gestures, differentiating 
between body parts and more re­.ned detection of limb pose, direction and acceleration will lead to a 
new generation of dance-based rhythm gaming. To explore howtomove dance-basedrhythmgamestothenextlevel,wehave 
developedRealDance [Charbonneau et al. 2009],a dancinggame prototypethat usesfourWiimotes,attachedtothewristsand 
ankles (see Figure1). SinceRealDance requiresa playerto use botharms andlegs, the player needsto know 
which of theirfour limbs to use, where they are expected to move, and when they are expected to moveinthe 
dance sequence. Thus,itisimportantnotonlyto detect thesefullbody gestures,buttoalso understandhowtobestconvey 
the visual cues needed during gameplay. These visual interfaces willhavetoevolvetoadapttothe nuancesand 
greater information requirementsofthese games,whileatthesametimeremainingeasy andfun. In this paper, 
we presentaformal user study comparing threediffer­entvisual interfaces,Timeline, MotionLines,and Beat 
Circlesfor playingRealDance. Section2 examineswork relatedtofull body motion games and surveys the visual 
interfaces used in rhythm­basedgames. Section3introduces theRealDance prototype.Sec­tion4discussesthe 
three visualinterfaces we developedfor Real-Dance, basedonareviewofexistingrhythm-basedgames.Sections 
5and6present theuserstudy andresults while Sections7 and8 discussfutureworkand conclusions.  2 Related 
Work 2.1 Brief Rhythm Gaming Survey Seventy-six rhythm-based video games from the last decade were surveyedto 
identify trendsand categoriesofinterface design. Early on, icons streaming across the screen in a timeline 
fashion were used to indicate the timeof ajoystick tilt orabuttonpress. This interfacemadeup61%ofthesurveyedgameswith 
littlevariation among them,including Rock Band, Guitar Hero,and Dance Dance Revolution.A similarinterface 
style, which accounted foranad­ditional 18%of thegames,arrangedthe icons radially around the screen.Inthisinterface, 
icons oftenemerge fromthe middleofthe screenand projectoutineight directions,splittingthe player sfo­custo 
multiplescreen positions forincreased dif.culty. Gameplay sometimes involves an analog thumb stick. Notable 
examples in this category are Gitaroo Man and EyeToy: Groove. Thelatest generation ofgaming devices hasintroduced 
newinter­action. While some gamescontinue theiconscrollinginterfaces,a newformofrhythmgameplayevolved 
incorporatingthe absolute positioningof theNintendoDS touchscreen.Inthe Japanese game Osu! Tatakae! Ouendan, 
players musttap circlesinthe correct or­der and in the correct rhythm, as well as trace lines on the 
screen. Thiswas wellreceivedand allowed iNISto createthe sequels Elite Beat Agents and Ouendan 2.However, 
onlythree othergames were found to use a similar interface since its release in 2005. In total, thesegames 
account for5% ofthe surveyed games. Tracing lines has been used in other control schemes as well. InWe 
Cheer,the player holdsaWiimotein each hand, similarto pom­poms.Using colortodifferentiate leftandright, 
intricatearrowed lines formonthescreenandaniconmovesalong themto indicate the timingofthe motion. This 
interface allows for easydescription of movement in three dimensions and presents a clear distinction 
between largeand smallmovements. This interface also hasfew descendants: aNintendo DS game(Princess Debut)and 
an upcom­ing pop star game forthe NintendoWii, making theircontribution account for 4%. Theremaining12%ofthegames 
arenotrelevanttofullbodygam­ing. This includesthe Simon gameplay found in Space Channel 5 and some WarioWare, 
Inc minigames. In these cases, the player watches the game perform several actions and then must mimic 
them successfully. Another game, Unison, expects the player to determine fromthe on-screenavatar smovementshowto 
shiftthe controller sjoystick. These interfaces are notexploredin thiswork. 2.2 Rhythm Game Components 
The ability to track the body has enabled manyrelated applications in non-gamesand precededthedevelopmentoftheWiimote. 
Mo­tion capture, or mocap,[Moeslund andGranum 2001] recordsthe body s movementsinspace and has severalentertainment 
and mil­itary applications.The methodsvary,including magnetic, mechan­ical and accelerometer-basedtracking,butthe 
dominantversionis optical tracking. Body tracking has enabled interesting applica­tions such as virtualTai 
Chi[Chua et al. 2003] andMartialArts [H¨am¨al¨ainen et al. 2005]. The Body Music system[Khoo et al. 2008] 
uses interactionina physical spaceto entertainwhile teach­ingdifferentvaluesof music.Relatedtodanceandmusic,aswellas 
beingsimilartotheWiimote hardware, theSensemblesystem[Ayl­wardandParadiso2006]isusedto track dancemovementsusing 
accelerometers and gyroscopes attached to the ankles and wrists. Labanotation has become the most standardly 
used iconic repre­sentation and of dance movements. However with many differ­entshapes, positions, coloringmethods,and 
staffs forplacement, Labanotation literacy entailsalarge learning curve[Bureau 2007], sinceitwas createdforchoreographyandarchivalpurposes,stress­ing 
precision and accuracy over readability. Thestudyof rhythm gamesin academicshas received little atten­tion 
until recently. One survey compared music game peripheral controllers [Blaine 2004].Anotherconductedaninternationalsur­vey 
of the Dance Dance Revoluton community using an online questionnaire which included questions related 
to physical moti­vations,socialstructure,and prioritiesingameexperience[Hoys­niemi 2006].Other academic 
research hasinvestigated theheuris­ticsofgamedesign[Desurvire etal. 2004]. One promisingap­proach to 
understanding rhythm gaming is the GameFlow model [SweetserandWyeth 2005], whichinvestigatesenjoymentin 
game­play. To the best of our knowledge, our study is one of the .rst to explore different methods of 
expressing visual information in rhythm games.  3 The RealDance System RealDance seeksto pushthe limitsof 
theWiimote hardwareand to produceafull-body dancingexperience. The goalisfor players to feel likethey 
are dancing naturally andimprovingtheir perfor­mance. Using readily available commodity hardware theyreceive 
feedback directly tied to how well they complete each movement. RealDance doesnotrelyonbuttonpressing,astapleofvideogame 
controls whichcontradictsthe feelingof dancing. The playeris not spatially tethered to a speci.c location 
either. Movements which arerecognizable as gestures arescoredfor actingwithina certain time window, closer 
to a realistic dancing experience. FourWiimotes,attachedtotheuser swristsand ankles,supplythe data forRealDance.To 
implement our prototype,we neededaway to attachWiimotestothe user.Therequirementsforthese wearable attachmentsincludedcomfort, 
adjustability andsecure positioning. We measured the arms and legs of roughly twenty people of vari­able 
height, weight, and gender and used this data to design velcro straps for the forearms and shins. These 
straps were weaved into modi.edWiimotejackets. Our prototype is implementedinC#, using the BespokeXNA3DUI 
Framework [Varcholik 2009]ina Windows environment. 3.1 Gesture Scoring Thegesture scoringinRealDancehas 
threemajorconcerns. First, arangeofmovements needstobe detectableto matchthevarietyin dance. Second, 
these movements need to be reliably distinguished from one another. Third, cheating needstobeeliminated.In 
mo­tion detectiondevices likeWiimotes, players can cheat using the ambiguitiesinherentin accelerometer-basedinput.Inmanyrhythm 
gamesintendedfor full-body movement,players can obtainperfect scores without gettingupfromthesofa.Thisisfar 
fromthegame designer sintentand limits thefun and.tnessofthe gameplay. For each gesture in the choreography, 
we consider an interval T based on the expected duration of the movement. This segmenta­tion is completely 
independent of the input. Each gesture in the choreography is scored independently, so spurious motions 
in be­tweenexpected gestures are not penalized. The onlyinputstothe system arethe accelerationvectors 
A from theWiimotes.Wewill use w . W = {LH,LF,RH,RF} to refer to the four limbs (lefthand, left foot, 
etc.) when necessary. Similarly we will use d .D ={x, y, z}to refertothe individualdimensions,and t .T 
to refertoindividualinstantsof time. 3.1.1 Impulse motions An impulse motion, such as a punch, is characterized 
by a rapid deceleration occurring when the arm is fully extended. In a dance, this instant should line 
up with a strong beat in the music. We score an impulse motion by considering a one-beat interval T =[t0 
-0.5,t0 +0.5]centered around the expected beat. For theWiimotecorrespondingtotherelevant limb,we then 
selectthe time sample tk in the interval T corresponding to the maximal ac­celerationinthe negative Y 
direction, thelong axisoftheWiimote, tk =argmax-At,y. (1) T If this maximal accelerationis belowathreshold, 
then we conclude that no punch occurred, andthe scoreis zero. Otherwise, thescore ST is computedfromthe 
distancetotheexpectedbeat t0: ST =1-|tk -t0|. (2) If the gesture involves multiple limbs, the maximal 
acceleration value mustbegreater than the thresholdfor allinvolvedWiimotes. Theaverageof allthe tk is 
used to computethe score. 3.1.2 Impact motions An impact motion, such asastomp,is distinguished fromanim­pulse 
motionbythe presenceofasuddenshockwhentwo surfaces collide. This produces an easilyidenti.able changein 
acceleration values (jerk)overall threedimensions. In ordertoscore an impact motionfor oneWiimote, we 
.rst com­pute the change in acceleration vectors for each pair of adjacent timesamples.Wethenselect the 
timesample tk corresponding to thelargest magnitudeof jerk, tk =argmax|At -At-1|. (3) T Ifthis maximaljerkvalueforthe 
intervalislessthanathreshold,we conclude that no impact occurred, andthe scoreis zero. Otherwise, thescoreis 
calculatedinthe sameway asfor an impulse.   4 Visual Information The visualinterface presents the dance 
sequencethe playerisex­pected to perform. This includes three pieces of information for each move: which 
body part(s) to move, where to move them, and at what time. Threeinterface prototypes werecreated based 
upon the.ndingsof therhythmgamingsurvey. 4.1 Common Screen Elements Common screen elements across all 
threeinterfaces were usedto motivateand inform theplayers. The .rst motivatingscreen ele­ment used is 
an overall score, shown in the upper left corner of thescreen. The scoreiscomputedby rating each move 
asaMiss, Okay,Good,or Perfect. Thisratingis presentedtothe userbya label as well as an enjoyable cartoon 
character expression. Lastly, each dancesequenceis accompaniedbyasong and its music video. The informative 
screen elements were kept constant as much as possible. The icons used to indicate the body part(s) to 
move are similar acrossall threevisualinterfaces (see Figure2). Hands and feet icons are represented 
by a closed .st and a shoe, respectively. To differentiate between the sides of the body, the icons were 
col­ored green for left and purple for right. A stick .gure character performs the dance sequence along 
with the player. Almost all dance-based rhythm games have characters moving in the back­ground duringgameplay. 
In some games, such as Dance Dance Revolution,thisis purely aesthetic with little bearing onthe step 
pattern.Ingames such as Unison and We Cheer,the character itself guidesthe player. Figure 2: Icons representing 
each limb. 4.2 Timeline TheTimeline interface, as shown inFigure3(left), is inspiredby games likeDance 
Dance Revolution and All Star Cheer Squad.The player knows which limbto moveby theicon, whereto move 
them by directional arrows and when to move them by their streaming into a box at the left side of the 
screen. The icons stream along from right to left, similar to musical notes, with vertical lines rep­resenting 
beats of the song. We chose this streaming style over farto near streaming (forexampleinRock Band)because 
the side scrolling methodis mostcommonin dancerhythmgames,andhav­ingtheicons startdeepintheviewingplanewouldmakethemtiny 
and harderto distinguish.Thisinterface takesup little visual space comparedtothe otherinterfacesbut placesaheavy 
representational burden on theicons. 4.3 Motion Lines TheMotionLines interface, as showninFigure3(middle),isin­spiredbygames 
like We Cheer. The player knows which limb(s) to moveby theicon, whereto move them as indicatedby the 
path line s relative screen position and when to move by the appearance andmovementoftheiconalonga path.Forconsistency,thepath 
is thesamecolor as theicon. In this interface, thespatialareaof thescreenis utilizedastheicons are presented 
aroundthestick.g­ure. Additionally,repeated motions presentoverlap problemsbut are still viewable when 
they are placed behind the more recent ac­tion. One potential bene.tofMotionLinesistheabilitytoshow durationand 
potentially show pauses.Thisisexempli.edbya step move,where theicons tracing back andforth on the pathindicates 
timingand could pausealongthepathfor addeddif.culty.  4.4 Beat Circles TheBeat Circles interface, as 
showninFigure3(right),is inspired by games like Osu! Tatakae! Ouendan. The player knows which limbstomovebytheicons,wheretomove 
themas indicatedby the positioningoftheicons aroundthe centralstick.gureandwhen to move them by the disappearance 
of the collapsing circle. Beat Circlesuses muchofthe screen real estate, likeMotionLines,but does not 
suffer from the MotionLineoverlap issue.  Figure 3: The three interfaces displaying the same moves. 
Left: Timeline. Middle: Motion Lines. Right: Beat Circles.  5 Usability Study We conducteda user study 
comparingTimeline, MotionLines,and Beat Circles, in thecontext of our RealDance videogameproto­type,byexamining 
each visual interface seffectivenessinconvey­ingdance sequence informationand assisting the playerin 
provid­inga rewardingexperience. Basedon early pilotstudies,wehy­pothesize that players would score higher 
with either the Motion Lines orBeat Circlesinterfaces than with theTimelineinterface, because MotionLines 
andBeat Circlesinherently providethe spa­tialinformation neededto performthemovementsrequiredinReal-Dance. 
This inherent spatialinformationis not presentintheTime­lineinterface becauseitexclusively usesiconsto 
presentnotonly the timing information,but which limb to useand where tomove it. Asaresultofthe complexitiesof 
using theTimelineinterface in RealDance, we also hypothesized that playerswill prefer Motion Lines orBeat 
CirclesovertheTimelineinterface. 5.1 Subjects and Apparatus Twenty-four (13 male, 11 female) participants 
were recruited from theUniversityofCentral Floridaand thesurrounding area with ages ranging from 18-29. 
Of the 24 participants, 19 had no formal dance experience, andofthose, sixdo not dancesocially.Seventeen 
par­ticipants played videogames morethan oncea monthand14 had played DDR at least once. The experiment 
duration ranged from forty minutes to an hour and .fteen minutes, depending on how long the user spent 
with the questionnaires. All participants were paid10 dollars fortheir time. Theexperimental setupconsistedof 
a dual-core desktopPC with an nVidia GeForce 8500 graphics card, using a 50 inch Samsung DLP 3D HDTV 
display with a refresh rate of 60 Hz. Graphics were displayed at a resolution of 1920 x 1080. Participants 
had an areaof approximatelysixsquarefeetin frontoftheSamsungdis­playto interact withthegame. An opaque 
plastic curtainenclosed the space so that only the moderator and participant were present during theexperiment.Thiswas 
done forprivacyand the comfort ofthe participants.Theexperiment moderatorsattothesideofthe study space 
and controlled the software via a wireless mouse. 5.2 Experimental Task Thetask participants performedwasto 
playRealDanceby moving their arms and legs in time with the music when instructed to do so by the interface. 
Two songs were chosen for the experiment. In thepracticesessions, the GhostbustersTM theme song was used 
Hand Foot Compound Left hand up Left foot steps Both hands upward Left hand side Right foot steps Both 
hands to the right Right hand up Left foot kicks Left hand left/right hand right Right hand side Right 
hand up/left foot kicks Left hand up/right foot kicks Jump (both feet stepping) Table 1: The individual 
moves in the dance game becauseof itsslow, catchytempo.Forthe actualexperimental task, we choseThrillerTM 
by Michael Jackson. This song is also well knownandhastheaddedbene.tofa recognizable dancesequence. Fortheexperiment,the 
dancing choreographywasdesignedtofo­cus on movements that were easiest to differentiate visually and 
matched the Thriller dance routine well. In total, there were 13 unique movementsparticipants had toperform 
(seeTable1).  5.3 Experimental Design and Procedure We useda three-waywithin-subjectsfactorialdesignwhere 
the in­dependentvariablewas visualinterface technique(Timeline, Mo-tionLines,and Beat Circles) andthe 
dependent variablewas the danceroutinescore. Detailsonthescoring mechanism canbe found in Section 3.1. 
The maximum obtainable score for the Thriller gameplay sessions was 6700 points. One hundred points were 
awardedfora Perfect move,75for Good ,50for Okay and0 fora Miss .Compoundmoves werescoredfrom0to100foreach 
body part.Boththeoverall scoreand thescore for each individual movement was recorded. In addition, we 
measured participants preferences for each interface using a post-technique questionnaire that askedparticipants 
to respond to a series of 12 statements using aseven-point Likert scale(1 equals stronglydisagreeand7 
equals stronglyagree) andthree open-ended questions on what theyliked, what theydisliked, and what theyfound 
frustrating about each in­terface. Roomwasalso providedatthe bottomfor additionalcom-ments(seeTable 2). 
The experiment began when participants entered the enclosed space. Participants were given a standard 
consent form explain­ing what they would be asked to do. Next, they .lled out a pre­questionnairethatasked 
about theirdancing andvideo gameexpe­rience. Participants were then shown a sheet of icons (see Figure 
2) which appeared in all three interfaces: the four icons designat­ingbody parts,the cartoonfaceavatar,andthestick.gure. 
Once Post-Technique Questionnaire PT1 Ifelt like the images on the screen matched the music well. PT2 
Ifelt like the moves Iwas asked to do matched the music well. PT3 Iwas able to follow the suggested movements 
easily. PT4 This interface made the dance moves easy to understand. PT5 This interface made the experience 
more fun. PT6 The icons were moving too fast and Ididn thave time to respond. PT7 Icouldn tunderstand 
where to move based on the position of the icons. PT8 There were too manyvisual objects on the screen 
confusing me. PT9 The stick .gure avatar helped me make sense of the dance moves. PT10 The scores Ireceived 
matched how well Ithought Idid. PT11 When Iplayed this game, Ifelt like Iwas dancing. PT12 This interface 
made the game play more enjoyable. PT13 Describe anything you found frustrating about this interface. 
PT14 What did you like about this interface? PT15 What did you dislike about the interface? PT16 Please 
provide anyadditional comments about this interface. Post-Questionnaire PQ1 Which interface do you feel 
you performed the best in? PQ2 Which interface felt like the most fun? PQ3 Which interface did you .nd 
the easiest to understand? PQ4 Which interface made you feel most like you were dancing? PQ5 Which interface 
did you .nd to be most visually pleasing? PQ6 Which interface seemed to match upwith the music the best? 
PQ7 Which interface did you like the least? PQ8 Whydid you dislike this interface? PQ9 Which interface 
did you like the most? PQ10 Whydid you prefer this interface? Table 2: Post-Technique Questionnaire 
and Post-Questionnaire participants werefamiliarwith theseicons,the moderator helped attachthevelcroWiimotesleevestothe 
participants armsandlegs so that they were tight,but not uncomfortable. After being suited fortheexperiment, 
participants wereintroduced to thescoring elements on thescreen.For each visual interface, the moderator 
read a description of the interface, then guided partici­pantsthroughtwo practicesessions.The practiceruns 
wereidenti­cal.After the practicesession, participantswould playRealDance with Thriller. After each gameplay 
session(two practicetrialsand one real trial), participants were given the post-technique question­nairefor 
thegiven visual interface. Thus, participants played the RealDance game nine times,three for each interface.To 
reduceor­deringeffects, we randomized thegameplaysessions across partic­ipants.There aresix differentpermutations 
forordering the three interfaces; since there were 24 participants, each permutation oc­curred four times. 
After completing the gameplay sessions, par­ticipants weregivena.nal post-questionnaire usedtogauge their 
overall preferences.  6 Results 6.1 Learning Effects Although we randomized theorderingofthe gameplay 
sessions,the choreographywasthe same for each interface. To ensure thedata wasunbiased,wecomparedtheoverall 
scoresof each participantin arepeated measures oneway analysisofvariance(ANOVA)tosee ifthescores improvedbasedontheorderof 
thegameplaysessions. The results showed there was no signi.cant improvement in scores (F2,22 =0.306,p 
=0.738)based on gameplay session order, in­dicatingthat our counter-balancingwassuf.cient forremovingany 
orderbias. Hand Foot Compound Timeline 48.39 (17.48) 52.32 (16.46) 40.69 (15.95) Motion Lines 59.29 (16.27) 
64.58 (14.65) 44.40 (14.13) Beat Circles 64.18 (18.87) 60.93 (14.93) 52.44 (16.12) Table 3: Mean scores 
broken into move type for each visual inter­face. Standard deviations are in parentheses. 6.2 Overall 
Score Analysis Figure 4: Overall Score Means A repeated measures one way ANOVA was performed to deter­mine 
if visual interface type had a signi.cant effect on overall score.Visual interface typewasfoundtobe signi.cant(F1.40,22 
= 8.68,p < 0.05).1 The mean scores areshownin Figure4.Tofur­therexplorehowoverall scorevarieddueto visual 
interface,a post­hoc analysis with threepairwisecomparisonswasconducted. To controlfor thechanceofTypeIerrors,a 
Holm s sequentialBonfer­roni adjustment [Holm 1979] with three comparisons at a =0.05 wasperformed.Participantsintheexperiment 
scored signi.cantly higher with MotionLines(t23 = -4.38,p < 0.0167)and Beat Circles(t23 = -3.26,p < 0.025)than 
with theTimeline inter­face. Therewas no signi.cant difference between scores forMo­tionLines andBeat 
Circles(t23 = -1.20,p =0.243). This result impliesthatina game whereall body partsare used,participants 
performed betterinthe twointerfaces that werespatially oriented to the player, taking advantageofthe 
entirescreen.  6.3 Detailed Score Analysis Tofurtheranalyze theresults,webroketheoverall scoreintomove­ment 
types based on whether a single hand, singlefoot, orcom­pound movement consistingoftwo feet,two hands 
or one foot and one handwas used.IntheRealDance sThrillersequence, thereare atotalof52movementsparticipantshadtoperform 
consistingof13 handmoves,22footmoves,and16compoundmoves.To calculate hand andfoot scores, the totalscore 
for these moves weresummed anddividedbythe numberofmovesforeachtypeforeach partic­ipant and then the 
mean was taken across all participants. Since compoundmoveshavetwomovesassociatedwiththem,each com­pound 
move scorewasdividedbytwo .rst then followed thesame procedureas the hand andfoot scores,ensuringall 
scores were out of 100 points.Table3shows the meansand standarddeviationsof each move type for each visual 
interface. Arepeated measures two way ANOVA was calculated on the de­tailed score data with visual interface 
type and move type as the independent variables. Both visual interface type(F1.43,22 = 9.95,p < 0.05)and 
move type(F1.58,22 =22.32,p < 0.05)as wellastheir interaction(F4,20 =2.42,p < 0.05)werefound to 1Sincethe 
testviolated thesphericity assumption,aGreenhouse-Geisser correction was used. be signi.cant.2 From 
these results, we were most interested in un­derstanding how a particular visual interface affected participants 
scores for each move type. Thus, we conducted a post-hoc anal­ysis with nine pairwise comparisons,controllingfor 
thechanceof TypeIerrors usingHolm ssequentialBonferroni adjustment [Holm 1979] at a =0.05.Ofthe nine 
comparisons,participants scored signi.cantly higher whenperformingfoot(t23 = -3.98,p < 0.0056)and hand(t23 
= -3.50,p < 0.00625)movements using Motion Linesincomparisontofootandhandmovementsusingthe Timeline. 
In addition, participants scored signi.cantly higherwhen performinghandmovements(t23 = -3.16,p<0.0071)with 
Beat Circlesover hand movementswith theTimeline. Note that com­parisons between Beat Circles with compound 
movements and the Timelinewithcompound movements(t23 = -2.61,p =0.016) as wellasBeat Circleswith foot 
movementsand theTimelinewith foot movements(t23 = -2.30,p =0.031)were not signi.cant due to theBonferroni 
correction. Theseresults further indicatethat both Beat Circlesand MotionLines provideaninterface to 
RealDance that makes it easier to understand the required dance movements overthe traditionalTimelineinterface. 
 6.4 Questionnaire Analysis 6.4.1 Post-Technique Results For each post-technique questionnaire, participants 
were asked to respond to 12 statements (See Table 2) using a seven-point Lik­ertscale(1 equals stronglydisagreeand7 
equals stronglyagree) to gauge their reactions on each visual interface. To analyze the data, we conducted 
Friedman tests on each statement across the post-technique questionnaires followedbyWilcoxon SignedRank 
testswhenappropriate.For eachWilcoxon SignedRanktest, three comparisons were madeand Holm s SequentialBonferroni 
adjust­ment [Holm 1979] was used at a =0.05 to controlfor thechance ofType-I errors. Threeoutofthe12statements 
werefoundtobe signi.cant(see Figure6) and arediscussedbelow. Figure 5: Post-technique questionnaire 
results. Easy to Follow? Signi.cant differences werefound with whichin­terface was easiest to follow(.22 
=7.32,p < 0.05). Beat Cir­cles was considered signi.cantly easier to follow than the Time­line(Z = -2.69,p 
< 0.0167). Motion Lines was also signi.­cantly easiertofollowthantheTimeline(Z = -2.39,p < 0.025). However, 
there was no discernable difference between Beat Circles andMotionLines(Z = -0.80,p =0.424). These results 
corre­late with the score data to support that users found it easier with a mirror-likeinterface that 
utilizes spatialinformation. 2Both visual interface type and move type violated the sphericity as­sumption, 
so Greenhouse-Geisser corrections were used. Position of the icons more confusing? Signi.cant differences 
were foundwitheach interface sabilityto presentinformationonwhere to move(.22 =13.62,p < 0.05). TheTimelinewas 
considered moreconfusing thaneither Beat Circles(Z = -3.08,p < 0.0167) or MotionLines(Z = -2.38,p < 0.025). 
Once again, there was no signi.cantresultwhencomparing Beat CirclestoMotionLines (Z = -1.71,p =0.087). 
BecauseofthenatureoftheTimeline, participants could not use the position of the icons as a hint for moving 
their limbs. Score matched how well you felt you did? Signi.cant differences werefound with how well 
participants felt theydid based on their score(.22 =6.19,p < 0.05). Beat Circles was signi.cantly better 
than Motion Linesinthis category(Z = -2.50,p < 0.0167). No signi.cant differences werefound between Beat 
Circlesand the Timeline(Z = -0.76,p =0.46)and between MotionLines and theTimeline due to theBonferroni 
correction(Z = -2.0,p = 0.046). Some participants were unsure abouthowfast they were supposedtomovewith 
MotionLines.Sincethescoringsystemwas held constant betweenthe visualinterfaces, participantsexpectedto 
giveastrong accelerationattheendofeachmove.WiththeMotion Lines, manyparticipants moved slower at .rst 
and were not going in the correct direction when the icon reached the end of the path line.Participants 
also mayhave been in.uencedby thefact that, overall, theyscored betterwithBeat Circles. In addition, 
participants wereaskedto respondto open-endedstate­mentsfor each visual interface. SeeTable2 torefer 
to questions PT13 through PT16. Timeline. More than half of the participants noted that theyliked beingabletosee 
the nextseriesof movesapproaching, whichwas particularly trueof thosewhodidnotplayalotof videogamesor 
had no experience with visual timelines in similar .elds. One par­ticipantstated, Maybeits becauseIhavemusic 
training,butIliked being able to glance slightly ahead like with sheet music. Even though this interface 
providedalotof advance knowledge, many participants still founditdif.culttoknowwhentostartan action. 
One participant said, It was hard to know how to move, when to start movingforastep, andhow/whatangleto 
movemy hands. Severalparticipants also felt confusionbetween left andright,or hand andfoot icons. Some 
participants found theinterface to be too quick. One participantstated, The stepsatsome points were too 
close together and didn t seem to give enough reaction time. Other timelinebased gamessuchasDDR ofteninclude 
optionsto spaceouttheicons;however,the timing mustremainthe same,so puttingphysical space between them 
meansthatthevelocityof the iconsmustbegreater.Thiswouldmaketheicon scrollinglookeven faster.Finally,several 
participants,speci.cally thosewho wherefa­miliarwith otherrhythm-basedgames,suggestedhaving multiple 
timelines,perhaps oneforarmsand oneforlegs,to lessenconfu­sion. Thisisafairassessmentsince manysight 
readinggamesdo so,butitwould not solve left/right confusionorspeed issues. Motion Lines. Almost half 
of the participants noted that Motion Linesgavethema bettersenseof wheretogo. One participant re­sponded, 
It was a more natural representation of movement. The screen actslikeamirror,showingpathsforthe usertomove 
their body partsalongin an intuitiveway. In addition, severalpartici­pantsstatedMotionLinesmadeit clear 
whichbodyparttheyhadto move, which is another bene.t of using the screen in its entirety as a visualindicator.One 
majorissuewith MotionLines that halfof the participants mentioned was dealing with repeated movements. 
Manyparticipants found this frustrating as it was dif.cult to know aheadoftimeifamovement shouldberepeated.Finally,afewpar­ticipants 
mentionedconfusingtheir feetin MotionLines,especially the diagonal kicks which went along with punching 
your arm diag­onally.Thiswasanoversightin our implementation, sincestepping gaveaback andforth motiontohelpthe 
user knowwhentolifttheir leginthe airbut the kicking motion did not. We plantoaddress these issuesin 
futureversionsof theRealDance prototype. Beat Circles. Halfofthe participants noted that theicon position 
with Beat Circles helped them know where to move. One partic­ipant stated, The direction of the movement 
was visually there . There was no need to guess where your limb needed to end up. Severalparticipants 
also felt the timingwas much easierwith this interface. As with MotionLines,there were problemswith repeated 
movements. In the caseofBeat Circles, the issuewasoverlapping circles. A majority of the participants 
mentioned this issue. At one point in the choreography there are several steps with the right foot, thenseveral 
stepswith the leftfoot. Since theyareclose to­gether this causedoverlapping circlesthat manyfoundoverwhelm­ing. 
Three participants also mentionedthatthe stick.gurewas not necessary: Thestick.gure actuallyconfusedmebeingsopromi­nentin 
thescene. The descriptionofthisinterfacein theexperi­ment mentioned the stick .gure as a reference point 
for where the iconsare positioned,sothatmaybewhypeoplediscussedthestick .gureon this questionnairebut 
not theothers. 6.4.2 Post-Questionnaire Results Figure 6: Post experiment questionnaire results. Aftertheexperiment, 
participants wereaskedto choose oneofthe three visual interfaces in response to a set of eight questions 
(see Figure 6)and to explain why they chose the interface they liked the most and the least. Chi-squared 
tests were run on each ques­tiontodetermineiftheresponses werenot uniformly distributed. OnlyPQ1was foundtobe 
signi.cant(.22 =7.63,p < 0.05), in­dicatingparticipants felt theyperformed thebestwithBeat Circles. Although 
noneofthe other questions werefoundtobe signi.cant, thegraphin Figure6showsan interesting trendin that,exceptfor 
whichinterface participants liked theleast,Timelinewas theleast chosen interface for each question. In 
addition, theTimelinewas onlyfavoredin six outof the24 responses (25%). This data seems to corroboratewith 
theother results, indicating that theTimelineis not the mostideal interface fora full body rhythm game 
such as RealDance. When asked toexplainwhy they preferredtheTimelineinterface the most, participants 
mentionedthattheyfound it easyto prepare forfuturemovements. Oneparticipant stated, This interfacegave 
an easyto understand predictionofwhen each motionwouldhap­penandhowmanytimesto completeeachmovement. 
Outofthe 10 participants whochoseMotionLines as their most preferredin­terface,eightchoseitbecauseitwas 
easiestforthemtofollowand the othertwothoughtitletthembestfeellikethey were dancing. ForBeat Circles, 
mostsaid theypreferred the interface because they foundittobe easytouse.Timingwas also mentionedasbeingvery 
easytofollowinthisinterface. Most peoplechoseTimelineastheir leastfavorite interface. Many participants 
claimedthattheTimelineinterfacewas dif.cultto un­derstand. One participantstated, I hadtoread into thearrowsym­bolstoo 
much,by thetimeI comprehendedthem, they had gone past theTimeline. Too muchwork! Another user noted that 
the Timelineinterface seemed likeitwas meantfor discrete controls like playing a musical instrument, 
but not well adapted to a full body dancinggame.Forthose participants wholeast preferredBeat Circles,theyfeltitwasdif.culttodeterminetheexact 
motionsthey neededto perform.One participantstated, Itwas easiestto follow and most aesthetically pleasing(thoughit 
didn t clarify theexact notions as wellasthe motion lines).   7 Discussion and Future Work Theresultsofboththe 
performancedataandthe self-reportedques­tionnaires indicatedthattheTimelineinterfaceistheleastadequate 
inafull body dancevideo game.Participants performed theworst with it, found it harder to follow and thought 
the positioning of the iconswas mostconfusing. Thiswasinspiteof thefact that most of them hadplayedmany 
videogamesand that theTimelineinter­face is used by almost all current rhythm games. As more rhythm games 
use 3D user interfaces, designers shouldfocus on incorpo­ratingspatialinterfaceslikeMotionLinesorBeat 
Circlesaspartof presenting visual information to users. However, participants struggled with repeated 
movesfor bothspa­tialinterfaces. Onewayto deal with this problemin MotionLines would be to implement 
a partially transparent icon moving prior to when the person actually must execute the action. Another 
op­tionistohavea sectionofthe screenshowasmallversionof the next move, similar to how Tetris shows the 
player what the next piece will be.This designisimplementedinthe dancesimulation game Princess Debut 
DS,aMotionLineinterface whichshows tiny screenshotsonthetopscreenoftheDSconsole.Inthe caseofBeat Circles,itwouldhavebeen 
easiertotellthe circlesapartiftheyhad been coloredto match theleftand right icons, or had thicker lines 
as theyapproached the execution point. TheTimeline interface made it easy todistinguish repeated move­ments,butparticipants 
struggled with differentiating left/right and hands/feet.Commentsbysome participants notethatthe hands 
and feetcouldappearondifferentlinesto addressthisandthatovertime manygamers memorizecolor associations. 
Evenso, as thedance movements become morecomplex,the limited spaceoftheTime­lineisproblematic. With all 
thevariety possiblein humanmove­ment,creatingicons for each createsarepresentationalburdenand is much 
less intuitivethan using on-screen positionfor disambigua­tion. Timingwasa complaint foundin discussionofallthree 
interfaces no matterwhich interface the participant liked theleast. This shows that manypeoplehaveadifferentideaofwhatmakes 
timing hard, butitalsohighlightshow crucialitistoenjoyable gameplay.Judg­ingbythese comments, knowing 
when to movewas the most im­portant issue to the participants, more important than what moves theywere 
asked to do. Users also suggested the possibility of com­bining interfaces. Game designers interested 
in the best possible solutionshouldtakethe strengthsand weaknessesof each into ac­count and see if a 
combination could be best. Manypathsremain opentoinvestigationinthe future.Understand­ing which screen 
elements command visual attention can be used to improve gameplay. This is especially true regarding 
the stick .gure. Another issueworth studyingin moredetailispreparation time. The time prior to execution 
was chosen by instinct as we de­veloped the prototype,butaformaluserstudywouldgive insight into what 
timewouldgive the best performance andusability re­sults. We also plan on using the results of this experiment 
to design an improvedvisualinterface forthe RealDance system.Witha better interface, we will be able 
to create more distinct movements and continue towards the goal of making the game capable of teach­ing 
dance. RealDance will also need to be adjusted to allow for more precise scoring. Several of the user 
suggestions are worth exploration as well, such as changing thecolorof theicons after execution andaudio 
ortactile feedback.  8 Conclusion Sight readingrhythmgames areasuccessful genreofvideo games, and mostofthem 
use theTimelineinterface well. However, now thatallcurrent consolesareexploring3Duser interfaces,afullbody 
dancegameis.nally possible. This will requireavisualinterface totell whichbody part(s)tomove,wheretomovethemandatwhat 
time. Threedifferent visual interfaces werestudiedto determinehowthey conveyinformationtothe playerin 
RealDance,afullbody dance­basedrhythmgame. This study concludedthattheTimeline inter­face, the current 
dominant rhythm game interface, is not thebest at relaying information. Instead, participants performed 
better with interfaces that used the entire screen to help differentiate left and right movements. Participants 
also found these spatial interfaces to be easier overall. Finer-grained design tradeoffs of each inter­face 
were also identi.ed and reported. We believe the results of our study will improve the visualinterfaces 
forRealDance andare applicable to anyfull body rhythm game. 9 Acknowledgements This work was partially 
supported by IARPA, SAIC, and NSF (IIP0750551). Additionally, the authors wish to thank Charles Hughesand 
theanonymous reviewersforvaluable feedback.  References AYLWARD,R., AND PARADISO,J.A. 2006. Sensemble: 
awire­less, compact, multi-user sensor system for interactive dance. In NIME 06: Proceedings of the 2006 
conference on New in­terfaces for musical expression, IRCAM Centre Pompidou, Paris, France, France, 
134 139. BLAINE,T. 2004. Theconvergenceof alternate controllers and musical interfaces in interactive 
entertainment. In NIME 05: Proceedings of the 2005 conference on New interfaces for mu­sical expression, 
National University of Singapore, Singapore, Singapore, 27 33. BOWMAN,D. A., KRUIJFF,E., LAVIOLA JR., 
J. J., AND POUPYREV,I. 2004. 3D User Interfaces: Theory and Prac­tice.Addison-Wesley,Boston. BUREAU, 
D. N., 2007. Labanotation, September. http://www.dancenotation.org/DNB/index.html. CHARBONNEAU ,E., MILLER,A., 
WINGRAVE,C. A., AND LAVIOLA,J.J. 2009. Poster:Realdance: Anexplorationof 3d spatial interfaces fordancinggames. 
IEEE Computer Soci­ety,LosAlamitos,CA,USA,vol.0, 141 142. CHUA,P. T., CRIVELLA,R., DALY,B.,HU,N.,SCHAAF,R., 
VENTURA,D., CAMILL,T., HODGINS,J., ANDPAUSCH,R. 2003.Training forphysical tasksinvirtual environments: 
Tai chi. In VR 03: Proceedings of the IEEE Virtual Reality 2003, IEEE Computer Society,Washington, DC,USA, 
87. DESURVIRE,H., CAPLAN,M., AND TOTH,J.A. 2004. Using heuristicstoevaluate theplayabilityofgames. In 
CHI 04: CHI 04 extended abstracts on Human factors in computing systems, ACM,NewYork,NY,USA, 1509 1512. 
H¨AL¨¨AM¨AINEN,P., ILMONEN,T., HOYSNIEMI,J., LINDHOLM, M., AND NYK ¨ ANEN,A. 2005. Martialartsinarti.cial 
reality. In CHI 05: Proceedings of the SIGCHI conference on Human factors in computing systems,ACM,NewYork,NY, 
USA, 781 790. HOLM,S. 1979.A simplesequentially rejective multipletest pro­cedure. Scandinavian Journal 
of Statistics 6, 2, 65 70. HOYSNIEMI, J. 2006. International survey on the dance dance revolutiongame. 
Comput. Entertain. 4,2,8. KHOO,E. T., MERRITT,T.,FEI,V.L.,LIU,W., RAHAMAN, H., PRASAD,J., AND MARSH,T. 
2008. Body music:physical explorationofmusic theory.In Sandbox 08: Proceedings of the 2008 ACM SIGGRAPH 
symposium on Video games,ACM,New York,NY, USA, 35 42. LAVIOLA,J. 2008. Bringing vr andspatial3dinteractiontothe 
masses through videogames. IEEE Computer Graphics and Ap­plications 28, 5, 10 15. MOESLUND,T. B., AND 
GRANUM,E. 2001.Asurveyofcom­puter vision-based human motion capture. Comput. Vis. Image Underst. 81, 
3, 231 268. SWEETSER,P., AND WYETH,P. 2005. Game.ow:a modelfor evaluatingplayer enjoymentin games. Comput. 
Entertain. 3,3, 3 3. VARCHOLIK, P. 2009. The bespoke 3dui xna framework: A low-cost platform forprototyping 
3d spatial interfaces in video games. InSandbox 2009: ACM SIGGRAPH Video Game Pro­ceedings.  Games Cited 
1. All Star Cheer Squad. THQ(Wii, DS), 2008. 2. Dance Dance Revolution. Konami (Arcade), 1999. 3. Dance 
Dance Revolution Hottest Party. Konami (Wii), 2007. 4. Elite Beat Agents. Nintendo (DS), 2006. 5. EyeToy: 
Groove. SCE(Playstation2), 2004. 6. Gitaroo Man. Koei (Playstation2), 2002. 7. Guitar Hero. Activision(Playstation), 
2005. 8. Osu! Tatakae! Ouendan. Nintendo (DS), 2005. 9. Ouendan 2. Nintendo (DS), 2007. 10. Princess 
Debut. Natsume (DS), 2008. 11. Rock Band. HarmonixMusic Systems(Playstation), 2007. 12. Space Channel 
5. Sega (Dreamcast), 2000. 13. Unison. Tecmo(Playstation2), 2001. 14. WarioWare, Inc.: Mega Microgame$. 
Nintendo (GBA), 2003. 15. We Cheer. Namco Bandai(Wii), 2008.   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>NSF</funding_agency>
			<grant_numbers>
				<grant_number>IIP0750551</grant_number>
			</grant_numbers>
		</article_sponsors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1581093</article_id>
		<sort_key>200</sort_key>
		<display_label>Pages</display_label>
		<article_publication_date>08-04-2009</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Rock Band]]></title>
		<subtitle><![CDATA[a case study in the design of embodied interface experience]]></subtitle>
		<page_from>127</page_from>
		<page_to>134</page_to>
		<doi_number>10.1145/1581073.1581093</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1581093</url>
		<abstract>
			<par><![CDATA[<p>There has been a recent surge of novel interface devices available for home gaming systems. With the rise in popularity of games like Guitar Hero and consoles such as Nintendo's Wii comes new opportunities for game design at the interface level. In this paper we propose three interrelated dimensions for the analysis of embodied and gestural game interface hardware devices. We demonstrate how gestural and embodied interactions can be understood as ludic, kinesthetic and narrative experiences. We ground this discussion in a close analysis of the interface affordances of the game Rock Band and demonstrate how these three dimensions allow us to understand more clearly the place of the interface in the design and the experience of games.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[embodiment]]></kw>
			<kw><![CDATA[game interface]]></kw>
			<kw><![CDATA[ludology]]></kw>
			<kw><![CDATA[narrative]]></kw>
			<kw><![CDATA[novel interaction]]></kw>
			<kw><![CDATA[physical interaction design]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Theory and methods</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>User-centered design</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003123.10010860.10010859</concept_id>
				<concept_desc>CCS->Human-centered computing->Interaction design->Interaction design process and methods->User centered design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003126</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->HCI theory, concepts and models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003123.10011758</concept_id>
				<concept_desc>CCS->Human-centered computing->Interaction design->Interaction design theory, concepts and paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1570349</person_id>
				<author_profile_id><![CDATA[81388594272]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Joshua]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanenbaum]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser University, Surrey, BC, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570350</person_id>
				<author_profile_id><![CDATA[81100172958]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bizzocchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser University, Surrey, BC, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Arsenault D. Guitar Hero: "Not like playing guitar at all"? <i>Loading.. 1</i> (2).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>682900</ref_obj_id>
				<ref_obj_pid>646183</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bardram J. E. and Bertelsen, O. W. Supporting the development of transparent interaction. in <i>Human-Computer Interaction</i>, Springer Berlin / Heidelberg, 1995, 79--90.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bizzocchi J. <i>Ceremony of Innocence</i> and the Subversion of Interface: Cursor Transformation as a Narrative Device <i>DAC 2003: Digital Arts and Culture</i>, Melbourne, Australia, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bizzocchi J. Games and Narrative: An Analytical Framework. <i>Loading - The Journal of the Canadian Games Studies Association, 1</i> (1). 5--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1081271</ref_obj_id>
				<ref_obj_pid>1081262</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Bizzocchi J. and Woodbury, R. A Case Study in the Design of Interactive Narrative: the Subersion of the Interface. <i>Simulation and Gamin, 34</i> (4).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Bogart A. and Landau, T. <i>The Viewpoints Book: A Practical Guide to Viewpoints and Composition</i>. Theatre Communications Group, New York, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Bolter J. D. and Grusin, R. <i>Immediacy, Hypermediacy, and Remediaton</i>. The MIT PRess, Cambridge, Mass, USA, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Coleridge S. T. <i>Biographia Literaria</i>. E. P. Dutton, London &amp; New York, 1952 c1906.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Csikszentmihalyi M. <i>Flow: The Psychology of Optimal Experience</i>. Harper Perennial, New York, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>970985</ref_obj_id>
				<ref_obj_pid>970983</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Dourish P. What we talk about when we talk about context. <i>Personal and Ubiquitous Computing, 8</i> (1). 19--30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Ermi L. and M&amp;#228;yr&amp;#228;, F., Fundamental Components of the Gameplay Experience: Analysing Immersion. in <i>Digital Games Research Association (DiGRA) 2005</i>, (Vancouver, British Columbia, Canada, 2005), Digital Games Research Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Gibson J. J. The Theory of Affordances. in Shaw, R. E. and Bransford, J. eds. <i>Perceiving, Acting and Knowing</i>, Lawrence Erlbaum Associates, Hillsdale, N.J., 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[John T. Is "Rock Band" Drumming Just Like The Real Thing? Cartel Tests The Theory, MTV Multiplayer, 2007.http://multiplayerblog.mtv.com/2007/10/17/is-rock-band-drumming-just-like-the-real-thing-cartel-tests-the-theory/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Lin B. Narrative Interface Design: The Use of Interface Elements to Enhance the Narrative Experience in Videogames <i>School of Interactive Arts and Technology</i>, Simon Fraser University, Surrey, BC, 2007, 152.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[McLuhan M. The Playboy Interview. in McLuhan, E. and Zingrone, F. eds. <i>The Essential McLuhan</i>, Harper Collins, New York, New York, 1998, 233--269.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>551862</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Murray J. <i>Hamlet on the Holodeck: the future of narrative in cyberspace</i>. The MIT Press, Cambridge, Massachusetts, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Norman D. A. <i>The Design of Everyday Things</i>. Doubelday/Currency, New York, New York, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Rohrer T. The Body in Space: Dimensions of Embodiment. <i>Body, Language and Mind, 2</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Rock Band: A Case Study in the Design of Embodied Interface Experience Joshua Tanenbaum Simon Fraser 
University 250 13450 102nd Avenue Surrey, BC V3T 0A3 Canada 1-778-858-4113 joshuat@sfu.ca Jim Bizzocchi 
Simon Fraser University 250 13450 102nd Avenue Surrey, BC V3T 0A3 Canada 1-778-782-7437 jimbiz@sfu.ca 
 Abstract There has been a recent surge of novel interface devices available for home gaming systems. 
With the rise in popularity of games like Guitar Hero and consoles such as Nintendo s Wii comes new opportunities 
for game design at the interface level. In this paper we propose three interrelated dimensions for the 
analysis of embodied and gestural game interface hardware devices. We demonstrate how gestural and embodied 
interactions can be understood as ludic, kinesthetic and narrative experiences. We ground this discussion 
in a close analysis of the interface affordances of the game Rock Band and demonstrate how these three 
dimensions allow us to understand more clearly the place of the interface in the design and the experience 
of games. Categories and Subject Descriptors H.5.2 [Information Interfaces and Presentation]: User Interfaces 
-Input devices and strategies, Interaction styles, Theory and Methods, Haptic I/O, User-centered design. 
General Terms Design, Human Factors, Theory Keywords Game Interface, Embodiment, Narrative, Ludology, 
Novel Interaction, Physical Interaction Design 1. Introduction The lead singer grips her microphone in 
sweat drenched hands, eyes closed. There is a perfect moment of anticipation before the song, when the 
crowd of spectators goes quiet and the air is charged with electricity. Behind her, the drummer counts 
off four quick beats, clicking his sticks together in the air. Cymbal crash, and the guitar and bass 
both growl, and the audience roars back. The lead singer can feel the rhythm of the music vibrating through 
the floorboards in time with the pounding of the kick drum. Next to her, the guitar player is pogo-ing 
up and down in time with the music, his long hair flailing around his head. The song segues out of its 
intro and the singer belts out the first lyrics as the crowd goes wild. She wields her voice with carefully 
crafted precision an instrument that she has honed as finely as any of her bandmates and tonight the 
music rewards her by coming easily. A grin breaks across her face; she whips her head back, and dances 
in time with the music as the crowd joins in. Copyright &#38;#169; 2009 by the Association for Computing 
Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org. 
Sandbox 2009, New Orleans, Louisiana, August 4 6, 2009. &#38;#169; 2009 ACM 978-1-60558-514-7/09/0008 
$10.00 This story is remarkable, not because it is about a singer in a rock n roll band, but because 
it is about a group of gamers playing a video game with their friends at a party. The game is Rock Band, 
and the experience described is one of a new generation of embodied and gestural game interactions that 
have become very popular. As digital games have evolved, so have the interface devices that we use to 
play them. For many years, the video game arcade was the best place for players to enjoy a particular 
type of specialized game interface - those that offered a robust, embodied experience that physically 
immersed them in the world of the game. In Namco s Alpine Racer (1992) players maneuvered a downhill 
skier by shifting and tilting their body, with their feet strapped into a pair of free-floating skis 
; Atari s Road Burners (1999) simulated motorcycle racing by placing players on a motorcycle shaped controller 
which they could steer by turning the handlebars or by tilting the bike from side to side. Interfaces 
such as the light gun and the driving wheel moved a version of this embodied experience out of the arcade 
and into the home, with many examples of each available for home gaming consoles and computer systems. 
Recently, however, the use of embodied game hardware devices in home systems has exploded, led by the 
unprecedented popularity of Nintendo s Wii a gaming console that relies on a combination of accelerometer 
data and camera vision to track positions of controllers in each hand. The Wii is by no means the first 
home system to use gesture as input the Sony Eyetoy uses image tracking for gestural control; the Sega 
Activator used IR sensing; and the venerable Nintendo Power Glove used ultrasonic pulses for positioning. 
Nonetheless, it is the first such generalizable gesture-friendly device to achieve such widespread popularity. 
Concurrent with the appearance of the Wii, another new interface phenomenon has emerged as a significant 
force in the home gaming landscape: the music simulation game. Rock Band (Harmonix, 2007) is one of the 
latest members of this new species. Popularized by the game Guitar Hero, the genre is notable for the 
use of unique and specialized game controllers, much like its sister genres: the rhythm game (as exemplified 
by games such as Dance Dance Revolution and Pump It Up, Exceed) and the Karaoke Game (as exemplified 
by games such as SingStar and Karaoke Revolution). Rock Band combines the vocal interface of the Karaoke 
Game with a variation of the guitar controller introduced in Guitar Hero and provides a new interface 
device in the form of an electronic drum controller. [Figure 1] Rock Band s three controllers represent 
a range of the possibilities for new gestural and embodied game interface devices. To unpack the operations 
of these three novel interfaces we propose an analysis along three related dimensions: the ludic, the 
kinesthetic and the narrative. We argue that these three dimensions allow us to understand more clearly 
the place of the interface in the design and the experience of Rock Band.  Figure 1: The Three Rock 
Band Controllers 2. Ludic Interaction We use the term ludic from game studies as a shorthand for describing 
the aspects of game experience concerned with the pleasures of tactical and strategic gameplay in general, 
and with the notion of winning and losing in particular. The pleasures of ludic performance have often 
been linked to Csikszentmihalyi s notion of Flow, which proposes that pleasurable experiences often exist 
in a balanced state between boredom and frustration [Csikszentmihalyi 1990]. It is generally accepted 
that interactions at the ludic level must maintain a balance between being too simple, and thus boring, 
or too challenging, and thus frustrating. This balanced state is known as the Flow Channel [Csikszentmihalyi 
1990]. Using Csikszentmihalyi s concept as a starting point, we propose two distinct dimensions to more 
clearly articulate the understanding of ludic interface operation: ludic efficiency and granularity. 
 2.1 Ludic Efficiency We define ludic efficiency here as: the extent to which an interface device eases 
or hinders the player s attempt to perform any given operation within the game. Ludic efficiency explicitly 
remaps Csikszentmihalyi s notion of challenge onto a continuum. The most efficient interface is the one 
which is the easiest and most simplistic, the one which affords the shortest interactional path to success. 
A ludically inefficient interface is one that is challenging, and actively problematizes the interaction 
needed to succeed. To a certain extent, all game interfaces are at least slightly problematic, and need 
to be, in order to support a satisfying experience. We use the term ludic optimization to refer to the 
process of balancing these two sides of the ludic efficiency spectrum in order to provide a satisfying 
game experience. We can imagine the most efficient interface as a simple button, which when pressed 
 automatically wins the game. This game would not be very much fun, but the interface would be highly 
efficient. This button is an extreme example of the simplistic side of the spectrum. In contrast, we 
can imagine a game where the interface actively frustrates the interactions of the player by randomly 
changing the mapping of the controls, without cause. This game would also not be very much fun, due to 
the highly inefficient operations of the interface. This interface is an extreme example of the problematic 
side of the spectrum. Based on the flow channel s balance between boredom and frustration, there is an 
optimal position for an interface that balances the simplistic against the problematic. In many cases 
the design goal of the interface may be easing ludic efficiency. If other game variables are sufficiently 
challenging, the optimal interface would be an efficient one. However there are also examples of game 
interfaces where the interface is designed to problematize ludic efficiency. In the popular Soul Calibur 
games, ludic success relies on memorizing and executing complicated combinations of buttons and joystick 
movements. One of the central pleasures of this game is mastering a highly problematic interface. An 
example of a fighting game that uses more simplistic interface to achieve similar results is Capcom vs. 
SNK 2 EO, which allowed players to map these complicated operations to positions on a single analog joystick. 
Finding a balance between an interface which is too simplistic and an interface which is too problematic 
is a careful design choice, and one which is often central to the strategic experience of play, as it 
provides one of the sets of constitutive constraints that bound the game experience. The interface of 
Rock Band is particularly interesting in terms of its design stance towards ludic efficiency. The efficiency 
of Rock Band s interface varies with the controller chosen1. The microphone, for instance, provides a 
direct audio interface between the voice of the player and the game software, and does little to mediate 
that interaction (feedback is provided via an arrow that indicates whether the sung pitch is higher or 
lower than the expected pitch in real time). In this case, the game operation required for ludic success 
is perfectly aligned with the activity supported by the interface. Not only is the microphone a very 
simplistic interface, it is also a uniquely specialized one; the microphone could not be used as a controller 
for any game that does not rely on audio signals as input. We describe this type of controller as highly 
constrained in that it cannot be used for a more general purpose; however these constraints are optimizing 
constraints in that they uniquely afford the desired interaction. In the case of vocal interactions with 
the game, the challenge is mediated by two factors: the vocal capabilities of the player, and the extent 
to which the system forgives mistakes. Figure 2: Ludic Efficiency Scale for Rock Band Controllers The 
drum and guitar controllers are also specialized to the interactions demanded by the game, and are subsequently 
more efficient than a more general purpose interface, such as the standard Playstation or X-Box controller. 
[Figure 2] In particular, the physical design of both interfaces supports player success by mapping hardware 
positions and colors directly to visuals on the screen. The red, yellow, blue and green drum pads correspond 
to red, yellow, blue, and green notes on the screen, and are spatially laid-out to correspond with the 
screen based feedback [Figure 3]. Ludic success with the drums is a function of hitting the correct pad 
when prompted by the screen interface. The guitar is similarly designed, however, ludic success requires 
the player to select the right chord button with her left hand, and trigger it at the appropriate time 
by activating 1 While all three controllers share the same game narrative one of playing rock music it 
should be noted that the play itself across each of the three instruments is highly individuated; the 
vocalist and the drummer are playing very different games, even within the same shared experience. As 
such, it seems fair to treat each of these interactions as three distinct games that happen to overlap 
the same content domain. This being said, the game mechanics of the guitar and the drum controllers are 
more similar to each-other than they are to the microphone controller.  Figure 3: The software and hardware 
interface in action. the strum button with her right hand. Both interfaces, however, are more problematic 
than the microphone, in that both require a set of interface specific learned skills in order to succeed 
at the game. It might be argued that the guitar is slightly more problematic, due the increased complexity 
of the interaction. It is of particular interest to note that each of the interfaces in Rock Band finds 
a different point of ludic optimization on the spectrum between simplistic and problematic. In evaluating 
their relative efficiencies, we do not apply any value judgment to the interfaces; we simply observe 
how the various solutions across all three interfaces yield three distinct play experiences. The source 
of challenge in these experiences shifts progressively away from manipulation of an abstract interface 
device, and closer to the performance of a literal activity as the interface becomes more simplistic. 
We return to this notion of literal and abstract interactions in Section 4.  2.2 Granularity Granularity 
is another ludic interface dimension we find useful, particularly in relationship to predictability. 
Interface granularity in Rock Band is primarily related to the game s temporal resolution. Temporal resolution 
is the number of times that the system samples the interface state within a given period of time. We 
see a connection between the resolution of the system and perceptual capabilities of the player. We posit 
that systems whose granularity exceeds a resolution which the player can perceive may be experienced 
as unpredictable, while systems whose granularity is below a perceptual threshold for a given player 
may be experienced as unresponsive or clunky . One common complaint that players have in games is that 
there is a lag between their actions and those occurring on screen. This is a case of low temporal resolution 
interfering with the game experience. In the case of all three of the interface devices provided with 
Rock Band, input is measured at a very high temporal resolution. Vocal input appears to be continuous, 
and only the physical limitations of the button pushing appear to restrict the granularity of the drum 
and guitar controllers. The high temporal granularity of the system introduces some challenges of predictability, 
in that the controllers are able to measure nuances not immediately apparent to the players. An example 
of this is the vocal controller at the higher difficulty settings. Unlike the other controllers, which 
are modeling discreet and binary controller states ( Did the player push the right button at the right 
time? ), the vocal controller measures a continuous analog audio signal. The vocal system must assess 
whether or not the player is singing at the right time, whether or not she is singing the correct pitch, 
and in the case of atonal spoken word parts, whether she is enunciating the correct phonemes. While it 
is possible to scale the difficulty of the songs on the other controllers by simply introducing more 
complex patterns of button pushing, in order to make the vocals more difficult the game must increase 
the temporal resolution, narrowing the zone within which it considers a given audio input correct. At 
the highest levels this requires a degree of control over vocal intonation that is not perceptible to 
most players, and which is difficult to consistently reproduce. In this case the high granularity of 
the system, while predictable from the perspective of the system s technical design, is experienced as 
unpredictable when experienced by any but the most trained vocal performers. 3. Kinesthetic Interactions 
Beyond concerns of winning and losing, there is a physical pleasure associated with engaging in Rock 
Band with the entire body. Traditional video game interactions take the player outside of an awareness 
of the body. A player using a control pad is often only aware of her body when something happens to disrupt 
the experience, such as when she contracts the repetitive stress injury known colloquially as Gamer s 
Thumb . By contrast, the interface of Rock Band leverages the player s sense of bodily perception, and 
incorporates it within the play experience in a meaningful way. In this section we will look at this 
notion of embodiment in some detail, and consider how systems for evaluating physical gesture and movement 
in other domains, such as theater and dance, may be used to understand the interactions in Rock Band. 
3.1 Embodiment Embodiment is a widely used term which commonly refers to an awareness of one s bodily 
condition, or presence. Rohrer terms the notion of bodily awareness phenomenological embodiment, and 
also describes a form of embodiment that arises from the particular socio-cultural situation in which 
it is located [Rohrer 2007]. We connect this to notions of context and the concept of affordance.  
3.1.1 Phenomenological Embodiment Phenomenological embodiment refers to our own conscious awareness of 
the role of our bodies in shaping our identities and experiences [Rohrer 2007]. Rock Band leverages this 
awareness both implicitly and explicitly. The guitar controller contains a tilt sensor which responds 
when the guitar neck is rocked upwards. This interaction requires the player to drop one shoulder down, 
while raising her fretting hand upward. This twists the player s torso and shifts her center of gravity. 
 The result of this simple interface action is that the player rocks her body into a stereotypical guitar 
pose, assuming the postures of hundreds of lead guitar players throughout the ages. In doing so, the 
player activates a special ability in the game called Overdrive which results in a temporarily higher 
score, as well as a more enthusiastic performance from the player s virtual on­screen counterpart. For 
drummers, the interface relies on continuous and explicit bodily awareness in order to succeed, due to 
the demands on the player s coordination of arm and foot motions. In addition to the explicitly embodied 
interactions found in the design of the interface, there is an implicit focus on the player s body that 
emerges from the game s emphasis on musical performance. While it is possible to play Rock Band as a 
purely strategic exercise, one of the central pleasures of the game is bodily engaging in the fantasy 
of being a Rock Star. This often means moving in rhythm to the music, dancing with the microphone, pogo-ing 
up and down while playing guitar, twirling the drum sticks, and generally performing the role of a rock 
star. All of these behaviors, while not explicitly required in order to succeed at the game, are afforded 
and encouraged by the design of the interface and an implicit aspect of the bodily experience of the 
game. As we will see in the Section 4, there is a close relationship between these embodied activities 
and the game narrative.  3.1.2 Embodied Context Another facet of embodiment that is at work in Rock 
Band illustrates Rohrer s notion of the socio-cultural situation. In his survey of embodiment he describes 
how habitual interactions with the environment shape our embodied mind , writing: The experiential worlds 
with which we interact are more than simply physical; we are born into social and cultural milieus which 
transcend our individual bodies in time. Tools are an excellent example of the elements of our physical 
world that come to us already shaped by socio­cultural forces which predate each individual s body, if 
not the human body in general. [Rohrer 2007] Tools are shaped by an exchange of the affordances of the 
task to which the tool is to be applied and the affordances of the body which will be using the tool. 
This notion of affordances is one which was first described in 1977 by J.J. Gibson [Gibson 1977], and 
articulated in terms of design more recently by Donald Norman [Norman 1988]. Norman describes the relationship 
between tools and the body in terms of a three part conceptual model comprised of the notions of affordance, 
constraints and mappings: Consider a pair of scissors: even if you have never seen or used them before, 
you can see that the number of possible actions is limited. The holes are clearly there to put something 
into and the only logical things that will fit are fingers. The holes are affordances: they allow the 
fingers to be inserted. The sizes of the holes provide constraints to limit the possible number of fingers: 
the big hole suggests several finger, the small hole only one. The mapping between holes and fingers 
 the set of possible operations is suggested and constrained by the holes. [Norman 1988]. Similarly, 
the different interfaces of Rock Band provide distinct affordances, constraints, and mappings. The microphone 
affords being held in the hand, and constrains game interactions to vocalizations (and occasionally hitting 
the mic with the palm of the hand to simulate hitting a tambourine, or hand clapping.) The mapping between 
these two presents no ambiguity about how the microphone is to be used, but this conceptual model only 
works due to the pre-existing socio-cultural models that have shaped the evolution of the microphone 
into a recognizable object. Similarly, the guitar controller is the sum of its affordances and constraints, 
resulting in a conceptual model of use that invokes many of the bodily experiences of actual guitar playing. 
The drum controller has some of the clearest constraints in that it must be played sitting down, using 
both hands and feet. Beyond the conceptual models of the interface as tools, there are even broader contextual 
implications embedded within the Rock Band interface. Embodied interaction theorist Paul Dourish regards 
context as arising from a social situation, asking how and why, in the course of their interactions, 
do people achieve and maintain a mutual understanding of the context for their actions? [Dourish 2004] 
This notion of context as socially constructed provides us with a useful perspective for understanding 
the setting in which Rock Band s interactions occur. Although it is possible to enjoy the game alone, 
there is an implicit social experience built into the design of Rock Band s gameplay and narrative. Unlike 
Guitar Hero, which glorifies the lead guitar player as an icon, Rock Band pays homage to the band as 
a collaborative vehicle. When one player fails at a song, another player is able to rescue him. The ensemble 
nature of the controllers reinforces this, as does the narrative framing of the game experience. Rock 
Band also remediates the rock concert experience from both sides, creating an environment that allows 
non-players to engage as spectators, and encourages players to perform their parts for their audience, 
as well as for the game score. This social setting draws the game experience out of what is happening 
on the screen, and causes it to infuse the relationships of all of the bodies in the room. 3.2 Viewpoints 
Theater and dance have long been concerned with the ways in which meaning emerges from the movements 
and positions of the human body. Viewpoints is a framework from dance and theater designed to interrogate 
and design bodily actions for performative purposes [Bogart and Landau 2005]. These same tools may be 
brought to bear on the kinesthetic experiences that arise as a result of the affordances of gestural 
and embodied game interface devices. The viewpoints framework is split into two categories space and 
time containing nine distinct viewpoints. The Viewpoints of Time include: Tempo, Duration, Kinesthetic 
Response, and Repetition. The Viewpoints of Space include: Shape, Gesture, Architecture, Spatial Relationship, 
and Topography. We will not explore all nine of these viewpoints in depth in this paper, but we will 
consider several of them in context of the interactions afforded by Rock Band.  3.2.1 Viewpoints of 
Time Rock Band belongs to a genre commonly known as rhythm games and so the viewpoints of tempo, duration, 
and repetition are central to the ludic play of the game. In Viewpoints, there are two distinct forms 
of repetition: internal and external. Internal repetition is used to describe the repetition of movements 
within the human body, such as heartbeat or breathing pattern; external repetition is used to refer to 
repetition of movements and tempos outside the body [Bogart and Landau 2005]. These are most apparent 
in the play afforded by the Drum controller, which requires both internal repetition (players must create 
and maintain an internal model of the song s rhythm) and external repetition (players must follow visual 
and auditory clues on the screen) in order to succeed. Kinesthetic response is defined as a spontaneous 
reaction to motion which occurs outside you; the timing in which you respond to the external events of 
movement or sound; the impulsive movement that occurs from a stimulation of the senses. [Bogart and Landau 
2005] This is observable across all Rock Band interactions as players move their bodies in response to 
the music, but is most afforded by the vocal interface, which leaves the player free to move her body 
without sacrificing ludic success. In our own play we have noticed a correlation between dramatic performance, 
ludic success and kinesthetic response: as we have grown more skilled in the ludic aspects of the game 
we are more likely to rock out narratively and kinesthetically.  3.2.2 Viewpoints of Space Shape is 
the contour or outline the body makes in space. [Bogart and Landau 2005] In regards to game interface 
we can consider shape as the posture which the device places the player in. This corresponds closely 
to Norman s notion of constraints. Gesture is shape with a beginning, middle and end [Bogart and Landau 
2005]. Norman s notion of mapping includes a set of possible operations , which closely corresponds to 
this notion of gesture. In our discussion of the embodied affordances of Rock Band s interface we have 
already considered how the design of the devices places the player into a particular shape and affords 
specific gestures. As we will discuss in the section 4, these embodied interactions invoke a narrative 
of rock and roll performance that goes beyond the constraints of ludic play. This means that even if 
the player is playing with the minimum amount of kinesthetic attachment to the experience, the interface 
has placed her bodily into the role of the Guitar player through its physical constraints and ludic requirements. 
The remaining three viewpoints of space Architecture, Spatial Relationship, and Topography deal with 
the context in which the play occurs. Architecture deals with the design of the physical environment 
in which the performance occurs; spatial relationship addresses the relationship between the bodies of 
the performers, and topography describes the pattern described by a performer s movement through space 
[Bogart and Landau 2005]. While the experience of Rock Band is constrained somewhat by architectural 
and spatial factors, it does not engage the player in her environment as much as it might. Specifically, 
the game affords little awareness of the topography or stage picture painted through play. However, it 
is possible to alter the context of play in ways that draw on these three viewpoints. It is not uncommon 
to see performances of Rock Band at gaming conventions and expos, and in these cases the architecture, 
spatial relationship, and topography of the play become significant aspects of the game s appeal. By 
placing the game on a stage with lights and effects and by encouraging the players to be aware of their 
role as performers for a real audience, it is possible to transform Rock Band from a game about rock 
music to a simulation of a rock concert. Even inside more humble settings, this phenomenon can be observed, 
with players rocking out for friends and family at social gatherings, as described in section 3.1.2. 
As a final observation on the place of the kinesthetic in our overall evaluation of Rock Band, it is 
interesting that the temporal kinesthetic perspective correlates strongly with the ludic. This is not 
surprising for a game where the winning and losing are dependent on timing. The spatial kinesthetic perspective, 
on the other hand, correlates more with the narrative - for reasons which our discussion in the next 
section should make clear. 4. Narrative Interactions The role of narrative and story within the larger 
game experience is an ongoing, and sometimes controversial, issue in the area of games studies. Luckily, 
the overblown conflict between narratologists and ludologists has subsided, and researchers are concentrating 
their efforts on developing a more focused understanding of how narrative can inflect and enhance the 
experience of a game. Bizzocchi maintains that an emphasis on the overall narrative arc, although critical 
to the understanding of story in media such as film or the novel, is of limited utility in understanding 
interactive media such as electronic games. He argues that the examination of narrative parameters such 
as storyworld, character, and emotion is more useful in this regard [Bizzocchi 2007]. Rock Band is not 
an experience with the tightly controlled narrative arc of a film, but there is a looser central narrative 
in the progression of the player s character from unknown musician to rock superstar. As the band moves 
through the game, the player unlocks progressively higher-profile concert venues, and is awarded with 
the trappings of rock and roll success: cooler clothes and cooler instruments. The game invites players 
to live out the positive aspects of the rock n roll fantasy, without invoking the darker side that is 
often a part of such rise­to-stardom narratives. One of the parameters Bizzocchi cites is the narrativized 
interface , which involves the incorporation of narrative elements and sensibilities within the design 
of the game interface [Bizzocchi and Woodbury 2003]. We have already seen how the embodied aspects of 
the interface can kinesthetically invoke certain aspects of narrative experience. In this section we 
will take a more focused and explicit look at the role of narrative within the game s interface. 4.1 
The Narrativized Interface: Iconic and Functional Bizzocchi identified two broad strategies for narrativizing 
the interface. The first is modifying the look of the interface to reflect narrative themes such as character. 
The second is to modify the functionality of the interface so that the player s interactions were shaped 
and channeled in ways that expressed and reinforced narrative concepts [Bizzocchi 2003]. Lin extended 
this look at the narrativized interface, starting with a formal classification of interface structure, 
and then identifying six distinct strategies used by game designers to incorporate narrative within the 
design and the experience of the interface [Lin 2007] . Lin and Bizzocchi agree that two possibilities 
for interface narrativization are the look of the interface ( iconic narrativity) and the actual operation 
of the interface ( functional narrativity). Both narrativization strategies can be seen in Rock Band, 
with differences in emphasis depending on the instrument. For example, the drum and the guitar interfaces 
are both narrativized at the iconic level - that is, each of these interfaces resemble the instruments 
they represent. However, because the interactions with the Rock Band drums more directly resemble the 
interactions with an actual drum kit, the drums represent a greater degree of functional narrativisation 
than the guitar.  4.2 Metaphorical vs. Literal Interactions Functional narrativization of an embodied 
and gestural interface can be situated on an axis from the literal to the metaphoric. A more literal 
functional narrativization within an embodied interface will afford and channel physical interactions 
that more directly reproduce the movement, gesture and functionality of its real world referent. A more 
metaphoric functional narrativization will afford and channel physical interactions that evoke (rather 
than reproduce) a real world referent. [Figure 4] Figure 4: Metaphorical &#38; Literal Functional Narrativization 
 The drum controller more accurately affords the functionality of its reference instrument than the guitar 
controller, so the latter is more metaphorical and the former is more literal. However, the microphone 
voice controller is even more literal than either. The player performing the vocal part of the song does 
not play at singing the music, but must instead actually sing the song in order to succeed. Unlike the 
other interactions in Rock Band, which represent points embedded within this continuum, the vocal performances 
require that players engage in the represented activity completely and authentically. This is a rare 
example of an almost completely literal narrativized interface design. In contrast to all three of the 
Rock Band interface devices, the conventional generic input device, such as the PS3 6-axis controller, 
affords little if any narrativized gestural or embodied interaction. These standardized controllers fall 
off the metaphoric end of the scale any relationship they have to narrativized action is purely abstract. 
[see Figure 4] Metaphorically or literally, each of the three Rock Band interfaces offers the user a 
narratively-rich channel for interactive experience. This robust narrative connection is one of the most 
potent attributes of the interface and the game. Each individual interface allows us to play and to be 
a character whose role we understand, admire, and covet a rock and roll performer. The combination allows 
us to enter a richly simulated storyworld in the socially rewarding context of a live band. Narrative 
and gameplay are united in a mutually reinforcing act of performance. 5. Conclusions In this paper we 
have presented three separate dimensions for the analysis of interface in the game Rock Band: the Ludic, 
the Kinesthetic, and the Narrative. While these separate dimensions have allowed us to explicitly address 
specific phenomenon within the game experience, we do not claim that these vectors of analysis are truly 
isolated from each other. Over the course of this analysis we have encountered a number of issues that 
have manifested across all three vectors. We view the places where these categories overlap and merge 
with each other as indicators of larger, more universal phenomena. These junctures throw into relief 
the broader issues at play within the realm of embodied and gestural game interactions. We see connections 
across our categories in the dynamics of immersion within the game. Another common theme that runs through 
our analysis is the understanding of how embodied interfaces leverage a player s prior knowledge. Finally, 
we would like to discuss the relationship between the concept of transparent interface and our three 
dimensions. 5.1 Three Types of Immersion Immersion is often positioned as the holy grail of interactive 
experience. It has been described as the sensation of being surrounded by a completely other reality 
[ ] that takes over all of our attention, our whole perceptual apparatus [Murray 1997]. As a phenomenon, 
immersion is complex and difficult to quantify. Ermi and Mäyrä parse immersion into three distinct categories: 
imaginative immersion, challenge-based immersion, and sensory immersion [Ermi and Mäyrä 2005]. Ermi and 
Mäyrä s conception of immersion echoes the formulations of earlier media theorists. Their imaginative 
immersion corresponds to Coleridge s notion of the willing suspension of disbelief [Coleridge 1952 c1906]. 
Murray extends Coleridge s notion and suggests that in interactive environments the interactor actively 
creates belief in the fictional world [Murray 1997]. Ermi and Mäyrä explicitly ground their challenge-based 
immersion in Csikszentmihalyi s notion of Flow, which we discussed in section 2 [Csikszentmihalyi 1990]. 
Sensory immersion is consistent with McLuhan s claim that there is a strong connection between the human 
sensorium and the mediated experience [McLuhan 1998]. We see a strong correlation between our three dimensions 
of interaction and Ermi and Mäyrä s three-fold conception of immersion. In exploring the operations of 
the interface at the ludic level, we described several design metrics which were rooted in dynamics of 
challenge and flow. In considering the ways in which narrative played out at the interface level, we 
saw several techniques that supported the imagined experience of character and storyworld. Finally, in 
our discussion of the kinesthetic aspects of the Rock Band interface, we described the manner in which 
the player is immersed in the sensory and embodied experience of the game. The effectiveness of the Rock 
Band interfaces lies in the fact that they support immersion across all three of Ermi and Mäyrä s directions. 
By optimizing the ludic efficiency, extending the game sensorium into the bodily context of the player, 
and transforming the experience into an embodied representation of narrative these interfaces provide 
a complex gateway for multivalent immersion.  5.2 Prior Knowledge One widely held belief about embodied 
game controllers is that they are more intuitive , or more transparent , than a traditional game controller. 
We will unpack the notion of transparency in further detail below. For the moment we d like to propose 
that one reason for the pervasiveness of this belief is due to the extent to which the interface leverages 
prior knowledge and experience into the gameplay, and consequentially how the interface allows successful 
gameplay to be transposed back into real world capabilities. In Rock Band, we find a clear example of 
this in the social conventions surrounding the use of the microphone controller. When we introduced the 
game at a social function, the first thing that became apparent was that the person with the microphone 
was responsible for selecting the song for the group to perform. This was due to a number of observable 
factors, but the most significant was that the vocalist needed familiarity with the song in order to 
succeed at the game, whereas the guitar player and drummer did not. It was also observed that the vocal 
controller quickly became the subject of the most social pressure, due in part to its reliance on actual 
singing ability. In Rock Band, a skilled vocalist who is familiar with the songs is able to succeed much 
more immediately than an inexperienced or unskilled vocalist. In this sense, the interface allows the 
player to translate prior knowledge directly into ludic success. Conversely, we suspect that learning 
how to play the game better could result in the player attaining a greater degree of real world vocal 
skill. To a certain extent, the narrative of the lead singer as band front-man is invoked by the simple 
fact that the vocal part relies on a greater degree of prior knowledge. This is congruent with the idea 
that the vocal controller is more narratively literal than the other interface devices. By comparison, 
we see a far more tenuous connection between the act of playing guitar, and the interactions afforded 
by the guitar controller in Rock Band. The relationship between prior guitar playing experience and success 
at playing the guitar controller is in fact the subject of some debate. Arsenault describes the simulational 
fidelity of Guitar Hero s (and by extension Rock Band s) guitar controller as favoring breadth over depth. 
He describes in detail the ways in which the guitar controller fails to simulate activities required 
by an actual guitar, concluding that the controller broadly simulates the core tasks of harmony, melody, 
and rhythm, but in an abstracted sense [Arsenault 2008]. MTV performed an informal study in which it 
asked a band of professional rock musicians to play their corresponding instruments in order to explore 
contentions that musical skill could translate into successful play [John 2007]. The results suggested 
that actual guitar ability confounded success in the game. There s only five buttons on this guitar, 
where on the real guitar there s tons of frets. So you have to really pay attention to where all the 
dots are going and what you re supposed to be playing than how the song [actually goes]. [John 2007] 
While this is only a single case, it does indicate that understanding how the real-world playing of the 
song actually goes can interfere with success in the game-world playing of the song. This supports our 
notion that the guitar interaction in Rock Band is problematized by prior knowledge, rather than simplified 
by it. Unlike the vocal controller, the operation of the guitar controller is a specialized and learned 
skill that does not map directly to any prior life experience and so while the design of the hardware 
interface supports success in the game, it also requires an extensive learning period in order to be 
mastered. By extension, it is fair to assume that skill acquired through the use of the guitar controller 
does not translate back to skill at playing an actual guitar. One common assertion within the gaming 
community is that drumming in Rock Band can train players to drum in real life. In the aforementioned 
MTV study, this was at least partially borne out, as is evidenced in the comments of the drummer playing 
Rock Band Drums: He also thought playing the Rock Band drum kit was more authentic to real drumming when 
it was played on expert mode. The easier [the song is in the game], it takes away what s actually in 
the song, he explained. But if you actually know how the drums go in your head and you re reading the 
notes [on the screen], it doesn t really make sense in the tune. I guess the harder [the difficulty] 
is, the more real the song is [to actually playing it]. [John 2007] It is especially notable that the 
drummer claimed to find the difficult level songs to closely resemble actual drumming. It is evident 
from this comment that the higher difficulty setting allowed him to leverage his own prior knowledge 
into a higher score. The drum controller s interaction paradigm draws on the skills needed by actual 
drummers (keeping time, maintaining multiple beats simultaneously, coordinating hands and feet), but 
operates via a simplified abstracted interface. The perception of the drum controller as a bridge between 
the ludic activity and the actual activity is a valid one. Much like the guitar controller, the drum 
set consists of a set of color-coded buttons, which correspond to a stream of color-coded instructions 
that scroll toward the player on the screen. Like the guitar controller, mastery of the drum kit requires 
extensive practice; however, like the voice controller it is possible for players with prior drumming 
experience to bypass a large portion of the learning curve. It seems reasonable to suggest that this 
learning curve operates both ways, although more research would be needed to confirm that Rock Band could 
act as a training aid for real drumming. When someone says an interface is intuitive, we suggest that 
what they are actually talking about is the extent to which the interface leverages prior knowledge, 
however, the discussion above includes an implicit warning against interfaces that bear a resemblance 
to an actual activity, but whose operation may be confounded by prior knowledge. In the next section 
we will look at the similar, but distinct notion of transparency in interface.  5.3 Transparency One 
common claim made about gestural and embodied interfaces is that they are transparent. Like claims that 
an interface is intuitive, it also operates on a relationship between the expectations of the user and 
operations of the interface. For the most part, this notion of transparency in gestural and embodied 
interfaces has remained unexplored. In this section we will discuss some common theories of transparency 
from HCI and explore how they can be understood in terms of our three dimensions. Transparency is a condition 
that is commonly associated with the idea of the user s awareness of the interface disappearing, resulting 
in an unmediated relationship between the interactor and the activity being performed, as in Bolter and 
Grusin s notion of transparent immediacy [Bolter and Grusin 1999]. Research on transparency in interaction 
suggests that transparency is a contingent phenomenon, emerging from a context of use. Bardram and Bertelsen 
write that first transparent interaction is not a property of the interface by itself, but a quality 
of the use activity; and second, that transparent interaction is developed by the user during interaction. 
[Bardram and Bertelsen 1995] They use Activity Theory to analyze the conditions which support the development 
of transparent interaction, concluding that it is impossible to identify absolute transparent features 
which need to be included in any interaction. However, they do recommend possible options which may help 
when designing for transparency. One is the notion of supporting development in use: the design of interactions 
that automatically train the interactor in the intended purposes of the system. Another is the notion 
of initial familiarity, which draws on metaphors from the users prior experience in order to initiate 
deeper exploration and learning of the system [Bardram and Bertelsen 1995]. These conditions suggest 
that a transparent interface is one which affords the type of interactions required for success within 
the system, while leveraging the prior experience of the user in order to bridge the gap between her 
existing capabilities and those needed to succeed within the system. For these reasons, claims about 
interfaces that are intuitive are bound up in issues of prior knowledge and transparency. It is possible 
to see the conditions for transparency throughout our earlier analysis of Rock Band. We argued that the 
vocal interface leverages a player s prior knowledge into success in the game, and supports learning 
real world skills through the acquisition of game skills. In this sense the vocal controller for Rock 
Band is highly transparent in that is draws on initial familiarity, while the highly specific design 
of the controller affords the single interaction needed for ludic success: vocalization. The same can 
be said of the drum controller. The guitar does not draw on initial familiarity at the same level; however 
it ably supports development in use, by affording the interactions needed to succeed, constraining the 
ways in which the interactor uses it to those which are ludically appropriate and narratively salient, 
and ultimately mapping to a set of operations that support the learning of the interface. At the same 
time, the guitar controller also shows how prior knowledge can interfere with the learning of an interface 
s operation if that knowledge fails to properly align with the interactions actually afforded by that 
interface. 6. Future Work We have presented three different perspectives for the analysis of embodied 
and gestural interface devices, and demonstrated their utility through a close analysis of the game Rock 
Band. We have connected these perspectives to larger themes that we believe further inform the understanding 
of the experiences afforded by these devices. The next phase of our research is to determine the reach 
of our three perspectives in the analysis of a wider range of embodied and gestural game interface devices. 
A related question is whether these analytical tools have further utility as design guidelines. We hope 
this is the case, because we feel that these are important questions. We believe that embodied interface 
devices such as those used in Rock Band and the Wii will continue to grow in popularity and in their 
cumulative impact on user experience. The true test of our scholarship will be in its ability to help 
in both the analysis and in the evolution of this new generation of interface devices. 7. Acknowledgements 
This research has been undertaken as part of the Creativity Assistive Tools for Games Project, funded 
by Canadian Heritage. Additional financial support has been provided by the Province of British Columbia 
through the Ministry of Advanced Education in the form of a Pacific Century Graduate Scholarship. 8. 
References ARSENAULT D. Guitar Hero: "Not like playing guitar at all"? Loading... 1 (2). BARDRAM J.E. 
AND BERTELSEN, O.W. Supporting the development of transparent interaction. in Human-Computer Interaction, 
Springer Berlin / Heidelberg, 1995, 79-90. BIZZOCCHI J. Ceremony of Innocence and the Subversion of Interface: 
Cursor Transformation as a Narrative Device DAC 2003 : Digital Arts and Culture, Melbourne, Australia, 
2003. BIZZOCCHI J. Games and Narrative: An Analytical Framework. Loading - The Journal of the Canadian 
Games Studies Association, 1 (1). 5-10. BIZZOCCHI J. AND WOODBURY, R. A Case Study in the Design of Interactive 
Narrative: the Subersion of the Interface. Simulation and Gamin, 34 (4). BOGART A. AND LANDAU, T. The 
Viewpoints Book: A Practical Guide to Viewpoints and Composition. Theatre Communications Group, New York, 
2005. BOLTER J.D. AND GRUSIN, R. Immediacy, Hypermediacy, and Remediaton. The MIT PRess, Cambridge, Mass, 
USA, 1999. Coleridge S.T. Biographia Literaria. E.P. Dutton, London &#38; New York, 1952 c1906. CSIKSZENTMIHALYI 
M. Flow: The Psychology of Optimal Experience. Harper Perennial, New York, 1990. DOURISH P. What we talk 
about when we talk about context. Personal and Ubiquitous Computing, 8 (1). 19-30. ERMI L. AND MÄYRÄ, 
F., Fundamental Components of the Gameplay Experience: Analysing Immersion. in Digital Games Research 
Association (DiGRA) 2005, (Vancouver, British Columbia, Canada, 2005), Digital Games Research Association. 
GIBSON J.J. The Theory of Affordances. in Shaw, R.E. and Bransford, J. eds. Perceiving, Acting and Knowing, 
Lawrence Erlbaum Associates, Hillsdale, N.J., 1977. JOHN T. Is "Rock Band" Drumming Just Like The Real 
Thing? Cartel Tests The Theory, MTV Multiplayer, 2007.http://multiplayerblog.mtv.com/2007/10/17/is-rock­band-drumming-just-like-the-real-thing-cartel-tests-the­theory/ 
LIN B. Narrative Interface Design: The Use of Interface Elements to Enhance the Narrative Experience 
in Videogames School of Interactive Arts and Technology, Simon Fraser University, Surrey, BC, 2007, 152. 
MCLUHAN M. The Playboy Interview. in McLuhan, E. and Zingrone, F. eds. The Essential McLuhan, Harper 
Collins, New York, New York, 1998, 233-269. MURRAY J. Hamlet on the Holodeck: the future of narrative 
in cyberspace. The MIT Press, Cambridge, Massachusetts, 1997. NORMAN D.A. The Design of Everyday Things. 
Doubelday/Currency, New York, New York, 1988. ROHRER T. The Body in Space: Dimensions of Embodiment. 
Body, Language and Mind, 2.   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1581094</section_id>
		<sort_key>210</sort_key>
		<section_seq_no>5</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Kinesthetic movement in games II]]></section_title>
		<section_page_from>135</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1570351</person_id>
				<author_profile_id><![CDATA[81365593292]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Karen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schrier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1581095</article_id>
		<sort_key>220</sort_key>
		<display_label>Pages</display_label>
		<article_publication_date>08-04-2009</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Art of defense]]></title>
		<subtitle><![CDATA[a collaborative handheld augmented reality board game]]></subtitle>
		<page_from>135</page_from>
		<page_to>142</page_to>
		<doi_number>10.1145/1581073.1581095</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1581095</url>
		<abstract>
			<par><![CDATA[<p>In this paper, we present Art of Defense (AoD), a cooperative handheld augmented reality (AR) game. AoD is an example of what we call an AR Board Game, a class of tabletop games that combine handheld computers (such as camera phones) with physical game pieces to create a merged physical/virtual game on the table-top. This paper discusses the technical aspects of the game, the design rationale and process we followed, and the resulting player experience. The goal of this research is to explore the affordances and constraints of handheld AR interfaces for collaborative social games, and to create a game that leverages them as fully as possible. The results from the user study show that the game is fun to play, and that by tightly registering the virtual content with the tangible game pieces, tabletop AR games enable a kind of social play experience unlike non-AR computer games. We hope this research will inspire the creation of other handheld augmented reality games in the future, both on and off the tabletop.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[augmented reality]]></kw>
			<kw><![CDATA[collaborative game]]></kw>
			<kw><![CDATA[handheld game]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1570352</person_id>
				<author_profile_id><![CDATA[81537317456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Duy-Nguyen]]></first_name>
				<middle_name><![CDATA[Ta]]></middle_name>
				<last_name><![CDATA[Huynh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570353</person_id>
				<author_profile_id><![CDATA[81440609381]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Karthik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raveendran]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570354</person_id>
				<author_profile_id><![CDATA[81339537775]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570355</person_id>
				<author_profile_id><![CDATA[81385604335]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kimberly]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Spreen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570356</person_id>
				<author_profile_id><![CDATA[81100099162]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Blair]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[MacIntyre]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2156926</ref_obj_id>
				<ref_obj_pid>2156905</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Barkhuus, L., Chalmers, M., Tennent, P., Hall, M., Bell, M., Sherwood, S., and Brown, B. 2005. Picking pockets on the lawn: The development of tactics and strategies in a mobile game. In <i>Proceedings of UbiComp 2005</i>, Springer, 358--374.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Billinghurst, M., Imamoto, K., Kato, H., and Tachibana, K. 2000. Magic paddle: A tangible augmented reality interface for object manipulation. <i>In Proceedings of the IEEE and ACM International Symposium on Augmented Reality</i>, 111--119.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bj&amp;#246;rk, S., Falk, J., Hansson, R., and. Ljungstr, P. 2001. Pirates! using the physical world as a game board. In <i>Proceedings of Interact 2001</i>, 9--13.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bradley, D., and Roth, G. 2007. Adaptive thresholding using the integral image. <i>Journal of Graphics Tools 12</i>, 2, 13--21.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1152430</ref_obj_id>
				<ref_obj_pid>1152399</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Henrysson, A., Billinghurst, M., and Ollila, M. 2005. Virtual object manipulation using a mobile phone. In <i>ICAT '05: Proceedings of the 2005 International Conference on Augmented Tele-existence</i>, ACM, New York, NY, USA, 164--171.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1179865</ref_obj_id>
				<ref_obj_pid>1179849</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Henrysson, A., Billinghurst, M., and Ollila, M. 2006. Ar tennis. In <i>SIGGRAPH '06: ACM SIGGRAPH 2006 Sketches</i>, ACM, New York, NY, USA, 13.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1321295</ref_obj_id>
				<ref_obj_pid>1321261</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Henrysson, A., Marshall, J., and Billinghurst, M. 2007. Experiments in 3d interaction for mobile phone ar. In <i>GRAPHITE '07: Proceedings of the 5th International Conference on Computer graphics and interactive techniques in Australia and Southeast Asia</i>, ACM, New York, NY, USA, 187--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1514363</ref_obj_id>
				<ref_obj_pid>1514339</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Klein, G., and Murray, D. 2007. Parallel tracking and mapping for small ar workspaces. In <i>ISMAR '07: Proceedings of the 2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality</i>, IEEE Computer Society, Washington, DC, USA, 1--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>506523</ref_obj_id>
				<ref_obj_pid>506443</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Mandryk, R. L., and Maranan, D. S. 2002. False prophets: exploring hybrid board/video games. In <i>CHI '02: CHI '02 extended abstracts on Human factors in computing systems</i>, ACM, New York, NY, USA, 640--641.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1413718</ref_obj_id>
				<ref_obj_pid>1413634</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Mulloni, A., Wagner, D., and Schmalstieg, D. 2008. Mobility and social interaction as core gameplay elements in multi-player augmented reality. In <i>DIMEA '08: Proceedings of the 3rd international conference on Digital Interactive Media in Entertainment and Arts</i>, ACM, New York, NY, USA, 472--478.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1178845</ref_obj_id>
				<ref_obj_pid>1178823</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Peitz, J., Bj&amp;#246;rk, S., and J&amp;#228;ppinen, A. 2006. Wizard's apprentice gameplay-oriented design of a computer-augmented board game. In <i>ACE '06: Proceedings of the 2006 ACM SIGCHI International Conference on Advances in Computer Entertainment technology</i>, ACM, New York, NY, USA, 79.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1357283</ref_obj_id>
				<ref_obj_pid>1357054</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Szentgyorgyi, C., Terry, M., and Lank, E. 2008. Renegade gaming: practices surrounding social use of the nintendo ds handheld gaming system. 1463--1472.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2154290</ref_obj_id>
				<ref_obj_pid>2154273</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Wagner, D., Pintaric T., and Ledermann, Florianand Schmalstieg, D. 2005. Towards massively multiuser augmented reality on handheld devices. In <i>Proceedings of the Third International Conference on Pervasive Computing</i>, 208--219.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1605358</ref_obj_id>
				<ref_obj_pid>1605298</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Wagner, D., Langlotz, T., and Schmalstieg, D. 2008. Robust and unobtrusive marker tracking on mobile phones. In <i>ISMAR 2008: Proceedings of the 7th IEEE/ACM International Symposium on Mixed and Augmented Reality, 2008.</i>, 121--124.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1605359</ref_obj_id>
				<ref_obj_pid>1605298</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Wagner, D., Reitmayr, G., Mulloni, A., Drummond, T., and Schmalstieg, D. 2008. Pose tracking from natural features on mobile phones. <i>ISMAR 2008: Proceedings of the 7th IEEE/ACM International Symposium on Mixed and Augmented Reality, 2008</i>. (Sept.), 125--134.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1501816</ref_obj_id>
				<ref_obj_pid>1501750</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Xu, Y., Gandy, M., Deen, S., Schrank, B., Spreen, K., Gorbsky, M., White, T., Barba, E., Radu, I., Bolter, J., and MacIntyre, B. 2008. Bragfish: exploring physical and social interaction in co-located handheld augmented reality games. In <i>ACE '08: Proceedings of the 2008 International Conference on Advances in Computer Entertainment Technology</i>, ACM, New York, NY, USA, 276--283.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Art of Defense: A Collaborative Handheld Augmented Reality Board Game Duy-Nguyen Ta Huynh*, Yan Xu 
, Karthik Raveendran , Kimberly Spreen§, Blair MacIntyre¶ School of Interactive Computing and GVU Center 
Georgia Institute of Technology Abstract In this paper, we present Art of Defense (AoD), a cooperative 
hand­held augmented reality (AR) game. AoD is an example of what we call an AR Board Game, a class of 
tabletop games that com­bine handheld computers (such as camera phones) with physical game pieces to 
create a merged physical/virtual game on the table­top. This paper discusses the technical aspects of 
the game, the design rationale and process we followed, and the resulting player experience. The goal 
of this research is to explore the affordances and constraints of handheld AR interfaces for collaborative 
social games, and to create a game that leverages them as fully as possi­ble. The results from the user 
study show that the game is fun to play, and that by tightly registering the virtual content with the 
tan­gible game pieces, tabletop AR games enable a kind of social play experience unlike non-AR computer 
games. We hope this research will inspire the creation of other handheld augmented reality games in the 
future, both on and off the tabletop. CR Categories: K.8.0 [Personal Computing]: General Games I.3.6 
[Computer Graphics]: Methodology and Techniques Interaction techniques Keywords: augmented reality, collaborative 
game, handheld game 1 Introduction Over the past few years, advances in the capabilities of 3D graphics, 
processing and display technologies in PDAs and camera phones have made these devices attractive platforms 
for mobile augmented reality (AR) games1. While AR has long been the subject of re­search labs and science 
.ction novels, the next generation of mo­bile devices will .nally enable a huge number of people to expe­rience 
applications and games that put graphics out in the world around them. The current consumer devices are 
barely up to the task, but have allowed a variety of games to be prototyped, both by *e-mail: duynguyen@gatech.edu 
e-mail: kraveendran@gatech.edu e-mail: yxu7@gatech.edu §kim.spreen@gatech.edu ¶e-mail: blair@gatech.edu 
 1In this paper, we use AR to refer to games where the graphics and/or sound are tightly registered, 
or aligned, with the players view of the physical world. AR is part of the larger category of mixed reality 
games or alternate reality games, which also include games that use the player location as input to the 
game, or make references to the world around the game. Copyright &#38;#169; 2009 by the Association for 
Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal 
or classroom use is granted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org. 
Sandbox 2009, New Orleans, Louisiana, August 4 6, 2009. &#38;#169; 2009 ACM 978-1-60558-514-7/09/0008 
$10.00 Figure 1: The Art of Defense game. (Top) Views through the phones. (Bottom) Two views of the 
play space. academic researchers (e.g., [Henrysson et al. 2006], [Wagner et al. 2005], [Mulloni et al. 
2008], [Xu et al. 2008]) and non-academics2. In many cases, these games have been created to demonstrate 
or test new technologies, especially software for tracking the mobile device relative to the world (e.g., 
marker trackers such as Stb-Tracker3). Designing handheld AR games is challenging, both be­cause the 
technology is dif.cult to work with, but also because we do not yet understand how to create effective 
AR play experiences. In our work, we have been focused not on advancing tracking and graphics technology, 
but rather on exploring the affordances of AR for play, especially as a vehicle for social gaming. In 
this paper, we present our work on Art of Defense (AoD), a co­operative handheld AR game. AoD is an example 
of what we have come to call AR board games, or games that use handheld comput­ers (e.g., camera phones) 
and tangible props to combine the tangible elements of board games with the continuous simulation of 
a com­puter game. The handheld AR interface merges the physical and digital worlds, creating the illusion 
for the player that their device is a window on the hybrid world. Using computer vision techniques (e.g., 
in AoD we combine marker-based tracking, color recognition, and contour recognition) allows the players 
to control the game with tangible objects, using physical manipulation of the props (tiles and tokens) 
to control objects and actions in the game. In this paper, we present AoD as an example AR board game, 
as well as discuss the technical aspects of the game, the design rationale and process we followed, and 
the resulting player experience. AoD is a strategy-based Tower Defense style game, in which two players 
work together to protect their central tower from waves of attacking enemies. Over the past year, the 
game has been tested informally and re.ned, including numerous demonstrations to vis­itors to our lab 
and attendees at conferences. Most recently, we ran a formal user study with six pairs of participants. 
The subjects found the game fun to play, and allowed us to con.rm that the col­laborative social experience 
created by this AR game encourages 2For example, see the prototypes at http://www.cellagames.com. 3http://handheldar.net/stbtracker.php 
 communication and social behaviors of the sort not seen in other kinds of collaborative computer games. 
This project is part of a long-term effort aimed at understanding the physical and social interactions 
facilitated by handheld AR games. Our previous work on Brag.sh [Xu et al. 2008], a co-located com­petitive 
handheld AR game, found that the AR interface enabled players to integrate their perception of each others 
physical move­ments, and to incorporate direct social interactions, into the game play. In contrast, 
AoD aims to create a cooperative experience. In Brag.sh, we used a .xed game board; here we use tangible 
game board pieces and input tokens to increase the players sense of agency in the game. A tile-based 
game board is also much more portable (making it well suited to games that use mobile phones), and allows 
us to naturally limit the player s visibility (to areas near the hex tiles) as part of the game design, 
further encouraging inter­action with the physical space. Through the design and evaluation of AoD, we 
hoped to answer the following research questions: What are the affordances and constraints of handheld 
AR?  How can we transfer our understanding of the technology into the game experience that we want to 
create?  What kind of play experience does the game support?  The paper is organized as follows. First, 
we discuss how our work is related to, and distinguished from, prior literature. Second, we discuss the 
affordances and constraints of handheld AR technology, and how they impacted the design of the game. 
Third, we introduce the game design itself, and the design rationale we used. Last, we report the qualitative 
and quantitative .ndings from a user study with six groups of players.  2 Related Work There has been 
a great deal of interest in making games with hand­held devices, due to their size and mobility. Early 
outdoor mixed re­ality games on handheld devices (e.g., Treasures [Barkhuus et al. 2005], Pirates! [Bj¨ 
ork et al. 2001]) used GPS for local position­ing and placed virtual objects at locations in the physical 
world. However, those games did not use AR technology to enhance the player s perception of virtual objects. 
Without consistently align­ing the virtual world with recognizable landmarks in the physical world, it 
is dif.cult for users to merge the two worlds in their heads in the same way they do with AoD. Recent 
advancements in mobile phone hardware have enabled the creation of 3D augmented reality applications 
on handheld devices. Invisible Train [Wagner et al. 2005] was one of the .rst AR systems on a handheld 
device that could track .ducial markers and render 3D graphics in real time. However, it was not a fully 
realized game. AR Tennis [Henrysson et al. 2006] not only tracks the markers and renders the scene in 
3D, but also utilizes the handheld device as an interaction tool. Nevertheless, AR Tennis suffers from 
the problem that the handheld device is not only an interaction tool but also the display system; the 
game is awkward because players cannot easily control their view of the virtual world while simultaneously 
using the device as a tennis racket. [Mulloni et al. 2008] and our previous game, Brag.sh, explored handheld 
AR games in a social context but did not fully utilize the affordances of handheld AR, such as its potential 
for tangible interaction techniques, to more fully draw the player into the combined physical/virtual 
play space. Tangible interaction techniques have long been a goal for AR system design, since the freedom 
of user movement afforded by AR creates the need for direct interaction with the physical ele­ments of 
the merged world. Many techniques have been designed for AR applications based on input devices, such 
as paddles or cubes [Billinghurst et al. 2000] with markers on their visible sur­faces. However, these 
markers can only de.ne the spatial relation­ship between the physical and virtual worlds, so, virtual 
and phys­ical elements are still separated even though they coexist on the same space. The naturalness 
of sketch-based interfaces in games4 has inspired us to transform user sketches into meaningful virtual 
game objects. Furthermore, the use of physical objects in board games has also inspired us to design 
tangible game tokens as our game pieces. A number of research projects have explored such hybrid physi­cal/virtual 
games. For example, in False Prophets , players use tangible pieces to move around a digital game board 
projected onto a touch-sensitive table [Mandryk and Maranan 2002]. In Wizard s Apprentice , the physical 
board and digital display are shown to the player in parallel [Peitz et al. 2006]. In these games, the 
physical game board is wired to recognize player s actions. But in AoD, the mobile device is in charge 
of actively recognizing these actions in­stead. The hybrid world is shown via the displays on the players 
handheld devices. The mobility of the interface allows the game to be played anywhere. Each of these 
different interface technologies and approaches will lead to different play experience.  3 Handheld 
AR Issues Our main research goal is to explore game structures and mechan­ics that could leverage the 
advantages of AR on handheld devices. Mobile AR offers a range of game elements that designers can leverage, 
drawn from both digital games (e.g., 3D interactive graph­ics, sound and various interaction techniques) 
and traditional board games (e.g., physical game pieces and game boards). Players may have an increased 
awareness of their physical space, and can easily interact with other players and the environment [Xu 
et al. 2008]. Although recent advances in AR technologies have enabled AR to be used in commercial console 
games5, game designers need to be aware of a range of sometimes severe technological limitations when 
designing such games for mobile devices. In this section, we discuss the potential as well as technical 
limitations of mobile AR games, and suggest some possible design options that can help overcome these 
limitations, or leverage them as part of the game design. 3.1 Display and Tracking The appeal of merging 
3D graphics with the physical world, and the possibility of viewing it naturally from any viewpoint, 
are two oft-cited advantages of AR over conventional computer games. By moving AR from head mounted displays 
(HMDs) to handheld devices, the user is freed from the ergonomics issues of current HMDs. Using mobile 
phones for AR also makes AR interfaces ubiquitous: people usually have their phones with them, and can 
therefore play the games anywhere and anytime. A major problem of mobile AR is that the small viewing 
area of handheld devices limits the amount of the merged world a user can perceive at any one time. Fortunately, 
building on the idea of seam­ful design [Barkhuus et al. 2005], the small viewing area can be used as 
a design element. Since players cannot observe the whole hybrid play space at one time, game designs 
that purposely limit in­formation about what is happening in some parts of the game world 4For example, 
Crayon Physics http://www.crayonphysics.com/ 5For example, see Eye of Judgment: http://www.eyeofjudgment.com 
 may lessen the impact of the small visible area. For instance, nu­merous strategy games, (e.g., Warcraft 
III) use a fog-of-war that prevents players from viewing game activity in areas unoccupied by player 
units. Moreover, the ability to naturally control the view­ing location allows the player to very easily 
move from one view to another, and from a farther-back overview to in-close detail views. The second 
problem with mobile AR lies in the tracking technology for accurate and real-time registration of the 
virtual and physical worlds. Although recent advances in tracking technologies enable very accurate and 
robust 3D tracking in large scale environments, they are still computationally too expensive to execute 
on current smart phones. Wagner has demonstrated a natural feature tracker for mobile phones [Wagner 
et al. 2008b] that tracks off of 2D im­ages, but it cannot yet track many objects at the same time, which 
severely limits the possibilities for user interaction with different physical game pieces. In this paper, 
our solution is based on a con­ventional marker tracking approach for AR [Wagner et al. 2008a], but the 
discussion is general enough to be applied to a natural fea­ture tracking method when such technology 
is more mature. The current limits of processing capability and camera resolution put constraints on 
the range of possible distances between the hand­held device and the tracked object. On one hand, the 
captured image resolution has to be small enough to guarantee fast image process­ing on the phone and 
real-time tracking speed. On the other hand, the area of the marker viewed in the captured image has 
to be large enough for the tracking algorithm to calculate a stable and accurate 3D camera pose. Consequently, 
the camera resolution limits the player s range of motion; the camera needs to be far enough from the 
game space to get an entire marker in view, but not so far as to lose tracking. While the whole game 
space could be large, this constraint again limits the amount of information the player can perceive 
from the AR world. Another problem facing mobile AR is the tradeoff between porta­bility and .xed game 
boards. Multi-marker tracking techniques, which use a large board with many pre-located markers to provide 
a large play space, are very common in AR. However, a large marker board con.icts with our portability 
goal, since it is inconvenient for players to carry a large game board with them. Advanced methods that 
track physical objects with no prior knowledge (e.g., [Klein and Murray 2007]) could address this issue 
in the future, but for now, mobile AR game designers have to address this constraint in order to make 
their games portable. Instead of using a .xed large marker board, we build on the tile­laying game mechanic 
made popular by games such as Klaus Teu­ber s Settlers of Catan6. In AoD, each hex-shaped tile has a 
track­able marker on it. One hex tile serves as the center of the world and during the game, the player 
puts new markers next to the existing ones to build up the game map. By limiting the number of tiles, 
players are required to remove existing tiles and put them at new locations to explore the world. As 
long as the map con.guration of those tiles is known and consistent, the system can steadily track the 
game space and register the virtual content. By leveraging marker tiles, or similarly compact game pieces, 
handheld AR games can become truly portable using current technology, because the player can easily carry 
the tiles with them everywhere. A dynamic marker-tile game board is a compelling mechanic for AR game 
design. Aside from making the game portable, the marker tiles also require tangible interactions with 
the game pieces. The fact that the AR world can only be seen where the markers exist reinforces the limited 
visibility of the handheld, as discussed above, allowing for hidden information or fog-of-war effects. 
6http://en.wikipedia.org/wiki/Settlers of Catan  3.2 Tangible Interaction As discussed in the previous 
section, the dynamic tile-based map building mechanic allows players to physically interact with the 
game pieces. However, the marker tile map is only the ground on which the augmented content sits in the 
physical world. We would also like to allow players to interact with virtual game objects. There have 
been examples of tangible interaction in mobile AR (e.g., [Henrysson et al. 2005], [Henrysson et al. 
2007]). In the simplest form, one can use buttons on the phone to interact with the virtual world. To 
enhance tangible interaction, one could use a tracked paddle, cup, or cube (e.g., by putting with markers 
on them, as in [Billinghurst et al. 2000]). One could also detect the user s gestures, or sense the motion 
of the phone [Henrysson et al. 2007]. We do not explore the last two options in AoD, although newer de­vices 
such as Apple s iPhone point toward such techniques being practical in the future. Using objects with 
markers on them to interact with the virtual world creates a well-de.ned relationship between the physical 
and virtual worlds: the markers de.ne the location and meaning of the virtual objects in the physical 
world. The biggest hurdle is that in AR, the markers typically lack meaning to humans, and simply serve 
as placeholders for virtual objects. Even if one can put some meaningful picture inside the marker, the 
number of AR objects is bounded by the number of marker patterns and their locations in the physical 
space. Therefore, one objective of our design was to seek out interaction techniques that go beyond simple 
spatial rela­tionships between physical and virtual worlds. Early in our design, we decided to use markers 
that have a hollowed out interior [Wagner et al. 2008a], allowing the user to draw a sketch that can 
be analyzed by the game. This sketch can be used to control the game, creating units and changing their 
properties. Parameters such as accuracy of the sketch could be used to enhance the result­ing virtual 
unit, create variations, and provide greater feedback to the user by aligning the appearance of the marker 
tile to the virtual content. In this way, the sketch techniques could help to bridge the semantic gap 
between the physical and virtual world. Although the sketch interaction is compelling, in a mobile AR 
game it is dif.cult for people to hold the phone, steady the marker tile and sketch at the same time. 
Sketching also takes time, which is not suitable for fast paced games. An alternative to sketching is 
to use pre-designed tokens as place holders for virtual objects, akin to game pieces in board games. 
Such a token needs to be carefully designed so that, on one hand, it could convey some meaningful concept 
to the player about the object it represents, but, on the other hand, remain simple enough to be recognized 
by the phone accurately and quickly. Compared to sketches, tokens are more tangible and easier to use 
in the game, but they are .xed in property and meaning.  4 Game Description Art of Defense is a networked 
two player cooperative game inspired by the Tower Defense genre. The objective of the game is to sur­vive 
as many waves of enemies as possible by preventing them from reaching the base (see Figure 1). Enemies 
approach along prede­.ned paths that converge at the base and deal damage proportional to their strength. 
The game ends when the health of the base tower falls to zero. Players begin with a common pool of money 
that can be used to create new towers or upgrade existing ones. These tow­ers attack enemies and earn 
money for each killed enemy unit. Players start with a single hexagonal tile (corresponding to the base) 
placed on the table. No tokens may be placed on this tile. There are 15 map tiles excluding the base 
(much fewer than the current 10×11 game map). The map can be explored by placing additional tiles adjacent 
to existing ones (and adding them by viewing both tiles through the phone). Adding a new tile reveals 
the map corre­sponding to that region (including enemy units) and also shows the terrain in the neighboring 
tiles. Tiles can be removed and reused (towers can only be created on tiles that are in place in the 
world). Each player is assigned a unique color (either red or blue) that re­mains constant throughout 
the course of the game. The red player can only build red towers, which in turn can only damage the red 
en­emies. Similarly, the blue player is restricted to blue towers, which solely attack blue enemies. 
Towers can be built on empty tiles on the map by placing a triangle token of the appropriate color and 
turning it to point the tip in the desired direction of .re. The player adds the tower to the game by 
viewing the tile through the phone and pressing a button. Each tower costs a .xed amount to construct 
but can be rotated at any time for no cost. There are three upgrade tokens (magenta, green and black) 
corre­sponding to an increase in the damage dealt to a unit, rate of .re and range, respectively. Each 
may be applied up to a maximum of three times per tower. To upgrade a tower, the player must place the 
token on the tile containing the tower and con.rm the action on the phone. The .nal score is based on 
the time since the start of the game that the base tower has remained standing.  5 Game Design Discussion 
Picking the strategy genre allowed us to incorporate elements from board games (map building using hexagonal 
tiles, tangible game tokens) and their computer counterparts (limited visibility due to fog of war, real-time 
unit movement). Tower defense games typ­ically need little to no micromanagement of player units, with 
the main player actions being the placement of towers (which automat­ically .re at enemies without player 
intervention) and upgrading of towers. Further, the pace of such games is moderate and does not require 
rapid button presses (clumsy on most mobile devices) or violent camera movements (disruptive for vision 
based tracking). Combining the gameplay of tower defense with the exploration as­pect of other real-time 
strategy games (such as Blizzard Entertain­ment s Starcraft and Microsoft s Age of Empires) aligned perfectly 
with our vision of the player interacting with game pieces to deploy units and to also develop an understanding 
of a larger space than could be seen on the phone screen at a single instant of time. AoD was developed 
iteratively over the past year, and we received feedback from a wide variety of people through the development 
cycle. We began with paper/physical prototypes to understand the interactions that we were designing, 
and then developed a version for a desktop computer to test the speci.c game mechanics with tracking 
and augmented graphics. Once we had ironed out all bugs, we ported the code to the Symbian platform. 
Building a desktop version .rst saved us a great deal of time and effort. However the powerful CPU made 
it very easy to overlook the bottlenecks that could exist on the lower-powered phone. On numerous occa­sions, 
we had to scale back or discard features after realizing that the phone could not support the level of 
complexity that we desired. For instance, advanced pathing for enemies, higher polygon mod­els and detailed 
textures did not make it into the current version. In the following sections, we describe the game components 
and their iterative re.nement through the development process. 5.1 Map Building We opted to embed a 
hollow square marker in a hex game tile for several reasons. The shape permits the addition of tiles 
in six direc­ (a) (b) Figure 2: Old interface concepts. (a) First interface for map build­ing. (b) Sketch-based 
tower creation. (a) (b) Figure 3: Current UI for adding tiles (a) New tile is highlighted in yellow 
(b) The added tile is highlighted in green tions and avoids the ambiguity of diagonal movement that is 
com­monly seen in games using square tiles. Furthermore, the distance covered is equal in all six directions. 
Finally, we found it easier to handle hex tiles than square ones because they could be gripped without 
obscuring the marker. We used a laser cutter to fashion a set of hex tiles and soon discovered that this 
shape had the added bene.t of better inter-tile alignment and led to more stable tracking because pieces 
were less likely to move during gameplay. We ex­perimented with different sizes for the tile such that 
the inner space was large enough to permit sketching without the user accidentally crossing the boundary. 
We .nally settled on a hollow marker with a size of 5.5cm, with each side of the hexagon being 4.85cm. 
To add a tile to the map, the player had to place the new tile next to an already added one and press 
a button to con.rm this action. In our earliest prototype, only portions of the map with tiles under 
them were revealed to the user (see Figure 2(a)). This proved to be confusing because it was not clear 
which direction the players needed to explore to .nd a path, leading them to add and remove tiles frequently. 
In the next iteration, we added limited visibility near tiles, where spaces next to tiles would show 
a dimly lit version of the terrain, but not enemy units (see Figure 2). This visual cue was suf.cient 
for players to add tiles in the areas that they believed were important (usually around paths leading 
to the tower). Adding a tile requires both the to-be-added and one of the existing tiles to be in view. 
However, with the limited .eld of view of the phone s camera, it is possible to lose sight of all of 
the existing tiles. Designing an intuitive interface on the phone for supporting this map-building feature 
proved to be challenging. Our earliest prototype displayed the possible positions, at which a new tile 
could be added, with the position closest to the new tile highlighted in a darker shade. However, it 
was not obvious to the user if both tiles were being tracked at the moment of pressing the button to 
con.rm the action. This led to unnecessary key presses and frustration in poor lighting conditions. We 
remodeled this system to highlight the existing tracked tiles in green, while new tiles are highlighted 
in yellow if they are close enough and are being tracked, or in red if they are too far away (see Figure 
3). Once added, the new tile is highlighted in green as well. While this interface does not handle  
(a) (b) (c) Figure 4: Building and upgrading a tower. (a) Triangular token creates and orients tower. 
(b) Power-up tokens. (c) Applying a power-up to a tower. lost tracking either, we have found that players 
tend to look for the yellow signal as an indication to con.rm the action and this made the tile addition 
process work smoothly under reasonable lighting conditions.  5.2 Sketching Our original design for AoD 
allowed players to sketch shapes (cor­responding to different units) in the empty space inside the hol­low 
marker (see Figure 2(b) ). Our .rst prototype supported lines (walls), squares (traps), triangles (directional 
towers) and circles (omnidirectional towers).The players had to look at the tile with the phone to con.rm 
the correct shape was detected. The size and posi­tion of the hand drawn shape, as well as the accuracy 
of the shape, were used as parts of the game. However integrating the sketch recognition effectively 
into AoD proved to be dif.cult. Even after we found suitable erasable ma­terials for the center of the 
tiles, implementing a robust, fast sketch recognizer was not possible on top of everything else the mobile 
phone needed to do during each rendering cycle. Most sketch-based games (on a computer or Nintendo DS) 
use stylus based input that is highly accurate because they receive a densely sampled version of the 
shape. In our case, recognition had to be performed on a single image at a fairly low resolution with 
considerable error (non­closed shapes, faded segments due to varying pressure applied by the user). While 
more expensive thresholding algorithms alleviated some of these issues, sketching also forced the user 
to place the phone down or risk moving the other game tiles while performing the action. Ultimately, 
we decided against adding this feature to the .nal version of the game. While sketching could provide 
a unique means of user expression, our prototype was unable to utilize this fully due to technology limitations. 
 5.3 Game Tokens As an alternative to sketching as a means of tangible interaction, we again looked to 
board games for inspiration. Physical pieces are appealing because they lower the learning curve for 
the player and create a mapping from the physical to virtual world. Further, the detection algorithm 
for sketching could be reused for these pieces, without many of the problems created by hand-drawn sketches. 
For the current game, we use exactly one kind of player unit: a di­rectional tower that is represented 
by a triangle (see Figure 4(a)). Players could rotate the piece and look at it through the phone to have 
it shoot in the desired direction. While a more elaborate game could easily use more types of tokens, 
we wanted to re.ne the inter­actions and keep the game relatively simple to avoid overwhelming the casual 
player. Switching to game tokens also allowed us to do away with menu screens for unit upgrades. Using 
three uniquely colored tokens for the various upgrades (see .gure 4) meant that players simply had to 
place the desired token on a tower to upgrade it. To support this op­eration, we added a color detection 
phase to our existing algorithm and calibrated the colors according to the lighting conditions and the 
phone s camera when the game starts. 5.4 Cooperative Play One of our goals for AoD was to create a collaborative 
experience and test our intuitions about how AR would foster social interac­tion. Redesigning a primarily 
single player game concept (tower defense) into a cooperative game that fostered the desired interac­tions 
required more than just making the game harder. We consid­ered several ways of dividing responsibilities 
amongst the players to encourage cooperation (for instance, one option was to allow one of the players 
to observe and explore the map while the other added towers), but we found such unequal divisions of 
labor to be disrup­tive to the player experience and to the balance of the game. In­stead, we decided 
to allow both players to explore the map but each to only add towers of a certain color. Towers of one 
color (e.g., blue) could in.ict damage on enemies of the same color, but not on other (e.g., red). This 
helped us to avoid the scenario where players would split up the space physically and focus solely on 
their halves. Furthermore, the pool of money was shared between the players, forcing them to make strategic 
decisions together. An advantage of this color-based division was that it allowed us to design levels 
that could test the extent of cooperation between the players by varying the proportion of red and blue 
enemies in each wave. For exam­ple, if players did not communicate with each other, a path that was poorly 
guarded against blue enemies could be easily overrun.  6 Technology Our game targets the Nokia N95 
running the Symbian OS. The N95 contains an ARM-11 based Texas Instrument OMAP2 processor running at 
330MHz, and a 5 megapixel camera that can capture video at 640×480 up to 30 frames per second (fps). 
The graphics pipeline on the N95 is designed for OpenGLES 1.17, a compact version of OpenGL for embedded 
devices. After receiving an image from the camera for processing, we need to display it on the screen 
by uploading it to the texture mem­ory. OpenGLES only accepts textures with size 2m × 2n . Unlike OpenGL 
on the PC, updating part of the texture using glTexSubIm­age2D on the phone with OpenGLES is much slower 
than upload­ing a whole new texture with glTexImage2D. Moreover, pixel-by­pixel copy between different 
image buffers for different purposes (texture mapping and tracking) is not cheap on the phone. For all 
of these reasons, there is an important tradeoff in the image res­olution for fast processing speed, 
tracking accuracy and viewing quality; we needed to carefully choose a good resolution that bal­ances 
those factors. Our experiments found that a 256×192 video image offered a good balance of viewing quality, 
processing speed and tracking accuracy. Images at 320×240 are also a good choice, trading off reduced 
speed for more stable tracking and registration. We use the Edgelib cross-platform game engine8 to develop 
our game. Edgelib version 3.3 has some rendering problems when we combined it with native OpenGLES (which 
we need to do to set the projection and model-view matrices to match the camera view). We eventually 
replaced most of Edgelib s graphics engine by our own, based on the Milkshape model format and optimized 
with .xed­point operations for fast computation on N95 hardware. Despite the rendering problems, Edgelib 
provides us a good framework for 7http://www.khronos.org/opengles 8http://www.edgelib.com  cross-platform 
development with easy input button mapping, sound (using the Hekkus library9) and networking. For marker 
tracking, we use StbTracker library10 and manage the data structures for dynamic multi-marker tracking. 
By default, Stb-Tracker uses all observed correspondences of marker corners in the map to optimize the 
camera pose. Although this approach is accu­rate and stable for .xed markers in a large multi-marker 
board, it is not well suited to our situation where the marker tiles are not rigidly .xed relative to 
each other. The gaps and improper alignment of the marker tiles will cause the optimization process to 
converge to some incorrect local minimum, leading to errors and .ickering in regis­tration. Consequently, 
we simply use the 4 corners of the largest marker observed in the current frame for pose optimization. 
Due to the fact that the biggest marker does not change frequently, this simple approach helps reduce 
the .ickering problem, is robust to sloppy tile placement, and also speeds up the optimization process 
signi.cantly. As noted above, recognition of hand-drawn shapes is a dif.cult problem. We have tested 
different image processing techniques (e.g., edge .nding, Hough transforms) to extract simple geometric 
primitives from the sketch and then apply a hierarchical classi.ca­tion system, but most are too expensive 
to operate on the phone in real time. We ended up using a variant of the approach used by most AR marker 
trackers to quickly recognize simple geomet­ric shapes such as lines, triangles, rectangles or circles. 
The al­gorithm works by .rst thresholding the image, extracting the con­tours of any blobs inside a marker, 
searching for extreme points on the contours, and determining the contour shapes by the number of extreme 
points. In order to enhance the recognition accuracy, we use an adaptive thresholding technique [Bradley 
and Roth 2007] to cope with the uneven local lighting properties on different parts of the sketches, 
and guarantee that the sketch contour is continuous after being thresholded. For color-.lled geometric 
shapes on the tokens, we use the same recognition algorithm, but without using the adaptive thresholding 
technique since the .lled interior of the shape already guarantees the boundary is continuous. In order 
to calibrate the camera for color recognition, we place all color tokens to be recognized at speci.c 
places relative to the base marker (the position of each token is highlighted by rendering a circle of 
that token s color in the alpha channel). After that, we calculate the mean color of the camera pixels 
in a 5×5 region at the center of each circle. During game play, the sketch or token color is classi.ed 
by the closest Euclidian distance of its average pixel colors to one of those mean colors.  7 User Study 
7.1 Goals To understand the play experience and improve the game design, we conducted a small user study 
with twelve participants. Speci.­cally, we hoped to understand how the augmented reality interface affected 
game play, and the kinds of cooperative play that occurred. These research questions were not merely 
designed to gather the feedback to improve AoD. Rather, we were interested in .nding the patterns of 
behavior related to game interface and cooperative game design to inform the design of future handheld 
AR games. 9http://www.shlzero.com 10http://handheldar.net/stbtracker.php  7.2 Participants We recruited 
twelve participants on campus. We recruited each per­son separately and ensured that each pair of participants 
were not friends before the study. All of the participants were undergraduate and graduate students from 
the computer science and computational media majors, aged 21-26. Two of them were female. The partic­ipants 
were from six different countries. Six participants did not use English as their native language. All 
but two had not played augmented reality games before the user study. All but two had the experience 
of playing with strangers in online games or sports. In the following sections, the participants are 
referred to using a con­vention like G1-A (the number is the group id, and the letter is the player id 
in the group.) 7.3 Procedure and Setting The user study included three parts. First, the players learned 
the game interface. The participants were given a phone and game pieces, without the game running. A 
researcher gave an intro­duction to the game controls by following a pre-written script and showing a 
pre-recorded instructional video. Second, the partici­pants played the game together. Each team was given 
at most three times to play. The video of the game sessions and player behavior was recorded. One observer 
took notes while the game was played. Third, the participants .lled out the questionnaire independently 
af­ter playing the game. Afterwards, a semi-structured interview was conducted to understand more about 
the incentives and experience. The user study was set up in a research lab. The two players sat on each 
side of the table, facing each other. The physical game tokens and tiles were piled at one end of the 
table. An instruction sheet showing how to use the phone buttons was provided to each player, for reference. 
To capture and synchronize the video of play behavior and game events, we recorded the game screens and 
top­down video for both players using a video quadbox (see Figure 5). Figure 5: User Study Setting 
 7.4 Measures 7.4.1 Recorded Video and Observation The recorded video synchronized the game screens 
with the top­down videos that captured the physical movements of both players. With this data, we were 
able to reconstruct what went on during game play, how the players moved and the problems they encoun­tered. 
In addition, one researcher took observation notes during the game play, to record interesting moments 
of play and to generate related questions for the interview.  7.4.2 Questionnaire The questionnaire 
was designed based on previous studies (e.g., our own work, [Mandryk and Maranan 2002]), and was divided 
into three sections: feedback about the game in general, evaluation of the game interface, and feedback 
about social interaction. We used cross-checking questions to address the problem of misreporting. 7.4.3 
Interview At the end of the user study, we conducted a semi-structured in­terview. While the observation 
and video focused on .nding the behavioral patterns, the interview focused on understanding the subjective 
experience of the players, including the strategies they adopted, their awareness of the other player, 
and the comparison between this game and other games they had played before.  7.5 Findings and Discussion 
7.5.1 Feedback About The Game Through the user study, the participants gave us positive feedback about 
AoD (see Figure 6). We interviewed the participants about what they liked and disliked about the game. 
Cooperative play , Tangible interface , and Game concept were most frequently mentioned as pros of the 
game. The graphics went off when I moved my phone (registration problem), blocking the other one s view 
, need a better overview of the bigger world were most fre­quently mentioned as cons. Figure 6: Feedback 
Scores 7.5.2 Feedback About the AR Interface Interestingly, the handheld AR interface is related to 
both what the participants liked and disliked, as mentioned above. While people agree that the interface 
brings more fun to the game (4.3/5), the score for the interface gave enough feedback is just a little 
bit above neutral (3.3/5). Below are some more speci.c observations. Handheld AR interface as the lens 
to the hybrid world: The handheld AR interface works like a lens onto the merged physical­digital game 
world. Like many games that have a fog-of-war effect, the players cannot see further than the spaces 
nearby. In AoD, the players need to physically move to explore the game space. So­cial interaction and 
collaboration is encouraged because of this, as discussed in the next section. However, this limited 
view also intro­duces some problems. For example, many players want to have a better sense of what is 
happening on the rest of the map. The lack of information about the enemy attack makes the game hard 
to plan ahead (G3-A, G2-B, G5-B). Another issue is occlusion, meaning that the handheld is blocking the 
way of the other player . The players worked out their own ways around this issue after they realized 
it would be a problem. They either reached out to point to the marker from another direc­tion, or waited 
for the other person to .nish the action .rst. Inter­estingly, in our previous study of Brag.sh, we found 
that players intentionally blocked the way of each other to occupy the better position in the competitive 
play. In both cases, the players under­stand how the interface works, however, the design needs to con­sider 
these kinds of space-related interactions. Emergent play: Trust, reference and communication: We found 
that the three teams (G1, G3, G4) who got the highest scores shared a similar kind of strategy. One player 
would be in charge of two of the paths (there were four paths in the game level we used), and guide the 
other player to the location where they need to per­form an action. Through observation and recorded 
video, we found that this process could be broken down into three major compo­nents: trust, reference 
and communication. All of these are closely related to the design of the interface. When players were 
using the handheld interface as a lens onto the game world, the limited area shown on the screen signi.cantly 
af­fected game play. The strategy of moving around the whole map and making sure there are enough towers 
in the right places is inef­.cient with this limited view. The successful players realized that they 
could rely on the other player to cover half of the map and pay attention to only those places indicated 
by the other player. These groups mentioned that they needed to trust each other to make this strategy 
work (G1, G3, G4). As mentioned by G4, the immedi­ate feedback assured by face-to-face interaction was 
necessary for creating trust between players who did not know each other. In this game, how to refer 
to a certain location on the map became a key element for the collaborative process. One group found 
it hard to develop a successful reference protocol in the game (G6), while most other groups used pointing 
gestures assisted with language (G1-G5). Two groups also used physical tokens that they would place at 
speci.c locations to refer to later (G3, G4). The two basic protocols: pointing (directing-to) and placing 
(placing-for) can ef­.ciently draw the attention of the other player. The tangible pieces facilitate 
using physical space as a reference system, and become an ef.cient form of communication mediation. 
 7.6 Social interaction It is common to use games as ice-breaking media to bootstrap com­munication 
between strangers. This user study had some initial .ndings in the space of co-located cooperative computer 
games. In the questionnaire, the users reported that they feel comfortable playing with a stranger (4.2/5); 
and they have a neutral opinion about whether they think the play experience will be better with a friend 
(3.2/5). They enjoyed talking with the other player (4.4/5), while they believed that the other player 
was also willing to commu­nicate (4.1/5). They preferred to play the game with a real person instead 
of a computer (4/5). The communication includes verbal conversation, hand gestures and body movements. 
We also observed that all the groups started to talk more after the .rst round. As participants played 
longer, their conversation was initiated by and centered around the game play. In some cases, we saw 
that the two players did not talk at all for as long as several min­utes during the .rst round (G2, G6); 
but they started talking after the .rst round .nished and reported that face-to-face communica­tion contributes 
to their game play in the interview. The conversa­tions that happened between rounds were typically re.ections 
on the previous game, the theories about which strategy may work better, and the plan for the next round. 
The conversation during the game was concentrated on passing knowledge about the game to the other player, 
informing the other player about the game status and asking for speci.c cooperation. Prior research 
showed that co-located handheld gaming does not foster social interaction between players during gameplay 
[Szent­gyorgyi et al. 2008]. The user study of AoD showed a good amount of player interaction that leveraged 
social cues, including non-verbal communication.  8 Conclusions and Future Work We have discussed the 
potential and limitations of mobile AR games, and analyzed them in the context of a speci.c game design. 
AoD demonstrates speci.c mechanics that leverage and balance the game elements with the technology constraints. 
We also show that dynamic marker-tile-based map building and tangible interaction techniques, with either 
sketches or game tokens, allow AR games to be tightly integrated with the players perception of the world, 
creating the illusion that the game really is on the table top. By giving each player their own view 
of the hybrid space, handheld AR games allow us to make use of the core features of traditional board 
games. Our study shows that AoD, with its tangible inter­action and board game elements, is fun to play 
and encourages so­cial interaction and communication between players. As AR tech­nology and mobile phone 
hardware improves, we believe that AR board games will become increasingly popular. As we move forward, 
we hope to leverage the rapidly improving sensing and display technology on mobile devices (e.g., natural 
fea­ture tracking, multitouch screens and orientation sensors) to create even more compelling games. 
In the long run, we hope to explore the social potential of a much wider class of handheld AR games, 
especially those in large-scale outdoor environments. We believe that, with full mobility and immersion 
in the combined physi­cal/virtual world, both tabletop and outdoor handheld AR games will transform the 
way people interact, collaborate and play games with each other.  Acknowledgements We are grateful to 
Turner Broadcasting and Nokia for support, Imagination GmbH and Graz University of Technology for pro­viding 
access to StbTracker, and Elements Interactive Mobile B.V. for access to EDGELIB. We would like to acknowledge 
Kris Krit­manorote and Iesha Hazel for game artwork and 3D models, and Timothy White and Michael Gorbsky 
for their design input.  References BARKHUUS, L., CHALMERS, M., TENNENT, P., HALL, M., BELL, M., SHERWOOD, 
S., AND BROWN, B. 2005. Picking pockets on the lawn: The development of tactics and strategies in a mobile 
game. In Proceedings of UbiComp 2005, Springer, 358 374. BILLINGHURST, M., IMAMOTO, K., KATO, H., AND 
TACHIBANA, K. 2000. Magic paddle: A tangible aug­mented reality interface for object manipulation. In 
Proceedings of the IEEE and ACM International Symposium on Augmented Reality, 111 119. BJ ¨ORK, S., FALK, 
J., HANSSON, R., AND LJUNGSTR, P. 2001. Pirates! using the physical world as a game board. In Proceed­ings 
of Interact 2001, 9 13. BRADLEY, D., AND ROTH, G. 2007. Adaptive thresholding using the integral image. 
Journal of Graphics Tools 12, 2, 13 21. HENRYSSON, A., BILLINGHURST, M., AND OLLILA, M. 2005. Virtual 
object manipulation using a mobile phone. In ICAT 05: Proceedings of the 2005 International Conference 
on Augmented Tele-existence, ACM, New York, NY, USA, 164 171. HENRYSSON, A., BILLINGHURST, M., AND OLLILA, 
M. 2006. Ar tennis. In SIGGRAPH 06: ACM SIGGRAPH 2006 Sketches, ACM, New York, NY, USA, 13. HENRYSSON, 
A., MARSHALL, J., AND BILLINGHURST, M. 2007. Experiments in 3d interaction for mobile phone ar. In GRAPHITE 
07: Proceedings of the 5th International Confer­ence on Computer graphics and interactive techniques 
in Aus­tralia and Southeast Asia, ACM, New York, NY, USA, 187 194. KLEIN, G., AND MURRAY, D. 2007. Parallel 
tracking and map­ping for small ar workspaces. In ISMAR 07: Proceedings of the 2007 6th IEEE and ACM 
International Symposium on Mixed and Augmented Reality, IEEE Computer Society, Washington, DC, USA, 1 
10. MANDRYK, R. L., AND MARANAN, D. S. 2002. False prophets: exploring hybrid board/video games. In CHI 
02: CHI 02 ex­tended abstracts on Human factors in computing systems, ACM, New York, NY, USA, 640 641. 
MULLONI, A., WAGNER, D., AND SCHMALSTIEG, D. 2008. Mo­bility and social interaction as core gameplay 
elements in multi­player augmented reality. In DIMEA 08: Proceedings of the 3rd international conference 
on Digital Interactive Media in Enter­tainment and Arts, ACM, New York, NY, USA, 472 478. PEITZ,J.,BJ¨APPINEN,A.2006.Wizard 
sappren-ORK, S., AND J¨tice gameplay-oriented design of a computer-augmented board game. In ACE 06: Proceedings 
of the 2006 ACM SIGCHI In­ternational Conference on Advances in Computer Entertainment technology, ACM, 
New York, NY, USA, 79. SZENTGYORGYI, C., TERRY, M., AND LANK, E. 2008. Rene­gade gaming: practices surrounding 
social use of the nintendo ds handheld gaming system. 1463 1472. WAGNER, D., PINTARIC, T., AND LEDERMANN, 
FLORI-ANAND SCHMALSTIEG, D. 2005. Towards massively multi­user augmented reality on handheld devices. 
In Proceedings of the Third International Conference on Pervasive Computing, 208 219. WAGNER, D., LANGLOTZ, 
T., AND SCHMALSTIEG, D. 2008. Robust and unobtrusive marker tracking on mobile phones. In ISMAR 2008: 
Proceedings of the 7th IEEE/ACM International Symposium on Mixed and Augmented Reality, 2008., 121 124. 
WAGNER, D., REITMAYR, G., MULLONI, A., DRUMMOND, T., AND SCHMALSTIEG, D. 2008. Pose tracking from natural 
fea­tures on mobile phones. ISMAR 2008: Proceedings of the 7th IEEE/ACM International Symposium on Mixed 
and Augmented Reality, 2008. (Sept.), 125 134. XU, Y., GANDY, M., DEEN, S., SCHRANK, B., SPREEN, K., 
GORBSKY, M., WHITE, T., BARBA, E., RADU, I., BOLTER, J., AND MACINTYRE, B. 2008. Brag.sh: exploring physical 
and social interaction in co-located handheld augmented real­ity games. In ACE 08: Proceedings of the 
2008 International Conference on Advances in Computer Entertainment Technol­ogy, ACM, New York, NY, USA, 
276 283.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1581096</article_id>
		<sort_key>230</sort_key>
		<display_label>Pages</display_label>
		<article_publication_date>08-04-2009</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A framework for exertion interactions over a distance]]></title>
		<page_from>143</page_from>
		<page_to>150</page_to>
		<doi_number>10.1145/1581073.1581096</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1581096</url>
		<abstract>
			<par><![CDATA[<p>Exertion games are an emerging form of computer games that aim to leverage the advantages of sports and exercise in order to support physical, social and mental health benefits. Despite the increased attention these games received recently, there is a lack of understanding of what role the game's design plays in encouraging people to invest physical effort into these games. We aim to contribute to this understanding by presenting a framework for "Exertion Interactions over a Distance", consisting of three core concepts: exertion, sociality and engagement. To demonstrate the usefulness of our framework we utilize a networked game called "Remote Impact" that encourages intense physical exertion. We hope our work can support researchers in gaining an understanding of this exciting new field, whilst also aiding designers in the creation of new games, leveraging the associated benefits of exertion.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[design space]]></kw>
			<kw><![CDATA[exertion interface]]></kw>
			<kw><![CDATA[exhausting]]></kw>
			<kw><![CDATA[framework]]></kw>
			<kw><![CDATA[physical]]></kw>
			<kw><![CDATA[social]]></kw>
			<kw><![CDATA[sports]]></kw>
			<kw><![CDATA[tangible]]></kw>
			<kw><![CDATA[videoconferencing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1570357</person_id>
				<author_profile_id><![CDATA[81100260236]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Florian]]></first_name>
				<middle_name><![CDATA['Floyd']]></middle_name>
				<last_name><![CDATA[Mueller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Distance Lab, Horizon Scotland, Forres, Moray, UK and The University of Melbourne, Melbourne, Australia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570358</person_id>
				<author_profile_id><![CDATA[81100392019]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Stefan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Agamanolis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Distance Lab, Horizon Scotland, Forres, Moray, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570359</person_id>
				<author_profile_id><![CDATA[81100490463]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Frank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vetere]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Melbourne, Melbourne, Australia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570360</person_id>
				<author_profile_id><![CDATA[81100536739]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Martin]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Gibbs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Melbourne, Melbourne, Australia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1057239</ref_obj_id>
				<ref_obj_pid>1057237</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Benford, S., Schn&amp;#228;delbach, H., Koleva, B., Anastasi, R., Greenhalgh, C., Rodden, T., Green, J., Ghali, A., Pridomore, T. &amp; Gaver, B. 2005. Expected, sensed, and desired: A framework for designing sensing-based interaction. ACM Transactions on Computer-Human Interaction (TOCHI), 12, 3--30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bianchi-berthouze, N., Kim, W. &amp; Patel, D. 2007 Does Body Movement Engage You More in Digital Game Play? and Why? Affective Computing and Intelligent Interaction. 102--113.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bogost, I. 2007 Persuasive Games: The Missing Social Rituals of Exergames. http://seriousgamessource.com/features/feature_013107_exergaming_1.php. Accessed 20 April 2009]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1371221</ref_obj_id>
				<ref_obj_pid>1371216</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[de Kort, Y. A. W. &amp; Ijsselsteijn, W. A. 2008. People, places, and play: player experience in a socio-spatial context. Computers in Entertainment (CIE), 6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>513034</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Dourish, P. 2001 Where the Action Is: The Foundations of Embodied Interaction. MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Eyetoy Eyetoy. http://eyetoy.com. Accessed 20 April 2009]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>640634</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Fogg, B. J. 2002 Persuasive Technology: Using Computers to Change What We Think and Do. Morgan Kaufmann.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1517770</ref_obj_id>
				<ref_obj_pid>1517744</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Fogtmann, M. H., Fritsch, J. &amp; Kortbek, K. J. 2008 Kinesthetic Interaction - Revealing the Bodily Potential in Interaction Design. OZCHI '08: Conference of the computer-human interaction special interest group (CHISIG) of Australia on Computer-Human Interaction. Cairns, Australia, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Gamercize gamercize.com. http://gamercize.net. Accessed 20 April 2009]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Gaver, W. W. 1996. Affordances for Interaction: The Social Is Material for Design. Ecological Psychology, 8, 111--129.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Gratton, C. &amp; Henry, I. P. 2001 Sport in the City: The Role of Sport in Economic and Social Regeneration. Routledge.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Graves, L., Stratton, G., Ridgers, N. D. &amp; Cable, N. T. 2007. Comparison of energy expenditure in adolescents when playing new generation and sedentary computer games: cross sectional study. BMJ, 335, 1282--1284.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[H&amp;#228;m&amp;#228;l&amp;#228;inen, P., Ilmonen, T., H&amp;#246;ysniemi, J., Lindholm, M. &amp; Nyk&amp;auml;&amp;#228;nen, A. 2005 Martial arts in artificial reality. ACM New York, NY, USA, 781--790.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1297131</ref_obj_id>
				<ref_obj_pid>1297126</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Hummels, C., Overbeeke, K. C. J. &amp; Klooster, S. 2007. Move to get moved: a search for methods, tools and knowledge to design for expressive and rich movement-based interaction. Personal and Ubiquitous Computing, 11, 677--690.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1241025</ref_obj_id>
				<ref_obj_pid>1240866</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Jacob, R., Girouard, A., Hirshfield, L., Horn, M., Shaer, O., Solovey, E. &amp; Zigelbaum, J. 2007 Reality-based interaction: unifying the new generation of interaction styles. CHI '07: CHI '07 extended abstracts on Human factors in computing systems. ACM Press, 2465--2470.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1225516</ref_obj_id>
				<ref_obj_pid>1225302</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Jocelyn, S. &amp; Karon, M. 2007. Communicating emotion through a haptic link: Design space and methodology. Int. J. Hum.-Comput. Stud., 65, 376--387.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1142429</ref_obj_id>
				<ref_obj_pid>1142405</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Klemmer, S. &amp; Hartmann, B. 2006 How Bodies Matter: Five Themes for Interaction Design. Design of Interactive Systems.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Kreijns, K., Kirschner, P. A. &amp; Jochems, W. 2002. The sociability of computer-supported collaborative learning environments. Educational Technology &amp; Society, 5, 8--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Larssen, A. T., Loke, L., Robertson, T., Edwards, J. &amp; Sydney, A. 2004. Understanding Movement as Input for Interaction--A Study of Two Eyetoy&amp;trade;#8482; Games. Proc. of OzCHI'04.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1357136</ref_obj_id>
				<ref_obj_pid>1357054</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Lindley, S. E., Le Couteur, J. &amp; Berthouze, N. L. 2008 Stirring up experience through movement in game play: effects on engagement and social behaviour. Proceeding of the twenty-sixth annual SIGCHI conference on Human factors in computing systems. Florence, Italy, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1297127</ref_obj_id>
				<ref_obj_pid>1297126</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Loke, L., Larssen, A., Robertson, T. &amp; Edwards, J. 2007. Understanding movement for interaction design: frameworks and approaches. Personal and Ubiquitous Computing. 11, 691--701.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1044942</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Mccarthy, J. &amp; Wright, P. 2004 Technology as Experience. The MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Moen, J. 2006 KinAesthetic Movement Interaction: Designing for the Pleasure of Motion. Stockholm: KTH, Numerical Analysis and Computer Science.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Mokka, S., V&amp;#228;&amp;#228;t&amp;#228;nen, A., Heinil&amp;#228;, J. &amp; V&amp;auml;&amp;#228;lkkynen, P. 2003 Fitness computer game with a bodily user interface. Carnegie Mellon University Pittsburgh, PA, USA, 1--3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>642709</ref_obj_id>
				<ref_obj_pid>642611</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Mueller, F., Agamanolis, S. &amp; Picard, R. 2003 Exertion Interfaces: Sports over a Distance for Social Bonding and Fun. Proceedings of the SIGCHI conference on Human factors in computing systems. Ft. Lauderdale, Florida, USA, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1324922</ref_obj_id>
				<ref_obj_pid>1324892</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Mueller, F., Agamanolis, S., Vetere, F. &amp; Gibbs, M. R. 2007a Brute force as input for networked gaming. Proceedings of the 19th Australasian conference on Computer-Human Interaction: Entertaining User Interfaces. Adelaide, Australia, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1400985</ref_obj_id>
				<ref_obj_pid>1400885</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Mueller, F., Agamanolis, S., Vetere, F. &amp; Gibbs, M. R. 2008a Remote Impact: Shadowboxing over a Distance. ACM SIGGRAPH 2008 posters. Los Angeles, California, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1517772</ref_obj_id>
				<ref_obj_pid>1517744</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Mueller, F., Gibbs, M. &amp; Vetere, F. 2008b Taxonomy of Exertion Games. Ozchi '08: Conference of the computer-human interaction special interest group (CHISIG) of Australia on Computer-Human Interaction. Cairns, Australia, ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1518938</ref_obj_id>
				<ref_obj_pid>1518701</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Mueller, F., Gibbs, M. &amp; Vetere, F. 2009 Design Influence on Social Play in Distributed Exertion Games. CHI '09.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1297132</ref_obj_id>
				<ref_obj_pid>1297126</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Mueller, F., Stevens, G., Thorogood, A., O'brien, S. &amp; Wulf, V. 2007b. Sports over a Distance. Personal and Ubiquitous Computing, 11, 633--645.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1240708</ref_obj_id>
				<ref_obj_pid>1240624</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[O'Brien, S. &amp; Mueller, F. 2007 Jogging the distance. Proceedings of the SIGCHI conference on Human Factors in computing systems. San Jose, California, USA, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Pate, R. R., Pratt, M., Blair, S. N., Haskell, W. L., Macera, C. A., Bouchard, C., Buchner, D., Ettinger, W., Heath, G. W. &amp; King, A. C. 1995. Physical activity and public health. A recommendation from the Centers for Disease Control and Prevention and the American College of Sports Medicine. JAMA, 273, 402--407.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Pcgamerbike pcgamerbike.com. http://pcgamerbike.com/Overview.php. Accessed 20 April 2009]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Ratey, J. 2008 Spark: The Revolutionary New Science of Exercise and the Brain. Little, Brown and Company.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1215723</ref_obj_id>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Salen, K. &amp; Zimmerman, E. 2003 Rules of Play: Game Design Fundamentals. The Mit Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Weinberg, R. S. &amp; Gould, D. 2006 Foundations of Sport and Exercise Psychology. Human Kinetics.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[WII Wii. http://wii.nintendo.com. Accessed 20 April 2009 Wii Sports. http://wii.nintendo.com/software_wii_sports.html. Accessed 20 April 2009]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Wright, P. 2008 Whole Body Interaction. Workshop at HCI International 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Framework for Exertion Interactions over a Distance Florian Floyd Mueller1,2, Stefan Agamanolis1, 
Frank Vetere2, Martin R. Gibbs2 1Distance Lab 2Interaction Design Group Horizon Scotland The University 
of Melbourne Forres, Moray Melbourne UK Australia floyd@exertioninterfaces.com, stefan@distancelab.org, 
f.v@unimelb.edu.au, martin.gibbs@unimelb.edu.au ABSTRACT Exertion games are an emerging form of computer 
games that aim to leverage the advantages of sports and exercise in order to support physical, social 
and mental health benefits. Despite the increased attention these games received recently, there is a 
lack of understanding of what role the game s design plays in encouraging people to invest physical effort 
into these games. We aim to contribute to this understanding by presenting a framework for Exertion Interactions 
over a Distance , consisting of three core concepts: exertion, sociality and engagement. To demonstrate 
the usefulness of our framework we utilize a networked game called Remote Impact that encourages intense 
physical exertion. We hope our work can support researchers in gaining an understanding of this exciting 
new field, whilst also aiding designers in the creation of new games, leveraging the associated benefits 
of exertion. Author Keywords Framework, design space, Exertion Interface, physical, tangible, videoconferencing, 
sports, exhausting, social. CR Categories H5.2. Information Interfaces and presentation (e.g., HCI): 
User Interfaces.  INTRODUCTION Physical exertion, such as exhibited in sports, has been attributed 
to major benefits. From a physical health perspective, sport can contribute to weight loss, addressing 
the obesity issue, and reduce the risk of cardiovascular disease, diabetes, several types of cancer and 
more [Pate et al. 1995]. From a social and mental health viewpoint, sport is believed to teach social 
skills, provide opportunities to socialize [Weinberg and Gould 2006], encourage team- Copyright &#38;#169; 
2009 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part 
or all of this work for personal or classroom use is granted without fee provided that copies are not 
made or distributed for commercial advantage and that copies bear this notice and the full citation on 
the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM 
Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org. Sandbox 2009, New Orleans, Louisiana, August 
4 6, 2009. &#38;#169; 2009 ACM 978-1-60558-514-7/09/0008 $10.00 building and support individual growth 
and community development [Gratton and Henry 2001]. Despite the many benefits of exertion, involvement 
in exertion activities varies: for example, most people in the developed world do not achieve the minimum 
amount of recommended exercise [Weinberg and Gould 2006]. There are many reasons why people do not engage 
in physical activity. Amongst the main factors are concerns that most prescribed exercise programs are 
not engaging enough [Ratey 2008]. Also, most people prefer to exercise with others, and joining peers 
for sports activities has been found to increase uptake, engagement, and satisfaction [Weinberg and Gould 
2006], however, finding suitable partners can be difficult [O'Brien and Mueller 2007]. In fact, many 
of the proposed strategies that aim to increase participation in physical activity suggest involving 
social support mechanisms as well as facilitating intrinsic motivation by making the exertion experience 
enjoyable and meaningful [Weinberg and Gould 2006]. Game research has begun to contribute to this area, 
mainly through designs that augment sport activities with an interactive game component to facilitate 
an increased level of engagement. These designs often comprise of attaching a workout machine to a game 
console, such as connecting an exercise bike to an XBox [PCGamerbike] or a foot-stepper to a Playstation 
[Gamercize]. The aim of these approaches is to foster engagement with the game to distract the user from 
the discomfort that comes with exercise [Fogg 2002]. These approaches have been criticized, however, 
for relying on activities such as stair-climbing and treadmill-running that have high drop-out rates 
[Bogost 2007], activities which users have described as lacking purpose and meaning [Weinberg and Gould 
2006]. Other approaches such as the Nintendo Wii [Wii] and the Eyetoy [EyeToy] allow for less monotonous 
free-form movements, however, their benefit in terms of energy expenditure has been criticized for not 
being high enough to contribute towards the recommended daily amount [Graves et al. 2007]. It has also 
been suggested that these games do not properly support the social rituals that are afforded by exertion 
activity, in particular when compared with traditional sports [Bogost 2007]. Most emerging interactive 
systems that consider exertion aim to address individual aspects associated with non­participation in 
physical activity. They either try to increase engagement, but fall short in supporting the creation 
of a meaningful exertion activity, or they facilitate a more meaningful activity, but can only generate 
limited exertion levels well below those associated with traditional sports. Some systems focus on promoting 
the exertion aspect, but fall short in facilitating social support mechanisms. Despite the potential 
to contribute to successful exercise participation, there seems to be a lack of understanding of how 
informed game design can integrate these exertion and social aspects successfully. In order to support 
the development of such an understanding, we offer a theoretical framework that is aimed at guiding research 
in this area. We believe that one of the key advantages of augmenting exertion activities with technological 
advances is the opportunity to support distributed participants. Supporting geographically distant participants 
can expand the number of possible sports-partners, enabling activities that otherwise might fail if no 
local co-participant is available [Weinberg and Gould 2006]. Supporting distributed participants also 
allows for utilizing social support structures despite being apart [Mueller et al. 2003]. We therefore 
focus on supporting participants in geographically distant locations in our investigations on exertion 
games.  OVERVIEW We begin by describing related work on frameworks and systems that consider exertion 
in computer games. We then argue that a dedicated framework has the opportunity to highlight the unique 
aspects exertion can contribute to interactive systems in a way no previous work has done. We explain 
our proposed framework of Exertion Interactions over a Distance . This framework has three core concepts, 
exertion, engagement and sociality, and we argue that meaning can ascribe a value aspect to these concepts. 
To demonstrate the usefulness of our framework, we utilize a networked game called Remote Impact , which 
encourages intense physical exertion. We conclude with a research agenda for future work in light of 
the opportunities and dangers of facilitating exertion in interactive games. RELATED WORK Frameworks 
Prior work has considered the importance of the user s body in interactive systems [Dourish 2001]. As 
a result of this research, theoretical models have emerged to better understand the role of bodily interactions 
[Loke et al. 2007]. For example, Larssen et al. applied three movement­based frameworks to an analysis 
of a game played with an EyeToy camera [Larssen et al. 2004]. They found that although these frameworks 
can be useful in analyzing specific aspects of the game, there is still a lack of understanding of the 
role of the body in interactive systems, in particular when it comes to exertion in games. Bianchi-Berthouze 
et al. investigated the benefits of including bodily actions in interactive games. Their work found that 
such an approach could unleash regulatory properties of emotion, leading to more engagement in games 
[Bianchi-Berthouze et al. 2007]. In a follow-up study, the researchers investigated the effects of introducing 
a co-player to such a game [Lindley et al. 2008]. The authors found that the quality of the engagement 
changes, from hard fun to social fun , and suggest that this change was facilitated by the bodily movements 
that were natural to the scenario of the game . De Kort et al. describe a framework for the sociality 
characteristics in games, arguing that gaming is often as much about social interaction, as it is about 
interaction with the game content [de Kort and Ijsselsteijn 2008]. Some of their sociality characteristics 
have been considered in the exertion work by Fogtmann [Fogtmann et al. 2008], which builds upon the findings 
on kinesthetic movements [Moen 2006], suggesting a link between mediated social and exertion activities. 
However, there is still a lack of theoretical understanding how these social concepts take shape in exertion 
interactions that are supported by interactive systems. Exertion in Interactive Systems Exertion has 
been considered in computer games before, however, mostly from an implementation rather than a conceptual 
perspective. Hämäläinen et al. developed an exertion game that tracks martial art athletes to map their 
movements onto virtual avatars [Hämäläinen et al. 2005]. Although multiple players can play Kick Ass 
Kung Fu simultaneously, there is no analytical understanding of how the engagement of the game and social 
aspects interact, and how this interaction is supported by the design. Mueller et al. have created a 
physically effortful game that can be played over a distance based on soccer. Although exertion and connectedness 
as concepts were used in a user study, it is still to be investigated how the engagement from the game 
contributed to its success [Mueller et al. 2003]. The Virtual Fitness Center [Mokka et al. 2003] uses 
exercise bicycles positioned in front of a video screen. The physical movements conducted on the exercise 
bicycle are used as input to modify the representation of 3D virtual environment data. Unfortunately, 
this system has not been evaluated, therefore it is unclear how social support from co-riders affected 
performance, and how much the feedback from the virtual environment contributed to engagement.  APPROACH 
Prior work has investigated exertion in interactive systems, and designs exist that consider social and 
engagement aspects in regards to exertion. However, there is a lack of understanding as to how these 
aspects are facilitated through the game s design. We aim to contribute to this understanding by presenting 
a framework that encompasses three aspects: exertion, sociality and engagement. As we focus on networked 
games, we call our framework Exertion Interactions over a Distance . EXERTION INTERACTIONS OVER A DISTANCE 
 Fig. 1: Framework for Exertion Interactions over a Distance Our framework consists of 3 core concepts: 
exertion, sociality and engagement (Figure 1). The centerpiece meaning provides a lens to see how these 
concepts are interwoven and mutually engaged. We begin by discussing the role of meaning and then describe 
each concept, including how meaning can add a value aspect to each. Meaning We use the notion of meaning 
to ascribe a value aspect to our framework. Meaning can refer to the value of an activity around one 
of the core concepts exertion, sociality and engagement -, but more importantly, it interweaves them 
all, offering a way of describing and analyzing the quality of their interrelationships. Just like McCarthy 
and Wright, we use meaning to understand the user s engagement with technology [McCarthy and Wright 2004], 
as meaning is constructed in practical acts of engagement with the physical and social world [Wright 
2008]. We believe meaning can also help us understand the sociality aspect in distributed social interactions, 
as suggested by Dourish [Dourish 2001], and we further suggest that meaning is also useful when trying 
to analyze how these aspects relate to exertion: Weinberg and Gould laid the groundwork by suggesting 
that exertion activities can be evaluated in terms of how meaningful they are [Weinberg and Gould 2006]. 
Considering the notion of meaning can help us understand exertion, as exertion can describe a quality 
of the interaction, while, in turn, interaction creates meaning [Hummels et al. 2007]. Exertion Exertion 
refers to the act of exerting, involving skeletal muscles, which results in physical fatigue, often associated 
with physical sport. An exertion interaction utilizes an input mechanism in which the user is investing 
physical exertion. An exertion interface has been previously defined as being physically exhausting and 
requiring intense physical effort [Mueller et al. 2003]. Exertion and Meaning People need to see purpose 
and meaning in their exertion activities, otherwise participation rates will drop [Weinberg and Gould 
2006]. One way of making exertion activity meaningful is by augmenting it with an interactive game experience, 
however it must be meaningful to the user in regards to the activity. For example, we propose that exercise 
bicycles connected to computer screens should map the pedaling efforts to the characteristics of the 
virtual world, so that if the avatar climbs up a hill, the user has to invest more effort, attributing 
meaning (getting over the hill) to the exertion activity (pedaling harder). Another way of supporting 
the construction of meaning in exertion activities could be to orient the actions on the real, physical 
world. We borrow the term naïve physics [Jacob et al. 2007] to explain this further. Naïve physics is 
the informal human perception of basic physical principles, or in other words, common sense knowledge 
about the physical world. This includes concepts like gravity, friction and velocity [Jacob et al. 2007]. 
We postulate that an exertion action could be made more meaningful if a player who hits a virtual object 
experiences kinesthetic feedback, as the player would expect this feedback in the real world.  Sociality 
Sociality is the extent to which a system can give rise to and support social interactions between the 
users of that system. This extent depends on the quality of the social affordances inherent in the system 
[Kreijns et al. 2002]. Gaver calls them affordances for sociality [Gaver 1996], and they can derive from 
the physical environment, but can also be facilitated by opportunities for verbal and non-verbal communication. 
Sociality and Meaning Providing a context for sociality, for example through aspects of a game that offer 
a reason to communicate [Mueller et al. 2007b], can facilitate opportunities for social interaction [de 
Kort and Ijsselsteijn 2008]. When players participate in social play, they communicate via gameplay, 
and aspects of the game become a context for stylized communication, mediated through social interaction 
[Salen and Zimmerman 2003]. This meaning making through social interaction has been suggested to influence 
engagement [de Kort and Ijsselsteijn 2008] and exertion [Weinberg and Gould 2006]. An example of facilitating 
meaningful sociality is the support of awareness of a player s own, but also his/her partner s exertion 
level. Physical fatigue is a key element in exertion interactions, and knowing one s own and the other 
s level of fatigue can help to determine how to play the game and therefore provide a context for social 
interaction.  Engagement Engagement can be described as our involvement with technology [McCarthy and 
Wright 2004]. Engagement in games is most commonly associated with a player s involvement with the gameplay. 
Here, it includes engagement with the exertion activities facilitated by the game. It has been suggested 
that this engagement increases if the exertion increases [Bianchi-Berthouze et al. 2007], however, as 
exertion can quickly result in fatigue, negative effects on engagement can occur quickly, a characteristic 
salient to exertion interactions. Engagement and Meaning One way of facilitating engagement in exertion 
interactions is by supporting meaningful play. For meaningful play to occur, a player should be able 
to perceive the immediate outcome of an (exertion) action, and the outcome of this action should be woven 
into the game system as a whole [Salen and Zimmerman 2003]. REMOTE IMPACT We now demonstrate how our 
framework can be useful in the analysis a distributed exertion game. We use Remote Impact (Figure 2), 
a distributed game inspired by combat sport, as it encourages intense physical exertion [Mueller et al. 
2008a, Mueller et al. 2007a]. We show how our framework helps in structuring user data, and supports 
identifying characteristic themes. Gameplay The gameplay of Remote Impact is as follows: Two remote 
players enter identical interaction spaces. They are facing a sensitive padded playing area, on which 
two shadows are projected, that of the remote person, and their own shadow. These shadows appear to be 
created by a light source behind each of the players, i.e. if they step closer to the interaction area, 
their shadows increase in size. If the players face the interaction surface, they can stand as if standing 
next to each other, because each surface shows the silhouettes of both people. The interaction areas 
are large enough to cover both body shapes from head to toe, each spanning a complete surface area of 
2.10 x 2.50 meters. The players can also hear each other through an echo-and noise-cancelling videoconferencing-quality 
speakerphone. Once the game starts, both players try to execute impacts on each other s shadow. They 
can target any area of their partner s body, and administer hits with their hands, feet, arms, legs, 
or their entire body. They can hit with a flat hand or use their fists. An impact on the remote person 
s shadow area is considered a successful hit. The bigger the intensity of the hit, the more points are 
scored. The players scores are visible to both parties. If a hit is placed within the shadow area of 
the remote person, a visual indicator in a comic pow -style is displayed on the impact spot and a sound 
effect is played to indicate for both players that a successful hit occurred. The player with the most 
points wins the game.  Technical Implementation Each station consists of a dedicated impact area, constructed 
of mattress-like foam and two layers of fabric. The foam is covered with a durable, but soft and lightweight 
rip-stop cover sheet. We wanted to detect the location as well as the intensity of hits and kicks without 
exposing technology that the user could break during the exertion action. We found no existing system 
that could meet our requirements; therefore we created our own sensing system. The impact of the user 
s body is measured by detecting the deformation of the surface area: upon impact, the fabric exhibits 
pulling forces which extent all the way across to the edges of the impact area. Along the edges of the 
interaction area are stretch sensors, which stretch when an impact occurs. Our sensing system forms a 
grid of 42 distinct impact locations that we found sufficient considering that most impacts occur with 
a fist or foot. Our approach for large surface interaction has the advantage that the sensors and electronics 
are moved away from the impact area, encouraging users to exhibit intense physical effort. Unlike many 
other large-scale interactive surfaces, our sensing system can differentiate between fast successive 
impacts, detects the intensity of hits, and supports multi-touch.  Image Recognition One challenge we 
faced was the cone-shape capture area of the cameras, used for the videoconferencing system. Videoconferencing 
systems require the actors to stay a certain distance away from the projection screen, because this is 
where the camera is located, often attached to, or peaking through, a hole in the projection surface, 
capturing the local action. But such camera placement is problematic for impact games when the player 
needs to actually contact the projection surface. The conical capturing area of the videoconferencing 
camera does not provide adequate coverage when the player approaches the contact surface, in particular 
once she/he blocks all available light from entering the camera lens. We therefore opted for an alternative 
approach to visualize the surface actions on the remote end: a camera mounted behind the user captures 
his/her actions. This captures all body movements, even when interacting with the surface area close-up. 
However, instead of distributing the videostream of the participant s back to the remote end, we use 
image analysis to detect the contours of the person and display his/her silhouette instead, reducing 
the unfamiliarity of videoconferencing a person s backside. We use a segmentation algorithm and distribute 
the generated vision analysis result over a network connection to the remote end. The user is able to 
determine the other person s body interactions in real-size, even when this person is standing close 
to the projection surface. However, the silhouette functionality takes away any facial expressions that 
might contribute to the enjoyment and social interactions between remote participants [Mueller et al. 
2007b].  ANALYSIS Exertion Remote Impact encourages extreme physical effort by rewarding players with 
more points if they hit the surface area hard. It also supports interactions with all limbs and the torso, 
even concurrently, encouraging a full-body workout. Remote Impact also takes into account a person s 
location, hence encouraging the players to move back and forth, further demanding physical effort. Players 
exertion actions can help construct meaning because they are modeled after combat sports, in which a 
player tries to control the opponent , interacting with one another s activity [Mueller et al. 2008b]. 
The players actions involve concepts of offence and defense during gameplay via their bodies, characteristic 
of traditional combat games [Mueller et al. 2008b], hence the exertion activity supports meaning making 
for the players based on their prior knowledge. The concept of naïve physics incorporated into the design 
helps the user to experience meaningful exertion. When hitting the surface area, the player experiences 
kinesthetic feedback, in contrast to, for example, the Nintendo Wii boxing game, another combat inspired 
game in which the user has to hit thin air in order to score points, with no kinesthetic feedback [Wii 
Sports]. Players expect such feedback from the real world [Jacob et al. 2007], hence Remote Impact is 
better aligned with naïve physics than Wii s boxing. It should be noted, however, that this approach 
to feedback through a deformation of a mattress represents a design tradeoff [Jacob et al. 2007], a more 
advanced system would also consider kinesthetic feedback coming from the remote player. Remote Impact 
also supports an aspect of spatial scale: the remote player is projected in life-size, in contrast to 
a miniaturized representation. By comparing the size of their opponent with their own, players can estimate 
the amount of exertion needed to win the game, a design aspect contributing to the construction of meaning 
between exertion action and engagement. Technological constraints can limit the detection capabilities 
of exertion actions, for example, many sensor systems in commercial games cannot adequately account for 
simultaneous actions of multiple limbs. Remote Impact supports the creation of meaning better as the 
sensing system does not require the players to artificially constrain themselves and interact sequentially: 
in the real world, exertion actions can happen simultaneously, and Remote Impact s detection system recognizes 
these actions simultaneously. Sociality Players in Remote Impact can communicate via an always­on two-way 
audioconference. Remote Impact also includes a life-size videostream of the remote person, artistically 
rendered as a shadow. Participants can therefore add meaning to their interaction through body language 
via their shadow representation. Research has shown that players can communicate affect through body 
posture [Bianchi-Berthouze et al. 2007]. The players in Remote Impact have therefore the opportunity 
to assess the remote person s exertion levels via their shadow s posture, but also via their breathing 
that they hear over the audioconference, adding to the sociality between the two sites. Besides using 
a videoconferencing component, people have used touch to communicate meaning across geographically distant 
locations [Jocelyn and Karon 2007]. The players in Remote Impact have also the opportunity to communicate 
through touch, contributing to their social interaction, however, the current implementation is a design 
tradeoff: the feedback on the remote end is visual instead of kinesthetic. On the other hand, the system 
supports varying degrees of intensity, and hence allows players to send a message [Mueller et al. 2009], 
inscribing meaning via the associated score and the impact s distributed noise.  Engagement Remote Impact 
aims to support engagement by modeling the gameplay on an existing exertion activity; it encourages actions 
known from combat sports such as wrestling, martial arts or boxing. These actions facilitate engagement 
of the entire body by supporting full-body contact; for the system, these activities are sensable , sensible 
and desirable [Benford et al. 2005], in contrast to many exertion games in which technological fragility 
often limits full-body contact. Meaningful engagement is facilitated through meaningful play: the player 
perceives the immediate outcome of exertion actions through visual, audio and kinesthetic feedback. In 
terms of integration, the outcome of an exertion action is woven into the game system as hits score points. 
The more points a player scores, the more likely she/he will defeat the other player though exposing 
oneself with a hitting action makes one vulnerable to being hit. This makes Remote Impact a game of offense 
and defense similar to combat sports, aiming to facilitate comparable engagement.  USER STUDY In order 
to understand how participants play Remote Impact, we invited 20 volunteers. They were asked to organize 
themselves into pairs, and each team played a dedicated gaming session of at least 20 min and was interviewed 
together afterwards. The game interactions as well as the interviews were videotaped for a coding analysis. 
The interview was semi-structured, and the participants were encouraged to freely share their opinions. 
Our goal was not a formal evaluation per se, but rather to investigate what kinds of results an analysis 
that is guided by our framework could produce. Exertion People used excessive force when playing, and 
were kicking, slapping, boxing and slamming vigorously. During the interviews, participants expressed 
that they were much more exhausted than they thought they would be. Some of them were very out of breath: 
This is the toughest exercise I had for weeks and This is more exhausting than the hour of squash I played 
earlier. Some participants asked if they could break because they felt too exhausted to continue. Stress 
relief One participant noted that the game had a therapeutic effect on him. He wanted to have the game 
in his workplace, because that could be a great tool to let off steam in high stress work environments 
. Two participants, who played following their working day, commented: We should do that everyday after 
work, to get rid of our aggression . Sociality One participant made an interesting point in regards to 
the sociality of related physical body experiences such as combat sports: Although martial arts are not 
traditionally regarded as team sports, the technological augmentation turns it into a more social experience, 
extending its traditional role . Shadow One participant noted in the interview that she missed the ability 
to see the other person s facial expressions: Especially if it is a stranger, it would be important to 
see how serious she or he is taking it . She suggested a videoconference display attached to the side, 
because you would only need it during the breaks, when you have time to breath . Engagement Competitiveness 
Although designed as a social game, players appeared to appreciate the competitiveness of the game, indicated 
by their comments such as: This time I will beat you . However, some players got so involved in hitting 
successively that it seemed that they were unable to divert their visual focus to glance at the score. 
Interaction Style Some players followed a clear offensive approach in which they were trying to hit as 
often as possible, whereas others were more concerned about not getting hit. Neither tactic seemed to 
provide a distinct advantage. The strategy changed the nature of the exercise, however: people with a 
defensive style were using their legs more by running left and right, and some were bending down, trying 
to minimize their shadow surface area, whereas offense players were quickly out of breath because of 
their fast intensive hits. Physical Contact When asked if it was awkward that the game required hitting 
another person, the participants said that they did not see it as such. Some said that the stylized representation 
helped regarding the object to be hit as a comic figure . Others compared it to computer game characters, 
and one player mentioned that it is like in sport: you get your aggression out, but that does not mean 
if you are a boxer, you are beating up other people on the street .  Framework helps identifying opportunities 
Our framework helped us identifying unique aspects characteristic to exertion games that might be missed 
otherwise. For example, by using our framework, we identified the themes physical contact and interaction 
style as described above. By relating them back to the three core concepts while being sensitized to 
the notion of meaning, we found the notion of risk, in particular physical risk, an important aspect 
in order to understand exertion games. Risk is associated with the irreversibility of the users actions 
[Klemmer and Hartmann 2006], but can also be related to the risk of having an unpleasant physical experience 
by the means of the interaction. We were surprised how much participants valued the reduction of risk 
in Remote Impact when compared to traditional combat sports: It s great that you cannot get injured! 
One player had participated in recreational boxing before and compared the Remote Impact experience in 
terms of the risk for the other player and himself. We also observed that several players exploited this 
reduction of risk and played a purely offense approach by hitting as hard and as often as possible, without 
being concerned of getting hit. Risk is an important element of most sports; some even say that if a 
sport does not hurt, it was not a good game [Weinberg and Gould 2006]. This complex role of risk in exertion 
interactions is an opportunity for future work.  FUTURE WORK So far, our work has focused on computer 
games in a competitive environment. Although the encouragement of exertion seems to facilitate competitive 
aspects in games [Mueller et al. 2009], collaboration behavior seems to be an intriguing area for future 
investigation, as sports research suggests social facilitation effects amongst participants [Weinberg 
and Gould 2006]. We also encourage future research that can contribute to our understanding of exertion 
games in which multiple players are involved, in particular how sociality aspects change when teams instead 
of individuals are playing against one another. Furthermore, we have also yet to consider the influence 
of audience members in exertion games. DISCUSSION AND CONCLUSIONS We have presented a framework for 
Exertion Interactions over a Distance to contribute to our understanding of exertion in interactive systems, 
with a focus on networked computer games. We explained our choice of three core concepts, exertion, sociality 
and engagement, and argue that by considering the notion of meaning, we can gain insights into how these 
concepts are intertwined while also ascribe a value aspect to them. We have demonstrated that our framework 
can be useful in the analysis of exertion games; our example application was a novel competitive exertion 
game that supports distributed participants called Remote Impact. The framework helped identifying where 
design tradeoffs were made and aided in recognizing opportunities for improvement. We also used our framework 
to support an analysis of player data. The framework helped in structuring the analysis and identifying 
salient characteristic aspects. The framework also aided in revealing opportunities for future research, 
such as the augmentation of risk in exertion interactions. We hope our framework can support researchers 
in analyzing existing exertion games, while also guide designers in creating future systems. Research 
in other application domains such as work and education might also benefit from our framework as these 
areas begin to consider the many benefits of exertion. We hope with our approach we are able to guide 
future research in this exciting new area. FUTURE WORK We like to thank colleagues at the Interaction 
Design Group and Distance Lab who helped with the project, in particular Matt Karau, as well as all the 
volunteers.  REFERENCES BENFORD, S., SCHNÄDELBACH, H., KOLEVA, B., ANASTASI, R., GREENHALGH, C., RODDEN, 
T., GREEN, J., GHALI, A., PRIDMORE, T. &#38; GAVER, B. 2005. Expected, sensed, and desired: A framework 
for designing sensing-based interaction. ACM Transactions on Computer-Human Interaction (TOCHI), 12, 
3-30. BIANCHI-BERTHOUZE, N., KIM, W. &#38; PATEL, D. 2007 Does Body Movement Engage You More in Digital 
Game Play? and Why? Affective Computing and Intelligent Interaction. 102-113. BOGOST, I. 2007 Persuasive 
Games: The Missing Social Rituals of Exergames. http://seriousgamessource.com/features/feature_013107 
_exergaming_1.php. Accessed 20 April 2009 DE KORT, Y. A. W. &#38; IJSSELSTEIJN, W. A. 2008. People, places, 
and play: player experience in a socio-spatial context. Computers in Entertainment (CIE), 6. DOURISH, 
P. 2001 Where the Action Is: The Foundations of Embodied Interaction. MIT Press. EYETOY EyeToy. http://eyetoy.com. 
Accessed 20 April 2009 FOGG, B. J. 2002 Persuasive Technology: Using Computers to Change What We Think 
and Do. Morgan Kaufmann. FOGTMANN, M. H., FRITSCH, J. &#38; KORTBEK, K. J. 2008 Kinesthetic Interaction 
-Revealing the Bodily Potential in Interaction Design. OZCHI '08: Conference of the computer-human interaction 
special interest group (CHISIG) of Australia on Computer-Human Interaction. Cairns, Australia, ACM. GAMERCIZE 
gamercize.com. http://gamercize.net. Accessed 20 April 2009 GAVER, W. W. 1996. Affordances for Interaction: 
The Social Is Material for Design. Ecological Psychology, 8, 111-129. GRATTON, C. &#38; HENRY, I. P. 
2001 Sport in the City: The Role of Sport in Economic and Social Regeneration. Routledge. GRAVES, L., 
STRATTON, G., RIDGERS, N. D. &#38; CABLE, N. T. 2007. Comparison of energy expenditure in adolescents 
when playing new generation and sedentary computer games: cross sectional study. BMJ, 335, 1282-1284. 
HÄMÄLÄINEN, P., ILMONEN, T., HÖYSNIEMI, J., LINDHOLM, M. &#38; NYKÄNEN, A. 2005 Martial arts in artificial 
reality. ACM New York, NY, USA, 781-790. HUMMELS, C., OVERBEEKE, K. C. J. &#38; KLOOSTER, S. 2007. Move 
to get moved: a search for methods, tools and knowledge to design for expressive and rich movement­based 
interaction. Personal and Ubiquitous Computing, 11, 677-690. JACOB, R., GIROUARD, A., HIRSHFIELD, L., 
HORN, M., SHAER, O., SOLOVEY, E. &#38; ZIGELBAUM, J. 2007 Reality-based interaction: unifying the new 
generation of interaction styles. CHI '07: CHI '07 extended abstracts on Human factors in computing systems. 
ACM Press, 2465-2470. JOCELYN, S. &#38; KARON, M. 2007. Communicating emotion through a haptic link: 
Design space and methodology. Int. J. Hum.-Comput. Stud., 65, 376-387. KLEMMER, S. &#38; HARTMANN, B. 
2006 How Bodies Matter: Five Themes for Interaction Design. Design of Interactive Systems. KREIJNS, K., 
KIRSCHNER, P. A. &#38; JOCHEMS, W. 2002. The sociability of computer-supported collaborative learning 
environments. Educational Technology &#38; Society, 5, 8­ 22. LARSSEN, A. T., LOKE, L., ROBERTSON, T., 
EDWARDS, J. &#38; SYDNEY, A. 2004. Understanding Movement as Input for Interaction A Study of Two Eyetoy 
Games. Proc. of OzCHI '04. LINDLEY, S. E., LE COUTEUR, J. &#38; BERTHOUZE, N. L. 2008 Stirring up experience 
through movement in game play: effects on engagement and social behaviour. Proceeding of the twenty-sixth 
annual SIGCHI conference on Human factors in computing systems. Florence, Italy, ACM. LOKE, L., LARSSEN, 
A., ROBERTSON, T. &#38; EDWARDS, J. 2007. Understanding movement for interaction design: frameworks and 
approaches. Personal and Ubiquitous Computing, 11, 691-701. MCCARTHY, J. &#38; WRIGHT, P. 2004 Technology 
as Experience. The MIT Press. MOEN, J. 2006 KinAesthetic Movement Interaction: Designing for the Pleasure 
of Motion. Stockholm: KTH, Numerical Analysis and Computer Science. MOKKA, S., VÄÄTÄNEN, A., HEINILÄ, 
J. &#38; VÄLKKYNEN, P. 2003 Fitness computer game with a bodily user interface. Carnegie Mellon University 
Pittsburgh, PA, USA, 1-3. MUELLER, F., AGAMANOLIS, S. &#38; PICARD, R. 2003 Exertion Interfaces: Sports 
over a Distance for Social Bonding and Fun. Proceedings of the SIGCHI conference on Human factors in 
computing systems. Ft. Lauderdale, Florida, USA, ACM. MUELLER, F., AGAMANOLIS, S., VETERE, F. &#38; GIBBS, 
M. R. 2007a Brute force as input for networked gaming. Proceedings of the 19th Australasian conference 
on Computer-Human Interaction: Entertaining User Interfaces. Adelaide, Australia, ACM. MUELLER, F., AGAMANOLIS, 
S., VETERE, F. &#38; GIBBS, M. R. 2008a Remote Impact: Shadowboxing over a Distance. ACM SIGGRAPH 2008 
posters. Los Angeles, California, ACM. MUELLER, F., GIBBS, M. &#38; VETERE, F. 2008b Taxonomy of Exertion 
Games. OZCHI '08: Conference of the computer-human interaction special interest group (CHISIG) of Australia 
on Computer-Human Interaction. Cairns, Australia, ACM Press. MUELLER, F., GIBBS, M. &#38; VETERE, F. 
2009 Design Influence on Social Play in Distributed Exertion Games. CHI '09. MUELLER, F., STEVENS, G., 
THOROGOOD, A., O BRIEN, S. &#38; WULF, V. 2007b. Sports over a Distance. Personal and Ubiquitous Computing, 
11, 633-645. O'BRIEN, S. &#38; MUELLER, F. 2007 Jogging the distance. Proceedings of the SIGCHI conference 
on Human Factors in computing systems. San Jose, California, USA, ACM. PATE, R. R., PRATT, M., BLAIR, 
S. N., HASKELL, W. L., MACERA, C. A., BOUCHARD, C., BUCHNER, D., ETTINGER, W., HEATH, G. W. &#38; KING, 
A. C. 1995. Physical activity and public health. A recommendation from the Centers for Disease Control 
and Prevention and the American College of Sports Medicine. JAMA, 273, 402-407. PCGAMERBIKE pcgamerbike.com. 
http://pcgamerbike.com/Overview.php. Accessed 20 April 2009 RATEY, J. 2008 Spark: The Revolutionary New 
Science of Exercise and the Brain. Little, Brown and Company. SALEN, K. &#38; ZIMMERMAN, E. 2003 Rules 
of Play: Game Design Fundamentals. The MIT Press. WEINBERG, R. S. &#38; GOULD, D. 2006 Foundations of 
Sport and Exercise Psychology. Human Kinetics. WII Wii. http://wii.nintendo.com. Accessed 20 April 2009 
Wii Sports. http://wii.nintendo.com/software_wii_sports.html. Accessed 20 April 2009 WRIGHT, P. 2008 
Whole Body Interaction. Workshop at HCI International 2008.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1581097</article_id>
		<sort_key>240</sort_key>
		<display_label>Pages</display_label>
		<article_publication_date>08-04-2009</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[A unified approach for physically-based simulations and haptic rendering]]></title>
		<page_from>151</page_from>
		<page_to>159</page_to>
		<doi_number>10.1145/1581073.1581097</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1581097</url>
		<abstract>
			<par><![CDATA[<p>Based on our new geometric data structure, the <i>inner sphere trees</i>, we present fast and stable algorithms for different kinds of collision detection queries between rigid objects at haptic rates. Namely, proximity queries and the penetration <i>volume</i>, which is related to the water displacement of the overlapping region and thus corresponds to a physically motivated force.</p> <p>The latter allows us to define a novel penalty-based collision response scheme that provides continuous forces and torques which are applicable to physically-based simulations as well as to haptic rendering scenarios. Moreover, we present a time-critical version of the penetration volume computation that is able to achieve very tight bounds within a fixed budget of query time.</p> <p>The main idea of our new data structure is to bound the object from the <i>inside</i> with a set of <i>non-overlapping</i> bounding volumes.</p> <p>The results show performance at haptic rates both for proximity and penetration volume queries, independent from the polygon count of the objects.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[collision detection]]></kw>
			<kw><![CDATA[collision response]]></kw>
			<kw><![CDATA[haptic rendering]]></kw>
			<kw><![CDATA[physically-based simulation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Object hierarchies</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010244</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Hierarchical representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011066.10011067</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Development frameworks and environments->Object oriented frameworks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1570361</person_id>
				<author_profile_id><![CDATA[81314492809]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rene]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Clausthal University, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570362</person_id>
				<author_profile_id><![CDATA[81100035966]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gabriel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zachmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Clausthal University, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1016055</ref_obj_id>
				<ref_obj_pid>1016050</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Agarwal, Guibas, Nguyen, Russel, and Zhang. 2004. Collision detection for deforming necklaces. <i>CGTA: Computational Geometry: Theory and Applications 28</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1449679</ref_obj_id>
				<ref_obj_pid>1449660</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Barbi&amp;#269;, J., and James, D. L. 2008. Six-dof haptic rendering of contact between geometrically complex reduced deformable models. <i>IEEE Transactions on Haptics 1</i>, 1, 39--52.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1323910</ref_obj_id>
				<ref_obj_pid>1323740</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Birgin, E. G., and Sobral, F. N. C. 2008. Minimizing the object dimensions in circle and sphere packing problems. <i>Computers &amp; OR 35, 7</i>, 2357--2375.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Cameron, S. 1997. Enhancing GJK: Computing minimum and penetration distances between convex polyhedra. In <i>Proceedings of International Conference on Robotics and Automation</i>, 3112--3117.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1632615</ref_obj_id>
				<ref_obj_pid>1632592</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Faure, F., Barbier, S., Allard, J., and Falpou, F. 2008. Image-based collision detection and response between arbitrary volumetric objects. In <i>ACM Siggraph/Eurographics Symposium on Computer Animation, SCA 2008, July, 2008</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Fisher, S. M., and Lin, M. C., 2001. Fast penetration depth estimation for elastic bodies using deformed distance fields.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>740295</ref_obj_id>
				<ref_obj_pid>647909</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[G&amp;#228;rtner, B. 1999. Fast and robust smallest enclosing balls. In <i>ESA</i>, Springer, J. Nesetril, Ed., vol. 1643 of <i>Lecture Notes in Computer Science</i>, 325--338.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Gilbert, E. G., Johnson, D. W., and Keerthi, S. S. 1988. A fast procedure for computing the distance between complex objects in three-dimensional space. <i>IEEE Journal of Robotics and Automation 4</i>, 193--203.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Hammer, B., Hasenfuss, A., and Villmann, T. 2006. Magnification control for batch neural gas. In <i>ESANN</i>, 7--12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Hasegawa, S., and Sato, M. 2004. Real-time rigid body simulation for haptic interactions based on contact volume of polygonal objects. <i>Comput. Graph. Forum 23</i>, 3, 529--538.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Hoff, K. E., Zaferakis, A., Lin, M., and Manocha, D. 2002. Fast 3d geometric proximity queries between rigid &amp; de-formable models using graphics hardware acceleration. Tech. Rep. TR02--004, Department of Computer Science, University of North Carolina - Chapel Hill, Mar. 7. Fri, 8 Mar 2002 20:06:33 GMT.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614309</ref_obj_id>
				<ref_obj_pid>614259</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Hubbard, P. M. 1995. Collision detection for interactive graphics applications. <i>IEEE Transactions on Visualization and Computer Graphics 1</i>, 3 (Sept.), 218--230.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Johnson, D. E., and Cohen, E. 1998. A framework for efficient minimum distance computations. In <i>Proceedings of the IEEE International Conference on Robotics and Automation (ICRA-98)</i>, IEEE Computer Society, Piscataway, 3678--3684.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>797570</ref_obj_id>
				<ref_obj_pid>795683</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Johnson, D. E., and Willemsen, P. 2003. Six degree-of-freedom haptic rendering of complex polygonal model. In <i>HAPTICS</i>, IEEE Computer Society, 229--235.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>545266</ref_obj_id>
				<ref_obj_pid>545261</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Kim, Y., Otaduy, M., Lin, M., and Manocha, D. 2002. Fast penetration depth computation for physically-based animation. In <i>Proceedings of the 2002 ACM SIGGRAPH Symposium on Computer Animation (SCA-02)</i>, ACM Press, New York, S. N. Spencer, Ed., 23--32.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>777856</ref_obj_id>
				<ref_obj_pid>777792</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Kim, Otaduy, Lin, and Manocha. 2003. Fast penetration depth estimation using rasterization hardware and hierarchical refinement (short). In <i>COMPGEOM: Annual ACM Symposium on Computational Geometry</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Larsen, E., Gottschalk, S., Lin, M., and Manocha, D. 1999. Fast proximity queries with swept sphere volumes. In <i>Technical Report TR99--018</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311600</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[McNeely, W. A., Puterbaugh, K. D., and Troy, J. J. 1999. Six degrees-of-freedom haptic rendering using voxel sampling. In <i>Siggraph 1999</i>, Addison Wesley Longman, Los Angeles, A. Rockwood, Ed., Annual Conference Series, ACM Siggraph, 401--408.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1652578</ref_obj_id>
				<ref_obj_pid>1652316</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Mendoza, C., and O'Sullivan, C. 2006. Interruptible collision detection for deformable objects. <i>Computers &amp; Graphics 30</i>, 3, 432--438.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Morris, D. 2006. Algorithms and data structures for haptic rendering: Curve constraints, distance maps, and data logging. In <i>Technical Report 2006--06</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311550</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[O'Brien, J. F., and Hodgins, J. K. 1999. Graphical modeling and animation of brittle fracture. In <i>SIGGRAPH '99: Proceedings of the 26th annual conference on Computer graphics and interactive techniques</i>, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 137--146.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Quinlan, S. 1994. Efficient distance computation between non-convex objects. In <i>In Proceedings of International Conference on Robotics and Automation</i>, 3324--3329.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Redon, S., and Lin, M. C. 2006. A fast method for local penetration depth computation. <i>Journal of Graphics Tools: JGT11</i>, 2, 37--50.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Renz, M., Preusche, C., Ptke, M., Peter Kriegel, H., and Hirzinger, G. 2001. Stable haptic interaction with virtual environments using an adapted voxmap-pointshell algorithm. In <i>In Proc. Eurohaptics</i>, 149--154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Schuermann, A. 2006. On packing spheres into containers (about Kepler's finite sphere packing problem). In <i>Documenta Mathematica</i>, vol. 11, 393--406.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Trenkel, S., Weller, R., and Zachmann, G. 2007. A benchmarking suite for static collision detection algorithms. In <i>International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision (WSCG)</i>, Union Agency, Plzen, Czech Republic, V. Skala, Ed.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>334711</ref_obj_id>
				<ref_obj_pid>334709</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Van Den Bergen, G. 1999. A fast and robust GJK implementation for collision detection of convex objects. <i>Journal of Graphics Tools: JGT4</i>, 2, 7--25.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Weller, R., and Zachmann, G. 2009. Inner sphere trees for proximity and penetration queries. In <i>Robotics: Science and Systems (RSS)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>836122</ref_obj_id>
				<ref_obj_pid>522258</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Zachmann, G. 1998. Rapid collision detection by dynamically aligned DOP-trees. In <i>Proc. of IEEE Virtual Reality Annual International Symposium; VRAIS '98</i>, 90--97.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1275375</ref_obj_id>
				<ref_obj_pid>1275268</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Zhang, L., Kim, Y. J., Varadhan, G., and Manocha, D. 2007. Generalized penetration depth computation. <i>Computer-Aided Design 39</i>, 8, 625--638.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Uni.ed Approach for Physically-Based Simulations and Haptic Rendering Rene Weller* Gabriel Zachmann 
Clausthal University, Germany Clausthal University, Germany Figure 1: Our new data structure, the Inner 
Sphere Tree, is based on sphere packings in arbitrary polygonal objects (left). They are suitable for 
different kinds of geometric queries, namely proximity queries (middle) and the penetration volume (right). 
Abstract Based on our new geometric data structure, the inner sphere trees, we present fast and stable 
algorithms for different kinds of collision detection queries between rigid objects at haptic rates. 
Namely, proximity queries and the penetration volume, which is related to the water displacement of the 
overlapping region and thus corre­sponds to a physically motivated force. The latter allows us to de.ne 
a novel penalty-based collision re­sponse scheme that provides continuous forces and torques which are 
applicable to physically-based simulations as well as to haptic rendering scenarios. Moreover, we present 
a time-critical version of the penetration volume computation that is able to achieve very tight bounds 
within a .xed budget of query time. The main idea of our new data structure is to bound the object from 
the inside with a set of non-overlapping bounding volumes. The results show performance at haptic rates 
both for proximity and penetration volume queries, independent from the polygon count of the objects. 
CR Categories: I.3.5 [Computing Methodologies]: Compu­tational Geometry and Object Modeling Geometric 
algorithms, Object hierarchies; I.3.7 [Computing Methodologies]: Three-Dimensional Graphics and Realism 
Animation, Virtual reality Keywords: physically-based simulation, collision detection, hap­tic rendering, 
collision response 1 Introduction *e-mail: rwe@tu-clausthal.de e-mail:zach@tu-clausthal.de Copyright 
&#38;#169; 2009 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies 
of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM 
Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org. Sandbox 2009, New Orleans, Louisiana, August 
4 6, 2009. &#38;#169; 2009 ACM 978-1-60558-514-7/09/0008 $10.00 The progress of visual and aural sensations, 
thanks to the devel­opment of more powerful hardware in conjunction with improved algorithms, has revolutionized 
the immersion of modern computer games. Since the visual feedback and effects of today s games have become 
extremely mature, it will be more and more important for games to provide realistic feedback to other 
senses, such as our haptic sense. On the hardware side, this has become possible in recent years by the 
advent of .rst inexpensive haptic devices on the consumer mar­ket, such as the Falcon from Novint. Research 
on force-feedback devices and algorithms has been done over 10 years, and has only fairly recently been 
introduced to games.1 However, while there is a large body of research on how to render forces given 
a collision and its contact information, the computation of the latter for massive models is still a 
challenge. First of all, this is due to the much higher effort to compute contact information. Second, 
this is due to the update rates that are necessary for haptic rendering, which need to be much higher 
than for visual rendering, i.e., 250 1000 Hz. And third, de.ning the contact information such that continuous 
contact forces can be derived is not always obvious. Therefore, one of the major challenges in haptic 
rendering for games is the computation of continuous forces at haptic rates.A solution to this challenge 
can also be utilized to do physically-based simulation of rigid bodies, which has become increasingly 
popular in games over the past few years. In this paper, we take advantage of the fact that in rendering 
hap­tic forces, as well as in most real-time applications that involve physically-based simulation, an 
absolutely correct determination of the forces acting on the virtual objects is not necessary. 1.1 Main 
Contributions Based on our new data structure, the Inner Sphere Trees (IST), we present the following 
novel ideas: -new contact information for stable penalty forces, i.e. the pene­tration volume; 1 This 
is in analogy to Nintendo s Wii, which is a transfer and adaptation of the research on novel, intuitive, 
unintrusive input devices over the past 15 years. -a uni.ed algorithm that can compute both an approximate 
mini­mal distance and the approximate penetration volume for a pair of rigid objects; i.e., the application 
does not need to know in advance which situation currently exists between the pair of ob­jects; -a time-critical 
variant of the penetration volume calculation, which runs only for a pre-de.ned time budget, including 
a new heuristic to derive good error bounds, the expected overlap vol­ume; -a novel collision response 
scheme to compute stable and contin­uous forces and torques based on the penetration volume. The main 
idea of the ISTs is that we do not build an (outer) hi­erarchy based on the polygons on the boundary 
of an object, like most other BVHs do, but we .ll the interior of the model with a set of non-overlapping 
simple volumes that cover the object s volume densely. We used spheres in our implementation because 
of their convenient properties, but the idea of using inner BVs for lower bounds instead of outer BVs 
for upper bounds can be extended analogously to other kinds of volumes. On top of these inner BVs, we 
create a hierarchy in order to accelerate the computation of the approximate proximity and penetration 
volume queries. The penetration volume corresponds to the amount of water being displaced by the overlapping 
parts of the objects and, thus, leads to a physically motivated and continuous penalty force. According 
to [Fisher and Lin 2001, Sec. 5.1], it is the most complicated yet ac­curate method to de.ne the extent 
of intersection, which was also reported earlier by [O Brien and Hodgins 1999, Sec. 3.3]. However, to 
our knowledge, there are no algorithms to compute it ef.ciently as yet. Our data structure supports all 
kinds of object representations, e.g. polygon meshes or NURBS surfaces. They only have to be watertight. 
The construction of the ISTs consists of two main steps. First, we have to .ll the object densely with 
a set of non-overlapping spheres. Therefore, we extended a .ood .lling voxelization algo­rithm and combined 
it with a special sphere creation scheme. The second step is to build a hierarchy upon these inner spheres 
for which we use a clustering approach. We describe the algorithm to compute both approximative separa­tion 
distance and penetration volume based on the ISTs. In addition, we include improvements of the algorithm 
in quality and speed. The results show that our new data structure can answer both kinds of queries at 
haptic rates with a negligible loss of accuracy.  2 Previous Work Collision detection has been extensively 
investigated by researchers in the past decades. There exist a large variety of freely avail­able libraries 
for collision detection queries (see e.g. [Trenkel et al. 2007]). However, the number of libraries that 
also support the com­putation of proximity queries or the penetration depth is manage­able. Additionally, 
most of them are not designed to work at haptic refresh rates. In the following, we will give a short 
overview of classical and also state of the art approaches. 2.1 BVH based data structures In [Johnson 
and Cohen 1998] a generalized framework for mini­mum distance computations that depends on geometric 
reasoning and includes time-critical properties is presented. The PQP library [Larsen et al. 1999] uses 
swept sphere volumes as BVs in combi­nation with several speed-up techniques for fast proximity queries. 
We used it in this paper to compute the ground truth for the prox­imity queries. Sphere trees have also 
been used for distance com­putation [Quinlan 1994; Hubbard 1995; Mendoza and O Sullivan 2006]. The algorithms 
presented there are interruptible and they are able to deliver approximative distances. Moreover, they 
all com­pute a lower bound on the distance, while our ISTs derive an upper bound. Thus, a combination 
of these approaches with our ISTs could deliver error bounds in both directions. A local minimum distances 
for a stable force feedback computation is proposed by [Johnson and Willemsen 2003]. They use spatialized 
normal cone pruning for the collision detection. Another classical algorithm for proximity queries is 
the GJK [Gilbert et al. 1988; van den Bergen 1999], which computes the dis­tance between a pair of convex 
objects, by utilizing the Minkowski sum of the two objects. Extensions to the GJK algorithms also al­low 
to measure the penetration depth [Cameron 1997]. [Zhang et al. 2007] presented an extended de.nition 
of the pene­tration depth that also takes the rotational component into account, called the generalized 
penetration depth. However, this approach is computationally very expensive and, thus, might currently 
not be fast enough for haptic interaction rates. [Redon and Lin 2006] ap­proximate a local penetration 
depth by .rst computing a local pen­etration direction and then use this information to estimate a local 
penetration depth on the GPU. Other GPU approaches have been presented by [Kim et al. 2003; Kim et al. 
2002; Hoff et al. 2002] that also support proximity queries in image resolution. There is very little 
literature on penetration volume computation. [Hasegawa and Sato 2004] explicitly construct the intersection 
vol­ume of convex polyhedra and apply this method to 6-DOF haptic rendering. However, this method is 
applicable only to very simple geometries. Another approach mentioned by [Faure et al. 2008] computes 
an approximation of the intersection volume from layered depth im­ages on the GPU. While this approach 
is applicable to deformable geometries, it is restricted to image space precision. 2.2 Voxel based data 
structures Most 6-DOF haptic rendering approaches use the Voxmap Pointshell method [McNeely et al. 1999]. 
The main idea is to di­vide the virtual environment into a dynamic object, that is allowed to move freely 
through the virtual space and static objects that are .xed in the world. The static environment is discretized 
into a set of voxels, whereas the dynamic object is described by a set of points that represents its 
surface. During query time, for each of these points it is determined with a simple boolean test, whether 
it is lo­cated in a .lled volume element or not. [Renz et al. 2001] presented extensions to the classic 
VPS, including optimizations to force cal­culation in order to increase its stability. However, even 
these opti­mizations cannot completely avoid the limits of VPS, namely alias­ing effects, the huge memory 
consumption and the strict disjunction between dynamic and static objects. Closely related to VPS are 
distance .eld based methods. [Barbic.and James 2008] generates continuous contact forces for 6-DOF haptic 
rendering by using a pointshell of reduced deformable mod­els in combination with distance .elds.  
3 Construction of the Inner Sphere Tree In this section we describe the construction of our data structure. 
The goal is to .ll an arbitrary object densely with a set of disjoint (i.e. non-overlapping) spheres 
such that the volume of the object is covered well while the number of spheres is as small as possible. 
 Figure 2: The different stages of our sphere packing algorithm: First, we voxelize the object (left) 
and compute distances from the voxels to the closest triangle (second image; transparency = distance). 
Then, we pick the voxel with the largest distance and put a sphere at its center. We proceed incrementally 
and, eventually, we obtain a dense sphere packing of the object (right). In a second step, we build a 
hierarchy over this set of spheres. We chose spheres for volumes, because they offer a trivial and very 
fast overlap test. Moreover, they are rotationally invariant, and it is easy, in contrast to AABBs or 
OBBs, to compute the exact intersection volume. 3.1 Let there be Spheres Filling objects densely with 
spheres is a highly non-trivial task. Bin packing, even when restricted to spheres, is still a very active 
.eld in geometric optimization and far away from being solved for gen­eral objects [Birgin and Sobral 
2008; Schuermann 2006]. In our implementation of the inner sphere trees, we use a simple heuristic based 
on discrete distance .eld that offers a good trade-off between accuracy and speed in practice. Currently, 
we voxelize the object as an intermediate step (by a sim­ple .ood .lling algorithm). But instead of only 
storing whether or not a voxel is .lled, we additionally store the distance d from the center of the 
voxel to the nearest point on the surface. After the voxelization, we generate the inner spheres greedily. 
We choose the voxel V * with the largest distance d * to the surface. We create an inner sphere with 
radius d * and centered on the center of V * . All voxels whose centers are contained in this sphere 
will not be considered any further. Additionally, we have to update all voxels Vi within a radius 2d 
* around V * and distance d(Vi,V * ) < di + d *; their di must now be set to the new free radius. This 
is, because they are now closer to the sphere around V * than to a triangle on the hull. This process 
stops, when there is no voxel left. After these steps, the object is .lled densely with a set of non­overlapping 
spheres. The density can be controlled by the number of voxels. 3.2 Building the Hierarchy Based on 
the sphere packing, we create a bounding volume hier­archy. To do so, we use a top-down wrapped hierarchy 
approach, according to the notion of [Agarwal et al. 2004], where inner nodes are tight BVs for all their 
leaves, but they do not necessarily bound their direct children. We start with a bounding sphere for 
all inner spheres, which becomes the root node of the hierarchy. To compute that, we use the fast and 
stable smallest enclosing sphere algorithm proposed in [G¨artner 1999]. Then, we recursively divide the 
set of inner spheres into subsets in order to create the children. The partitioning of the spheres can 
be done by using any effective technique. In our examples, we use an approach based on a clus­tering 
algorithm called batch neural gas with magni.cation control (see [Hammer et al. 2006; Weller and Zachmann 
2009] for details). Experiments with our data structure have shown that a branching Algorithm 1: checkDistance( 
A, B, minDist ) input : A, B = spheres in the inner sphere tree in/out : minDist = overall minimum distance 
seen so far if A and B are leaves then // end of recursion minDist = min{distance(A, B), minDist} else 
// recursion step forall children a[i] of A do forall children b[j] of B do if distance(a[i], b[j]) < 
minDist then checkDistance( a[i], b[j], minDist ) factor of 4 produces the best results. Additionally, 
this has the bene­.t that we can use the full capacity of SIMD units in modern CPUs. In the following, 
we will call those spheres in the hierarchy that are not leaves hierarchy spheres. Spheres at the leaves, 
which were created in Section 3.1, will be called inner spheres. Note that hier­archy spheres are not 
necessarily contained completely within the object.  4 BVH Traversal Our algorithm(s) for answering 
proximity queries and for comput­ing the penetration volume work similarly to the classic recursive schemes 
that simultaneously traverse two given hierarchies [Zach­mann 1998]. As a by-product, our algorithm can 
return a witness realizing the separation distance in the case of non-collision, and a partial list of 
intersecting polygons in the case of a penetration. In the following, we describe algorithms for these 
two query types, but it should be obvious how they can be modi.ed in order to pro­vide an approximate 
yes-no answer. This would further increase the speed. First, we will discuss the two query types separately, 
in order to point out their speci.c requirements and optimizations. Then, we explain how they can be 
combined into a single algorithm. 4.1 Proximity Queries Our algorithm for proximity queries works like 
most other classical BVH traversal algorithms. We simply have to maintain, in addi­tion, a lower bound 
for the distance. If a pair of leaves, which are inner spheres, is reached, we update the lower bound 
so far (see Algorithm 1). During traversal, there is no need to visit branches of the bounding volume 
test tree that are farther apart than the current  Figure 3: After constructing the sphere packing (see 
Section 3.1), every voxel can be intersected by several non-overlapping spheres (left). These do not 
necessarily account for the whole voxel space (blue space in the left picture). In order to account for 
these voids, too, we simply increase the radius of the sphere (blue sphere) that covers the center of 
the voxel (right). minimum distance, because of the bounding property. This guaran­tees a high culling 
ef.ciency. 4.1.1 Improving runtime In most collision detection scenarios, there is a high spatial and 
tem­poral coherence, especially when rendering at haptic rates. Thus, in most cases, those spheres realizing 
the minimum distance in a frame are also the closest spheres in the next frames, or they are at least 
in the neighborhood. Therefore, using the distance from the last frame yields a good initial bound for 
pruning during traversal. Thus, in our implementation we store pointers to the closest spheres as of 
the last frame and use their current distance to initialize minDist in Algorithm 1. If the application 
is only interested in the distance between a pair of objects, then, of course, a further speed-up can 
be gained by aban­doning the traversal once the .rst pair of intersecting inner spheres is found (in 
this case the objects must overlap). Moreover, our traversal algorithm is very well suited for paralleliza­tion. 
During recursion, we compute the distances between 4 pairs of spheres in one single SIMD implementation, 
which is greatly facilitated by our hierarchy being a 4-ary tree. 4.1.2 Improving accuracy Obviously, 
Algorithm 1 returns only an approximate minimum dis­tance, because it utilizes only the distances of 
the inner spheres for the proximity query. Thus, the accuracy depends on their density. Fortunately, 
it is very easy to alleviate these inaccuracies by simply assigning the closest triangle (or a set of 
triangles) to each inner sphere. After determining the closest spheres with Algorithm 1, we add a subsequent 
test that calculates the exact distance between the triangles assigned to those spheres. This simple 
heuristic reduces the error signi.cantly even with relatively sparsely .lled objects, and it does not 
affect the running time (see Figure 8).  4.2 Penetration Volume Queries In addition to proximity queries, 
our data structure also supports a new kind of penetration query, namely the penetration volume. This 
is the volume of the intersection of the two objects, which can be interpreted directly as the amount 
of the repulsion force, if it is considered as the amount of water being displaced. Obviously, the algorithm 
to compute the penetration volume (see Algorithm 2) does not differ very much from the proximity query 
test: we simply have to replace the distance test by an overlap test and maintain an accumulated overlap 
volume during the traversal. Algorithm 2: computeVolume( A, B, totalOverlap ) input : A, B = spheres 
in the inner sphere tree in/out : totalOverlap = overall volume of intersection if A and B are leaves 
then // end of recursion totalOverlap += overlapVolume( A, B ) else // recursion step forall children 
a[i] of A do forall children b[j] of B do if overlap(a[i], b[j]) > 0 then computeVolume( a[i], b[j], 
totalOverlap ) 4.2.1 Filling the gaps The algorithm described in Section 3.1 results in densely .lled 
ob­jects. However, there still remain small voids between the spheres that cannot be completely compensated 
by increasing the number of voxels. As a remedy, we assign an additional, secondary radius to each in­ner 
sphere, such that the volume of the secondary sphere is equal to the volume of all voxels whose centers 
are contained within the ra­dius of the primary sphere.This guarantees that the total volume of all secondary 
spheres equals the volume of the object, within the ac­curacy of the voxelization, because each voxel 
volume is accounted for exactly once. Certainly, these secondary spheres may slightly overlap, but this 
simple heuristic leads to acceptable estimations of the penetration volume. 4.2.2 Improvements Similar 
to the proximity query implementation, we can utilize SIMD parallelization to speed up both the simple 
overlap check and the volume accumulation. Furthermore, we can exploit the observation that a recursion 
can be terminated if a hierarchy sphere (i.e., an inner node of the sphere hierarchy) is completely contained 
inside an inner sphere (leaf)of the other IST. In this case, we can simply add the total volume of all 
of its leaves to the accumulated penetration volume. In order to do this quickly, we store the total 
volume Voll(S) = Vol(Sj ), (1) Sj .Leaves(S) where Sj are all inner spheres below S in the BVH. This 
can be done in a preprocessing step during hierarchy creation.  4.2.3 Time-critical computation of penetration 
volume In most cases, a penetration volume query has to visit many more nodes than the average proximity 
query. Consequently, the running time on average is slower, especially in cases with heavy overlaps. 
In the following, we will describe a variation of our algorithm for penetration volume queries that guarantees 
a prede.ned query time budget. This is essential for time-critical applications such as haptic rendering. 
A suitable strategy to realize time-critical traversals is to guide the traversal by a priority queue 
Q. Then, given a pair of hierarchy spheres S and R, a simple heuristic is to use Vol(S n R) for the 
 Figure 4: We estimate the real penetration volume (brown) during our time-critical traversal by the 
density in the hierarchy spheres (green and red) and the total volume of the leaf spheres. priority in 
Q. In our experience, this would yield acceptable upper bounds. Unfortunately, this simple heuristic 
also leads to very bad lower bounds in cases where only a relatively small number of inner spheres can 
be visited (unless the time budget permits an almost complete traversal of all overlapping pairs). A 
simple heuristic to derive an estimate of the lower bound could be to compute Vol(Ri n Sj ), (2) (R,S).QRi.ch(R), 
Sj .ch(S) where ch(S) is the set of all direct children of node S. Equation 2 amounts to the sum of the 
intersection of all direct child pairs of all pairs in the p-queue Q. Unfortunately, the direct chil­dren 
of a node are usually not disjoint and, thus, this estimate of the lower bound could actually be larger 
than the upper bound. In order to avoid this problem, we introduce the notion of expected overlap volume 
in order to estimate the overlap volume more accu­rately. The only assumption we make is that for any 
point inside S, the distribution of the probability that it is also inside one of its leaves is uniform. 
Let (R, S) be a pair of spheres in the p-queue. We de.ne the density of a sphere as Voll(S) p(S)= (3) 
Vol(S) with voll(S) de.ned similarly to equation 1 as the accumulated volume of all inner spheres below 
S. This is the probability that a point inside S is also inside one of its leaves (which are disjoint). 
Next, we de.ne the expected overlap volume Vol(R, S) as the probability that a point is inside R n S 
and also inside the intersection of one of the possible pairs of leaves, i.e., Vol(R, S)= p(S) · p(R) 
· Vol(R n S) Voll(R) · Voll(S) · Vol(R n S) (4) = Vol(R) · Vol(S) (see Figure 4). In summary, for the 
whole queue we get the expected overlap vol­ume Vol(R, S) (5) (R,S).Q Clearly, this volume can be maintained 
during traversal quite easily. More importantly, this method provides a much better heuristic for sorting 
the priority queue: if the difference between the expected Algorithm 3: compVolumeTimeCritical( A, B 
) input : A, B = root spheres of the two ISTs estOverlap = Vol(A, B) Q = empty priority queue Q.push( 
A, B ) while Q not empty &#38; time not exceeded do (R, S)= Q.pop() if R and S are not leaves then estOverlap 
= Vol(R, S) forall Ri . children of R, Sj . children of S do estOverlap += Vol(Ri,Sj ) Q.push( Ri,Sj 
) overlap Vol(R, S) and the overlap Vol(R n S) is large, then it is most likely that the traversal of 
this pair will give the most bene­.t toward improving the bound; consequently, we insert this pair closer 
to the front of the queue. Algorithm 3 shows the pseudo code of this approach. (Note that p(S)=1 if S 
is a leaf, and therefore Vol(R, S) returns the exact intersection volume at the leaves.)  4.3 The Uni.ed 
Algorithm In the previous sections, we introduced the proximity and the pen­etration volume computation 
separately. However, it is of course possible to combine both algorithms. This yields a uni.ed algo­rithm 
that can compute both the distance and the penetration vol­ume. To that end, we start with the distance 
traversal. As soon as we .nd the .rst pair of intersecting inner spheres, we simply switch to the penetration 
volume computation. This is correct because all pairs of inner spheres we visited so far did not overlap 
and thus they could not increase the penetration volume. Thus, we do not have to visit them again and 
can continue with the traversal of the rest of the hierarchies using the penetration volume algorithm. 
If we do not meet an intersecting pair of inner spheres, the uni.ed algorithm still reports the minimal 
separating distance.  5 Collision Response In this section, we describe how to use the penetration volume 
to compute continuous forces in order to enable a stable haptic render­ing. Mainly, there exist three 
different approaches to resolve colli­sions: the penalty-based method, the constraint-based method and 
the impulse-based method. The constraint-based approach com­putes constraint forces that are designed 
to cancel any external ac­celeration that would result in interpenetrations. Unfortunately, this method 
has at least quadratic complexity in the number of contact points. The impulse-based method resolves 
contacts between ob­jects by a series of impulses in order to prevent interpenetrations. It is applicable 
to real-time simulations but the forces may not be valid for bodies in resting contact. So, we decided 
to use the penalty-based method, that computes penalty forces based on the interpenetration of a pair 
of objects. The main advantages are its computational simplicity, which makes it applicable for haptic 
rendering, and its ability to simulate a vari­ety of surface characteristics. Moreover, the use of the 
penetration volume eliminates inconsistent states that may occur when only a penetration depth (e.g. 
a minimum translational vector) is used. 5.1 Contact Forces Algorithm 2, and its time-critical derivative, 
return a set of overlap­ping spheres or potentially overlapping spheres, resp. We compute a force for 
each of these pairs of spheres (Ri,Sj ) by: f(Ri)= kc Vol(Ri n Sj )n(Ri) (6) where kc is the contact 
stiffness, Vol(Ri n Sj ) is the overlap vol­ume, and n(Ri) is the contact normal. Summing up all pairwise 
forces gives the total penalty force: f(R)= f(Ri) (7) RinSj . =Ø In order to compute normals for each 
pair of spheres, we augment the construction process of the ISTs: in addition to storing the dis­tance 
to the object s surface, we store a pointer to the triangle that realizes this minimum distance. While 
creating the inner spheres by merging several voxels (Section 3.1), we accumulate a list of tri­angles 
for every inner sphere. We use the normals of these triangles to compute normal cones, which are de.ned 
by an axis and an an­gle. They tightly bound the normals of the triangles that are stored in the list 
of each inner sphere. During force computation, the axes of the normal cones cR and cS are used as the 
directions of the force, since they will bring the penetrating spheres outside the other object in the 
direction of the surface normals (see Figure 5). Note that f(Ri) .f(Sj ). = If the cone angle is too 
large (i.e., a p), then we simply use the vector between the two centers of the spheres. Obviously, 
this force is continuous in both cases, because the move­ment of the axes of the normal cones and also 
the movement of the centers of the spheres are continuous, provided the path of the ob­jects is continuous. 
See Figure 6 for results from our benchmark. 5.2 Torques In rigid body simulation, the torque t is usually 
computed as t = (Pc-Cm)×f, where Pc is the point of collision, Cm is the center of mass of the object 
and f is the force acting at Pc. Like in the section before, we compute the torque separately for each 
pair (Ri,Sj ) of intersecting inner spheres: t(Ri)=(P(Ri,Sj ) - Cm) × f(Ri) (8) Again, we accumulate 
all pairwise torques to get the total torque: t (R)= t (Ri) (9) RinSj . =Ø We de.ne the point of collision 
P(Ri,Sj) simply as the center of the intersection volume of the two spheres (see Figure 5). Obviously, 
this point moves continuously if the objects move continuously. In combination with the continuous forces 
f(Ri) this results in a con­tinuous torque.  6 Results We have implemented our new data structure in 
C++ on a PC run­ning Windows XP with an Intel Pentium IV 3GHz dual core CPU and 2GB of memory. We extended 
Dan Morris Voxelizer [Morris 2006] to compute the initial distance .eld. We used several hand recorded 
object paths for our benchmarks. For the proximity queries, we focused on very close con.gurations, 
Figure 5: Left: We compute a normal cone for each inner sphere. The cone bounds a list of triangles that 
is associated with the sphere. Note that the spread angle of the normal cone can be 0 if the sphere is 
closest to a single triangle (e.g., the green sphere). Right: The axis of the normal cones cR and cS 
are used for the force direction. The center PR,S of the spherical cap de.nes the contact point. within 
a distance range of about 0 10% of the object s BV size, be­cause these are most stressing and also more 
relevant to real world scenarios than larger distances. The paths for the penetration vol­ume tests concentrate 
on light to medium penetrations of about 0 10% of the object s volume, because this best resembles the 
usage in haptic applications and physically-based simulations. In addi­tion, we included some heavy penetrations 
of 50% of the object s volume to stress our algorithm. We used several different objects to test the 
performance of our al­gorithms with a polygon count ranging up to 700k triangles per ob­ject in the armadillo 
scene (see Figure 7). We voxelized each object in different resolutions in order to evaluate the trade-off 
between the number of spheres and the accuracy. This trade-off is indepen­dent of the object s complexity 
but depends on the density of inner spheres.2 We used PQP to compute the exact distance and measure the 
quality of our distance approximation. The running time of PQP is not di­rectly comparable to our ISTs 
because of the more time consuming exact distance computation. Thus, we did not include the timings in 
our plots. Just to give the reader a sense of the speed-up: our ap­proximative approach is between 20 
120 times faster than the exact PQP, depending on the density of the inner spheres. To our knowledge, 
there are no publicly available implementations to compute the penetration volume ef.ciently. In order 
to evalu­ate the quality of our penetration volume approximation, we used a tetrahedralization of the 
objects. The non-overlapping tetrahe­dra .ll the objects without any gaps, and thus we can calculate 
the intersection volume exactly. Additionally, we build a hierar­chy on top of the tetrahedra in order 
to accelerate penetration vol­ume queries. However, the runtime of this approach is not appli­cable to 
real-time applications due to bad BV .tting and the costly tetrahedron-tetrahedron overlap volume calculation. 
It takes more than 2 sec/frame on average within all our scenarios. The results from our benchmark prove 
that the distance queries can be done at haptic rates even for very large objects with hundreds of thousands 
of polygons (see Figure 7). The approximation error is less than 1%. The accuracy can be further improved 
by the simple extension described in Section 4.1.2. With the highest sphere count, 2 Please visit cg.in.tu-clausthal.de/research/ist/ 
to watch some videos of our benchmarks. Force Torque 1.6 130 0.35 100 Magnitude Magnitude 1.4 Direction 
Direction 0.3 90 120 0.25 80 1 110 0.2 70 Force Direction in Deg  Torque Magnitude Torque Direction 
in Deg Force Magnitude 100 90 0.15 60 0.1 50 0.2 80 0.05 40 0 0 30 0 100 200 300 400 500 0 100 200 300 
400 500 Frame Frame Figure 6: Left: magnitude (red) and direction (blue) of the force arising between 
two copies of the object shown in the middle, one of which is being moved on a pre-recorded path. Right: 
torque magnitude (red) and direction (blue) in the same scene. the error is below .oating point accuracy 
with only a negligible longer running time (see Figure 8). Also, our penetration volume queries perform 
at haptic rates of at least 200 Hz on average (see Figure 9). Again, the error is consid­erably smaller 
than 1% when using an adequate amount of inner spheres. However, in the case of deeper penetrations, 
it is possible that the traversal algorithm may exceed its time budget for haptic rendering. In this 
case, our time-critical traversal guarantees ac­ceptable estimations of the penetration volume even in 
worst-case scenarios and multiple contacts (see Figure 10).  7 Conclusions and Future Work We have presented 
a novel hierarchical data structure, the inner sphere trees, that supports both proximity queries and 
penetration volume computations with one uni.ed algorithm. Both kinds can be answered at rates of about 
1 kHz (which makes the algorithm suitable for haptic rendering) even for very complex objects with several 
hundreds of thousands of polygons. For proximity situations, typical average runtimes are in the order 
of less than 0.5 msec with 500 000 spheres per object and an error of about 0.5%. Obviously, the running 
times depend much more on the intersection volume in penetration situations; here, we are in the order 
of around 5 msec on average with 250 000 spheres and an er­ror of about 0.7%. The balance between accuracy 
and speed can be speci.ed by the user, and it is independent of the object complexity, because the number 
of leaves of our hierarchy is mostly indepen­dent of the number of polygons. For time-critical applications, 
we described a variant of our algorithm that stays within a .xed time budget while returning an answer 
as good as possible . Our algorithm for both kinds of queries can be integrated into exist­ing simulation 
software very easily, because there is only a single entry point, i.e., the application does not need 
to know in advance whether or not a given pair of objects will be penetrating each other. Memory consumption 
of our inner sphere trees is similar to other sphere hierarchies, depending on the prede.ned accuracy 
(in our experiments, it was always in the order of a few MB). This is very modest compared to voxel-based 
approaches. Another big advantage of our penetration volume algorithm is that it yields a continuous 
measure for penetration and force direction as well as a stable heuristic for torque computations. Last 
but not least, inner sphere trees are perfectly suited for SIMD acceleration techniques and allow algorithms 
to make heavy use of temporal and spatial coherence. Our novel approach opens up several avenues for 
future work. First of all, we are currently working on replacing the intermediate dis­tance .eld by a 
Voronoi-based approach to generate better sphere packings. This is a challenging task, because several 
goals should be met: accuracy, query ef.ciency, and small build times. We are con.dent that this will 
also result in analytically predictable error bounds. Furthermore, a GPU implementation of the force 
computation should result in further speed-ups. This is possible, because the forces and torques depend 
only on computations on a set of inde­pendent pairs of spheres, which is (almost) trivially parallelizable. 
Another option could be the investigation of inner volumes other than spheres. This could improve the 
quality of the volume cover­ing, because spheres do not .t well into some objects, especially if they 
have many sharp corners or thin ridges. Until now, our approach is restricted to watertight objects. 
In the fu­ture, we plan to extend the ISTs so that they are also able to handle arbitrary objects, including 
thin sheets and open geometries. Fi­nally, a challenging task would be to extend our approach also to 
deformable objects. Acknowledgment This work was partially supported by DFG grant ZA292/1-1 and BMBF 
grant Avilus / 01 IM 08 001 U.  References AGARWAL, GUIBAS, NGUYEN, RUSSEL, AND ZHANG. 2004. Collision 
detection for deforming necklaces. CGTA: Compu­tational Geometry: Theory and Applications 28. BARBI C., 
J., AND JAMES, D. L. 2008. Six-dof haptic rendering of contact between geometrically complex reduced 
deformable models. IEEE Transactions on Haptics 1, 1, 39 52. BIRGIN, E. G., AND SOBRAL, F. N. C. 2008. 
Minimizing the object dimensions in circle and sphere packing problems. Com­puters &#38; OR 35, 7, 2357 
2375. CAMERON, S. 1997. Enhancing GJK: Computing minimum and penetration distances between convex polyhedra. 
In Proceedings of International Conference on Robotics and Automation, 3112 3117.  Figure 7: Left: snapshot 
of distance computation in the armadillo scene (700k triangles) Center: average and maximum query time; 
note the two different y axes: the left one is for the average running times, the right one for the maximum 
times. Right: relative error compared to exact distance. Figure 8: Improvement of the accuracy (IA, 
right group of measurement) described in Section 4.1.2. Left: average running time. Center: error compared 
to exact distance. Right: snapshot of the scenario. Figure 9: Right: snapshot from the screwdriver scene 
that we used for penetration volume computation (488k triangles). Center: average and maximum query time; 
note the two different y axes. Right: relative error compared to exact volume. FAURE, F., BARBIER, S., 
ALLARD, J., AND FALIPOU, F. 2008. Image-based collision detection and response between arbitrary volumetric 
objects. In ACM Siggraph/Eurographics Symposium on Computer Animation, SCA 2008, July, 2008. FISHER, 
S. M., AND LIN, M. C., 2001. Fast penetration depth estimation for elastic bodies using deformed distance 
.elds. G¨ARTNER, B. 1999. Fast and robust smallest enclosing balls. In ESA, Springer, J. Nesetril, Ed., 
vol. 1643 of Lecture Notes in Computer Science, 325 338. GILBERT, E. G., JOHNSON, D. W., AND KEERTHI, 
S. S. 1988. A fast procedure for computing the distance between complex objects in three-dimensional 
space. IEEE Journal of Robotics and Automation 4, 193 203. HAMMER, B., HASENFUSS, A., AND VILLMANN, T. 
2006. Mag­ni.cation control for batch neural gas. In ESANN, 7 12. HASEGAWA, S., AND SATO, M. 2004. Real-time 
rigid body simu­lation for haptic interactions based on contact volume of polyg­onal objects. Comput. 
Graph. Forum 23, 3, 529 538. HOFF, K. E., ZAFERAKIS, A., LIN, M., AND MANOCHA, D. 2002. Fast 3d geometric 
proximity queries between rigid &#38; de­formable models using graphics hardware acceleration. Tech. 
Rep. TR02-004, Department of Computer Science, University of North Carolina -Chapel Hill, Mar. 7. Fri, 
8 Mar 2002 20:06:33 GMT. HUBBARD, P. M. 1995. Collision detection for interactive graphics applications. 
IEEE Transactions on Visualization and Computer Graphics 1, 3 (Sept.), 218 230. JOHNSON,D.E., AND COHEN,E.1998.Aframeworkforef.cient 
minimum distance computations. In Proceedings of the IEEE International Conference on Robotics and Automation 
(ICRA­98), IEEE Computer Society, Piscataway, 3678 3684. JOHNSON, D. E., AND WILLEMSEN, P. 2003. Six 
degree-of­freedom haptic rendering of complex polygonal model. In HAP-TICS, IEEE Computer Society, 229 
235. KIM, Y., OTADUY, M., LIN, M., AND MANOCHA, D. 2002. Fast penetration depth computation for physically-based 
animation. In Proceedings of the 2002 ACM SIGGRAPH Symposium on Computer Animation (SCA-02), ACM Press, 
New York, S. N. Spencer, Ed., 23 32. KIM, OTADUY, LIN, AND MANOCHA. 2003. Fast penetration depth estimation 
using rasterization hardware and hierarchical re.nement (short). In COMPGEOM: Annual ACM Symposium on 
Computational Geometry. LARSEN, E., GOTTSCHALK, S., LIN, M., AND MANOCHA, D. 1999. Fast proximity queries 
with swept sphere volumes. In Technical Report TR99-018. MCNEELY, W. A., PUTERBAUGH, K. D., AND TROY, 
J. J. 1999. Six degrees-of-freedom haptic rendering using voxel sampling. In Siggraph 1999, Addison Wesley 
Longman, Los Angeles, A. Rockwood, Ed., Annual Conference Series, ACM Siggraph, 401 408. MENDOZA, C., 
AND O SULLIVAN, C. 2006. Interruptible colli­sion detection for deformable objects. Computers &#38; Graphics 
30, 3, 432 438. MORRIS, D. 2006. Algorithms and data structures for haptic ren­dering: Curve constraints, 
distance maps, and data logging. In Technical Report 2006-06. O BRIEN, J. F., AND HODGINS, J. K. 1999. 
Graphical modeling and animation of brittle fracture. In SIGGRAPH 99: Proceed­ings of the 26th annual 
conference on Computer graphics and interactive techniques, ACM Press/Addison-Wesley Publishing Co., 
New York, NY, USA, 137 146. QUINLAN, S. 1994. Ef.cient distance computation between non­convex objects. 
In In Proceedings of International Conference on Robotics and Automation, 3324 3329. REDON, S., AND LIN, 
M. C. 2006. A fast method for local pene­tration depth computation. Journal of Graphics Tools: JGT 11, 
2, 37 50. RENZ, M., PREUSCHE, C., PTKE, M., PETER KRIEGEL, H., AND HIRZINGER, G. 2001. Stable haptic 
interaction with virtual environments using an adapted voxmap-pointshell algorithm. In In Proc. Eurohaptics, 
149 154. SCHUERMANN, A. 2006. On packing spheres into containers (about Kepler s .nite sphere packing 
problem). In Documenta Mathematica, vol. 11, 393 406. TRENKEL, S., WELLER, R., AND ZACHMANN, G. 2007. 
A benchmarking suite for static collision detection algorithms. In International Conference in Central 
Europe on Computer Graphics, Visualization and Computer Vision (WSCG), Union Agency, Plzen, Czech Republic, 
V. Skala, Ed. VAN DEN BERGEN, G. 1999. A fast and robust GJK implementa­tion for collision detection 
of convex objects. Journal of Graph­ics Tools: JGT 4, 2, 7 25. WELLER, R., AND ZACHMANN, G. 2009. Inner 
sphere trees for proximity and penetration queries. In Robotics: Science and Systems (RSS). ZACHMANN, 
G. 1998. Rapid collision detection by dynamically aligned DOP-trees. In Proc. of IEEE Virtual Reality 
Annual In­ternational Symposium; VRAIS 98, 90 97. ZHANG, L., KIM, Y. J., VARADHAN, G., AND MANOCHA, D. 
2007. Generalized penetration depth computation. Computer-Aided Design 39, 8, 625 638.   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>BMBF</funding_agency>
			<grant_numbers>
				<grant_number>Avilus/01 IM 08 001 U</grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>DFG</funding_agency>
			<grant_numbers>
				<grant_number>ZA292/1-1</grant_number>
			</grant_numbers>
		</article_sponsors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1581098</article_id>
		<sort_key>250</sort_key>
		<display_label>Pages</display_label>
		<article_publication_date>08-04-2009</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Giving your self to the game]]></title>
		<subtitle><![CDATA[transferring a player's own movements to avatars using tangible interfaces]]></subtitle>
		<page_from>161</page_from>
		<page_to>168</page_to>
		<doi_number>10.1145/1581073.1581098</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1581098</url>
		<abstract>
			<par><![CDATA[<p>We investigate the cognitive connection players create between their own bodies and the virtual bodies of their game avatars through tangible interfaces. The work is driven by experimental results showing that execution, perception and imagination of movements share a common coding in the brain, which allows people to recognize their own movements better. Based on these results, we hypothesize that players would identify and coordinate better with characters that encode their own movements. We tested this hypothesis in a series of four studies (n=20) that tracked different levels of movement perception abstraction, from own body to that of an avatar's body controlled by the participant, to see in which situations people recognize their own movements. Results show that participants can recognize their movements even in abstracted and distorted presentations. This recognition of 'own' movements occurs even when people do not see themselves, but just see a puppet they controlled. We conclude that players - if equipped with the appropriate interfaces - can indeed project and decipher their own body movements in a game character.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[body memory]]></kw>
			<kw><![CDATA[common coding]]></kw>
			<kw><![CDATA[game avatar]]></kw>
			<kw><![CDATA[puppet]]></kw>
			<kw><![CDATA[tangible user interface]]></kw>
			<kw><![CDATA[video game]]></kw>
			<kw><![CDATA[virtual character]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.4</cat_node>
				<descriptor>Psychology</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>K.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010476.10011187</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010455.10010459</concept_id>
				<concept_desc>CCS->Applied computing->Law, social and behavioral sciences->Psychology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1570363</person_id>
				<author_profile_id><![CDATA[81324491962]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ali]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mazalek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology, Atlanta, GA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570364</person_id>
				<author_profile_id><![CDATA[81440594515]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sanjay]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chandrasekharan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Calgary, Calgary, Alberta, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570365</person_id>
				<author_profile_id><![CDATA[81318491041]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nitsche]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology, Atlanta, GA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570366</person_id>
				<author_profile_id><![CDATA[81440613711]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Welsh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Calgary, Calgary, Alberta, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570367</person_id>
				<author_profile_id><![CDATA[81440598789]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Geoff]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thomas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology, Atlanta, GA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570368</person_id>
				<author_profile_id><![CDATA[81440622201]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Tandav]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sanka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology, Atlanta, GA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1570369</person_id>
				<author_profile_id><![CDATA[81440596202]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Clifton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology, Atlanta, GA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>300536</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Balakrishnan R., Fitzmaurice G., Kurtenbach G., Singh K. 1999. Exploring interactive curve and surface manipulation using a bend and twist sensitive input strip. In <i>Proceedings of the ACM Symposium on Interactive 3D Graphics (I3DG '99)</i>, ACM Press, 111--118.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Barsalou, L. W. 1999. Perceptual symbol systems. <i>Behavioral and Brain Sciences, 22</i>, 577--660.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Beardsworth, T., Buckner, T. 1981. The ability to recognize oneself from a video recording of one's movements without seeing one's body. In <i>Bulletin of the Psychonomic Society</i>, 18 (1), 19--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Brass, M., Bekkering, H., Prinz, W. 2002. Movement observation affects movement execution in a simple response task. <i>Acta Psychologica</i>, 106, 3--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Brass, M., Heyes, C. 2005. Imitation: is cognitive neuroscience solving the correspondence problem? <i>Trends in Cognitive Sciences, 9</i>, 489--495.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Calvo-Merino, B., Glaser, D. E., Grezes, J., Passingham, R. E., Haggard, P. 2005. Action observation and acquired motor skills. <i>Cerebral Cortex</i>, 15, 1243--1249]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Chandrasekharan, S., Athreya, D., Srinivasan, N. 2006. Twists and Oliver Twists in mental rotation: complementary actions as orphan processes. In Ron Sun, ed., <i>Proceedings of the 28th Annual Conference of the Cognitive Science Society</i>, Vancouver, Sheridan, 1092--1097.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Cutting, J. E., Kozlowski, L. T. 1977. Recognizing friends by their walk: Gait perception without familiarity cues. <i>Bulletin of the Psychonomic Society</i>, 9, 353--356.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Decety, J. 2002. Is there such a thing as a functional equivalence between imagined, observed and executed actions. In A. N. Meltzoff &amp; W. Prinz, Eds., <i>The Imitative Mind: Development, Evolution and Brain Bases</i>. Cambridge University Press, 291--310.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Difede, J., Hoffman, H. 2002. Virtual reality exposure therapy for World Trade Center post-traumatic stress disorder: A case report. <i>CyberPsychology &amp; Behavior, 5</i>, 6, 529--535.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>199424</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Esposito, C., Paley, W. B. 1995. Of mice and monkeys: A specialized input device for virtual body animation. In <i>Proceedings of Symposium on Interactive 3D Graphics</i>, 109--213.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Fadiga, L., Fogassi, L., Pavesi, G., Rizzolatti, G. 1995. Motor facilitation during action observation: a magnetic stimulation study. <i>Journal of Neurophysiology</i>, 73, 2608--2611.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Fadiga, L., Craighero, L., Buccino, G. Rizzolatti, G. 2002. Speech listening specifically modulates the excitability of tongue muscles: a TMS study. <i>European Journal of Neuroscience</i>, 15, 399--402.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Gallese, V., Ferrari P. F., Kohler E., Fogassi L. 2002. The eyes, the hand and the mind: behavioral and neurophysiological aspects of social cognition. In: <i>The Cognitive Animal</i>. Bekoff, M., Allen, C., Burghardt, M. (Eds.), MIT Press, 451--462.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360626</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Hecker, C., Raabe, B., Enslow, R. W., DeWeese, J.; Maynard, J., van Prooijen, K. 2008. Real-time motion retargeting to highly varied user-created morphologies. <i>ACM Transaction on Graphics, 27</i>, 3, 27:1--27:11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Hegarty, M. 2004. Mechanical reasoning as mental simulation. <i>Trends in Cognitive Sciences</i>, 8, 280--285.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Hommel, B., M&amp;#252;sseler, J., Aschersleben, G., Prinz, W. 2001. The theory of event coding (TEC): A framework for perception and action planning. <i>Behavioral &amp; Brain Sciences</i>, 24, 910--937.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Hunt, D., Moore, J., West, A., Nitsche, M. 2006. Puppet Show: Intuitive puppet interfaces for expressive character control. In <i>Medi@terra 2006, Gaming Realities: A Challenge for Digital Culture</i>, Fournos Centre, Manthos Santorineos, Ed., 159--167.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Hurley, S., Chater, N. 2005. <i>Perspectives on imitation: From neuroscience to social science. Vol. 1: Imitation, human development and culture</i>, MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1207289</ref_obj_id>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Isbister, K. 2006. <i>Better game characters by design: A psychological approach</i>. Elsevier / Morgan Kaufmann.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258715</ref_obj_id>
				<ref_obj_pid>258549</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Ishii, H., Ullmer, B. 1997. Tangible bits: towards seamless interfaces between people, bits and atoms. In <i>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 1997)</i>, ACM Press, 234--241.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>142792</ref_obj_id>
				<ref_obj_pid>142750</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Jacob, R., Sibert, L. 1992. The perceptual structure of multidimensional input device selection. In <i>Proceedings of Human Factors in Computing Systems (CHI '92)</i>, ACM Press, 211--218.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Jeannerod, M. 1997. <i>The cognitive neuroscience of action</i>. Blackwell.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Jeannerod, M. 2006. From volition to agency: The mechanism of action recognition and its failures. In, <i>Disorders of Volition</i>, N. Sebanz &amp; W. Prinz, Eds., MIT Press, 175--192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Knoblich, G., Flach, R. 2001. Predicting the effects of actions: interactions of perception and action. <i>Psychological Science</i>, 12 6, 467--472.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Knoblich, G., Prinz, W. 2001. Recognition of self-generated actions from kinematic displays of drawing. <i>Journal of Experimental Psychology: human perception and performance</i>, 27, 2, 456--465.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Knoblich, G., Sebanz, N. 2006. The social nature of perception and action. <i>Psychological Science</i>, 15, 3, 99--104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Lombard, M., Ditton, T. 1997. At the heart of it all: The concept of telepresence. <i>Journal of Computer-Mediated Communication</i>, 3, 2, 1--39.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1255080</ref_obj_id>
				<ref_obj_pid>1255047</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Mazalek, A., Nitsche, M. 2007. Tangible interfaces for real-time 3D virtual environments. In <i>Proceedings of the international Conference on Advances in Computer Entertainment Technology</i>, ACM Press, 155--162.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Prinz, W. 2005. An ideomotor approach to imitation. In <i>Perspectives on imitation: From neuroscience to social science, Vol. 1</i>. S. Hurley, N. Chater, Eds., MIT Press, 141--156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Repp, B. H., Knoblich, G. 2004. Perceiving action identity: How pianists recognize their own performances. In <i>Psychological Science</i>, 15, 604--609]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Rothbaum, B., Anderson, P., Zimand, E., Hodges, L., Lang, D., Wilson, J. 2006. Virtual Reality exposure therapy and standard (in vivo) exposure therapy in the treatment of fear of flying. <i>Behavior Therapy</i>, 37, 1, 80--90.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1246846</ref_obj_id>
				<ref_obj_pid>1246838</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Slater, M. 1999. Measuring presence: A response to the Witmer and Singer questionnaire. <i>Presence: Teleoperators and Virtual Environments</i>, 8, 5, 560--566.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>523015</ref_obj_id>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Turkle, S. 1996. <i>Life on the screen. Identity in the age of the internet</i>. Weidenfeld &amp; Nicolson.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Ullmer B., Ishii, H. 2001. Emerging frameworks for tangible user interfaces. In <i>Human-Computer Interaction in the New Millennium</i>, John M. Carroll, Ed., Addison-Wesley, 579--601.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Vingerhoets, G., de Lange, F. P, Vandemaele, P, Deblaere, Achten, E. 2002. Motor imagery in mental rotation: an FMRI study. <i>Neuroimage</i>, 17, 1623--1633.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Virpet Theater project, Entertainment Technology Center (ETC), Carnegie Mellon University, Pittsburgh, PA, USA. http://www.etc.cmu.edu/projects/virpets/spring03/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Welsh, T. N., Elliott, D. 2004. Movement trajectories in the presence of a distracting stimulus: Evidence for a response activation model of selective reaching. <i>Quarterly Journal of Experimental Psychology-Section A</i>, 57, 1031--1057.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Wexler, M., Kosslyn S. M., Berthoz, A. (1998). Motor processes in mental rotation. <i>Cognition</i>, 68, 77--94.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1246762</ref_obj_id>
				<ref_obj_pid>1246761</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Witmer, B., Singer M. 1998. Measuring presence in virtual environments: A presence questionnaire. <i>Presence: Teleoperators and Virtual Environments</i>, 7, 3, 225--240.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Wohlschlager, A. 2001. Mental object rotation and the planning of hand movements. <i>Perception and Psychophysics</i>, 63 (4), 709--718.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Giving your self to the game: transferring a player s own movements to avatars using tangible interfaces 
 Ali Mazalek1, Sanjay Chandrasekharan2, Michael Nitsche1, Tim Welsh2, Geoff Thomas1, Tandav Sanka1, Paul 
Clifton1 Digital Media Program1 Georgia Institute of Technology Atlanta, GA, USA {mazalek, michael.nitsche, 
gpthomas, tandav, gtg747a}@gatech.edu ABSTRACT We investigate the cognitive connection players create 
between their own bodies and the virtual bodies of their game avatars through tangible interfaces. The 
work is driven by experimental results showing that execution, perception and imagination of movements 
share a common coding in the brain, which allows people to recognize their own movements better. Based 
on these results, we hypothesize that players would identify and coordinate better with characters that 
encode their own movements. We tested this hypothesis in a series of four studies (n=20) that tracked 
different levels of movement perception abstraction, from own body to that of an avatar s body controlled 
by the participant, to see in which situations people recognize their own movements. Results show that 
participants can recognize their movements even in abstracted and distorted presentations. This recognition 
of own movements occurs even when people do not see themselves, but just see a puppet they controlled. 
We conclude that players if equipped with the appropriate interfaces can indeed project and decipher 
their own body movements in a game character. Author Keywords Common coding, body memory, video game, 
virtual character, tangible user interface, game avatar, puppet. ACM Classification Keywords H.5.2 [Information 
Interfaces and Presentation]: User Interfaces---input devices and strategies, interaction styles; J.4 
[Social and Behavioral Sciences]: Psychology; J.5 [Arts and Humanities]: Performing arts. K.8 [Personal 
Computing]: Games. Copyright &#38;#169; 2009 by the Association for Computing Machinery, Inc. Permission 
to make digital or hard copies of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions 
from Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org. Sandbox 2009, New 
Orleans, Louisiana, August 4 6, 2009. &#38;#169; 2009 ACM 978-1-60558-514-7/09/0008 $10.00 Cognitive 
&#38; Motor Neuroscience Lab2 Faculty of Kinesiology University of Calgary Calgary, Alberta, Canada {schandra, 
twelsh}@ kin.ucalgary.ca  INTRODUCTION Players engage and identify often very intensely with the virtual 
game characters under their control. Virtual avatars can become important projection planes for a player 
s agency in the game world and are often seen as dramatic connections to a game world. In this work, 
we combine approaches from cognitive science, tangible interfaces, and virtual worlds to investigate 
this connection on the level of the body, movement, and comprehension of movement. A rapidly expanding 
research stream in cognitive science and neuroscience suggests that execution, perception and imagination 
of action share a common representation in the brain. Known as common coding theory, this work suggests 
that when humans perceive and imagine actions, our motor system is activated implicitly. A common instance 
of this simulation process is familiar to cinema goers: while watching an actor or car moving along a 
precipice, viewers move their arms and legs or displace body weight to one side or another, based on 
what they would like to see happening in the scene [Prinz 2005]. Anecdotal reports suggest similar effects 
are seen in sports fans and novice video game players. Such simulation of others actions underlie our 
ability to project ourselves into different character roles. Whether the actions are performed by an 
animated character in a virtual world or a human being in a film, we understand the actions of others 
through our own body memory reservoir, which is leveraged to predict actions and movements in the world. 
A central result of work in common coding is that the neural system underlying the simulation (the mirror 
neuron system) may be better activated when watching one s own actions. [Knoblich and Sebanz 2006] report 
that people can recognize their own clapping from a set of recordings of clapping, and pianists can pick 
out their own rendition of a piece from a set of recordings of the same piece. Applying this own-movement 
effect , we are seeking to build a video game that uses tangible interfaces to transfer a player s own 
movements to a virtual character. The motivation for this work is two-fold. One, the own-movement effect 
suggests that if characters encode a player s own movements, the player would both identify and coordinate 
better with the character. In a game setting this could trigger higher levels of engagement and better 
control. Second, it is possible, based on common coding theory, that novel movements executed by such 
a personalized character may be transferred back to the player via the perception-action link, thus improving 
a player s ability to execute such movements in imagination, and, perhaps, also in the real world (see 
also [Jeannerod 1997]). This might indicate that virtual characters can be valuable tools for teaching 
certain movements in fields such as physiotherapy. To encourage a relatively direct mapping of movements 
from the real to the virtual world, we are designing a tangible game interface in two phases. In the 
first phase, we record a person s body movements and test whether users can identify their own movements 
under two conditions, a perception-only condition (no feedback), and a control-and­perception condition 
(with feedback). In the first condition, players will see different simplified digital representations 
of their own movement, such as movements in silhouette, in a figure, in an animated character, in different 
proportions etc. In the second condition, players will control these representations using a tangible 
user interface such as a puppet. Instead of seeing their own body movements mapped into the game world, 
the puppet s moves will drive the game animations. In both conditions, we test the extent to which players 
can identify their own movements in the game character. Testing these two conditions would allow us to 
build up a base matrix of situations where players can identify their own movements in the character 
 the space of different representations, movements and perspectives under which self-identity is maintained. 
Once this matrix is developed and we show the connection between player and game character, the second 
phase will test one possible effect of such self-identification with a character: we will map a person 
s movement to a virtual character, and then examine whether interacting with such a personalized game 
character executing novel body movements improves a player s imagination of such movements. In the present 
paper, we report the results of a set of experiments conducted during the first phase of the project 
(building the matrix). The experiments demonstrate that users can recognize their own movements in simplified 
and abstracted representations. We first present an overview of results from common coding, virtual environments 
and tangible interfaces that drive our research. Then we describe the experimental design from the first 
phase of the project, and present the results. We conclude with future directions and implications of 
this work.  BACKGROUND The research proposed here builds on three separate fields: cognitive science, 
virtual environments, and tangible user interfaces. Common coding theory from cognitive sciences provides 
the theoretical and experimental basis for developing technological tools for enhancing human imagination 
and action. This cognitive model links perception, action and imagination, and can help us to better 
understand how to employ our body memories in developing novel computational media. To this end, tangible 
interfaces combined with virtual environments can provide a link between physical actions and the digital 
space. Video game spaces and real-time game engines provide the digital space into which a user can project 
their expressions and solutions, and tangible interfaces provide a physical form factor that naturally 
maps control onto a high level of granularity in action within the virtual world. In this section, we 
provide an overview of the state of the art in these three related areas of research and discuss the 
ways in which they drive and support the project.  The Common Coding Approach The common coding view 
argues for a shared representation in the brain that connects an organism s movement (motor activation), 
its observation of movements (perceptual activation), and imagination of movements (simulation). This 
common coding allows any one of these movements (perception, action, imagination) to generate the other 
two movements ([Prinz 2005]; also see [Decety 2002, Hommel et al. 2001]). The central insight emerging 
from the common coding approach is a body-based resonance the body acts similar to a tuning fork, replicating 
all movements it detects. To illustrate, going round and round can make you dizzy, but equally, watching 
something go round and round can also make you dizzy. This is because observing a movement leads to an 
implicit replication of the (spinning) movement by the body. The replication and simulation of the spinning 
movement in the observer then activates the perceptual effects of the action (dizziness) in the mind 
of the observer. However, all the replicated movements are not overtly executed. Most stay covert because 
the overt movement is inhibited. But such replication generates a representation of the movement in body 
coordinates, which plays a role in cognition and imagination. In this way, the common coding hypothesis 
can also explain the ability of two people to coordinate task performance (say in a multi-player game) 
because perceiving the other s actions activates one s own action system, leading to an intermingling 
of perception and action across players [Knoblich and Sebanz 2006]. Perception-Action common coding When 
participants execute an action A (say tapping fingers on a flat surface), while watching a non-congruent 
action on a screen (say another person moving in a direction perpendicular to the tapping), the speed 
of the performed action A slows down, compared to the condition when the participant is watching a congruent 
action on screen [Brass et al. 2002]. This is because the perceived opposite movement generates a motor 
response that interferes with the desired tapping pattern. A similar interference effect has been shown 
for competing movements within an individual movement trajectories of participants veer away or towards 
the location of a competing non-target object [Welsh and Elliott 2004]. Supporting many such behavioral 
results, neuro-imaging experiments show action areas are activated when participants passively watch 
actions on screen ([5] provides a review). Expert performers of a dance form (such as ballet and capoeira) 
when watching video clips of the dances in which they are experts, show strong activation in premotor, 
parietal and posterior STS regions, compared to when watching other dance forms. Non-dancer control participants 
do not show this effect [Calvo-Merino et al., 2005]. Similar motor activation has been shown for expert 
piano players watching piano playing [Repp and Knoblich, 2004]. When we observe goal-related behaviors 
executed by others (with mouth, hand, foot) the same cortical sectors are activated as when we perform 
the same actions [Gallese et al. 2002]. We do not overtly reproduce the observed action, but our motor 
system acts as if we are executing the observed action. The neuronal populations that support such action 
co­representation are termed mirror neurons (see [Hurley and Chater 2005] for a review). In contrast, 
motor areas are not activated when humans watch actions not part of our repertoire (such as barking). 
Perceiving an action also primes the neurons coding for the muscles that perform the same action [Fadiga 
et al. 1995, Fadiga et al. 2002]. Imagination-Action common coding Effects of this common coding have 
been found in multiple disciplines. When sharpshooters imagine shooting a gun, their entire body behaves 
as if they are actually shooting a gun [Barsalou 1999]. Similarly, imagining performing a movement helps 
athletes perform the actual movement better [Jaennerod 1997]. The time to mentally execute actions closely 
corresponds to the time it takes to actually perform them [Decety 2002, Jeannerod 2006], and responses 
beyond voluntary control (such as heart and respiration rate) are activated by imagining actions, to 
an extent proportional to the actual performance of the action. While imagining a mental rotation, if 
participants move their hands or feet in a direction that is not compatible to the mental rotation, their 
performance suffers [Wohlschlager 2001]. Also, planning another action can interfere with mental rotation 
[Wohlschlager 2001]. [Wexler et al. 2004] shows that unseen motor rotation during mental rotation leads 
to faster reaction times and fewer errors when the motor rotation is compatible with the mental rotation 
than when they are incompatible. In some cases motor rotation made complex mental rotations easier, and 
speeding/slowing the motor rotation speeded/slowed the mental rotation. Some complex mental rotations 
automatically generate involuntary hand movements [Chandrasekharan et al. 2006]. Links between imagination 
and action have also been found in mechanical reasoning, such as how people imagine the behavior of pulleys 
or gears. [Hegarty 2004]. Imaging experiments support these results, showing that premotor areas are 
activated while participants do mental rotation [Vingerhoets et al. 2002].  3D Game Worlds 3D spaces 
have become widely accessible and familiar to their player through countless video games. Players can 
navigate these worlds and perform specialized interactions in them, usually via an avatar as a projection 
plane and access point to the virtual world. In that way, virtual characters are focus points for the 
player s agency in the game world and expressive channels for their interactions. Player-Character relations 
Often highly individualized in appearance, specialized in their virtual abilities, and equipped with 
items gathered during long playing hours or extensive avatar customization before the game, virtual characters 
belong to their players. They can become manifestations of the player s individual play achievements 
and unique preferences. It is no wonder that players identify with their game avatars and create a personal 
connection to their characters [Turkle 1996; Isbister 2006]. A widespread paradigm is that of the player 
as actor with the avatar as a representation of the performance in the virtual world. Through customization 
and gradual mastering of the controls, players closely connect to their virtual alter egos to the point 
where players can feel situated in the virtual. The close mental connections between physical player 
body and virtual world have been utilized in numerous virtual training applications in the area of Serious 
Games. These range from treatment of the fear of flying [Rothbaum et al. 2006] to treatment of post-traumatic 
stress disorder in the wake of the 9/11 attacks [Difede and Hoffman 2002] to military combat simulations. 
However, the detailed mechanisms of how the projection from the player onto the avatar operates are not 
entirely clear. There are various suggestions to explain and measure player s presence (e.g. [Slater 
1999] vs. [Witmer and Singer 1998]) and models to define and track immersion (e.g. [Lombard and Ditton 
1997]) but the cognitive connection between player and virtual character remain obscure. While we know 
that this connection exists and is highly effective at times, we cannot precisely tell why or how it 
works. Our focus is specifically on the cognitive connection between the player and the avatar body. 
Within this area we are not interested in questions of appearance or customization of game characters, 
but concentrate on their movements. Movement expression The mapping of a player s ergodic participation 
onto the virtual character s in-world actions is often highly abstracted. A player might trigger a highly 
complex animation sequence through a single button press as animations and usually pre-recorded elements 
and defined by the game designer who maps them on the interaction design for the specific game title. 
These pre-defined sets of animations are by and large inaccessible to the average player. An avatar s 
movements, thus, are not unique but mostly pre-defined and largely repetitive. Engines can blend between 
different animations and create hierarchies between them, but even most advanced titles such as the Unreal 
3 engine still base animations on pre-captured motion data. At the same time, the flexibility and complexity 
increases: the number of bones and the animation details grow exponentially, procedural animation can 
be added [Hecker et al. 2008], and physics can be applied to the skeleton. The expressive quality of 
animation systems improves dramatically, but the conceptual underpinnings of the limited control mechanisms 
combined with largely pre-canned and inaccessible animations still dominate video games, blocking out 
more direct mirroring of players onto their virtual bodies. Thus, even as games become platforms for 
self-expression and socialization, featuring highly advanced animation and control technologies, they 
mostly follow outdated paradigms that prevent direct and creative control of the animation system.  
Tangible Interfaces When players move through a virtual environment, they use a control interface to 
project their intentions or expressions into the virtual space. With the exception of some new physical 
game interfaces like Nintendo's WiiRemote, most game systems use generic controllers for this purpose, 
such as keyboards, mice, joysticks and gamepads. These are generally two-axis pointing devices and button 
arrays that provide low-bandwidth single-channel data streams. Yet complex characters have many degrees 
of freedom, which cannot be easily controlled with input devices that provide at most two degrees of 
freedom. This requires a high level of abstraction between the control device and the virtual object. 
Jacob and Sibert describe this as a mismatch between the perceptual structure of the manipulator and 
manipulation task [Jacob and Sibert 1992]. They have demonstrated that for tasks that require manipulating 
several integrally related quantities (e.g. 3D position), a device that generates the same number of 
integrally related values as required by the task (e.g., Polhemus tracker) is better than a 2D positioning 
device (e.g., mouse). Since high level abstraction limits the players ability to precisely control their 
character across all its degrees of freedom, it also restricts their freedom to generate different movements 
and expressions in the virtual space. For example, if walking forward is controlled by the 'w' key, the 
player will not be able to easily access a range of walking expressions. Given the limited form factors 
of existing human-computer interfaces, designers and researchers are exploring new ways to integrate 
the physical and digital spaces. These efforts fall under emerging areas of digital interaction, such 
as tangible user interfaces (TUIs) or tangible interaction. TUIs aim to extend our means of digital input 
and output beyond a primarily audiovisual mode, to interactions that make better use of the skills that 
humans have with their hands and bodies [Ishii and Ulmer 1997, Ulmer and Ishii 2001]. The approach couples 
digital information with physical artifacts that act as both controls and representations for the underlying 
systems they embody. TUIs take advantage of our manual dexterity and capitalize on the well-understood 
affordances and metaphors of everyday physical objects. They can provide approaches for mapping player 
expressions into the virtual space in two ways. First, TUIs can provide a high level of granularity across 
many degrees of freedom in the physical world. Second, TUIs can be designed in a physical form that naturally 
maps the real onto the virtual. Related approaches are already used in professional production companies, 
which have increasingly turned to puppetry and body motion tracking to inject life into 3D character 
animation. Putting a performer in direct control of a character via puppetry, or capturing body motion 
for real­time or post-processed animated character control, helps translate the nuances of natural motion 
to virtual characters and increases their expressive potential. For example, The Character Shop's Waldo 
devices are telemetric input devices for controlling puppets (e.g. Jim Henson's Muppets) that are designed 
to fit a puppeteer's body. Waldos allow puppeteers to control multiple axes of movement on a virtual 
character at once, unlike older lever systems that required a team of operators to control different 
parts of a single puppet. A limitation of motion capture puppetry is that it typically requires significant 
clean-up of sensor data in post processing. The high price point also precludes its use in the consumer 
space for enhancing the expressive potential of everyday game players. In interaction research, a number 
of efforts have centered on new physical interfaces for character control and animation. For example, 
the Monkey Input Device is an 18" tall monkey skeleton with sensors at its joints, providing 32 degrees 
of freedom for real-time character manipulation [Esposito and Paley 1995]. Researchers have also used 
Measurand's ShapeTape, a fiber optic-based 3D bend and twist sensor, for direct manipulation of 3D curves 
and surfaces [Balakrishnan et al 1999]. Others have used puppeteering techniques with various input devices 
(joysticks, MIDI controllers) to manipulate 3D virtual characters in real-time [Virpet project]. Additionally, 
our own past research used paper hand puppets tracked by computer vision [Hunt et al. 2006] and tangible 
marionettes with accelerometers [Mazalek and Nitsche 2007] to control characters in the Unreal game engine. 
However, to our knowledge none of the work on tangible interfaces for virtual character control has applied 
common coding theory to enhance the user's identification with a virtual character. As such, our project 
provides a unique interdisciplinary approach towards the design of systems that can help to enhance the 
user's experience and abilities.  EXPERIMENTAL DESIGN The first stage of the project outlined here investigates 
the extent of the connection between the player s own movement and that of an abstracted virtual entity. 
We are interested in this connection because it creates a channel wherein players make a direct connection 
between their own physical movements and that of the virtual avatar. Our ultimate objective is to use 
this channel to transfer novel movements executed by the character on screen back to the player, via 
the common coding between perception of movements and imagination/execution of movements. This could 
be useful in training games involving cognitive processes linked to action and also in medical rehabilitation 
tasks, e.g. for patients with stroke or movement disorders. We conducted four experiments to assess 
the hypothesis that a person can identify her own movement even when the movement is visually abstracted. 
A series of studies of biological movement [Beardsworth and Buckner 1981, Cutting and Kozlowski 1977, 
Knoblich and Flach 2001, Knoblich and Prinz 2001], have shown that when a person sees a visually abstract 
representation of her movement, (something as simple as a light-point animation, see figure 2), one can 
recognize the image s movements as one s own. There were two types of experiments. The first type analyzed 
participants ability to recognize their body movement (study one and two); the second type analyzed participants 
ability to recognize the way they move a puppet (study three and four). These studies enable us to establish 
the spectrum of self-recognition. We were interested in discovering whether participants were able to 
recognize the movements they make while using a control interface (like a puppet). This can allow us 
to establish whether a user will perceive the movements of a virtual character controlled by a tangible 
user interface as their own movement. In turn, this determines whether it is possible to use an external 
interface (e.g. puppet rather than body motion capture) as the basis for extending a user's body memory. 
Each study asked a specific question: Study One: Can participants identify their own body movements when 
they are represented as a proportionately correct but visually abstracted movement? Study Two: Can participants 
identify their own body movements when they are represented as proportionately standardized (not in their 
own natural proportions) and visually abstracted movement? Study Three: Participants move a physical 
puppet; both, their own movements and the puppet s movements are captured. A visually simplified video 
of the person moving the puppet is played alongside videos of other participant s puppet movement. Can 
participants recognize their own movements relative to other participants movement? Study Four: Same 
as three, except that the participants see only the puppet s visually simplified movement, not their 
own actions involved in moving the puppet. Can they distinguish between puppets manipulated with their 
own movements and puppets manipulated by others? There were a total of twenty participants in this study: 
ten participants (5 male, 5 female) participated in the body movement experiments; and ten participants 
(5 male, 5 female) in the puppet movement experiments. None of them was an experienced puppeteer.  Recording 
and Recognition Sessions In each experiment, light-emitting diodes (LEDs) were attached to key points 
of articulation of the participant s body: head, torso and limbs. The participant s movements were recorded 
then by camera. This generated abstract images of body movement, where only the moving light points of 
the LEDs were visible (figure 2). In the first two studies (Body, Body Proportion), LEDs were attached 
to participants and they were asked to execute two actions: walk and jump. The walk was a natural walking 
style, and the jump was a moderate jump, straight up and down (figure 1a&#38;b). Body proportions of 
the participant were unaltered in study one. In study two, the video s body proportions were altered 
using post­production techniques to a standard body size. For each participant, 5 walk and 5 jump trials 
were captured, for Body and Body Proportion studies. Ten or more days after the recording session, participants 
returned for two blocks of recognition sessions (Body and Body Proportion). In each session, participants 
watched a series of trials, each with two clips of visually abstracted movement (figure 2a&#38;b). One 
clip showed the participant s own action (e.g., jump) and the other showed the same action performed 
by another participant. The participant was asked to identify which video displayed her own action. There 
were 70 trials each for Body and Body Proportion sessions. The two sessions were counterbalanced half 
the participants were shown the videos from Body first, followed by those from Body Proportion, whereas 
the other half were shown videos from Body Proportion first, followed by those from Body. For each video 
trial, the program picked a random video clip of the participant from a list, and another random video 
clip from a list of others making the same movement. The location on the screen where the video was presented 
(left, right) was also random. Participants were asked to press P if they thought their video clip was 
on the right, and Q if they thought it was on the left. The videos looped until the participant made 
a choice. The video presentation program kept track of the randomizations of files and locations, the 
key press responses of participants, and the time it took for a participant to respond. Studies Three 
and Four (Puppet &#38; Puppeteer, Puppet Only) followed the same design, except that the participant 
made movements with a puppet. Participants again had LEDs attached to their bodies at key points of articulation. 
They were given a puppet, also with LEDs attached (figure 1c&#38;d) and asked to manipulate the puppet 
so that it appeared to be walking or jumping. Cameras captured the movement of both the participant and 
the puppet. The participants then returned for a recognition session, where they tried to recognize their 
own movement, in two blocks (Puppet &#38; Puppeteer condition, Puppet Only condition). In the Puppet 
&#38; Puppeteer condition, the participants viewed side-by-side videos of self and others manipulating 
the puppet. They was asked to determine which clip represented their own puppet manipulations (figure 
2c&#38;d). In the Puppet Only condition, the participants viewed video clips of just the puppet. They 
was asked to determine which clip represented their manipulation of the puppet (figure 2e&#38;f). These 
two experiments had 60 trials each, and the conditions were counterbalanced, with half the participants 
viewing the Puppet &#38; Puppeteer condition first and the other half viewing the Puppet Only condition 
first.  RESULTS For each participant, we computed the proportion of correct self-identifications (figure 
3). Since the guessing probability is .5, values significantly greater than .5 indicate that participants 
recognized their own movement. Accuracy: Participants showed high levels of identification in all studies. 
All accuracy measures were significantly above chance level. The mean proportions of correct identifications 
are as follows: Body condition: 95.71 .2 (SD=3.49, = 296.51, p<.00001); Body Proportion condition: 94.71 
(SD=5.42, .2 = 283.22, p<.00001); Puppet &#38; Puppeteer condition: 84.33 (SD=18.36, .2 = 177.86, p<.00001); 
Puppet Only condition: 82 (SD=23.5, .2 = 182.73, p<.00001). The high standard deviations in the last 
two conditions are due to one participant performing very poorly, averaging 40 and 31.6 percent correct 
scores, and another participant scoring 100%. Reaction Times: We allowed participants to take their 
own time in responding, so there is wide variability within this data. However, a rough trend can be 
identified, where participants took more time in the Body Proportion condition than the Body one. In 
the Puppet experiment, the Puppet &#38; Puppeteer condition took more time than the Puppet Only condition. 
Gender: Previous experiments have shown that people can accurately recognize the gender of a pointwalker 
[Cutting and Kozlowski 1977]. So it is possible that in trials where the two videos showed participants 
with different gender, people made the recognition decision by recognizing the other person s gender, 
and then eliminating that video. To check whether this occurred, we analyzed the data based on the same/different 
gender in the video. The proportion of correct identifications for same gender trials and different gender 
trials were extracted for each condition, and compared using T-tests. No significant differences were 
found between the two cases, though there was a trend (P<.08) towards more accuracy for different gender 
judgments in the Body condition (see figure 4). The lack of significant difference between the two gender 
combinations, indicates that the self-identification was based on a simulation of the movements seen 
on video, rather than a logic-based elimination process.  DISCUSSION Overall, the results show a higher 
recognition rate of own body movements than of puppet movements (~95% vs. ~80%). However, we could not 
identify a dramatic decline in the level of recognition. There was no significant difference between 
the Body condition vs. the Body Proportion condition. Participants seemed to recognize their own movements, 
regardless of body proportion.  There is a larger gap between the body movement and the puppeteering 
studies, but since the self-recognition rates are still far higher than chance, we interpret this as 
part of an expected decline mainly due to unfamiliarity with the puppet and not as a principle loss of 
self-recognition. There is also no significant difference between the Puppet &#38; Puppeteer condition 
(study three) and the Puppet Only condition (study four). Participants were still able to identify their 
own movements in the Puppet Only (study four) condition. This was surprising, as none of the participants 
in the study had any puppeteering experience. It would be interesting to compare these results with professional 
puppet players as participants or do a long­term study of players using the interface. Overall, the results 
show an effective translation of self to the character, suggesting that we indeed project ourselves to 
the movements of characters whose movements derive in second order from our own body memory; probably 
through a common coding system. We believe these results could be exploited to develop new media and 
new interfaces. It opens up questions regarding our identification with virtual actors and the feedback 
loop that avatars can generate with our own body memory.  CONCLUSION The research illustrates our ongoing 
work at the interface between game worlds, new interfaces and common coding theory. Such a connection 
suggests new paradigms of character control and interface design. These can inform new game design approaches 
as well as invite a rethinking of the player-avatar relationship. For example, it might be highly relevant 
for Serious Games in the health sector. However, while the current experiments show that the underlying 
connection between own body memory and virtual character stay intact, they do not yet offer the necessary 
interfaces to control the character, nor do they clarify what kind of avatar representations work best. 
Our ongoing work maps a person s movement to a virtual character through a tangible interface that works 
like a digital puppetry controller. In our future work, we will examine whether perceiving a personalized 
video game character executing novel body movements can augment a player s body memory and teach a player 
in that way. ACKNOWLEDGEMENTS We thank the Synaesthetic Media Lab, the Digital World and Image Group 
and the Cognitive and Motor Neuroscience Lab for helping shape our ideas. This work is supported by funding 
from NSF-IIS grant #0757370 and the Alberta Ingenuity Fund.  REFERENCES BALAKRISHNAN R., FITZMAURICE 
G., KURTENBACH G., SINGH K. 1999. Exploring interactive curve and surface manipulation using a bend and 
twist sensitive input strip. In Proceedings of the ACM Symposium on Interactive 3D Graphics (I3DG '99), 
ACM Press, 111-118. BARSALOU, L. W. 1999. Perceptual symbol systems. Behavioral and Brain Sciences, 22, 
577-660. BEARDSWORTH, T., BUCKNER, T. 1981. The ability to recognize oneself from a video recording of 
one's movements without seeing one's body. In Bulletin of the Psychonomic Society, 18 (1), 19-22. BRASS, 
M., BEKKERING, H., PRINZ, W. 2002. Movement observation affects movement execution in a simple response 
task. Acta Psychologica, 106, 3-22. BRASS, M., HEYES, C. 2005. Imitation: is cognitive neuroscience solving 
the correspondence problem? Trends in Cognitive Sciences, 9, 489-495. CALVO-MERINO, B., GLASER, D.E., 
GREZES, J., PASSINGHAM, R.E., HAGGARD, P. 2005. Action observation and acquired motor skills. Cerebral 
Cortex, 15, 1243-1249 CHANDRASEKHARAN, S., ATHREYA, D., SRINIVASAN, N. 2006. Twists and Oliver Twists 
in mental rotation: complementary actions as orphan processes. In Ron Sun, ed., Proceedings of the 28th 
Annual Conference of the Cognitive Science Society, Vancouver, Sheridan, 1092-1097. CUTTING, J. E., KOZLOWSKI, 
L.T. 1977. Recognizing friends by their walk: Gait perception without familiarity cues. Bulletin of the 
Psychonomic Society, 9, 353-356. DECETY, J. 2002. Is there such a thing as a functional equivalence between 
imagined, observed and executed actions. In A.N. Meltzoff &#38; W. Prinz, Eds., The Imitative Mind: Development, 
Evolution and Brain Bases. Cambridge University Press, 291­ 310. DIFEDE, J.,HOFFMAN, H. 2002. Virtual 
reality exposure therapy for World Trade Center post-traumatic stress disorder: A case report. CyberPsychology 
&#38; Behavior, 5, 6, 529-535. ESPOSITO, C., PALEY, W.B. 1995. Of mice and monkeys: A specialized input 
device for virtual body animation. In Proceedings of Symposium on Interactive 3D Graphics, 109­ 213. 
FADIGA, L., FOGASSI, L., PAVESI, G., RIZZOLATTI, G. 1995. Motor facilitation during action observation: 
a magnetic stimulation study. Journal of Neurophysiology, 73, 2608 2611. FADIGA, L., CRAIGHERO, L., BUCCINO, 
G. RIZZOLATTI, G. 2002. Speech listening specifically modulates the excitability of tongue muscles: a 
TMS study. European Journal of Neuroscience, 15, 399-402. GALLESE, V., FERRARI P.F., KOHLER E., FOGASSI 
L. 2002. The eyes, the hand and the mind: behavioral and neurophysiological aspects of social cognition. 
In: The Cognitive Animal. Bekoff, M., Allen, C., Burghardt, M. (Eds.), MIT Press, 451-462. HECKER, C., 
RAABE, B., ENSLOW, R.W., DEWEESE, J.; MAYNARD, J., VAN PROOIJEN, K. 2008. Real-time motion retargeting 
to highly varied user-created morphologies. ACM Transaction on Graphics, 27, 3, 27:1-27:11. HEGARTY, 
M. 2004. Mechanical reasoning as mental simulation. Trends in Cognitive Sciences, 8, 280-285. HOMMEL, 
B., MÜSSELER, J., ASCHERSLEBEN, G., PRINZ, W. 2001. The theory of event coding (TEC): A framework for 
perception and action planning. Behavioral &#38; Brain Sciences, 24, 910-937. HUNT, D., MOORE, J., WEST, 
A., NITSCHE, M. 2006. Puppet Show: Intuitive puppet interfaces for expressive character control. In Medi@terra 
2006, Gaming Realities: A Challenge for Digital Culture, Fournos Centre, Manthos Santorineos, Ed., 159-167. 
HURLEY, S., CHATER, N. 2005. Perspectives on imitation: From neuroscience to social science. Vol. 1: 
Imitation, human development and culture, MIT Press. ISBISTER, K. 2006. Better game characters by design: 
A psychological approach. Elsevier / Morgan Kaufmann. ISHII, H., ULLMER, B. 1997. Tangible bits: towards 
seamless interfaces between people, bits and atoms. In Proceedings of the SIGCHI Conference on Human 
Factors in Computing Systems (CHI 1997), ACM Press, 234-241. JACOB, R., SIBERT, L. 1992. The perceptual 
structure of multidimensional input device selection. In Proceedings of Human Factors in Computing Systems 
(CHI '92), ACM Press, 211-218. JEANNEROD, M. 1997. The cognitive neuroscience of action. Blackwell. JEANNEROD, 
M. 2006. From volition to agency: The mechanism of action recognition and its failures. In, Disorders 
of Volition, N. Sebanz &#38; W. Prinz, Eds., MIT Press, 175-192. KNOBLICH, G., FLACH, R. 2001. Predicting 
the effects of actions: interactions of perception and action. Psychological Science, 12 6, 467-472. 
KNOBLICH, G., PRINZ, W. 2001. Recognition of self-generated actions from kinematic displays of drawing. 
Journal of Experimental Psychology: human perception and performance, 27, 2, 456-465. KNOBLICH, G., SEBANZ, 
N. 2006. The social nature of perception and action. Psychological Science, 15, 3, 99-104. LOMBARD, M., 
DITTON, T. 1997. At the heart of it all: The concept of telepresence. Journal of Computer-Mediated Communication, 
3, 2, 1-39. MAZALEK, A., NITSCHE, M. 2007. Tangible interfaces for real-time 3D virtual environments. 
In Proceedings of the international Conference on Advances in Computer Entertainment Technology, ACM 
Press, 155-162. PRINZ, W. 2005. An ideomotor approach to imitation. In Perspectives on imitation: From 
neuroscience to social science, Vol. 1. S. Hurley, N. Chater, Eds., MIT Press, 141-156. REPP, B.H., KNOBLICH, 
G. 2004. Perceiving action identity: How pianists recognize their own performances. In Psychological 
Science, 15, 604-609 ROTHBAUM, B., ANDERSON, P., ZIMAND, E., HODGES, L., LANG, D., WILSON, J. 2006. Virtual 
Reality exposure therapy and standard (in vivo) exposure therapy in the treatment of fear of flying. 
Behavior Therapy, 37, 1, 80-90. SLATER, M. 1999. Measuring presence: A response to the Witmer and Singer 
questionnaire. Presence: Teleoperators and Virtual Environments, 8, 5, 560-566. TURKLE, S. 1996. Life 
on the screen. Identity in the age of the internet. Weidenfeld &#38; Nicolson. ULLMER B., ISHII, H. 2001. 
Emerging frameworks for tangible user interfaces. In Human-Computer Interaction in the New Millennium, 
John M. Carroll, Ed., Addison-Wesley, 579-601. VINGERHOETS, G., DE LANGE, F.P, VANDEMAELE, P, DEBLAERE, 
ACHTEN, E. 2002. Motor imagery in mental rotation: an FMRI study. Neuroimage, 17, 1623-1633. VIRPET THEATER 
PROJECT, Entertainment Technology Center (ETC), Carnegie Mellon University, Pittsburgh, PA, USA. http://www.etc.cmu.edu/projects/virpets/spring03/. 
WELSH, T.N., ELLIOTT, D. 2004. Movement trajectories in the presence of a distracting stimulus: Evidence 
for a response activation model of selective reaching. Quarterly Journal of Experimental Psychology 
Section A, 57, 1031-1057. WEXLER, M., KOSSLYN S.M., BERTHOZ, A. (1998). Motor processes in mental rotation. 
Cognition, 68, 77-94. WITMER, B., SINGER M. 1998. Measuring presence in virtual environments: A presence 
questionnaire. Presence: Teleoperators and Virtual Environments, 7, 3, 225-240. WOHLSCHLAGER, A. 2001. 
Mental object rotation and the planning of hand movements. Perception and Psychophysics, 63 (4), 709-718. 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>NSF-IIS</funding_agency>
			<grant_numbers>
				<grant_number>0757370</grant_number>
			</grant_numbers>
		</article_sponsors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
</content>
</proceeding>
