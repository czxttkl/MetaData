<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>06/25/2012</start_date>
		<end_date>06/25/2012</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Venezia]]></city>
		<state></state>
		<country>Italy</country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES383</series_id>
		<series_title><![CDATA[International Conference on Supercomputing]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>2322156</proc_id>
	<acronym>FutureHPC '12</acronym>
	<proc_desc>Proceedings of the Future HPC Systems</proc_desc>
	<conference_number>2012</conference_number>
	<proc_class>workshop</proc_class>
	<proc_title></proc_title>
	<proc_subtitle>the Challenges of Power-Constrained Performance</proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-1-4503-1453-4</isbn13>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2012</copyright_year>
	<publication_date>06-25-2012</publication_date>
	<pages>31</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>In the past, the computational requirements of important scientific applications have often driven innovations in high performance computing. Presently major changes in computer architectures are taking place that are often driven by consumer electronics. In addition as systems continue to scale in size, we expect that power consumption will become a major concern for future generation supercomputers. Current systems consume more than a Megawatt per Petaflop. Reaching Exascale levels of computation - a 100x improvement in performance from today - will not be possible if power requirements continue to scale like today. The optimization of power and energy at all levels, from application to system software and to hardware at both processor and system scales is required.</p> <p>The challenges ahead are many-fold. Increasing parallelism, memory systems, interconnection networks, storage and uncertainties in programming models all add to the complexities. The recent trend with integrating accelerators in large-scale systems provides additional challenges in marshaling the increased parallelism and data movements. The interplay between performance, power, and reliability also leads to complex trade-offs.</p> <p>This workshop aims at reviewing the latest development of HPC systems, that in the near future may evolve in directions substantially different from today's paradigms. We are interested in assessing their potential impact on scientific computing.</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP917</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Architecture</sponsor_name>
			<sponsor_abbr>SIGARCH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>2012</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>2322157</article_id>
		<sort_key>10</sort_key>
		<display_label>Article No.</display_label>
		<pages>4</pages>
		<display_no>1</display_no>
		<article_publication_date>06-25-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[EURORA]]></title>
		<subtitle><![CDATA[a European architecture toward exascale]]></subtitle>
		<page_from>1</page_from>
		<page_to>4</page_to>
		<doi_number>10.1145/2322156.2322157</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2322157</url>
		<abstract>
			<par><![CDATA[<p>EURORA stands for EURopean many integrated cORe Architecture and it is a prototype architecture co-designed by CINECA and EUROTECH, and partially founded by PRACE 2IP project[1]. EURORA design target highly efficient cooling system (hot water) high density and sustainable TCO (Total Cost of Ownership) and, at the same time, allowing high throughput and high scalability for popular European scientific applications. EURORA will serve to evaluate system metrics related to the system efficiency such as PUE (Power Usage Effectiveness), Floating point operations per second per Watt (Flops/Watt), with the aim of envisage new, cost effective, power constrained, architectural solutions for the exaflops architectures of tomorrow. Moreover EURORA allow the porting and testing of an optimized subset of MPI library calls able to cope with both 3D torus interconnect primitives and MIC communication primitives.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[3D Torus topology]]></kw>
			<kw><![CDATA[Free cooling]]></kw>
			<kw><![CDATA[GROMACS]]></kw>
			<kw><![CDATA[MIC]]></kw>
			<kw><![CDATA[PUE]]></kw>
			<kw><![CDATA[accelerators]]></kw>
			<kw><![CDATA[energy efficiency]]></kw>
			<kw><![CDATA[hot water]]></kw>
			<kw><![CDATA[quantum ESPRESSO]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>C.5.1</cat_node>
				<descriptor>Super (very large) computers</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010520.10010521.10010537.10010541</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Distributed architectures->Grid computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010522.10010525</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Serial architectures->Superscalar architectures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10010362</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Massively parallel and high-performance simulations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010971.10011120.10010541</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Software system structures->Distributed systems organizing principles->Grid computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010528.10010536</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Parallel architectures->Multicore architectures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010537.10010541</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Distributed architectures->Grid computing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010522.10010525</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Serial architectures->Superscalar architectures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10010362</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Massively parallel and high-performance simulations</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010971.10011120.10010541</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Software system structures->Distributed systems organizing principles->Grid computing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010528.10010536</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Parallel architectures->Multicore architectures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Economics</gt>
			<gt>Experimentation</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P3665946</person_id>
				<author_profile_id><![CDATA[81448599429]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carlo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cavazzoni]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Casalecchio di Reno (Bo), Italy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[c.cavazzoni@cineca.it]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[http://www.prace-ri.eu]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[http://www.eurotech.com/en/hpc/hpc+solutions/data+center+hpc/Aurora+Systems.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[F.Pivanti "The AuroraScience Project", SC10 conference, New Orleans, LA USA, November 13--19, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[http://www.netlib.org/benchmark/hpl/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[GROMACS 4: Algorithms for Highly Efficient, Load-Balanced, and Scalable Molecular Simulation]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[P. Giannozzi, et al J.Phys.:Condens.Matter, 21, 395502 (2009) http://dx.doi.org/10.1088/0953-8984/21/39/395502.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[http://www.extoll.de/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[http://liinwww.ira.uka.de/~skampi/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[http://software.intel.com/en-us/articles/intel-mpibenchmarks/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[http://web.infn.it/aurorascience/images/aurora/docs/as4isc10.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 EURORA: a European architecture toward Exascale Carlo Cavazzoni Giampietro Tecchiolli Giovanni Erbacci 
CINECA EUROTECH CINECA via Magnanelli 3/6 via Fratelli Solari 3/a via Magnanelli 3/6 Casalecchio di Reno 
(Bo), Italy Amaro (Ud), Italy Casalecchio di Reno (Bo), Italy +39 0516171411 +39 0433485411 +39 0516171411 
c.cavazzoni@cineca.it giampietro.tecchiolli@eurotech. g.erbacci@cineca.it com ABSTRACT EURORA stands 
for EURopean many integrated cORe Architecture and it is a prototype architecture codesigned by CINECA 
and EUROTECH, and partially founded by PRACE 2IP project[1]. EURORA design target highly efficient cooling 
system (hot water) high density and sustainable TCO (Total Cost of Ownership) and, at the same time, 
allowing high throughput and high scalability for popular European scientific applications. EURORA will 
serve to evaluate system metrics related to the system efficiency such as PUE (Power Usage Effectiveness), 
Floating point operations per second per Watt (Flops/Watt), with the aim of envisage new, cost effective, 
power constrained, architectural solutions for the exaflops architectures of tomorrow. Moreover EURORA 
allow the porting and testing of an optimized subset of MPI library calls able to cope with both 3D torus 
interconnect primitives and MIC communication primitives. Categories and Subject Descriptors C.5.1 [Computer 
System Implementation]: Large and Medium ( Mainframe ) Computers Super (very large) computers. General 
Terms Design, Economics, Experimentation. Keywords Free cooling, hot water, 3D Torus topology, PUE, 
energy efficiency, accelerators, MIC, GROMACS, Quantum ESPRESSO. 1. INTRODUCTION Constraints in term 
of Flops/Watt, Flops/m2 and Flops/Dollar are becoming more and more relevant in guiding the design and 
selection process of supercomputers. At the same time the machine designers and the users would not like 
to see these constraints slowing down the rate of improvement of the supercomputer performances. In particular, 
concerning Italy (CINECA is the center designed to host the largest supercomputer for academic research) 
we already know that the budget, the floor space and the power consumption will remain nearly the same 
in the next years. To keep the pace with the performance Permission to make digital or hard copies of 
all or part of this work for personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies bear this notice and the full 
citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to 
lists, requires prior specific permission and/or a fee. Future HPC system'12, Jun 25 June 25 2012, Venice, 
Italy ACM 9781450314534/12/06. improvement of other infrastructures and remain competitive we start 
a codesign effort together with EUROTECH in order to optimize the architecture parameters and satisfy 
the constraints without giving up with the performance improvements. The EURORA prototype is results 
of this codesign effort, it is based on the AURORA[2] architecture by EUROTECH and represents a possible 
evolution that eventually may become commercially available in three years time. The EURORA prototype 
will be built using the same rack and the same cooling technology of AURORA, but the chassis, the cold 
plates, the node cards and the interconnect are being codesigned with CINECA specifically for the EURORA 
prototype. In particular in EURORA a new node card with a high speed low latency network is combined 
with two new Many Integrated Core (MIC) Intel processors (code named Knight Corner). The new components 
and the new design concepts of EURORA will be realized and integrated into a new architecture by EUROTECH. 
In this paper the architectural characteristics of EURORA will be presented and the differences with 
current AURORA architecture will be discussed. 2. AURORA ARCHITECTURE AURORA is the brand name of the 
current EUROTECH supercomputer solution[2]. The main characteristic of AURORA supercomputer are hot water 
cooling, small footprint, proprietary 3D torus network. AURORA removes component generated heat using 
hot liquid cooling (water up to 50 C), with no need for air conditioning, in any climate zone. This 
potentially allows reaching a data center PUE1 as low as 1.05. Each AURORA computational node can produce 
a temperature gap between 3 C and 5 C in the cooling liquid. Setting up more the racks in a multistage 
heating configuration, it is possible to warm up the coolant enough to be utilized for producing air 
conditioning, generating electricity or simply heating a building. Direct on component liquid cooling, 
a feature that limits on board hot spots. AURORA systems are among the best in class in terms of computing 
density per unit rack. AURORA mounting Sandy Bridge can have up to to 4096 cores/512 CPUs/256 blades 
hosted in a single 48U rack. In other words, this means over 66 Tflops per m 2, a reduced floor occupation 
and easier installation. 1 Power usage effectiveness (PUE) is the ratio of total amount of power used 
by a computer data center facility to the power delivered to computing equipment. An Infiniband switched 
network coexists with an optional FPGA driven 3D Torus nearest neighbor network based on AuroraScience[3]. 
Three independent synchronization networks (system, subdomain and local) preserve efficiency at Petascale 
by guaranteeing that the communication and the scheduling of all nodes are automatically handled. 3. 
EURORA ARCHITECTURE 3.1 Main Characteristics The basic building block of EURORA, like AURORA, is the 
water cooled chassis. Each chassis can host 16 node cards or, alternatively, 8 node cards plus 16 expansion 
cards (two expansion cards per node, see Figure 1), and it has its own water inlet and outlet collectors. 
A full rack can host up to 16 chassis. The EURORA chassis and corresponding cold plates will be designed 
to host 8 node cards and 16 MIC Intel expansion cards, for a maximum rack configuration of 128 nodes 
and 256 MIC cards (note that within a rack, it is possible to mix chassis with expansion cards and chassis 
with processor cards only). The EURORA prototype will be assembled and installed at CINECA. It will consist 
of a half rack, with 128 Intel SandyBridge processors (1024 cores) and 128 Intel MIC processors (7680 
cores, estimated value), an Infiniband FDR interconnect for data and storage, a high speed FPGA based 
3DTorus interconnect for data, and a low latency network for sync signals (based on FPGA too). The overall 
peak performance of the EURORA prototype will be of the order of 180TFlops for less than 50 KWatt (estimated 
from NDA communication). A half rack system with 64 hybrid nodes forms a 4x4x4 3DTorus topology which 
is a reasonable size for studying the network properties (six hops to get from one node to the furthest 
node) and to project the results to estimate the behavior for larger configurations. The prototype system 
will be primarily intended to test different kinds of high speed interconnect (the FPGA based 3D Torus, 
and Infiniband), MIC processors, and the energy efficiency of the system. For performance evaluation 
and efficiency measurements, HPL[4] and two popular European scientific applications (GROMACS[5] and 
QuantumESPRESSO[6]) will be used. 3.2 EURORA Chassis Each Eurora Chassis has the 48 Vdc power supplied 
directly to the computing nodes. A Root Card, integrated in the chassis controls the 8 nodes, contains 
a 36 port Infiniband switch, provides backplane level support and monitoring. The computing nodes are 
connected over the backplane to this Infiniband switch forming the lowest level of the Infiniband switching 
architecture. The chassis, equipped with nodes and Root Card is the smallest EURORA module that will 
be eventually commercially available. The chassis, as a module, is standalone and can be provided with 
a touchscreen for operative control. In each chassis both the computing node including the accelerator 
modules and the Root Card are liquid cooled by aluminium cold plates. 16 chassis can be hosted in an 
EURORA rack and multiple racks scale up to a petascale system. Difference in the chassis design with 
respect to AURORA are minimal. 3.3 Node Design As anticipated above each node includes a node card and 
two MIC expansion cards. The node card (see Figure 2) that will be used in EURORA is designed to host 
two Intel Xeon E5 series processors, 16GByte of ECC DDR3 1600MHz, a 160GByte SSD non volatile memory, 
a ALTERA Stratix V SGXB6 FPGA chip, 2 PCIx3 bus, a 56 Gbps FDR Infiniband (Mellanox ConnectX3) port and 
a Patsburg chip. Each expansion card can be configured to host an Intel MIC processor or, alternatively, 
an nVidia Kepler card. Unfortunately at the time of writing the details of the Intel MIC (code named 
Knight Corner) chip have not yet been disclosed. We estimate that MIC processor will consist of 60 cores 
with up to 16 GB GDR5, and will deliver up to 1.4 TFlops double precision peak performance. The PCI interface 
integrated with the MIC processor will be PCIe2 x16 interface. With respect to AURORA, the main differences 
in the node design are the integrations of Intel MIC expansion cards, the new layout, new Infiniband 
FDR port and new FPGA based Network Interface Controller (NIC) processor. The presence of the expansion 
cards has also implied a new design of the aluminum cold plate. Figure 2 Logical layout of the EURORA 
node card. SDBEP blocks represents the Intel Xeon E5 processor. The NIC is implemented using new Altera 
Stratix V FPGA Chip. 3.4 Network Characteristics EURORA networking includes two high bandwidth, low 
latency networks, an Infiniband FDR and a proprietary 3D torus network. Both networks are available for 
user parallel applications and in particular to implement MPI communications. The Infiniband FDR network 
has a standard switched topology and is implemented using 36 ports switches integrated with the chassis. 
16 ports of each switch are available to connect node cards within the chassis, and 20 ports are available 
for external (outside chassis) connectivity through QSFP connectors. Each rack can be connected with 
a maximum of 16*20 QSFP cables of arbitrary length. In AURORA the Infiniband network has the same implementation 
but uses QDR Mellanox ConnectX2 chips. The 3D Torus network has been designed adhoc for EURORA, it is 
based on a NIC implemented using an ALTERA Stratix V FPGA chip. The FPGA will be programmed using EXTOLL[7] 
VHDL code. In AURORA the 3D Torus network is implemented using ALTERA Stratix IV FPGA and AuroraScience 
VHDL code. With the new chip and the new VHDL code the 3D Torus network bandwidth will increase from 
120Gbit/s to 480Gbit/s per node card (6 links, each carrying 80Gbit/s). The latency of the new 3D Torus 
network will be in the order of one microsecond. 4 cables per chassis are required to connect the neighboring 
chassis and two cables per rack are required to connect neighboring racks. From a programming standpoint, 
the 3D Torus network supports an optimized subset of the MPI library; the possibility of partitioning 
the system into subdomains permits to create system partitions that communicate on independent tori, 
effectively creating independent execution domains. In the EURORA design the Infiniband network is mainly 
used for I/O traffic (Lustre filesystem) and to connect EURORA to the outside SAN, whereas the 3D Torus 
network is dedicated to the MPI traffic (parallel user applications). EURORA networking includes three 
different synchronization networks, allowing implementation of efficient timing alignment among different 
processes, resulting in higher computation efficiency (e.g. applicationlevel context switching) and 
reducing OS jitter. Also, efficient debugging of applications and network primitives is made possible, 
thanks to fast exception and trap propagation mechanisms, running on dedicated network and hardware. 
Implementation of machine level synchronization objects such as global clock, barriers, semaphores is 
also possible. These features are very useful to deliver consistent performance and scalability in large 
systems. The EURORA synchronization networks are characterized by a very low propagation delay and very 
high expandability. There is also an extra synchronization link between nodes along the same lines as 
the 3DTorus connection to be used by the communication library. Within each rack, one of the Root Cards 
is assigned the super root role, guaranteeing the distribution of signals within the rack in a tree topology. 
The tree topology ensures that the same delay applies to all chassis in each rack. Finally EURORA networking 
includes a Gbit Ethernet, a monitoring (connecting temperature and power consumption sensors) and a management 
network.  4. ENERGY EFFICIENCY ASPECT EURORA cooling infrastructure is (a part the new cold plate for 
the node cards designed to cope with MIC accelerators) is the same of the AURORA one. EURORA has no fans, 
all the heat is removed using liquid cooling. Each EURORA rack has two 2 pipes (actually, four couples 
per rack), one for inlet, one for outlet of cooling liquid. Inside each rack, the liquid is distributed 
to the different cold plates, utilized to directly cool the components. The cold plates are fitted to 
the distribution channels via quick disconnect hydraulic connectors The coolant is collected from the 
cold plates into the outlet pipes. The inlet and outlet pipes are connected to the liquid circuit that 
brings the water to the heat exchanger. Inlet coolant temperature can vary in a wide range, with the 
bottom value being dictated by ambient humidity and condensation prevention; the upper bound is set by 
practical considerations such as preventing the metalwork from becoming too hot to touch. Typical inlet 
coolant temperature can range from 18C to 50C. The temperature gap between inlet and outlet water flow 
in each EURORA rack is between 3C and 5C. The use of hot water (50C) as coolant allow the use of drycoolers, 
cooling the system to the outside air, without the use of chillers, and reducing the PUE. If by means 
of free cooling and efficient power supply technology (48Vdc) the EURORA prototype will displays a PUE 
in the range of 1.10, as should be possible by design, it will demonstrate the possibility of having 
a high energy efficiency, high density, hybrid system installed in warm regions of south Europe. To reach 
the maximum efficiency the system has not to be collocated in the same machine room with air cooled systems, 
otherwise, part of the heat will be removed by air and not by water (increasing the PUE). 5. EVALUATION 
METHODS GROMACS and Quantum ESPRESSO will be ported on MIC accelerators and will be used to evaluate 
performance, scalability and energy efficiency. Both codes present computational kernels (like 3D parallel 
FFT, and parallel linear algebra) that rely heavily on the performance of MPI, and are quite sensitive 
to OS jitters. In particular we will compare Energy Consumption/Application Throughput2 ratio with other 
energy efficient solutions such as IBM BG/Q. To measure energy efficiency of the system the customized 
sensors network will be used to record instantaneous power consumption. Another important goal will be 
the evaluation of the new 3DTorus interconnect and the possibility to be used for MICtoMIC communications. 
This will be done extending the MPI implemented for the 3D Torus network3 to include the Intel SCIF4 
primitives. Then this implementation of MICtoMIC communications, will be compared with standard IntelMPI 
implementation over Infiniband (already provided with Intel Software Development Toolkit for MIC processor). 
The evaluation of the MICtoMIC communication will be done measuring the bandwidth and the latency as 
compared to those obtained using the host processors alone. In particular, for this 2 An optimized subset 
of MPI has been ported by AuroraScience collaboration[10]. 3 Application throughput will be expressed 
in Step or Iteration per seconds for a given benchmark dataset. 4 Symmetric Communication InterFace driver 
layer (SCIF). A driver level interface that provides low level, efficient, asynchronous communication 
of data to and from Intel Many Integrated Core cards, such as Knights Corner. SCIF exposes a distributed 
communication software interface very similar to sockets programming. The same programmer interface is 
exposed whether the implementation is running on a Xeon host or a MIC card, therefore making it 'symmetric' 
as far as functionality and code development is concerned. This enables other communications layers such 
as TCP/IP, OFED and standard sockets to be more easily built upon SCIF. purpose, standard benchmarks 
like scampi[8] or Intel MPI benchmark[9] will be used.  6. CONCLUSION The EURORA codesigned architecture 
allows the production of a prototype that will serve to validate the new design concepts optimized for 
power constrained performances. Moreover the codesign research effort has proved that the collaboration 
between supercomputing center and vendor is a key factor to develop and promote an European HPC platform. 
EURORA will also give the opportunity to port popular European applications and experiment new programming 
models that are extremely interesting in the perspective of an exascale systems. As result of the collaboration 
and the codesign activity with EUROTECH an Italian exascale lab has been setup involving CINECA, EUROTECH 
and a network of main Italian institutions interested in keeping the pace with the exascale roadmap 
to enable new science. 7. ACKNOWLEDGMENTS We acknowledge PRACE and EC commission (Grant agreement number: 
RI283493), for partially funding EURORA prototype. 8. REFERENCES [1] http://www.praceri.eu [2] http://www.eurotech.com/en/hpc/hpc+solutions/data+center+ 
hpc/Aurora+Systems. [3] F.Pivanti The AuroraScience Project , SC10 conference, New Orleans, LA USA, November 
1319, 2010. [4] http://www.netlib.org/benchmark/hpl/. [5] GROMACS 4: Algorithms for Highly Efficient, 
LoadBalanced, and Scalable Molecular Simulation [6] P. Giannozzi, et al J.Phys.:Condens.Matter, 21, 
395502 (2009) http://dx.doi.org/10.1088/09538984/21/39/395502. [7] http://www.extoll.de/. [8] http://liinwww.ira.uka.de/~skampi/ 
[9] http://software.intel.com/enus/articles/intelmpibenchmarks/ [10] http://web.infn.it/aurorascience/images/aurora/docs/as4isc10. 
pdf  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>PRACE and EC commission</funding_agency>
			<grant_numbers>
				<grant_number>RI-283493</grant_number>
			</grant_numbers>
		</article_sponsors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Author</copyright_holder_name>
				<copyright_holder_year>2012</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2322158</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<pages>11</pages>
		<display_no>2</display_no>
		<article_publication_date>06-25-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Janus2]]></title>
		<subtitle><![CDATA[an FPGA-based supercomputer for spin glass simulations]]></subtitle>
		<page_from>1</page_from>
		<page_to>11</page_to>
		<doi_number>10.1145/2322156.2322158</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2322158</url>
		<abstract>
			<par><![CDATA[<p>We describe the past and future of the Janus project. The collaboration started in 2006 and deployed in early 2008 the Janus supercomputer, a facility that allowed to speed-up Monte Carlo Simulations of a class of model glassy systems and provided unprecedented results for some paradigms in Statistical Mechanics. The Janus Supercomputer was based on state-of-the-art FPGA technology, and provided almost two order of magnitude improvement in terms of cost/performance and power/performance ratios. More than four years later, commercial facilities are closing-up in terms of performance, but FPGA technology has largely improved. A new generation supercomputer, Janus2, will be able to improve by more than one orders of magnitude with respect to the previous one, and will accordingly be again the best choice in Monte Carlo simulations of Spin Glasses for several years to come with respect to commercial solutions.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[FPGA]]></kw>
			<kw><![CDATA[large scale simulations]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>J.2</cat_node>
				<descriptor>Physics</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010432.10010441</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Physics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010441</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Physics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Experimentation</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P3665947</person_id>
				<author_profile_id><![CDATA[81503681324]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[The Janus Collaboration]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M. M&#233;zard, G. Parisi and M.A. Virasoro, Spin Glass Theory and Beyond (World Scientific, Singapore, 1987); A. P. Young (editor), Spin Glasses and Random Fields, (World Scientific, Singapore, 1998).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[The Janus Collaboration, Phys. Rev. Lett. 101, (2008) 157201]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[The Janus Collaboration, Comp. Phys. Comm. 178, (2008) 208-216; IANUS: Scientific Computing on an FPGA-based Architecture, in Proceedings of ParCo2007, Parallel Computing: Architectures, Algorithms and Applications (NIC Series Vol. 38, 2007) 553-560; Computing in Science & Engineering 8, (2006) 41-49; Computing in Science & Engineering 11, (2009) 48-58.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[S. F. Edwards and P. W. Anderson, J. Phys. F: Metal Phys. 5, (1975) 965-974; ibid. 6, (1976) 1927-1937.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[A.D. Sokal, Functional Integration: Basics and Applications (1996 Cargese School) ed. C. DeWitt-Morette, P. Cartier and A. Folacci (1997 New York: Plenum)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[G. Parisi and F. Rapuano, Phys. Lett. B 157, (1985) 301-302.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[The Janus Collaboration: J. Stat. Phys. 135, 1121-1158 (2009); Phys. Rev. B 79, 184408 (2009); J. Stat. Mech. (2010) P05002; J. Stat. Mech. (2010) P06026; Phys. Rev. Lett. 105, 177202 (2010); Phys. Rev. B 84, 174209 (2011); Proc. Natl. Acad. Sci. USA (2012) 109, 6452-6456]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Hukushima, K., Nemoto, K. Exchange Monte Carlo Method and Application to Spin Glass Simulations J Physical Soc Japan 65,1604--1608 (1995), arXiv:cond-mat/9512035, E. Marinari, Optimized Monte Carlo Methods in Advances in Computer Simulation. Lecture Notes in Physics, 1998, Volume 501/1998, 50-81 (1996), arXiv:cond-mat/9612010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2186987</ref_obj_id>
				<ref_obj_pid>2186958</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[M. Guidetti et al., Spin Glass Monte Carlo Simulations on the Cell Broadband Engine in Proc. of PPAM09, (Lecture Notes on Computer Science (LNCS) 6067, Springer 2010) 467-476. M. Guidetti et al., Monte Carlo Simulations of Spin Systems on Multi-core Processors (Lecture Notes on Computer Science (LNCS) 7133 K. Jonasson (ed.), Springer, Heidelberg 2010) 220-230.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Janus2: an FPGA-based Supercomputer for Spin Glass Simulations * The Janus Collaboration ABSTRACT 
We describe the past and future of the Janus project. The collaboration started in 2006 and deployed 
in early 2008 the Janus supercomputer, a facility that allowed to speed-up Monte Carlo Simulations of 
a class of model glassy systems and provided unprecedented results for some paradigms in Statistical 
Mechanics. The Janus Supercomputer was based on state-of-the-art FPGA technology, and provided almost 
two order of magnitude improvement in terms of cost/performance and power/performance ratios. More than 
four years later, commercial facilities are closing-up in terms of performance, but FPGA technology 
has largely improved. A new generation supercomputer, Janus2, will be able to improve by more than one 
orders of magnitude with respect to the previous one, and will accordingly be again the best choice in 
Monte Carlo simulations of Spin Glasses for several years to come with respect to commercial solutions. 
 Categories and Subject Descriptors J.2 [Physical Sciences and Engineering]: Physics  General Terms 
Algorithms, Performance, Experimentation  Keywords FPGA, Large Scale Simulations 1. INTRODUCTION The 
Janus supercomputer [3] was designed and assembled .ve years ago as a big facility for running simulations 
of lattice systems with discrete variables. The application around which we took most of the architectural 
design decisions was the Monte Carlo simulations of glassy systems * See the Additional Authors section 
for a detailed collaboration member list Permission to make digital or hard copies of all or part of 
this work for personal or classroom use is granted without fee provided that copies are not made or distributed 
for pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst 
page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior 
speci.c permission and/or a fee. ICS2012 Workshop Future HPC systems: the Challenges of Power-Constrained 
Performance Venice, Italy, 2012 Copyright 2012 ACM 978-1-4503-1453-4/12/06 ...$15.00. de.ned on regular 
lattices. Spin glasses [1] are a wide category of prototypical glassy systems. Spin variables, taking 
a small set of discrete values (e.g. two states, up and down) sit at the nodes of a regular D-dimensional 
lattice, and tend to take the same value of their neighbors if the coupling de.ned on the edge connecting 
the corresponding nodes are positive (and to misalign if the coupling is negative). Coupling on nodes 
are .xed during the simulation and taken to be positive or negative at random throughout the lattice 
(quenched disorder con.guration). The deceivingly simple rules (we will brie.y describe them later), 
governing the evolution of a single node, re.ects in a very complex collective dynamics, especially 
when one deals with very large lattice (order 106 nodes), as required to compute results that can be 
compared with experiments on glassy materials. One striking property of glassy materials shared by the 
systems by means of which we model them is their extremely slow relaxation dynamics. At low temperature 
(below the critical temperature) glasses remains out of equilibrium at all relevant experimental time 
scales. This translates into (relatively) long-lasting experiments, to which we must compare numerical 
results. One usually relates the basic step in the simulated dynamics, the Monte Carlo step (the trial 
of one spin-reversal on all the nodes of the lattice), to the average spin-.ip time in real samples, 
which is estimated to be order 1 ps. Some real experiments on materials span a time scale of order 1 
second and more, corresponding to 1012 complete lattice updates in the simulated dynamics. If one has 
to get in touch with relevant length scales (besides the time scale mentioned), the lattice size cannot 
be too small (actually, a practical recipe to tailor lattice sizes as a function of the programmed simulation 
time, in order to correctly reproduce the out-ofequilibrium behavior, has been one of the main results 
of our investigation [2]). For a three-dimensional cubic lattice, a number of sites of 1003 is su.cient 
to study the system up to experimentally relevant time scales. The simulation program then consists then 
in some 1018 spin-.ip trials. In addition, simulation must be repeated on several copies of the system, 
to cope with .nite size e.ects; in o.-equilibrium computations, one usually deals with about 100 di.erent 
independent samples to minimize the e.ect of one particular disorder con.guration, amplifying the computational 
burden to 1020 spin-.ip trials. However, this is not the case for equilibrium simulations, in which 
one deals with thousands of copies of smaller systems; however the computational effort is of the same 
order of magnitude in both cases. At the time the project started, available commercial CPU technology 
made it possible to develop simulation codes that partially exploited the (in principle very large) available 
parallelism; a large optimization e.ort involving simultaneous handling of di.erent sites, wise memory 
organization (multispin coding), vectorization (SSE instructions, two-or quadcore parallel implementation) 
resulted in a spin-.ip time of order 1 ns. A cluster of order 100 CPUs would then need some 109 seconds 
(that is, more than 30 years) to complete the simulation campaign described above. The advent of Janus 
marked a substantial progress: Janus deploys 256 processors built with state-of-the-art FPGAs. The simple 
dynamical rules governing a single spin variable can be easily implemented as a small block of logical 
rules; a single FPGA hosts up to 1000 single-spin-.ip engines. With a conservative clock frequency of 
62.5 MHz, one Janus node has a 16 ps single spin-.ip time. A simulation program as the one outlined above 
can be completed in just a few months, making it a viable option. In more recent years, progress in computer 
architectures pushed towards increasing parallelism; on one side, we have many-core processors with intrinsics 
on sets of wider registers; also, GPU technology provides processors with thousands of streaming processors. 
Careful optimization of our applications for these architectures increases performance by one order of 
magnitude with respect to standard architectures, coming close to Janus performances, but still not 
making it possible for e.g. a graduate student to complete a large simulation campaign during its PhD 
studies. On the other side, a performance increase that is compatible with Moore s law is inevitably 
going to .nally close-up and compete with Janus on spin glass applications. Besides, FPGA technology 
improved too, and larger and faster programmable devices will be shortly available on the market; we 
foresee that a new generation FPGA-based supercomputer, named Janus2, will be able to outperform its 
ancestor by at least two order of magnitude, guaranteeing for itself a long lifetime in terms of absolute 
performance and cost/performance and power/performance ratios in the simulation of spin glass systems. 
In what follow, we brie.y describe a typical spin glass model and the structure of a typical Monte Carlo 
simulation and discuss why its implementation is more e.ective on FPGAs than on conventional CPUs (and 
GPUs). We then describe the hardware and software architecture of Janus, and give performance an power 
consumption comparison .gures with respect to other systems available at the time the project started 
and available today. We .nally describe the new Janus2 supercomputer, that is at an early architectural 
design stage, and discuss expected performance and expected its expected lifetime as a top speed Carlo 
engine for spin glasses. 2. A SAMPLE SPIN GLASS APPLICATION We start by introducing a sample application 
and describe a typical computational kernel, discussing its features and implications for an FPGA and 
CPU based implementation. The three-dimensional Edwards-Anderson model [4] is easily described in terms 
of the energy of the system: E = -siJij sj ; (1) (ij) L3 where si are spin variables (modeling magnetic 
moments at atomic lattice sites) taking values +1 (spin up) and -1 (spin down), sitting at the nodes 
of a three-dimensional cubic lattice of linear size L. Jij are the strengths of the interaction (couplings) 
along the edges connecting nearestneighbor nodes; a positive Jij favors alignment of the spins of the 
two neighboring nodes; a negative value favors misalignment. The sum in (1) spans all pairs of nearest 
neighbors. The values in the set of couplings Jij is usually extracted from a distribution with zero 
mean and unit variance; we consider the case in which the couplings are +1 or -1 with probability 0.5 
(binary model). A set of 3L3 Jij is a sample of the system; the couplings in a sample remain .xed and 
do not participate in the dynamics; The local energy of a single spin, say sk at site k, is determined 
by the interaction with its six neighbors only (x,y,z one lattice-site steps in the three directions 
of the lattice): E(sk)= -skfk; (2) fk =Jkj sj , (3) j=kx,ky,kz for a given con.guration of its neighbors 
sj , it s a two valued function, taking values E(+1) = -fk and E(-1) = fk. The local .eld fk is determined 
only by nearest neighbors of sk. If we make the assumption that the spin sk is always at equilibrium 
with its nearest spins (the local .eld) at a given temperature T , the probabilities of the spin to be 
up or down is given by the Boltzmann-Gibbs distribution: exp [边fk] P (sk = 1) = , (4) exp [fk] + exp[-fk] 
with  =1/T the inverse temperature (we take the Boltzmann constant kB to be one in our units). This 
de.nes the Heat-Bath algorithm (see ref. [5] for a review on dynamic Monte Carlo algorithms): at any 
given time, we may decide if the spin sk is up or down by comparing the probability to be up with a (pseudo-)random 
number . extracted uniformly in the interval [0, 1). The recipe for the simulation of the dynamics of 
a single sample of the model (1) is then as follows: 1. Extract the complete Jij con.guration (each Jij 
can be +1 or -1 with equal probability. 2. Extract the complete si con.guration (each spin can be up 
or down with equal probability; note that by eq. (4) this implies preparing the system at very high temperatures). 
 3. Begin a trial spin-.ip: pick a site k at random, each site equal probability. 4. compute the local 
.eld E(sk), eq. (3) and the spin up probability P (+1), eq. (4) 5. Pick a uniformly distributed pseudo-random 
number 0 = .< 1 from your favorite generator. 6. If .<P (+1), then put sk = +1, otherwise put sk = -1; 
end of the trial spin-.ip. 7. Repeat from step 3 above as many times as needed.  A Monte Carlo steps 
in this scheme is a number of trial spin-.ip equal to the number of sites in the lattice; it is the duration 
of the Monte Carlo steps to be compared to the average spin-.ip times in experiments, as discussed in 
the introduction. Each Monte Carlo step produces a brand new spin con.guration on the lattice; at regime, 
one would expect that the sampled con.gurations follow the Boltzmann-Gibbs distribution at the given 
inverse temperature  for the total energy (1) (but this is not actually the case for o.equilibrium 
simulations of spin glasses, as real equilibrium is never reached, conforming to the behavior of samples 
of glassy materials; notwithstanding this, the chosen microscopic dynamics still tends to real equilibrium). 
A nice property of this scheme is that when one performs a huge number of Monte Carlo steps, the statistical 
properties of the observables that one measures (averages over all sites and averages over Monte Carlo 
steps) do not depend anymore on the particular order in which the algorithm visits each lattice sites.1 
It turns out that this property is independent of any particular lexicographic order we could impose 
on the site-visiting scheme, so it is useful to choose an order, and impose that each site is visited 
once and only once in each Monte Carlo step, and always in the same order. This brings substantial simpli.cation 
and makes room for a very e.cient exploitation of the internal parallelism, as we will see shortly. As 
pointed out earlier, when one completes the necessary N Monte Carlo steps, the simulation must be repeated 
for a di.erent realization of the disorder con.guration, repeating the scheme above from point 1, generating 
results for many (hundreds, thousands) disorder samples. Yet another degree of replication is needed, 
however: some properties of the system comes out when comparing two independent simulations of the same 
sample, starting from independent initial spin con.gurations. Usually each sample must be simulated twice 
at least; the two identical copies with independent histories are called replicas. Let s discuss some 
of the usual techniques used in coding e.cient routines for the problem depicted above. The principal 
optimization is usually obtained by multi-spin coding. In order to compute the local .eld acting on a 
spin (step 4), we need to perform a small number of products and sums of two-valued variables. We then 
turn to the representation: sk . Sk = (1+ sk)/2 , (5) Jij . J ij = (1+ Jij )/2 , (6) fk . Fk = J kj . 
Sj = (6 - fk)/2 , (7) j where the products are substituted by logical XORs. There is an obvious advantage: 
spins and couplings may be arranged in long integer words (say 64-bit), and products of many spin and 
coupling variables may be carried out by bitparallel logical operations on machine (e.g. 64-bit) words 
(this technique also o.ers huge memory saving). Some complication arises when one has to extract the 
values of the local .eld; still, Fk takes only seven integer values between 0 and 6, and only three 64-bit 
words are needed to accumulate the sums of XOR products in a bitwise fashion. Note that few values for 
Fk means that P (+1) values (whose computation might in principle be very lengthy) can be precomputed 
Both random and deterministic sequences guarantee that the asymptotic distribution for the spin con.gurations 
on the whole lattice obey the Boltzmann-Gibbs distribution at the given temperature 1/ with energy given 
by Eq. 1, see ref. [5] for details. and stored in a look-up table, addressed by the values of Fk. This 
e.cient representation has two main variants depending on the internal or external level of parallelism 
involved when .lling each multi-spin coded word. In the Asynchronous Multi-Spin Coding (AMSC) approach, 
one .lls bit positions in the long word with spins (or couplings) belonging to the same site of independent 
samples. In the Synchronous Multi-Spin Coding approach, one .lls the bits with spin variables (and couplings) 
taken from a single sample, choosing of course a set of spins that can be updated simultaneously. A possibility 
consists in classifying the lattice sites as black or white following a checkerboard scheme. We see that 
if we choose to update all the black sites .rst, and subsequently all the white ones, the particular 
order in which we visit them is completely irrelevant to the speci.c outcome, in terms of spin con.guration, 
of the Monte Carlo step. We may then in principle update in parallel all black (white) sites, as they 
have only white (black) neighbors. For what we discussed earlier, any particular update order is good 
as long as, when one performs a measure, each spin has been visited an equal number of times on average. 
Both approaches have advantages and weaknesses. In the AMSC scheme, being all spins in a multi-spin-coded 
word pertaining to completely independent samples, we might tolerate some amount of correlation between 
samples (sampleto-sample .uctuations will still be the main source of statistical uncertainty) and 
use a single random number for all the probability comparisons (step 6) in the spin-.ip trials of each 
of the 64 spins of a multi-spin-coded word. This amounts to an important saving in terms of random number 
generation and increase in the overall throughput. This is not possible at all in the SMSC scheme, in 
which any small spurious correlation between spins in the same sample would produce unphysical artifacts. 
On the other side, the AMSC processes just one spin at a time for each single sample, so the wall-clock-time 
needed to complete a given simulation is not reduced by this technique. The SMSC instead accelerates 
the simulation time for single sample, shortening considerably the wall-clock time for completing the 
simulation of a single realization; however, if many samples are needed, SMSC is of no help. Sometimes 
a trade-o. between the two schemes is needed to meet the features of the speci.c architecture on which 
the application is implemented. But when one has to deal with very large lattices, as in o.equilibrium 
dynamics simulations, having a large number of samples is less critical than in equilibrium simulations, 
and the SMSC is the approach of choice. At the time the Janus project started, it was clear, analyzing 
the structure of the data sets and operation sequences involved in the algorithm presented here, that 
a recon.gurable device would have been an outstanding choice for an e.cient implementation. First of 
all, most computations involve a small number of logical operations on a small set of discrete variables 
(just two values in the simplest case discussed above).  There is a lot of internal parallelism, unveiled, 
for instance, in the checkerboard decomposition.  The main computational kernel (steps 4, 5, and 6) 
when repeated over the whole lattice, gives rise to regular loops performing the same set of operation 
on  a data base which has a regular structure, with data values stored in memory in regular patterns 
that do not depend on the computation itself; Simple statemachines might control the .ow of data from 
and to a set of identical computational blocks. The nature of the main basic computation block may be 
cast to exclusively combinatorial logic.  Pseudo-random number generation, which is an important time-consuming 
task in the algorithm, is completely independent on the rest of details of the algorithm; a set of 
specialized cores could compute pseudorandom numbers with the necessary sustained rate.  The ideal 
machine for spin glass application would then be a large set of identical cores, able to perform e.ciently 
only the small set of logical manipulation and a little arithmetics; a single control structure can drive 
all the cores working concurrently and performing all the same thread at the same time. The cost in 
term of each core would be order 1000 gates, and thousands of them could be arranged in recon.gurable 
devices. Modern FPGA also come with su.cient internal RAM blocks to store dynamical variables and sustain 
the required bandwidth. Traditional architectures, at the time the project started, were poor in extracting 
the internal parallelism. Even with the SMSC scheme, the generation of several high-quality random numbers 
(one per spin) was the principal bottle-neck in the computation. Traditional CPU are in fact tailored 
to manage complex basic computations (integer and .oatingpoint arithmetics) on (relatively) large data 
sets (32-and 64-bit words). The ideal spin glass machine would be instead more similar to an application 
speci.c GPU, with data paths tailored to perform the speci.c sequence of logical operations, a control 
structure shared by a number of cores larger than in state-of-the-art GPUs, data storage on on-chip memory 
only and a memory controller optimized for typical access patterns as required by the chosen algorithm. 
We resorted then to assemble a mesh of FPGA processors, that gave us freedom to meet all requirements 
of the prototypical application. Janus is essentially an array of hundreds of thousand of properly tailored 
small computational cores for the SMSC simulations of systems of interacting discrete variables on regular 
structures. Resorting to recon.gurable devices as our enabling technology allows for a large degree 
of customization and recon.guration of the applications. In this sense, Janus is not an application 
speci.c facility, performing well on a wide range of applications, and with outstanding performances 
on spin glasses.  3. THE JANUS ARCHITECTURE The smallest subsystem of Janus is the Janus board (see 
.g. 1), that contains 16 + 1 FPGA-based components: 16 SPs (Simulating Processors) and an IOP (the Input-Output 
Processor). A board needs an host PC to be operated (but a single PC may control several boards). The 
SP is the unit in which we exploit internal parallelism, e.ciently implementing SMSC algorithms for 
the chosen applications. External parallelism is usually obtained by farming SPs (16 per board) and driving 
more boards. In its full con.gura- Figure 1: Top: a schematic view of the Janus board internal ed external 
connections. Bottom: a picture of a Janus board, outside is enclosure box. tion, Janus is a stack of 
16 independent boards, totaling 256 SPs. There are 8 Linux hosts, each connected to two boards. On a 
single board, the 16 SPs are connected in a 2D (44) toroidal network with nearest neighbors physical 
links. More complex communications between nodes may be performed by the IOP processor, as more point-to-point 
connection links the IOP to each SP in the board. For some applications, the internal parallelism is 
applied at board level (splitting the simulation of a single sample to 2 or more SPs in the board) and 
replicating simulations on several boards. All the devices inside the Janus Board can be managed through 
the IOP, which is connected to the host PCs via a gigabit Ethernet interface. The host PC runs a standard 
Linux operating systems, and a set of speci.c C libraries designed on top of raw socket API allow a user 
to get through the link to and from the IOP. We choose the Xilinx Virtex-4 LX200 FPGA as the recon.gurable 
device for IOPs and SPs. The main clock in the board is 62.5 MHz and it is distributed to all IOP and 
SP devices. Such a conservative choice has been useful to guarantee mapping of our application to very 
dense .rmware codes (we reached near 95% resource occupation for our heaviest applications). The 16 SPs 
and the IOP are housed on daughter cards, plugged onto the Janus motherboard. The choice of daughter 
cards came from ease of maintenance and possibility of hardware upgrade. 3.1 IOP The architectural choices 
for the IOP comes directly from the features of typical spin glass : a Janus board acts as a coprocessor 
to a standard PC running the simulations, connected on a standard network interface, and with limited 
interactions between host and board with respect to computational tasks. The FPGA on the IOP is a Xilinx 
Virtes-4 LX200; the IOP module hosts 8 MB of static memory, a PROM programming device (to load the IOP 
s FPGA .rmware on power-up) and some I/O interfaces: two Gigabit Ethernet and a serial link (useful for 
debugging purposes). The IOP s FPGA .rmware is not designed to be recon.gured during uptime: it s main 
task is to manage data streams between the host PC and the SPs. The IOP .rmware is logically divided 
in two main blocks:  The IOlink block is responsible for managing the low level details of communications 
between IOP and host PC, in particular, manages the Gigabit Ethernet protocol and performs data integrity 
checks.  The MultiDev block comprises several sub-blocks (devices), each responsible od one speci.c 
task; there is a device for each hardware subsystem that might be reached for control or data transfer; 
among them:  A Program Interface (ProgInt) connected to the programming interfaces of the FPGAs loads 
.rmware modules onto the SPs  Memory Interfaces (MemInt) to read/write internal FPGA memory and the 
external 8 MB static memory.  A SP Interface (SPint) is responsible for routing data to SPs and collecting 
data from them.   The Stream Router routes the data streams from the IOlink to each speci.c device 
in the MultiDev, depending on control data present in the data stream itself.  The logic occupancy 
of the IOP .rmware is small compared to typical simulation .rmwares in SPs; the IOP occupancy is 4% 
of logic and 18% of total internal memory, leaving room for implementation of more complex tasks: to 
perform some global algorithmic operation, for example, or TM to include an instance of a MicroBlaze 
microprocessor in order to further loose the coupling to the host PC.  3.2 Running applications on Janus 
The SPs are the computational nodes of Janus. Each SP may operate independently or together with other 
SPs, depending on the .rmware loaded by the user program running in the host PCs. The user application 
consists basically of two layers: a .rmware layer, loaded on the SPs, implements the many cores performing 
the computational tasks as described in section 2. The main .ow of the simulation, data initialization, 
.rmware selection and load to SPs, data loading to SPs, SPs operation start, status check and stop, 
and data retrieve are managed by the user application. From the user point of view, the Janus machine 
appears as a grid of 32 identical nodes (each host PC manages two boards, but could in principle manage 
many more). A software layer, the Janus Operating System (JOS), comprises a program running as a background 
process in the host PC (josd); it performs the abstraction and acts as a job queue manager, reserving 
resources to users jobs depending on their requests on number and topology of SPs (one might require 
sixteen SPs anywhere in the machine or require them on a single board, actually reserving it as a whole). 
josd sits on the low level communication libraries that write and read data to and from the Gigabit Ethernet 
devices; josd is also aware of internal IOP functions, and is able to translate high-level user requests 
to data streams that IOP can understand and route to devices in the MultiDev. For instance, if an user 
wants to send a bu.er as a new con.guration for spins to a speci.c SP, josd builds the data burst to 
be sent through Ethernet to the IOlink; the data burst contains the appropriate headers that will be 
stripped by the Stream Router and the data stream will reach the appropriate device (the SPint); the 
latter will route the data stream to the chosen SP node. At this point, it is up to the user to ensure 
that data reaching the SPs be correctly interpreted by the SP. Development of a Janus application consists 
in facts of two tasks: programming the .rmware (in a hardware description language, VHDL is our choice, 
for instance) for the SPs, complying with the I/O requirements of the SPint in the IOP, and programming 
the mid-level library functions that, based on the josd interface to user application (the jlib library), 
implement the chosen protocol for communication with the SPs. In other words, the user application and 
the SP .rmware can speak their own language, that will be incapsulated in the data .ow between josd and 
the SPint in the IOP. Other functionalities common to any possible application (.rmware loading onto 
SPs, data read/write to memory interfaces of the MultiDev, read/write of communication interfaces status 
registers in the IOlink) can be accessed by other JOS user interfaces (JOSlib), or through an interactive 
shell built on top of them. 3.3 A Sample Spin Glass Application Implementation We brie.y describe our 
FPGA implementation of the algorithm described in Section 2; for more details we refer the reader to 
Refs. [3]. We try to exploit all FPGA resources (mainly con.gurable logic and RAM blocks) to achieve 
best performance. The Virtex-4 LX200 FPGA by Xilinx comes with many small RAM blocks that we can logically 
combine and stack to reproduce the 3D array of spins on a lattice of size L (L 2D memories with width 
L and depth L). In addition, we have to simulate two replicas at least as mentioned in section 2. Then, 
we consider two replicas of a single disorder sample and divide them in black and white sites in a checkerboard 
scheme; then, we arrange all black spins of one replica together with all white spins of the other replica 
in the same 3D memory structure. We end up with two mixed 3D memory structures (we call them structure 
P and structure Q); each spin in one structure has neighbors only in the other structure: no spins in 
the same structure are neighbors of each other in the physical lattices. Now, we can read or write an 
L-wide word from each memory of the 3D structure P per clock cycle; it turns out that having L memories, 
we can read an entire plane of the 3D memory P , update it and write back to memory at the next clock 
cycle; we only need three planes of neighbors from the 3D memory structure Q; if we update planes from, 
say, bottom to top; of the three planes of neighbors in Q needed to update a plane in P , two of them 
will be neighbors also for the subsequent plane of the structure P . At regime, spanning the whole 3D 
mem Figure 2: Diagram of the SP .rmware operation for the sample application. In the UE, the logic 
block H computes the local .eld, and use it as an address to the probability Look-Up Table L; the MC 
check block compares the probability value with an incoming pseudo-random number RND and returns the 
outcome as the new spin value. ory structure, only one read and at most one write is needed from each 
RAM block in order to update an entire plane of the P structure. We can then update an entire plane of 
L2 spins per clock cycle. Besides, we instantiate identical 3D memory structures to store all the necessary 
coupling constants, to be fetched with the same rate as the spins in the P structure. We then arrange 
a set of L2 identical update engines (UE); each UE receives in input the six neighbors of a single spin 
and combinatorially computes the local .eld; the pre-computed and constant-in-time spin-up probabilities, 
normalized to 32-bit unsigned integers, are stored in a Look-Up Table; each UE contains its own table 
and accesses it independently; the UE also receives a 32-bit word of random bits; a comparison between 
the addressed 32-bit probability and the 32-bit random number is performed and the new value of the updated 
spin is returned; Look-Up Tables un UEs are small 32-bit wide memories, for which we exploit distributed 
RAM (instantiated from con.gurable logic and not consuming RAM blocks). Once the whole P structure is 
updated, the controlling state machine interchanges the roles of P and Q, and the update of the Q structure 
starts with the same procedure described above. When the Q structure is updated, a Monte Carlo step is 
complete. The whole machinery is depicted in .g. 2 A key advantage of the FPGA implementation is that 
we can generate pseudo-random numbers concurrently with the rest of the computation and feed them to 
UEs at a proper rate. We usually resort to the 32-bit Parisi-Rapuano random number generator [6]. A single 
Parisi-Rapuano 32-bit shift register is a sequence I(k) of 32-bit integers; if we initialize externally 
the .rst 62 values, the set of rules I(k)= I(k - 24) + I(k - 55) (8) R(k)= I(k) . I(k - 61) , generates 
I(k) as the new element of the sequence and R(k) is the generated pseudo-random value. This simple generation 
rule is easily arranged on con.gurable logic only, so that exploiting cascaded structures each wheel 
is capable of hundreds of iterations per clock cycle. (a close inspection of the rules, for example, 
reveals that 24 consecutive values can always be generated independently; requiring more than 24 numbers 
at once, forces keeping intermediate values on registers and cascading the necessary combinatorial logic) 
This saves us a lot of resources, as a single shift register requires to keep 62 32-bit integers in 
registers or distributed RAM, and we need hundreds of generated random numbers per clock cycle. Increasing 
the number of random generation by a single wheel increase its combinatorial complexity but saves resources; 
having more generators is resource greedy but allow for easier routing of paths at compilation time. 
The trade o. between the number of generators and how many numbers any of them could produce per clock 
cycle impacts on the total number of UEs we may insert in the implementation. Another constraint on the 
number of UEs is the size of the system: it is advisable, to reduce the complexity of the control logic, 
that the number of UEs be an integer divider of the number L2 of sites in a plane; It is usually possible 
to place more than 512 UEs and up to 1024 UEs in a single FPGA, thus sustaining an update time of 16 
ps per spin at 62.5 MHz clock speed. 3.4 Other applications There are straightforward generalizations 
of the model presented in Section 2. If we consider q-valued spin variables, we obtain the q-states 
disordered Potts model (the Edwards-Anderson model is the 2-states Potts glass). We can also easily add 
dilution variables (a binary variable on each site representing the presence or absence of a spin on 
that site) and forcing .elds (another binary variable representing a preferred orientation for a spin, 
adding to the local .eld). When dealing with equilibrium simulations, one must ensure that equilibrium 
has been reached for the simulated dynamics. Due to the sluggish dynamics that characterize these systems, 
it is very hard to approach equilibrium by means of a local simulated dynamics like the Heat-Bath algorithm 
described in Section 2. In these situations, one resorts to the Parallel Tempering scheme [8], in which 
the local dynamics is interleaved with global moves that try to change the temperature of the systems. 
Actually, one simulates in parallel a number N of replicas of a samples, each at a di.erent temperature, 
from very high (the system freely moves away from its initial con.guration) to very low (the system con.guration 
is almost frozen, correlations are hard to decay with time). Periodically, the interchange of temperature 
is proposed between the replicas, so that frozen ones get the chance to evolve at higher temperature 
and decorrelate faster; even if the computational burden appears to increase by a factor N, the decrease 
in terms of time needed to thermalize a single sample at constant temperature is dramatic. When the algorithm 
is at regime, each replica is always in equilibrium at the respective temperature. The (Monte Carlo) 
time needed to reach the equilibrium condition strongly depend on the disorder of each sample: for such 
reason, even if we need thousands of samples to extract statistical measures, it is convenient to pursue 
synchronous parallelism. The computation for the trial temperature exchange is as follows: one has to 
compute the di.erence in total energy .E between the two replicas, an multiply it by the di.erence . 
in the inverse temperature of the two replicas, and then generate a uniformly distributed random number 
in [0, 1). Then the following condition log .< ..E (9) triggers the acceptance of the temperature exchange 
(if ..E is positive then the test is always successful2 ); the trials are performed in a speci.c order, 
usually among replicas with adjacent  values and starting from the hottest replicas down to the coldest. 
To implement parallel tempering, we simulate the Heat-Bath dynamics of the N replicas inside one FPGA, 
arranging them in P , Q pairs as described above (now we must di.erentiate the Look-Up Tables as the 
two replicas in the P , Q structures have di.erent temperatures; there will be in total N sets of 7 probability 
values; we store all of them on RAM blocks, and the sets are indexed by a beta index; each replica refers 
to a single probability set using the beta index; when the parallel simulation of a pair of replicas 
starts, we load the corresponding sets of of probabilities into the UEs Look-Up Tables consistently with 
the checkerboard scheme); every now and then, say each 10 complete lattice sweeps of all replicas, we 
perform the Parallel Tempering update; the .E computation is straightforward (it amounts, random number 
generation apart, as a full lattice sweep, with inhibited P or Q memory writes; the local energies on 
the sites as computed in the UEs can be summed up in one clock-cycle by a pipelined binary tree adder) 
but the logarithm computation takes several cycles; being it completely independent on the 10 Monte 
Carlo steps outcome, it can be put in parallel with the Heat-Bath dynamics to avoid slowing down the 
simulation. Due to limited resource in the SPs FPGAs, our implementation of Parallel Tempering is limited 
to N = 128 replicas of a single samples of linear size up to L = 32; In order to speed up the simulations 
of particularly hard samples, a variant implementation provides the Parallel Tempering scheme at board 
level, distributing replicas among the 16 SPs and performing the temperature exchange protocol in the 
IOP. We successfully programmed and used the Janus computer to simulate various glassy systems, studying 
in great detail their properties at equilibrium and out of equilibrium. The interested reader can .nd 
in Refs. [7] an exhaustive survey on grounbreaking results.  4. JANUS PERFORMANCES We refer mainly 
to the sample algorithm outlined in section 2. We consider an application running at 62.5 MHz clock 
cycle and performing the conservative number of 800 updates per clock cycle (this is actually the .rmware 
version used for production in the work presented in [2]) in a single SP. We are trying to transition 
from a state with replica i at temperature i and replica j at temperature j to a state with  values 
interchanged. Equation 9 comes from requiring that either the probability of the transition or its inverse 
be the maximum possible (unity): this is essentially then a Metropolis algorithm [5] for moving around 
replicas in the space of  values. In what follows we consider the single spin-update-time (SUT, the 
time needed to perform steps 3, 4, 5, and 6 of the procedure described in section 2 on a single spin 
variable) as the unit of performance. On an SP running 1024 updates per clock cycle, the SUT is 16 ps. 
In 2007, one year after the project started, our best Ising spin glass application on a Core 2 Duo CPU 
ran at 400 ps SUT with a fully AMSC implementation on 256 independent samples; our best SMSC implementation 
ran at 1 ns SUT. Since then, commodity CPUs have largely improved in terms of explicit parallelism; the 
performance gap with Janus is closing-up, being the latter still 10 times more e.cient than any commercially 
available solution. We performed several performance measures on a wide range of many-core systems (Cell 
Broadband Engine, 4-core Nehalem Intel CPU Xeon 5560, Tesla C1060 GP-GPU) [9], and some partial tests 
on a 8-core Intel Sandy Bridge processor (Xeon E52680). Results are summarized in Table 1. We described 
the SP of Janus as a module for strict SMSC, and that we trivially obtain repetition by farming over 
the whole Janus machine. A comparison to commercial processor is still unfair in this sense, since the 
latter greatly bene.t, in terms of performance, by the use of AMSC techniques. Usually, a mixed degree 
of parallelization has been introduced by either mixing the AMSC data-word .lling scheme with internal 
(synchronous) parallelization or the SMSC scheme with external (asynchronous) parallelization, using 
all available cores on the CPU or vector registers, in order to exploit peculiar features and to maximize 
performances. In what follows, we compare Janus performances to mixed AMSC-SMSC implementations, reporting 
on two kind of SUT measures: the time needed for completing the simulation divided by the total amount 
of trial spin-.ips needed to update a single sample (single-system SUT): this has to be compared with 
the full SMSC implementation in a single Janus SP; the time needed for completing the simulation divided 
by the total amount of trial spin-.ips needed to update all the simulated sample (global SUT): we compare 
this to the performance of a Janus board, in which one may consider both AMSC and SMSC levels of parallelism. 
Another important point in making comparisons is that many-core CPUs performances become better as the 
lattice size increases, exploiting more e.ciently the possibility to split the system on several cores. 
In addition, a comparison for very large sizes is not possible, as the Janus SPs is strongly limited 
by the on-chip RAM, which is about 6 Mbit for the devices we chose (we need roughly 5 bits of memory 
per lattice size; at largest sizes, routing di.culties between logical devices and RAM blocks arise, 
as the use of FPGA resources approaches 100%). In this respect, standard CPUs do not su.er for very large 
lattice sizes as long as there is enough cache memory, and even for the reasonably largest lattice size, 
multi-spin-coded implementation have very low memory occupancies. On the other side, performance of Janus 
depends on lattice sizes only by our particular choices in the .rmware implementation: having a number 
of UEs equal to an integer multiple of a power of 2 greatly simpli.es our coding; it comes from requiring 
the number of UEs to be equal to or an integer submultiple of the number of sites in a lattice plane; 
it happens then that our code for L = 16 runs at 256 updates per clock-cycle, the L = 32 and L = 64 codes 
run at 1024, the L = 80 code runs at 800, the L = 48 code runs at 768 and the L = 96 code can be com 
 single-system SUT (ns/spin) L Janus SP I-NH (8 Cores) CBE (8-SPE) CBE (16-SPE) Tesla C1060 I-SB (16 
cores) 16 0.063 0.98 0.83 1.17 32 0.016 0.26 0.40 0.26 1.24 0.37 48 0.021 0.34 0.48 0.25 1.10 0.23 64 
0.016 0.20 0.29 0.15 0.72 0.12 80 0.020 0.34 0.82 1.03 0.88 0.17 96 0.027 0.20 0.42 0.41 0.86 0.09 128 
 0.20 0.24 0.12 0.64 0.09 global SUT (ns/spin) L Janus I-NH (8 Cores) CBE (8-SPE) CBE (16-SPE) Tesla 
C1060 I-SB (16 cores) 16 0.004 (16) 0.031 (32) 0.052 (16) 0.073 (16) 32 0.001 (16) 0.032 ( 8) 0.050 (8) 
0.032 (8) 0.31 (4) 0.048 ( 8) 48 0.0013 (16) 0.021 (16) 0.030 (8) 0.016 (16) 0.27 (4) 0.015(16) 64 0.001 
(16) 0.025 ( 8) 0.072 (4) 0.037 (4) 0.18 (4) 0.015 ( 8) 80 0.0013 (16) 0.021 (16) 0.051 (16) 0.064 (16) 
0.22 (4) 0.011 (16) 96 0.0017 (16) 0.025 ( 8) 0.052 (8) 0.051 (8) 0.21 (4) 0.012 ( 8) 128  0.025 ( 8) 
0.120 (2) 0.060 (2) 0.16 (4) 0.011 ( 8) Table 1: Comparison of Janus performance on the sample application 
to some commercial processor: Intel Nehalem (dual socket board with two 4-core Xeon 5560), Tesla C1060 
and Intel Sandy-Bridge (dual socket board with two 8-core Xeon E5-2680). In the upper part, the single-system 
spin update time, to be compared with the SMSC implementation of a single SP of Janus. In the lower part, 
the global spin-update time, to be compared to the global spin-update time of a whole Janus board. The 
number of independent systems simulated in parallel in the AMSC scheme is shown in parentheses piled 
meeting timing requirement at 576 updates per cycle only . There is an intrinsic limit in the number 
of update cells which is around 103 with our implementation, due to resource limits in the FPGA; our 
code for the L = 64 lattice running at 1024 updates per clock cycle occupies 93% of the available logic 
cells and 57% of memory (RAM blocks) resources. The upper part of Table 1 show single-system SUTs for 
Janus compared to that of some commercial architectures, for several lattice sizes. In the lower part, 
the comparison between global SUTs is made with a Janus board (16 SPs); number in parentheses represent 
the degree of external parallelism adopted by each solution (i.e. the number of independent systems 
updated in parallel). The .gures we show for the GPU processor deserve a further comment. Floating-point 
application usually get large advantages by the use of GPUs, but this is not the case for our sample 
application; in facts, the Tesla C1060 does not really outperform other standard architectures, and 
keeps one order of magnitude less performing than Janus hardware. GP-GPU are more e.ective on applications 
with reduced memory accesses; the computational kernel of our application requires few operations: the 
local .eld of a single spin, for instance, can be computed with 6 XORs and 5 sums on a set of 12 variables 
(6 neighbor spins and 6 couplings). The GP-GPU performances are then limited by memorybandwidth (and 
thread synchronization overheads) despite peak performances are one order of magnitude larger than competing 
multi-core architectures. Let s now try an estimate and comparison of power consumption, computing approximatively 
the energy needed for a typical simulation campaign, as, for instance the one from which we obtained 
the results presented in [2]. We performed 1011 Monte Carlo steps in the Heat-Bath dynamics for a three 
dimensional Edwards-Anderson model, of linear size L = 80, with 256 samples, each replicated twice, and 
farming out the samples to the 256 SPs of the whole machine. Each SP simulated a single couple of replicas, 
so it performed 803  2 spin .ips, at a net single-system  1011 SUT of 20 ps, which amount to approximately 
2000000 sec onds (. 24 days). For this application, 16 boards run uninterruptedly at a constant power 
consumption of around 560 W each (35 W per SP, neglecting IOP s contribution, and considering all power 
conversion between the power supply and the SPs), for a total energy cost of 166002000000 . 18 GJ. 
Let s take the best .gure for the global SUT (allowing the best total energy performance) for the L 
= 80 case from table 1, the 8-cores Intel Sandy-Bridge: 0.011 ns global SUT corresponding to 0.17 ns 
single-system SUT running 16 samples in parallel. This count for completing the above program with a 
16 processors farm (256 samples) running uninterruptedly for 17000000 seconds (201 days; note that .ve 
years ago on dual-core CPUs, the wall-clock time would have been 10 times longer), and consuming in total 
26 GJ, counting a contribution of 95 W per CPU. Six years ago, our best code on a single dual-core CPU 
performed at 0.385 ns global SUT with no external parallelism implemented (full AMSC); the two cores 
together would have simulated all the 256 samples in parallel, with a single-system SUT of ~ 100 ns, 
(taking 5000 times longer than Janus), and with a power consumption of about 65 W would have needed 660 
GJ. A SMSC implementation running at 3 ns single-system SUT would have shortened considerably the wall-clock 
time (roughly a factor 30), at the cost of a worse global SUT (256 CPUs involved), and then approximately 
8 time more power consumption (about 5 TJ). We see then that recent standard architecture did indeed 
.ll the gap with Janus in terms of energy consumption. Still, the Janus computer, based on six-year-old 
technology, remains architecture of choice for spin glass simulations. The .gures we presented show 
that Janus is capable of performing a given simulation in 10 times shorter wall-clock-time. Still, FPGA 
technology has followed its course and we expect that today state-of-the-art FPGA based processors would 
outperform the Janus SPs. Finally, we remark that straightforward generalizations of the Edwards-Anderson 
model, e.g. the q-states disordered Potts model, come with small degradation of performances in terms 
of spin update time (and almost no degradation device logic cells distributed RAM block RAM XC4VLX200 
200448 1392 Kb 6048 Kb XC7VX485T 485760 8175 Kb 37080 Kb Table 2: Resource comparison between FPGAs 
in Janus (Xilinx XC4VLX200) and Janus2 (Xilink XC7VX485T) (.gures reported from Xilinx datasheets). 
at all in terms of SUT/log2 q). In facts, we only need to make room for log2q bits to store for a single 
Potts variable, paying a cost in simulable system sizes and number of UEs (that will be slightly more 
complex than described in the 2-states case) but we can always recon.gure the FPGA to perfectly balance 
the data bandwidth needed to feed the grid of local updating engines. On standard CPUs, this small increase 
in complexity re.ects invariably in heavier computational kernels and increased bandwidth demand with 
the same data-paths. In addition, we also expect, with the new supercomputer, to expand both time and 
length scales of our simulations of an order of magnitude at least, with a limited increase in power 
requirement. A similar improvement in simulations with standard processors would imply roughly 100 times 
more computation, and then 100 more power consumption (supposing an optimistic linear increase in wall-clock 
time).  5. JANUS2 The next generation of the Janus supercomputer will respect the general architecture 
of its predecessor, with many improvements made possible by technology advances in the last years. We 
plan the following main improvements: Latest-generation FPGA devices.  A tighter coupling between the 
IOP and the host PC.  A faster and more .exible communication between IOP and SPs inside one board and 
across boards.  Janus2 will be a again cluster of processing boards. Each board is a set of 16 SPs and 
an IOP as in Janus, built as piggy-back modules on a processing board (PB). The PB provides all electrical 
links and connectors to arrange the SP grid into a 4  4  13D toroidal network: the added third dimension 
with respect to the Janus board allow for communication between SPs on adjacent boards; a Janus2 machine 
with a number B of boards will be a 4  4  B 3D toroidal network. The PB also provides the 125 MHz master 
clock to all devices. The IOP features the largest increase in complexity with respect to other Janus 
components. It will integrate all of the previous IOP functionality and will integrate the host PC. A 
Computer-On-Module (COM) express board plugs onto the IOP piggy-back module. The COM express system 
will have an Intel Nehalem or Sandy-Bridge class CPU and at least 4 GB DDR3 memory. A Solid State Disk 
will be connected to the COM express via SATA links. The core of the IOP will of course be an FPGA performing 
all data-routing and featuring all control features described for the Janus IOP (see Sec 3.1). We select 
the Xilinx Virtex7 XC7VX485T. The FPGA processor will export a 8x PCI Express Gen 2 link to the COM 
module and provide enough high-speed serial links (GTX) to all the SPs on the board and to IOPs in other 
boards. The IOP interfaces to the world via an In.niband adapter; there will be service connections 
also: two Gigabit Ethernet channel and two serial interfaces (one of each type for both the COM and 
the FPGA). The IOP of course provides also the programming interface for on-the-.y con.guration of the 
SPs FPGAs; each SP will be con.gurable independently one on each other. The SP also features one Xilinx 
Virtex-7 XC7VX485T. At variance with the previous Janus SP, it will also host some DDR3 DRAM. It will 
have direct connections to neighboring SPs in the 3D logical toroidal network (1 per space directions, 
6 total), physically implemented with high-speed serial transceivers in the FPGA (5 GB/s transfer rate 
minimum in each direction). The fast interconnection network and the new FPGA features provide large 
margins for performance improvements with respect to the previous Janus computer. Janus 2 features FPGAs 
with more than twice the logic, 6 times more distributed memory and 6 times more block RAM (see table 
2) than the largest Virtex-4 devices. The available logic permits a factor 2 increase in terms of number 
of updates per clock-cycle. Another factor 2 comes from the faster clock of the system (in Janus it was 
62.5 MHz). This amounts for a global SUT increase of a factor 4. Please note that we consider running 
even at faster speed inside the SPs, and that we are conservatively supposing that our application will 
run at the rate given by the master clock. Another factor 2 improvement could come from extreme tailoring 
our application to the new available fast FPGA) The main di.erence with Janus will be the highspeed 
direct connections between the SPs in the 3D mesh: each link is about .ve times faster than each link 
in the 2D SP mesh of a Janus board. Taking in consideration the sample application described in section 
2, available memory and connection speed allow for simulating two replicas of a single sample of a 3D 
spin glass on a lattice of linear size L = 1200 dividing it into sublattices of 300  300  150 among 
the 128 SPs of 8 adjacent boards. With a sustained update rate of 2000 spins per clock on 300  300 
planes cycle inside each SPs, the most demanding bandwidth would be the border spins transfer through 
the links connecting SPs in di.erent boards, requiring 2000 bits transfered in 150 cycles (before they 
become necessary to start the next update sequence in the neighbor SP) corresponding to 3.3 Gb/s. Such 
implementation would then be capable of an update rate of 256000 spins per clock-cycle, corresponding 
to a single-system SUT of 32  10-15 (and the global SUT is just half that amount in a compete con.guration 
with 16 Janus2 boards): in Janus2 many SPs in many boards will be able to concur in exploiting all the 
internal parallelism. With such very large simulated lattice sizes, the .uctuations of measures between 
di.erent disorder samples become less important, and we could be contented with tens of samples instead 
of hundreds. A simulation program as the one described at the end of section 4 (two replicas, 1011 Monte 
Carlo steps) with only 16 samples, would obtain relevant results in about one year (if one decides to 
sacrify a large fraction of the machine durability to a single ambitious task). In a single SP the single-system 
SUT would be around 4 times better than in Janus. With only two SPs, we could allocate two replicas of 
a L = 150 system and simulate them Figure 3: Sketch of performance growth in time of our simulation 
codes following the technology improvement; each point correspond to the time (abscissae) we actually 
get a code running on a speci.c architecture; the obtained performances, as reported in table 1 are 
in ordinates, in billions of updated spins per second units. Green points are the Intel series (one 
CPU); the red point corresponds to the CBE and the blue point is the Tesla GPU. The black line are performances 
of one board of the Janus machines (Janus2 is expected starting scienti.c production in mid 2013). The 
red line represents the performance growth predicted by a Moore s Law with doubling performance each 
18 months (taking our .rst implementations in 2005). The green line is the performance growth trend extracted 
by considering only our data points for the Intel series, giving a doubling in performance each year, 
approximately. with a 4500 updates per spin rate and complete the task in a month. On a farm of 16 8-cores 
Sandy-Bridge processors it would take almost two years (assuming the measured performance for a L = 
128 system from table 1 applies). It is very di.cult to foresee the equivalent energy consumption, as 
we can only forecast the SPs power consumption by the software development tools, without direct measures 
on the running hardware. We foresee an optimistic .gure of 50 W per SP, obtaining a 30 GJ total needed 
to complete the simulation program for a L = 150 system (it would be 5 GJ for the L = 80 lattice). The 
farm of 16 commercial CPUs (Sandy-Bridge) would consume 92 GJ. The Janus2 computer then establish again 
near two orders of magnitude in the gap with commercial processors. Power consumption improvements are 
sizeable but no so impressive as in that case of Janus with respect to its competitors six years ago; 
in this respect, standard CPUs have largely improved in terms of energy-per-performance in the last 
years. 6. CONCLUSIONS We have described two Janus generations of supercomputer for Spin Glass application. 
In the second generation we expect a signi.cant improvement with respect to the previous one, due to 
progresses in FPGA technology and some major architectural design modi.cations (mainly the interconnection 
network topology) that will signi.cantly decrease wall-clock time of simulations of Spin Glasses. We 
expect Janus2 to be a durable facility; we can compare the performances of one board of the Janus machine 
and the expected performances of a Janus2 board to the performance growth trend of commercial CPUs (see 
.g. 3). In both cases, we consider a technology available when we have optimized code ready for testing; 
of course, in the case of janus machines, this includes the necessary long development time; on the 
other side, carefully optimize the simulation code on commercial CPUs can be a hard and lengthy task. 
In .gure 3 we compare best performance values taken from table 1. The black line represents performances 
in terms of billions of spins updated per second for a single board of a Janus-type computer (we expect 
Janus2 to start operation in summer 2013). We also report on the .gure (the green line) the performance 
growth trend we experienced with the Intel series for our applications (a doubling in performance every 
year approximately) and the red line is the expected growth since 2005 by a Moore s law (doubling each 
18 months). We have only two measurements of the performances of FPGAs in Spin Glass applications, and 
one of them is the conservative estimate we provide for Janus2. Performance increase with recon.gurable 
devices appear slower than the average performance growth rate of other technologies. It should be noted 
that, in the case of Intel processors, scaling of performances in our application either fell behind 
Moore s law or surpassed it between two consecutive microarchitecture releases (as in the case of Nehalem-Sandy 
Bridge transition). Newer FPGAs get larger and faster, but the technology used to improve them could 
not be ideal to our applications (this is the case, for example, of the largest devices in the Virtex-7 
family, whose structure made of stacked smaller devices may introduce a bottleneck in the data-paths 
due to the physical links between layers, that are slower than the typical signal path in any smaller 
device). On the other side, any increase of a factor X in the target clock frequency is transferred directly 
as a factor X in performances, and is usually easier to meet time requirements when compiling a .rmware 
for a larger devices. If the trend in Intel performance growth should continue, and if the estimate we 
provide for the Janus2 computer holds, the latter has an advantage of almost four years, which is enough 
to ensure a signi.cant output of scienti.c production. If the trend continues by the usual Moore s law 
starting form the Sandy-Bridge performances, Janus2 will remain unrivaled for two years more than in 
the previous case. 7. ADDITIONAL AUTHORS M. Baity-Jesi, L.A. Fernandez, V. Martin-Mayor and B. Seoane 
(Departamento de Fisica Teorica I, Universidad Complutense, 28040 Madrid, and Instituto de Biocomputacion 
y Fisica de Sistemas Complejos (BIFI), 50018 Zaragoza, Spain); R.A. Ba nos, A. Cruz, J. Monforte-Garcia 
and A. Tarancon (Departamento de Fisica Teorica, Universidad de Zaragoza, 50009 Zaragoza, and BIFI); 
J.M. Gil-Narvion, M. Guidetti and S. Perez-Gaviro (BIFI); A. Gordillo-Guerrero (Departamento de Ingenieria 
Electrica, Electronica y Automatica, Universidad de Extremadura, 10071 Caceres, and BIFI); D. I 
niguez (Fundacion ARAID, Diputacion General de  Aragon, Zaragoza, and BIFI); A. Maiorano and D. Yllanes 
(Dipartimento di Fisica, Sapienza Universit`a di Roma, 00185 Rome, Italy and BIFI); F. Mantovani (Dipartimento 
di Fisica, Universit`a di Ferrara, 44100 Ferrara, Italy); E. Marinari, G. Parisi and F. Ricci-Tersenghi 
(Dipartimento di Fisica, IPCF-CNR, UOS Roma Kerberos and INFN, Sapienza Universit`a di Roma, 00185 Rome, 
Italy); A. Mu noz-Sudupe (Departamento de Fisica Teorica I, Universidad Complutense, 28040 Madrid, 
Spain); D. Navarro (Departamento de Ingenieria, Electronica y Comunicaciones and Instituto deInvestigacion 
en Ingenieria) de Aragon (I3A), Universidad de Zaragoza, 50018 Zaragoza, Spain); M. Pivanti (Dipartimento 
di Fisica, Sapienza Universit`a di Roma, 00185 Rome, Italy); J.J. Ruiz-Lorenzo (Departamento de Fisica, 
Universidad de Extremadura, 06071 Badajoz, Spain and BIFI); S.F. Schifano (Dipartimento di Matematica 
e Informatica, Universit`a di Ferrara and INFN -Sezione di Ferrara, 44100 Ferrara, Italy); P. Tellez 
(Departamento de Fisica Teorica, Universidad de Zaragoza, 50009 Zaragoza, Spain); R. Tripiccione (Dipartimento 
di Fisica, Universit`a di Ferrara and INFN -Sezione di Ferrara, 44100 Ferrara, Italy)  8. REFERENCES 
[1] M. Mezard, G. Parisi and M.A. Virasoro, Spin Glass Theory and Beyond (World Scienti.c, Singapore, 
1987); A. P. Young (editor), Spin Glasses and Random Fields, (World Scienti.c, Singapore, 1998). [2] 
The Janus Collaboration, Phys. Rev. Lett. 101, (2008) 157201; [3] The Janus Collaboration, Comp. Phys. 
Comm. 178, (2008) 208-216; IANUS: Scienti.c Computing on an FPGA-based Architecture, in Proceedings of 
ParCo2007, Parallel Computing: Architectures, Algorithms and Applications (NIC Series Vol. 38, 2007) 
553-560; Computing in Science &#38; Engineering 8, (2006) 41-49; Computing in Science &#38; Engineering 
11, (2009) 48-58. [4] S. F. Edwards and P. W. Anderson, J. Phys. F: Metal Phys. 5, (1975) 965-974; ibid. 
6, (1976) 1927-1937. [5] A.D. Sokal, Functional Integration: Basics and Applications (1996 Carg`ese School) 
ed. C. DeWitt-Morette, P. Cartier and A. Folacci (1997 New York: Plenum) [6] G. Parisi and F. Rapuano, 
Phys. Lett. B 157, (1985) 301-302. [7] The Janus Collaboration: J. Stat. Phys. 135, 1121-1158 (2009); 
Phys. Rev. B 79, 184408 (2009); J. Stat. Mech. (2010) P05002; J. Stat. Mech. (2010) P06026; Phys. Rev. 
Lett. 105, 177202 (2010); Phys. Rev. B 84, 174209 (2011); Proc. Natl. Acad. Sci. USA (2012) 109, 6452-6456 
[8] Hukushima, K., Nemoto, K. Exchange Monte Carlo Method and Application to Spin Glass Simulations J 
. Physical Soc Japan 65,1604 aAS1608 (1995), arXiv:cond-mat/9512035, E. Marinari, Optimized Monte Carlo 
Methods in Advances in Computer Simulation. Lecture Notes in Physics, 1998, Volume 501/1998, 50-81 (1996), 
arXiv:cond-mat/9612010. [9] M. Guidetti et al., Spin Glass Monte Carlo Simulations on the Cell Broadband 
Engine in Proc. of PPAM09, (Lecture Notes on Computer Science (LNCS) 6067, Springer 2010) 467-476. M. 
Guidetti et al., Monte Carlo Simulations of Spin Systems on Multi-core Processors (Lecture Notes on Computer 
Science (LNCS) 7133 K. Jonasson (ed.), Springer, Heidelberg 2010) 220-230.   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2012</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>2322159</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<pages>10</pages>
		<display_no>3</display_no>
		<article_publication_date>06-25-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[On the scalability of the clusters-booster concept]]></title>
		<subtitle><![CDATA[a critical assessment of the DEEP architecture]]></subtitle>
		<page_from>1</page_from>
		<page_to>10</page_to>
		<doi_number>10.1145/2322156.2322159</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2322159</url>
		<abstract>
			<par><![CDATA[<p>Cluster computers are dominating high performance computing (HPC) today. The success of this architecture is based on the fact that it proffits from the improvements provided by mainstream computing well known under the label of Moore's law. But trying to get to Exascale within this decade might require additional endeavors beyond surfing this technology wave. In order to find possible directions for the future we review Amdahl's and Gustafson's thoughts on scalability. Based on this analysis we propose an advance architecture combining a Cluster with a so called Booster element comprising of accelerators interconnected by a high performance fabric. We argue that this architecture provides significant advantages compared to today's accelerated clusters and might pave the way for clusters into the era of Exascale computing. The DEEP project has been presented aiming for an implementation of this concept. Six applications from fields having the potential to exploit Exascale systems will be ported to DEEP.We analyze one application in detail and explore the consequences of the constraints of the DEEP systems on its scalability.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[DEEP]]></kw>
			<kw><![CDATA[cluster]]></kw>
			<kw><![CDATA[exascale]]></kw>
			<kw><![CDATA[scalability]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>C.4</cat_node>
				<descriptor>Design studies</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.2</cat_node>
				<descriptor>Earth and atmospheric sciences</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.5.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010432.10010437</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Earth and atmospheric sciences</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520</concept_id>
				<concept_desc>CCS->Computer systems organization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944.10011123.10011673</concept_id>
				<concept_desc>CCS->General and reference->Cross-computing tools and techniques->Design</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P3665948</person_id>
				<author_profile_id><![CDATA[81503671396]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Damian]]></first_name>
				<middle_name><![CDATA[Alvarez]]></middle_name>
				<last_name><![CDATA[Mallon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[J&#252;lich Supercomputing Centre, J&#252;lich, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[d.alvarez.mallon@fzjuelich.de]]></email_address>
			</au>
			<au>
				<person_id>P3665949</person_id>
				<author_profile_id><![CDATA[81100597618]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Norbert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eicker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[J&#252;lich Supercomputing Centre, J&#252;lich, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[n.eicker@fz-juelich.de]]></email_address>
			</au>
			<au>
				<person_id>P3665950</person_id>
				<author_profile_id><![CDATA[81503669875]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Maria]]></first_name>
				<middle_name><![CDATA[Elena]]></middle_name>
				<last_name><![CDATA[Innocenti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Katholieke Universiteit Leuven, Heverlee, Belgium]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[MariaElena.Innocenti@wis.kuleuven.be]]></email_address>
			</au>
			<au>
				<person_id>P3665951</person_id>
				<author_profile_id><![CDATA[81100460009]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Giovanni]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lapenta]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Katholieke Universiteit Leuven, Heverlee, Belgium]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[giovanni.lapenta@wis.kuleuven.be]]></email_address>
			</au>
			<au>
				<person_id>P3665952</person_id>
				<author_profile_id><![CDATA[81100518885]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lippert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[J&#252;lich Supercomputing Centre, J&#252;lich, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[th.lippert@fz-juelich.de]]></email_address>
			</au>
			<au>
				<person_id>P3665953</person_id>
				<author_profile_id><![CDATA[81503641048]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Estela]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Suarez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[J&#252;lich Supercomputing Centre, J&#252;lich, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[e.suarez@fz-juelich.de]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[http://www.top500.org]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[http://www.deep-project.eu]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[http://http://www.mpi-forum.org]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Gordon E. Moore, "Cramming more components onto integrated circuits.", Electronics. 19, Nr. 3, 1965, pp. 114-117.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[www.cse.nd.edu/Reports/2008/TR-2008-13.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[http://www.theregister.co.uk/2010/11/22/ibm_blue_gene_q_super]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[http://developer.nvidia.com/gpudirect]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[http://www.green500.org]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[H. Baier et al., "QPACE: power-efficient parallel architecture based on IBM PowerXCell 8i", Computer Science - R&D 25 (2010), pp. 149-154. doi:10.1007/s00450-010-0122-4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1465560</ref_obj_id>
				<ref_obj_pid>1465482</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Gene Amdahl (1967), "Validity of the Single Processor Approach to Achieving Large-Scale Computing Capabilities", (PDF), AFIPS Conference Proceedings (30), pp. 483-485.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>42415</ref_obj_id>
				<ref_obj_pid>42411</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[John L. Gustafson, "Re-evaluating Amdahl's Law", Communications of the ACM 31(5), 1988, pp. 532-533.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Charles Clos, "A Study of Non-blocking Switching Networks", The Bell System Technical Journal, 1953, vol. 32, no. 2, pp. 406-424]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[http://www.intel.com/pressroom/archive/releases/2010/20100531comp.htm]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[http://newsroom.intel.com/servlet/JiveServlet/download/38-6968/Intel_SC11_presentation.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1679697</ref_obj_id>
				<ref_obj_pid>1678990</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Mondrian N&#252;ssle et al., "A resource optimized remote-memory-access architecture for low-latency communication", The 38th International Conferenceon Parallel Processing (ICPP-2009), September 22-25, Vienna, Austria.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[H. Fr&#246;ning und H. Litz, Effcient Hardware Support for the Partitioned Global Address Space, 10th Workshop on Communication Architecture for Clusters (CAC2010), co-located with 24th International Parallel and Distributed Processing Symposium (IPDPS 2010), Atlanta, Georgia, 2012.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1773092</ref_obj_id>
				<ref_obj_pid>1772998</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[S. Markidis, G. Lapenta and Rizwan-Uddin, "Multi-scale simulations of plasma with iPIC3D", Mathematics and Computers in Simulation, pp. 1509-1519, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[J. U. Brackbill and D. W. Forslund, "Simulation of low frequency, electromagnetic phenomena in plasmas", Journal of Computational Physics, 1982, p. 271.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>641286</ref_obj_id>
				<ref_obj_pid>641282</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[P. Ricci, G. Lapenta and J. U. Brackbill, "A simplified implicit Maxwell solver", Journal of Computational Physics (2002), p. 117.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>32611</ref_obj_id>
				<ref_obj_pid>32608</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[B. Marder, "A method for incorporating Gauss' law into electromagnetic PIC codes", J. Comput. Phys., vol. 68 (1987), p. 48]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[A. Bruce Langdon, "On enforcing Gauss' law in electromagnetic particle-in-cell codes", Computer Physics Communications, vol. 70, Issue 3 (1992).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[A. Duran, E. Aygua&#233;e, R. M. Badia, J. Labarta, L. Martinell, X. Martorell and J. Planas, "OmpSs: A Proposal for Programming Heterogeneous Multi-Core Architectures", in Parallel Processing Letters, vol. 21, Issue 2 (2011) pp. 173-193.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[G. R. Gao, T. L. Sterling, R. Stevens, M. Hereld and W. Zhu, "ParalleX: A Study of A New Parallel Computation Model", in Proc. of 21th International Parallel and Distributed Processing Symposium (IPDPS 2007), Long Beach, California, USA]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 On the Scalability of the Clusters-Booster concept A critical assessment of the DEEP architecture 
Damian Alvarez Mallon Jlich Supercomputing Centre Forschungszentrum Jlich 52425 Jlich, Germany d.alvarez.mallon@fzjuelich.de 
Giovanni LapentaKatholieke Universiteit Leuven BE-3001 Heverlee, Belgium giovanni.lapenta@ wis.kuleuven.be 
ABSTRACT Cluster computers are dominating high performance computing (HPC) today. The success of this 
architecture is based on the fact that it pro.ts from the improvements provided by mainstream computing 
well known under the label of Moore s law. But trying to get to Exascale within this decade might require 
additional endeavors beyond sur.ng this technology wave. In order to .nd possible directions for the 
future we review Amdahl s and Gustafson s thoughts on scalability. Based on this analysis we propose 
an advance architecture combining a Cluster with a so called Booster element comprising of accelerators 
interconnected by a high performance fabric. We argue that this architecture provides signi.cant advantages 
compared to today s accelerated clusters and might pave the way for clusters into the era of Exascale 
computing. The DEEP project has been presented aiming for an implementation of this concept. Six applications 
from .elds having the potential to exploit Exascale systems will be ported to DEEP. We analyze one application 
in detail and explore the consequences of the constraints of the DEEP systems on its scalability.  Categories 
and Subject Descriptors C.4 [Performance of Systems]: Design Studies; C.5 [Computer System Implementation]: 
General; J.2 [Physical Sciences and Engineering]: Earth and atmospheric sciences  General Terms Design, 
Performance Permission to make digital or hard copies of all or part of this work for personal or classroom 
use is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage 
and that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, 
to post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICS2012 
Workshop Future HPC systems: the Challenges of Power-Constrained Performance Venice, Italy, 2012 Copyright 
2012 ACM 978-1-4503-1453-4/12/06 ...$10.00. Norbert Eicker Jlich Supercomputing Centre Forschungszentrum 
Jlich 52425 Jlich, Germany n.eicker@fz-juelich.de Thomas Lippert Jlich Supercomputing Centre Forschungszentrum 
Jlich 52425 Jlich, Germany th.lippert@fz-juelich.de Maria Elena Innocenti Katholieke Universiteit 
Leuven BE-3001 Heverlee, Belgium MariaElena.Innocenti@wis.kuleuven.be Estela Suarez Jlich Supercomputing 
Centre Forschungszentrum Jlich 52425 Jlich, Germany e.suarez@fz-juelich.de  Keywords Exascale, Cluster, 
DEEP, Scalability 1. INTRODUCTION Today HPC architectures are dominated by cluster computers. The prosperity 
of this concept stems from the ability to gain from the advances in mainstream computing during the last 
decades. The modular setup of clusters allows for the replacement of single components improving the 
overall performance of such systems. For example, a new generation of CPUs providing more compute power 
might replace their predecessor without the necessity to change the high speed fabric or the complex 
software stack at the same time. With systems at Petascale in production today the next goal in HPC is 
to reach Exascale by the end of the decade. Obviously, this target introduces new challenges. First of 
all there are technological problems like energy e.ciency or resiliency to be overcome. Furthermore, 
it is questionable, if general purpose CPU will still be competitive from an energy e.ciency point of 
view with more specialized solutions like accelerators, namely GPGPUs1 . Since the way accelerators 
are employed in today s systems limits their scalability signi.cantly, it will become a necessity to 
review the idea of the cluster architecture in HPC in order to prolong their success into the future. 
To get a better idea in which direction clusters have to be developed we re-investigated the requirements 
set by applications potentially capable of using Exascale systems. On one hand there are highly scalable 
parallel kernels with simple communication patterns. On the other hand less scalable kernels will limit 
the overall scalability of the application. Typically, the latter will show much more complex communication 
patterns. Based on this analysis we have proposed a novel HPC architecture combining a classical cluster 
system with a so called Booster system comprising of accelerator nodes and a high speed interconnect 
having a less .exible but very scalable torus topology. The basic idea is to use the Booster 1General 
Purpose Graphical Processing Units in fact these are highly optimized .oating point units. for the highly 
scalable kernels of potential Exascale applications. The cluster part of the system will be reserved 
for the less scalable and more complex kernels. For a high performance deployment of an Exascale application 
to the Cluster-Booster system it will become inevitable to make use of both parts concurrently. The Cluster-Booster 
concept is the basis of the EU Exascale project DEEP [2]. DEEP s aim is to implement the Booster on 
the basis of Intel s MIC technology [13] and the EXTOLL interconnect [15]. MIC s .rst commercially available 
incarnation developed under the code name Knights Corner (KNC) will act as the accelerator processor 
in the Booster nodes while the EXTOLL fabric developed at the University of Heidelberg will be used for 
the Booster fabric. Enabling application developers to make use of the DEEP system will require a complex 
software stack to be designed and implemented during the 3-year project. Six di.erent applications will 
be ported to the system. This paper is organized as follows: In the next section we explore the space 
of HPC architectures by means of analyzing results taken from the TOP500 list. With that we identify 
the challenges arising from the goal to reach Exascale by the end of the decade. In section 4 general 
ideas concerning scalability in HPC are revised. Section 5 presents the DEEP project including the idea 
of the combined Cluster-Booster architecture, and DEEP s implementation of the Booster hardware and the 
software stack. Before we conclude, section 6 presents a Space Weather application that will be ported 
to DEEP and draws some ideas concerning scalability of the DEEP system from this example. 2. HPC ARCHITECTURES 
IN TOP500 Having collected almost 20 years of statistics the TOP500 list2 [1] is a powerful tool for 
identifying trends in HPC architectures. It shows that starting with very .rst examples in the late 
90 s today s clusters are the dominant architecture in HPC. Their stage was set by commodity processors 
that became su.ciently powerful in respect to .oating point performance (e.g. Compaq s Alpha, AMD s Opteron 
or Intel s Xeon processors) together with high bandwidth and low latency networks that enabled the connection 
of such commodity nodes (at that time MyriNet and Quadrics, later Gigabit Ethernet and now superseded 
by In.niBand). Last but not least only the availability of a complete open source software stack enabled 
the available hardware to form the general purpose supercomputers as we know them today. Distributed 
memory systems like clusters require a convenient programming paradigm. Even before clusters became 
available with the advent of MPP systems the message passing interface (MPI) [3] became the predominant 
standard for the programming of communication operations as required for parallel applications. Therefore, 
the availability of free, high quality implementations of the MPI standard was crucial. Without them 
clusters would not have been usable for practical applications at that time. After all, the availability 
of source code enabled the community to implement the necessary adaptions required by every hardware 
innovation in this .eld. Since then cluster architectures took over more than 80% of the market leaving 
the remainder for MPP systems3 . This 2This paper refers to the November 2011 version of this list. 3These 
systems are represented today by IBM s BlueGene incredible success is caused by the ability to bene.t 
from the enhancements of computer technology in general. Of course, this development is not mainly driven 
by a relatively small market like HPC. Instead, it is pushed by mainstream markets like enterprise servers, 
personal computing or gaming. Combined with the ever increasing costs of technology development designing 
a new generation of processors requires investments in the range of billions of e competing HPC architectures 
were only able to survive in very special niche markets like the very high end of HPC. The most crucial 
feature of HPC architectures is not the highest performance of a single component but the balance of 
all building blocks in the system. E.g. most HPC systems don t rely on the highest performing CPU available 
at a given time. Instead, a compromise that is more balanced to the available memory and network bandwidths 
is used. In the end this will show a higher energy e.ciency or optimizes the ratio of price vs. performance. 
Another important result derived from the TOP500 list is the development of the available compute power. 
Combined with results of historical HPC systems it shows that HPC systems gain a factor of 1000 in performance 
per decade. It is important to face the fact that this development outperforms Moore s law , i.e. the 
observation that semiconductor technology doubles the number of transistors per unit area every 18 month 
[4]. Thus, in order to achieve this outcome a signi.cant increase of the systems parallelism beyond sheer 
accretion of transistors per chip was necessary in HPC. This re.ects in the observation that basically 
all current HPC architecture are massively parallel. 3. THE EXASCALE CHALLENGE Running systems with 
multi-Peta.op compute power today4, the community thinks about the next step, i.e. having Exascale systems 
(1018 .oating point operations per second) by the end of the decade. An analysis done by a group of experts 
four years ago showed [5] that an Exascale systems using an updated version of today s technology and 
concepts would run into severe trouble: Power consumption A projection of today s architectures and 
machines onto technology of 2018 predicts a power requirement of several 100 MW for an Exascale system, 
rendering the energy requirements as completely unacceptable.  Resiliency Projecting the trend of ever 
increasing number of components in HPC systems to the end of the decade5 will lead to millions of components. 
Combined with today s components mean time to failure (MTTF) it would make Exascale systems practically 
unusable since the whole system s MTTF will drop into the range of hours or even minutes.  Memory hierarchies 
and I/O The increasing gap between the increment of compute performance and bandwidth of both, memory 
and storage, will require  systems and the Cray XT series. One might argue that the latter are cluster 
systems, too. 4Examples are the K computer at RIKEN or the upcoming IBM Sequoia system at LLNL and Blue 
Waters at NCSA. 5Already JSC s Peta.op system JUGENE based on BlueGene/P technology has about 72,000 
nodes with 294912 cores. additional layers in the memory hierarchy of Exascale systems like higher level 
CPU caches or .ash memory as disk caches. Concurrency With ever increasing levels of parallelism it 
becomes harder and harder for the users to extract performance out of this systems. Additionally, it 
introduces new requirements on the scalability of Exascale applications as we discuss in the next section. 
Thus, in order to achieve the ambitious goal of Exascale by the end of the decade one has to hunt for 
new concepts. Having this challenges in mind the ability of clusters to reach Exascale has to be reviewed. 
In particular, the question has to be raised, if the central concept of clusters the utilization of 
commodity CPUs is competitive compared to proprietary developments for HPC. The yardstick regarding 
competitiveness for the next years is set by IBM s e.orts leading to the BlueGene/Q system. An analysis 
of preliminary results [6] create reasonable doubt that commodity CPUs will be su.cient in the future. 
This is due to their limitations on energy e.ciency and the superior ratio of price vs. performance of 
the BlueGene/P technology. Both originate from the fact that commodity CPUs carry too much dead freight 
required for their general purpose objectives compared to their .oating point abilities which are the 
key capabilities in the .eld of HPC. Since commodity processors will not be su.cient one has to strive 
for a new working horse in HPC systems. A good candidate are GPGPUs providing an order of magnitude higher 
.oating point performance compared to commodity CPUs today at the cost of limited usability. They represent 
the current endpoint of a long development of accelerators that might be used as co-processors enhancing 
the capabilities of general purpose processors signi.cantly. Even though the di.erent incarnations of 
accelerators are versatile6 they share some features leading to a superior energy e.ciency compared 
to commodity CPUs. This includes the lack of complex hardware mechanisms for out of order operations 
and speculative execution. Instead, simultaneous multi-threading (SMT) is used to keep the execution 
units busy while waiting for data fetched from memory. Furthermore, wide vector registers are used for 
.oating point operations in order to increase the instruction level parallelism (ILP). Since these mechanism 
are leading to more lightweight cores compared to the ones used in today s general purpose multi-core 
CPUs, current accelerator devices share the attribute of many-core architectures. In fact, 40% of the 
top 10 systems in the current TOP500 list are already equipped with accelerator cards. Nevertheless, 
the prevailing architecture of accelerated clusters7 suffers from severe limitation concerning balance 
and, thus, scalability. This is due to the fact that their accelerators are neither directly connected 
to the cluster node s main memory nor capable to directly send or receive data from their local memory. 
In the end, data is forced to be copied back 6Early examples are dedicated devices from ClearSpeed, ranging 
over processors originally developed for gaming like the Cell Broadband Engine and the aforementioned 
GPG-PUs, towards newer developments like Intel s MIC architecture. 7We will distinguish accelerated 
clusters, i.e. classical clusters comprising of nodes equipped with accelerators, and clusters of accelerators. 
and forth between main memory and the memory directly attached to accelerator using up the scarce resource 
of bandwidth on the node s system bus. At the same time latency for doing communication between the 
actual compute elements that is the accelerator is signi.cantly increased due to the required detour 
through main memory8 . A good way out of this dilemmas would be to have a cluster of accelerators. In 
this concept the node is consisting of an accelerator only accompanied by some memory and a high speed 
interconnect saving the commodity CPU. In particular, programs running on the accelerator cores shall 
be capable to initiate communication operations without the support of some commodity processor. A good 
example for this concept is the QPACE system [9] that was ranked #1 of the GREEN500 list [8] of the most 
energy e.cient HPC systems in the world. Nevertheless, this concept has limitations, too. Besides the 
problem of .nding accelerators that are capable to run autonomously they might be not .exible enough 
to drive a high speed interconnect e.ciently. Furthermore, the gain introduced by the direct connection 
between the compute element and the interconnect fabric might be wasted by the fact that accelerators 
 as highly dedicated devices su.er when running general purpose codes. Thus, a radically new concept 
is required for clusters systems that shall be competitive at Exascale. 4. CONSIDERATIONS ON SCALABILITY 
Talking about Exascale it is crucial to discuss the e.ects of parallelism on scalability. Amdahl s law 
[10] states that the scalability of a parallel program is inherently limited by the sequential part of 
this application. Granted that the run time of the parallel part of a program is given by p  t on a 
single processor, it leaves a sequential part of (1 - p)t. Executing this program on a computer with 
N processors, the parallel part s run time reduces to p  t/N. By de.nition, the sequential part will 
not bene.t from additional processors. The speedup S is given by the ratio of the run times on a parallel 
machine TN and on a serial machine TS , i.e.: TS (1 - p)+ p 1 S == p = p (1) (1 - p)+ (1 - p)+ TNNN In 
practice this speedup is limited by the sequential part of a given program. E.g. a program with a sequential 
part of just 1% will be limited in its speedup to 100 independent of the amount of processors used. Nevertheless, 
the practical impact of Amdahl s law today might be constricted. As an example the fact that some applications 
are able to scale on BlueGene systems with O(100000) processors shows that these seem to have a vanishing 
sequential part. In fact, the behavior of such applications is better described by Gustafson s law [11]. 
While Amdahl assumed that using larger machine will leave the problem untouched, in reality the problem 
size usually is scaled with the capabilities of the machine. E.g. if a scientist wants to solve a problem 
in molecular dynamics, a machine twice as capable usually is not used for solving the problem in half 
the time but to tackle a problem twice as big in the same time or to 8There are e.orts to attenuate the 
problems resulting from that, e.g. NVIDIA s GPUDirectTM [7] do a more detailed simulation which requires 
to double the amount of operations. Therefore, in Gustafson s law the amount of work done on a system 
N times more capable than the serial one is w = (1 - p)+ pN. Overall this leads to a practical speedup 
of (1 - p)+ Np S = =(1 - p)+ Np (2) (1 - p)+ p assuming that executing the sequential part will take 
the same amount of time on the parallel system as on the serial one. It is obvious that for a large number 
of processors the speedup is dominated by N. Of course, Gustafson postulates the possibility to implement 
the parallel portion of a program in a scalable way. Unfortunately, in practice there are several caveats. 
E.g. any collective communication operation on a parallel system like a global sum or a broadcast will 
inherently introduce costs beyond O(1) and is therefore not scalable. Additionally it turns out to be 
expensive to build really scalable fabrics of high speed interconnects comprising full .exibility on 
communication patterns at reasonable prices9 . This leads to the fact that scalability of the parallel 
part of an application might be signi.cantly restricted in reality. Thus, a di.erent viewpoint to the 
question on how to reach Exascale might be taken by looking at actual applications and their inherent 
scalability. Since HPC systems undoubtedly will still be massively parallel by the end of the decade 
with an even higher degree of parallelism than today, application s scalability will play a crucial role 
in order to exploit the computational power of such systems. Analyzing JSC s application portfolio today 
one .nds basically two classes of applications. 1. Highly scalable codes using very regular communication 
patterns. These are the codes that are able to exploit JSC s BlueGene/P system today and the BlueGene/Q 
in the near future. 2. Less scalable but signi.cantly more complex programs. Most often their codes 
require complicated communication patterns. These requirements constrain such applications to clusters. 
 A more detailed view on the second class unveils that among them are several applications that have 
highly scalable kernels, too. I.e. in principle they should be able to also exploit BlueGene type of 
machines. Nevertheless, in analogy to the serial work in Amdahl s law their scalability is limited by 
the least scalable kernel. With respect to Exascale it will be highly desirable to run applications of 
the second class on such systems, too. This wish is reinforced by the expectation that this class will 
become more important in the future. In fact, due to the growing degrees of parallelism in Exascale systems, 
even codes on the save side of the .rst class today might be shifted towards the latter class soon. Furthermore, 
we expect simulation codes to become more complex due additional aspects of a given scienti.c question 
to be included limiting their scalability. Last but not least problems completely out of range today 
due to their complexity might become feasible 9Full crossbar switches today are limited to several dozens 
ports; the trick to use a fat tree topology introduces an overhead of O(p 2) for p ports [12]. Figure 
1: Sketch of the DEEP architecture. at the availability of Exascale systems. Most probably they will 
belong to the second class, too. Putting all together a future cluster architecture requires di.erent 
ingredients to be .exible enough for running complex applications at the Exascale.  5. THE DEEP ARCHITECTURE 
5.1 The Cluster-Booster Concept The concept of the DEEP architecture is sketched in .gure 1. It foresees 
a Cluster element comprising of nodes (CN) connected by a highly .exible switched network labeled as 
In.niBand here. It is accompanied by a so called Booster element build out of Booster nodes (BN). Each 
BN hosts an accelerator capable to autonomously boot and run its own operating system. The BNs are interconnected 
by a highly scalable torus network sketched as red lines and named EX-TOLL in the .gure. Booster Interfaces 
(BI) connect the Booster and its network to the Cluster s fabric. The Cluster-Booster architecture provides 
several advantages: The Cluster element allows to run complex kernels. There is no restriction to run 
parallel kernels with complicated communication patterns or highly irregular codes as on today s highly 
scalable MPP machines.  The Booster element is able to run highly scalable kernels in the most energy 
e.cient way. The limitations of today s accelerated clusters are avoided by enabling the compute elements 
 i.e. the accelerators in the BNs  to do direct communication to remote BNs. The ratio between the 
amount of work to be executed on the commodity CPU and the accelerator cannot be expected to be .xed 
among di.erent applications or even between di.erent kernels of a single applications. The proposed 
architecture supports this fact by detaching the accelerator. Accelerators are no longer part of the 
physical CN and might be assigned to CNs in dynamical way.  At the same time the dynamical assignment 
of CNs and BNs improves resiliency. Faulty BNs do not a.ect CNs and vice versa.  Furthermore, the architecture 
supports the user to employ the high degree of parallelism of future machines. Today the type of kernels 
to be o.oaded onto accelerators is very limited due to their missing ability to communicate e.ciently 
with other accelerators. In contrast, the DEEP architecture allows more complex kernels to be o.oaded 
to a Booster. These kernels might even include communication, as long as the corresponding communication 
patterns are regular enough and do not swamp the torus topology. In this context the Booster might be 
seen as highly scalable system on its own. Thinking of the highly scalable codes that are able to exploit 
BlueGene or QPACE today, it should be possible to run them on the Booster alone. In the other extreme 
BNs might be assigned to single CNs and used in the same fashion as in today s accelerated clusters. 
Both use cases and anything in between are possible without modi.cation of the hardware concept but 
are just implemented by means of con.guration on the level of system and application software. To re.ect 
all this features, the architecture was named dynamical Exascale evaluation platform (DEEP). 5.2 Coping 
with Amdahl s Law The Cluster-Booster architecture aims to o.oads kernels with high scalability onto 
the Booster while leaving kernels with limited scalability on the Cluster element. Let s assume the highly 
idealized situation where the highly scalable code fraction is H and leaving a fraction of 1 - H for 
the less scalable parts. Given a number h of cores on the Booster and a number of f cores on the Cluster, 
and assuming the Cluster s multi-core units to be a factor c times faster than the Booster s many-core 
units, Amdahl s law predicts the speedup S of the DEEP Cluster-Booster Architecture: 1 S = (3) 1-HH + 
fc h For a 10 PFlop/s Booster with h = 500000, a Cluster with f = 10000, a di.erence on the e.ective 
performance of a factor of 4 between the Cluster s multi-core and the Booster s many-core units and a 
highly scalable code fraction of H =0.95, the speedup S would reach a value of 320000 as compared with 
a single core on the Booster. This corresponds to a parallel e.ciency of 64%. It is evident that the 
e.ciency strongly depends on the value of H. Since H is approaching 1 in a weak scaling scenario as advocated 
by Gustafson one can be optimistic to evade severe performance degradation that would be expected for 
O(k) concurrent kernels on the Booster. Note that the communication between cluster and Booster was 
not taken into account in this consideration. The optimal balance between h and f is speci.c for a given 
application. The DEEP architectural concept allows adjusting this balance dynamically. The recipe to 
achieve scalability for the DEEP grand challenge applications is to maximize H in the breakdown of the 
code into kernels while reducing data transfers to the bare minimum or by hiding the Cluster Booster 
communication behind computation.  5.3 The DEEP Hardware So far we have described above the Cluster-Booster 
architecture on an abstract level. Now we will concentrate on the actual implementation as planned within 
the DEEP project. The DEEP system will consist of two principal parts: an Intel Xeon based Cluster and 
an attached Booster using Intel Knights Corner accelerators. While the Cluster is using conventional 
components like Sandy Bridge processors or Mellanox QDR In.niBand HCAs and switches, the Booster will 
exploit the .rst implementation of Intel s Many Inte- Figure 3: EXTOLL block diagram. grated Cores (MIC) 
architecture. It is currently developed under the code name Knights Corner (KNC). The Booster will use 
the EXTOLL interconnect technology designed at University of Heidelberg as a communication network. 
5.3.1 Booster Node Card (BNC) The BNC is the physical realization of the Booster Nodes. It houses two 
independent BNs of the 3D Torus. Each one is comprising of a KNC card and an EXTOLL ASIC currently under 
development at EXTOLL GmbH, a spin-o. of University of Heidelberg. The KNC card includes the actual processor 
and memory based on GDDR5 technology. It is connected to the EXTOLL ASIC via a 16 PCIe gen2 interface. 
A baseboard management controller (BMC) will allow for management and monitoring of the BNC s components. 
The KNC accelerator is announced to have > 50 cores running an extension of the x86 instruction set[13]. 
The cores are much simpler than today s Xeon CPUs. They will operate in order and without speculative 
execution. Compute throughput is provided by vector computing units integrated with the cores, which 
can operate on 512 bits (8 double precision numbers) in one instruction and o.er advanced masking and 
memory transfer functionality. The cores have local caches and a shared L2 cache. Main memory is based 
on GDDR5, and we expect memory sizes on par with the latest GPGPUs. At Supercomputing 2011, a double 
precision DGEMM matrix multiply was shown on alpha silicon with >1 TeraFlop/s performance[14]. The EXTOLL 
architecture was designed from scratch for the needs of HPC. The technology speci.cally optimizes latency, 
message rate and scalability. Therefore the host software, the host interface, the network interface 
hardware and the actual network with its switch and link latency were optimized. EXTOLL is a one chip 
solution, realized either as FPGA today or as an ASIC in the near future. Figure 3 shows the top level 
block diagram of the EXTOLL device. The whole device can be divided into three main parts: the host interface, 
the network interface and the network. The host interface houses a PCIe IP core, which is able to operate 
in root port mode. It is attached to the network interface using an internal bridging hardware. The network 
interface features an on chip network called HTAX, which  Figure 2: High level view of the Cluster-Booster 
software stack. connects the host interface and all communication engines and functional units of EXTOLL 
to each other. There are three main communication engines available in EXTOLL. VELO (Virtualized Engine 
for Low Overhead) offers two sided communication semantics (send/receive) with very low latency and 
very high message rates. It might be used for an e.cient implementation of the handling of small to medium 
MPI messages. The second communication engine is RMA (Remote Memory Access) o.ering one sided operations. 
Both of these units are virtualized and o.er direct user space access for hundreds of processes, as 
required for the usage by many-core processors. The last communication engine is the SMFU (Shared Memory 
Functional Unit)[16]. It is capable to tunnel arbitrary PCIe requests over the EXTOLL network. A typical 
use case is non coherent, global address space programming using CPU loads and stores. Another application 
is the forwarding of device DMAs to memory of a remote EXTOLL node. The third component of EXTOLL is 
the actual network. It implements six links which can be interconnected arbitrarily. Therefore, a natural 
choice for the topology is a 3D torus. Beyond that EXTOLL also o.ers a so called 7th link . It might 
be used to attach out of torus nodes to the network, for example I/O nodes. The internal switch of EX-TOLL 
is a non-blocking crossbar. Packets are delivered in order, if deterministic routing is chosen. The routing 
is table based and not restricted to torus topologies.  5.3.2 Booster Interface Card (BIC) The interface 
between Booster and Cluster will be realized by Booster Interface Cards (BIC). It will contain a regular 
Sandy Bridge generation Intel CPU, the same baseboard management controller (BMC) as the BNC, a NIC for 
the EXTOLL network and a HCA for In.niBand connectivity. The two latter are connected to the CPU via 
PCIe. The BICs are responsible for booting up and controlling the KNCs over the EXTOLL network. The BIC 
CPU maps a boot image at a certain memory address, and the KNC cards access this image in a PCIe window, 
with EXTOLL tunneling the PCIe communication using SMFU. In addition, the BICs pass data between the 
In.niBand and EX-TOLL networks. 5.3.3 Interconnect Network and Topology To provide unlimited scalability 
of the Booster interconnect and match with HPC applications that use spatial data partitioning, a 3D 
torus con.guration has been chosen. EX-TOLL is currently implemented on FPGAs, and the .rst DEEP evaluation 
boards will therefore use FPGAs (Altera Stratix V). This provides full functionality, but reduced performance 
is expected. For the DEEP system, the plan is to move to an ASIC implementation currently under development. 
The DEEP Booster will consist of a total of 512 Booster Nodes arranged in a 8  8  8 topology. 32 BNs 
(i.e. 16 BNCs) will be housed in a chassis and are accompanied by two BICs building the interface to 
the Cluster of the DEEP system. Compared to the accumulated bandwidth of the 16 BNs connected to a single 
BIC or the 128 nodes in the Cluster the bandwidth of a single BIC appears to be limited. Nevertheless, 
since the Cluster-Booster architecture allows for o.oading complex kernels the amount of data that has 
to be sent back and forth between the two parts is con.ned. Furthermore, the ability of the BNs to directly 
communicate among each other constraints the majority of communication operations within the Booster 
and relieves the pressure from the BICs.  5.4 The Cluster-Booster Software Stack In order to bene.t 
from the Booster element of the DEEP architecture the user is forced to identify highly scalable kernels 
capable to be o.oaded. In a way this is similar to the identi.cation of kernels to be o.oaded onto the 
GPGPUs of accelerated clusters today.  Figure 4: Collective o.oad example of an application with 2D 
domain decomposition. A Cluster-Booster software stack as sketched in .gure 2 will support the user in 
expressing the di.erent levels of parallelism of the problem. The application s kernels of limited scalability 
remain on the Cluster part and makes use of a standard software stack comprising of standard compilers, 
MPI, etc. Highly scalable kernels will be o.oaded to the Booster with the help of a resource management 
layer. For that, a Cluster-Booster communication layer is required in order to bridge the gap between 
the more .exible Cluster interconnect and the highly scalable Booster network. In order to implement 
the highly scalable kernels on the Booster an additional software stack is required there. Since the 
Booster might be seen as a cluster of accelerators it is no surprise to .nd a software stack similar 
to the one on the Cluster. Besides native compilers supporting the processors of the Booster nodes a 
communication library is required. For simplicity MPI is chosen here, too. Of course in contrast to 
the Cluster-MPI the Booster-MPI has to support the Booster interconnect. The application itself is formed 
by both types of kernels running on the Cluster and Booster parts of the system glued together by the 
resource management layer.  5.5 Programming Model In the Exascale era a concurrency of at least O(109) 
is expected. Its management requires improvements in today s programming models. The most popular programming 
model currently used in scienti.c computing is based on message passing, using mainly MPI. However, MPI 
is starting to show some drawbacks on current Petascale machines, that will become more important in 
future systems. Frequent synchronization points present in many applications will hinder scaling. Replication 
of data across processes, and the increasing memory requirements for managing O(106) connections will 
be showstoppers. Therefore, improvements in programming models are required. The present relies on hybrid 
programming models with MPI and an intra-node parallel programming model (OpenMP, CUDA, OpenCL). Some 
of those programming models takes advantage of accelerators. OmpSs is a proposal with means to use CPUs 
and accelerators, providing at the same time asynchronicity through taski.cation [22], allowing improved 
programmability and performance advantages. Other approaches propose more radical changes in design, 
getting rid of MPI at all [23]. The nature of the DEEP architecture requires extra innovations in the 
programming model. Current paradigms can not cope with it, since they provide means to o.oad code from 
a CPU to an accelerator that are part of the same node. In DEEP the accelerators are decoupled from the 
nodes in the Cluster side, and become part of the Booster. This feature provides more .exibility than 
in traditional GPGPU clusters, allowing a near optimal use of CPU and accelerator cores. Given the foreseen 
requirements for Exascale computing, and the features of the DEEP architecture, the o.oad mechanism 
has to provide: An easy to use interface for o.oading computing and data to the Booster.  Certain degree 
of portability.  Individual o.oading, to enable applications to o.oad independent kernels or tasks to 
accelerators.  Collective o.oading, to enable applications to o.oad collaborative kernels or tasks to 
the Booster.  Asynchronicity to allow overlapping of di.erent phases in the Cluster and in the Booster. 
 The .rst two items can be easily achieved using an annotations approach. The parameters speci.ed in 
the annotations will tell the runtime, if it should o.oad the function to the Booster, to an accelerator, 
or to another CPU core. This way, small changes in the runtime and the application code will enable portability 
without major rewriting of the code. The third item requires an o.oad mechanism similar to any other 
o.oad mechanism present in current clusters of accelerators. The only major di.erence is that the accelerator 
is not local to the cluster node, providing more .exibility, but at the same time the data has to be 
moved through slower channels than the internal buses, which requires a bigger granularity in the o.oaded 
kernels, to overcome the penalty for transferring the data. The fourth item is a collective o.oad, where 
every process in the Cluster o.oads its data to the Booster roughly at the same time, allowing the processes 
on the accelerators to communicate and cooperate. This is necessary e.g. when the application decomposes 
the domain and some operations are more suitable for the Cluster and others for the Booster. Then on 
both sides of the systems processes have to exchange data such as the boundaries of the local domains. 
An example of this is depicted in .gure 4. OmpSs s model satis.es the last item individually on the Cluster 
and the Booster, since it already supports multicore architectures like Xeon is going to be ported to 
the MIC architecture. However, it is desirable that the o.oad mechanism does not block the execution 
in the Cluster while the Booster performs the required computations. For that two mechanisms are possible: 
(1) An o.oading task runtime approach, o.oading individual tasks to the Booster and automatically building 
the dependency graph between tasks, like in the OmpSs model; and (2) an approach where the function to 
be o.oaded returns a handler that the programmer can use for checking for completion later.  6. DEEP 
S SCIENTIFIC APPLICATIONS The amount of today s scienti.c applications that will be able to e.ciently 
run on the millions of cores of a future Exascale machine is very limited. A reason for that is the 
fact that even highly scalable applications often contain kernels with complex communication patterns. 
These limit the overall scalability of such codes. One of the main goals in DEEP is to enlarge the portfolio 
of Exascale applications using the DEEP Cluster for the less scalable part of the applications while 
their highly scalable parts pro.t from the computing power of the Booster. To evaluate the DEEP Concept 
six HPC applications representative for future Exascale computing and data handling requirements will 
be ported to the DEEP system in the coming three years. These applications coming from the .elds of 
Health and Biology, Climatology, Seismic Imaging, Computational Engineering, Space Weather, and Superconductivity 
 have a high impact on industry and society and present very di.erent algorithmic structures. In this 
paper we focus on one of the selected applications: Space Weather. After a brief analysis we discuss 
how it can pro.t from the characteristics of the DEEP architecture. The aim of Space Weather simulations 
is to emulate the magnetized plasma traveling from the Sun to the Earth. They are used to study the conditions 
on the Sun, solar wind, and Earth s magneto and ionosphere that can disrupt spaceborne and ground-based 
technology. iPIC3D [17] (written in C++ and MPI) is an implicit-moment-method particle-in-cell code [18, 
19] that allows studying a large portion of the system while retaining the full detailed microphysics. 
The main challenge in this HPC application is its wide variety of space and temporal scales due to the 
vast gap between the scales in which the plasma electrons and ions interact. Exascale computing would 
allow Space Weather researchers to simulate the whole Sun-Earth system in a single simulation run, something 
impossible with today s Petascale machines. iPIC3D simulates the evolution of particles on a discrete 
 Figure 5: Block diagram showing the operations done in iPIC3D at each time step. grid, taking the surrounding 
electric and magnetic .elds into account. The logical block operations performed in iPIC3D for each time 
step are shown in .gure 5, where the most communication intensive block (B2) is marked in green and the 
most computational expensive blocks (B4 and B5) in red. iPIC3D shows a very good scalability. Most of 
the execution time is spent in the particle simulation, making this part of the code (blocks B4 and B5) 
a perfect candidate to be o.oaded to the DEEP Booster. Furthermore, the only communication required in 
these blocks is in the form of point to point, nearest neighbor communication, needed in B4 for the few 
particles that exchange subdomain, i.e. particles that move from one BN to a neighbor one. On the other 
hand the .eld solver (block B2) has a small impact on the execution time but is more demanding for the 
network layer as it requires global communication. Therefore it is .tting naturally on the DEEP Cluster. 
In this scenario, communication between Cluster and Booster is very limited: only the .elds and moments(1% 
of the memory) need to be transferred from one side to the other, while all the particles (99% of the 
memory) stay on the Booster side. Communication and computation can be easily overlapped in the Space 
Weather application, for instance by running in parallel several instances of the same scenario with 
dif ferent pseudo-random input data as required for ensemble simulations. The approach chosen for iPIC3D 
in DEEP is presented in .gure 6, and consists in dividing blocks B2 and B4 into various steps, such that 
at every moment two concur rent operations, one on the Booster and one on the Cluster, are executed. 
Barriers are required at key points to synchro nize the process. In this approach the two most expensive 
operations are done simultaneously: the Poisson solver in the Cluster and the particle pusher to the 
Booster. Estima tions predict similar time span for both Booster (faster but with many more operations 
to do) and Cluster (slower but with fewer operations to do). This method constitutes a simple modi.cation 
of the stan dard iPIC3D and is based on solid experience, since o.oad ing particles onto accelerators 
is commonplace10 . No disad vantages or impact on the physics accuracy are expected, 10Delayed Poisson 
correction is novel but similar methods based on incomplete correction e.g. Marder [20] and Lang don 
[21] methods exist  Figure 6: Scheme of the propose method to adapt iPIC3D to DEEP architecture while 
the cost of the run is reduced essentially by the speed increase of the Booster over the Cluster. At 
this point a rough estimation of the bene.ts of the DEEP architecture can be made. On the JUDGE cluster 
at JSC, a Scalasca pro.ling run with 1024 processes gave the information on table 1 Part Execution time 
Time on communications B2 8% 5.5% B4 60% 20% B5 30% 8% Table 1: Pro.ling of iPIC3D The data implies 
that B1 and B3 can be safely ignored, since together they account for ~2% of the time. B5 takes 30% of 
the execution time, but since the communications in this phase use only 8% this, obviously most of the 
time the application is preparing the structures to be communicated. In DEEP the ratio between Cluster 
nodes and Booster nodes is 1:4. The ratio of the memory per node will most likely be 3:1. The peak compute 
power per node ratio can be assumed as 1:4 (~250 GFlop/s for 2x Sandy Bridge processors vs. ~1000 GFlop/s 
for KNC). Bearing all the previous statements in mind the following estimations can be made: B4 will 
be executed on the Booster, which means a maximum theoretical speedup of 44 = 16 for the computational 
part. This is hardly achievable in practice. Assuming a 50% e.ciency of the Booster relative to the Cluster 
which is a more feasible number for this application, when vector units are not fully exploitable, the 
speedup would be 8. This applies to 80% of the run time of B4. The other 20% is spend on communications. 
EXTOLL s features might allow to speed up those communications by a factor of 6 realis tically11, reducing 
the total time of B4 to a ~13.5% of its original time. Even though B5 is not easy to be optimized on 
the Booster, the higher number of cores of KNC vs. Xeon can help to speed it up. KNC s cores are simpler, 
work at a lower fre quency, and are therefore slower than standard cores12 . As suming a relative speed 
of cores of 1:4, the tasks required to put data in place in order to be communicated will be 4 slower. 
However, a Booster node has ~4 the number of cores of a Cluster node, and it can manage twice the num 
ber of hardware threads. Furthermore, the Booster will have 4 more nodes, and therefore every node will 
have less data to reorder. The speedup of this tasks can be naively rated at ~8, leading to a total 
time of B5 of ~20% of its original time. We have on one hand that B4 and B5 can be reduced to ~13.5% 
and ~20% of their original time. Moreover, B2 can be executed concurrently on the Cluster, while B4 and 
the most expensive part of B5 are executed on the Booster, drag ging it out of the critical path. Therefore, 
just B4 and B5 will be on the critical path. This gives a run time reduction to 0.02 + 0.6  0.135+ 0.3 
0.2= ~0.161 of the original. On the other hand this result is only achievable due to the fact 111.5 
better performance per link, and 4 more links, since the data is distributed over 4 more nodes 12The 
superior .oating point capabilities of KNC might not come into play for this task. that communication 
between Cluster and Booster is limited to a small amount of data. Furthermore, this application enables 
for hiding the communication behind computation by introducing ensemble simulation. Nevertheless, this 
shows that there are applications capable to exploit the potential of the DEEP architecture. Just for 
comparison purposes, the run time required for the application using a cluster architecture, with the 
same number of nodes as nodes in the DEEP architecture (Cluster nodes and Booster nodes), assuming linear 
scaling, would be ~1/5. However, the Booster part, including I/O, will consume roughly 2 more power 
than the Cluster part. This leads to a power consumption per node of roughly half of a Cluster node allowing 
the application to run consuming 0.2+0.8  0.5= ~0.6 the amount of energy than using a standard architecture, 
ignoring storage and switches.  7. CONCLUSIONS AND OUTLOOK We presented a novel HPC architecture extending 
the concept of classical clusters and newer developments of accelerated clusters, i.e. cluster systems 
with nodes equipped with accelerators besides the commodity processors. Our concept includes a Cluster 
that is accompanied by a so called Booster comprising of nodes of accelerators and a high speed interconnect 
with torus topology. We argue that our concept provides the additional .exibility and scalability that 
is required to pave cluster architectures the way into the future of Exascale. We report on the physical 
implementation of this concept as it will be realized in the DEEP project. Furthermore, both the hardware 
and software architectures of the DEEP system are presented. We use a Space Weather application in order 
to evaluate the Cluster-Booster concept on a theoretical level, showing important bene.ts in performance 
and energy e.ciency. 8. ACKNOWLEDGMENTS The research leading to these results has received funding 
from the European Community s Seventh Framework Programme (FP7/2007-2013) under Grant Agreement n . 287530. 
 9. REFERENCES [1] http://www.top500.org [2] http://www.deep-project.eu [3] http://http://www.mpi-forum.org 
[4] Gordon E. Moore, Cramming more components onto integrated circuits. , Electronics. 19, Nr. 3, 1965, 
pp. 114-117. [5] www.cse.nd.edu/Reports/2008/TR-2008-13.pdf [6] http://www.theregister.co.uk/2010/11/22/ibm_ 
blue_gene_q_super [7] http://developer.nvidia.com/gpudirect [8] http://www.green500.org [9] H. Baier 
et al., QPACE: power-e.cient parallel architecture based on IBM PowerXCell 8i , Computer Science -R&#38;D 
25 (2010), pp. 149-154. doi:10.1007/s00450-010-0122-4. [10] Gene Amdahl (1967), Validity of the Single 
Processor Approach to Achieving Large-Scale Computing Capabilities , (PDF), AFIPS Conference Proceedings 
(30), pp. 483-485. [11] John L. Gustafson, Re-evaluating Amdahl s Law , Communications of the ACM 31(5), 
1988, pp. 532-533. [12] Charles Clos, A Study of Non-blocking Switching Networks , The Bell System Technical 
Journal, 1953, vol. 32, no. 2, pp. 406-424 [13] http://www.intel.com/pressroom/archive/ releases/2010/20100531comp.htm 
[14] http://newsroom.intel.com/servlet/JiveServlet/ download/38-6968/Intel_SC11_presentation.pdf [15] 
Mondrian Nussle et al., A resource optimized remote-memory-access architecture for low-latency communication 
, The 38th International Conference on Parallel Processing (ICPP-2009), September 22-25, Vienna, Austria. 
[16] H. Froning und H. Litz, E.cient Hardware Support for the Partitioned Global Address Space, 10th 
Workshop on Communication Architecture for Clusters (CAC2010), co-located with 24th International Parallel 
and Distributed Processing Symposium (IPDPS 2010), Atlanta, Georgia, 2012. [17] S. Markidis, G. Lapenta 
and Rizwan-Uddin, Multi-scale simulations of plasma with iPIC3D , Mathematics and Computers in Simulation, 
pp. 1509-1519, 2010. [18] J. U. Brackbill and D. W. Forslund, Simulation of low frequency, electromagnetic 
phenomena in plasmas , Journal of Computational Physics, 1982, p. 271. [19] P. Ricci, G. Lapenta and 
J. U. Brackbill, A simpli.ed implicit Maxwell solver , Journal of Computational Physics (2002), p. 117. 
[20] B. Marder, A method for incorporating Gauss law into electromagnetic PIC codes , J. Comput. Phys., 
vol. 68 (1987), p. 48 [21] A. Bruce Langdon, On enforcing Gauss law in electromagnetic particle-in-cell 
codes , Computer Physics Communications, vol. 70, Issue 3 (1992). [22] A. Duran, E. Ayguae, R. M. Badia, 
J. Labarta, L. Martinell, X. Martorell and J. Planas, OmpSs: A Proposal for Programming Heterogeneous 
Multi-Core Architectures , in Parallel Processing Letters, vol. 21, Issue 2 (2011) pp. 173-193. [23] 
G. R. Gao, T. L. Sterling, R. Stevens, M. Hereld and W. Zhu, ParalleX: A Study of A New Parallel Computation 
Model , in Proc. of 21th International Parallel and Distributed Processing Symposium (IPDPS 2007), Long 
Beach, California, USA   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>European Community's Seventh Framework Programme (FP7/2007-2013)</funding_agency>
			<grant_numbers>
				<grant_number>287530</grant_number>
			</grant_numbers>
		</article_sponsors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2012</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
</content>
</proceeding>
